Link,Tools_Found,Methods_Found,Post_Text,Cluster
https://dev.to/mobidev/applying-ai-for-early-dementia-diagnosis-and-prediction-5en7,,,"applying ai for early dementia diagnosis and prediction - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse andrew makarov for mobidev posted on jul 18, 2022 • originally published at mobidev.biz applying ai for early dementia diagnosis and prediction # ai # nlp # datascience mobidev would like to acknowledge and give its warmest thanks to the dementiabank which made this work possible by providing the data set. mental illnesses and diseases that cause mental symptoms are somewhat difficult to diagnose due to the uneven nature of such symptoms. one such condition is dementia. while it’s impossible to cure dementia caused by degenerative diseases, early diagnostics help reduce symptom severity with the proper treatment, or slow down illness progression. moreover, about 23% of dementia causes are believed to be reversible when diagnosed early. communicative and reasoning problems are some of the earliest indicators used to identify patients at risk of developing dementia. applying ai for audio and speech processing significantly improves diagnostic opportunity for dementia and helps to spot early signs years before significant symptoms develop. in this study, we’ll describe our experience creating a speech processing model that predicts dementia risk, including the pitfalls and challenges in speech classification tasks. ai speech processing techniques artificial intelligence offers a range of techniques to classify raw audio information, which often passes through pre-processing and annotation. in audio classification tasks we generally strive to improve the sound quality and clean up any present anomalies before training the model. if we speak about classification tasks involving human speech, generally, there are two major types of audio processing techniques used for extracting meaningful information: automatic speech recognition or asr is used to recognize or transcribe spoken words into a written form for further processing, feature extraction, and analysis. natural language processing or nlp , is a technique for understanding human speech in context by a computer. nlp models generally apply complex linguistic rules to derive meaningful information from sentences, determining syntactic and grammatical relations between words. pauses in speech can also be meaningful to the results of a task, and audio processing models can also distinguish between different sound classes like: human voices animal sounds machine noises ambient sounds all of the different sounds above may be removed from the target audio files because they can worsen overall audio quality or impact model prediction. how does ai speech processing apply to dementia diagnosis? people with alzheimer’s disease and dementia specifically have a certain number of communication conditions such as reasoning struggles, focusing problems, and memory loss. impairment in cognition can be spotted during the neuropsychological testing performed on individuals. if recorded on audio, these defects can be used as features for training a classification model that will find a difference between a healthy person, and an ill one. since an ai model can process enormous amounts of data and maintain accuracy of its classification, the integration of this method into dementia screening can improve overall diagnostic accuracy. dementia-detection systems based on neural networks have two potential applications in healthcare: early dementia diagnostics . using recordings of neuropsychological tests, patients can learn about the early signs of dementia long before brain cell damage occurs. applying even phone recordings with test results appears to be an accessible and fast way to screen population compared to conventional appointments. tracking dementia progression . dementia is a progressive condition, which means its symptoms tend to progress and manifest differently over time. classification models for dementia detection can also be used to track changes in a patient’s mental condition and learn how the symptoms develop, or how treatment affects manifestation. so now, let’s discuss how we can train the actual model, and what approaches appear most effective in classifying dementia. how do you train ai to analyze dementia patterns? the goal of this experiment was to detect as many sick people as possible out of the available data. for this, we needed a classification model that was able to extract features and find the differences between healthy and ill people. the method used for dementia detection applies neural networks both for feature extraction and classification. since audio data has a complex and continuous nature with multiple sonic layers, neural networks appear superior to traditional machine learning for feature extraction. in this research 2 types of models were used: speech-representation neural network which accounts for extracting speech features (embeddings), and classification model which learns patterns from feature-extractor output in terms of data, recordings of cookie theft neuropsychological examination are used to train the model. image source: researchgate.net in a nutshell, cookie theft is a graphic task that requires patients to describe the events happening in the picture. since people suffering from early symptoms of dementia experience cognitive problems, they often fail to explain the scene in words, repeat thoughts, or lose the narrative chain. all of the mentioned symptoms can be spotted in recorded audio, and used as features for training classification models. analyzing data for the model training and evaluation we used a dementiabank dataset consisting of 552 cookie theft recordings. the data represents people of different ages split into two groups: healthy, and those diagnosed with alzheimer diseases — the most common cause of dementia. the dementiabank dataset shows a balanced distribution of healthy and ill people, which means neural networks will consider both classes during the training procedure, without skewing to only one class. the dataset contains samples with different length, loudness and noise level. the total length of the whole dataset equals 10 hours 42 min with an average audio length of 70 seconds. in the preparation phase, it was noted that the duration of the recordings of healthy people is overall shorter, which is logicall, since ill people struggle with completing the task. however, relying just on the speech length doesn’t guarantee meaningful classification results. since there can be people suffering from mild symptoms, or we can become biased for quick descriptors. data preprocessing before actual training, the obtained data has to go through a number of preparation procedures. audio processing models are sensitive to the quality of recording, as well as omission of words in sentences. poor quality data may worsen the prediction result, since a model may struggle to find a relationship between the information where a part of recording is corrupted. preprocessing sound entails cleaning any unnecessary noises, improving general audio quality, and annotating the required parts of an audio recording. the dementia dataset initially has approximately 60% poor quality data included in it. we have tested both ai and non-ai approaches to normalize loudness level and reduce noises in recordings. huggingface metricgan model was used to automatically improve audio quality, although the majority of the samples weren’t improved enough. additionally, python audio processing libraries and audacity were used to further improve data quality. for very poor quality audio, additional cycles of preprocessing may be required using different python libraries, or audio mastering tools like izotope rx . but, in our case, the aforementioned preprocessing steps dramatically increased data quality. during the preprocessing, samples with the poorest quality were deleted, accounting for 29 samples (29 min 50 sec length) which is only 4% of total dataset length. approaches to speech classification as you might remember, neural network models are used in conjunction to extract features and classify recordings. in speech classification tasks, there are generally two approaches: converting speech to text, and using text as an input for the classification model training. extracting high-level speech representations to conduct classification on them. this approach is an end-to-end solution, since audio data doesn’t require conversion into other formats. in our research, we use both approaches to see how they differ in terms of classification accuracy. another important point is that all feature extractors were trained in two steps. on the first iteration, the model is pre-trained in a self-supervised way on pretext tasks such as language modeling (auxiliary task). in the second step, the model is fine-tuned on downstream tasks in a standard supervised way using human-labeled data. the pretext task should force the model to encode the data to a meaningful representation that can be reused for fine-tuning later. for example, a speech model trained in a self-supervised way needs to learn about sound structure and characteristics to effectively predict the next audio unit. this speech knowledge can be re-used in a downstream task like converting speech into text. modeling to evaluate the results of model classification, we’ll use a set of metrics that will help us determine the accuracy of the model output. recall evaluates the fraction of correctly classified audio records of all audio records in the dataset. in other words, recall shows the number of records our model classified as dementia. precision metric indicates how many of those records classified with dementia are actually true. f1 score was used as a metric to calculate harmonic mean out of recall and precision. the formula of metric calculation looks like this: f1 = 2*recall*precision / (recall + precision). additionally, as in the first approach when we converted audio to text, word error rate is also used to calculate the number of substitutions, deletions, and insertions between the extracted text, and the target one. approach 1: text-to-speech in dementia classification for the first approach, two models were used as feature extractors: wav2vec 2.0 base and nemo quartznet . while these models convert speech into text, and extract features from it, the huggingface bert model performs the role of a classifier. extracted by wav2vec text appeared to be more accurate compared to quartznet output. but on the flipside, it took significantly longer for wav2vec 2.0 to process audio, which makes it less preferable for real-time tasks. in contrast, quartznet shows faster performance due to a lower number of parameters. the next step was feeding the extracted text of both models into the bert classifier for training. eventually, the training logs showed that bert wasn’t trained at all. this could possibly happen due to the following factors: converting audio speech into text basically means losing  information about the pitch, pauses, and loudness. once we extract the text, there is no way feature extractors can convey this information, while it’s meaningful to consider pauses during the dementia classification. the second reason is that the bert model uses predefined vocabulary to convert word sequences into tokens. depending on the quality of recording, the model can lose the information it’s unable to recognize. this leads to omission of, for example, incorrect words that still make sense to the prediction results. as long as this approach doesn’t seem to bring meaningful results, let’s proceed to the end-to-end processing approach and discuss the training results. approach 2: end-to-end processing neural networks represent a stack of layers, where each of the layers is responsible for catching some information. in the early layers, models learn the information about raw sound units also called low-level audio features. these have no human-interpretable meaning. deep layers represent more human-understandable features like words and phonemes. end-to-end approach entails the use of speech features from intermediate layers. in this case, speech representation models (albert or hubert) were used as feature extractors. both feature extractors were used as a transfer learning while classification models were fine-tuned. for these classification tasks we used two custom s3prl downstream models: an attention-based classifier that was trained on snips dataset and a linear classifier that is trained on fluent commands dataset, but eventually both models were fine-tuned using dementia dataset. looking at inference results of the end-to-end solution, it’s claimed that using speech features, instead of text, with fine-tuned downsample models led to more meaningful results. namely, the combination of hubert and an attention-based model shows the most concise result among all approaches. in this case, classifiers learned to catch relevant information that could help differentiate between healthy people and those with dementia. for the explicit description of what models and methods for fine-tuning were used, you can download the pdf of this article. how to improve the results? given the two different approaches to dementia classification with ai, we can derive a couple of recommendations to improve the model output: use more data . dementia can have different manifestations depending on the cause and patient age, as symptoms will basically vary from person to person. obtaining more data samples with dementia speech representations allows us to train models on more diverse data, which can possibly result in more accurate classifications. improve preprocessing procedure . besides the number of samples, data quality also matters. while we can’t correct the initial defects in speech or actual recording, using preprocessing can significantly improve audio quality. this will result in less meaningful information lost during the feature extraction and have a positive impact on the training. alter models . as an example of end-to-end processing, different upstream and downstream models show different accuracy. trying different models in speech classification may result in improvement of classification accuracy. as the test results show, applying neural networks to analyzing dementia audio recordings can generate accurate suggestions. training neural networks for speech classification tasks is a complex exercise that requires data science expertise as well as audio processing knowledge. top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand dallasapper dallasapper dallasapper follow joined jun 11, 2022 • aug 19 '22 dropdown menu copy link hide glad there are more articles on this topic! maybe they will find the cure soon__ like comment: like comment: 2 likes like comment button reply collapse expand reservedzz reservedzz reservedzz follow joined aug 20, 2022 • aug 20 '22 dropdown menu copy link hide glad there is more and more research in this area. both my grandparents had severe cases of dementia, and we were not able to help them in any way except by caring about them. like comment: like comment: 2 likes like comment button reply collapse expand mokhtariadis mokhtariadis mokhtariadis follow joined sep 1, 2022 • sep 1 '22 dropdown menu copy link hide but i hope that by the time i age and develop similar issues, there will be a better cure for them. i have taken a brain test , which shows i am in danger. i really don’t want to get dementia or alzheimer’s when i am old. i do some mental gymnastics every day, which seems to help, but i am still afraid that all the progress i made will disappear once i become an old man. like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse mobidev follow ai/ ml / ds / ar / iot mobidev is a software engineering company with business units in the us and the uk, as well as r&d centers in poland and ukraine. mobidev has 13 years of experience building ai-powered solutions, implementing ml, ds, ar and iot. ml consulting more from mobidev 12 augmented reality trends of 2023: new milestones in immersive technology # mobile # ai # webdev how to build face recognition app # datascience # machinelearning 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/voxel51/data-augmentation-is-still-data-curation-3e1g,,,"data augmentation is still data curation - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse jimmy guerrero for voxel51 posted on mar 5, 2024 • edited on apr 2, 2024 • originally published at voxel51.com data augmentation is still data curation # computervision # machinelearning # ai # datascience author: jacob marks (machine learning engineer at voxel51 ) how to test your transformations before training illustration of data augmentation applied to a single natural image. traditionally, data augmentation is performed on-the-fly during training. this is great... if you know exactly what augmentations you want to apply to your dataset. however, if you're just getting started with a new dataset, you may not know what augmentations are appropriate for your data. should you include rotations? how much blurring is reasonable? does it make sense to employ random cropping? these questions just scratch the surface. when effective, data augmentation can significantly boost model performance by reducing overfitting and turning a small set of collected data into a much larger and more diverse data moat. but when left unchecked, data augmentation transformations can completely confuse your model, burying the original high quality data in a glut of digital garbage. this post will introduce the process of data augmentation, highlight a few common failure modes in computer vision, and show you how to avoid similar pitfalls in your own pipelines. in order to do so, we will use the open source computer vision libraries fiftyone and albumentations to generate, visualize, and understand our data transformations in real time! what is data augmentation broadly speaking, data augmentation is any process that involves increasing the size of the training set by modifying the original data. typically, data augmentation is used to fill expected gaps in the original data and reduce overfitting to the specific data that you were able to collect and curate. it is also a handy technique for mitigating class imbalance : by augmenting the number of samples for underrepresented classes, you can restore balance to the training distribution. often, augmentations can account for 90% of the training data, or even more. in the context of computer vision, modifications can be made via geometric transformations like rotations and reflections, transformations which blur or add noise, or transformations that simulate different lighting conditions, such as changing the brightness, contrast, or saturation. for most of these augmentations, if you just need to transform the raw images, then torchvision’s transforms module is a great solution. if you want to take your labels (bounding boxes, masks, and keypoints) along for the ride, then you’ll need a purpose-built image augmentation library, such as albumentations , imgaug , or augmentor . there are also more sophisticated image augmentation techniques for changing the scenery, background, and weather conditions in images. if you’re interested in this level of control over your augmentations, check out kopikat and stability ai’s sky replacer . while data augmentation itself does not include completely synthetic data generation, augmentation is often used in conjunction with synthetic data generation approaches. synthetic data from nvidia omniverse can be orders of magnitude cheaper than collecting similar data in the field — combining this with (computationally inexpensive) data augmentation can lead to still further cost savings! the perils of blind data augmentation when applied irresponsibly, data augmentations can degrade model performance and have severe real–world consequences. some transformations clearly push beyond the bounds of the desired data distribution — too much blurring makes an image unrecognizable; vertically flipping a portrait (leading to an upside down face) is clearly undesirable for most use cases. but the damage done by blindly boosting dataset size can be far more subtle and pernicious. let’s look at two examples, to make this explicit. suppose you’re working for a wildlife conservation organization, using computer vision to count the number of blue-throated macaws . at last count, there were only around 350 in the wild , so you only have a few images to start with, and you want to augment your data. blue-throated macaw. image courtesy of wikimedia commons your field cameras take pretty high-resolution images, so you augment the data by randomly cropping 600x600 patches from your original images. when you randomly crop, some of the resulting augmentations look  like this: 600x600 pixel random crops of the image above. but there’s a problem. if you use this to train your model, the model might incorrectly tag blue-and-gold macaws — which are far more abundant ( more than 10,000 in the wild ), share an overlapping geographic range and apart from their head look pretty similar. this might significantly throw off your population estimates. blue and yellow macaw. image courtesy of ketian chen to hammer this idea  home, let’s look at another example. suppose you’re building a model to detect pneumonia from chest x-rays. typically, pneumonia shows up in these images as an abnormally opaque region within the chest, so teaching a neural net to diagnose it should be possible, but you only have hundreds of images — far too few to train your desired model. one of the augmentations you are interested in performing is changing the contrast in the images. each lab from which you are receiving data sends you x-ray images with different amounts of contrast, so it seems reasonable to turn each image into a set of images across the spectrum of contrast. but there’s a problem here too. turning the contrast up or down may be viable for some images. however, too high of a contrast can also change the perceived diagnosis. consider the image on the left side below, of a non-pneumatic patient from the chestx-ray14 dataset . now look at the image on the right, after the contrast has been increased and a region made substantially more opaque. if we retained the training label from the left, we would be telling the model that images like this are non-pneumatic. this could potentially cause confusion and result in false negative diagnoses. left: lung without pneumonia (image from chestx-ray14 dataset ). right: contrast-heightened augmentation of left image. testing transformations with albumentations and fiftyone the examples highlighted in the last section may not apply in your use case, but there are countless ways that augmentations can make a mess out of high quality data. albumentations has 80+ transformations , many of which give you multiple control knobs to turn. and these transformations can be composed, altogether amounting to a massive space of possible augmentations. to ensure that your augmentations are reasonable, in domain, and add diversity to your dataset, it is absolutely essential that you test out your transformations before including them in a training loop. fortunately, the albumentations plugin for fiftyone allows you to do just this! in particular, you can: apply albumentations transformations view samples generated by last augmentation save augmentations to the dataset , and save transformations you found useful the augmentation transforms not only the raw image, but also any object detections , keypoints , instance segmentations , semantic segmentations , and heatmap labels on the transformed samples. setup to get started, first make sure you have fiftyone and albumentations installed: pip install -u fiftyone albumentations enter fullscreen mode exit fullscreen mode then download the albumentations plugin with fiftyone’s plugin cli syntax: fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin enter fullscreen mode exit fullscreen mode for this walkthrough, we’ll pretend that our goal is to train a vision model for an autonomous vehicle application, but we are starting from just a handful of labeled images. in particular, we’ll take just the first 10 images from the kitti dataset , which contains left stereo images from road scenes. import fiftyone as fo
import fiftyone.zoo as foz

dataset = foz.load_zoo_dataset(""kitti"", split=""train"", max_samples=10)
session = fo.launch_app(dataset) enter fullscreen mode exit fullscreen mode to make things more fun — and to show that this plugin allows you to experiment with all different types of labels — let’s add some pose estimation keypoints with ultralytics, and some relative depth maps with hugging face’s transformers library: pip install -u transformers ultralytics enter fullscreen mode exit fullscreen mode ## add depth maps
from transformers import automodelfordepthestimation
depth_model = automodelfordepthestimation.from_pretrained(
    ""intel/dpt-large""
)
dataset.apply_model(depth_model, ""depth"")


## add keypoints
from ultralytics import yolo
pose_model = yolo('yolov8x-pose.pt')
dataset.apply_model(pose_model, ""keypoints"")

session = fo.launch_app(dataset) enter fullscreen mode exit fullscreen mode creating augmentations pressing the backtick "" ` "" key on the keyboard, and typing “augment” in. press the augment_with_albumentations option. this is an operator in the fiftyone plugin system, and by interacting with the ui-based input form, we will be able to specify what transform we want to apply. let’s try a simple example of randomly cropping boxes out of each image. to do so, we will use the randomcropfromborders transform from albumentations: notice how as we interact with the input form, the contents dynamically change. in this case, when we select the transformation we want to apply, we are greeted with input items for each argument taken by that transform function. this is made possible through the use of python’s inspect module — each argument is processed (input and output) in semi-automated fashion by utilizing the docstrings and function signatures of albumentations’ transformations. also notice that we chose to generate just one augmentation per sample from this transform — hence going from 10 to 20 samples. for transformations which involve randomness, it can be helpful to generate multiple augmentations to investigate the broader range of possible generations. isolating the augmented samples if we wanted to isolate the samples we just generated, as opposed to viewing them in line with the original samples, we could do so by invoking the view_last_albumentations_run operator: if we want to keep them, then we can save the augmentations to the dataset with the save_albumentations_augmentations operator. otherwise, they will be treated as temporary — for the purposes of experimentation — and deleted when you next generate augmentations. inspecting the generating transformation perhaps even more importantly, running get_last_albumentations_run_info will display for us a formatted compilation of all of the parameters used to construct the prior transformation and generate these augmentations: if we are satisfied with this transformation and the hyperparameters employed, we can save it, either for composition with other transforms in our exploration, or to use in your inference pipelines: composing transformations in production-grade inference pipelines, augmentations are often generated by composing multiple augmentation transformations to each base sample. for instance, you might apply a random brightness shift, followed by a random crop, and finally some sort of blur. let’s see this in action: this is of course just one combination, and yet even this indicates that perhaps if we want to combine cropping with brightness changes, we should be intentional about the minimum size of the cropped region or the maximum amount of darkening we add. and this will all depend on the particular application! conclusion whether you’re building a low-latency embedded vision model for real-time detection or you’re building the next state of the art multimodal foundation model, it almost goes without saying that data augmentation is an essential ingredient in the training process. yet far too often, we treat data augmentation as a black-box component and heuristically determine what transformations to apply. but if you’re optimizing your model architecture, and painstakingly pouring over your ground truth data to ensure the highest quality, there’s no reason not to take the same care with your data augmentation. i hope this post hammers home the importance of understanding what transformations you are applying, and gives you the tools you need to start treating data augmentation like data curation! top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse voxel51 follow more from voxel51 elderly action recognition: no one should age alone, ai’s promise for the next generation of elders # computervision # ai # machinelearning # datascience journey into visual ai: exploring fiftyone together — part iv model evaluation # computervision # machinelearning # ai # datascience how to tame your (data) dragon # computervision # ai # machinelearning # datascience 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/gkampitakis/memory-leaks-in-go-3pcn,,,"memory leaks in go - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse georgios kampitakis posted on mar 18, 2024 • edited on aug 14, 2024 memory leaks in go # go # learning # performance # observability in this post, we are going to have a look at what is a memory leak why are memory leaks bad common causes for memory leaks in go methods for identifying memory leaks investigate memory leaks what is a memory leak let's understand what memory leak is. reading from wikipedia a memory leak is a type of resource leak that occurs when a computer program incorrectly manages memory allocations in a way that memory which is no longer needed is not released. go has a garbage collector that does a very good job managing memory for us, tracking down memory that is no longer used and can be returned back to the system. still there are some cases where we can end up with either memory leaks or our system needing excessive memory to work. why are memory leaks bad before looking on cases that can make a go program ""waste"" memory let's first discuss why it can be bad having memory leaks and why we need to be mindful: system reliability and stability is affected. 
the system might behave unpredictably and crash from oom (out of memory) errors. inefficient resource utilization resulting in increased costs. performance degradation. as memory leaks accumulate the performance of the system may degrade, affecting responsiveness and efficiency. memory leaks also create additional pressure on the garbage collector resulting also in increased cpu usage. difficult to track down and debug, especially for bigger systems with complex interactions. common causes for memory leaks in go in this section we will see some cases that can cause memory leaks. some of those cases are specific to go and some others more general. unbounded resource creation creating resources without a limit can be seens as a type of memory leak. for example if we have a cache that only ever grows our service eventually will crash with oom (out of memory) error. the solution would be to restrict how many items can a cache hold (e.g. ttl). this applies to many resources, like goroutines (more about this later), http connections or open files. we should always be mindful to have limits on creating resources. something you should keep in mind regarding maps in go. they don't shrink after elements are deleted runtime: shrink map as elements are deleted #20135 long lived references keeping references to objects that your service no longer needs can result to memory leaks as the garbage collector sees the references and cannot free the memory. some cases where you can keep references unintentionally is with global variables, never ending goroutines, maps or not resetting pointers. there is a special case in go for holding references unintenionally with reslicing a slice (this also applies to strings). in our example we have a function readdetails that opens a big file and returns only a portion of it, so we slice the data []byte and return it. in go slices share the same underlying memory block ( go slices: usage and internals ). that means, that even if we are only interested in a very small subset of the data we are still keeping in memory, (referenced) the whole file. the correct way here, would be to call bytes.clone(data[5:10]) so that the data will no longer be referenced and subsequently collected by the garbage collector. you can also read more information for go slices at go slices: usage and internals robust generic functions on slices goroutines go runtime is doing a great job in spawning and managing goroutines, a lightweight thread, but as mentioned on the 'unbounded resource creation' section, (realistically) there is a limit of how many goroutines you can have at any time, bounded to the underlying system your service is running on. more over, iniside goroutines you can allocate or reference resources. so you need to make sure that your goroutines are properly terminated and memory is finally released. let's see the example below. we have a function that creates a new goroutine every second to execute a task, allocates a big data slice does some processing and then hangs forever. this code, has two problems creates an unbounder number of goroutines due to not termination of those goroutines resources allocated are never going to be released deferring function calls deferring a big number of functions can also cause a type memory of memory leak. the most common mistake is when you call defer inside a loop but the defer calls are pushed into a stack and only executed in lifo order at the end of the calling function. in the example below we are processing files in a for loop and calling .close on defer. the problem with below code is if we call processmanyfiles with a lot of files we are only going to close all the files after we are done processing. the correct way to handle this case would be to break opening and processing the file to a separate function so when called in the for loop each file we be closed before moving to the next one. not stopping time.ticker update(14-08-2024): this is no longer true after the release of go 1.23. read more on timer changes in the release notes. finally, another common case for memory leak is the time.ticker . as stated in the docs the resources are only released when stopping the ticker. correct way to handle a time.ticker methods for identifying memory leaks the most straightforward method to notice if your service is having a memory leak is to observe memory consumption over time. in distributed systems, observability is the ability to collect data about programs' execution, modules' internal states, and the communication among components. from wikipedia there are some memory patterns, that if you notice them on your service, you should get at least suspicious that something is wrong. in the first image, we are seeing a memory graph of our service per pod and the memory is only growing. there are some big spikes in memory, the memory goes down but never back to the previous value. this is a good indication that something is kept into memory and never released. so we should investigate what exactly is using this memory and why. on the second image, this pattern with the abrupt (cliff like) reduction in memory appears when our service reaches the system's memory limit and crashes with an out of memory error. you need to keep in mind that in both cases just by noticing this pattern can't be enough to conclude that your service is having memory leak. on the 1st image your service can still be on the start phase where it needs to allocate memory for initialisation or ""warming caches"" and on the second there is no doubt this is an oom error but it can be the case you have wrongly sized your service so there is not enough memory. in both cases you should look deeper to identify the causes. investigate memory leaks so if you are suspecting a memory leak where you should look at? where is the culprit? you could answer simply, take a look at your code, and that would be a valid answer. but in a complex system with external factors and interactions it's not that straightforward. profiling profiling can help, profiling tools analyze the complexity and costs of a go program such as its memory usage and frequently called functions to identify the expensive sections of a go program. profiling is useful for identifying expensive or frequently called sections of code. the go runtime provides profiling data in the format expected by the pprof visualization tool. the profiling data can be collected during testing via go test or endpoints made available from the net/http/pprof package. users need to collect the profiling data and use pprof tools to filter and visualize the top code paths. from go diagnostics the profile type we are interested in for investigating memory leaks is the heap . heap profile reports memory allocations samples used to monitor current and historical memory usage, and to check for memory leaks. go has two ways for capturing profiles, either via tests or enabling profiling over http. go test -cpuprofile cpu.prof -memprofile mem.prof -bench . enter fullscreen mode exit fullscreen mode and package main import ( _ ""net/http/pprof"" ) func main () { ... http . listenandserve ( "":6060"" , nil ) } enter fullscreen mode exit fullscreen mode we are not going to get into more details on how you can setup all this as it would be a whole new post you can read more at net/http/pprof . it's worth knowing that go tool pprof supports different types of visualisations for analysing the profiling data you have captured. listing of the most expensive calls as text. visualization of the most expensive calls as a graph. visualization of the most expensive calls as weblist. but there is a problem with this process, it's too manual and not always so straightforward. when do you capture the profiles? do you know under what conditions the memory leak happens is it on a dev environment or in production under specific load? continuous profiling with continuous profiling we can collect metrics and data from our services while serving real traffic. ""continuous profiling is the process of collecting application performance data in a production environment and making the data available to developers"" there are monitoring platforms like datadog where with very few steps you can onboard your service and provide tools to slice and dice on the submitted data to investigate memory leaks or other various performance issues. and personally one of the best features, comparing profile to a previous time period or even a previous version. the graphs are from datadog . the gopher art is from marialetta . top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse georgios kampitakis follow just a curious individual trying to understand how things work (or break 🙃) location london education bachelor's degree in computer science work software engineer at datadog joined aug 17, 2019 more from georgios kampitakis [boost] # node # performance # learning # javascript tracking down high memory usage in node.js # node # performance # learning # javascript http connection churn in go # go # learning # performance # http 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/berthaw82414312/guide-to-chrome-remote-debugging-3ok7,Selenium,Integration Testing; Unit Testing,"guide to chrome remote debugging - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse bertha white posted on jul 10, 2024 guide to chrome remote debugging # chromeremotedebugging # remotedebugging # webapptesting debugging is a daunting task when working in remote environments. but fear not! chrome remote debugging is here to save the day. this powerful feature lets you debug your web applications on a remote device using google chrome devtools. in this guide, we’ll delve into the nitty-gritty of chrome remote debugging, providing you with all the information you need to become a pro. plus, we'll explore how headspin can make your life easier by facilitating testing on a real device cloud . what is chrome remote debugging? chrome remote debugging is a powerful feature provided by google chrome that allows developers to debug web pages and web applications running on remote devices. this capability is particularly beneficial when users access the web from various devices with varying screen sizes, operating systems, and browser versions. by enabling remote debugging, developers can ensure that their applications perform optimally across different environments without possessing every device physically. here’s a closer look at the key components and advantages of chrome remote debugging: key components chrome devtools the cornerstone of chrome remote debugging is the chrome developer tools (devtools), an integrated web development and debugging tool suite. devtools includes features like the elements panel for inspecting html and css, the console for running javascript, the network panel for analyzing network activity, and the sources panel for setting breakpoints and debugging javascript code. remote target device the target device is the remote device you want to debug. this could be an android smartphone, tablet, or another computer. the target device runs the web page or application you wish to debug. communication channel remote debugging relies on a communication channel between your local development machine and the target device. this is typically established via usb for android devices but can also be done over a network connection. advantages cross-device debugging chrome remote debugging allows developers to test and debug their applications on various devices without interacting with each one physically. this is essential for ensuring a consistent user experience across different platforms. real-time inspection in real-time, developers can inspect and modify the dom, css, and javascript. this immediate feedback loop helps quickly identify and fix issues that may not be apparent in a traditional desktop development environment. performance analysis using devtools, developers can analyze network activity, identify performance bottlenecks, and monitor resource usage on the target device. this is crucial for optimizing the performance of web applications, especially on mobile devices with limited resources. enhanced debugging capabilities features like setting breakpoints, stepping through code, and profiling javascript execution provide deep insights into the behavior of web applications. these tools help developers understand and resolve complex issues more efficiently. setting up chrome remote debugging getting started with chrome remote debugging involves a few straightforward steps. here’s how you can set it up: enable remote debugging on your device: enable usb debugging by accessing developer options for android devices. connect the android device you have to a computer via usb. launch chrome with remote debugging: open your terminal or command prompt. run the following command: chrome.exe --remote-debugging-port=9222 connect to the remote device: open chrome on your desktop and navigate to chrome://inspect. click the ""configure..."" button and add the target device's ip address. you should now see the remote device listed. click ""inspect"" to start debugging. using chrome devtools for remote debugging once you’ve set up chrome remote debugging, you can use chrome devtools to inspect and debug your web application. chrome devtools helps developers understand and optimize their web applications. here’s a breakdown of the key features and how to use them effectively: elements panel using the elements panel, you can inspect and modify your web page's dom (document object model) and css in real-time. this is useful for making quick changes and seeing the results immediately. you can: inspect elements: hover over any element on your web page to highlight it in the elements panel. click on the element to view its html and css. edit html: double-click on any html tag to edit it. this helps with testing changes without altering the actual codebase. edit css: modify css properties directly in the styles pane. you can add, remove, or change properties to see how they affect the element's appearance. console panel the console panel is your go-to place for executing javascript, logging messages, and monitoring the performance of your application. here’s what you can do: execute javascript: run any javascript code directly in the console to test snippets or debug issues. log messages: use console.log() to print messages and variables to the console, helping you understand the flow of your application. monitor errors: view any javascript errors or warnings on your web page. clicking on an error will take you to the corresponding line of code. network panel the network panel is essential for analyzing network requests and responses. this helps you identify performance bottlenecks. key features include: view network requests: see a list of all network requests made by your web page, including xhr (xmlhttprequest) and fetch api calls. analyze performance: check the timing of each request, such as dns lookup, tcp handshake, and content download, to identify slow areas. inspect responses: click on any request to view its headers, payload, and response. this is useful for debugging api calls and ensuring correct data is returned. sources panel the sources panel is where you can debug your javascript code. it provides a comprehensive set of tools to help you find and fix issues: set breakpoints: click on the line number in your javascript code to set breakpoints. this pauses the execution of your code at that line, allowing you to inspect variables and the call stack. step through code: use the step over, step into, and step out buttons to navigate your code line by line. this helps you understand the flow and identify where issues occur. watch expressions: add variables or expressions to the watch pane to monitor their values as you pass your code. application panel the application panel provides insight into your web app’s storage, including cookies, local storage, session storage, and indexeddb. here’s what you can do: manage cookies: view, edit, and delete cookies used by your web application. inspect storage: look at local storage, session storage, and indexeddb data. this is useful for understanding how your app saves state and user data. best practices for chrome remote debugging to make the most out of chrome remote debugging, consider these best practices: use stable connections: ensure your network connection is stable to avoid disruptions during debugging sessions. a dropped connection can interrupt your workflow and cause a loss of valuable debugging progress. keep chrome updated: regularly update chrome to benefit from new features and bug fixes. chrome updates often include improvements to devtools and the remote debugging protocol, which can enhance your debugging experience and fix known issues. secure your remote debugging setup: use secure methods to connect to remote devices, especially when debugging over the internet. this can include using vpns, ssh tunnels, or other secure networking techniques to protect your data and prevent unauthorized access. document your debugging process: keep detailed notes of your debugging steps and findings to facilitate collaboration and future reference. a shared document or a dedicated debugging log can help track issues, solutions, and insights, making it easier for team members to understand and build on each other's work. automate repetitive tasks: use automation tools to streamline repetitive debugging tasks. tools like puppeteer or selenium can automate interactions with your web application, allowing you to focus on diagnosing and fixing issues. common issues and troubleshooting even with the best setup, you might encounter some issues. here are common problems and how to troubleshoot them: device not detected: check usb debugging: ensure usb debugging is enabled in your device's developer options. verify connection: make sure the device is properly connected via usb. restart devices: a simple restart of your device and computer can resolve connectivity issues. update drivers: ensure that your computer has the latest usb drivers installed for your device. this is particularly important for windows users. connection timeouts: stable network: check that your network connection is stable. unstable or slow networks can cause timeouts when connecting to remote devices. firewall settings: ensure your firewall does not block the remote debugging port (usually 9222). ip address configuration: double-check that you've entered the correct ip address of the remote device in chrome's ""configure..."" settings. devtools not loading: clear cache and cookies: sometimes, issues with loading devtools can be resolved by clearing your browser's cache and cookies. go to chrome settings > privacy and security > clear browsing data. incognito mode: try accessing devtools in incognito mode to rule out any extensions or cached data causing the issue. disable extensions: certain chrome extensions can interfere with devtools. try disabling them to see if that resolves the issue. laggy performance: close unnecessary tabs: running too many tabs can slow down your browser. close unnecessary tabs to free up resources. check device resources: ensure that other processes do not overwhelm your debugging remote device. you can use the device's task manager or settings to check resource usage. network speed: verify that your local and remote networks have sufficient bandwidth. slow network speeds can cause lag in the devtools interface. how headspin enhances browser testing headspin offers a comprehensive platform that significantly enhances browser testing by allowing you to test on a real device cloud. here's how headspin can help: access to real devices: test your web apps on various real devices and browsers, ensuring compatibility and performance across different environments. automated testing: utilize headspin’s automated testing features to run extensive test suites without manual intervention. detailed analytics: gain insights into performance metrics and identify issues quickly with headspin's detailed analytics and reporting tools. global reach: test your applications on devices worldwide to ensure they perform well in different locations. wrapping up chrome remote debugging is an invaluable tool for modern web development, enabling developers to debug and test web applications across various devices and environments. following this guide, you should understand how to set up and use chrome remote debugging effectively. additionally, leveraging platforms like headspin can further enhance your testing process by providing access to a real device cloud and comprehensive testing tools. original source: https://www.headspin.io/blog/ultimate-guide-chrome-remote-debugging top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse bertha white follow automation consultant | selenium | devops | agile | automation | digital transformation | big data | unit testing | integration testing work senior automation consultant joined jun 13, 2023 more from bertha white testing your web apps on the right devices: a comprehensive guide # webapptesting # apptesting # webtesting guide to automated testing tools for web apps # webapptesting # automatedtestingtools # automatedtesting 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/doriansabitov/how-to-connect-2-salesforce-orgs-best-practices-and-use-cases-5a6g,,,"how to connect 2 salesforce orgs: best practices and use cases - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse dorian sabitov posted on oct 9, 2024 • originally published at sfapps.info on oct 9, 2024 how to connect 2 salesforce orgs: best practices and use cases # blog # howto are your company’s business processes way too complicated for just a single salesforce organization? or do you want to split business processes, directions, or units in different salesforce organizations? maybe your company is going through a merger and acquisition process when you need to work with an additional salesforce organization, and you want to keep it separate? when a company faces these kinds of challenges, it’s time to consider simultaneous usage of multiple salesforce organizations, or orgs for short. best practices and use cases for integration between two salesforce orgs option #1: salesforce connect option #2: salesforce to salesforce option #3: rest and soap apis option #4: middleware solutions option #5: change sets obstacles to beware during salesforce orgs connection faqs about connecting 2 salesforce orgs how do you choose the right tool for integrating two salesforce orgs? what are the benefits of connecting two salesforce orgs? how can data security be maintained during the integration of two salesforce orgs? what are the best practices for a successful salesforce org integration? in conclusion: best practices and use cases for integration between two salesforce orgs for convenience, let’s imagine our 2 salesforce orgs as below: managing two salesforce organizations with the right tools can facilitate this process. among the many available salesforce tools for admins and developers, let’s take a look at the best practices in this field: option #1: salesforce connect this native salesforce tool allows you to connect two salesforce orgs and configure access to the second org data from the first org. access takes place in almost real-time, automatically, without storing data inside first org. setting up and administering the connection takes place inside the first org. setting up salesforce connect, step 1: adding new external data source setting up salesforce connect, step 2: choose external data source type * use cases: * when a single company owns and manages both salesforce organizations, and data access can be configured without additional approvals from the other party. for example, a multinational corporation can unify customer data across regions, a healthcare network can share patient records, and a retail chain can synchronize inventory data, enhancing efficiency, data accuracy, and collaboration across business units. insight: when integrating two salesforce orgs, it’s essential to consider the following issues and aspects: data synchronization : determine how frequently and which data needs to be synchronized between the salesforce orgs. security : implement robust security measures to protect sensitive data during the integration process. performance : optimize the integration to ensure it doesn’t impact the performance of either org. error handling : develop mechanisms to handle potential errors and failures gracefully inside your salesforce org. option #2: salesforce to salesforce like salesforce connect, the native salesforce to salesforce tool allows you to connect 2 salesforce orgs and configure access to second org data from the first org. but this method uses a different approach. connection settings take place in both orgs. importantly, the settings of the second org determine exactly what information will be available in the first org. according to the settings, data is shared from the second org and copied to the first org. changes of shared data are synchronized automatically. additionally, settings are made in the salesforce classic setup and have specific features. this method, like the one mentioned above, can be implemented by a salesforce administrator without the involvement of a developer, which is more economically feasible for the company because the salesforce admin salary is comparatively lower. setting up salesforce to salesforce tool * use cases: * two partner companies want to share records between their salesforce organizations for the purpose of collaboration, but each company retains full control over the amount of data that is shared. option #3: rest and soap apis rest and soap apis can be used for building custom integrations between two salesforce orgs. they allow for data exchange and process automation, making them suitable for complex or specific data and process requirements. unlike the previous methods, this method is more difficult to implement and requires writing apex code, but, accordingly, it has wider possibilities. for example, creating an integration through rest api generally requires the following steps: use cases: complex, custom integrations, integrations with specific business requirements, or integrations performing specific business processes. option #4: middleware solutions there is a wide spectrum of off-the-shelf middleware solutions on the market, which can be used to integrate two salesforce organizations. for example: mulesoft – a salesforce company, provides a unified platform for connecting applications, data, and devices with api-led connectivity; jitterbit – offers an ai-infused, low-code platform for integration, orchestration, and automation; informatica – delivers comprehensive data management solutions, including data integration, quality, and governance; talend – provides a scalable data management platform that combines data integration, quality, and governance in a single, low-code solution; boomi – offers a cloud-based integration platform that connects applications, data, and processes; gearset – specializes in salesforce deployment and continuous integration, simplifying the release management process; heroku connect – part of the salesforce ecosystem, synchronizes data between salesforce and heroku postgres, enabling real-time data integration and consistency; and others. these tools are designed to implement complex enterprise-level integration projects with a high need for scalability and flexibility, especially when such integrations involve other external systems or databases outside of salesforce. technical implementation of such integrations is individual and depends on the chosen solution. use cases : first of all, it is advisable to use such solutions when large and complex integrations are necessary, for which the methods listed above are not sufficient. for example, when integration takes place with the participation of third-party services or databases, or the need to change data in real-time, consolidate data from different services for the purpose of analytics. option #5: change sets change sets, a native tool for integrating two salesforce organizations, deserves special consideration. unlike the previous tools, aimed at data exchange, change sets is a tool for exchanging metadata, i.e., settings and customizations of the salesforce organization. it is a simple tool that allows you to conveniently transfer changes made in the first org (sandbox organization) to the second org (product organization). it is a basic development tool that includes strict connection settings for the purpose of transferring changes, and a procedure for acceptance and testing of received changes before they are applied. it should be noted that not all changes can be included in a change set: components available in change sets . illustration of the change set exchange process setup from second org (production organization), where we should allow inbound change sets from sandboxes based on this production org: setting up change sets, step 1: find sandbox in deployment settings setting up change sets, step 2: allowing inbound changes setup from first org (sandbox organization), where we can now create our first change set: setting up change sets, step 3: create a change set use cases: this tool is designed for uncomplicated deployment tasks, and its convenience and ease of use facilitate simple tasks of configuration and customization of organizations. this allows you to create and test the necessary changes in a separate environment of salesforce, connect orgs for change set, and then conveniently deploy them to the working organization. obstacles to beware during salesforce orgs connection when connecting two salesforce organizations, several obstacles can arise, which may include, but not be limited to the following: data synchronization issues, such as inconsistent data formats and duplicate records, can complicate integration. security concerns, including data privacy and access control, must be meticulously managed to prevent unauthorized access. differing customization and configuration settings between orgs can lead to conflicts and errors. performance impacts, especially with large data volumes, can slow down operations. ensuring compliance with regulatory requirements across different regions can add complexity. addressing these challenges proactively is crucial for a smooth and secure two salesforce orgs connection. these issues could be prevented by involvement of relevant specialists through our salesforce admin services. looking for professional help with integration of 2 salesforce orgs? request our admin services! explore more faqs about connecting 2 salesforce orgs how do you choose the right tool for integrating two salesforce orgs? choosing the right tool depends on factors such as the complexity of your integration needs, budget, existing technology stack, and specific features required, such as real-time data synchronization or ease of use. what are the benefits of connecting two salesforce orgs? connecting two salesforce orgs can enhance data consistency, streamline business processes, improve collaboration across departments, and provide a unified view of customer data. how can data security be maintained during the integration of two salesforce orgs? data security can be maintained by implementing robust access controls, encryption, regular security audits, and adhering to salesforce’s best practices for data protection. what are the best practices for a successful salesforce org integration? best practices include thorough planning, ensuring data quality, using reliable integration tools, conducting regular testing, and maintaining clear communication between teams involved in the integration process. in conclusion: in this article, we have explored some of the best and most popular existing practices for how to integrate two salesforce orgs. beyond the methods discussed, there are numerous other tools that can serve different types and processes of integration and data exchange between two salesforce organizations. from full real-time integration to periodically sync data between two salesforce orgs. the selection, setting, and customization of the appropriate integration method should first and foremost start with a comprehensive assessment of the specific needs, features, and purpose of such integration, that can be facilitated by involving relevant specialists within salesforce admin services . it should also not be forgotten that any integration requires careful planning around data management policies and a clear understanding of product licensing implications. the post how to connect 2 salesforce orgs: best practices and use cases first appeared on salesforce apps . top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse dorian sabitov follow i am a skilled salesforce administrator and developer 6x certified. i'm quite interested in salesforce and all of its uses in the modern corporate world. location ukraine joined oct 8, 2023 more from dorian sabitov who are salesforce revops consultants # blog # talentmarket top salesforce apps for government # appreviews # blog # salesforce hire salesforce apex developer: pitfalls to avoid # blog # talentmarket 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/iaadidev/step-by-step-instructions-for-forward-proxy-setup-c22,,,"step-by-step instructions for forward proxy setup - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse aaditya kediyal posted on jun 19, 2024 step-by-step instructions for forward proxy setup # forwardproxy # webdev # beginners # serverless setting up a forward proxy can be a powerful tool for managing network traffic, enhancing privacy, and improving security. whether you are an it professional, a developer, or just someone interested in network technologies, understanding how to set up and configure a forward proxy is a valuable skill. this guide will walk you through the process of setting up a forward proxy, covering the basics, the benefits, and providing code snippets to help you get started. table of contents introduction what is a forward proxy? benefits of using a forward proxy getting started prerequisites choosing the right proxy software setting up a forward proxy with squid installation basic configuration advanced configuration testing your proxy setting up a forward proxy with nginx installation basic configuration advanced configuration testing your proxy enhancing your proxy setup security measures performance tuning common use cases caching web content access control and monitoring anonymity and privacy troubleshooting and maintenance common issues regular maintenance tasks conclusion 1. introduction what is a forward proxy? a forward proxy is an intermediary server that forwards client requests to other servers. it acts as a gateway between the client and the internet, making requests on behalf of the client and returning the responses to the client. this setup allows the proxy to manage and control access to resources, provide anonymity, and optimize performance. benefits of using a forward proxy privacy and anonymity : by masking the client's ip address, a forward proxy can enhance privacy. access control : proxies can be used to control access to certain websites or services. caching : they can cache frequently requested content to improve load times and reduce bandwidth usage. security : proxies can filter traffic and block malicious content. 2. getting started prerequisites before setting up a forward proxy, ensure you have the following: a server or virtual machine with a linux-based operating system (ubuntu, centos, etc.). root or sudo access to the server. basic understanding of networking and command-line operations. choosing the right proxy software there are several proxy software options available. two of the most popular are squid and nginx. squid is highly configurable and widely used, especially for caching purposes, while nginx is known for its high performance and is often used as a web server or reverse proxy. 3. setting up a forward proxy with squid installation to install squid on ubuntu, follow these steps: sudo apt update sudo apt install squid -y enter fullscreen mode exit fullscreen mode for centos: sudo yum install squid -y enter fullscreen mode exit fullscreen mode basic configuration after installation, the main configuration file is located at /etc/squid/squid.conf . open this file in your preferred text editor. sudo nano /etc/squid/squid.conf enter fullscreen mode exit fullscreen mode to set up a basic forward proxy, add the following lines: http_port 3128 acl localnet src 192 . 168 . 1 . 0 / 24 # replace with your network range http_access allow localnet http_access deny all enter fullscreen mode exit fullscreen mode advanced configuration to enhance the functionality and security of your squid proxy, consider the following configurations: caching : configure caching to improve performance. cache_dir ufs / var / spool / squid 100 16 256 maximum_object_size 4096 kb enter fullscreen mode exit fullscreen mode access control : define acls to control access. acl allowed_sites dstdomain . example . com http_access allow allowed_sites enter fullscreen mode exit fullscreen mode logging : enable and configure logging for monitoring. access_log / var / log / squid / access . log cache_log / var / log / squid / cache . log enter fullscreen mode exit fullscreen mode testing your proxy after configuring squid, restart the service: sudo systemctl restart squid enter fullscreen mode exit fullscreen mode to test your proxy, configure your web browser or client to use the proxy server's ip address and port (3128). 4. setting up a forward proxy with nginx installation to install nginx on ubuntu, use the following commands: sudo apt update sudo apt install nginx -y enter fullscreen mode exit fullscreen mode for centos: sudo yum install nginx -y enter fullscreen mode exit fullscreen mode basic configuration open the nginx configuration file: sudo nano /etc/nginx/nginx.conf enter fullscreen mode exit fullscreen mode add the following configuration to set up a basic forward proxy: http { server { listen 8080 ; location / { proxy_pass http ://$ http_host $ request_uri ; proxy_set_header host $ host ; proxy_set_header x - real - ip $ remote_addr ; proxy_set_header x - forwarded - for $ proxy_add_x_forwarded_for ;
        }
    }
} enter fullscreen mode exit fullscreen mode advanced configuration to enhance nginx's proxy capabilities, consider these advanced configurations: ssl/tls : secure the proxy with ssl/tls. server { listen 443 ssl ; ssl_certificate / path / to / cert . pem ; ssl_certificate_key / path / to / key . pem ; location / { proxy_pass http ://$ http_host $ request_uri ; proxy_set_header host $ host ; proxy_set_header x - real - ip $ remote_addr ; proxy_set_header x - forwarded - for $ proxy_add_x_forwarded_for ;
       }
   } enter fullscreen mode exit fullscreen mode load balancing : distribute requests across multiple servers. upstream backend { server backend1 . example . com ; server backend2 . example . com ;
   } server { listen 8080 ; location / { proxy_pass http :// backend ;
       }
   } enter fullscreen mode exit fullscreen mode testing your proxy after configuring nginx, restart the service: sudo systemctl restart nginx enter fullscreen mode exit fullscreen mode configure your web browser or client to use the proxy server's ip address and port (8080) to test the setup. 5. enhancing your proxy setup security measures authentication : require users to authenticate before using the proxy. for squid, add: auth_param basic program / usr / lib / squid / basic_ncsa_auth / etc / squid / passwd auth_param basic children 5 auth_param basic realm squid proxy - caching web server auth_param basic credentialsttl 2 hours acl authenticated proxy_auth required http_access allow authenticated enter fullscreen mode exit fullscreen mode for nginx, use: location / { auth_basic ""restricted"" ; auth_basic_user_file / etc / nginx /. htpasswd ; proxy_pass http ://$ http_host $ request_uri ;
   } enter fullscreen mode exit fullscreen mode ip whitelisting : only allow specific ips to use the proxy. for squid: acl allowed_ips src 192 . 168 . 1 . 100 / 32 http_access allow allowed_ips enter fullscreen mode exit fullscreen mode for nginx: location / { allow 192 . 168 . 1 . 100 ; deny all ; proxy_pass http ://$ http_host $ request_uri ;
   } enter fullscreen mode exit fullscreen mode performance tuning squid : increase cache size and memory usage. cache_mem 256 mb maximum_object_size_in_memory 512 kb enter fullscreen mode exit fullscreen mode nginx : optimize worker processes and connections. worker_processes auto ; worker_connections 1024 ; enter fullscreen mode exit fullscreen mode 6. common use cases caching web content caching helps reduce bandwidth usage and improves response times for frequently accessed resources. squid is particularly effective for this purpose. access control and monitoring proxies can restrict access to certain websites or services, making them useful in corporate environments to enforce internet usage policies. anonymity and privacy by masking the client's ip address, a forward proxy can help users maintain anonymity online and protect their privacy. 7. troubleshooting and maintenance common issues connection refused : ensure the proxy server is running and the correct ports are open. authentication problems : verify the authentication configurations and user credentials. slow performance : check for network issues, optimize configurations, and ensure adequate server resources. regular maintenance tasks log monitoring : regularly check log files for unusual activity. software updates : keep your proxy software up to date to ensure security and performance. configuration backups : maintain backups of your configuration files to quickly restore in case of issues. 8. conclusion setting up a forward proxy can significantly enhance your network's functionality, security, and performance. whether you choose squid or nginx, the steps outlined in this guide provide a comprehensive approach to configuring and managing a forward proxy. by understanding and implementing these configurations, you can effectively control network traffic, improve user privacy, and optimize resource usage. remember, the key to a successful proxy setup is continuous monitoring and maintenance. regularly update your configurations, monitor logs, and stay informed about best practices and security updates. with these practices, your forward proxy will serve as a robust tool for managing and securing your network. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand kelly r denehy kelly r denehy kelly r denehy follow joined may 27, 2021 • oct 31 '24 dropdown menu copy link hide for the nginx ssl config, your sample explicitly shows http: rather than https: in the proxy_pass line: proxy_pass http://$http_host$request_uri; enter fullscreen mode exit fullscreen mode is this correct, or should the protocol be https? like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse aaditya kediyal follow 🚀 10x developer | full stack dev| devops specialist
🔗 exploring web 3.0 & blockchain 🌟 open source  | 💡 competitive programmer | location delhi , india joined jun 1, 2024 more from aaditya kediyal essential devops principles for beginners # devops # practice # beginners # guide introduction to the periodic table of devops tools # devops # tutorial # beginners # tools deploying an application using apache as a web server # apache # deployment # webdev # beginners 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/ripplexdev/evm-sidechain-devnet-is-now-available-for-testing-and-development-3ec8,,,"evm sidechain devnet is now available for testing and development - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse mayukha vadari for ripplex developers posted on jun 26, 2023 evm sidechain devnet is now available for testing and development # xrpl # opensource # blockchain # web3 as of june 26, the evm sidechain for the xrp ledger is available on a new version of devnet (v2). this new version features a decentralized bridge design using the xls-38d specification and supports xrp, iou, and erc-20 token transfers in both directions between the xrp ledger and evm sidechain. this evm sidechain uses a proof of authority (poa) consensus mechanism. developers are encouraged to test the new functionality of the devnet v2, as well as the functionality of the bridge using all supported token types. connect to the new version of the evm sidechain here: name: evm sidechain rpc url: https://rpc-evm-sidechain.xrpl.org network identifier: 1440002 digital asset: xrp evm block explorer url: https://evm-sidechain.xrpl.org bridge url: https://bridge.devnet.xrpl.org for support, updated documentation is available at opensource.ripple.com . developers are also welcome to engage in the xrpl developer discord , which has a dedicated evm-sidechain channel. full list of updates: evm sidechain: improved block time from 5.4 to 3.5 s/block improved chain performance upgrading tendermint to comet bft consensus algorithm implemented poa validator election mechanism explorer: improved sidechain block explorer, now including smart contract verification developed explorer on the cosmos layer (in progress) bridge: included faucet support to facilitate onboarding (for xrp and tokens) included support to bridge erc-20 and iou tokens implemented a witness bridge to now use xchain transactions to natively secure bridge transfers between xrpl and the sidechain with xls-38d witness bridge can now be distributed and operated by multiple parties - up to 32 signers on xrpl witness keys can be stored on aws kms to safely secure the keys dapps: launched gnosis safe dapp used to secure the bridged assets on the evm sidechain part open dapp interface to everyone for safely creating a multisig account in the evm sidechain the evm sidechain is being developed by peersyst in partnership with ripple. developers can iterate on their projects and applications faster and use popular smart contract languages like solidity, while having access to xrp liquidity and a secure bridge to the xrp ledger. the original version of the evm sidechain announced last october here will be available at the following urls for the next 1 month after which it will be retired. developers are encouraged to use the new evm sidechain instead to deploy their apps. evm sidechain v1 (which will be retired in a month) rpc url: https://archived-rpc-evm-sidechain.xrpl.org evm block explorer url: https://archived-evm-sidechain.xrpl.org bridge url: https://archived-bridge.devnet.xrpl.org a security audit of the evm sidechain is being conducted in july, the results of which will be published for the community to review. peersyst and ripple will take action to address any issues, and that information will also be shared with the community as well. the evm sidechain is expected to go live on mainnet following the approval of the xls-38d bridge amendment. the bridge is dependent on the proposal of the amendment, followed by at least 80% of the xrpl validator community accepting the proposal and holding their vote for at least two weeks. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse ripplex developers follow making it easy to build on the xrp ledger. join the xrpl developer community on discord xrpl devs discord more from ripplex developers permission delegation: unlocking a new era of xrpl account management # xrpl # web3 # blockchain # ripple permission delegation security audit findings # xrpl # blockchain # ripple # web3 token escrow security audit findings # xrpl # rwa # tokenization # escrow 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/documatic/data-encryption-securing-data-at-rest-and-in-transit-with-encryption-technologies-1lc2,,,"data encryption: securing data at rest and in transit with encryption technologies - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse jatin sharma for documatic posted on aug 16, 2023 data encryption: securing data at rest and in transit with encryption technologies # webdev # beginners # programming # computerscience in the current digital age, it is extremely important to ensure the security of sensitive information as it is constantly transmitted and stored. one of the most effective tools in achieving this goal is encryption. but what exactly does it mean when data is encrypted? in this article, we will dive into the world of encryption, exploring its meaning, how it works, and the importance of implementing encrypted technologies to safeguard your data. table of content introduction understanding data encryption importance of data security in the digital age data encryption basics symmetric encryption asymmetric encryption role of cryptographic keys in encryption securing data at rest with encryption importance of encrypting data at rest full disk encryption file-level encryption database encryption best practices for data at rest encryption encryption algorithms key management access control and authentication regular security assessments employee training and awareness securing data in transit with encryption importance of encrypting data in transit ssl/tls protocols for secure communication vpn (virtual private network) encryption email encryption best practices for data in transit encryption use strong encryption protocols implement virtual private networks (vpns): enable two-factor authentication (2fa) regularly update software and systems be cautious with public wi-fi common encryption technologies and tools advanced encryption standard (aes) rsa encryption elliptic curve cryptography data encryption in various environments encryption in cloud computing encryption for on-premises systems mobile device encryption encryption in enterprise networks key lifecycle management key generation key usage key storage key rotation key deletion or key disposal introduction data encryption is a method of securing sensitive information by converting it into an unreadable format, known as ciphertext, through the use of encryption algorithms. in today's digital era, it is crucial to prioritize the security of our personal and confidential data due to the increasing number of data breaches and cyberattacks. understanding data encryption data encryption involves the process of converting plaintext (readable and understandable data) into an unreadable format, known as ciphertext, using encryption algorithms. this encryption process makes it difficult for individuals to access and understand the encrypted data without the decryption key. encryption algorithms operate based on mathematical formulas that dictate how the encryption and decryption processes function. these algorithms use cryptographic keys to convert plaintext into ciphertext and vice versa. encryption strength depends on algorithm complexity and the length and randomness of cryptographic keys used. the primary goal of data encryption is to ensure the confidentiality and privacy of sensitive information. by encrypting data, even if it is accessed by unauthorized parties, it remains unreadable and useless without the decryption key. importance of data security in the digital age as in the current modern world, data is the new treasure for big companies like google, microsoft, and others as well. if you are using some popular and trusted service, then you might be sure that your data is protected. you might be right there. however, even big companies' data is getting breached, or sometimes they sell it to marketing companies who pay these companies, and that’s all they have to do. and after that your data just got shared to some third party company. data encryption basics data encryption is the process of converting data into a format that can only be accessed by authorized entity. it ensures the confidentiality and integrity of sensitive information. encryption algorithms play a crucial role in this process. there are two main types of encryption algorithms: symmetric encryption symmetric encryption uses a single key for both encryption and decryption. the key is shared between the sender and the recipient. when encrypting data, the sender uses the key to transform the plaintext into ciphertext. the recipient then uses the same key to decrypt the ciphertext and retrieve the original plaintext. this method is efficient and fast, making it suitable for securing large amounts of data. however, the challenge lies in securely sharing and managing the secret key among the parties involved, as anyone with the key can decrypt the information. for example: const cryptojs = require ( "" crypto-js "" ); // use a key and iv of 16 bytes (128 bits) const key = cryptojs . enc . utf8 . parse ( "" sixteen byte key "" ); const iv = cryptojs . enc . utf8 . parse ( "" sixteen byte iv "" ); // use aes encryption in cbc mode const encrypted = cryptojs . aes . encrypt ( "" a message i want to encrypt "" , key , { mode : cryptojs . mode . cbc , iv : iv }); const decrypted = cryptojs . aes . decrypt ( encrypted , key , { mode : cryptojs . mode . cbc , iv : iv }); console . log ( decrypted . tostring ( cryptojs . enc . utf8 )); // a message i want to encrypt enter fullscreen mode exit fullscreen mode asymmetric encryption asymmetric encryption, also known as public-key encryption, uses a pair of keys: a public key and a private key. the public key is used to encrypt data, while the private key is used to decrypt it.  the public key is freely distributed and used for encryption, while the private key is kept secret and used for decryption. this approach addresses the key distribution problem of symmetric encryption but can be slower due to the complexity of the mathematical operations involved. fox example: const crypto = require ( ' crypto ' ); // generate key pair const keypair = crypto . generatekeypairsync ( ' rsa ' , { moduluslength : 2048 , // size of key in bits publickeyencoding : { type : ' spki ' , format : ' pem ' }, privatekeyencoding : { type : ' pkcs8 ' , format : ' pem ' } }); // get public and private keys const publickey = keypair . publickey ; const privatekey = keypair . privatekey ; // encrypt with public key const cipher = crypto . publicencrypt ( publickey , buffer . from ( ' message to encrypt ' ) ); // decrypt with private key const decrypted = crypto . privatedecrypt ({ key : privatekey , padding : crypto . constants . rsa_pkcs1_padding }, cipher ); console . log ( decrypted . tostring ()); // decrypted message enter fullscreen mode exit fullscreen mode role of cryptographic keys in encryption cryptographic keys play an important role in encryption. they are used to encrypt and decrypt data. there are two main types of cryptographic keys - public keys and private keys as mentioned previously. data encrypted with one key can only be decrypted with the other key. longer key lengths provide stronger encryption. generally, 2048-bit or higher keys are recommended for better security. keys must be generated, stored and managed securely to prevent compromise. these keys are used with encryption algorithms like rsa or aes. the same algorithm is used for both encryption and decryption, but different keys are used. securing data at rest with encryption data at rest refers to information that is stored and saved on a physical storage drive, such as hard drives, solid-state drives, and other storage devices. this data not actively being used or transmitted. even though the data might not be in motion, it's still vulnerable to unauthorized access, especially if the storage device is lost, stolen, or compromised. examples of data at rest include files stored on a computer's hard drive, data stored on a usb drive, or information saved in a database. importance of encrypting data at rest encrypting data at rest is essential for protecting sensitive information from unauthorized access. without encryption, if a malicious user gains physical access to the storage device, they can easily read and steal sensitive data encryption transforms the data into an unreadable format that can only be deciphered with the appropriate decryption key. this adds an extra layer of protection and ensures that even if the storage device is compromised, the data remains secure. full disk encryption full disk encryption (fde) is a method of encrypting an entire storage device, all data on the device, including the operating system and user files, is encrypted. in this encryption, data can only be accessed with the correct encryption key. fde provides a high level of security for data at rest, as it protects against unauthorized access even if the storage device is stolen or lost. file-level encryption file-level encryption involves encrypting individual files or folders rather than the entire storage device. each file is encrypted separately, and decryption occurs when the authorized user accesses the file. this approach provides more granular control over which files are encrypted, but it requires managing encryption keys for each file. database encryption database encryption focuses on securing data stored within databases. this can include encrypting the entire database, specific tables, or even individual columns containing sensitive information. database encryption ensures that even if an attacker gains access to the database files, the data remains encrypted and unreadable without the appropriate keys. best practices for data at rest encryption following are the 5 most important best practices for data at rest encryption: encryption algorithms use strong encryption algorithms like aes (advanced encryption standard) with appropriate key lengths (128-bit, 256-bit). strong encryption ensures that even if unauthorized individuals gain access to the encrypted data, deciphering it remains extremely challenging. key management implement robust key management practices. store encryption keys separately from the encrypted data, preferably in hardware security modules (hsms) or trusted key management systems. proper key management prevents unauthorized access to sensitive information. access control and authentication enforce strong access controls and authentication mechanisms. only authorized users with proper authentication credentials should be able to access the encrypted data. multi-factor authentication adds an extra layer of security. regular security assessments conduct routine security assessments and audits to identify vulnerabilities and weaknesses in your encryption implementation. regular testing helps you stay ahead of potential threats and ensures that your encryption remains effective. employee training and awareness educate your employees about data security and encryption best practices. employees should understand their role in maintaining the security of encrypted data, including how to handle encryption keys, use secure authentication, and follow proper data handling procedures. securing data in transit with encryption data in transit refers to any information that is being transmitted over a network. imagine you're sending a message, sharing a photo, or conducting a financial transaction online – all of these actions involve data in transit. the data is moving between your device and a server, and during this journey, it could potentially be intercepted by unauthorized parties. importance of encrypting data in transit encrypting data in transit is crucial for maintaining the confidentiality and integrity of sensitive information. it’s like putting your information into a secure envelope before sending it. without encryption, your data could be captured and read by hackers or cybercriminals who might misuse it.  encryption transforms your data into a code that only the authorized recipient can convert to the original format, making it extremely difficult for anyone else to understand. it will appear as a jumble of unreadable characters to anyone without the decryption key. this adds an extra layer of security to your data. ssl/tls protocols for secure communication ssl and tls are security protocols. they provide secure and encrypted communication between websites and web browsers. this ensures that the data sent between them remains private and nobody can access it. many websites use ssl/tls to protect data that is sensitive. they keep your information safe while it is being transferred. when you see https:// at the start of a website address, it means their connection uses ssl or tls. this helps protect your passwords and all your information while they are being transferred to the website. ssl/tls protocols are commonly used by websites that deal with financial information like online stores and banking websites. they encrypt the data that you send to these websites, like credit card details and login credentials. this makes online transactions and communications more secure. vpn (virtual private network) encryption a virtual private network (vpn) encrypts your internet traffic to provide privacy and security when you use public networks. when you connect to a vpn, all of your network traffic is encrypted and tunneled through the vpn's secure server. this prevents anyone from snooping on or interfering with your data in transit. vpns use various encryption standards like aes-256, openvpn, and ipsec to encrypt your data in transit. this turns your data into unreadable ciphertext that can only be decrypted by the vpn server and your device. when you connect to a vpn server, you are assigned an ip address from that vpn provider. this hides your actual ip address and changes your apparent location. vpn encryption does provide a high level of security and privacy. however, it depends on the vpn provider and the encryption standards they use. some providers may have weaknesses that compromise the security. email encryption email encryption uses cryptographic techniques to encode email messages so that only the intended recipient can read them. when an encrypted email is sent, it is converted into unreadable ciphertext using the recipient's public key. only the recipient's private key can decrypt the message and convert it back into readable plaintext. there are two main types of email encryption: end-to-end encryption and transport layer encryption. end-to-end encryption ensures that your message is encrypted on your device and can only be decrypted by the recipient's device. this means that even email service providers cannot access the content. transport layer encryption, on the other hand, secures the email's path while it's in transit between email servers. it prevents unauthorized access to the email's content during its journey. best practices for data in transit encryption following are the 5 most important best practices for data in transit encryption: use strong encryption protocols employ reputable encryption protocols like ssl (secure sockets layer) or its successor tls (transport layer security) for securing data while it's in transit. these protocols establish a secure and encrypted connection between your device and the server, ensuring that data remains confidential and protected from interception. implement virtual private networks (vpns): utilize virtual private networks to create an encrypted ""tunnel"" for your data to travel through. vpns add an extra layer of security, especially when using public wi-fi networks, by encrypting your data's path and preventing potential eavesdropping. enable two-factor authentication (2fa) whenever possible, enable two-factor authentication for your accounts. 2fa adds an extra verification step, usually a code sent to your phone, which enhances security even if someone gains access to your password. regularly update software and systems keep your operating systems, web browsers, and security software up to date. software updates often include patches for security vulnerabilities, minimizing the risk of exploitation by attackers. be cautious with public wi-fi exercise caution when using public wi-fi networks, as they can be vulnerable to data interception. if you must use public wi-fi, connect through a trusted vpn to encrypt your data and make it more secure. common encryption technologies and tools there are various encryption technologies and tools that are commonly used to secure data, communications, and networks. these encryption methods play a crucial role in ensuring the confidentiality and integrity of sensitive information. advanced encryption standard (aes) the advanced encryption standard (aes) is a widely adopted method for keeping data secure by converting it into a scrambled form that can only be understood with the correct decryption key. think of it as a secret code that locks and unlocks information. aes can be likened to a digital lock that uses a specific key to secure and unscramble data. let's say you want to send a private message to your friend. you'd use aes to encrypt your message with a secret key that only you and your friend know. here's a python example using the pycryptodome library: from crypto.cipher import aes from crypto.random import get_random_bytes from crypto.util.padding import pad , unpad # encrypt key = get_random_bytes ( 16 ) cipher = aes . new ( key , aes . mode_cbc ) plaintext = b ' this is a secret message ' # pad the plaintext to be a multiple of the block size padded_plaintext = pad ( plaintext , aes . block_size ) ciphertext = cipher . encrypt ( padded_plaintext ) # decrypt decipher = aes . new ( key , aes . mode_cbc , iv = cipher . iv ) decrypted_padded_text = decipher . decrypt ( ciphertext ) # unpad the decrypted plaintext decrypted_text = unpad ( decrypted_padded_text , aes . block_size ) print ( "" original message: "" , plaintext ) print ( "" encrypted message: "" , ciphertext ) print ( "" decrypted message: "" , decrypted_text . decode ( ' utf-8 ' )) enter fullscreen mode exit fullscreen mode rsa encryption rsa encryption is like having a pair of virtual locks and keys. you use one key (public key) to lock your message, and the recipient uses the other key (private key) to unlock it. this ensures only the intended recipient can read the message. imagine sending a letter in a locked box that only the recipient's unique key can open. it is named after its inventors, ron rivest, adi shamir, and leonard adleman. here's a python example using the cryptography library: from cryptography.hazmat.primitives import serialization from cryptography.hazmat.primitives.asymmetric import rsa , padding # generate rsa key pair private_key = rsa . generate_private_key ( public_exponent = 65537 , key_size = 2048 ) public_key = private_key . public_key () # encrypt using rsa public key message = b "" hello, rsa encryption! "" ciphertext = public_key . encrypt ( message , padding . oaep ( mgf = padding . mgf1 ( algorithm = hashes . sha256 ()), algorithm = hashes . sha256 (), label = none )) # decrypt using rsa private key decrypted_message = private_key . decrypt ( ciphertext , padding . oaep ( mgf = padding . mgf1 ( algorithm = hashes . sha256 ()), algorithm = hashes . sha256 (), label = none )) print ( "" original message: "" , message . decode ( ' utf-8 ' )) print ( "" encrypted message: "" , ciphertext ) print ( "" decrypted message: "" , decrypted_message . decode ( ' utf-8 ' )) enter fullscreen mode exit fullscreen mode elliptic curve cryptography elliptic curve cryptography (ecc) is a modern asymmetric encryption algorithm that provides strong security with relatively short key sizes compared to rsa. ecc is based on the mathematics of elliptic curves over finite fields. it has a ability to provide the same level of security with much smaller key sizes compared to other algorithms. for example, a 256-bit ecc key is considered to be as secure as a 3072-bit rsa key. this makes ecc more efficient in terms of computational power and memory usage, which is especially important for resource-constrained devices such as mobile phones and smart card. data encryption in various environments in today's world, ensuring the security of our sensitive information is of utmost importance. data encryption plays a crucial role in safeguarding our data from unauthorized access and potential breaches. implementing encryption techniques in different environments helps enhance data security. encryption in cloud computing cloud computing has become popular for storing and processing data. when it comes to encryption in cloud computing, there are two key areas to consider: cloud storage encryption when we keep our information in the cloud, it's crucial to ensure that even if someone gets into the cloud server without permission, they can't make sense of the data. cloud storage encryption changes the data into a secret code that only someone with the correct key can understand. so, if a hacker breaks into the cloud, the taken data stays unreadable unless they have the right key. encryption for cloud-based applications many of the applications we use daily, like email or collaboration tools, are hosted in the cloud. encryption for these applications involves securing the data that travels between your device and the cloud server. this prevents hackers from intercepting sensitive information while it's in transit. encryption for on-premises systems on-premises systems basically means that a company or organization stores its important data and software on its own computers and servers within its own physical space, like their office building or data center. they have control over these systems because they're right there where they work. it's like turning your information into a secret code. when data is encrypted, it's really hard to understand without a decryption key. this key is like the key to your locked box at home. only the people who have this key can turn the secret code back into the actual information. mobile device encryption mobile devices, like smartphones and tablets, store a vast amount of personal and sensitive information. mobile device encryption involves scrambling the data on your device so that only you, with your unique password or pin, can access it. even if your device is lost or stolen, the data remains protected as long as your password isn't compromised. encryption in enterprise networks in large organizations, data is often shared across various departments and locations through interconnected networks. encryption in enterprise networks ensures that data moving between different parts of the organization is encrypted. this safeguards the data against potential breaches or eavesdropping during transmission. key lifecycle management when we talk about encryption, the key is like a special secret that unlocks and locks our important data. just like how we need to keep our house keys safe, we also need to manage encryption keys properly to keep our data safe. key lifecycle management is like taking care of these special keys throughout their entire ""life.” it includes key generation, distribution, usage, storage, and eventually, key retirement. key generation the first step in key lifecycle management is generating strong and random encryption keys. this process typically involves using cryptographic algorithms to create keys with a high level of entropy or randomness. key usage once we have these keys, we use them to encrypt our data so that only people with the right key can understand it.  it is important to ensure that keys are used correctly and securely to maintain the confidentiality and integrity of the encrypted information. key storage storing these keys securely is very important. just like we keep our house keys in a safe place, we need to make sure nobody else can get their hands on our encryption keys. key rotation key rotation and disposal are critical components of key management to maintain the security of encrypted data over time. they involve periodically changing encryption keys and securely disposing of old or compromised keys. key deletion or key disposal sometimes, we don't need certain keys anymore. just like you might throw away an old, worn-out key, we have to dispose of encryption keys properly so they can't be misused. wrapping up in a world where our personal information and sensitive data are more valuable than ever, keeping them safe is a top priority. data encryption, the art of turning information into a secret code, has become our digital guardian. we've explored how encryption technology works wonders, whether your data is resting peacefully on a server or traveling through the vast online highways. by understanding encryption at rest and in transit, you've gained insight into how your data is shielded from prying eyes. remember, encryption at rest ensures that your data takes a nap in a secure fortress whenever it's not in use, while encryption in transit guards your data as it journeys from one digital stop to another. in conclusion, embracing encryption empowers you to take control of your digital life's security. as you use online services, shop, communicate, and work remotely, encryption stands as a stalwart shield, defending your information from potential threats. so, whether your data is taking a rest or embarking on a journey, encryption technologies are your trusted companions, ensuring your privacy and peace of mind in this interconnected age. if you want more articles on similar topics just let me know in the comments section. and don't forget to ❤️ the article. i'll see you in the next one. in the meantime you can follow me here: jatin sharma follow turning web dreams into reality, just like tony stark's vision top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse documatic follow more from documatic transitioning to a microservices architecture: overcoming obstacles # javascript # beginners # programming # webdev data privacy laws: navigating compliance in the age of big data # beginners # programming # webdev # database the future of cloud computing: predictions and trends # webdev # javascript # beginners # tutorial 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/higordiego/descubra-como-encontrar-buckets-secretos-na-nuvem-da-amazon-s3--105m,,,"descubra como encontrar buckets ocultos na nuvem da amazon s3! 👀🔍 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse higor diego posted on oct 28, 2023 descubra como encontrar buckets ocultos na nuvem da amazon s3! 👀🔍 # aws # security # vulnerabilities # gobuster hoje vamos falar sobre uma parada que é pura emoção: encontrar diretórios ocultos nos famosos s3 buckets da amazon. mas, antes de irmos fundo nesse assunto, um aviso importante: aqui é tudo na base da educação, ok? se você fizer lambança e der ruim, a responsa não é minha. sempre peça permissão antes de sair fazendo varredura, força bruta ou exploração em sistemas alheios. beleza? 😉 amazon s3 o amazon s3 é um serviço de armazenamento de objetos altamente escalável e durável oferecido pela amazon web services (aws). ele foi projetado para armazenar, gerenciar e recuperar grandes quantidades de dados de forma confiável e segura na nuvem. o s3 é amplamente utilizado por empresas de todos os tamanhos para armazenar uma variedade de tipos de dados, como arquivos, imagens, vídeos e backups, tornando-o uma escolha popular para soluções de armazenamento na nuvem. sua escalabilidade, alta disponibilidade e segurança o tornam uma ferramenta essencial para diversas aplicações, desde hospedagem de sites até análise de dados e backup de informações críticas. configurações e riscos de segurança s3 o amazon s3 é uma plataforma incrivelmente versátil, atendendo a uma ampla gama de necessidades, desde o armazenamento de backups e dados de websites até a execução de análises de big data e o gerenciamento de arquivos. no entanto, ao lidar com um volume tão vasto e diversificado de dados em um único local, surgem preocupações legítimas sobre segurança. então, quais são essas preocupações de segurança e quais configurações de segurança um administrador pode implementar para proteger adequadamente um ""bucket"" e seus arquivos no amazon s3? em primeiro lugar, é fundamental entender que o amazon s3 é considerado uma ""plataforma publicamente acessível"". isso significa que, com a url e as permissões apropriadas, qualquer ""bucket"" pode ser acessado de qualquer lugar por meio de solicitações http, semelhante ao que um navegador faz para acessar um site comum. a acessibilidade de um ""bucket"" por meio de sua url depende das medidas de segurança habilitadas ou não. no entanto, é importante destacar que o amazon s3 não é um recurso escondido, disponível apenas após passar por várias camadas de autenticação. pelo contrário, é um recurso que pode ser acessado por meio dos endpoints da aws de qualquer lugar na web, o que representa o principal risco de segurança. qualquer ""bucket"" s3 e seus dados associados têm o potencial de serem acessíveis. no que diz respeito à segurança do amazon s3, as verificações implementadas pelo serviço para autorizar o acesso a um recurso s3, seja um ""bucket"" ou um objeto, são robustas. o s3 avalia as permissões em nível de usuário, por meio de políticas de ""bucket"", listas de controle de acesso (acls) de ""bucket"" e acls de objeto. essas camadas de segurança são essenciais para garantir que apenas usuários autorizados possam acessar e manipular os dados armazenados no s3. como encontrar buckets abertos da amazon s3 nos últimos anos, presenciamos incidentes alarmantes de vazamentos de dados em grandes empresas devido à exposição de ""buckets"" no amazon s3. se você quiser conferir uma lista dos vazamentos mais notórios, pode encontrá-la aqui . através dessas falhas de segurança, uma ampla gama de informações confidenciais foi, inadvertidamente, tornada acessível ao público. estamos falando de números de seguro social, fotos pessoais, registros de vendas, nomes de usuário e senhas, históricos médicos e relatórios de crédito, entre outras coisas. ferramentas de pesquisa de bucket s3 se você está interessado em explorar ""buckets"" disponíveis publicamente no amazon s3, é necessário utilizar uma ferramenta que execute testes para verificar a existência desses ""buckets"". quando um nome de ""bucket"" não existe, o serviço retornará um código de erro chamado ""nosuchbucket"". felizmente, existem várias ferramentas disponíveis para essa finalidade. inicialmente, algumas das primeiras ferramentas de busca de ""buckets"" no s3 incluíam opções como o bucket lazy s3, bucket_finder, aws cred scanner, sandcastle, mass3, dumpster diver, s3 bucket finder, gobuster e s3scanner. essas ferramentas desempenharam um papel importante ao verificar palavras-chave nos nomes de ""buckets"" publicamente acessíveis, embora apresentassem algumas limitações. os resultados muitas vezes incluíam ""buckets"" irrelevantes e exibiam apenas os primeiros mil arquivos de seu conteúdo. o que é gobuster desenvolvido na linguagem go, o gobuster é um scanner de alta performance que se revela uma ferramenta valiosa para localizar diretórios ocultos, urls, subdomínios e buckets s3. a pergunta que frequentemente surge é: ""e quanto ao ffuf?"" embora o ffuf seja um web fuzzer notável, o gobuster emerge como uma alternativa ainda mais veloz e versátil. além disso, o gobuster oferece suporte a extensões que ampliam suas funcionalidades, o que é um diferencial. o programa também é capaz de dimensionar suas operações com o uso de múltiplas threads e realizar varreduras paralelas para agilizar os resultados. agora, quanto à instalação do gobuster, o processo é bastante simples. se você estiver usando o kali linux ou o parrot os, o gobuster já estará pré-instalado para você. se, por outro lado, você utiliza o ubuntu ou um sistema baseado no debian, é possível instalar o gobuster com facilidade utilizando o apt: linux apt install gobuster enter fullscreen mode exit fullscreen mode macos brew install gobuster enter fullscreen mode exit fullscreen mode após concluir a instalação, será necessário contar com uma lista de palavras. essa lista pode englobar senhas, nomes de usuário, subdomínios e assim por diante. existem várias fontes onde você pode obter listas de palavras úteis. minha recomendação é utilizar o seclists. o seclists é um repositório abrangente de diversas listas usadas em avaliações de segurança. isso inclui coleções de nomes de usuário, senhas, urls, e muito mais. se você estiver utilizando o kali linux, pode encontrar o seclists no diretório /usr/share/wordlists. para experimentar o gobuster em tempo real, você tem a opção de usar seu próprio site como alvo ou, de forma prática, empregar um aplicativo da web chamado damn vulnerable web application (dvwa). o dvwa é uma aplicação da web propositadamente mal configurada, criada para ser vulnerável de forma intencional. é uma ferramenta frequentemente utilizada por profissionais de testes de penetração para praticar e aprimorar suas habilidades em ataques a aplicações web. módulo dns o dns, que significa sistema de nomes de domínio, é um pilar da internet que converte nomes de domínio fáceis de entender para endereços ip, que são usados por computadores para identificar recursos na rede. ele age como uma espécie de ""catálogo telefônico"" da internet, permitindo que navegadores e aplicativos localizem servidores e sites correspondentes aos nomes de domínio inseridos pelos usuários. o dns desempenha um papel crucial na facilitação da navegação na web e na comunicação online. o modo dns é empregado para descobrir subdomínios ocultos dentro de um domínio-alvo. por exemplo, se você possui um domínio chamado meudomínio.com, é possível utilizar o gobuster para encontrar subdomínios como admin.meudominio.com, suporte.meudominio.com e assim por diante. ┌── ( higordiego㉿host ) -[~]
└─ $ gobuster dns -h uses dns subdomain enumeration mode

usage:
  gobuster dns [ flags]

flags: -d , --domain string      the target domain -h , --help help for dns --no-fqdn do not automatically add a trailing dot to the domain, so the resolver uses the dns search domain -r , --resolver string    use custom dns server ( format server.com or server.com:port ) -c , --show-cname show cname records ( cannot be used with '-i' option ) -i , --show-ips show ip addresses --timeout duration   dns resolver timeout ( default 1s ) --wildcard force continued operation when wildcard found

global flags: --debug enable debug output --delay duration        time each thread waits between requests ( e.g. 1500ms ) --no-color disable color output --no-error don 't display errors
  -z, --no-progress           don' t display progress -o , --output string         output file to write results to ( defaults to stdout ) -p , --pattern string        file containing replacement patterns -q , --quiet don 't print the banner and other noise
  -t, --threads int           number of concurrent threads (default 10)
  -v, --verbose               verbose output (errors)
  -w, --wordlist string       path to the wordlist. set to - to use stdin.
      --wordlist-offset int   resume from a given position in the wordlist (defaults to 0) enter fullscreen mode exit fullscreen mode iremos executar uma enumeração de dns, você pode executar com o seguinte comando: ┌── ( higordiego㉿host ) -[~]
└─ $ gobuster dns -d test.io -w /usr/share/wordlists/dirb/common.txt =============================================================== gobuster v3.6
by oj reeves ( @thecolonial ) & christian mehlmauer ( @firefart ) =============================================================== [ +] domain:     test.io [ +] threads:    10 [ +] timeout:    1s [ +] wordlist:   /usr/share/wordlists/dirb/common.txt =============================================================== starting gobuster in dns enumeration mode =============================================================== found: api.test.io

progress: 4614 / 4615 ( 99.98% ) =============================================================== finished =============================================================== enter fullscreen mode exit fullscreen mode módulo aws s3 o amazon s3 é amplamente adotado por um grande número de empresas para distribuir conteúdo, abrangendo desde material público, como imagens de sites, até arquivos privados e confidenciais. agora, vamos explorar o comando de ajuda para identificar os diversos parâmetros oferecidos pelo gobuster em seu modo s3. ┌── ( higordiego㉿host ) -[~]
└─ $ gobuster s3 -h uses aws bucket enumeration mode

usage:
  gobuster s3 [ flags]

flags: --client-cert-p12 string            a p12 file to use for options tls client certificates --client-cert-p12-password string   the password to the p12 file --client-cert-pem string            public key in pem format for optional tls client certificates --client-cert-pem-key string        private key in pem format for optional tls client certificates ( this key needs to have no password ) -h , --help help for s3 -m , --maxfiles int                      max files to list when listing buckets ( only shown in verbose mode ) ( default 5 ) -k , --no-tls-validation skip tls certificate verification --proxy string                      proxy to use for requests [ http ( s ) ://host:port] or [ socks5://host:port] --random-agent use a random user-agent string --retry should retry on request timeout --retry-attempts int                times to retry on request timeout ( default 3 ) --timeout duration                  http timeout ( default 10s ) -a , --useragent string                  set the user-agent string ( default ""gobuster/3.6"" ) global flags: --debug enable debug output --delay duration        time each thread waits between requests ( e.g. 1500ms ) --no-color disable color output --no-error don 't display errors
  -z, --no-progress           don' t display progress -o , --output string         output file to write results to ( defaults to stdout ) -p , --pattern string        file containing replacement patterns -q , --quiet don 't print the banner and other noise
  -t, --threads int           number of concurrent threads (default 10)
  -v, --verbose               verbose output (errors)
  -w, --wordlist string       path to the wordlist. set to - to use stdin.
      --wordlist-offset int   resume from a given position in the wordlist (defaults to 0) enter fullscreen mode exit fullscreen mode vamos criar um arquivo chamado 'bucket_list.txt' e adicionar uma lista de nomes que podem ser usados como nomes de buckets no amazon s3. após a criação do arquivo, seguiremos adiante com a execução do comando a seguir. ┌── ( higordiego㉿host ) -[~]
└─ $ gobuster s3 -w bucket_list.txt =============================================================== gobuster v3.6
by oj reeves ( @thecolonial ) & christian mehlmauer ( @firefart ) =============================================================== [ +] threads:                 10 [ +] wordlist:                bucket_list.txt [ +] user agent:              gobuster/3.6 [ +] timeout:                 10s [ +] maximum files to list:   5 =============================================================== starting gobuster in s3 bucket enumeration mode =============================================================== [ error] get ""https://dev.app.test.io.com.br.s3.amazonaws.com/?max-keys=5"" : tls: failed to verify certificate: x509: certificate is valid for * .s3.amazonaws.com, s3.amazonaws.com, not dev.app.test.io.com.br.s3.amazonaws.com [ error] get ""https://beta.test.io.com.br.s3.amazonaws.com/?max-keys=5"" : tls: failed to verify certificate: x509: certificate is valid for * .s3.amazonaws.com, s3.amazonaws.com, not beta.test.io.com.br.s3.amazonaws.com [ error] get ""https://echo.test.io.s3.amazonaws.com/?max-keys=5"" : tls: failed to verify certificate: x509: certificate is valid for * .s3.amazonaws.com, s3.amazonaws.com, not echo.test.io.s3.amazonaws.com [ error] get ""https://test.io.com.br.s3.amazonaws.com/?max-keys=5"" : tls: failed to verify certificate: x509: certificate is valid for * .s3.amazonaws.com, s3.amazonaws.com, not test.io.com.br.s3.amazonaws.com [ error] get ""https://test.com.br.s3.amazonaws.com/?max-keys=5"" : tls: failed to verify certificate: x509: certificate is valid for * .s3.amazonaws.com, s3.amazonaws.com, not test.com.br.s3.amazonaws.com [ error] get ""https://app.test.io.com.br.s3.amazonaws.com/?max-keys=5"" : tls: failed to verify certificate: x509: certificate is valid for * .s3.amazonaws.com, s3.amazonaws.com, not app.test.io.com.br.s3.amazonaws.com

http://prod_test.s3.amazonaws.com/

http://test-images.s3.amazonaws.com/

progress: 8 / 9 ( 88.89% ) =============================================================== finished =============================================================== enter fullscreen mode exit fullscreen mode módulo de diretório a função de exploração de diretórios no gobuster desempenha um papel fundamental na busca por arquivos ocultos e caminhos de url. isso engloba uma ampla variedade de recursos, abrangendo desde imagens e arquivos de script até praticamente qualquer tipo de arquivo disponível na internet. segue abaixo o comando para buscar o módulo dir no dns achado no passo anterior. ┌── ( higordiego㉿host ) -[~/documentos/artigo-s3]
└─ $ gobuster dir -w files_list.txt -u http://test-images.s3.amazonaws.com/ =============================================================== gobuster v3.6
by oj reeves ( @thecolonial ) & christian mehlmauer ( @firefart ) =============================================================== [ +] url:                     http://test-images.s3.amazonaws.com/ [ +] method:                  get [ +] threads:                 10 [ +] wordlist:                files_list.txt [ +] negative status codes:   404 [ +] exclude length:          243 [ +] user agent:              gobuster/3.6 [ +] timeout:                 10s =============================================================== starting gobuster in directory enumeration mode =============================================================== /image/b26086426c7e483b2924e775485b3215.jpg ( status: 200 ) [ size: 629968]
/image/contrato.jpg ( status: 200 ) [ size: 2830252] =============================================================== finished =============================================================== enter fullscreen mode exit fullscreen mode como posso evitar ataque de enumeração com o gobuster ? se você está preocupado com a segurança do amazon s3 e deseja evitar a enumeração de buckets, aqui estão algumas medidas que você pode adotar: controle de acesso: utilize políticas de controle de acesso e permissões estritas no amazon s3 para garantir que apenas as pessoas autorizadas possam listar, acessar ou modi monitore atividades: implemente a monitorização de atividades em seu amazon s3 para detectar tentativas de acesso não autorizado ou enumeração de buckets. restrinja o acesso por referência: utilize referenciamento condicional para restringir o acesso ao bucket com base no cabeçalho ""referer"" da solicitação http. bloqueie acesso a diretórios: configure os diretórios dentro do bucket de forma a não permitir listagem, o que dificulta a enumeração de subdiretórios. proteja suas credenciais: mantenha suas credenciais seguras e evite compartilhá-las de maneira inadequada. realize auditorias regulares: realize auditorias regulares em sua configuração do amazon s3 para garantir que suas medidas de segurança estejam atualizadas e eficazes. use soluções de proteção contra bots: serviços de proteção contra bots como o cloudflare impedirão qualquer ataque de força bruta, tornando incrivelmente difícil atacar seu aplicativo web. conclusão lembre-se de que a segurança do amazon s3 é crucial, pois o acesso não autorizado aos seus buckets pode levar a vazamentos de dados e outras vulnerabilidades. certifique-se de seguir as melhores práticas de segurança da aws para proteger adequadamente seus recursos no s3. referências https://medium.com/quiknapp/fuzz-faster-with-ffuf-c18c031fc480 https://medium.com/@janijay007/s3-bucket-misconfiguration-from-basics-to-pawn-6893776d1007 https://takahiro-oda.medium.com/project-perform-s3-bucket-enumeration-using-various-s3-bucket-enumeration-tools-f4f63923b28 https://medium.com/stealthsecurity/finding-hidden-directories-sub-domains-and-s3-buckets-using-gobuster-5c2e3c41ff96 top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse higor diego follow sou arquiteto de soluções com 8 anos de experiência, mesclando as áreas de desenvolvimento, arquitetura, liderança técnica e devops. location juazeiro do norte - ce work arquiteto de soluções joined sep 20, 2019 more from higor diego guia de passo a passo para escrever um exploit. # security # exploit # tutorial # owned enumeração de dns # security # dns # tutorial # osint discover how to find hidden buckets in amazon s3 cloud. # security # s3 # osint # tutorial 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/maximsaplin/all-m3-macbook-pro-configs-ranking-by-computeramssd-per--pca,,,"all {m3 macbook pro} configs: ranking by compute/ram/ssd per $ - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse maxim saplin posted on nov 12, 2023 all {m3 macbook pro} configs: ranking by compute/ram/ssd per $ # apple # macbook # development # hardware it's been almost a week since new m3 macbook pro have been released. there's plenty of reviews on youtube discussing the new black colour, counting cpu cores and testing performance. though in this video the author has shared a google spreadsheet with specs and prices for all 83 configurations available for purchase. some of the specs included ram and ssd sizes, geekbench cpu score. ""what is the value for a buck?"" popped in my head. right away i cloned the spreadsheet and started playing with formulas. here's my take on value for money in different categories: category inches proc p cores e cores ram ssd geekbench6 multi core score price best compute/$ 14 m3 4 4 8 512 11763 $1,599 best ram/$ 14 m3 max 12 4 128 512 21215 $4,499 best ssd/$ 14 m3 max 10 4 36 8192 18519 $5,399 best everything/$ 14 m3 max 10 4 96 8192 18519 $6,199 most expensive 16 m3 max 12 4 128 8192 21215 $7,199 for the best ""compute"", ""ram"" and ""ssd"" i divided geekbench multicore score, ram amount and ssd size (correspondignly) by the model price and sorted with largest value at the top. i.e. best ram/$ was the model that scored 7.35 mb/$ and was at the top of the list. for the ""best in everything"" i used an aggregate score that first normalised each of the 3 columns (cpu, ram, ssd) and then averaged them and divided by price. here's the spreadsheet disclaimer/considerations this is more of a thought experiment rather than a holistic testing more is better is valid for ram and ssd, while for 14"" vs 16"" might not be the case e.g. i am writing this article on 14"" mbp and value it's compact size more there's a review that suggests that 8gb m3 base model is significantly inferior to bigger models even in cpu department as soon as you start multitasking and consume some of the ram top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse maxim saplin follow ツ manager, engineer, open-source maintainer education computer science @bntu, mba @bsu work epam, delivery partner joined oct 12, 2019 more from maxim saplin the power of pragmatism: engineering cultures and china's ascendancy # development # culture # productivity # opensource python 3.13 rc1 - a quick cpu benchmark # python # programming # performance # development 4090 - ecc on vs ecc off # ai # machinelearning # llm # hardware 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/stripe/payment-link-data-passing-1njp,,,"payment link data passing - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse cj avilla for stripe posted on nov 14, 2022 • originally published at cjav.dev payment link data passing # payments # stripe # tutorial payment links are stripe’s no-code tool for quickly getting started with collecting payments. one thing that can be a little tricky, though, is passing data with your payment links. in this blog post, we'll show you three ways to do it! here’s an example url for a test mode payment link: https://buy.stripe.com/test_dr6blt5ckaz2fwoevz` . you can visit this page and test the flow with 4242 4242 4242 4242 any future expiration and any cvc code. notice that by default this payment link will show you a stripe hosted confirmation page thanking you for your test purchase. i’ve also created this payment link that will redirect instead of showing the default confirmation page: https://buy.stripe.com/test_8wm8zhe8qaz2dog8xa . the code for this demo is available here: https://replit.com/@stripe/confirmation#index.js . you can fork and remix directly on replit to experiment. client_reference_id query string parameters the client_reference_id query string parameter is most useful when you want to associate payments with customers in your database. it's common to use the id of the authenticated user in your database as the value for the client_reference_id . https://buy.stripe.com/test_8wm8zhe8qaz2dog8xa?client_reference_id=my-customer-id enter fullscreen mode exit fullscreen mode after the customer successfully pays, you can retrieve the related checkout session object that relates to their payment session. the checkout session will contain the client_reference_id you set. there are two common ways to get the checkout session after successful payment. one is to listen for the checkout.session.completed webhook event notification, which paul covers in depth in his article about fulfilment . the other is to use the checkout session id embedded in the url when the customer redirects back to your site. in the next section, you'll learn how to configure the paymentlink so that the checkout session id is available in the url for your confirmation page. checkout session id placeholder to configure a payment link to pass the checkout session's id to the confirmation page, you'll need to update the payment link settings. from the payment link edit page, on the tab for ""confirmation page,"" select ""don't show a confirmation page"" and instead enter the url where your customer should redirect. anywhere in that url you can add the literal string {checkout_session_id} . before stripe redirects the customer to this url, that placeholder will be replaced with the id of the checkout session. https://buy.stripe.com/test_9aq7vdggygxqac48xc enter fullscreen mode exit fullscreen mode utm query string parameters when marketing your business it can be useful to know which marketing campaigns or channels are driving the most conversion so you can invest more in what’s working. if you're running a campaign and want to track how many payments come through from each source, you can add utm parameters to the query string. payment links currently supports the following utm codes: utm_source, utm_content, utm_medium, utm_term, and utm_campaign. when customers complete a payment, they are redirected to a url with the utm code parameters specified in your payment link url. note that the utm params are not available on the checkout session object. https://buy.stripe.com/test_8wm8zhe8qaz2dog8xa?utm_source=replit&utm_medium=blog&utm_campaign=no-code&utm_term=no-code-payments enter fullscreen mode exit fullscreen mode after completing payment successfully with that test link, look in the url for the utm params that were passed through. conclusion combining all three approaches enables us to pass a client reference id in the query string for a specific user and re-retrieve the checkout session on our confirmation page using the checkout session’s id in the query string. we can also build attribution with utm params that passed through the query string url in the redirect. https://buy.stripe.com/test_9aq7vdggygxqac48xc?client_reference_id=all-together&utm_source=replit&utm_medium=blog&utm_campaign=no-code&utm_term=no-code-payments enter fullscreen mode exit fullscreen mode using stripe payment links is a great way to automate your payment flows without having to write any code. developers can easily include the links in applications, and no-coders can use them to create payments pages without any technical knowledge. additionally, you can track payments coming through from different sources by adding utm parameters to the query string. this makes it easy to see how well your campaigns are performing. we hope you'll find these tools helpful as you start accepting payments on your website or application. top comments (4) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand martin machava martin machava martin machava follow location vienna work software engineer joined mar 13, 2022 • nov 17 '22 • edited on nov 18 • edited dropdown menu copy link hide hi, i found your article because i do this the same way as you do but client_reference_id in checkout.session.completed event is null . can you, please, verify in events that your client_reference_id field is non null? are you creating session before serving payment link or are you using raw payment link with client_reference_id url parameter? parameter prefilled_email seems to work fine. edit: ""client_reference_id can be composed of alphanumeric characters, dashes, or underscores, and be any value up to 200 characters. invalid values are silently dropped"" ok, that's why. i was sending email address as reference which contains invalid character ""@"" and that's why it's dropped. like comment: like comment: 1 like like comment button reply collapse expand rajesh angira rajesh angira rajesh angira follow joined jul 27, 2023 • jul 27 '23 dropdown menu copy link hide i have tried adding client_reference_id in payment link and its only populating in checkout session completed but not getting populating in payment_intent.payment_failed when payment is failed. i have also tried setting metadata in payment link but not receiving metadata back in neither checkout session completed nor payment_intent.payment_failed (success/failed). can someone plz help me to set metadata in payment link and receiving this metadata in payment_intent.payment_failed event and checkout session completed event like comment: like comment: like comment button reply collapse expand praneeth praneeth praneeth follow joined feb 19, 2024 • feb 19 '24 dropdown menu copy link hide @rajesh_angira i am running into same issue. were you able to figure this out? please let me know. thanks! like comment: like comment: like comment button reply collapse expand ameft ameft ameft follow location italy joined may 19, 2020 • jan 13 '23 dropdown menu copy link hide all this possibilities, pretty important when using payment links, should be better referenced when creating a payment link, especially {checkout_session_id}! i came up with the idea of using checkout_session_id all by myself knowing already that checkout worked similarly... like comment: like comment: like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse stripe follow stay updated with stripe developer news . be the first to hear about product updates and developer resources. get updates more from stripe stripe developer digest sessions 2024 # stripe # stripedevdigest # showdev # news march stripe developer digest # stripe # stripedevdigest # showdev # news february stripe developer digest # stripe # stripedevdigest # showdev # news 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/documatic/what-is-bcrypt-how-to-use-it-to-hash-passwords-5c0g,,,"what is bcrypt. how to use it to hash passwords - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse shubham sharma for documatic posted on nov 17, 2022 what is bcrypt. how to use it to hash passwords # programming # tutorial # webdev # node the bcrypt npm package is a javascript implementation of the bcrypt password hashing function that allows you to easily create a hash out of a password string. unlike encryption which you can decode to get back the original password, hashing is a one-way function that can’t be reversed once done. when the user submits a password, the password will be hashed and your javascript application needs to store the hash in the database. later when the user wants to authenticate his or her account, you need to compare the password input with the hash stored in your database to see if it matches. the bcrypt library makes the process easy by providing you with methods to hash and compare passwords. ** to start using the library, you need to install it with your package manager: ** npm install bcrypt # or yarn add bcrypt enter fullscreen mode exit fullscreen mode ** then include the module to your javascript code with require: ** const bcrypt = require ( "" bcrypt "" ); enter fullscreen mode exit fullscreen mode creating a password hash with bcrypt to generate a password using the bycrypt module, you need to call on the hash() method which accepts the following three parameters: the password string that you wish to hash the number of rounds to secure the hash. the number commonly 
ranges from 5 to 15 the callback function to execute when the hash process is finished, passing along the error message and the hash result here’s an example of bcrypt.hash() processing the password string ""generic"": const bcrypt = require ( "" bcrypt "" ); bcrypt . hash ( "" generic "" , 5 , function ( err , hash ) { console . log ( hash ); // todo: store the hash in your password db }); enter fullscreen mode exit fullscreen mode or you can use the synchronous method equivalent called hashsync(): const bcrypt = require ( "" bcrypt "" ); const myplaintextpassword = "" generic "" ; const hash = bcrypt . hashsync ( myplaintextpassword , 5 ); console . log ( hash ); enter fullscreen mode exit fullscreen mode generating salt for the hash a hashing function requires you to add salt into the process. a salt is simply a random data that’s used as an additional input to the hashing function to safeguard your password. the random string from the salt makes the hash unpredictable. in the code examples above, the salt is auto-generated by bcrypt module, but you can actually generate the salt first before hashing the password. to generate a salt, you can use the gensalt() method from the module: const bcrypt = require ( "" bcrypt "" ); bcrypt . gensalt ( 10 , function ( err , salt ) { console . log ( salt ); // the random salt string }); enter fullscreen mode exit fullscreen mode once you have the salt, you can pass it to the hash() method as follows: const bcrypt = require ( "" bcrypt "" ); bcrypt . gensalt ( 10 , function ( err , salt ) { bcrypt . hash ( "" generic "" , salt , function ( err , hash ) { console . log ( hash ); // store hash in your password db. }); }); enter fullscreen mode exit fullscreen mode there’s also a synchronous equivalent for the method called gensaltsync(): const salt = bcrypt . gensaltsync ( 10 ); const hash = bcrypt . hashsync ( "" generic "" , salt ); enter fullscreen mode exit fullscreen mode note that there’s no differences on the resulting hash whether you generate a salt separately or automatically, but the cpu usage may be lowered when you separate the salt generation and the hash process. you should test if your nodejs server can handle auto-generating the salt and hash first before separating them. a note on the salt round number the salt generation for your hash function can range from a few seconds to many days, depending on how many rounds you passed. the bcrypt module will go through 2^rounds to generate the salt to give you a secure hash. according to the documentation , here’s the amount of time to process the salt generation on a 2ghz core computer: rounds = 8 : ~40 hashes/sec rounds = 9 : ~20 hashes/sec rounds = 10: ~10 hashes/sec rounds = 11: ~5  hashes/sec rounds = 12: 2-3 hashes/sec rounds = 13: ~1 sec/hash rounds = 14: ~1.5 sec/hash rounds = 15: ~3 sec/hash rounds = 25: ~1 hour/hash rounds = 31: 2-3 days/hash enter fullscreen mode exit fullscreen mode the number is just an estimation, so you may want to test the highest rounds with the fastest generation time that your server can support. verifying a password with bcrypt once you saved the hash to the database, you can compare the user’s plain text input with the stored hash using the compare() method. the compare() method accepts three parameters: the plain string password for comparison the hash string created earlier and the callback function once the comparison process is 
finished the method will pass the error err object and the boolean value result, telling you whether the comparison matches or not. here’s an example of the compare() method in action: const bcrypt = require ( "" bcrypt "" ); bcrypt . hash ( "" generic "" , 5 , function ( err , hash ) { console . log ( hash ); // the hash returned, continue to compare bcrypt . compare ( "" generic "" , hash , function ( err , result ) { console . log ( "" generic: "" , result ); // generic: true }); bcrypt . compare ( "" falsy "" , hash , function ( err , result ) { console . log ( "" falsy: "" , result ); // falsy: false }); }); enter fullscreen mode exit fullscreen mode the module also provides the synchronous method comparesync for you to use: const bcrypt = require ( "" bcrypt "" ); const myplaintextpassword = "" generic "" ; const hash = bcrypt . hashsync ( myplaintextpassword , 5 ); const result = bcrypt . comparesync ( myplaintextpassword , hash ); console . log ( result ); // true enter fullscreen mode exit fullscreen mode after the comparison is finished, you need to provide the right authentication code according to the returned result. using promise or async/await instead of callback function finally, bcrypt module also supports the use of promise and async/await code style, so you can use them instead of callbacks to make your code cleaner. here’s an example of using promises in the hash process: const bcrypt = require ( "" bcrypt "" ); bcrypt . hash ( "" generic "" , 5 ) . then (( hash ) => { return bcrypt . compare ( "" generic "" , hash ) . then (( result ) => { console . log ( "" generic: "" , result ); // generic: true }); }) . catch (( err ) => { console . log ( err ); }); enter fullscreen mode exit fullscreen mode and another example using the async/await style: const bcrypt = require ( "" bcrypt "" ); async function passwordhashtest ( password ) { const hash = await bcrypt . hash ( password , 5 ); const result = await bcrypt . compare ( password , hash ); console . log ( result ); // true } passwordhashtest ( "" generic "" ); // test the async function enter fullscreen mode exit fullscreen mode and that’s how the bcrypt module in nodejs works ;) if you like our blog post then please follow our page and give a like to this post. thanks. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse documatic follow more from documatic transitioning to a microservices architecture: overcoming obstacles # javascript # beginners # programming # webdev data privacy laws: navigating compliance in the age of big data # beginners # programming # webdev # database the future of cloud computing: predictions and trends # webdev # javascript # beginners # tutorial 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/niexq/implement-a-react-component-that-automatically-calculates-width-and-height-based-on-resizeobserver-3fko,,,"implement a react component that automatically calculates width and height based on resizeobserver - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse niexq posted on jan 11, 2022 implement a react component that automatically calculates width and height based on resizeobserver # react # javascript # webdev # github 🏠 homepage 📦 install yarn add @oyyds/react-auto-sizer # or npm i @oyyds/react-auto-sizer -s enter fullscreen mode exit fullscreen mode 🔨 use import autosizer from ' @oyyds/react-auto-sizer ' ; const autosizecomponent = () => { return ( < div > < autosizer > {({ width , height }) => ( < div style = {{ width , height , }} > content < /div > )} < /autosizer > < /div > ); }; enter fullscreen mode exit fullscreen mode 🧩 business scene now most business scenarios need to be compatible with big data, such as big data table, big data tree, big data drop-down box , etc., and all big data components need to specify width and height , most of the actual business interface needs to calculate the width and height in real time, and react-auto-sizer is to complete the task of automatic calculation of width and height . 🧑‍💻 coding at the beginning of the pre-research, windows was bound to resize , but due to resize , there will be performance problems when the window changes, and some extreme pixels will appear jitter. ; resizeobserver, the interface can listen to changes in the content area of element or the bounding box of svgelement . the content area needs to be subtracted from the padding. -- from mdn resizeobserver best choice， use react hook useeffect ，the core code is as follows： const updatestate = usecallback ( ( newwidth : number , newheight : number , entry : resizeobserverentry ) => { // update state // onresize width, height props . onresize ({ width : newwidth , height : newheight }, entry ); }, [ childparams , disableheight , disablewidth , onresize ], ); const observer = usememo ( () => new resizeobserver (( entries : resizeobserverentry []) => { for ( const entry of entries ) { const contentrect = entry . contentrect ; const width = math . trunc ( contentrect ?. width || 0 ); const height = math . trunc ( contentrect ?. height || 0 ); updatestate ( width , height , entry ); } }), [ updatestate ], ); useeffect (() => { if ( ! _autosizerref ?. current ?. parentnode ) { throw new error ( ' not found autosizer parentnode ' ); } observer . observe ( _autosizerref ?. current ?. parentnode as element ); return () => { observer . disconnect (); }; }, [ observer ]); enter fullscreen mode exit fullscreen mode focus： observer.observe(_autosizerref?.current?.parentnode as element) ,listen parent dom node contentrect : resizeobserverentry return a domrectreadonly read-only property contentrect ， object containing the new size of the observed element, property： { ""x"" : 0 , ""y"" : 0 , ""width"" : 300 , ""height"" : 200 , ""top"" : 0 , ""right"" : 300 , ""bottom"" : 200 , ""left"" : 0 } enter fullscreen mode exit fullscreen mode contentrect returns the content box, which is the size of the content area (for detailed reasons, please refer to zhang xinxu's introduction to resizeobserver) so contentrect.width 、 contentrect.height is the width and height we need ⭐️ thanks for reading github： https://github.com/niexq/react-auto-sizer ,  thanks for reading and welcome star 🐳 source of inspiration react-virtualized-auto-sizer resizeobserver detect dom size changes js api resizeobserver top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse niexq follow joined nov 22, 2021 trending on dev community hot just launched my portfolio – would love your feedback👋 # portfolio # nextjs # programming # webdev introducing dev education tracks: expert-guided tutorials for learning new skills and earning badges # deved # career # ai # gemini rediscovering my passion: from burnout back to excitement # devjournal # developer # career # leadership 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/qbentil/puppeteer-screen-recorder-client-expressjs-5259,,Unit Testing,"puppeteer screen recorder server – express.js - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse bentil shadrack posted on oct 12, 2022 • edited on oct 17, 2022 puppeteer screen recorder server – express.js # javascript # node # puppeteerscreenrecorder # express node & express js (7 part series) 1 http methods ""get"", ""post"", ""put"", ""patch"", ""delete"" 2 how to write custom error handler middleware in express.js using javascript 👩‍💻 ... 3 more parts... 3 puppeteer screen recorder server – express.js 4 send email in node.js with nodemailer using gmail account 5 top 5 design practices of a restful api using express.js 6 effective unit testing for rest apis with node.js, typescript 7 swagger & express: documenting your node.js rest api it's imperative to spend some time learning about the implementation and operation of screen recording and include it in your projects. the power of screen recording outweighs that of any screen capture and video editing combo. when a web application has screen recording enabled, users are more likely to save content, look for problems with, and find solutions for, digital screens. one key benefit of screen recording it its ability to demonstrate how to complete tasks on a computer step-by-step. it can also stop, fast-forward, and rewind videos. this method enables the student, teacher, or learner to readily study the content and understand what is happening. in this article, i will guide you on implementing this functionality in a web browser using react and puppeteer-screen-recorder. environment & dependencies node.js is an open-source, cross-platform, back-end javascript runtime environment that runs on a javascript engine and executes javascript code outside a web browser. it is primarily used for non-blocking, event-driven servers, due to its single-threaded nature. puppeteer-screen-recorder is a puppeteer plugin that uses the native chrome devtool protocol for capturing video frame by frame. also supports an option to follow pages that are opened by the current page object. prerequisites • knowledge of javascript programming. • knowledge in express.js and node.js • a code editor such as vscode getting started environment setup: navigate to you project directory on the command prompt // run the following command and follow the commands to set up your environment npm init enter fullscreen mode exit fullscreen mode final output should look like the snippet below👇 install dependencies express nodemon puppeteer-screen-recorder npm i express nodemon puppeteer - screen - recorder enter fullscreen mode exit fullscreen mode create entry file (index.js) note: to understand how to navigate the puppeteer codebase and  understand how it is implemented, i used askyourcode . this will save you a lot of time looking for code snippets and implementation online. import errorhandler from "" ./middlewares/errorhandler.js "" ; import { puppeteerscreenrecorder } from "" puppeteer-screen-recorder "" ; import express from "" express "" ; import puppeteer from "" puppeteer "" ; // init app const app = express (); app . use ( express . json ()); // routes app . post ( "" /record-screen "" , async ( req , res , next ) => { try { const { url } = req . body ; const filename = url . replace ( / [ : /] /g , "" _ "" ); // replace : and / with _ to avoid errors const browser = await puppeteer . launch (); const page = await browser . newpage (); const recorder = new puppeteerscreenrecorder ( page ); await recorder . start ( `./videos/ ${ filename } .mp4` ); // start recording await page . goto ( "" https://qbentil.com "" ); await recorder . stop (); await browser . close (); res . status ( 200 ). json ({ success : true , message : "" video recorded successfully "" , file : `./videos/ ${ filename } .mp4` , }); } catch ( err ) { next ( err ); } }); // error handler middleware app . use ( errorhandler ); // start server app . listen ( 5000 , () => { console . log ( `server started on port ${ 5000 } 🚀` ); }); enter fullscreen mode exit fullscreen mode nb: to use es6 synthax import dependencies, set type: 'module' in the package.json file. final project structure: test you server with any api testing tool/client. in my case, i use thunder client extension in vscode. testing screenshot of recorder video in this article, we only learnt how to setup an api client to record sites when called. you can use askyourcode to explore the package codebase puppeteer-screen-recorder for more configurations to the recorded video such as video size, duration, resolution and many more. i recommend my error handling article for how i implemented the error handling middlerware. happy hacking! bentil here🚀 if you find this content helpful, please like, comment and share node & express js (7 part series) 1 http methods ""get"", ""post"", ""put"", ""patch"", ""delete"" 2 how to write custom error handler middleware in express.js using javascript 👩‍💻 ... 3 more parts... 3 puppeteer screen recorder server – express.js 4 send email in node.js with nodemailer using gmail account 5 top 5 design practices of a restful api using express.js 6 effective unit testing for rest apis with node.js, typescript 7 swagger & express: documenting your node.js rest api top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand ethel akrasi akosua ethel akrasi akosua ethel akrasi akosua follow joined aug 11, 2022 • oct 13 '22 dropdown menu copy link hide great❤️‍🔥👏 like comment: like comment: 2 likes like comment button reply collapse expand konadu akwasi akuoko konadu akwasi akuoko konadu akwasi akuoko follow building software (software and devops eng) 🏗️ technical writer ✍️ about me: konadu.dev/about 🛠built konadu.dev 

talking about software engineering, devops

aws certified location kumasi, ghana joined jul 28, 2021 • oct 12 '22 dropdown menu copy link hide thanks brother, a good guide indeed. happy hacking to you too :) like comment: like comment: 2 likes like comment button reply collapse expand bentil shadrack bentil shadrack bentil shadrack follow software engineer ||
promoting & sharing educative tips & resources from devs💻 and for devs💻| buy me a ☕
https://www.buymeacoffee.com/qbentil email bentilshadrack72@gmail.com location accra, ghana education university of ghana, legon pronouns he/him/his work software engineer joined sep 2, 2021 • oct 12 '22 dropdown menu copy link hide thank you bro🙌 like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse bentil shadrack follow software engineer ||
promoting & sharing educative tips & resources from devs💻 and for devs💻| buy me a ☕
https://www.buymeacoffee.com/qbentil location accra, ghana education university of ghana, legon pronouns he/him/his work software engineer joined sep 2, 2021 more from bentil shadrack from rest to graphql: why and how i made the switch # javascript # programming # restapi # graphql swagger & express: documenting your node.js rest api # webdev # javascript # api # node effective unit testing for rest apis with node.js, typescript # typescript # node # testing # api 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/hbko/adding-an-executable-target-to-a-rust-library-3hej,,,"adding an executable target to a rust library - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse konstantin posted on may 16, 2022 • originally published at iamkonstantin.eu adding an executable target to a rust library # rust # library # targets # snippets using cargo, we can easily create a library project with the new command: cargo new pumpkin-rs --lib enter fullscreen mode exit fullscreen mode of course, the primary purpose of libraries is to encapsulate some kind of functionality that can be then exported for use by other libraries or applications. for the purpose of this post, let’s define a simple function in src/lib.rs which will be the essence of our library. later we will see how we can use this function from a runnable binary target in the same project. pub fn say_hello () { println! ( ""hello from the library"" ); } enter fullscreen mode exit fullscreen mode you may run cargo build just to double-check everything builds successfully… and voilà! cargo targets in certain situations, it may be helpful to have a library project include a runnable component - a stand-alone, runnable program that can be executed. cargo targets allow us to define different combinations of source files which can be compiled in a project. targets can specify what part of the source code is a library, binary, example, unit test, or benchmark. the cool thing is that cargo also uses folder structure conventions to guess which source file does what. for example, src/lib.rs is where it will look for the library code, executable code is expected in src/main.rs or src/bin/ . add a default binary target following the conventions above, we can add a default executable to a library project by creating src/bin/pumpkin.rs : use pumpkin_rs :: say_hello ; fn main () { say_hello () } enter fullscreen mode exit fullscreen mode running cargo run will execute the binary! 🎉 not only that, but cargo build now also produces 2 separate outputs - we get the compiled binary for the library as well as the executable. more than one binary targets we are not limited to a single binary target. there can be more than one! let’s create src/bin/tomato.rs fn main () { println! ( ""hello 🍅"" ) } enter fullscreen mode exit fullscreen mode now, if we just run cargo run like before, we’d get an error message because there are two available binary targets, and cargo doesn’t know which one we want to use. the error message helpfully suggests we can use the target configuration in cargo.toml to define a default-run key or pass the --bin argument to the run command. cargo run -- bin tomato enter fullscreen mode exit fullscreen mode top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse konstantin follow long-time human in tech, indie dev. i build things for humans on mobile and the web. location belgium pronouns he/him joined nov 4, 2018 trending on dev community hot where is the vibe in ""vibe coding"" and what happened to music monday? # watercooler # ai # webdev # productivity 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development troubleshooting production with github copilot: the guide for real humans (and bots with good taste) # githubcopilot # ai # productivity # learning 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/hieunh1801/react-native-firebase-fcm-firebase-messaging-cloud-java-spring-boot-how-to-push-notification-3iac,,,"react native firebase fcm (firebase messaging cloud) + java spring boot: how to push notification - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse nguyễn hữu hiếu posted on oct 6, 2022 react native firebase fcm (firebase messaging cloud) + java spring boot: how to push notification # reactnative # rnfirebase # java # pushnotification how to push fcm notification with java this guide refer to this https://rnfirebase.io/messaging/usage#data-only-messages but for java scenarior just use for data-only message need an extra field to contain id of notification if use notification like normal => not have id want to custom notification send notification for quit + foreground + background @slf4j @springboottest public class fcmtests { @autowired firebasemessaging firebasemessaging ; @test public void test1 () throws firebasemessagingexception { log . info ( ""fcm sending..."" ); list < string > registrationtokens = collections . singletonlist ( ""<your fcm token here>"" ); // ios config aps aps = aps . builder () . setcontentavailable ( true ) . build (); apnsconfig apnsconfig = apnsconfig . builder () . setaps ( aps ) . build (); // android config androidconfig androidconfig = androidconfig . builder () . setpriority ( androidconfig . priority . high ) . build (); multicastmessage message = multicastmessage . builder () . putdata ( ""title"" , ""you can do everything"" ) . putdata ( ""id"" , ""1"" ) . putdata ( ""body"" , ""jumppp...."" ) . setandroidconfig ( androidconfig ) . setapnsconfig ( apnsconfig ) . addalltokens ( registrationtokens ) . build (); batchresponse response = firebasemessaging . sendmulticast ( message ); // see the batchresponse reference documentation // for the contents of response. log . info ( ""fcm send success count=[{}]"" , response . getsuccesscount ()); } } enter fullscreen mode exit fullscreen mode top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse nguyễn hữu hiếu follow location việt nam education mta work spmed joined nov 18, 2019 more from nguyễn hữu hiếu react use-zustand demo # react # reactnative # usezustand react-hook-form demo # react # reactnative # reacthookform react native + stream data: how to handle stream data like chatgpt? # reactnative # chatgpt # stream # javascript 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/chmich/setup-bootstrap-on-rails-7-and-vite-g5a,,,"setup bootstrap on rails-7 and vite - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse christian sedlmair posted on sep 20, 2022 • edited on sep 1, 2024 setup bootstrap on rails-7 and vite # rails # vite overview this guide may work, but as of 2024, we would prefer to build new projects without any style framework. css has become feature-rich enough. note we setup bootstrap scss files upon asset pipeline for avoide slowing down vite hmr. custom stylesheets can be added inside vite app/frontend/ . we setup one popover to make it working for checking if all the popper.js and bootstrapp javascript is working. ( alternative would be: minify the bootstrap css and add the minified version to the frontend folder. then, vite is clever enough for not loosing time for bundling that. on that way hmr would also rely fast and its cleaner because having only one asset pipeline) links bootstrap/getting-started/vite prerequesites vite jquery asset pipeline gem 'bootstrap', '~> 5.2.0' enter fullscreen mode exit fullscreen mode $ bundle install enter fullscreen mode exit fullscreen mode add app/assets/stylesheets/_settings.scss for configuring bootstrap sass variables rename app/assets/stylesheets/application.css to .scss if it isnt already done remove the lines //= require tree and //= require . if it isnt yet. @import ""settings"";
@import ""bootstrap""; enter fullscreen mode exit fullscreen mode we use only the scss files from the gem. on foundation i did the same by smylinking to the yarn package into the asset pipeline. the javascript we get from the yarn package: javascript packages $ npm i bootstrap @popperjs/core enter fullscreen mode exit fullscreen mode frontend/entrypoints/application.js // import 'bootstrap/js/src/alert'  
// import 'bootstrap/js/src/button'  
// import 'bootstrap/js/src/carousel'  
import 'bootstrap/js/src/collapse'  
import 'bootstrap/js/src/dropdown'  
// import 'bootstrap/js/src/modal'  
// import 'bootstrap/js/src/popover'  
import 'bootstrap/js/src/scrollspy'  
// import 'bootstrap/js/src/tab'  
// import 'bootstrap/js/src/toast'  
// import 'bootstrap/js/src/tooltip' 

import * as bootstrap from 'bootstrap';
window.bootstrap = bootstrap;

// initialize the page
window.addeventlistener('load', (event) => {
    initpage();
});
window.addeventlistener('turbo:render', (event) => {
    initpage();
});
function initpage() {
      // initialize popovers
  var popovertriggerlist = [].slice.call(document.queryselectorall('[data-bs-toggle=""popover""]'))
  var popoverlist = popovertriggerlist.map(function (popovertriggerel) {
    return new bootstrap.popover(popovertriggerel)
  }
} enter fullscreen mode exit fullscreen mode layout make sure the stylesheet_link_tag is present like described test view %button.btn.btn-lg.btn-danger{""data-bs-content"" => ""and here's some amazing content. it's very engaging. right?"", ""data-bs-title"" => ""popover title"", ""data-bs-toggle"" => ""popover"", :type => ""button""} click to toggle popover enter fullscreen mode exit fullscreen mode => if the popover works and looks like this, bootstrap styles are working, popper.js is working and popovers are initialized. yippie! overview p.s.: symlinking sass variables inside frontend/stylesheets/ , create a symlink like this: $ ln -s ../../assets/stylesheets/_settings.scss . enter fullscreen mode exit fullscreen mode then from all the stylesheets, by example frontend/stylesheets/components/button.scss , import it like this: @import '../settings'; enter fullscreen mode exit fullscreen mode symlinks to a file are well handled by git (otherwise than symlinks to a folder) and they should landing well on production machine. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse christian sedlmair follow rails developer location lucerne, switzerland work sedlmair.ch joined aug 29, 2022 more from christian sedlmair system testing # playwright # rails # javascript rails ujs on vite (⚠️ together with turbo?) # ujs # rails # vite setup turbo on vite # rails # vite # hotwire 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/mindplay/a-custom-date-picker-in-svelte-594o,,,"a custom date-picker in svelte - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse rasmus schultz posted on mar 3, 2022 a custom date-picker in svelte # svelte # showdev i have this fairly small, but fully functional, custom date-picker ui control, which i've been porting between ui frameworks for quite a few years. i ported this example to svelte about 3 years ago, and today i decided to update it to svelte 3. try it out 🚀 i always try this first with any new framework to see how they stack up. perhaps the most important thing demonstrated by this use case, is the ability to validate and conditionally accept a value into the control's internal date state - and the ability to modify the control's view state by navigating through months without changing the value. (it was quite a departure from the old version , in case you'd like to compare.) there's not a lot more to say about this. enjoy. 🙂 top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand renan franca renan franca renan franca follow • enjoy java☕ spring boot🍃 and oss 💯
• i love implementing automated tests😊
• @jhipster lite contributor🤓 and lover💙 • proud dad 🍼 location brazil education bachelor of computer science work software engineer joined jan 13, 2022 • mar 4 '22 dropdown menu copy link hide do you have this component in angularjs? like comment: like comment: 1 like like comment button reply collapse expand rasmus schultz rasmus schultz rasmus schultz follow passionate web-developer since 1998 using various languages, databases and tools. opinions all my own. joined dec 25, 2017 • mar 4 '22 dropdown menu copy link hide i did, many years ago - i'm afraid i haven't been very good at archiving these. i sort of want to go back and find or recreate them in different frameworks for comparison... that would be quite a lot of work though. i haven't always published these - a lot of times, it's just been something i do, as test for the framework, and as an exercise for myself. 🙂 like comment: like comment: 2 likes like comment button reply collapse expand renan franca renan franca renan franca follow • enjoy java☕ spring boot🍃 and oss 💯
• i love implementing automated tests😊
• @jhipster lite contributor🤓 and lover💙 • proud dad 🍼 location brazil education bachelor of computer science work software engineer joined jan 13, 2022 • mar 4 '22 dropdown menu copy link hide it's cool! thanks ☺️ like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse rasmus schultz follow passionate web-developer since 1998 using various languages, databases and tools. opinions all my own. joined dec 25, 2017 trending on dev community hot 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development meme monday # discuss # watercooler # jokes what was your win this week?! # weeklyretro # discuss 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/luizcalaca/nodejs-how-to-solve-routepost-requires-callback-functions-but-got-a-object-undefined-2h55,,,"node.js | how to solve: route.post() requires callback functions but got a [object undefined] - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse luiz calaça posted on jun 8, 2022 node.js | how to solve: route.post() requires callback functions but got a [object undefined] hi, devs! every day we need to solve the errors and bugs and usually they appears just all the time. let's observe examples of error messages in this context: error: route.get() requires a callback function but got a [object object] error: route.post() requires callback functions but got a [object undefined] what you need to do to solve? first thing: debug your code! if you are using vscode, use the debug tool at the left (a play button with a bug on it) if the message is coming with something undefined or object you need to looking for root cause. list of verifications to this error: 1 - you use the exports to permit other file to use you function? 2 - that function you call exists on its file? 3 - when you use module.exports take care to use correctly, remember if you use with module.exports = { functionexported } you need to use with a require in other file like const example = require('./example') and do example.functionexported to use. 4 - when you use routes and controllers to make yours endpoints , take care to exports the necessaries functions. do you have more tips and tricks to solve this error? post on comments. contacts email: luizcalaca@gmail.com instagram: https://www.instagram.com/luizcalaca linkedin: https://www.linkedin.com/in/luizcalaca/ twitter: https://twitter.com/luizcalaca top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand mohammed el badry mohammed el badry mohammed el badry follow joined nov 25, 2023 • aug 11 '24 dropdown menu copy link hide thank you for this blog. it helps me a lot! like comment: like comment: 2 likes like comment button reply collapse expand sagar chaurasia sagar chaurasia sagar chaurasia follow mern stack developer joined feb 27, 2024 • mar 28 '24 dropdown menu copy link hide thank you so much like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse luiz calaça follow backend specialist | node.js | aws | gcp | llm | openai api | javascript | typescript | python | tdd | software architecture |  docker | algorithms | speaker | technical writer | microservices location brazil education big data and machine learning | post graduated work software engineer joined jan 5, 2022 more from luiz calaça custom error handling in a rest api using typescript, express.js, joi validation, and object-oriented programming # typescript # javascript # tutorial # webdev good practices using node.js + sequelize with typescript # node # typescript # webdev # programming interesting middlewares in node.js and express.js api for security # node # security # javascript # webdev 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/vearutop/using-nginx-as-a-proxy-to-multiple-unix-sockets-3c7a,,,"using nginx as a proxy to multiple unix sockets - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse viacheslav poturaev posted on aug 8, 2022 using nginx as a proxy to multiple unix sockets # nginx # go # unixsocket tl;dr listening port may be a contended resource on a busy shared machine, unix sockets are virtually unlimited. nginx can expose them with a single port and prefixed urls. in some situations you may want to run many (instances of) applications on a single machine. each instance may need to provide internal information (e.g. prometheus /metrics , profiling/debug handlers) over restricted http. when number of instances grows it becomes a burden to provision listening ports without conflicts. in contrast, using unix sockets allows for more transparency (readable filenames) and scalability (easy to come up with unique name). here is a small demo program written in go that would serve trivial http service with unix socket. package main import ( ""context"" ""flag"" ""io/fs"" ""log"" ""net"" ""net/http"" ""os"" ""os/signal"" ) func main () { var socketpath string flag . stringvar ( & socketpath , ""socket"" , ""./soc1"" , ""path to unix socket."" ) flag . parse () if socketpath == """" { flag . usage () return } listener , err := net . listen ( ""unix"" , socketpath ) if err != nil { log . println ( err . error ()) return } // by default, unix socket would only be available to same user. // if we want access it from nginx, we need to loosen permissions. err = os . chmod ( socketpath , fs . modeperm ) if err != nil { log . println ( err ) return } httpserver := http . server { handler : http . handlerfunc ( func ( writer http . responsewriter , request * http . request ) { log . println ( request . url . string ()) if _ , err := writer . write ([] byte ( request . url . string ())); err != nil { log . println ( err . error ()) } }), } // setting up graceful shutdown to clean up unix socket. go func () { sigint := make ( chan os . signal , 1 ) signal . notify ( sigint , os . interrupt ) <- sigint if err := httpserver . shutdown ( context . background ()); err != nil { log . printf ( ""http server shutdown error: %v"" , err ) } }() log . printf ( ""service is listening on socket file %s"" , socketpath ) err = httpserver . serve ( listener ) if err != nil { log . println ( err . error ()) return } } enter fullscreen mode exit fullscreen mode now let's run a couple of instances in separate shells. ./soc -socket /home/ubuntu/soc1 enter fullscreen mode exit fullscreen mode ./soc -socket /home/ubuntu/soc2 enter fullscreen mode exit fullscreen mode here is a minimal nginx config to serve those instances with url prefixes. it would receive http://my-host/soc1/foo/bar , strip path prefix /soc1 and pass /foo/bar to soc1 . server {
    listen 80 default;

    location /soc1/ {
        proxy_pass http://soc1/;
    }
    location /soc2/ {
        proxy_pass http://soc2/;
    }
}

upstream soc1 {
    server unix:/home/ubuntu/soc1;
}

upstream soc2 {
    server unix:/home/ubuntu/soc2;
} enter fullscreen mode exit fullscreen mode every unix socket is defined as upstream and has /location statement in server . it is also possible to use unix sockets directly in /location , like in location /soc1/ {
        proxy_pass http://unix:/home/ubuntu/soc1;
    } enter fullscreen mode exit fullscreen mode however it has an unwanted limitation that you can not add trailing / to proxy_pass . and this means that url will be passed as is, e.g. soc1 will receive /soc1/foo instead of /foo . to avoid such limitation we can use named upstream and add trailing / to proxy_pass . location /soc1/ {
        proxy_pass http://soc1/; # mind trailing ""/"".
    } enter fullscreen mode exit fullscreen mode top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand azcraft azcraft azcraft follow a guy interested in programming, linux, tech stuff, gaming and pretty much everything. education currently going to university joined jan 10, 2022 • oct 12 '22 dropdown menu copy link hide thanks a lot, that was very useful to me. :d like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse viacheslav poturaev follow born in the snows of siberia, wandering around the world, seeking the gold. location berlin work staff engineer at adjust joined aug 8, 2019 more from viacheslav poturaev building a portable face recognition application with go and dlib # go # dlib # machinelearning # facerecognition streaming generated data as io.reader at high speed in go # benchmark # go using code coverage to debug large go application # go # testing 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/koladev/django-tip-use-decimalfield-for-money-3f63,,,"django tip: use decimalfield for money - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse mangabo kolawole posted on apr 6, 2022 • originally published at medium django tip: use decimalfield for money # django # decimal # float when working with money values in django, the first thought is to use floatfield to represent currency in the model. however, floatfield uses the float type internally which comes with some precision issues. the problem take a look at this piece of code. it's just a simple operation with float numbers in python. >>> . 1 + . 1 + . 1 == . 3 false >>> . 1 + . 1 + . 1 0.30000000000000004 enter fullscreen mode exit fullscreen mode normally, you think that the addition will make sense but because of some issues with the float approximation, the equation is not equal to 3. you can fix these issues using rounding but if you are dealing with money values and precision matters, it might be time to use decimals. the solution basically, use decimals instead of floats when precision matters. let's rewrite the previous example but with decimal . >>> from decimal import decimal >>> decimal ( ' .1 ' ) + decimal ( ' .1 ' ) + decimal ( ' .1 ' ) == decimal ( ' .3 ' ) true >>> decimal ( ' .1 ' ) + decimal ( ' .1 ' ) + decimal ( ' .1 ' ) decimal ( ' 0.3 ' ) enter fullscreen mode exit fullscreen mode notice that here we are initializing the decimals values with string values. you can use floats but as we said earlier, floats have their approximation issues. then when working with decimal in django with the decimalfield , it's always a good habit to precise the decimal_places attribute when defining the field. class product ( models . model ): title = models . charfield ( max_length = 64 ) description = models . textfield () price = models . decimalfield ( max_digits = 6 , decimal_places = 2 ) enter fullscreen mode exit fullscreen mode you can learn more about decimalfield here . article posted using bloggu.io . try it for free. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse mangabo kolawole follow software engineer | technical writer | book author location remote education studied cs work software engineer joined nov 30, 2019 more from mangabo kolawole why i still use django for my saas projects in 2025 # python # django # beginners # programming next.js & django jwt authentication: django rest, typescript, jwt, wretch & djoser # nextjs # react # javascript # django build api key authentication in your django rest application: django, django rest, simple api key & docker # python # django 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/contactsunny/installing-zsh-and-oh-my-zsh-on-windows-11-with-wsl2-1p5i,,,"installing zsh and oh-my-zsh on windows 11 with wsl2 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse sunny srinidhi posted on nov 6, 2021 installing zsh and oh-my-zsh on windows 11 with wsl2 # windows # ohmyzsh # zsh # wsl before we begin, you might ask, why am i writing on something this trivial? i sold off my old macbook pro because i’m super excited about the new m1 pro macbook pros. i have pre-ordered one of those and am waiting for it to come. till then, i’m left with my gaming pc which is an asus zephyrus. so i thought i’ll make the best of it till i get my mac. this will be series of posts in which i show to setup a windows 11 pc to work with big data tools and technologies, mostly hadoop. so let’s get into it. now that windows 11 is officially available to the public, and given that windows is the most popular operating system used by developers (according to stackoverflow, refer screenshot below), i thought it would be fun to install a new wsl system on my windows 11 pc and try to set it up for big data tools. but before i could do that, i had to install zsh and oh-my-zsh on it. i switched to using zsh over half a decade ago and haven’t used bash or any other shell ever. i just love how zsh look, the ability to theme it, and the community surrounding it. so, i started with zsh, and here is how you can install it too. source: stackoverflow installing wsl2 on windows 11 the process of installing wsl on windows 11 is not different than that on windows 10. and if you are upgrading from a windows 10 pc which already had wsl installed, you don’t even have to do anything extra. i already had mine setup on windows 10 with ubuntu 20.04 lts. so that was readily available for me. but i wanted to setup everything fresh just to see if anything has changed (spoiler, it hasn’t). so i installed debian this time. to install any distro of linux on windows 11, just open up the microsoft store and search for your favorite linux distro. in the screenshot below, you can see that if i search for ubuntu, i get multiple versions of ubuntu listed in the store. just select the version that you want to the install and click the get button. after the installation is complete, the shell should open up automatically to complete the installation. soon after the installation is complete, you’ll see the bash shell something like the screenshot below. as you can see, the prompt is pretty ugly, at least to me. i know i can customize it, but not to the extent of what’s already built into zsh. now that we have debian installed, let’s start updating the packages and installing zsh and oh-my-zsh. installing zsh and oh-my-zsh updating package list and upgrade packages before we start installing any packages, we first to have to update the package list so that we can then upgrade all the packages that we have already installed, and also fetch the references to the latest version of all the dependencies. this is pretty simple and will take just a few seconds. run the following command in the terminal to update the references: sudo apt update enter fullscreen mode exit fullscreen mode once you update the package list, if you find any updates available, most probably you will, you can run the following command to upgrade all packages already installed. but let me also mention that this step is optional. sudo apt upgrade enter fullscreen mode exit fullscreen mode installing dependencies now that we have updated the package list, we have to install the dependencies that we’ll need to install zsh and oh-my-zsh. the dependency list isn’t really exhaustive, it’s just two packages. if i’m not mistaken, these two packages must come pre-installed if you install a full fat version of linux, but because this is a wsl version, i think it’s pretty stripped down. anyway, the two packages that we need to install are wget and git. and just to be clear, these are not required for installing zsh, but for oh-my-zsh. and to be completely honest, you can technically install oh-my-zsh without these packages as well, but there are benefits if you do install this way. for instance, oh-my-zsh will automatically check for updates if you install it using git. to install these two packages, run the following two commands one after another in your terminal: sudo apt install wget sudo apt install git enter fullscreen mode exit fullscreen mode together, it shouldn’t take more than a minute to install. these are pretty small packages. once you’re done with these two, we can finally move on to installing zsh. installing zsh in some cases, zsh should already be installed even in the wsl version of ubuntu or debian. but i’m not 100% sure because i already had ubuntu installed on wsl, as i mentioned. but if it’s not installed, it’s just one little command: sudo apt install zsh enter fullscreen mode exit fullscreen mode and that’s it, you have zsh installed on your windows 11 pc using wsl. this excites me very much for some reason. but we’re not yet done. let’s install oh-my-zsh. installing oh-my-zsh if you don’t know what oh-my-zsh is, you can read all about it here. installing this is another simple command. but it’s not using the apt package manager, but we’ll use wget and git to basically download the install script from the git repo and run that on our machine. to install oh-my-zsh, run the following command in your terminal: sh -c "" $( wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -o - ) "" enter fullscreen mode exit fullscreen mode because it’s an open source package, you can just to the link in that command and look at what the script is doing yourself. the whole process should look similar to what you see in the screenshot below. soon after the install, you can see the prompt change in your terminal. the default theme is applied. you can explore on how to get more themes and change it or customize it to your heart’s content. adding useful aliases if you’re a developer and use git a lot for your projects, there are some commands that you’ll be typing everyday. this shouldn’t come as a surprise, but most developers have handy aliases for these commands to make life a bit easier in the terminal. to add aliases, you’ll have to edit the .zshrc file in your home directory. for this, run the following command to open up the file: vi ~/.zshrc enter fullscreen mode exit fullscreen mode this will open the file in the vi editor. scroll to the end of the file and the following lines: alias ll = ""ls -ltra"" alias gd = ""git diff"" alias gcmsg = ""git commit -m"" alias gitc = ""git checkout"" alias gitm = ""git checkout master"" enter fullscreen mode exit fullscreen mode as you can see, these are pretty simple aliases. but also, they reduce typing a lot everyday when you add up the number of keystrokes at the end of the day. so, that’s pretty much it. and if you like what you see here, or on my medium blog and personal blog , and would like to see more of such helpful technical posts in the future, consider supporting me on patreon and github . top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand jramirez857 jramirez857 jramirez857 follow education northeastern university work software engineer in test joined oct 13, 2021 • may 4 '22 dropdown menu copy link hide just used this as i got a new windows 11 laptop. thanks for the guide! 🙌🙌 like comment: like comment: 2 likes like comment button reply collapse expand prayag bhatt prayag bhatt prayag bhatt follow location vadodara, india pronouns he/him joined jun 9, 2023 • apr 13 '24 dropdown menu copy link hide really helpful. thanks for the guide 🙌🏻✨ like comment: like comment: 1 like like comment button reply collapse expand stephenlit stephenlit stephenlit follow joined mar 29, 2024 • jan 7 dropdown menu copy link hide for some reason i am getting a fatal: not a git repository error like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse sunny srinidhi follow coding, reading, machine learning, sleeping, listening, watching, potato. india. 

http://blog.contactsunny.com

http://patreon.com/contactsunny

http://tiny.cc/7er8iz location bangalore education master's in embedded systems work lead data engineer joined aug 1, 2017 trending on dev community hot to index or not to index: which coding agent to choose? # javascript # discuss # devops # ai 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development how tdz pro changed the game for external remote meetings # remote # productivity # meetings # tdzpro 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/arctype/understanding-sql-dialects-4fmh,,,"understanding sql dialects
 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse arctype team for arctype posted on nov 17, 2021 • originally published at arctype.com understanding sql dialects # sql there is no sql standard. well, there is but no database fully implements it. become familiar with the wide variety of sql dialects to get the most out of your queries on each system. ​ what's a sql dialect? ​sql has a long history, filled with anecdotes. before we learn about dialects we need to understand how sql itself evolved. if you're not too familiar with this history, you might want to check out the detailed article below that explains how sql came to be and its significance in data processing: sql is 50 years old. what can we learn from its success? derek xiao for arctype ・ jan 18 ・ 11 min read #sql #database #discuss #programming ​ the word ""dialect"" means a variant of a language. in the real world, there are around 160 dialects of the english language.  just as human languages evolved over time, sql has also changed over the year, spawning its own dialects. ​ dialects exist for their own special purposes. for example, the postgresql dialect pl/pgsql was created to incorporate postgresql-specific syntax, which might not make sense in other database systems. regardless of these dialects, many databases have common syntax due to the ansi sql specification. similar to jdbc, rest, and many others, ansi sql was created so that vendors (both open source and commercial) can co-operate in a meaningful way. ​ why are sql dialects important to understand? ​speaking from experience, it is rare to write sql that will perfectly work across many database platforms. there will be variations that exist among different databases. this makes it all the more important to learn dialects of one or more database systems, just like how learning more than one programming language helps us become better developers. ​ structure of an sql query ​the word ""structured"" is pretty evident in the name structured query language itself. the structured nature of sql is the reason it became so popular and widely adopted. sql can be roughly split into sections as shown below: ​ data query language. sql queries that fetch data from tables.
​ data definition language. sql queries that alter the structure of tables. ​ data manipulation language. sql queries that alter data present in tables. ​ data control language. sql to grant/revoke user permissions. ​ transaction control language. sql statements to control transactions. ​ a typical sql dql query consists of select , from and a where clause. there are optional order by , join , limit and group by clauses as well. most queries, even in nosql database systems, more or less follow the above structure. in addition to this, some database systems have something called ""procedural sql,"" which is a lot more powerful. when we are talking about dialects it means we are talking about the sql features as a whole, which will include sql + custom functions + procedural sql. ​ aside: from a language perspective, sql has a fairly simple grammar when compared to programming languages. nevertheless, you might want to do some more analysis on a query from time to time. if you are interested in parsing and understanding sql queries, check out the antlr tool. ​ different dialects in sql and examples ​terms we generally know already, like select , where , and order by , are just the basics that most software developers use in day-to-day development. let's take a look at how some of the most famous dialects handle these basics and some custom examples around them. ​ pl/pg sql pl/pg sql is the sql dialect that the postgresql database system uses. unlike normal sql, pl/pgsql is turing complete , i.e it has the power of a full programming language. a basic structure of a pl/pg sql module will look like the example below: most procedural code will be declared as functions and can be called from regular sql queries. here is an example that fetches a user id from the username, taken from the documentation itself : this function can then be called as follow: this is just one simple example. there is a world of possibilities that exists with these powerful functions. the full documentation/guide can be found here . ​ tsql tsql or transact sql is an sql dialect for the microsoft sql server database. instead of looking at a function example as we did for postgresql, we will take an example for a normal dql query. ​ let's assume that we have a table called sales and we want to get the top 10 sales by amount . a normal sql query for it would look like - this same query can be simplified a lot by using tsql syntax. we might think this is a small improvement but we can add more complexity to this by selecting the top 2% of the rows. you can explore more about the top query and other proprietary syntax offered by sql server in the documentation . tsql is also turing complete similar to pl/pg sql . there is even an example of a language interpreter being completely written in tsql , although the language itself is not as complicated as a full-blown programming language. ​ pl/sql pl/sql is oracle's proprietary database procedural language. it works very similar to other procedural languages. a base structure for this is as follows, let's take a look at an example where this procedure calculates users that have salaries higher than 10000 . we are declaring the variables and then selecting the values of them from the users table. this can then be wrapped into a function and then called in a manner similar to our pl/pg sql example. pl/sql also qualifies as a turing complete language and is popularly used in financial institutions, where entire business applications are written on top of it. ​ these are real examples, but you can syntactical differences in this great answer by prolay chaudhury on quora . now that you have a flavor of the different dialects, let's compare and contrast them. ​ which sql dialect should i learn? consider the following... the major differences and similarities between these dialects are listed below. ​ licensing postgresql is open source, while  two are proprietary. this can be a deciding factor when deciding on database systems. the majority of open-source software has extensive documentation and a broad user base, while proprietary software has better features and support. with that said, even open-source databases like postgresql have enterprise support and features. ​ developer ecosystem pl/pg sql tends to be polyglot in nature, i.e its users live in various programming language communities. on the other hand, tsql is heavily used by the microsoft developer community and, similarly, pl/sql developers typically inhabit the oracle developer community. these considerations tend to affect which dialect developers choose. ​ feature set many of these dialects have been in the industry for a long time. while syntax and user experience are subjective, the feature sets between these dialects/databases tend to overlap a lot. if you are deciding on this factor alone, then it is very hard to distinguish between the different dialects, since most support the vast majority of use cases. ​ performance there are no explicit performance differences between the dialects, but they can vary depending upon how optimized our code is and the amount of data. most of these functions get compiled to native c code and hence do not incur any cost as far as compilation is concerned. ​ lists of all sql dialects the examples given above are just some of the most famous ones. there are plenty of others that are used in database systems of their own. below we give more resources compiled by database orms like hibernate, sql alchemy, and microsoft entity framework. ​ since orms interact with the database systems and provide wrappers on top of sql, they need to understand these dialects from the ground up. ​ here is a list of dialects that hibernate supports. a list supported by sql alchemy. finally a list of sql dialects supported by microsoft entity framework.
​ the references above are not exhaustive. if you are working on a database platform that is specialized, then it might have a dialect of its own. be sure to look at the documentation of the database you are using. ​ a note: sql in the big data world when the nosql revolution began, a lot of developers were beating the drums on death to sql . but sql came back with even more force in the big data world. here are some examples of the sql dialects that exist with many big data systems. ​ apache hive takes in a specific sql dialect and converts it to map-reduce. spark sql , for use in spark big data processing. cql , for working with cassandra. ksql , for sql on stream processing.
​ and a lot more. i would highly encourage readers to go through this video on sql in the big data world by michael stonebraker : sql and its dialects are not just for relational databases. their use extends far beyond rdbms. ​ conclusion sql dialects are beasts of their own. learning an entire dialect could take years to master. but what we should keep in mind is that syntax basics are all the same. on this front, we should re-think how we learn sql, and how it is even taught: reimagining the sql curriculum: change how sql is taught arctype team for arctype ・ mar 23 ・ 9 min read #sql #beginners ​ additionally, the turing completeness of a language should not be taken at face value. some dialects could even replace traditional programming languages. there are plenty of situations where you might want to write business logic in database systems simply because it is a lot easier to maintain and execute. there are slight performance advantages as well since the data never leaves the database cluster and is executed without any network overhead. ​ sql is very powerful. hopefully, this blog has made you aware of the most famous dialects that exist and why they do. as database systems evolve, so do the complexity of our queries and, in turn, we'll continue to push the boundaries of the sql language and of syntax itself. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand gloria gloria gloria follow work sales manager joined nov 23, 2021 • nov 25 '21 dropdown menu copy link hide thank you for the information. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse arctype follow the collaborative sql editor for teams try our free, collaborative sql editor today download arctype more from arctype top 5 google cloud database services - part 1 (sql) # programming # sql # database # tutorial deriving ideal indexes for your sql database: a guide # programming # sql # tutorial # productivity the darker side of alter table: a guide # programming # sql # tutorial # productivity 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/kotlin/idiomatic-kotlin-solving-advent-of-code-puzzles-passport-validation-1425,,,"idiomatic kotlin: solving advent of code puzzles, passport validation - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse sebastian aigner for kotlin posted on sep 1, 2021 • originally published at blog.jetbrains.com idiomatic kotlin: solving advent of code puzzles, passport validation # kotlin # adventofcode # codenewbie # 100daysofcode today in “idiomatic kotlin”, we’re looking at day 4 of the advent of code 2020 challenges , in which we tackle a problem that feels as old as programming itself: input sanitization and validation. day 4. passport processing we need to build a passport scanner that, given a batch of input text, can count how many passports are valid. you can find the complete task description at https://adventofcode.com/2020/day/4 . like many challenges, we first inspect our input: ecl:gry pid:860033327 eyr:2020 hcl:#fffffd
byr:1937 iyr:2017 cid:147 hgt:183cm

iyr:2013 ecl:amb cid:350 eyr:2023 pid:028048884
hcl:#cfa07d byr:1929

hcl:#ae17e1 iyr:2013
eyr:2024
ecl:brn pid:760753108 byr:1931
hgt:179cm enter fullscreen mode exit fullscreen mode the input is a batch of travel documents in a text file, separated by blank lines. each passport is represented as a sequence of key-colon-value pairs separated by spaces or newlines. our challenge is finding out how many passports are valid. for part one, “valid” means that they need to have all the required fields outlined by the security personnel: byr , iyr , eyr , hgt , hcl , ecl and pid (we conveniently ignore their request to validate the cid field). solving day 4, part 1 like many challenges, we start by reading our puzzle input as text and trim off any extraneous whitespace at the beginning and the end of the file. as per the description, passports are always separated by blank lines. a blank line is just two “returns”, or newlines, in a row, so we’ll use this to split our input string into the individual passports: val passports = file ( ""src/day04/input.txt"" ) . readtext () . trim () . split ( ""\n\n"" , ""\r\n\r\n"" ) enter fullscreen mode exit fullscreen mode (note that depending on your operating system, the line separator in text files is different: on windows, it is \r\n , on linux and macos, it’s \n . kotlin’s split method takes an arbitrary number of delimiters, allowing us to cover both cases directly.) we now have a list of passport strings. however, working with lists of raw strings can quickly get confusing. let’s use kotlin’s expressive type system to improve the situation and encapsulate the string in a very basic passport class. class passport ( private val text : string ) { } enter fullscreen mode exit fullscreen mode we then just map the results of our split-up input to passport objects: // . . . . map { passport ( it ) } enter fullscreen mode exit fullscreen mode from the problem description, we remember that key-value pairs are either separated by spaces or newlines within a single passport. therefore, to get the individual pairs, we once again split our input. the delimiters, in this case, are either a space or one of the newline sequences. fun hasallrequiredfields (): boolean { val fieldswithvalues = text . split ( "" "" , ""\n"" , ""\r\n"" ) } enter fullscreen mode exit fullscreen mode we then extract the key from each passport entry. we can do so by mapping our combined fieldswithvalues to only the substring that comes before the colon: fun hasallrequiredfields (): boolean { val fieldswithvalues = text . split ( "" "" , ""\n"" , ""\r\n"" ) val fieldnames = fieldswithvalues . map { it . substringbefore ( "":"" ) } return fieldnames . containsall ( requiredfields ) } enter fullscreen mode exit fullscreen mode the result of our function will be whether the fieldnames we extracted contain all required fields. the requiredfields collection can be taken directly from the problem statement and translated into a list: private val requiredfields = listof ( ""byr"" , ""iyr"" , ""eyr"" , ""hgt"" , ""hcl"" , ""ecl"" , ""pid"" /*""cid""*/ ) enter fullscreen mode exit fullscreen mode to calculate our final number, and get our first gold star for the challenge, we need to count the passports for which our function hasallrequiredfields returns true: fun main () { println ( passports . count ( passport :: hasallrequiredfields )) } enter fullscreen mode exit fullscreen mode with that, we have successfully solved the first part of the challenge and can set our sights on the next star in our journey. find the full code for the first part of the challenge on github . solving day 4, part 2 in part two of the challenge, we also need to ensure that each field on the passport contains a valid value. we are given an additional list of rules to accomplish this task, which you can again find in the problem description . years need to fall into specific ranges, as does a person's height depending on the unit of measurement. colors need to come from a prespecified list or follow certain patterns, and numbers must be correctly formatted. a refactoring excursion before we start building the solution for part 2, let’s briefly reflect on our code and find possible changes that will make adding this functionality easier for us. at this point in the challenge, we know that our passport class will need access to the different field names and their associated values. the classical data structure to store such kind of associative-dictionary information is a map. let’s refactor our code to store passport information in a map instead of a string. because turning an input string into a map is still a process that’s associated with the passport , i like encapsulating such logic in a companion object “factory” function. in this case, we can aptly call it fromstring : companion object { fun fromstring ( s : string ): passport { enter fullscreen mode exit fullscreen mode the implementation for fromstring partially reuses the normalization logic we had previously used in the first part of this challenge and expands it to create a map directly via kotlin’s associate function: fun fromstring ( s : string ): passport { val fieldsandvalues = s . split ( "" "" , ""\n"" , ""\r\n"" ) val map = fieldsandvalues . associate { val ( key , value ) = it . split ( "":"" ) key to value } return passport ( map ) } enter fullscreen mode exit fullscreen mode a passport object now encapsulates a map of string keys and string values: class passport ( private val map : map < string , string >) { enter fullscreen mode exit fullscreen mode interestingly enough, this change makes the implementation of the first part of our challenge trivial. we can simply check that the key set of our map contains all required fields: fun hasallrequiredfields () = map . keys . containsall ( requiredfields ) enter fullscreen mode exit fullscreen mode returning to solving part 2 for the second part of the challenge, we consider a passport valid if it contains all the required fields and has values that correspond to the official rules. to ensure that all fields have valid values, we can use the all function to assert that a predicate holds true for every single key-value pair in our map: fun hasvalidvalues (): boolean = map . all { ( key , value ) -> enter fullscreen mode exit fullscreen mode we can distinguish the different types of fields using a when expression. in this first step, we distinguish the different cases based on the keys in our map: when ( key ) { enter fullscreen mode exit fullscreen mode each key we know gets a branch in this when statement. they all need to return a boolean value – true if the field is okay, false if the field violates the rules. the surrounding all predicate will then use those results to determine whether the passport as a whole is valid. the byr (birth year), iyr (issue year), and eyr (expiration year) fields all require their value to be a 4-digit number falling into a particular range: ""byr"" -> value . length == 4 && value . tointornull () in 1920 .. 2002 ""iyr"" -> value . length == 4 && value . tointornull () in 2010 .. 2020 ""eyr"" -> value . length == 4 && value . tointornull () in 2020 .. 2030 enter fullscreen mode exit fullscreen mode note that our combined use of tointornull together with the infix function in allows us to discard any non-numeric values, and ensure that they fall in the correct range. we can apply a very similar rule to the pid (passport id) field. we ensure that the length of the value is correct and ensure that all characters belong to the set of digits: ""pid"" -> value . length == 9 && value . all ( char :: isdigit ) enter fullscreen mode exit fullscreen mode validating ecl (eye color) just requires us to check whether the input is in a certain set of values, similar to the first part of our challenge: ""ecl"" -> value in setof ( ""amb"" , ""blu"" , ""brn"" , ""gry"" , ""grn"" , ""hzl"" , ""oth"" ) enter fullscreen mode exit fullscreen mode at this point, we have two more fields to validate: hgt (height) and hcl (hair color). both of them are a bit more tricky. let’s look at the hgt field first. the hgt (height) field can contain a measurement either in centimeters or inches. depending on the unit used, different values are allowed. thankfully, both “cm” and “in” are two-character suffixes. this means we can again use kotlin’s when function, grab the last two characters in the field value and differentiate the validation logic for centimeters and inches. like our other number-validation logic, we parse the integer and check whether it belongs to a specific range. to do so, we also remove the unit suffix: ""hgt"" -> when ( value . takelast ( 2 )) { ""cm"" -> value . removesuffix ( ""cm"" ). tointornull () in 150 .. 193 ""in"" -> value . removesuffix ( ""in"" ). tointornull () in 59 .. 76 else -> false } enter fullscreen mode exit fullscreen mode the last field to validate is hcl (hair color), which expects a # followed by exactly six hexadecimal digits – digits from 0 through 9 , and a through f . while kotlin can parse base-16 numbers, we can use this case to show off the sledgehammer method for validating patterns – regular expressions. those can be defined as kotlin strings and converted using the toregex function. triple-quoted strings can help with escape characters: ""hcl"" -> value matches """"""#[0-9a-f]{6}"""""" . toregex () enter fullscreen mode exit fullscreen mode our hand-crafted pattern matches exactly one hashtag, then six characters from the group of 0 through 9 , a through f . as a short aside for performance enthusiasts: toregex is a relatively expensive function, so it may be worth moving this function call into a constant. the same also applies to the set used in the validation for ecl – currently, it is initialized on each test. because the whole when -block is used as an expression, we need to ensure that all possible branches are covered. in our case, that just means adding an else branch, which simply returns true – just because a passport has a field we don’t know about doesn’t mean it can’t still be valid. else -> true enter fullscreen mode exit fullscreen mode with that, we have covered every rule outlined to us by the problem statement. to get our reward, we can now just count the passports that contain all required fields and have valid values: fun parttwo () { println ( passports . count { it . hasallrequiredfields () && it . hasvalidvalues () }) } enter fullscreen mode exit fullscreen mode we end up with a resulting number, which we can exchange for the second star. we’re clear for boarding our virtual flight. though this was probably not the last challenge that awaits us… find the complete solution for the second part of the challenge on github . conclusion for today’s challenge, we came up with an elegant solution to validate specific string information, which we extracted using utility functions offered by the kotlin standard library. as the challenge continued, we reflected on our code, identified more fitting data structures, and changed our logic to accommodate it. using kotlin’s when statement, we were able to keep the validation logic concise and all in one place. we saw multiple different ways of how to validate input – working with ranges, checking set membership, or matching a particular regular expression, for example. many real-world applications have similar requirements for input validation. hopefully, some of the tips and tricks you’ve seen in the context of this little challenge will also be helpful when you need to write some validation logic on your own. to continue puzzling yourself, check out adventofcode.com , whose organizers kindly permitted us to use their problem statements for this series. if you want to see more solutions for advent of code challenges in the form of videos, subscribe to our youtube channel and hit the bell to get notified when we continue our idiomatic journey. more puzzle solutions are coming your way! top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse kotlin follow a modern programming language that makes developers happier. content by the kotlin developer advocates, kotlin team members, and friends. 🎥 find more exciting kotlin content on our youtube channel ⬇️ subscribe on youtube more from kotlin idiomatic kotlin: solving advent of code puzzles, binary boarding # kotlin # adventofcode # codenewbie # 100daysofcode idiomatic kotlin: solving advent of code puzzles, day 2 # kotlin # adventofcode # codenewbie # 100daysofcode solving advent of code puzzles in idiomatic kotlin # kotlin # adventofcode # codenewbie # 100daysofcode 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/tuanlc/mongoerror-e11000-duplicate-key-error-collection-587l,,,"mongoerror: e11000 duplicate key error collection ?? - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse lê công tuấn posted on jul 30, 2021 • originally published at congtuanle.medium mongoerror: e11000 duplicate key error collection ?? # database # mongodb # experiencelearning # wfh tl;dr i've encountered the issue sometime in my developer path. so i decided to write down my experience and to firstly note to myself and secondly to help other developers/engineers who are a newbie in this topic. if you only need the fix, you can skip to the last part of this article! one of the most advantages of using nosql databases is the flexible feature that allows us to update the number of fields and their data types any time we want. i'm using nodejs and mongoose driver to connect to mongodb. and in the very beginning phase of a project development life and this help us to able to update collections and their fields. however, i encountered an error. mongoerror: e11000 duplicate key error collection: companies index: code_1 dup key: { code: null } enter fullscreen mode exit fullscreen mode what happened here? to understand, i would go further a bit with indexing and unique keys in databases. database indexes wikipedia defines: a database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure. indexes are used to quickly locate data without having to search every row in a database table every time a database table is accessed. indexes can be created using one or more columns of a database table, providing the basis for both rapid random lookups and efficient access of ordered records. if you want to query a field or set of fields without iterating all the entries in a table/collection, you can create indexes for these fields. to understand more how indexes are created & organized to support efficient queries, you can check at this however, the cost for indexes is not cheap, especially in the world of big data nowadays. because to make queries on indexed fields efficient, these indexes need to be stored in a fast query memory (for example ram). so, be careful when you want to add an index for a field, some factors should be put on the table to have good enough decisions: are data queried frequently? the user behaviors? regions that data are stored? etc and last but not least, an indexed field may be a non-unique value for each entry in the column. for example, you can have an index on the field ""region"" where we can have multiple users in the same region. unique keys in the real-life, there are use cases that we want to limit the appearance of one or a set of values of factors. for example, you want there is only 1 email that is used to register per user, no more. so, unique keys help you to achieve this constrain by defining the rule in the schema. for example in mongoose: const userschema = new schema ( { email : { type : string , index : true , unique : true , }, name : string , } ); enter fullscreen mode exit fullscreen mode you can see that the email attribute is unique. with this, you cannot add more than one user with the same email. if developers violate the rule, mongodb will throw errors. this helps us preventing developer mistakes. again, you can check more at wikipedia when mongo db schema become out of date with mongoose schema come back to the beginning error, let's me show you the mongoose schema that was defined by code: const companyschema = new schema ( { location : string ; name : string , } ); enter fullscreen mode exit fullscreen mode look good! right? but what makes the error? mongoerror: e11000 duplicate key error collection: companies index: code_1 dup key: { code: null } enter fullscreen mode exit fullscreen mode let's check a bit: i used mongo client by querying with the command line to check the existing keys for the companies collection. and the result was: > db.companies.getindexes () [ { ""v"" : 2, ""key"" : { ""_id"" : 1 } , ""name"" : ""_id_"" , ""ns"" : ""project-name.companies"" } , { ""v"" : 2, ""unique"" : true , ""key"" : { ""code"" : 1 } , ""name"" : ""code_1"" , ""ns"" : ""project-name.companies"" , ""background"" : true } ] enter fullscreen mode exit fullscreen mode yeah! there is an established index field for code and it is set to unique. and once an index is set, it is there until you remove it, and the rule unique is still there also. and the reason is the schema was modified due to the product business has changed. the previous schema was: const companyschema = new schema ( { code : { type : string , index : true , unique : true , }, location : string ; name : string , } ); enter fullscreen mode exit fullscreen mode so, this is a case when the mongo schema becomes out date with the mongoose schema that you defined in code. to fix this, i need to remove manually the unnecessary index key. mongo query provides some methods to remove indexes manually: note: you cannot drop the default index on the _id field. db.collection.dropindex() > db.companies.dropindex ( ""code_1"" ) enter fullscreen mode exit fullscreen mode db.collection.dropindexes this method will drop all non-_id indexes > db.companies.dropindexes () enter fullscreen mode exit fullscreen mode you can verify again by using the commandline: db.companies.getindexes() and it works! your comments & discussion are warmly welcomed! enjoy your job! top comments (4) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand binh2019 binh2019 binh2019 follow joined jul 31, 2021 • jul 31 '21 dropdown menu copy link hide it is useful. like comment: like comment: 2 likes like comment button reply collapse expand đồng phục hải triều đồng phục hải triều đồng phục hải triều follow thuong hieu san xuat dong phuc ao thun hang dau tai saigon email dongphuchaitrieu@gmail.com location saigon work founder at hai trieu joined sep 15, 2021 • nov 28 '21 dropdown menu copy link hide really helpful, thank bro! like comment: like comment: 2 likes like comment button reply collapse expand yasiramus yasiramus yasiramus follow an experienced web developer on a continuous journey of exploration and mastery in web technologies. dedicated to sharing insights and expertise through insightful posts, empowering fellow location accra, ghana. education developers in vogue pronouns her/she work a software engineer joined dec 25, 2021 • jan 11 '22 dropdown menu copy link hide thank you this was really helpfully like comment: like comment: 2 likes like comment button reply collapse expand asadul haq muhammad shani asadul haq muhammad shani asadul haq muhammad shani follow joined jul 29, 2022 • jul 29 '22 dropdown menu copy link hide thanks! super useful like comment: like comment: like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse lê công tuấn follow i'm interesting in software architecture and distributed system location hanoi, vietnam work technical leader at linagora vietnam joined may 9, 2019 trending on dev community hot what was your win this week?! # weeklyretro # discuss rediscovering my passion: from burnout back to excitement # devjournal # developer # career # leadership how to integrate a distributed cache for payment lookups # payment # distributedsystems # redis # database 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/saulodias/working-with-ng-content-on-storybook-4o3a,,,"working with ng-content on storybook - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse saulo dias posted on sep 7, 2021 working with ng-content on storybook # storybook # angular # ngcontent # template very often when creating a new story for an angular component on storybook you might need to insert content into components which have an ng-content area inside them. to do that you need to create a template for your story. here is a simple component, which has a div with and an ng-content area inside it. the component has two inputs, width and height . // paper.component.ts import { component , input } from ' @angular/core ' ; @ component ({ selector : ' cx-paper ' , template : `
    <div class=""paper"" [ngstyle]=""{ width: width, height: height }"">
      <ng-content></ng-content>
    </div>
  ` , styles : [ `
      .paper {
        border: navy solid 2px;
        padding: 10px;
      }
    ` , ], }) export class papercomponent { @ input () width : string ; @ input () height : string ; } enter fullscreen mode exit fullscreen mode the story for this component // paper.stories.ts import { story , meta } from ' @storybook/angular ' ; import { papercomponent } from ' ./paper.component ' ; export default { title : ' example/paper ' , component : papercomponent , } as meta ; const template : story < papercomponent > = ( args : papercomponent ) => ({ props : args , template : `
  <cx-paper [height]=""height"" [width]=""width"">
  this is a template test.
  </cx-paper>` , }); export const simpleexample = template . bind ({}); simpleexample . args = { height : ' 50px ' , width : ' 300px ' , } as partial < papercomponent > ; enter fullscreen mode exit fullscreen mode which should render like this: top comments (4) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand antonio sarcevic antonio sarcevic antonio sarcevic follow joined jun 10, 2021 • sep 23 '21 dropdown menu copy link hide now this works for writing stories but it doesn't work right with the argstable and doesn't show any output actions for me at least :z like comment: like comment: 1 like like comment button reply collapse expand saulo dias saulo dias saulo dias follow software developer graduated in automation and control engineering trying way too hard to become a brogrammer. location rio de janeiro, brazil education automation and control engineering at cefet/rj work solutions developer joined apr 29, 2021 • sep 26 '21 • edited on sep 26 • edited dropdown menu copy link hide if by argstable you mean the component inputs, it's working fine for me. i change the inputs and can see the component respond to those changes. i haven't been able to figure out how to make actions work with a template like that yet. like comment: like comment: 1 like like comment button reply collapse expand antonio sarcevic antonio sarcevic antonio sarcevic follow joined jun 10, 2021 • sep 27 '21 dropdown menu copy link hide argtable works in the story view but doesn’t in the docs view / docs page. if this actually works someday that would be very nice but right now storybook for angular is such a pain :z like comment: like comment: 3 likes like comment button reply collapse expand carlos aiello carlos aiello carlos aiello follow joined nov 16, 2021 • nov 16 '21 • edited on nov 16 • edited dropdown menu copy link hide why the show code part is not showing the content? this text is missing in the code: ""this is a template test."" like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse saulo dias follow software developer graduated in automation and control engineering trying way too hard to become a brogrammer. location rio de janeiro, brazil education automation and control engineering at cefet/rj work solutions developer joined apr 29, 2021 more from saulo dias browser storage and behaviorsubject: achieving consistent state in angular through storage decorators # angular # decorators # rxjs # behaviorsubject angular: the power of async pipe + observables. # angular # observables # rxjs # async string patterns in typescript # typescript # regex # string # angular 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/rdentato/utf-8-strings-in-c-2-3-3kp1,,,"utf-8 strings in c (2/3) - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse remo dentato posted on aug 4, 2021 • edited on oct 15, 2022 utf-8 strings in c (2/3) # c # utf8 # encoding to c and beyond (15 part series) 1 exceptional c 2 default parameters in c ... 11 more parts... 3 coroutines in c (1/3) 4 coroutines in c (2/3) 5 coroutines in c (3/3) 6 utf-8 strings in c (1/3) 7 utf-8 strings in c (2/3) 8 utf-8 strings in c (3/3) 9 (c) memories 10 express yourself (in c) 11 easy command line interfaces in c 12 what's in a c (nan)box? 13 a simple unit test framework for c 14 polymorphic c (1/2) 15 polymorphic c (2/2) validation an ironclad rule we should always follow is that: any utf-8 string must be validated before being used. i won't delve into the reasons why but they are very important. so, when in the previous post i said that "" you may handle a utf-8 encoded string as if it was not encoded "" (i.e. skipping validation), what i really meant is that you can do it only if you are 100% sure that the string has already been checked or it will be at a later time before being used in any meaningful way (including storing it in a db). if in doubt, you have to validate the string yourself. looking at table on the previous post (or on wikipedia ), it's clear that not every sequence of bytes is a valid utf-8 encoding. for example the sequence of bytes c2 41 is clearly invalid because the second byte of a multibyte utf-8 encoding must be greater that 0x7f. there are many possible ways to validate utf-8 encodings. there are extremely fast, parallel algorithms but our objective here is to understand how how validation works and let's get ready to deal with it. get characters the most basic operation we do on a sequence of characters is to get the next one. what we need is a function that takes a string, checks if the first bytes are a valid utf-8 encoding and, in case, returns the corresponding character plus the number of bytes the encoding spans. we also need to decide what to do with invalid encodings. the unicode standard reccomends that any invalid encoding should be replaced by the character u+fffd : � (a losenge containing a question mark) but, as c programmers, it seems odd to me to return something that is completely unrelated with the string or the error. let's decide that if we find an invalid sequence of bytes, we'll return the first one (i.e. a length of 1) and will set errno to einval to signal the error. character representation for the internal representation of characters, we have two choices: decode the codepoint and use the unicode value in our program directly use the encoded form the first one seems more logical but if you think about it, it's often a waste of time. consider comparing two strings, for example. comparing the encoded form is exactly the same as comparing the codepoint values but it saves the decoding step for the two strings. on the other hand, the second one is less user-friendly. if you want to use constants, you have to manually encode the codepoint value and use it. however, i claim that referring to ""ジ"" as 30b8 (the unicode codepoint) or as e382b8 (its utf-8 encoding) makes not such a big difference. any valid utf-8 encoding will fit into 4 bytes so let's define a new type u8chr_t : typedef uint32_t u8chr_t ; enter fullscreen mode exit fullscreen mode encoding length to determine the length of the encoding looking at its first byte we can define an array and a macro that uses the upper 4 bits of the byte as an index in the array: static uint8_t const u8_length [] = { // 0 1 2 3 4 5 6 7 8 9 a b c d e f 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 2 , 2 , 3 , 4 } ; #define u8length(s) u8_length[(((uint8_t *)(s))[0] & 0xff) >> 4]; enter fullscreen mode exit fullscreen mode the macro will tell us the length of the encoding of the first character. here are some examples: u8length ( ""abc"" ) -> 1 ( 'a' is "" \x41 "" ) u8length ( ""èfg"" ) -> 2 ( 'è' is "" \xc3\xa8 "" ) u8length ( "" \x82 hz"" ) -> 0 ( invalid ) u8length ( "" \xc3 "" ) -> 2 ( invalid , but we don ' t know yet ) enter fullscreen mode exit fullscreen mode note that the last two ones are invalid encoding. in the first case we can immediately say that it's a stray byte. in the second one we will have to look at the next bytes to recognize that the encoding is invalid. loading the encoding now that we have the expected length, we can load the encoding one byte at the time ( txt is the pointer to the string): u8chr_t encoding = 0 int len = u8length ( txt ); for ( int i = 0 ; i < len && txt [ i ] != '\0' ; i ++ ) { encoding = ( encoding << 8 ) | txt [ i ]; } enter fullscreen mode exit fullscreen mode note that the code does not read past the end of the string if there are less byte than expected. if that happens, we exit from the loop and the encoding variable will keep what has been found so far. validate! and now the core part: let's define a function to check if the loaded encoding is a valid one: int u8chrisvalid ( u8chr_t c ) { if ( c <= 0x7f ) return 1 ; // [1] if ( 0xc280 <= c && c <= 0xdfbf ) // [2] return (( c & 0xe0c0 ) == 0xc080 ); if ( 0xeda080 <= c && c <= 0xedbfbf ) // [3] return 0 ; // reject utf-16 surrogates if ( 0xe0a080 <= c && c <= 0xefbfbf ) // [4] return (( c & 0xf0c0c0 ) == 0xe08080 ); if ( 0xf0908080 <= c && c <= 0xf48fbfbf ) // [5] return (( c & 0xf8c0c0c0 ) == 0xf0808080 ); return 0 ; } enter fullscreen mode exit fullscreen mode the function has just five if 's: [1] check for ascii values. [3] exclude the utf-16 surrogates. the other three look at both the values and the bit pattern to ensure it's a valid encoding. let's see in detail the check [2] (the other works in the same way). this covers the range u+0080 - u+07ff (eleven bits) which is represented with two bytes: 110xxxxx 10xxxxxx . just replacing the bits, it's easy to see that the encoding for u+0080 is 0xc280 ( 1100 0010  1000 0000 ) and the encoding for u+07ff is 0xdfbf ( 1101 1111  1011 1111 ). any encoding lower than 0xc280 must be invalid (it comes from a non minimal encoding!), any encoding higher than 0xdfbf will be further verified by the other if's. however, not all the values in that range are valid: consider the value '0xcaf3' which is 1100 1010  1111 0011 in binary; the second byte is not in the form 10xx xxxx . masking the encoding with '0xe0c0' will reveal if the bits are what they are supposed to be: '0xc080'. if it seems confusing, just try with a couple of codepoints. i'm sure it will become clearer in no time. all the other if's work in the same way. i've written a more detalaid description on stack overflow as a self response. i wanted to be sure that this method was correct before using it and i tapped on the so collective wisdom. all togheter now now we can complete our initial function that will return the length of the next character in a string: int u8next ( char * txt , u8chr_t * ch ) { int len ; u8chr_t encoding = 0 ; len = u8length ( txt ); for ( int i = 0 ; i < len && txt [ i ] != '\0' ; i ++ ) { encoding = ( encoding << 8 ) | txt [ i ]; } errno = 0 ; if ( len == 0 || ! u8chrisvalid ( encoding )) { encoding = txt [ 0 ]; len = 1 ; errno = einval ; } if ( ch ) * ch = encoding ; return encoding ? len : 0 ; } enter fullscreen mode exit fullscreen mode note that at the end we added another check: if the character is ' \0 ' the length is also 0. in this way you can add the return value to the current pointer into the string and be sure that you will not past the end of the string. decode/encode decoding and encoding characters is just a matter of masking the bits of interest and shifting them in an integer. to understand what's going on in this functions, you should keep the table with the structure of utf-8 encoding in front of you. // from utf-8 encoding to unicode codepoint uint32_t u8decode ( u8chr_t c ) { uint32_t mask ; if ( c > 0x7f ) { mask = ( c <= 0x00efbfbf ) ? 0x000f0000 : 0x003f0000 ; c = (( c & 0x07000000 ) >> 6 ) | (( c & mask ) >> 4 ) | (( c & 0x00003f00 ) >> 2 ) | ( c & 0x0000003f ); } return c ; } enter fullscreen mode exit fullscreen mode // from unicode codepoint to utf-8 encoding u8chr_t u8encode ( uint32_t codepoint ) { u8chr_t c = codepoint ; if ( codepoint > 0x7f ) { c = ( codepoint & 0x000003f ) | ( codepoint & 0x0000fc0 ) << 2 | ( codepoint & 0x003f000 ) << 4 | ( codepoint & 0x01c0000 ) << 6 ; if ( codepoint < 0x0000800 ) c |= 0x0000c080 ; else if ( codepoint < 0x0010000 ) c |= 0x00e08080 ; else c |= 0xf0808080 ; } return c ; } enter fullscreen mode exit fullscreen mode conclusion encoding, decoding and validation may seem more complicated than they really are; some bit manipulation can do the trick very easily. nothing really new. what is a little bit more complicated is to extend the usual functions on characters. how can we get functions euivalent to isdigit() or tolower() ? except for the most trivial tasks we need those functions and this  will the the topic of next post . to c and beyond (15 part series) 1 exceptional c 2 default parameters in c ... 11 more parts... 3 coroutines in c (1/3) 4 coroutines in c (2/3) 5 coroutines in c (3/3) 6 utf-8 strings in c (1/3) 7 utf-8 strings in c (2/3) 8 utf-8 strings in c (3/3) 9 (c) memories 10 express yourself (in c) 11 easy command line interfaces in c 12 what's in a c (nan)box? 13 a simple unit test framework for c 14 polymorphic c (1/2) 15 polymorphic c (2/2) top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand ross mohn ross mohn ross mohn follow joined jul 3, 2023 • jul 3 '23 dropdown menu copy link hide thanks for this article. i'm now using parts of this code in my a4 terminal multiplexer project. one note, in the ""loading the encoding"" for() loop i had to or each character with 0xff. not sure if this is universal or just my code. for (int i=0; i<len && txt[i] != '\0'; i++) {
    encoding = (encoding << 8) | (txt[i] | 0xff);
} enter fullscreen mode exit fullscreen mode like comment: like comment: like comment button reply collapse expand remo dentato remo dentato remo dentato follow i'm an old-time programmer for work and for fun. c is my favorite color. temporarily hooked by ai. i love to share thoughts on programming that go beyond the basics. location italy work dev joined dec 11, 2019 • jul 20 '23 • edited on jul 20 • edited dropdown menu copy link hide thanks ross. happy to know it has been useful! if txt is a char or unsigned char there should be no need for the and (i saw your code has, correclty, a & ), but there are architecture where a char is larger than 8 bits, so doing it will be on the safe side. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse remo dentato follow i'm an old-time programmer for work and for fun. c is my favorite color. temporarily hooked by ai. i love to share thoughts on programming that go beyond the basics. location italy work dev joined dec 11, 2019 more from remo dentato polymorphic c (2/2) # api # c # programming polymorphic c (1/2) # c # programming # api a simple unit test framework for c # c # unittest # testing 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/ippatev/how-to-fix-problems-with-esbuild-in-yarn-57hh,,,"how to fix problems with esbuild in yarn - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse aleksandr ippatev posted on aug 10, 2021 how to fix problems with esbuild in yarn # yarn # npm # esbuild # javascript this plugin lets you use yarn with esbuild. we use it in order to build yarn itself! add the plugin to your dependencies: yarn add @yarnpkg/esbuild-plugin-pnp enter fullscreen mode exit fullscreen mode reference it via your esbuild configuration build api only : import {pnpplugin} from '@yarnpkg/esbuild-plugin-pnp';

await build({
  plugins: [pnpplugin()],
  // ...
}); enter fullscreen mode exit fullscreen mode top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand alexander van trijffel alexander van trijffel alexander van trijffel follow joined aug 22, 2021 • aug 22 '21 dropdown menu copy link hide can you share your full esbuild.config.js please? i am trying to get this working with yarn 2. but with yarn 2 pnp, i cannot run node esbuild.config.js in my project because packages resolving does not work when they are not present in node_modules. when i run node esbuild.config.mjs i get error error [err_module_not_found]: cannot find package 'esbuild' imported from snip /esbuild.config.mjs like comment: like comment: 1 like like comment button reply collapse expand alexander van trijffel alexander van trijffel alexander van trijffel follow joined aug 22, 2021 • aug 22 '21 dropdown menu copy link hide never mind, i found the solution. the build must be started with yarn node esbuild.config.js so that dependencies are resolved using .pnp.cjs if yarn is configured to use pnp. like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse aleksandr ippatev follow joined jun 19, 2020 more from aleksandr ippatev [boost] # javascript # programming # webdev # advanced [boost] # microservices # webdev # javascript # career how do revert all local changes in git? # git # github # gitlab # javascript 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/iggredible/the-easy-way-to-copy-text-in-tmux-319g,,,"the easy way to copy text in tmux - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse igor irianto posted on nov 22, 2021 the easy way to copy text in tmux # tmux # vim # vi # tutorial how to easily copy texts in tmux if you've used tmux for a while, you will realize that highlighting a text with a mouse in tmux to copy them is a painful experience. that's because it's now how tmux is designed. having used text editing softwares like microsoft word / google docs and to some extent, atom / vscode / intellij, it may feel natural to grab your mouse, highlight the body of texts, and press ctrl + c. not so with tmux. copying with tmux is more like copying in vim, where it's best done with a keyboard rather than a mouse. this is done in tmux copy mode .  in this article, i will show you how to utilize tmux copy mode to easily copy text. vi copy mode tmux is similar to vim in many ways. one of them is that they are modal programs. what that means is that to accomplish a specific task, you need to switch to a mode that's optimized for that task. in vim, we have a normal mode where you can move around with your keyboard. vim's normal mode is designed for moving (and to a certain extent, modifying) within your file. if you need to insert texts, you need to switch to vim insert mode. to navigate your terminal history and to copy text, you need to switch to tmux copy mode (technically it is vi-copy-mode, but to keep it simple, let's call it copy mode). so what exactly is a copy mode? if you try to copy a text from a tmux window with your mouse, you may have seen a warning saying that you can't simply copy the text. the correct way to copy a text from tmux is to first switch to copy mode, then copy it, then paste it. to enable this copy mode, in you tmux config file, add: setw -g mode-keys vi enter fullscreen mode exit fullscreen mode then save and source the tmux config. this will enable vi mode-keys . if you are unsure about tmux config, check out my previous article on tmux config. to enter the copy mode, use ctrl + [ . you should be able to see a yellow visual indicator on the top right of your tmux window. with this, you can now move around using vim navigation keys. although tmux copy mode doesn't translate to 100% vim navigation keys - overall they are good enough to feel natural. some navigation keys that you can use: h / j / k / l to move left/down/up/right w / w / e / e / b / b to move forward to the start / to the end / backward a word or word { / } to move backward / forward a paragraph ctrl + e / d / f to scroll down a line / half screen / whole screen ctrl + y / u / b to scroll up a line / half screen / whole screen / / ? to search forward / backward n / n to repeat the previous search forward / backward etc there are a lot more navigation keys. check out inside man tmux and search for the mode-keys keyword. in this mode, you can move around the terminal like you move around vim. to exit this mode, press q or enter . to begin highlighting, press space . note that your cursor will highlight the letter under it. it will expand the highlight as you move around your cursor. once you highlight all the text you want to copy, press enter . you should automatically exit the copy mode. the text that you copied are stored inside a tmux buffer. it is an allocated space in the memory where tmux temporarily stores data. to paste from the tmux buffer, press ctrl + ] . woot! congratulations, you've copied a text with tmux - stress-free! to  recap: add setw -g mode-keys vi in your tmux config ctrl + [ to enter tmux vi copy mode move around with vim navigation keys begin highlight with space copy with enter paste with ctrl + ] making copy-paste more intuitive i find a number of the copy-pasting keys unintuitive. let's modify them to mimic vim keybindings. recall that vim has a visual mode ( v ) that allows you to highlight a body of text. you can also ""yank"" text with y . finally, you can paste the yanked text with p or p . let's use similar keymaps. add these lines in the tmux config: bind -t copy-mode-vi v send -x begin-selection
bind -t copy-mode-vi y send-keys -x copy-pipe-and-cancel ""pbcopy""
bind p paste-buffer
bind -t copy-mode-vi mousedragend1pane send-keys -x copy-pipe-and-cancel ""pbcopy"" enter fullscreen mode exit fullscreen mode the first line, bind -t copy-mode-vi v send -x begin-selection , lets you begin selection (highlight) by pressing v while in copy mode ( copy-mode-vi ). the second line allows you to yank the text with y . the third line allows us to paste from the tmux buffer (where our text is saved) with ctrl + p . the last line is a bonus. it lets you copy the text that you highlight with the mouse (note that it has the same effect as the second line). notice that on the second and fourth line, i use pbcopy . i need to use it to pipe the copied text into the clipboard on my mac. if you're on linux, omit pbcopy : bind-key -t copy-mode-vi y send -x copy-selection-and-cancel
bind-key -t copy-mode-vi mousedragend1pane send-keys -x copy-pipe-and-cancel enter fullscreen mode exit fullscreen mode as always, don't forget to save and source the config file. with this new setting, the workflow is: enter the copy-mode with ctrl + [ position yourself to the start of the text you want to copy with vim navigation start highlight with v (similar to vim visual mode) yank the highlighted text with y paste with prefix + p i use an uppercase p instead of lowercase p because prefix + p is how tmux goes to the previous window (recall: prefix + p to go to the previous window and prefix + n to go to the next window. if you are not familiar with this, i'd highly recommend you to read my first tmux article). taking advantage of the vi mode the vi-copy-mode is not only useful for copying texts. you can use it to quickly search through your terminal. for example, in the terminal, if you are looking for a text, you would press cmd + f (or ctrl + f ) and type in the string that you want to search. this method has limitations. first, you can only search for that exact string (what if instead of searching for the string ""mux"", you want to search for ""mux"", ""mix"", and ""max"" simultaneously?). second, if you have multiple panes in a window, this method will search for the text in all panes (what if you need to constrain your search only to the current pane?). third, it is limited to what is currently displayed on tmux at the moment. due to the nature of tmux's scrolling, it can't search for texts that are above the displayed window. a better way to search is to leverage the vi-copy-mode. recall that in vim, you can search forward with / and backward with ? . with vi mode, you can do that too. suppose that i need to search for ""redis_1"" in my terminal. here's how i would do it. from inside a tmux session, enter the vi mode: prefix + [ enter fullscreen mode exit fullscreen mode then search backward with: ? + keyword enter fullscreen mode exit fullscreen mode in this case, keyword is ""redis_1"". i find myself using ? much more than / , because 80% of the time i would be searching backwards. tmux should highlight all ""redis_1"" texts. to keep jumping to the subsequent or preceding matches, use either n or n . sweet! but wait, there's more! just like you can use regular expressions in vim while searching, you can with tmux too! suppose that i want to search for words like ""redis_1"" or ""node_1"", or ""rails_1"", i can search for all of them with [a-z]+_1 : ? + [a-z]+_1 enter fullscreen mode exit fullscreen mode regex breakdown: [a-z] is the lowercase a-z + means one or more subsequent characters (which was a-z). [a-z]+ means one or more any lowercase alphabet character _1 is a literal underscore followed by a literal one. this will match ""redis_1"", ""node_1"", and ""rails_1"" (it will also match strings like ""java_1"", ""sidekiq_1"", etc. if you want to match only ""redis_1"", ""node_1"", or ""rails_1"", you can use a group match: ? + (redis|node|rails)_1 enter fullscreen mode exit fullscreen mode pretty cool! i won't go through regex here because it's outside the scope of this article. the point is, searching in tmux using vi mode is a powerful tool especially if you're proficient with regex (even if you don't, you can get pretty far with just a little regex knowledge). emacs copy mode if you look at the man tmux page, in addition to the vi-copy-mode, tmux also has an emacs-copy mode. it is similar to vi-copy-mode, but instead of using vi key bindings, it uses emacs key bindings. if you're more of an emacs guy (don't worry, we can still be friends) and want to set it up to use emacs keybindings instead, check out inside the man tmux page under mode-keys section. there you will find instructions on how to set it up, including a list of keymaps. conclusion in this article, we learned how to leverage the copy mode to make it easier to copy-paste from your tmux terminal. we also learned how to configure it to mimic vim copy-pasting behavior. this copy mode can be leveraged to be an effective search tool. tmux is a powerful tool. it complements with vim well. to make it even more powerful, you can configure it to behave like vim behaves. happy hacking! top comments (6) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand peter benjamin (they/them) peter benjamin (they/them) peter benjamin (they/them) follow {security,software,infrastructure} engineer. {rust,go,typescript} enthusiast. k8s herder. email petermbenjamin@gmail.com education ms, cyber security and ethical hacking pronouns they/them work software engineer joined jun 28, 2017 • nov 24 '21 dropdown menu copy link hide i have not experienced any problems selecting and copying text in tmux with a mouse after setting set -g mouse on . if you want to emulate some terminal features like ""auto copy selected text"", you can set the additional tmux setting: bind-key -t copy-mode-vi mousedragend1pane send-keys -x copy-pipe-and-cancel ""<clipboard_program>"" enter fullscreen mode exit fullscreen mode where <clipboard_program> on linux can be something like xclip and on mac would be pbcopy . from this point on, you can use the mouse to click-and-drag selection over text and once you let go, tmux will automatically copy to system clipboard. like comment: like comment: 6 likes like comment button reply collapse expand v0phan1ee v0phan1ee v0phan1ee follow email vophanlee@gmail.com joined mar 15, 2022 • nov 28 '22 dropdown menu copy link hide thanks for sharing ;) like comment: like comment: 2 likes like comment button reply collapse expand buntu ngcebetsha buntu ngcebetsha buntu ngcebetsha follow i am a software engineer at the south african radio astronomy observatory. location cape town, south africa education astrophysics, radio astronomy, university of cape town, rhodes university work software engineer at south african radio astronomy observatory joined apr 15, 2019 • apr 29 '23 dropdown menu copy link hide finally...thank you for this like comment: like comment: 1 like like comment button reply collapse expand aka_radix aka_radix aka_radix follow python, ts, js, ruby, go, lua work full stack engineer joined oct 21, 2022 • jan 29 '24 dropdown menu copy link hide thanks a lot, much appreciated! like comment: like comment: 1 like like comment button reply collapse expand jack moore jack moore jack moore follow i'm interested in infrastructure, automation, and reliability. work analyst joined mar 6, 2019 • nov 20 '24 dropdown menu copy link hide thank you for the post like comment: like comment: 1 like like comment button reply collapse expand leonid leonid leonid follow joined mar 11, 2024 • mar 11 '24 • edited on mar 11 • edited dropdown menu copy link hide igor, thank you for post! could you also show how to transfer data from tmux' buffer to system buffer (to another application) and vice versa? like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse igor irianto follow vim, rails, cheesy puns location dallas, tx joined apr 27, 2019 more from igor irianto vim global command # vim # global # ex # command learning vim regex # vim # neovim # regex # search vimgrep tips and tricks # vim # grep # vimgrep # tips 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/dandyvica/go-regular-expressions-53dn,,,"go regular expressions - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse dandy vica posted on jul 10, 2021 go regular expressions # go # tutorial this time, i wanted to tackle the regular expression package in go. in one of my recent projects, i had to use this library. but i have to confess it's not straightforward at first sight. i hope this article will unravel this feature. to use the library ( https://golang.org/pkg/regexp/ ), just add: import ( ... ""regexp"" ... ) enter fullscreen mode exit fullscreen mode to your code. following is a collection of tips for solving a specific problem. throughout the following examples, i used the assert go package to make it easier to understand. compiling the regular expression before anything, you need to compile your regular expression before using any package function. you can use compile() or mustcompile() . the difference between them is that the latter panics whenever an error is found compiling the regex but the former doesn't. if your code doesn't involve a regex provided at runtime, it's safer to use mustcompile() as you know immedialty whether your regex syntax is correct. note that the posix() versions require a posix regex rather than a perl-like regular expression. is my string matching the regexp ? to just check whether a string is matching your regex, you can use two different functions: // don't worry, fake phone number phone := ""202-555-0147"" phonere := `\d{3}\-\d{3}\-\d{4}` // returns: true <nil> matched , err := regexp . matchstring ( phonere , phone ) assert . true ( matched ) assert . nil ( err ) // or compile and test re := regexp . mustcompile ( phonere ) assert . true ( re . matchstring ( phone )) // but this returns true as well! assert . true ( re . matchstring ( ""202-555-0147mkljhfqdhmfj"" )) // but not this re = regexp . mustcompile ( `\d{3}\-\d{3}\-\d{4}$` ) assert . false ( re . matchstring ( ""202-555-0147mkljhfqdhmfj"" )) enter fullscreen mode exit fullscreen mode beware that the string 202-555-0147mkljhfqdhmfj is matched using the first regex. the explanation is given is the compile() definition and by the term leftmost : when matching against text, the regexp returns a match that begins as early as possible in the input (leftmost), and among those it chooses the one that a backtracking search would have found first. this so-called leftmost-first matching is the same semantics that perl, python, and other implementations use, although this package implements it without the expense of backtracking. enter fullscreen mode exit fullscreen mode you can influence this behavior with the longest() function. it's wiser to use mustcompile() or compile() if you have to reuse your regex to just compile it once. using capture groups capture groups are called submatches in the regexp package. you get access to capture groups using on findstringsubmatch() function: // use capture groups phonerecaps := `(\d{3})\-(\d{3})\-(\d{4})$` re = regexp . mustcompile ( phonerecaps ) // caps is a slice of strings, where caps[0] matches the whole match // caps[1] == ""202"" etc matches := re . findstringsubmatch ( phone ) // print out: there're 3 capture groups assert . equal ( re . numsubexp (), 3 ) assert . equal ( matches [ 0 ], ""202-555-0147"" ) assert . equal ( matches [ 1 ], ""202"" ) assert . equal ( matches [ 2 ], ""555"" ) assert . equal ( matches [ 3 ], ""0147"" ) assert . elementsmatch ( matches , [] string { ""202-555-0147"" , ""202"" , ""555"" , ""0147"" }) enter fullscreen mode exit fullscreen mode using named capture groups to fully benefit from the python-like named capture groups , you can't have a direct access to the value of the submatch for a particular name. you only have an indirect and unwieldy access: first get all names, the get the corresponding index for that name and then fetch the capture group string: // use named capture groups phonerenamedcaps := `(?p<area>\d{3})\-(?p<exchange>\d{3})\-(?p<line>\d{4})$` re = regexp . mustcompile ( phonerenamedcaps ) // print out: [ area exchange line], not that the first element is the empty string names := re . subexpnames () assert . elementsmatch ( names , [] string { """" , ""area"" , ""exchange"" , ""line"" }) // // indirect access to names matches = re . findstringsubmatch ( phone ) assert . len ( matches , 4 ) capname := names [ 1 ]; nameindex := re . subexpindex ( capname ); assert . equal ( matches [ nameindex ], ""202"" ) capname = names [ 2 ]; nameindex = re . subexpindex ( capname ); assert . equal ( matches [ nameindex ], ""555"" ) capname = names [ 3 ]; nameindex = re . subexpindex ( capname ); assert . equal ( matches [ nameindex ], ""0147"" ) enter fullscreen mode exit fullscreen mode splitting a string it might be useful sometimes to split a string delimited with characters matching a regexp: csv := ""a;b;c;;;;d;e;f;;;g"" split1 := regexp . mustcompile ( "";"" ) . split ( csv , - 1 ) split2 := regexp . mustcompile ( "";*"" ) . split ( csv , - 1 ) assert . len ( split1 , 12 ) assert . elementsmatch ( split1 , [] string { ""a"" , ""b"" , ""c"" , """" , """" , """" , ""d"" , ""e"" , ""f"" , """" , """" , ""g"" }) assert . len ( split2 , 7 ) assert . elementsmatch ( split2 , [] string { ""a"" , ""b"" , ""c"" , ""d"" , ""e"" , ""f"" , ""g"" }) enter fullscreen mode exit fullscreen mode replacing strings you can replace strings by providing a template made of references to a matched capture group. you can use $1 (or ${1} ) to refer to the first submatch, $2 for the second etc: csv = ""a;b;c;;;;d;e;f;;;g"" split := regexp . mustcompile ( ""(;+)"" ) // prints: ""a;b;c;d;e;f;g"" assert . equal ( split . replaceallstring ( csv , "";"" ), ""a;b;c;d;e;f;g"" ) digits := ""0123456789"" digitsre := regexp . mustcompile ( strings . repeat ( `(\d)` , 10 )) assert . equal ( digitsre . replaceallstring ( digits , ""$10$9$8$7$6$5$4$3$2$1"" ), ""9876543210"" ) enter fullscreen mode exit fullscreen mode you can use names instead: // using names rather than indexes digitsre = regexp . mustcompile ( `(?p<zero>\d)(?p<one>\d)(?p<two>\d)(?p<three>\d)(?p<four>\d)(?p<five>\d)(?p<six>\d)(?p<seven>\d)(?p<eight>\d)(?p<nine>\d)` ) assert . equal ( digitsre . replaceallstring ( digits , ""nine=${nine}, eight=${eight}, seven=${seven}, six=${six}, five=${five}, four=${four}, three=${three}, two=${two}, one=${one}, zero=${zero}"" ), ""nine=9, eight=8, seven=7, six=6, five=5, four=4, three=3, two=2, one=1, zero=0"" , ) enter fullscreen mode exit fullscreen mode hope this helps ! photo by mick haupt on unsplash top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse dandy vica follow senior software engineer.
always willing to learn new stuff. location france education master's degree in computer science work software architect, devops engineer joined apr 25, 2019 more from dandy vica implementing sha2 (256/512) algorithm with rust const generics # rust # tutorial # algorithms rust error management: lessons learned # rust # tutorial use mitmproxy as a personal firewall # python # tutorial # security 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/andyb1979/plotting-real-time-long-term-ecg-signals-with-android-app-3jkf,,,"plotting real-time long-term ecg signals with android app - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse andrew bt posted on apr 5, 2024 • originally published at scichart.com plotting real-time long-term ecg signals with android app electrocardiomatrix (ecm) is an intuitive way to review long-term ecg signals. the project developed at borjiginlab , at the university of michigan aims to promote early and accurate atrial fibrillation detection. android app was created to allow patients themselves identify atrial fibrillation and send the suspicious recordings to cardiologist. promoting early screening of cardiac abnormalities with ecm technology university of michigan’s jimo borjigin, ph.d. has developed a novel electrocardiomatrix ( ecm ) technology to permit efficient, intuitive, and accurate detection of cardiac abnormalities from electrocardiography (ecg) signals. ecm is expected to promote early and accurate atrial fibrillation (afib) detection, permit stroke prevention, and reduce costs of diagnosis. to promote early screening of afib in general population, our laboratory developed an android app that communicates with a medical-grade wearable ecg device. the single-lead ecg signal is continuously streamed to the mobile phone or tablet and transformed into ecm instantaneously, which can be inspected visually in real-time. normal sinus rhythm (left) and atrial fibrillation (right) on 5-minute long ecm this method permits accurate detection of paroxysmal afib with or without symptoms. with one glance of visual inspection, one can easily identify cardiac abnormalities, including afib, from long ecg/ecm recordings, with minimum training. patients can monitor their cardiac health at home during casual activities and send the report to their doctors for diagnosis. data visualization challenge for live streaming ecm on the mobile devices, the biggest challenge is charting performance for the large amount of data points from the heatmap that consists of thousands of heartbeats. paroxysmal atrial fibrillation may occur suddenly and stop on its own during daily routine, it can be hard to be captured in just a few minutes. ecm provides users an efficient way to review thousands of heartbeats at a time. for example, a 30min-long ecm contains 3,600,000 data points for the ecg at 200hz. data visualization of large amounts of data in real-time on mobile devices becomes crucial for app development and user experience. the app allows user to review all the saved recordings using scichart android charting library scichart is a high-performance charting solution on the market that is easy to implement to the mobile platforms. this application uses the following features: fifo charts series are used to display the top chart and visualize the ecg information in real-time. the heatmap chart type is to visualize the ecm data at the bottom of the application. the zooming and panning features of the scichart allow users to navigate the details of the ecm in different scales. the annotation features presents healthcare providers a way to highlight a short ecg strip within a long-term ecm, and to annotate irregular heartbeats or cardiac rhythms with simple taps for further analysis. highlight short ecg strip within ecm annotation functionalities for healthcare professionals relevance with the help of ecm, the cardiac rhythm changes and irregular heartbeats can be easily identified with visual inspection. the wearable hardware allows user monitoring continuous ecg for maximum 7 days with one coin battery. usually cardiologists review ecg signals in two ways. one is a few minutes of 12-lead ecg, which is ordered by the doctor when the patient feels something is wrong with their heart. however, some paroxysmal atrial fibrillation can be hard to be captured in a short period of time. another way is a holter/event monitor, which records continuous long-term ecg for days. it is not possible for a cardiologist to check the raw ecg data beat by beat, which is time consuming and error-prune. healthcare professionals only focus on the suspicious episodes that algorithm marked as arrhythmias, or the episodes with the marker when the patient pushed the button on the device. the false positives may overwhelm the cardiologists, while false negatives lead to misdiagnosis. that is where ecm comes in as it provides an affordable way that people can review long-term ecg with one glance. the accurate detection of atrial fibrillation outperformed the clinical team in our previous study ( https://pubmed.ncbi.nlm.nih.gov/31177972/ ). patients with known or suspected heart conditions and health-conscious individuals are able to monitor their ecg 24/7 and interpret ecm with instructions. patients can choose to share their recordings to the cardiologist or physician if they recognized cardiac abnormalities. the wearable device with chest belt or adhesive patch provides comfort and flexibility for daily routine. the ecm definitely has other application scenarios like bedside ecg monitor, nursing station, ambulatory ecg monitoring, ecg monitoring during anesthesia, and more. please visit http://ecmatrix.med.umich.edu for more information. further reading the whole article and research paper can be found here: electrocardiomatrix facilitates qualitative identification of diminished heart rate variability in critically ill patients shortly before cardiac arrest scichart android example: vital signs ecg example other blog posts where scichart for mobile is used in digital health domain: data visualization for the epilepsy research kit for kids top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse andrew bt follow former electronic engineer who works in software. with experience in languages from c/c++ to c#, javascript and typescript, and now specialises in performance optimisation and data visualisation location united kingdom joined dec 14, 2023 more from andrew bt powering research. development. innovation worldwide scichart – the fastest wpf charts. period. 2021 test results update is wpf dead? the data says anything but, here’s why 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/gridou/how-to-add-spring-boot-and-gradle-multi-project-builds-capabilities-to-your-nx-workspace-53cd,,,"announcing @jnxplus/nx-gradle - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse khalil la posted on sep 12, 2021 • edited on sep 30, 2023 announcing @jnxplus/nx-gradle # nx # springboot # gradle i like to use nx workspace to manage my projects. i don't put all my projects in one workspace but i use one workspace per multi-module project. it works nicely with node frameworks like angular, react, nest... but when its come to other languages and frameworks, we need to use custom plugins. in this article i will use my plugin @jnxplus/nx-gradle to add support of spring boot and gradle multi-project builds to nx workspace. @jnxplus/nx-gradle add to the spring boot world, an opinionated way to integrate spring boot apps/libs inside nx workspace using gradle multi-project builds. let's show you how to use it. we start by creating a workspace : open a terminal and run this command to create a new workspace : npx create-nx-workspace@latest enter fullscreen mode exit fullscreen mode when asked provide my-org as name and choose an empty workspace : devs> npx create-nx-workspace@latest
npx: installed 48 in 3.278s
√ workspace name ( e.g., org name ) · my-org
√ what to create in the new workspace · empty
√ use nx cloud? ( it 's free and doesn' t require registration. ) · no > nx  nx is creating your workspace. enter fullscreen mode exit fullscreen mode now and in the same terminal go inside my-org folder : cd my-org enter fullscreen mode exit fullscreen mode 1. install the plugin in the workspace root run this command to install the plugin : npm install --save-dev @jnxplus/nx-gradle enter fullscreen mode exit fullscreen mode 2. add spring boot and gradle wrapper support the following command adds spring boot and gradle support (gradle wrapper and config files) to the workspace. this only needs to be performed once per workspace. nx generate @jnxplus/nx-gradle:init enter fullscreen mode exit fullscreen mode i choose the version of java supported by my operating system and the default value for gradle root project: my-org> nx generate @jnxplus/nx-gradle:init
√ which version of java would you like to use? · 11
√ what rootprojectname would you like to use? · boot-multiproject
create checkstyle.xml
create gradle/wrapper/gradle-wrapper.jar
create gradle/wrapper/gradle-wrapper.properties
create gradle.properties
create gradlew
create gradlew.bat
create settings.gradle
update nx.json
update .gitignore enter fullscreen mode exit fullscreen mode as you see, the command added the following files : checkstyle.xml for linting. gradle wrapper and gradle executables for windows and linux :
using gradle wrapper we can distribute/share a project to everybody to use the same version and gradle's functionality(compile, build, install...) even if it has not been installed gradle.properties :
this file contain java, spring boot and dependency management versions that we will use for all apps and libs inside nx worspace. settings.gradle :
here we will add our apps and libs later so gradle will be able to  perform its tasks. we also updated nx.json file to add the plugin for dep-graph feature and .gitignore so we can ignore gradle build and cache folders. 3. usage generate an application nx generate @jnxplus/nx-gradle:application my-app enter fullscreen mode exit fullscreen mode when asked, provide answers or choose default : my-org> nx generate @jnxplus/nx-gradle:application my-app
√ what groupid would you like to use? · com.example
√ what version would you like to use? · 0.0.1-snapshot
√ which packaging would you like to use? · jar
update workspace.json
update nx.json
create apps/my-app/build.gradle
create apps/my-app/src/main/java/com/example/myapp/hellocontroller.java
create apps/my-app/src/main/java/com/example/myapp/myappapplication.java
create apps/my-app/src/main/resources/application.properties
create apps/my-app/src/test/java/com/example/myapp/hellocontrollertests.java
create apps/my-app/src/test/resources/application.properties
update settings.gradle enter fullscreen mode exit fullscreen mode check the settings.gradle , here we added a entry for my-app : rootproject.name = 'boot-multiproject' include ( 'apps:my-app' ) enter fullscreen mode exit fullscreen mode build the app nx build my-app enter fullscreen mode exit fullscreen mode if you look carefully to the console, you will see that we run the command : executing command : gradlew.bat :apps:my-app:bootjar enter fullscreen mode exit fullscreen mode since we choose a jar packaging, we run this command. for war packaging, we run bootwar . serve the app nx serve my-app enter fullscreen mode exit fullscreen mode here under the hood we run the command : bootrun . open http://localhost:8080 to see app working: test the app to test the app run this command : nx test my-app enter fullscreen mode exit fullscreen mode what i like to do to see if test command is really working is breaking a test, and run it again : so change the test and add a comma between hello and world : @test
public void shouldreturnhelloworld () throws exception { this.mockmvc.perform ( get ( ""/"" )) .anddo ( print ()) .andexpect ( status () .isok ()) .andexpect ( content () .string ( containsstring ( ""hello, world"" ))) ; } enter fullscreen mode exit fullscreen mode now rerun the same command : perfect, now the test target is failing. revert change and let's generate this time a library. to generate a library use this command : nx generate @jnxplus/nx-gradle:library my-lib enter fullscreen mode exit fullscreen mode when asked, provide answers or choose default : my-org> nx generate @jnxplus/nx-gradle:library my-lib
√ what groupid would you like to use? · com.example
√ what version would you like to use? · 0.0.1-snapshot
update workspace.json
update nx.json
create libs/my-lib/build.gradle
create libs/my-lib/src/main/java/com/example/mylib/helloservice.java
create libs/my-lib/src/test/java/com/example/mylib/helloservicetests.java
create libs/my-lib/src/test/java/com/example/mylib/testconfiguration.java
update settings.gradle enter fullscreen mode exit fullscreen mode check the settings.gradle , here we added a entry for my-lib : rootproject.name = 'boot-multiproject' include ( 'libs:my-lib' ) include ( 'apps:my-app' ) enter fullscreen mode exit fullscreen mode like an app, we can build it and test it : build : nx build my-lib enter fullscreen mode exit fullscreen mode test : nx test my-lib enter fullscreen mode exit fullscreen mode but we can't run it with serve target : my-org> nx serve my-lib > nx   error  cannot find target 'serve' for project 'my-lib' enter fullscreen mode exit fullscreen mode hope you like this article, and give this plugin a try :) you can find the github code here : https://github.com/khalilou88/my-org top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand joelbonetr 🥇 joelbonetr 🥇 joelbonetr 🥇 follow tech lead/team lead. senior webdev.

intermediate grade on computer systems-
high grade on web application development-
mba (+marketing+hhrr).
studied a bit of law, economics and design location spain education higher level education certificate on web application development work tech lead/lead dev joined apr 19, 2019 • apr 18 '23 dropdown menu copy link hide thank you! that's quite useful, the stack java + js is pretty common and monorepos are getting more attention as the time goes on 😁 like comment: like comment: 2 likes like comment button reply collapse expand gabriel duncan gabriel duncan gabriel duncan follow joined nov 30, 2023 • nov 30 '23 dropdown menu copy link hide that github url is 404. would you mind making it public again? thanks for the interesting article! like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse khalil la follow i am a full stack developer location paris joined sep 20, 2019 more from khalil la smooth maven project onboarding # nx # maven announcing @jnxplus/nx-maven # nx # springboot # maven how to deploy a nx workspace (angular + nestjs) easily with zeit now? # nx # angular # nestjs # now 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/seanpgallivan/solution-container-with-most-water-1907,,,"solution: container with most water - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse seanpgallivan posted on feb 17, 2021 solution: container with most water # algorithms # javascript # java # python leetcode solutions (161 part series) 1 solution: next permutation 2 solution: trim a binary search tree ... 157 more parts... 3 leetcode solutions index 4 solution: minimize deviation in array 5 solution: vertical order traversal of a binary tree 6 solution: count ways to make array with product 7 solution: smallest string with a given numeric value 8 solution: linked list cycle 9 solution: path with minimum effort 10 solution: concatenation of consecutive binary numbers 11 solution: minimum operations to make a subsequence 12 solution: longest harmonious subsequence 13 solution: simplify path 14 solution: building boxes 15 solution: decode xored permutation 16 solution: binary tree right side view 17 solution: find kth largest xor coordinate value 18 solution: change minimum characters to satisfy one of three conditions 19 solution: shortest distance to a character 20 solution: peeking iterator 21 solution: convert bst to greater tree 22 solution: copy list with random pointer 23 solution: valid anagram 24 solution: number of steps to reduce a number to zero 25 solution: shortest path in binary matrix 26 solution: is graph bipartite? 27 solution: maximum score from removing substrings (ver. 1) 28 solution: maximum score from removing substrings (ver. 2) 29 solution: sort the matrix diagonally 30 solution: the k weakest rows in a matrix (ver. 1) 31 solution: the k weakest rows in a matrix (ver. 2) 32 solution: letter case permutation 33 solution: container with most water 34 solution: arithmetic slices 35 solution: minimum remove to make valid parentheses 36 solution: roman to integer 37 solution: broken calculator 38 solution: find the most competitive subsequence 39 solution: longest word in dictionary through deleting 40 solution: search a 2d matrix ii 41 solution: score of parentheses 42 solution: shortest unsorted continuous subarray 43 solution: validate stack sequences 44 solution: divide two integers (ver. 1) 45 solution: divide two integers (ver. 2) 46 solution: maximum frequency stack 47 solution: distribute candies 48 solution: set mismatch (ver. 1) 49 solution: set mismatch (ver. 2) 50 solution: missing number 51 solution: intersection of two linked lists 52 solution: average of levels in binary tree 53 solution: short encoding of words (ver. 1) 54 solution: design hashmap (ver. 1) 55 solution: short encoding of words (ver. 2) 56 solution: design hashmap (ver. 2) 57 solution: remove palindromic subsequences 58 solution: add one row to tree 59 solution: integer to roman 60 solution: coin change 61 solution: check if a string contains all binary codes of size k 62 solution: binary trees with factors 63 solution: swapping nodes in a linked list 64 solution: encode and decode tinyurl 65 solution: best time to buy and sell stock with transaction fee 66 solution: generate random point in a circle 67 solution: wiggle subsequence 68 solution: keys and rooms 69 solution: design underground system 70 solution: reordered power of 2 71 solution: vowel spellchecker 72 solution: 3sum with multiplicity 73 solution: advantage shuffle 74 solution: pacific atlantic water flow 75 solution: word subsets 76 solution: palindromic substrings 77 solution: reconstruct original digits from english 78 solution: flip binary tree to match preorder traversal 79 solution: russian doll envelopes 80 solution: stamping the sequence 81 solution: palindrome linked list 82 solution: ones and zeroes 83 solution: longest valid parentheses 84 solution: design circular queue 85 solution: global and local inversions 86 solution: minimum operations to make array equal 87 solution: determine if string halves are alike 88 solution: letter combinations of a phone number 89 solution: verifying an alien dictionary 90 solution: longest increasing path in a matrix 91 solution: deepest leaves sum 92 solution: beautiful arrangement ii 93 solution: flatten nested list iterator 94 solution: partition list 95 solution: fibonacci number 96 solution: remove all adjacent duplicates in string ii 97 solution: number of submatrices that sum to target 98 solution: remove nth node from end of list 99 solution: combination sum iv 100 solution: n-ary tree preorder traversal 101 solution: triangle 102 solution: brick wall 103 solution: count binary substrings 104 solution: critical connections in a network 105 solution: rotate image 106 solution: furthest building you can reach 107 solution: power of three 108 solution: unique paths ii 109 solution: find first and last position of element in sorted array 110 solution: powerful integers 111 solution: prefix and suffix search 112 solution: course schedule iii 113 solution: running sum of 1d array 114 solution: non-decreasing array 115 solution: jump game ii 116 solution: convert sorted list to binary search tree 117 solution: delete operation for two strings 118 solution: super palindromes 119 solution: construct target array with multiple sums 120 solution: count primes 121 solution: maximum points you can obtain from cards 122 solution: range sum query 2d - immutable 123 solution: ambiguous coordinates 124 solution: flatten binary tree to linked list 125 solution: valid number 126 solution: binary tree cameras 127 solution: longest string chain 128 solution: find duplicate file in system 129 solution: minimum moves to equal array elements ii 130 solution: binary tree level order traversal 131 solution: find and replace pattern 132 solution: n-queens 133 solution: to lower case 134 solution: evaluate reverse polish notation 135 solution: partitioning into minimum number of deci-binary numbers 136 solution: maximum product of word lengths 137 solution: maximum erasure value 138 solution: n-queens ii 139 solution: maximum gap 140 solution: search suggestions system 141 solution: max area of island 142 solution: interleaving string 143 solution: maximum area of a piece of cake after horizontal and vertical cuts 144 solution: open the lock 145 solution: maximum performance of a team 146 solution: longest consecutive sequence 147 solution: min cost climbing stairs 148 solution: construct binary tree from preorder and inorder traversal 149 solution: jump game vi 150 solution: my calendar i 151 solution: stone game vii 152 solution: minimum number of refueling stops 153 solution: palindrome pairs 154 solution: maximum units on a truck 155 solution: matchsticks to square 156 solution: generate parentheses 157 solution: number of subarrays with bounded maximum 158 solution: swim in rising water 159 solution: pascal's triangle 160 solution: out of boundary paths 161 solution: redundant connection this is part of a series of leetcode solution explanations ( index ). if you liked this solution or found it useful, please like this post and/or upvote my solution post on leetcode's forums . leetcode problem #11 ( medium ): container with most water description: given n non-negative integers a1, a2, ..., an , where each represents a point at coordinate (i, ai) . n vertical lines are drawn such that the two endpoints of the line i is at (i, ai) and (i, 0) . find two lines, which, together with the x-axis forms a container, such that the container contains the most water. notice that you may not slant the container. examples: example 1: input: height = [1,8,6,2,5,4,8,3,7] output: 49 explanation: the above vertical lines are represented by array [1,8,6,2,5,4,8,3,7]. in this case, the max area of water (blue section) the container can contain is 49. visual: example 2: input: height = [1,1] output: 1 example 3: input: height = [4,3,2,1,4] output: 16 example 4: input: height = [1,2,1] output: 2 constraints: n == height.length 2 <= n <= 3 * 10^4 0 <= height[i] <= 3 * 10^4 idea: the first thing we should realize is that the amount of water contained is always going to be a rectangle whose area is defined as length * width . the width of any container will be the difference between the index of the two lines ( i and j ), and the height will be whichever of the two sides is the lowest ( min(h[i], h[j]) ). the brute force approach would be to compare every single pair of indexes in h , but that would be far too slow. instead, we can observe that if we start with the lines on the opposite ends and move inward, the only possible time the area could be larger is when the height increases, since the width will continuously get smaller. this is very easily observed with the use of visuals. let's say we start with a graph of h like this: the first step would be to find our starting container described by the lines on either end: we can tell that the line on the right end will never make a better match, because any further match would have a smaller width and the container is already the maximum height that that line can support. that means that our next move should be to slide j to the left and pick a new line: this is a clear improvement over the last container. we only moved over one line, but we more than doubled the height. now, it's the line on the left end that's the limiting factor, so the next step will be to slide i to the right. just looking at the visual, however, it's obvious that we can skip the next few lines because they're already underwater, so we should go to the first line that's larger than the current water height: this time, it doesn't look like we made much of a gain, despite the fact that the water level rose a bit, because we lost more in width than we made up for in height. that means that we always have to check at each new possible stop to see if the new container area is better than the current best. just lik before we can slide j to the left again: this move also doesn't appear to have led to a better container. but here we can see that it's definitely possible to have to move the same side twice in a row, as the j line is still the lower of the two: this is obviously the last possible container to check, and like the last few before it, it doesn't appear to be the best match. still, we can understand that it's entirely possible for the best container in a different example to be only one index apart, if both lines are extremely tall. putting together everything, it's clear that we need to make a 2-pointer sliding window solution . we'll start from either end and at each step we'll check the container area, then we'll shift the lower-valued pointer inward. once the two pointers meet, we know that we must have exhausted all possible containers and we should return our answer ( ans ). implementation: javascript was weirdly more performant when using both math.max() and math.min() rather than performing more basic comparisons, even with duplicated effort in the ternary. for the other languages, it made more sense (and was ultimately more performant) to only have to do the basic comparisons once each. javascript code: var maxarea = function ( h ) { let ans = 0 , i = 0 , j = h . length - 1 while ( i < j ) { ans = math . max ( ans , math . min ( h [ i ], h [ j ]) * ( j - i )) h [ i ] <= h [ j ] ? i ++ : j -- } return ans }; enter fullscreen mode exit fullscreen mode python code: class solution : def maxarea ( self , h : list [ int ]) -> int : ans , i , j = 0 , 0 , len ( h ) - 1 while ( i < j ): if h [ i ] <= h [ j ]: res = h [ i ] * ( j - i ) i += 1 else : res = h [ j ] * ( j - i ) j -= 1 if res > ans : ans = res return ans enter fullscreen mode exit fullscreen mode java code: class solution { public int maxarea ( int [] h ) { int ans = 0 , i = 0 , j = h . length - 1 , res = 0 ; while ( i < j ) { if ( h [ i ] <= h [ j ]) { res = h [ i ] * ( j - i ); i ++; } else { res = h [ j ] * ( j - i ); j --; } if ( res > ans ) ans = res ; } return ans ; } } enter fullscreen mode exit fullscreen mode c++ code: class solution { public: int maxarea ( vector < int >& h ) { int ans = 0 , i = 0 , j = h . size () - 1 , res = 0 ; while ( i < j ) { if ( h [ i ] <= h [ j ]) { res = h [ i ] * ( j - i ); i ++ ; } else { res = h [ j ] * ( j - i ); j -- ; } if ( res > ans ) ans = res ; } return ans ; } }; enter fullscreen mode exit fullscreen mode leetcode solutions (161 part series) 1 solution: next permutation 2 solution: trim a binary search tree ... 157 more parts... 3 leetcode solutions index 4 solution: minimize deviation in array 5 solution: vertical order traversal of a binary tree 6 solution: count ways to make array with product 7 solution: smallest string with a given numeric value 8 solution: linked list cycle 9 solution: path with minimum effort 10 solution: concatenation of consecutive binary numbers 11 solution: minimum operations to make a subsequence 12 solution: longest harmonious subsequence 13 solution: simplify path 14 solution: building boxes 15 solution: decode xored permutation 16 solution: binary tree right side view 17 solution: find kth largest xor coordinate value 18 solution: change minimum characters to satisfy one of three conditions 19 solution: shortest distance to a character 20 solution: peeking iterator 21 solution: convert bst to greater tree 22 solution: copy list with random pointer 23 solution: valid anagram 24 solution: number of steps to reduce a number to zero 25 solution: shortest path in binary matrix 26 solution: is graph bipartite? 27 solution: maximum score from removing substrings (ver. 1) 28 solution: maximum score from removing substrings (ver. 2) 29 solution: sort the matrix diagonally 30 solution: the k weakest rows in a matrix (ver. 1) 31 solution: the k weakest rows in a matrix (ver. 2) 32 solution: letter case permutation 33 solution: container with most water 34 solution: arithmetic slices 35 solution: minimum remove to make valid parentheses 36 solution: roman to integer 37 solution: broken calculator 38 solution: find the most competitive subsequence 39 solution: longest word in dictionary through deleting 40 solution: search a 2d matrix ii 41 solution: score of parentheses 42 solution: shortest unsorted continuous subarray 43 solution: validate stack sequences 44 solution: divide two integers (ver. 1) 45 solution: divide two integers (ver. 2) 46 solution: maximum frequency stack 47 solution: distribute candies 48 solution: set mismatch (ver. 1) 49 solution: set mismatch (ver. 2) 50 solution: missing number 51 solution: intersection of two linked lists 52 solution: average of levels in binary tree 53 solution: short encoding of words (ver. 1) 54 solution: design hashmap (ver. 1) 55 solution: short encoding of words (ver. 2) 56 solution: design hashmap (ver. 2) 57 solution: remove palindromic subsequences 58 solution: add one row to tree 59 solution: integer to roman 60 solution: coin change 61 solution: check if a string contains all binary codes of size k 62 solution: binary trees with factors 63 solution: swapping nodes in a linked list 64 solution: encode and decode tinyurl 65 solution: best time to buy and sell stock with transaction fee 66 solution: generate random point in a circle 67 solution: wiggle subsequence 68 solution: keys and rooms 69 solution: design underground system 70 solution: reordered power of 2 71 solution: vowel spellchecker 72 solution: 3sum with multiplicity 73 solution: advantage shuffle 74 solution: pacific atlantic water flow 75 solution: word subsets 76 solution: palindromic substrings 77 solution: reconstruct original digits from english 78 solution: flip binary tree to match preorder traversal 79 solution: russian doll envelopes 80 solution: stamping the sequence 81 solution: palindrome linked list 82 solution: ones and zeroes 83 solution: longest valid parentheses 84 solution: design circular queue 85 solution: global and local inversions 86 solution: minimum operations to make array equal 87 solution: determine if string halves are alike 88 solution: letter combinations of a phone number 89 solution: verifying an alien dictionary 90 solution: longest increasing path in a matrix 91 solution: deepest leaves sum 92 solution: beautiful arrangement ii 93 solution: flatten nested list iterator 94 solution: partition list 95 solution: fibonacci number 96 solution: remove all adjacent duplicates in string ii 97 solution: number of submatrices that sum to target 98 solution: remove nth node from end of list 99 solution: combination sum iv 100 solution: n-ary tree preorder traversal 101 solution: triangle 102 solution: brick wall 103 solution: count binary substrings 104 solution: critical connections in a network 105 solution: rotate image 106 solution: furthest building you can reach 107 solution: power of three 108 solution: unique paths ii 109 solution: find first and last position of element in sorted array 110 solution: powerful integers 111 solution: prefix and suffix search 112 solution: course schedule iii 113 solution: running sum of 1d array 114 solution: non-decreasing array 115 solution: jump game ii 116 solution: convert sorted list to binary search tree 117 solution: delete operation for two strings 118 solution: super palindromes 119 solution: construct target array with multiple sums 120 solution: count primes 121 solution: maximum points you can obtain from cards 122 solution: range sum query 2d - immutable 123 solution: ambiguous coordinates 124 solution: flatten binary tree to linked list 125 solution: valid number 126 solution: binary tree cameras 127 solution: longest string chain 128 solution: find duplicate file in system 129 solution: minimum moves to equal array elements ii 130 solution: binary tree level order traversal 131 solution: find and replace pattern 132 solution: n-queens 133 solution: to lower case 134 solution: evaluate reverse polish notation 135 solution: partitioning into minimum number of deci-binary numbers 136 solution: maximum product of word lengths 137 solution: maximum erasure value 138 solution: n-queens ii 139 solution: maximum gap 140 solution: search suggestions system 141 solution: max area of island 142 solution: interleaving string 143 solution: maximum area of a piece of cake after horizontal and vertical cuts 144 solution: open the lock 145 solution: maximum performance of a team 146 solution: longest consecutive sequence 147 solution: min cost climbing stairs 148 solution: construct binary tree from preorder and inorder traversal 149 solution: jump game vi 150 solution: my calendar i 151 solution: stone game vii 152 solution: minimum number of refueling stops 153 solution: palindrome pairs 154 solution: maximum units on a truck 155 solution: matchsticks to square 156 solution: generate parentheses 157 solution: number of subarrays with bounded maximum 158 solution: swim in rising water 159 solution: pascal's triangle 160 solution: out of boundary paths 161 solution: redundant connection top comments (5) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand problematicdude problematicdude problematicdude follow joined sep 2, 2021 • sep 2 '21 dropdown menu copy link hide since any smaller height than the current height won't gonna make any difference. you can add following code inside the outer while loop , where h = min(height[j],height[i]) while(height[i] <= h && i < j ) i++;
while(height[j]<= h && i < j ) j--; enter fullscreen mode exit fullscreen mode it can improve run time a lot. like comment: like comment: 3 likes like comment button reply collapse expand ashwinshirva ashwinshirva ashwinshirva follow joined jul 6, 2023 • jul 23 '23 dropdown menu copy link hide excellent article. very well written. thank you! in case someone is looking for video solution for this problem you can watch this: part 1: youtube.com/watch?v=kd3irlxun6g part 2: youtube.com/watch?v=khl8cnek65a above video and this article made understand this problem so easily! thank you so much! like comment: like comment: 1 like like comment button reply collapse expand joseph taiwo joseph taiwo joseph taiwo follow a beginner who is passionate about the tech world, looking to learn new skills, and establish a career in the tech industry.

sharing my experience and tips as i go along. email emperortj128@gmail.com location lagos, nigeria work student @altschoolafrica joined sep 17, 2022 • mar 3 '23 dropdown menu copy link hide wow, i used your javascript solution and it beat 93.8% of other submissions thanks for explaining it clearly with those diagrams like comment: like comment: 1 like like comment button reply collapse expand tuananh100502 tuananh100502 tuananh100502 follow joined apr 8, 2022 • apr 8 '22 dropdown menu copy link hide can i have a main function like comment: like comment: 1 like like comment button reply collapse expand partyush partyush partyush follow joined sep 27, 2022 • sep 27 '22 dropdown menu copy link hide nice explaination, but we can improve this logic further like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse seanpgallivan follow fledgling software developer; the struggle is a rational approximation. location seattle, wa, usa education flatiron school (software engineering) work full stack software engineer joined dec 16, 2019 more from seanpgallivan solution: redundant connection # algorithms # javascript # java # python solution: out of boundary paths # algorithms # javascript # java # python solution: pascal's triangle # algorithms # javascript # java # python 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/t410/how-to-ci-cd-using-pm2-for-your-node-js-project-404f,,,"how to ci/cd using pm2 for your node.js project - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse tayyib cankat posted on feb 15, 2021 • edited on jun 29, 2022 • originally published at t410.me how to ci/cd using pm2 for your node.js project # tutorial # pm2 # node # cicd why? you have a node.js project in your local machine but you don't know how to deploy it to your remote server or you use old fashioned way by copying the contents from your computer to the remote server using ftp? well, you can automate this process and make your life easier using pm2 *insert hooray gif here* what? pm2 is a process manager for node.js. it's like task manager in windows and activity monitor in macos. you can -including but not limited to- manage your application, scale, start and stop. but the most important feature we want is deploying . in this post, we will learn how to deploy our application to our remote server and run/build it with a single console command. how? step 1: create a project first, we obviously need a project. we create a folder and cd into it. mkdir pm2-deploy; cd pm2-deploy enter fullscreen mode exit fullscreen mode then we initialize the folder as a node project. npm init -y enter fullscreen mode exit fullscreen mode we can then go ahead and install express to serve static files in node environment. npm i express enter fullscreen mode exit fullscreen mode and we need to create a js file to write our code that will serve the folder public which we also need to create. i have created index.js in the root directory. you can rename it whatever you want but don't forget that you need to change the main field in the package.json file also. we also need an html file to be served in that public folder. your file structure now looks like this: here's my index.js express is serving a static folder named public and its contents in port 3000; nothing fancy here. in index.html we do nothing special. now we can use npm start enter fullscreen mode exit fullscreen mode we should see the console.log output which is pm2 project is now live @ localhost:3000 . we can check if that's working by going to that port. go to localhost:3000 in the browser, if you see yay! that's great. step 2: install pm2 globally we need to install pm2 npm package globally. we can install it by using npm i -g pm2 enter fullscreen mode exit fullscreen mode now onto step 3! step 3: initialize git we cannot have a ci/cd without a version control system, right? so we need to push our project to a git service. i will use github for that. when you create a git repo you will see the necessary instructions on how to push an existing project. but here are the necessary commands, just in case: git init
git remote add origin git@github.com:<your_github_username>/<your_repository_name>.git
git add .
git commit -m ""initial commit""
git branch -m main
git push -u origin main enter fullscreen mode exit fullscreen mode note: i strongly recommend using ssh connection instead of using https for github. your life will get better and securerer :) step 4: configuring the remote machine in this step, i won't go into the details of how to create/reserve a virtual remote machine but keep in mind that i am using ubuntu on an ec2 (aws) machine. first, we need to connect to the remote machine using ssh ssh -i path_to_key_file remote_username@remote_ip i assume you have already done nvm, npm installations, if not you can go ahead and check the nvm repo here: https://github.com/nvm-sh/nvm#installing-and-updating one important thing to do here. we need to move the lines that were added by nvm to our .bashrc file to the top. your system may be using .bash_profile or something else. just pay attention to the output of the nvm installation. these are the lines we need to move. open your favorite editor and move them to the top of the file. export nvm_dir=""$home/.nvm""
[ -s ""$nvm_dir/nvm.sh"" ] && \. ""$nvm_dir/nvm.sh""  # this loads nvm
[ -s ""$nvm_dir/bash_completion"" ] && \. ""$nvm_dir/bash_completion""  # this loads nvm bash_completion enter fullscreen mode exit fullscreen mode after saving and exiting the file we can install pm2 here too as we did in step 2. i -g pm2 enter fullscreen mode exit fullscreen mode after the installation startup enter fullscreen mode exit fullscreen mode will give you a simple instruction on how to make pm2 start automatically every time your remote system reboots. i strongly recommend doing that. now that we installed pm2, we need to create an ssh key and add it to github. in the remote machine, you can go ahead and type ssh-keygen -t ed25519 -c ""<your_github_email>"" enter fullscreen mode exit fullscreen mode further reading on how to create ssh key https://docs.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent the keygen will ask you the name of the key. if you want to change it ( i strongly advise you not to do that ) you need to give the full path here. you may just hit enter when asking for password. after creating the key we need to copy the contents of the public key. cat /home/ubuntu/.ssh/id_ed25519.pub enter fullscreen mode exit fullscreen mode go ahead and copy the text you see starting with ssh- and ending with your e-mail (included). then go to https://github.com/settings/keys while logged in to github then click new ssh key button. you can give a title and paste the copied text into the key field. we now have given authorization to our remote machine to connect to our github. but we need to connect to github just once to mark the connection trusted in our remote machine. to do that we can clone the repository into the remote machine. git clone git@github.com:t410/pm2-deploy.git enter fullscreen mode exit fullscreen mode of course, it will be your username and your repo name. the console will ask you if you want to continue connecting. type yes end hit enter. note: we cloned the repo into the user folder. /home/ubuntu for me. now the full project path is /home/ubuntu/pm2-deploy . we will use this path to update the ecosystem.config.js file in the next step. and now we are good to close the remote connection to the server. step 5: configuring the ecosystem.config.js file now that we have a remote server up&running and have already pushed the project into our repository, we need to properly configure the ecosystem.config.js file to tell pm2 where our project is, what to do with that, and where to push that. the file will look like this: notice there are 2 sections we are exporting: apps deploy the name field in the apps section is the name of our project which will be shown in pm2 process list. the script field is the script that pm2 will run when we deploy the project to the remote server. in this case, it will be the same as the main field in the package.json file. the rest of the fields are pretty self-explanatory. user is the username that you use to connect to the remote server using ssh host is the ip of the remote server path where do you want your project to be deployed in your remote server? remember we already cloned the repo into our remote server. we can go ahead and write that path here repo is the repository url in a format like git:github.com:/user/repo.git ref is the reference branch. which branch we want the remote server to pull key is the local path of the key that we use to connect our machine using ssh ""post-deploy"" takes commands which will be run at the remote machine after pulling the repo from github step 6: deploying we have configured our machine and pm2. we can now deploy our project to the remote machine. before deploying we need to commit and push the changes we have made. after that, for the first run, we need to tell pm2 that it needs to setup the project. pm2 deploy ecosystem.config.js production setup enter fullscreen mode exit fullscreen mode with this command pm2 connects to the remote, clones the repo from github. we can now deploy the project. pm2 deploy ecosystem.config.js production enter fullscreen mode exit fullscreen mode yay! now you are asking yourself: now what? well, we didn't set up a server like nginx but we can test if the project is working or not with curl . of course, we need to connect to the remote machine before doing that. curl http://localhost:3000 enter fullscreen mode exit fullscreen mode if you see the index.html output on the screen that's great news! you have done it! and also you can list the apps pm2 running with this command pm2 ls enter fullscreen mode exit fullscreen mode conclusion we made great progress here. we learned how to deploy our project with just one command. here's the sample repo: t410 / pm2-deploy description this is a sample project that demonstrates how to configure ecosystem.config.js file for pm2 made for this tutorial https://dev.to/t410/how-to-ci-cd-using-pm2-for-your-node-js-project-404f view on github i know i didn't tell you how to install nginx and serve the port we are using for our project but i will definitely do that in the near future and update here. if you have any issues with anything, feel free to tell me what's wrong on the comments section. top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand oscar calderon oscar calderon oscar calderon follow location el salvador work senior backend developer at ny startup joined nov 12, 2019 • sep 9 '21 dropdown menu copy link hide this is really interesting, thanks!! i like to play with a very small vm i have in vulture, and i wanted something really lightweight for cd. however there's something i don't understand. it seems it is assumed that the ecosystem file will be in your local computer. in my case, i have the ecosystem file in the vm itself along with my apps. do i still need to configure the ssh key for it to deploy in itself? or is there some alternate configuration to tell it to deploy locally instead of in a remote server? like comment: like comment: 1 like like comment button reply collapse expand tayyib cankat tayyib cankat tayyib cankat follow software engineer, problem solver location istanbul work software engineer at adcolony joined jan 14, 2020 • sep 16 '21 dropdown menu copy link hide thank you for your interest. well, the ecosystem file contains the necessary information along with instructions for the machine that the app will run on. so if you want your vm to run the app with pm2, and no cd you can of course use ecosystem file without ssh configurations. if you want to deploy the app and then run it on a remote vm without connecting to that vm from your local and physical machine then you can configure ssh information and pm2 will automate the connection, pulling from git, npm installing, running the app steps. so you are letting pm2 do the deploying and running footwork. to deploy, pm2 needs an ssh key to connect to the remote machine. that ssh configuration part is essential for cd operations. i hope this clarifies your question. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse tayyib cankat follow software engineer, problem solver location istanbul work software engineer at adcolony joined jan 14, 2020 more from tayyib cankat how to deploy ssl certificate [nginx] # ssl # nginx # tutorial 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/christensenjoe/classes-in-javascript-f9g,,,"classes in javascript - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse joe christensen posted on jul 19, 2021 • edited on jul 20, 2021 classes in javascript # javascript # webdev # beginners what are classes? a class is an object-oriented programming concept that is built into many object-oriented programming languages. specifically, a class is a blueprint for creating objects, initializing data, and defining functions. it is a way of organizing information about a type of objects in a reusable format. by using a class, a programmer can create specific instances of said class and access class-specific data and functions. as an example, say a you wanted to create cat, dog, and bunny objects. you could create each individually, copying near identical code for each new object, or you could create a template called animal. then, you could simply create your cat, dog, and bunny objects by passing special information into the animal template. classes are this concept of using templates to create different but similar objects without having to re-type code. classes in javascript with the update to es6 in 2015, javascript has also gained class functionality. although it is not as fleshed out and detailed as in other object-oriented languages, it is still there none-the-less. in javascript, we declare the creation of a new class using the class keyword followed by a capitalized class name and a pair of curly brackets as shown: class animal { } enter fullscreen mode exit fullscreen mode in order to actually make use of this class, however, we need to make an instance of it. we do this by creating a variable and initializing it to our class using the new keyword like so: class animal { } let dog = new animal (); enter fullscreen mode exit fullscreen mode now we have dog , an instance of our animal class. but... what exactly can we do with it? well, what kind of specifications does every animal have? a name, species, and age. in order to add this information to our animal class, we must create a constructor like so: class animal { constructor ( name , species , age ) { this . name = name ; this . species = species ; this . age == age ; } } let dog = new animal ( "" spot "" , "" dog "" , 4 ); enter fullscreen mode exit fullscreen mode this constructor takes in the information given when initializing a new instance of the class (where we create our dog object), and sets our animal's class-specific variables, or properties , to it. let's access and log some of our dog's data by using dot notation. class animal { constructor ( name , species , age ) { this . name = name ; this . species = species ; this . age == age ; } } let dog = new animal ( "" spot "" , "" dog "" , 4 ); console . log ( dog . name ) //returns ""spot"" console . log ( dog . species ) //returns ""dog"" console . log ( dog . age ) //returns 4 enter fullscreen mode exit fullscreen mode now lets add a class-specific function that makes use of our class data to create a special message. note: class-specific functions are called methods and don't require the function keyword. class animal { constructor ( name , species , age ) { this . name = name ; this . species = species ; this . age == age ; } cutepet () { return `aww, your ${ this . species } is so cute!` ; } } let dog = new animal ( "" spot "" , "" dog "" , 4 ); console . log ( dog . cutepet ()); //returns ""aww, your dog is so cute!"" enter fullscreen mode exit fullscreen mode once initialized, an instance of a class can access its methods the same way it accesses class properties, by using dot notation. but now that we have a basic class with some variables and methods, we can test the real usefulness of classes. class animal {...} let dog = new animal ( "" spot "" , "" dog "" , 4 ); let bird = new animal ( "" feathers "" , "" bird "" , 3 ); let cat = new animal ( "" mochi "" , "" cat "" , 2 ); console . log ( dog . cutepet ()); //returns ""aww, your dog is so cute!"" console . log ( bird . cutepet ()); //returns ""aww, your bird is so cute!"" console . log ( cat . cutepet ()); //what does this return? enter fullscreen mode exit fullscreen mode we initialized some new instances of our animal class called bird and cat like shown. knowing what dog.cutepet() and bird.cutepet() return, what would calling cat.cutepet() return? you guessed it! it would return ""aww, your cat is so cute!"" . it accesses the same method we defined in our animal class, except it uses the data provided when initializing the cat instance. public vs. private most classes have the concept of public and private variables. a public variable is one that programmers are allowed to access directly, while a private one is meant to be inaccessible outside of a class's scope. making use of these, programmers can better implement one of the major practices of object-oriented programming: encapsulation . while many object oriented languages have public and private keywords, the latest version of javascript does it a little differently. javascript class properties are declared public by default, meaning that programmers only have to specify when declaring private properties. we can declare a private class variable by adding a # to the front of every property declaration. when trying to access a private class property outside the class, javascript will return a [#variablename] out of scope syntax error. check out the mdn documentation on private class properties to learn more. note: until the introduction of private class features, javascript programmers simulated private class variables by adding an underscore to the front of a property name. adding an underscore did not actually prevent programmers from accessing and changing these variables. it just lets programmers know to treat them as private. accessor methods javascript provides special built in methods called accessor methods that allow us to achieve better data quality, simpler syntax, and more security when accessing the properties of a class. two common accessors are getters and setters . getter method one common accessor method is a getter . a getter method is one that returns a value, but lets us access it like we would a property. in order to make a getter, we must make sure that our method: has a return statement uses the get keyword does not have any parameters following these conditions, we declare a getter method like so: class animal { constructor ( name , species , age ) {...} get cutepet () { return `aww, your ${ this . species } is so cute!` ; } } enter fullscreen mode exit fullscreen mode although it doesn't look like much, adding that get keyword means that we can now access that method as a property like this: class animal {...} let dog = new animal ( "" spot "" , "" dog "" , 4 ); /**
console.log(dog.cutepet()) 
we no longer need these parentheses as it isn't 
a function call anymore.
**/ console . log ( dog . cutepet ); //returns ""aww, your dog is so cute!"" enter fullscreen mode exit fullscreen mode setter method another common javascript accessor method is a setter method. a setter method follows the same logic as a getter. instead of having to invoke a given method as a function, we can access it as a property of a given class. in order to make a setter, we must make sure that our method: uses the set keyword has a single parameter lets create a new setter method inside of our animal class that changes the species of our animal. class animal { constructor ( name , species , age ) {...} get cutepet () {...} set animalname ( name ) { this . name = name ; } } enter fullscreen mode exit fullscreen mode now we can change our dog's name like so: class animal {...} let dog = new animal ( "" spot "" , "" dog "" , 4 ); console . log ( dog . name ); //returns ""spot"" dog . animalname = "" fuzzy "" ; console . log ( dog . name ); //returns ""fuzzy"" enter fullscreen mode exit fullscreen mode as you can see, we can now access our animalname setter like we would a property and ""set it equal"" to a value. that value is automatically passed as a parameter to our setter method. if we didn't have our set keyword, we would instead have to invoke the function like this: class animal {...} let dog = new animal ( "" spot "" , "" dog "" , 4 ); console . log ( dog . name ); //returns ""spot"" dog . animalname ( "" fuzzy "" ); console . log ( dog . name ); //returns ""fuzzy"" enter fullscreen mode exit fullscreen mode final statements javascript classes are powerful tools that help create better, modularized, reusable, and cleaner code. and you now have the abilities to use them! if you want to research more into classes and how they work in javascript, take a look at the mdn documentation on classes . top comments (7) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand joe christensen joe christensen joe christensen follow software engineer writing helpful blogs on dev.to. reach out if you want me to write something specific! location san diego, ca joined jul 18, 2021 • jul 20 '21 dropdown menu copy link hide thank you for the added information! i'll update my article when i get the chance. do you know when javascript actually introduced private class properties? like comment: like comment: 3 likes like comment button reply collapse expand klajdi  ajdini klajdi  ajdini klajdi  ajdini follow passionate front-end student location denmark work student joined jul 19, 2020 • jul 19 '21 dropdown menu copy link hide really nice article! like comment: like comment: 3 likes like comment button reply collapse expand joe christensen joe christensen joe christensen follow software engineer writing helpful blogs on dev.to. reach out if you want me to write something specific! location san diego, ca joined jul 18, 2021 • jul 19 '21 dropdown menu copy link hide thank you! like comment: like comment: 2 likes like comment button reply collapse expand juanfabiorey juanfabiorey juanfabiorey follow joined jul 21, 2021 • jul 21 '21 dropdown menu copy link hide thanks a lot! like comment: like comment: 2 likes like comment button reply collapse expand žarko đurić žarko đurić žarko đurić follow location bijeljina joined jul 3, 2020 • jul 20 '21 dropdown menu copy link hide any books/resources you would recommend on functional programming with javascript? like comment: like comment: 2 likes like comment button reply collapse expand jake borromeo jake borromeo jake borromeo follow joined jul 10, 2021 • jul 21 '21 dropdown menu copy link hide sometimes i see properties declared inside the constructor and methods on the outside. what is the difference between declaring methods within the constructor vs outside the constructor? like comment: like comment: 1 like like comment button reply collapse expand punk croc punk croc punk croc follow joined nov 6, 2019 • jul 22 '21 dropdown menu copy link hide bad code. it should be outsite for organization. you can call it inside the constructor if you want it to run when instantiating a class, but it's creationg should be elsewhere, in a more organized way like comment: like comment: 3 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse joe christensen follow software engineer writing helpful blogs on dev.to. reach out if you want me to write something specific! location san diego, ca joined jul 18, 2021 trending on dev community hot introducing dev education tracks: expert-guided tutorials for learning new skills and earning badges # deved # career # ai # gemini i tested gemini cli and other top coding agents - here's what i found # webdev # programming # ai # cli how to optimize images for faster loading: a developer's complete guide # webperf # javascript # programming # webdev 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/roxie/jwt-auth-exception-handling-in-laravel-8-45gd,Postman,,"jwt-auth exception handling in laravel 8 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse funke olasupo posted on apr 26, 2021 jwt-auth exception handling in laravel 8 # laravel # php # jwt are you experiencing difficulties with handling jwt-auth exceptions in laravel 8 or you're curious to know how it works? throughout this article, i will be guiding you through an easy process of understanding it. introduction jwt-auth -> (json web token authentication for laravel and lumen). jwt is mainly used for authentication. after a user logs in to an application, the application will create a jwt and send it back to the user. subsequent requests by the user will include the jwt. the token tells the server what routes, services, and resources the user is allowed to access we will be creating a basic register and login api where authorized users can fetch their information from the database with jwt implemented and then handle some exceptions. let's begin🤩, i hope you enjoy this guide. step 1: creating a new laravel project you can create a new laravel project with the following command: laravel new jwt_exception_handling enter fullscreen mode exit fullscreen mode step 2: set up model and migrations for users we can set up models and migrations simultaneously like this: php artisan make:model user -m enter fullscreen mode exit fullscreen mode ps: a user model and migration already exists because they come default with laravel in app/models and database/migrations directory respectively. set up database connection this is done in the .env file based on where you are serving your database like this: ps: these configurations are for my own local machine, yours may be different. db_connection = mysql db_host = 127.0.0.1 db_port = 3306 db_database = jwt_handling db_username = root db_password = enter fullscreen mode exit fullscreen mode next is to set up the schema for our migration but since our create_users_table.php migration file comes default with laravel, we have nothing to do here. public function up () { schema :: create ( 'users' , function ( blueprint $table ) { $table -> id (); $table -> string ( 'name' ); $table -> string ( 'email' ) -> unique (); $table -> timestamp ( 'email_verified_at' ) -> nullable (); $table -> string ( 'password' ); $table -> remembertoken (); $table -> timestamps (); }); } enter fullscreen mode exit fullscreen mode finally we can run our migrations to our database. we run migrations to our database with this artisan cli command: php artisan migrate enter fullscreen mode exit fullscreen mode step 3: set up register method user controller. we  create controllers with this artisan cli command: php artisan make:controller usercontroller enter fullscreen mode exit fullscreen mode implement the user model in the usercontroller. use app\models\user ; enter fullscreen mode exit fullscreen mode define the register() method in the usercontroller class. the register() method validates a users' input and creates a user if the user credentials are validated. public function register ( request $request ) { $this -> validate ( $request , [ 'name' => 'required' , 'email' => 'required|email|unique:users' , 'password' => 'required' ]); $user = new user ([ 'name' => $request -> input ( 'name' ), 'email' => $request -> input ( 'email' ), 'password' => bcrypt ( $request -> input ( 'password' )) ]); $user -> save (); return response () -> json ([ 'message' => 'successfully created user' ], 201 ); } enter fullscreen mode exit fullscreen mode next, we want to define the login() method but before that, we need to import jwt. step 4: import jwt-auth. to pull in the latest version of jwt-auth, run this command: composer require tymon/jwt-auth enter fullscreen mode exit fullscreen mode add the service provider to the providers array and alias to the aliases array in the config/app.php config file like this: 'providers' => [ ... tymon\jwtauth\providers\laravelserviceprovider :: class , ], 'aliases' => [ 'jwtauth' => tymon\jwtauth\facades\jwtauth :: class , ], enter fullscreen mode exit fullscreen mode run the following command to publish the package config file: php artisan vendor:publish --provider = ""tymon \j wtauth \p roviders \l aravelserviceprovider"" enter fullscreen mode exit fullscreen mode you should now have a config/jwt.php file that allows you to configure the basics of this package. there is a helper command to generate a key that will be used to sign your tokens: php artisan jwt:secret enter fullscreen mode exit fullscreen mode this will update your .env file with something like jwt_secret = (key generated). step 5: set up jwt-auth in our project. update your user model firstly,we need to implement the tymon\jwtauth\contracts\jwtsubject contract on your user model, which requires you implement 2 methods getjwtidentifier() and getjwtcustomclaims() . your user model should be updated like this: <?php namespace app\models ; use illuminate\database\eloquent\factories\hasfactory ; use illuminate\foundation\auth\user as authenticatable ; use illuminate\notifications\notifiable ; use tymon\jwtauth\contracts\jwtsubject ; class user extends authenticatable implements jwtsubject { use hasfactory , notifiable ; /**
     * the attributes that are mass assignable.
     *
     * @var array
     */ protected $fillable = [ 'name' , 'email' , 'password' , ]; /**
     * the attributes that should be hidden for arrays.
     *
     * @var array
     */ protected $hidden = [ 'password' , 'remember_token' , ]; /**
     * the attributes that should be cast to native types.
     *
     * @var array
     */ protected $casts = [ 'email_verified_at' => 'datetime' , ]; /**
     * return a key value array, containing any custom claims to be added to the jwt.
     *
     * @return array
     */ public function getjwtcustomclaims () { return []; } /**
     * get the identifier that will be stored in the subject claim of the jwt.
     *
     * @return mixed
     */ public function getjwtidentifier () { return $this -> getkey (); } } enter fullscreen mode exit fullscreen mode next, we need to configure auth gaurd you'll need to update the config/auth.php file with these changes to configure laravel to use the jwt guard to power your application authentication. ps: update only these arrays and leave the others. 'defaults' => [ 'guard' => 'api' , 'passwords' => 'users' , ], ... 'guards' => [ 'api' => [ 'driver' => 'jwt' , 'provider' => 'users' , ], ], enter fullscreen mode exit fullscreen mode step 6: set up login method in usercontroller first we will implement jwt and its exception to our usercontroller. use jwtauth ; use tymon\jwtauth\exceptions\jwtexception ; enter fullscreen mode exit fullscreen mode now back to our usercontroller class, we define the login() method for users to login. this will validate users' input. the auth method accepts a request containing an email and password which will be checked against the user database for authentication. once you authenticate, the controller returns a jwt that you need to keep. public function login ( request $request ) { $this -> validate ( $request , [ 'email' => 'required|email' , 'password' => 'required' ]); $credentials = $request -> only ( 'email' , 'password' ); try { if ( ! $token = jwtauth :: attempt ( $credentials )) { return response () -> json ([ 'error' => 'invalid credentials' ], 401 ); } } catch ( jwtexception $e ) { return response () -> json ([ 'error' => 'could not create token' ], 500 ); } return response () -> json ([ 'token' => $token ], 200 ); } enter fullscreen mode exit fullscreen mode step 7: set up getuser method in the usercontroller. still, in our usercontroller, we define getuser() method which is responsible for getting users' personal information from the database. public function getuser (){ $user = auth ( 'api' ) -> user (); return response () -> json ([ 'user' => $user ], 201 ); } enter fullscreen mode exit fullscreen mode step 8: exception handling firstly, we create a route middleware to parse the token of an authenticated user.
we make a middleware with this artisan cli command: php artisan make:middleware jwtmiddleware enter fullscreen mode exit fullscreen mode a class of jwtmiddleware at app/http/middleware/jwtmiddleware.php directory will be created. next, we implement jwtauth in the jwtmiddleware.php like this: use jwtauth ; enter fullscreen mode exit fullscreen mode the middleware comes with a default method handle and now we parse the generated token of an authenticated user like this: public function handle ( request $request , closure $next ) { $user = jwtauth :: parsetoken () -> authenticate (); return $next ( $request ); } enter fullscreen mode exit fullscreen mode add the middleware to the array of routemiddleware in the kernel.php like this: protected $routemiddleware = [ ....... 'auth.jwt' => \app\http\middleware\jwtmiddleware :: class , ]; enter fullscreen mode exit fullscreen mode exception handling in handlers class. we will create the handlers for our exception in handler.php in app/exceptions directory. our exceptions will be in the register() method that comes default with handler.php . we will be handling the following exceptions: invalid token, expired token, jwtexception but we need to implement these exceptions in handler.php . use response ; use exception ; use throwable ; use tymon\jwtauth\exceptions\jwtexception ; use tymon\jwtauth\exceptions\tokenexpiredexception ; use tymon\jwtauth\exceptions\tokeninvalidexception ; enter fullscreen mode exit fullscreen mode now, to the register() method: public function register () { $this -> renderable ( function ( tokeninvalidexception $e , $request ){ return response :: json ([ 'error' => 'invalid token' ], 401 ); }); $this -> renderable ( function ( tokenexpiredexception $e , $request ) { return response :: json ([ 'error' => 'token has expired' ], 401 ); }); $this -> renderable ( function ( jwtexception $e , $request ) { return response :: json ([ 'error' => 'token not parsed' ], 401 ); }); } enter fullscreen mode exit fullscreen mode step 9: set up routes. seeing we're are creating an api, we will set up our routes in routes/api.php . we need to implement our usercontroller first. use app\http\controllers\usercontroller ; enter fullscreen mode exit fullscreen mode then we set up our routes. we will be adding the jwtmiddleware to the routes for getting users' details, this way we can see the work of exception handling. route :: post ( '/register' , [ usercontroller :: class , 'register' ]); route :: post ( '/login' , [ usercontroller :: class , 'login' ]); route :: get ( '/user' , [ usercontroller :: class , 'getuser' ]) -> middleware ( 'auth.jwt' ); enter fullscreen mode exit fullscreen mode congratulations, you have successfully built an api with jwt-auth😍, now let us test it with postman. step 10: test with postman. this shows that our registration works with valid input. this shows that our login works with valid input. now to get users' information, we need to pass the token alongside the request and we do this by setting authorization to bearer (token) in our headers like this: let's verify if our exception handling works. for jwtexception, we do not pass token with the request: for tokeninvalidexception, we put in the wrong token: for tokenexpiredexception, we resend that token after a long period of time depending on the time to live for the token. congratulations on building an api and implementing jwt-auth and also handling some exceptions😍. guess what?😊 the code for this practice is open-source here on my github . these are my humble opinions so please, if you have any contradicting or buttressing opinions, do well to reach me on twitter . if you need more information on jwt-auth, visit the official documentation . thank you for reading till the end🤝🤩😍 top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand tim eckel tim eckel tim eckel follow joined jan 10, 2024 • jan 10 '24 dropdown menu copy link hide doesn't work.  when signing in it throws the following error: tymon\jwtauth\jwtguard::login(): argument #1 ($user) must be of type tymon\jwtauth\contracts\jwtsubject, illuminate\auth\genericuser given, called in /var/www/html/vendor/tymon/jwt-auth/src/jwtguard.php on line 124 like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse funke olasupo follow backend engineer | technical author location lagos, nigeria joined mar 25, 2021 more from funke olasupo oauth authentication in laravel: social login with laravel socialite # laravel # php # oauth send emails in laravel 8 using gmail's smtp server # laravel # php # tutorial how to fetch resources randomly in laravel with a single line of code. # laravel # codenewbie # 100daysofcode 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/wasinaseer/difference-between-aggregation-and-composition-1p1h,,,"difference between aggregation and composition - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse muhammad wasi naseer posted on aug 9, 2021 difference between aggregation and composition # java # aggregation # composition # oop aggregation and composition are basically the concepts about how to manage objects. aggregation aggregation uses loose coupling , which means that it does not hold the actual object in the class. instead, the object is being passed via getter or setter function. it gives the benefit that when the object of the parent class dies then the child class object remains intact. class house { private table table ; public void settable ( table table ){ this . table = table } public table gettable (){ return this . table ; } } public class main ( string [] args ){ table table = new table (); // this house' table is set externally and will not be deleted // upon deleting this house object house house = new house (); house . settable ( table ); } enter fullscreen mode exit fullscreen mode composition composition uses tight coupling , which means that it holds the object. an instance variable is created and the object is stored in it. whenever the object of the parent class removed/deleted then the child class object will also be deleted. class human { // this heart will be deleted and nowhere to access once its // human gets deleted private heart heart = new heart (); } public class main ( string [] args ){ // this human's heart is set internally and will be deleted // upon deleting this human object human human = new human (); } enter fullscreen mode exit fullscreen mode top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse muhammad wasi naseer follow sde @ amazon | java backend engineer location madrid, spain education bachelors in computer science (iba) work software development engineer @ amazon joined oct 16, 2020 more from muhammad wasi naseer what are solid principles? # design # solid # designpatterns # java why strings are immutable in java? # java # strings # programming memory management in java | heap vs stack # heap # stack # memory # java 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/fluffynuts/orthogonality-2d2p,,,"orthogonality - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse davyd mccoll posted on jun 8, 2020 orthogonality # pragmatic # programmer # abstraction definition orthogonality is a mathematical term referring to lines which are 90° to each other: seems abstract the concept applied to software engineering may seem initially abstract, but the idea is quite simple: the axes on a graph are orthogonal to each other and provide the example we need: if we consider two axes, x and y: we can see that if we only alter x, moving in any direction, we don't require a change in y. this is because x and y are orthogonal, and this is the point we're getting to in software terms: orthogonality in software refers to parts of the system which are not directly related. often these parts interact with each other, but changes to one do not directly affect the other. getting practical a common example of orthogonality is your business logic versus how your data is stored. for a business which sells items and needs to generate invoices, it doesn't really matter where we store invoices and line items -- flat files (never underestimate the humble flat file!), local database, amazon s3 buckets, whatever. this is a common place where the principle applies. for example, if you're using entity framework (.net) or knex (node), you could quite easily swap out the method of storage (mssql, mysql, postgres) or even radically change the method of storage (mongo, couchdb) without having to change other layers in the app -- if you've ensured that there's a layer of abstraction in between those interacting layers. consider: if we write traditional ado.net code (with newer c# syntax, because it's neater!): var customers = new list < customer >(); using var conn = new sqlconnection ( configurationmanager . connectionstrings [ ""main"" ]. connectionstring ); using var cmd = conn . createcommand (); cmd . commandtext = ""select * from customers where is_trial = 1"" ; using var reader = cmd . executereader (); while ( reader . next ()) { customers . add ( maprowtocustomer ( reader )); } return customers ; enter fullscreen mode exit fullscreen mode then we're quite tied into using mssql and ado.net here, especially if we have code like this scattered throughout the codebase. if you were using ef, this would be hidden from you: no need for each part of the code to know where to find a configured connection string no need to know how to open a connection no need to know about the command/reader semantics of ado.net no need to dispose all of those (the clean new syntax hides 3 dispose calls!) no need to know the correct sql dialect to perform the query for example: using var db = new context (); return db . customers . where ( c => c . istrial ) . tolist (); enter fullscreen mode exit fullscreen mode as with everything, there are trade-offs -- at codeo, we take a bit of a middle stance, using a cqrs pattern with commands / queries which have explicit sql in them, so there are definitely parts of the code which are coupled with the database implementation. this has been done in the name of performance. but the rest of those concerns are tucked away in a common set of code. another common example is presentation (eg html, css, xaml) vs logic (eg javascript, c#, sql). even in the logic arena, we can have well-defined areas of abstraction which interact but changes in one don't (or shouldn't) cause changes in another. a counter-example in the real-world is the control system of an helicopter: changes along one axis of movement require compensation along others. becoming an helicopter pilot is no trivial task! orthogonality goes hand-in-hand with keeping things dry because as common concerns are abstracted from code, we often find that a good abstraction will make the consumer unaware of the nuts-and-bolts of the code at the core of that abstraction. benefits of observing orthogonality isolation of common logic allows for across-the-board improvements or changes without affecting other parts of the system avoid vendor lock-in: if you've kept orthogonal concerns well-separated with abstract interfaces between them, it will be easier to swap out parts when necessary to address concerns of pricing or performance easier to test: when you don't have to spin up an entire functional ""world"" for tests, you can write smaller, faster tests that are: more likely to be run (faster) less likely to break when the system changes (maintainable) will better show where issues are introduced (more focused) design whilst programming for orthogonality, observe: the ""s"" in solid (single responsibility) if a piece of code is doing too much, chances are good it's crossing orthogonal boundaries layering of your application instead of shoving all code into one giant file, consider the many types of layered approaches which exist, some of the simplest and well-known being: onion architecture mvc / mvvm this layering can be applied to documentation too consider keeping presentation and content separate: write your documentation in a markup language which can be rendered neatly later (eg markdown) keep code de-coupled reveal the least necessary to let parts of the system interact with each other consider the law of demeter instead of changing state on objects, ask them to do so; in this way, when a behavioral change goes hand-in-hand with that state change, it can be accomplished within the class where the change is required, instead of outside code having to do so avoid global data global data becomes a point of coupling between different parts of the system. if you have a global static applicationsettings class which everyone is looking at, you can't test parts of the app in parallel with different configurations. if you have an injectable iapplicationsettings object, you can fake out the values for settings without having to actually back them from a database or wherever your settings come from. the settings themselves are orthogonal to the storage! the takeaway whilst you're busy solving all of your requirements, always keep a look-out for orthogonal concerns and see how you can keep them from becoming entangled. this will provide you more freedom to change your system when required to do so. you will find it easier to keep orthogonal code apart and dry if you're constantly critical of your own code. always look for neater ways to get things done. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse davyd mccoll follow code monkey extraordinaire location durban work full stack dev at codeo joined oct 23, 2018 more from davyd mccoll keeping dry # pragmatic # programmer # duplication efficient, eloquent email # pragmatic # programmer # communication # email communication is the key # pragmatic # programmer # communication 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/pavel_polivka/using-java-inner-classes-for-jackson-serialization-4ef8,,,"using java inner classes for jackson serialization - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse pavel polívka posted on dec 15, 2020 using java inner classes for jackson serialization # java i recently picked up some work in progress done by a collegues not longer with a company. he wrote a lot of code, that probably never really compiled and my task was make sense of it and finish his work. as part of making sense of it i made it compile. next step was to run it. here i ran into ugly jackson serialization exceptions. jist of the error massages was this. can only instantiate non-static inner class by using default, no-argument constructor enter fullscreen mode exit fullscreen mode strange i did not see any new classes without default constructor. i was digining in it for few seconds and figure out the source of my issues were inner classes added into some dto classes. these inner classes did not have any constructors but they still were source of my issues. tl;dr just add static keyword to your inner class. class exampleclass { static class innerclass { public int getcount () { return 42 ; } } } enter fullscreen mode exit fullscreen mode how so? java has few types of inner classes. anonymous, static and non-static inner classes. the non static inner classes (including the anonymous ones) have set of hidden variables passed by hidden constructor, all done at compile time. so if you have something like this class exampleclass { class innerclass { public int getcount () { return 42 ; } } } enter fullscreen mode exit fullscreen mode compiler will produce something like this public class exampleclass { ... } class exampleclass $innerclass { private final exampleclass parent ; exampleclass $innerclass ( exampleclass p ) { parent = p ; } public int getcount () { return 42 ; } } enter fullscreen mode exit fullscreen mode why the hell is compiler doing something like this? this is part of the magic why inner classes can access all members of enclosing class. including the private ones. static inner classes are just normal classes without any abilities to read members of encosling classes. they do not have any hidden constructors, etc... that is why when you want to have inner class part of your jackson serialiaziation it needs to be inner static class. jackson is refusing to work with non static classes as there is no easy way to instantiate those. for more java tips you can follow me on twitter top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse pavel polívka follow father, developer, bad sci-fi writer, love to learn stuff location prague work senior software developer joined aug 27, 2020 more from pavel polívka what to expect from java interview in 2021 # java # interview # career debugging maven tests in intellij idea # java # intellij # maven how to get thread dump from heap dump # java # jvm # tooling 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/mistersingh179/rails-like-scope-methods-in-objection-js-nodejs-orm-dgj,,,"rails like scope methods in objection.js (nodejs orm) - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse mister singh posted on mar 10, 2020 rails like scope methods in objection.js (nodejs orm) # node # javascript # orm # rails let say we have a model called label const { model } = require('objection')
class label extends model {
    static get tablename() {
        return ""labels""
    }
    static get jsonschema () {
        return {
            type: 'object',
            required: [],
            properties: {
                id: { type: 'integer' },
                name: { type: 'string' }
            }
        }
    }
} now we want to get the last label in the model. const label = await label.query().orderby('id', 'desc').limit(1).first() although this gets us the last label, it has a few shortcomings: it is verbose it requires too much repeated typing and thus prone to errors its harder to test it doesn't read well and things only get worse when its used in conjunction with other methods here are 3 ways to approach this: modifiers a regular class method custom querybuilder object lets dive into each of these one-by-one. approach 1: modifiers modifiers is my preferred way to solve this. we specify a function on the modifiers object which: receives the query as a param it then modifies the query by adding its filters etc. label.modifiers.last = query => {
    query.orderby('id', 'desc').limit(1).first()
} now lets get the last record by using this modifier const label = await label.query().modify('last') this reads so much better, encapsulates all logic under one function and we can test that one function easily. the logs show that it ran: select ""labels"".* from ""labels"" order by ""id"" desc limit 1 with params lets build another modifier which gets all labels which start with the passed in letters label.modifiers.startswith = (query, letters) => {
    query.where('name', 'like', `${letters}%`)
} now lets run it labels = await label.query().modify('startswith', 'xyyz') and logs show: select ""labels"".* from ""labels"" where ""name"" like ""ac%"" combining multiple modifier functions this is where i think modifier functions start to shine, just like scopes do in rails. so lets say we need the last label which starts with 'a'. we can achieve this by using our startswith & last modifier functions together. const label = await label.query().modify('startswith','a').modify('last') and our logs have: select ""labels"".* from ""labels"" where ""name"" like ""a%"" order by ""id"" desc limit 1 approach 2: class method on label a regular static method on label class. we can have this method return the last record: label.last = () => {
    return await label.orderby('id', 'desc').limit(1).first()
} this gets the job done, but not as good as a modifier function. yes it reads good and encapsulates the work but it doesn't return the query object and thus can't be chained approach 3: custom querybuilder we can build our custom query object and have label class use our query object. on our custom query object we can define a custom methods which modify the query() object directly. this will allow us to modify the query by calling an internal method of the query object, without writing the words modify and explicitly making it clear that we are modifying the query. lets see an example: class myquerybuilder extends querybuilder {
  last () {
    logger.info('inside last')
    this.orderby('id', 'desc').limit(1).first()
    return this
  }
}

class label exteds model {
    static get querybuilder () {
        return myquerybuilder
    }
} now to use it: cons label = await label.query().last() i think this approach is an abuse of power. it works, but we have a cleaner way of modifying the query and we should do that instead of defining a custom query object which has special internal methods. i think this custom query class might have good use cases though for other things like logging, making some other service calls etc. conclusion modifiers are great. the ability to chain them make them an asset. what's next use modifiers with complex queries which use: join graphfetch (eager loading) use ref where we have ambiguous table names top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse mister singh follow null !== undefined joined feb 13, 2020 trending on dev community hot comparing qodo and github copilot 🕵️ # ai # vscode # productivity # programming where is the vibe in ""vibe coding"" and what happened to music monday? # watercooler # ai # webdev # productivity how to optimize images for faster loading: a developer's complete guide # webperf # javascript # programming # webdev 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/frosnerd/writing-my-own-boot-loader-3mld,,,"writing my own boot loader - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse frank rosner posted on nov 7, 2020 • edited on aug 13, 2024 writing my own boot loader # assembler # x86 # os # c writing my own operating system (5 part series) 1 writing my own boot loader 2 writing my own vga driver 3 writing my own keyboard driver 4 writing my own shell 5 writing my own dynamic memory management series introduction i have recently watched preventing the collapse of civilization by jonathan blow - amazing talk - and was curious if i would be able to write an operating system from scratch. so i googled a bit and stumbled upon the comprehensive os tutorial by carlos fenollosas, which is based on the very well written lecture writing a simple operating system — from scratch . so i set myself the goal to write an x86, 32 bit operating system. to make sure i really understood all the details, i decided to blog about my progress. so here goes the first blog post. we will write a simple boot loader from scratch, using x86 assembly language and load a very minimal operating system kernel written in c. for the sake of simplicity we will utilize bios and not mess with uefi. the post is structured as follows. before we jump into the details, it might make sense to look some things up in order to be able to follow my brief explanations. consequently, the next sections contains some key words that you can read up on. afterwards we are going to write our boot loader step by step. we then implement our minimalistic kernel written in c. in the last section we will wire everything together and boot our very own operating system. the source code can be found at github. prerequisites in order to keep this post short i am going to focus on what is most important to achieve our goal. this means that some things will be left unexplained. however, if you spend some time to read up on them in more detail in the course of this post, you should be able to follow along just fine. here is a list of topics that are useful to know / to read up on in order to understand the content of this post. basic understanding of computer architecture (cpu, memory, disk, bios) basic understanding of how to interact with x86 cpus ( x86 architecture , x86 assembler ) basic understanding of how to compile a c program (make, gcc, ld) in terms of tooling we will need an emulator ( qemu ) to run our operating system, an x86 assembler ( nasm ) to write our boot loader code, as well as a c compiler ( gcc ) and linker ( ld ) in order to create an executable operating system kernel. we will wire everything together using gnu make . tasks of a boot loader on an x86 machine, the bios selects a boot device, then copies the first sector from the device into physical memory at memory address 0x7c00. in our case this so called boot sector will hold 512 bytes. these 512 bytes contain the boot loader code, a partition table, the disk signature, as well as a ""magic number"" that is checked by the bios to avoid accidentally loading something that is not supposed to be a boot sector. the bios then instructs the cpu to jump to the beginning of the boot loader code, essentially passing control to the boot loader. in this tutorial we will be only concerned about the boot loader code, which will start the operating system kernel. this is necessary because we will not be able to fit the whole operating system into 512 bytes. in order to start our kernel, the boot loader will have to perform the following tasks: loading the kernel from disk into memory. setting up the global descriptor table (gdt). switching from 16 bit real mode to 32 bit protected mode and passing control to the kernel. organizing the codebase we are going to write the boot loader in x86 assembly using nasm. the kernel will be written in c. we will organize the code in multiple files to increase readability and modularity. the following files will be relevant for a minimal setup: mbr.asm is the main file defining the master boot record (512 byte boot sector) disk.asm contains code to read from disk using bios gdt.asm sets up the gdt switch-to-32bit.asm contains code to switch to 32 bit protected mode kernel-entry.asm contains assembler code to hand over to our main function in kernel.c kernel.c contains the main function of the kernel makefile wires the compiler, linker, assembler and emulator together so we can boot our operating system the next section focuses on writing the boot loader related files ( mbr.asm , disk.asm , gdt.asm , and switch-to-32bit.asm ). afterwards we will write the kernel and the entry file. finally, we are going to write everything together and attempt to boot. writing the boot loader master boot record file the main assembly file for the boot loader contains the definition of the master boot record, as well as include statements for all relevant helper modules. let's first take a look at the file as a whole and then discuss each section individually. [bits 16]
[org 0x7c00]

; where to load the kernel to
kernel_offset equ 0x1000

; bios sets boot drive in 'dl'; store for later use
mov [boot_drive], dl

; setup stack
mov bp, 0x9000
mov sp, bp

call load_kernel
call switch_to_32bit

jmp $

%include ""disk.asm""
%include ""gdt.asm""
%include ""switch-to-32bit.asm""

[bits 16]
load_kernel:
    mov bx, kernel_offset ; bx -> destination
    mov dh, 2             ; dh -> num sectors
    mov dl, [boot_drive]  ; dl -> disk
    call disk_load
    ret

[bits 32]
begin_32bit:
    call kernel_offset ; give control to the kernel
    jmp $ ; loop in case kernel returns

; boot drive variable
boot_drive db 0

; padding
times 510 - ($-$$) db 0

; magic number
dw 0xaa55 enter fullscreen mode exit fullscreen mode the first thing to notice is that we are going to switch between 16 bit real mode and 32 bit protected mode so we need to tell the assembler whether it should generate 16 bit or 32 bit instructions. this can be done by using the [bits 16] and [bits 32] directives , respectively. we are starting off with 16 bit instructions as the bios jumps to the boot loader while the cpu is still in 16 bit mode. in nasm, the [org 0x7c00] directive sets the assembler location counter. we specify the memory address where the bios is placing the boot loader. this is important when using labels as they will have to be translated to memory addresses when we generate machine code and those addresses need to have the correct offset. the kernel_offset equ 0x1000 statement defines an assembler constant called kernel_offset with the value 0x1000 which we will use later on when loading the kernel into memory and jumping to its entry point. preceding the boot loader invocation, the bios stores the selected boot drive in the dl register. we are storing this information in memory inside the boot_drive variable so we can use the dl register for something else without the risk of overwriting this information. before we can call the kernel loading procedure, we need to setup the stack by setting the stack pointer registers sp (top of stack, grows downwards) and bp (bottom of stack). we will place the bottom of the stack in 0x9000 to make sure we are far away enough from our other boot loader related memory to avoid collisions. the stack will be used, e.g., by the call and ret statements to keep track of memory addresses when executing assembly procedures. now the time has come to do some work! we will first call the load_kernel procedure to instruct the bios to load the kernel from disk into memory at the kernel_offset address. load_kernel makes use of our disk_load procedure which we will write later. this procedure takes three input parameters: the memory location to place the read data into ( bx ) the number of sectors to read ( dh ) the disk to read from ( dl ) as soon as we are done we will return to the next instruction call switch_to_32bit , which calls another helper procedure that we will write later. it will prepare everything needed in order to switch to 32 bit protected mode, perform the switch, and jump to the begin_32bit label when it is done, effectively passing control to the kernel. this concludes our main boot loader code. in order to generate a valid master boot record, we need to include some padding by filling up the remaining space with 0 bytes times 510 - ($-$$) db 0 and the magic number dw 0xaa55 . next, let's see how the disk_load procedure is defined so we can read our kernel from disk. reading from disk reading from disk is rather easy when working in 16 bit mode, as we can utilize bios functionality by sending interrupts. without the help of the bios we would have to interface with the i/o devices such as hard disks or floppy drives directly, making our boot loader way more complex. in order to read data from disk, we need to specify where to start reading, how much to read, and where to store the data in memory. we can then send an interrupt signal ( int 0x13 ) and the bios will do its work, reading the following parameters from the respective registers: register parameter ah mode (0x02 = read from disk) al number of sectors to read ch cylinder cl sector dh head dl drive es:bx memory address to load into (buffer address pointer) if there are disk errors, bios will set the carry bit. in that case we should usually show an error message to the user but since we did not cover how to print strings and we are not going to in this post, we will simply loop indefinitely. let's take a look at the contents of disk.asm now. disk_load:
    pusha
    push dx

    mov ah, 0x02 ; read mode
    mov al, dh   ; read dh number of sectors
    mov cl, 0x02 ; start from sector 2
                 ; (as sector 1 is our boot sector)
    mov ch, 0x00 ; cylinder 0
    mov dh, 0x00 ; head 0

    ; dl = drive number is set as input to disk_load
    ; es:bx = buffer pointer is set as input as well

    int 0x13      ; bios interrupt
    jc disk_error ; check carry bit for error

    pop dx     ; get back original number of sectors to read
    cmp al, dh ; bios sets 'al' to the # of sectors actually read
               ; compare it to 'dh' and error out if they are !=
    jne sectors_error
    popa
    ret

disk_error:
    jmp disk_loop

sectors_error:
    jmp disk_loop

disk_loop:
    jmp $ enter fullscreen mode exit fullscreen mode the main part of this file is the disk_load procedure. recall the input parameters we set in mbr.asm : the memory location to place the read data into ( bx ) the number of sectors to read ( dh ) the disk to read from ( dl ) first thing every procedure should do is to push all general purpose registers ( ax , bx , cx , dx ) to the stack using pusha so we can pop them back before returning in order to avoid side effects of the procedure. additionally we are pushing the number of sectors to read (which is stored in the high part of the the dx register) to the stack because we need to set dh to the head number before sending the bios interrupt signal and we want to compare the expected number of sectors read to the actual one reported by bios to detect errors when we are done. now we can set all required input parameters in the respective registers and send the interrupt. keep in mind that bx and dl are already set correctly by the caller. as the goal is to read the next sector on disk, right after the boot sector, we will read from the boot drive starting at sector 2, cylinder 0, head 0. after the int 0x13 has been executed, our kernel should be loaded into memory. to make sure there were no problems, we should check two things: first, whether there was a disk error (indicated by the carry bit) using a conditional jump based on the carry bit jc disk_error . second, whether the number of sectors read (set as a return value of the interrupt in al ) matches the number of sectors we attempted to read (popped from stack into dh ) using a comparison instruction cmp al, dh and a conditional jump in case they are not equal jne sectors_error . in case something went wrong we will run into an infinite loop. if everything went fine, we are returning from the procedure back to the main function. the next task is to prepare the gdt so we can switch to 32 bit protected mode. global descriptor table (gdt) as soon as we leave 16 bit real mode, memory segmentation works a bit differently. in protected mode, memory segments are defined by segment descriptors, which are part of the gdt . for our boot loader we will setup the simplest possible gdt, which resembles a flat memory model. the code and the data segment are fully overlapping and spanning the complete 4 gb of addressable memory. our gdt is structured as follows: a null segment descriptor (eight 0-bytes). this is required as a safety mechanism to catch errors where our code forgets to select a memory segment, thus yielding an invalid segment as the default one. the 4 gb code segment descriptor. the 4 gb data segment descriptor. a segment descriptor is a data structure containing the following information: base address: 32 bit starting memory address of the segment. this will be 0x0 for both our segments. segment limit: 20 bit length of the segment. this will be 0xfffff for both our segments. g (granularity): if set, the segment limit is counted as 4096-byte pages. this will be 1 for both of our segments, transforming the limit of 0xfffff pages into 0xfffff000 bytes = 4 gb. d (default operand size) / b (big): if set, this is a 32 bit segment, otherwise 16 bit. 1 for both of our segments. l (long): if set, this is a 64-bit segment (and d must be 0 ). 0 in our case, since we are writing a 32 bit kernel. avl (available): can be used for whatever we like (e.g. debugging) but we are just going to set it to 0 . p (present): a 0 here basically disables the segment, preventing anyone from referencing it. will be 1 for both of our segments obviously. dpl (descriptor privilege level): privilege level on the protection ring required to access this descriptor. will be 0 in both our segments, as the kernel is going to access those. type: if 1 , this is a code segment descriptor. set to 0 means it is a data segment. this is the only flag that differs between our code and data segment descriptors. for data segments, d is replaced by b, c is replaced by e and r is replaced by w. c (conforming): code in this segment may be called from less-privileged levels. we are setting this to 0 to protect our kernel memory. e (expand down): whether the data segment expands from the limit down to the base. only relevant for stack segments and set to 0 in our case. r (readable): set if the code segment may be read from. otherwise it can only be executed. set to 1 in our case. w (writable): set if the data segment may be written to. otherwise it can only be read. set to 1 in our case. a (accessed): this flag is set by the hardware when the segment is accessed, which can be useful for debugging. unfortunately the segment descriptor does not contain these values in a linear fashion but instead they are scattered across the data structure. this makes it a bit tedious to define the gdt in assembly. please consult the diagram below for a visual representation of the data structure. in addition to the gdt itself we also need to setup a gdt descriptor. the descriptor contains both the gdt location (memory address) as well as its size. enough theory, let's look at the code! below you can find our gdt.asm , containing the definition of the gdt descriptor and the two segment descriptors, along with two assembly constants in order for us to know where the code segment and the data segment are located inside of the gdt. ;;; gdt_start and gdt_end labels are used to compute size

; null segment descriptor
gdt_start:
    dq 0x0

; code segment descriptor
gdt_code:
    dw 0xffff    ; segment length, bits 0-15
    dw 0x0       ; segment base, bits 0-15
    db 0x0       ; segment base, bits 16-23
    db 10011010b ; flags (8 bits)
    db 11001111b ; flags (4 bits) + segment length, bits 16-19
    db 0x0       ; segment base, bits 24-31

; data segment descriptor
gdt_data:
    dw 0xffff    ; segment length, bits 0-15
    dw 0x0       ; segment base, bits 0-15
    db 0x0       ; segment base, bits 16-23
    db 10010010b ; flags (8 bits)
    db 11001111b ; flags (4 bits) + segment length, bits 16-19
    db 0x0       ; segment base, bits 24-31

gdt_end:

; gdt descriptor
gdt_descriptor:
    dw gdt_end - gdt_start - 1 ; size (16 bit)
    dd gdt_start ; address (32 bit)

code_seg equ gdt_code - gdt_start
data_seg equ gdt_data - gdt_start enter fullscreen mode exit fullscreen mode with the gdt and the gdt descriptor in place, we can finally write the code that performs the switch to 32 bit protected mode. switching to protected mode in order to switch to 32 bit protected mode so that we can hand over control to our 32 bit kernel, we have to perform the following steps: disable interrupts using the cli instruction. load the gdt descriptor into the gdt register using the lgdt instruction. enable protected mode in the control register cr0 . far jump into our code segment using jmp . this needs to be a far jump so it flushes the cpu pipeline, getting rid of any prefetched 16 bit instructions left in there. setup all segment registers ( ds , ss , es , fs , gs ) to point to our single 4 gb data segment. setup a new stack by setting the 32 bit bottom pointer ( ebp ) and stack pointer ( esp ). jump back to mbr.asm and give control to the kernel by calling our 32 bit kernel entry procedure. now let's translate that into assembly so we can write switch-to-32bit.asm : [bits 16]
switch_to_32bit:
    cli                     ; 1. disable interrupts
    lgdt [gdt_descriptor]   ; 2. load gdt descriptor
    mov eax, cr0
    or eax, 0x1             ; 3. enable protected mode
    mov cr0, eax
    jmp code_seg:init_32bit ; 4. far jump

[bits 32]
init_32bit:
    mov ax, data_seg        ; 5. update segment registers
    mov ds, ax
    mov ss, ax
    mov es, ax
    mov fs, ax
    mov gs, ax

    mov ebp, 0x90000        ; 6. setup stack
    mov esp, ebp

    call begin_32bit        ; 7. move back to mbr.asm enter fullscreen mode exit fullscreen mode after switching the mode we are ready to hand over control to our kernel. let's implement a dummy kernel in the next section. writing a dummy kernel c kernel having our basic boot loader functionality up and running we only need to create a small dummy kernel function in c that we can call from our boot loader. although leaving the 16 bit real mode means we will not have the bios at our disposal anymore and we need to write our own i/o drivers, we now have the ability to write code in a higher order language like c! this means we do not have to rely on assembly language anymore. for now the task of the kernel will be to output the letter x in the top left corner of the screen. to do that we will have to modify video memory directly. for color displays with vga text mode enabled the memory begins at 0xb8000 . each character consists of 2 bytes: the first byte represents the ascii encoded character, the second byte contains color information. below is a simple main function inside kernel.c that prints an x in the top left corner of our screen. void main () { char * video_memory = ( char * ) 0xb8000 ; * video_memory = 'x' ; } enter fullscreen mode exit fullscreen mode kernel entry when you take a look back into our mbr.asm , you will notice that we still need to call the main function written in c. to do that, we are going to create a small assembly program that will be placed at the kernel_offset location, in front of the compiled c kernel when creating the boot image. let's look at the contents of kernel-entry.asm : [bits 32]
[extern main]
call main
jmp $ enter fullscreen mode exit fullscreen mode as expected there is not much to do here. we only want to call our main function. to avoid errors in the assembly process, we need to declare main as an external procedure that is not defined within our assembly file. it is the task of the linker to resolve the memory address of main such that we can call it successfully. it is important to remember that the kernel-entry.asm is not included into our mbr.asm but will be placed at the front of the kernel binary in the course of the next section. so let's see how we can combine all the pieces we built. putting everything together in order to built our operating system image we are going to need a bit of tooling. we need nasm to process our assembly files. we need gcc to compile our c code. we need ld to link our compiled kernel object files and our compiled kernel entry into a binary file. and we are going to use cat to combine our master boot record and our kernel binary into a single, bootable binary image. but how do we wire all those neat little tools together? luckily there is another tool for that: make . so here goes the makefile : # $@ = target file
# $< = first dependency
# $^ = all dependencies # first rule is the one executed when no parameters are fed to the makefile all : run kernel.bin : kernel-entry.o kernel.o ld -m elf_i386 -o $@ -ttext 0x1000 $^ --oformat binary kernel-entry.o : kernel-entry.asm nasm $< -f elf -o $@ kernel.o : kernel.c gcc -m32 -ffreestanding -c $< -o $@ mbr.bin : mbr.asm nasm $< -f bin -o $@ os-image.bin : mbr.bin kernel.bin cat $^ > $@ run : os-image.bin qemu-system-i386 -fda $< clean : $( rm ) * .bin * .o * .dis enter fullscreen mode exit fullscreen mode it is important to note that you might have to cross compile ld and gcc in order to be able to compile and link into free standing x86 machine code. i had to do it on my mac at least. now let's compile, assemble, link, load our image into qemu , and look at the beautiful x in the top left corner of the screen! we made it! the next step will be to write some drivers so we can interface with our i/o devices but this will be covered in the next post :) cover image by michael dziedzic on unsplash . if you liked this post, you can support me on ko-fi . writing my own operating system (5 part series) 1 writing my own boot loader 2 writing my own vga driver 3 writing my own keyboard driver 4 writing my own shell 5 writing my own dynamic memory management top comments (68) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand leob leob leob follow joined aug 4, 2017 • nov 8 '20 • edited on nov 8 • edited dropdown menu copy link hide really cool, something totally different among all of the web development articles! used x86 assembly long time ago but only for a few tiny utility programs. like comment: like comment: 10 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • nov 8 '20 dropdown menu copy link hide i mean in the end my boot loader is also just a tiny utility program 😅. tbh i'm glad i don't have to use assembler for more than tiny programs because it requires a lot of mental capacity to even do simple things. like comment: like comment: 4 likes like comment button reply collapse expand c t c t c t follow just experimenting. joined nov 17, 2024 • nov 17 '24 • edited on nov 17 • edited dropdown menu copy link hide thank you very much for this! i did the entire process on my own without ever knowing of this article, or osdev. i noticed a few call parameters that i had incorrect in my own build, and your makefile parameters saved the day! i ported the entire project from your git to windows 7 x64 and migrated the makefile into my modular build scripts. as on windows, its required to use clang, lld, and objcopy. i also use readelf to dump a report of my elf file to text file for later review before calling objcopy. i run my build scripts in this way: 1 - bootloader 2 - kernel 3 - disk image 4 - emulation (qemu) note the ""@eabios"" - i just replaced 'x' with '@'. working successfully. much appreciated ;) like comment: like comment: 2 likes like comment button reply collapse expand liam824css liam824css liam824css follow work developer (microsoft) at microsoft. lnc joined jun 7, 2021 • jun 8 '21 dropdown menu copy link hide what is this erld: warning: cannot find entry symbol start; defaulting to 0000000000001000 ld: kernel.o: in function main': kernel.c:(.text+0x10): undefined reference to _global_offset_table ' make: *** [makefile:9: kernel.bin] error 1 ror? like comment: like comment: 1 like like comment button reply collapse expand maheswaran parameswaran maheswaran parameswaran maheswaran parameswaran follow joined dec 24, 2020 • aug 18 '21 dropdown menu copy link hide i know its late but add -fno-pie to gcc compilation like comment: like comment: 1 like like comment button reply collapse expand luidenka-golden luidenka-golden luidenka-golden follow joined feb 10, 2022 • feb 10 '22 dropdown menu copy link hide may i know the code with -fno-pie added? like comment: like comment: 1 like like thread thread parambir singh parambir singh parambir singh follow mes db ""hi, i'm hecker !"", 10, 0
mov bx, mes
print:
   mov ah, 0x0e
   mov al, byte [bx]
   int 0x10
   inc bx
   cmp al, 0
   je exit
   jne print
exit:
  hlt
times 510-($-$$) db 0
db 0x55, 0xaa education high school work student joined apr 15, 2023 • may 9 '23 dropdown menu copy link hide its --no-pie and not -fno-pie like comment: like comment: 1 like like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 8 '21 dropdown menu copy link hide i don't know? what did you do? like comment: like comment: 1 like like comment button reply collapse expand liam824css liam824css liam824css follow work developer (microsoft) at microsoft. lnc joined jun 7, 2021 • jun 11 '21 dropdown menu copy link hide i do start makefile like comment: like comment: 1 like like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 11 '21 dropdown menu copy link hide i wrote and tested this only on mac. are you using linux? like comment: like comment: 1 like like thread thread liam824css liam824css liam824css follow work developer (microsoft) at microsoft. lnc joined jun 7, 2021 • jul 14 '21 dropdown menu copy link hide yes like comment: like comment: 1 like like thread thread luidenka-golden luidenka-golden luidenka-golden follow joined feb 10, 2022 • feb 10 '22 dropdown menu copy link hide same problem in macos like comment: like comment: 1 like like comment button reply collapse expand havanduc2002 havanduc2002 havanduc2002 follow joined oct 24, 2023 • oct 24 '23 dropdown menu copy link hide adding -fno-pie in gcc fix the problem to me gcc -fno-pie -m32 -ffreestanding -c kernel.c -o kernel.o like comment: like comment: 1 like like comment button reply collapse expand (っ◔◡◔)っ♥ crypty ♥ (っ◔◡◔)っ♥ crypty ♥ (っ◔◡◔)っ♥ crypty ♥ follow joined may 27, 2021 • may 27 '21 • edited on may 27 • edited dropdown menu copy link hide hey! this is the best kernel series in the world! i want to make a tutorial about it on yt and a personal os, basing on this source code. but i don't know if i have the permissions. thank you, for the great series, and hope you reply me! like comment: like comment: 4 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 3 '21 dropdown menu copy link hide go ahead! like comment: like comment: 4 likes like comment button reply collapse expand (っ◔◡◔)っ♥ crypty ♥ (っ◔◡◔)っ♥ crypty ♥ (っ◔◡◔)っ♥ crypty ♥ follow joined may 27, 2021 • jun 4 '21 dropdown menu copy link hide thank you! like comment: like comment: 2 likes like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 4 '21 dropdown menu copy link hide please share your material once you're done so i can check it out and hit the like button ;) like comment: like comment: 3 likes like thread thread (っ◔◡◔)っ♥ crypty ♥ (っ◔◡◔)っ♥ crypty ♥ (っ◔◡◔)っ♥ crypty ♥ follow joined may 27, 2021 • jun 14 '21 dropdown menu copy link hide ok! like comment: like comment: 2 likes like comment button reply collapse expand james turner james turner james turner follow director of turner software | creator of brandvantage location adelaide, australia education bachelor of information technology work director at turner software joined aug 25, 2018 • nov 9 '20 dropdown menu copy link hide great post! i have always wanted to do my own os though ideally with c# syntax (tried with cosmos). it is something i want to try again at some stage - build a mini bootloader like you have and initialise an environment to run it. kinda like what you seem to get at in the article, i imagine drivers will be the harder part. looking forward to the next part! like comment: like comment: 3 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • nov 9 '20 dropdown menu copy link hide glad you liked it! working on a simple vga driver right now :) like comment: like comment: 3 likes like comment button reply collapse expand igor tashevski igor tashevski igor tashevski follow joined mar 24, 2021 • apr 6 '21 dropdown menu copy link hide my question is, how in the past, os was build without tooling, like nasm and etc... how for example u will load the kernel main function in the past without linker... how u can build simple bootloader, without this tools, ok maybe qemu can stay, but without all other tools... like comment: like comment: 2 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • apr 7 '21 dropdown menu copy link hide i guess it all started with a programmable computer and then people started building stuff. as soon as somebody ""invented"" assembler, you could use that to build a c compiler and linker. then you could rewrite the compiler in c and compile it with the assembly compiler. people used what they have and in parallel built more tools / abstractions to make it easier to build complex systems. the problem with that is that every layer of abstraction might make it easier to write a piece of software, but it does not make it simpler. the complexity is still there. and building an operating system is not an easy task. i mean there are thousands of developers busy doing that since decades and you are still lucky if your printer or scanner works... like comment: like comment: 1 like like comment button reply collapse expand darkyelox darkyelox darkyelox follow web and mobile developer always searching for new technologies and good practices for coding location palmira - colombia work systems technologist at self-employed joined may 2, 2020 • mar 7 '21 dropdown menu copy link hide great post, i have learning a lot from it, only two things: the segment descriptor url you provided is pointing to nothing or is bad. you should add makefile for linux users in the repo or here in your post, i have tried it in my po_os and it didn't work, searching a little i found that for compile the kernel with gcc you should add the -fno-pie parameter and that fixed it now working as expected. again great post so far, i'll keep learning from this in the next sections. like comment: like comment: 2 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • mar 10 '21 dropdown menu copy link hide the segment descriptor url you provided is pointing to nothing or is bad. fixed, thank you! you should add makefile for linux users in the repo or here in your post, i have tried it in my po_os and it didn't work, searching a little i found that for compile the kernel with gcc you should add the -fno-pie parameter and that fixed it now working as expected. glad you figured it out :) happy to accept a pr :) like comment: like comment: 1 like like comment button reply collapse expand mckeep82 mckeep82 mckeep82 follow joined jan 25, 2023 • jan 25 '23 dropdown menu copy link hide hi frank, i really enjoyed this lesson and have used it to supplement a lesson published by lucus darnell in his operating system from scratch lessons. would i have your permission to use your code base in lessons to my students? i used to have them work with grub for their assignments, but having a fully implemented bootloader compatible with qemu would be a great lesson. please let me know, thanks! like comment: like comment: like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • feb 3 '23 dropdown menu copy link hide sure, go ahead! do you mind linking folks to this post? :) like comment: like comment: 1 like like comment button reply collapse expand mckeep82 mckeep82 mckeep82 follow joined jan 25, 2023 • feb 3 '23 dropdown menu copy link hide absolutely! i will reference the post in the syllabus. perhaps i can put your information together with some of my other work and put it into an open source text book that we can use to distribute to the students. would you be interested? thanks for the response! like comment: like comment: 1 like like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • feb 3 '23 dropdown menu copy link hide absolutely! i will reference the post in the syllabus. thanks! which course is it? perhaps i can put your information together with some of my other work and put it into an open source text book that we can use to distribute to the students. i do not want to invest any time into this right now. my priorities shifted :) wiki.osdev.org has some good resources as well. like comment: like comment: 1 like like thread thread mckeep82 mckeep82 mckeep82 follow joined jan 25, 2023 • feb 3 '23 dropdown menu copy link hide this is for an os development course! also i wouldn't expect anything from you! i would just collect the information in an online source that you could access (if you wanted, you would also be free to entirely ignore it :)) like comment: like comment: 2 likes like comment button reply collapse expand liam824css liam824css liam824css follow work developer (microsoft) at microsoft. lnc joined jun 7, 2021 • jun 7 '21 dropdown menu copy link hide makefile is error my os is ubuntu and how makefile start? console:makefile:9: *** missing separator.  stop. why? like comment: like comment: 1 like like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 8 '21 dropdown menu copy link hide i wrote the makefile on mac. never tested it on ubuntu :) like comment: like comment: 1 like like comment button reply collapse expand liam824css liam824css liam824css follow work developer (microsoft) at microsoft. lnc joined jun 7, 2021 • jun 20 '21 dropdown menu copy link hide please test the makefile in ubuntu and create a new makefile for ubuntu in the repository. like comment: like comment: 1 like like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 20 '21 dropdown menu copy link hide why don't you do it? like comment: like comment: 1 like like thread thread sean wilkerson sean wilkerson sean wilkerson follow low-level stuff is fun. interpreters, compilers, drivers, kernels, networking, etc. education westlake high school work software engineer at bucket wonders joined may 24, 2018 • jun 22 '21 • edited on jun 22 • edited dropdown menu copy link hide for anyone else following along getting this error, a quick google shows that makefiles do not like spaces for tabs. so you have to convert all spaced indents to tabbed indents before running make . source: stackoverflow.com/questions/169317... like comment: like comment: 1 like like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • jun 22 '21 dropdown menu copy link hide cool! happy to accept a pr if that helps folks :) like comment: like comment: 1 like like comment button reply collapse expand hornet512 hornet512 hornet512 follow enjoy game and os development. location not, telling joined jan 30, 2021 • jan 30 '21 dropdown menu copy link hide when i tried the makefile part this is what i got: makefile:9: *** missing separator. stop. like comment: like comment: 2 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • feb 1 '21 • edited on feb 1 • edited dropdown menu copy link hide hm... might be that i screwed something up there. you could check the source code for a version that was at least working on my mac. if you find the problem feel free to report back so i can fix it :) like comment: like comment: 1 like like comment button reply collapse expand liam824css liam824css liam824css follow work developer (microsoft) at microsoft. lnc joined jun 7, 2021 • jun 7 '21 dropdown menu copy link hide kernel-entry.o : kernel-entry.asm nasm $ ^ | erase this blank and write a tab like comment: like comment: 1 like like comment button reply view full discussion (68 comments) code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 more from frank rosner writing my own dynamic memory management # memory # algorithms # datastructures # c writing my own shell # terminal # x86 # shell # c writing my own keyboard driver # hardware # x86 # os # c 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/advocatemack/detecting-credentials-in-source-code-open-source-or-commercial-okp,,,"detecting credentials in source code: open-source or commercial solutions? - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse mackenzie posted on nov 24, 2020 • edited on dec 4, 2020 • originally published at thesecuritybash.com detecting credentials in source code: open-source or commercial solutions? in modern software development we rely on hundreds, sometimes thousands of different building blocks . the glue that connects all the different building blocks are collectively known as secrets . these are typically api keys, credentials, security certificates and uris. these are the modern day master keys.they can provide access to cloud infrastructure, payment systems, internal messaging and user information to name a few. once an attacker has a secret, they can move laterally between systems to uncover additional information and secrets* , * and because they are authenticated, they look and appear like valid users, making it extremely difficult to detect. ( read more ) but even having established how sensitive these secrets are and why they should be tightly wrapped, this next statement may surprise you: these secrets are sprawled all over the internet, sitting in code repositories in public view. for the proprietor of the code, these secrets are difficult to identify, but malevolent actors out to find them have developed simple and effective tools to uncover secrets deeply buried and long forgotten in git history. there are plenty of articles, whitepapers and blog posts on the importance of protecting secrets, for example hashicorp and gitguardian have great resources on this topic. instead, i want to focus on the different tools available for detecting secrets as well as their pros and cons. but of course it is up to you, the reader, to decide which tools will be best to protect your secrets. three options for secrets detection when it comes to secrets detection, you can choose between 3 different approaches: building a custom solution in house using open-source projects using commercial products let's run through a few examples. building in house detection for some of us, the problem of secret sprawl poses a perfect problem to unpack. i would be lying if i haven't played myself with building some fun regular expression (regex) scripts to detect sensitive strings inside code. but building a comprehensive reliable secrets detection script is a huge task. first, you need to decide how to detect secrets.  there are two main options for this: using regex to detect fixed string credentials (like stripe api keys which begin with the same characters), or implement high entropy detection, which casts a large net but brings back a huge volume of results. method pros cons entropy: look for strings that appear random good for penetration testing, open sourcing a project or bug bounties because it brings a lot of results. these results must be reviewed manually. lots of false alerts (it is very frequent to see urls, file paths, database ids or other hashes with high entropy), which makes it impossible to use this method alone in an automated pipeline. some keys are inevitably missed because the entropy threshold to be applied depends on the charset used to generate the key and its length regular expressions: match known, distinct patterns low number of false alerts. known patterns make it easier to later check if the secret is valid or not or if this is an example or test key (see step 2). unknown key types will be missing credentials without a distinct pattern will be missed, which means lots of missed credentials! think about passwords that can be virtually any string in many possible contexts, apis that don’t have a distinct format, ... when using regular expression, you have a very limited scope of secrets you can detect leaving you open to vulnerabilities . using high entropy method, you will cast a wider net but also need to sort through more false positives . of course, in an ideal world you want to use both, but then you'd need to build in post-validators that can sift through the results to exclude likely false positives. if you are building this as an experiment for your personal projects, this can be a fun and exciting challenge. but when you bring in the challenges of detection at scale, you have to consider resources, alerting and mitigation. the challenge can quickly spiral into a huge project. it is always best to first learn from a real-life example, i would encourage anyone going down the path of building a secrets detection solution to first read about how sap built its internal secrets detection solution . if you are fixed on building a personal solution, i would have to advocate for beginning with one of the many open-source projects available to build upon . i know this can be less exciting than a personal challenge, but when you begin to unpack the scope of the problem, it will save you a ton of work. using open-source tools open-source tools are not just a good starting point for building your own custom decision patterns, but there are actually also great projects available that provide immediate value with minimal setup. popular open-source tools there is a huge list of open source detection tools available on github. below are a few that are both popular and well-maintained. tool description truffle hog one of the popular utilities to find secrets everywhere, including branches, commit history. truffle hog search using regex and entropy, and the result is printed on the screen. git secrets released by aws labs, as you can guess by the name – it scans for the secrets. git secrets would help prevent committing aws keys by adding a pattern. gitrob gitrob makes it easy for you to analyze the finding on a web interface. it’s based on go, so that’s a prerequisite. git hound a git plugin based on go, git hound, helps prevent sensitive data from being committed in a repository against pcre (perl compatible regular expressions). it’s available in a binary version for windows, linux, darwin, etc. useful if you don’t have go installed. pros and cons of open-source tools pros cons ability to define custom detectors slow performance at scale can be installed locally canceling the need for a third party investigation large number of false positives can result in disruptions to the workflow typically supported and maintained by interested parties (yelp for example) hard to enforce throughout an organization no alerting features and cannot be integrated into a seim does not allow for team collaboration and incident investigation while the detection reliability and efficiency vary between solutions, the detection systems all lack enterprise features such as alerting, audit trails and in-depth investigation. open-source solutions, in my opinion, are best used for bug bounty and one-off pen testing exercises where high volumes of positive results can be sorted through and evaluated. when these systems are put in place in regular production, particularly within organization, the results can be overwhelming and extremely restrictive to the workflow. that being said, there are still some clear advantages over commercial systems in some situations. using commercial tools along with many high profile cases of secrets being discovered inside git repositories including uber , many vendors have come to the party with solutions to combat this. from the many conversations around secrets detection, the biggest concern is vendor trust. you are essentially allowing a third party to find and detect the most sensitive information that you or your organization own. many vendors, including the big players like gitguardian , do offer an on premise version of their products. but this comes usually with an enterprise license which is costly for developers and smaller companies. the idea of allowing a third party to scan for secrets inside source code can be concerning, and there are definitely some considerations to take into account. the first is that secrets inside git repositories, private and public, should already be considered compromised. git provides the perfect platform to facilitate secret sprawl , because code is a leaky asset and git provides no audit log of who has access to it or where it has been cloned. so if secrets exist in code repositories, using a third-party application to scan for them, does not really increase the risk vector. commercial vendors also have larger teams and time dedicated to detecting secrets, making them more reliable in large scales but also offers additional enterprise features such as alerting, dashboards to allow investigation and remediation, as well as much easier set-up. all this means that the tool will fit into your workflow much better. the best example of a comparison between the two most predominant open-source and commercial vendors can be found here . commercial secrets detection solutions while there are additional vendors in the market, below are the four core competitors in the space that are the current market leaders. tool description gitguardian the most predominant detection solution with both private and public monitoring. with over 200+ secrets supported it has the largest detection capabilities on the market. gitguardian is a developer first company with excellent support as well as a product completely free for developers and open-source organizations. github token scanning github offers a commercial token scanning solution that covers 25 different secrets. this is currently only available for commercial clients with an advanced security license at a cost of $80 per developer but does come with other security features. gitlab secrets detection gitlab also offers limited secrets detection capabilities which come free on all plans since version 13.3. currently, they offer detection for 12 secret types by default. you can however add custom rules and regex. nightfall ai-powered scanner to detect api keys, secrets, sensitive information. watchtower radar api lets you integrate with github public or private repository, aws, gitlab, twilio, etc. the scan results are available on a web interface or cli output. pros and cons of commercial secrets detection pros cons sophisticated detection algorithms with greater number of support secrets third party access to source code real time detection closed source (with the expectation of some gitguardian products) alerting mechanisms built in credential validity checks (limited to gitguardian) contextual analysis of the code to reduce false positives audit trail of secrets and remediation steps role based authentication wrap up implementing secrets detection should always be part of the threat mitigation strategy of all developers and organisations. there are many available solutions on the market for both open-source and commercial vendors, all with their own considerations. while commercial vendors offer more sophisticated detection without buying commercial licenses, they come with the consideration of needing to provide third-party access to source code. although open-source solutions are a cost-effective solution, they can provide such a large number of false positives they become prohibitive to workflow. or you can build your own, but beware of the big task ahead of you. but in the end, it comes down to what works best for you and your organisation. additional resources to continue on this topic, check out the links below. secrets detection learning center (gitguardian) how to securely use secrets (hashicorp) academic research paper on secrets inside git repositories understanding secret sprawl (gitguardian) top comments (4) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand 🦄n b🛡 🦄n b🛡 🦄n b🛡 follow // , “it is not so important to be serious as it is to be serious about the important things. the monkey wears an expression of seriousness... but the monkey is serious because he itches.""(no/no) location (no/no) education (no/no) work security joined mar 1, 2018 • nov 24 '20 dropdown menu copy link hide wow, what a great resource! thank you for including links to additional resources. might i request an update for examples that give some context to this, perhaps on how to set this up as part of a static code analysis pipeline, with checkmarkx or fortify, and with sonarqube? if i make such a post, i'll include a link to yours. this has become a big deal in my line of work. like comment: like comment: 3 likes like comment button reply collapse expand mackenzie mackenzie mackenzie follow joined mar 25, 2020 • nov 26 '20 dropdown menu copy link hide this sounds awesome. maybe we can collab on a followup discussing other vulnerabilities and tools? if not, sounds like a fun topic for my next post. appreciate the comment. like comment: like comment: 1 like like comment button reply collapse expand keogami keogami keogami follow i'm a hobbyist programmer!! just a 'shady' guy with a laptop. 
ps. i love designing systems and network architecture location meerut, india education bca work hobbyist at nowhere xd joined sep 14, 2020 • nov 24 '20 dropdown menu copy link hide would love it if you could add examples for what a ""secret"" might look like neat resource tho xd cheers like comment: like comment: 1 like like comment button reply collapse expand max ivanov max ivanov max ivanov follow software engineer / cloud architect. i help developers use cloud platforms efficiently.

i write about azure, aws, serverless, full-stack typescript. location portugal work software engineer joined nov 20, 2020 • nov 24 '20 dropdown menu copy link hide comprehensive breakdown! bookmarked. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse mackenzie follow joined mar 25, 2020 more from mackenzie millions of .git folders exposed publicly by mistake # git # hacking # security # devops the state of security in australia: hacksydney and bsides give insight into security post-medibank and optus # security # appsec # australia # hacking source code as a vulnerability - a deep dive into the real security threats from the twitch leak 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/matheusgomes062/coalescing-operator-and-compound-assignment-operator-in-c-3ae3,,,"coalescing operator and compound assignment operator in c# - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse matheus gomes 👨‍💻 posted on jan 5, 2020 coalescing operator and compound assignment operator in c# # csharp # todayilearned hello! today i want to ask you this: have you ever used a null coalescing operator or even a compound assignment operator in c# ? i never. until today i had never heard about this things, so i want to share with you what i learned about and how it can be applied to your code. the problem let's say you want to give a given variable the value of null. namespace course { class program { static void main ( string [] args ) { double ? x = null ; double ? y = 10.0 ; console . writeline ( x . value ); } } } enter fullscreen mode exit fullscreen mode now, if we want to print the value on the screen it will accuse the following error: ""unhandled exception: system.invalidoperationexception: nullable object must have a value."" enter fullscreen mode exit fullscreen mode let's see how to get around this... the old way 👎 the old and 'commom way' to check this is using if else operators like this: namespace course { class program { static void main ( string [] args ) { double ? x = null ; double ? y = 10.0 ; // check if x is null if ( x . hasvalue ) console . writeline ( x . value ); else console . writeline ( ""x is null"" ); // check if y is null if ( y . hasvalue ) console . writeline ( y . value ) else console . writeline ( ""y is null"" ); } } } enter fullscreen mode exit fullscreen mode we see that in this case we cannot place a default value to x and y operators. so we display on screen when it is null. the null coalescing operator way 👌 first, a null coalescing operator (??) is used to define a default value for nullable value types or reference types. it returns the left-hand operand if the operand is not null, otherwise, it returns the right operand. so it's mainly used to simplify checking for null values and also assign a default value to a variable when the value is null. using our example: namespace course { class program { static void main ( string [] args ) { double ? x = null ; double ? y = 10.0 ; double x = x ?? 0.0 ; console . writeline ( x . value ); double y = y ?? 0.0 ; console . writeline ( y . value ); } } } enter fullscreen mode exit fullscreen mode this way i can make a default value on x and y when one of them is null. and so, we can print on screen! but can it be better? the compound assignment operator way 😎 the compound assignment operator (??=) was introduced on c# 8.0 and has made our job easier. it simply reduces what we have to write and has the same result. instead of writing double x = x ?? 0.0; we can just write double x ??= 0.0; namespace course { class program { static void main ( string [] args ) { double ? x = null ; double ? y = 10.0 ; double x ??= 0.0 ; double y ??= 0.0 ; } } } enter fullscreen mode exit fullscreen mode simple, right? hope you enjoyed this post, it's simple but it's something worth sharing for me. thanks for your time!😊 links: https://dzone.com/articles/nullable-types-and-null-coalescing-operator-in-c https://dev.to/mpetrinidev/the-null-coalescing-operator-in-c-8-0-4ib4 https://docs.microsoft.com/pt-br/dotnet/csharp/language-reference/operators/null-coalescing-operator top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand marcos henrique marcos henrique marcos henrique follow ""programming isn't about what you know; it's about what you can figure out.”
learning in public 🧑🏻‍💻 email wakeupmh@gmail.com location são josé dos campos work cloud engineer | aws community builder joined may 21, 2019 • jan 5 '20 dropdown menu copy link hide 🤘 like comment: like comment: 2 likes like comment button reply collapse expand saint4eva saint4eva saint4eva follow i love people and everything good. and i want to positively influence people and be influenced by positive people. i am in love with software development using .net technologies. joined dec 16, 2017 • jan 6 '20 dropdown menu copy link hide good article. thank you. like comment: like comment: 2 likes like comment button reply collapse expand arthur barbero arthur barbero arthur barbero follow newbie in life location brazil work intern at agrotools joined oct 22, 2019 • jan 5 '20 dropdown menu copy link hide awesome dude, simple but unknown by most programmers like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse matheus gomes 👨‍💻 follow be the change that you wish to see in the world. location brazil education bachelor in science and technology by unifesp work software engineer @ intive joined dec 17, 2019 more from matheus gomes 👨‍💻 tdd e seus fundamentos # testing # beginners # todayilearned # tdd how to increase your rendering performance by 70% in vue.js # todayilearned # vue # performance # functional food review, refactoring a long method # refactorit # food # csharp # programming 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/daltonfury42/aws-ses-gmail-free-business-email-id-4phj,,,"aws ses + gmail = free business email id - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse daltonfury42 posted on sep 12, 2020 • edited on may 23, 2021 aws ses + gmail = free business email id # startup # aws # productivity my website is hosted on aws free tier, and i wanted to create a free company/business email address for people to contact me. if you are already on aws, and want to create an email address like contact@simplq.me it doesn't cost anything. what you need: 1) an aws account 2) a domain name (simplq.me in my case) 3) gmail account (other email services should also work) what you don't need : a gsuite account (if your domain is on google domains, setting up email forwarding is easy.) i've covered all the necessary steps in brief, but if you need help or get stuck somewhere, let me know in the comments. if you want two way communication, not all regions support it. i set this up in us-west-2 (oregon) even though my website is hosted in ap-southeast-1 (singapore) . setup ses - simple email service on aws console, switch to us-west-2, go to ses, and verify your domain: if your dns is managed by route53, amazon can automatically update the entries, click on ""use route53"" button on the next page. otherwise, you have to manually set the entries in your current dns registrar. verify your current email this step is easy, your current gmail address that you want amazon to relay all communications to, verify it with ses. you'll get a confirmation email, as part of the verification process. configure ses email forwarder in the coming steps, we will configure ses to trigger a lambda which will forward emails to our personal email. create a blank node.js 12.x runtime lambda function with no triggers in the same region, and use this file as the function code. there is a config object in the code which requires some tweaking: var defaultconfig = {
  fromemail: ""contact@simplq.me"",
  subjectprefix: """",
  emailbucket: ""<s3-bucket-name>"",
  emailkeyprefix: ""mails/"",
  allowplussign: true,
  forwardmapping: {
    ""contact@simplq.me"": [
      ""<your-gmail-id>@gmail.com""
    ]
  }
}; enter fullscreen mode exit fullscreen mode fromemail should be the business email which your customers would see. we will later create an s3 bucket to store our emails. choose a bucket name and give it as emailbucket . in the forwardmapping section, you should configure the gmail address which you verified in the previous step. arithmetric/aws-lambda-ses-forwarder is a awesome repo, it supports many more configurations, you should check it out if you want to create and forward multiple emails, or forward emails to multiple people. attach this policy to the service role of the lambda, to give it access to the s3 bucket, and also ses: {
    ""version"": ""2012-10-17"",
    ""statement"": [
        {
            ""sid"": ""visualeditor0"",
            ""effect"": ""allow"",
            ""action"": [
                ""s3:putobject"",
                ""s3:getobject""
            ],
            ""resource"": ""arn:aws:s3:::<s3-bucket-name>/*""
        },
        {
            ""sid"": ""visualeditor1"",
            ""effect"": ""allow"",
            ""action"": ""ses:sendrawemail"",
            ""resource"": ""*""
        }
    ]
} enter fullscreen mode exit fullscreen mode create a rule in ses this ties everything together, go back to ses console and create a new email receiving -> rule set. you will set a rule, where you configure two ""actions"", one to save all emails to a s3 bucket which you can create from this screen, and another to trigger the lambda created in the previous setup to forward the mails. use the below screenshot as reference: at this point, if you send an email to your business email, the personal email id should receive it. test and make sure that it works, use cloudwatch logs for the lambda to debug in case of issues. configure gmail next is to add this new email as a new identity to your gmail account. go to ses's smtp settings and create a new smtp credential. at the end, you'll get a username and a password, which you should add to your gmail settings: last step - verification initially your newly configured amazon ses service will be quarantined (sandboxed) by amazon as a measure of protection against possible abuse and spam. to remove it from quarantine and allow normal mailing, as the last step, you need to open a support ticket to amazon and fullfill a request. otherwise you will see how the emails you send bounce with the following error message: 554 message rejected: email address is not verified. the following identities failed the check in region ... enter fullscreen mode exit fullscreen mode they approved within minutes in my case. go to sending statistics section to raise a request: that's it! your business email is ready to use. let me know in the comments section if you face any issues. hope you enjoyed it. -- if you want to reach out to me for consulting or mentoring, you can book a slot here . top comments (32) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand sebastian weigand sebastian weigand sebastian weigand follow location new york, new york work planetscale systems architect at google joined sep 14, 2020 • sep 14 '20 dropdown menu copy link hide pro-tip: you can do all of this with built-in functionality with google domains, where you can establish mail aliases and forwarding rules from your custom domain. there's no need for extra complication or additional services. see this article for more information. like comment: like comment: 2 likes like comment button reply collapse expand anurag singh anurag singh anurag singh follow automate everything | self-host every service | build tiny projects | backend dev | raspi • esp32 tinkerer email email@anuragsingh.dev location bengaluru joined jan 8, 2019 • sep 24 '20 • edited on sep 24 • edited dropdown menu copy link hide thanks for this info, i was using improvmx till now even though i have a google domain. like comment: like comment: 1 like like comment button reply collapse expand daltonfury42 daltonfury42 daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 • sep 14 '20 dropdown menu copy link hide yep, if your are willing to transfer your domain to google domains, this is looking very easy! like comment: like comment: 1 like like comment button reply collapse expand jörg rech jörg rech jörg rech follow full-stack developer, freelancer and entrepreneur (frontend | backend | devops | cloud | big data). cxo and founder of techmap.io co-founder and former cto of talentwunder.de location karlsruhe, germany education phd in software engineering work everything at techmap.io joined apr 19, 2017 • apr 1 '22 dropdown menu copy link hide hi! just implemented it for one of my domains and it works really well - thank you! one question though: do i need to ""register"" every name of a domain at gmail or should it work out of the box? for example: i have the following config in the lambda code: forwardmapping: {
    ""@<my_domain>.com"": [
      ""<me>@gmail.com""
    ]
  } enter fullscreen mode exit fullscreen mode however, only @.com is forwarded to gmail (where i added this email address) but emails to info@.com is only stored in s3 but not forwarded. like comment: like comment: 1 like like comment button reply collapse expand chris hayes chris hayes chris hayes follow frontend dev that loves design as much as coding. i enjoy toying with the latest tech, design, space, f1, hiking, and reading fantasy. i highly value empathy and compassion. location new haven, ct work software engineer at digital surgeons joined dec 2, 2019 • jul 22 '24 dropdown menu copy link hide for what it's worth - i haven't had that issue, it works as wildcard for me with all the email addresses i don't specifically mention. like comment: like comment: 1 like like comment button reply collapse expand wandetri wandetri wandetri follow hi! i am wande, a full stack dreamer! joined jul 16, 2019 • nov 2 '20 • edited on nov 6 • edited dropdown menu copy link hide hi thanks for this trick its working! but there is a problem, all of my sent email goes to promotion tab in gmail inbox edit after i modify the email content to longer and more 'personal' instead of 'test email', it goes to my main inbox. thanks! like comment: like comment: 2 likes like comment button reply collapse expand meir gabay meir gabay meir gabay follow i'm passionate about studying and teaching.
devops engineer at lsports location israel education industrial engineer, b.sc @ shenkar ; exact science education and technology, m.a @ tel-aviv univ. work devops engineer @ lsports joined sep 3, 2019 • sep 13 '20 dropdown menu copy link hide an honest question - is this better than using google's mx records in route53? i'm trying to understand the benefits of this method ref - support.google.com/a/answer/614969... like comment: like comment: 2 likes like comment button reply collapse expand daltonfury42 daltonfury42 daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 • sep 13 '20 • edited on sep 14 • edited dropdown menu copy link hide i've not gotten chance to play with gsuite / gapps for business much, but i am guessing this works only when you have a paid gsuite account? this solution is when you have a free personal google account. please correct me if i am wrong, though. like comment: like comment: 1 like like comment button reply collapse expand meir gabay meir gabay meir gabay follow i'm passionate about studying and teaching.
devops engineer at lsports location israel education industrial engineer, b.sc @ shenkar ; exact science education and technology, m.a @ tel-aviv univ. work devops engineer @ lsports joined sep 3, 2019 • sep 13 '20 • edited on sep 13 • edited dropdown menu copy link hide i think you are right, my assumption was that you have a gsuite account since you talked about a ""company/business email address"". i don't think it's possible to use mx records with a personal account, thanks for the clarification like comment: like comment: 2 likes like thread thread daltonfury42 daltonfury42 daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 • sep 14 '20 dropdown menu copy link hide hey. thanks for pointing this out.. i'll edit the post to make this clear. like comment: like comment: 2 likes like comment button reply collapse expand andrewrooke andrewrooke andrewrooke follow location queensland, australia work solutions architect joined sep 17, 2020 • sep 17 '20 dropdown menu copy link hide while this may be a good way to get started quickly, have you thought through the implications for spf and dmarc ? every time i create a business domain and email service, i ensure i configure spf and dmarc (and ideally dkim) to secure my email service. like comment: like comment: 1 like like comment button reply collapse expand shreyash gupta shreyash gupta shreyash gupta follow location fayetteville, ar, usa joined may 18, 2021 • may 18 '21 dropdown menu copy link hide hi, i am the co-founder of looseleaf which is a startup in the education sector of india. we currently want to get our domain emails using the aws ses. i would like to ask if you would be interested in helping us out as it seems that you know what you are doing. let's discuss further details below! contact email: officiallooseleaf@gmail.com our website: looseleaf.in like comment: like comment: like comment button reply collapse expand daltonfury42 daltonfury42 daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 • may 21 '21 dropdown menu copy link hide hi shreyash, i'd be happy to help. let's connect this weekend. like comment: like comment: 2 likes like comment button reply collapse expand shreyash gupta shreyash gupta shreyash gupta follow location fayetteville, ar, usa joined may 18, 2021 • may 21 '21 dropdown menu copy link hide just send me an email with your times that you can hop on a google meets call this weekend and then we will go on from there! like comment: like comment: 1 like like comment button reply collapse expand ankush jain ankush jain ankush jain follow experienced full stack engineer with over 12 years of expertise in developing web applications and cloud solutions, primarily focused on aws. proficient in .net, aws, azure, and javascript. location india joined may 29, 2024 • dec 17 '24 • edited on dec 17 • edited dropdown menu copy link hide created a video tutorial on the same topic inspired by this blog. creating business emails using amazon ses | sending & receiving emails with amazon ses youtube.com/watch?v=lhkxp9oli7u like comment: like comment: 1 like like comment button reply collapse expand aftab naveed aftab naveed aftab naveed follow joined dec 26, 2023 • dec 26 '23 dropdown menu copy link hide i just tried configuring  it on my gmail account, i was able to make the aws ses part working and can see emails arriving to my s3 bucket, however when i try to connect my gmails' ""send mail as "" i get this error authentication failed. please check your username/password.
server returned error: ""dns error: dns type 'aaaa' lookup of smtp.email-smtp.us-east-1.amazonaws.com responded with code nxdomain dns type 'a' lookup of smtp.email-smtp.us-east-1.amazonaws.com responded with code nxdomain, code: 553"" enter fullscreen mode exit fullscreen mode like comment: like comment: 1 like like comment button reply collapse expand lewis cowles lewis cowles lewis cowles follow location essex, uk joined may 13, 2017 • sep 14 '20 dropdown menu copy link hide i sort of love you for this, but you can setup any email host just using route53 and likely own less of a footprint, and escape gmail like comment: like comment: 1 like like comment button reply collapse expand daltonfury42 daltonfury42 daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 • sep 15 '20 dropdown menu copy link hide hey. i want to give this a try, can you give me a rough idea. can i do this using my personal gmail account? otherwise which mailbox providers allow this? like comment: like comment: 1 like like comment button reply collapse expand lewis cowles lewis cowles lewis cowles follow location essex, uk joined may 13, 2017 • sep 15 '20 dropdown menu copy link hide sure, so i don't know how much you understand dns, but the way i use a shared host to do this is to tell them i'm hosting a domain with them. i don't change the nameservers, because i love aws and am cheap. i can then go into their dns settings, take out the mx records and place them into my aws. you can use aws ui to do this, or terraform, or any other iac tooling that integrates with your cloud provider (doesn't have to be aws, can be azure, gcp or digitalocean afaik) there are some gotchas. i don't think heroku supports, but you can have heroku only own a cname. also you should not move other subdomains like webmail to your domain. let the shared host do that with ssl and all the bells and whistles to secure your email from attackers. you can also front that with ses and forwarding rules, although i was gutted to see that. like comment: like comment: 1 like like comment button reply collapse expand ypk ypk ypk follow location london education msc software engineering work developer joined oct 22, 2019 • sep 15 '20 dropdown menu copy link hide your s3bucket permissions throws an error: ""missing required field principal"" like comment: like comment: 1 like like comment button reply collapse expand daltonfury42 daltonfury42 daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 • sep 16 '20 dropdown menu copy link hide hi, can you add ses as a principal to your bucket's policy? in my case, since i created the bucket from within ses, the policy was set for me. stackoverflow.com/questions/418192... like comment: like comment: 1 like like comment button reply collapse expand ypk ypk ypk follow location london education msc software engineering work developer joined oct 22, 2019 • sep 18 '20 dropdown menu copy link hide i've followed other tutorial linked in one of the replies. yes you can add principal as it did allow me to. i'll be honest, this instructions are not walk in the park, i was expecting a push button solution. like comment: like comment: 1 like like comment button reply view full discussion (32 comments) some comments may only be visible to logged-in visitors. sign in to view all comments. code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse daltonfury42 follow dev by passion. 

for any help or consulting needs, you can book my time here  

https://www.hiretheauthor.com/daltonfury42 location bangalore work backend engineer joined jun 10, 2020 more from daltonfury42 understanding git ep01: the five zones of git # git # productivity # programming # tips two ways for beginners to start using docker to increase developer productivity # productivity # devops # docker # beginners my first attempt at micro-saas; suggestions and feedback please. # startup # react # webdev 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/frosnerd/writing-my-own-dynamic-memory-management-361g,,,"writing my own dynamic memory management - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse frank rosner posted on nov 30, 2020 • edited on aug 13, 2024 writing my own dynamic memory management # memory # algorithms # datastructures # c writing my own operating system (5 part series) 1 writing my own boot loader 2 writing my own vga driver 3 writing my own keyboard driver 4 writing my own shell 5 writing my own dynamic memory management introduction so far, whenever we needed some memory, e.g. to store a string, we allocated it like that: char my_string[10] . this statement tells the c compiler to allocate 10 consecutive bytes in memory that we can use to store our characters. but what if we do not know the size of the array at compile time? let's say the user wants to specify the length of the string. we could of course allocate a fixed amount of memory, e.g. 256 bytes. there is a high chance that this is too much, however, effectively wasting memory. another outcome would be that the statically allocated memory is not enough, making the program crash. dynamic memory management can solve this problem. our os should offer a way to allocate a flexible amount of memory that is determined at run time. to reduce the risk of running out of memory we also need functionality to make memory available again that is no longer used. in this blog post we want to design and implement a simple algorithm for dynamic memory management. the remainder of this post is structured as follows. first, we design the data structure that we will use to manage dynamic memory, as well as the allocation and deallocation algorithm. then, we implement a dynamic memory management for our kernel based on the theory from the previous section. design data structure to implement dynamic memory management, we will statically allocate a large memory region from which individual chunks can be borrowed to parts of our program. when the borrowed memory is no longer needed it can be returned to the pool. the question is: how do we keep track of the chunks that have been borrowed, i.e. dynamically allocated? we need a data structure that allows us to find available memory of at least the requested size. we also want to pick the smallest possible free region in order to avoid ending up with many small memory fragments. additionally it would be great if this operation can be performed with minimal effort. for our simple implementation we will used a doubly linked list. each element holds information about its chunk, specifically the address and size, whether it is currently allocated, as well as pointers to the previous and next element. we can find the optimal, i.e. the smallest possible, region by iterating through the entire list in o(n) , where n is the number of elements in the list. of course there are more efficient alternatives such as heaps but they are more complex to implement so we are going to stick to the list. now where do we store the list? we cannot statically allocate memory for it because we do not know how many chunks will be requested and thus how many elements the list will contain. but we also cannot allocate it dynamically because we are building dynamic memory allocation just now. we can overcome this problem by embedding the list elements within the large memory region that we reserve for dynamic allocation. for each chunk that is requested we store the respective list element just in front of that chunk. the following figure illustrates the initial state of a 1024 byte dynamic memory region. it contains a single 16 byte list element (the small dark green part in the beginning) indicating that the remaining 1008 bytes are free. now let's look into the allocation algorithm. allocation algorithm when new memory m of size s m is requested, we go through all available memory looking for an optimal chunk to use. given our list of memory chunks l , we attempt to find an optimal entry o , such that it is free ( free(o) ), sufficiently large ( s o ≥ s m ), and there is no smaller entry available ( ∀x ∈ l: free(x) → s o ≤ s x ). given an optimal segment o , we slice off the requested amount of memory to create a new segment p including a new list entry, effectively shrinking o to size s o - s m - s l , where s l is the size of a list element. the new segment will have size s p = s m + s l . we then return a pointer to the beginning of the allocated memory, right after the list element. if no optimal chunk exists the allocation is unsuccessful. the following figure shows the state of the segment list after the algorithm successfully allocated 256 bytes of memory. it contains two elements. the first one refers to a free chunk which takes up 752 bytes of dynamic memory. the second one represents the memory allocated for p1 and takes up the remaining 272 bytes. next, we will define an algorithm to free allocated memory. deallocation algorithm the basic version of the deallocation algorithm is very simple. given a pointer to an allocated region we obtain the respective list entry by looking at the memory region right in front of it. we then mark the chunk as free so that it will be considered next time new memory is requested. while this version of the algorithm appears to work it has a major problem: we are creating more and more list entries. this will leave the dynamic memory fragmented, making it harder and harder to allocate larger parts of memory. to solve this problem we merge the deallocated chunk with all adjacent free chunks. this is where our doubly linked list comes in handy as we can easily determine the previous and the next chunk by following the pointers. the following animation illustrates a more complex example of different allocations and deallocations of memory. with the theory covered, let's start implementing the functionality in c so we can use it inside our os. implementation doubly linked list we will model a chunk of dynamic memory as a struct containing its size (excluding the struct size) and whether it is used (i.e. not free). to make it a doubly linked list we add a prev and a next pointer. here goes the code. typedef struct dynamic_mem_node { uint32_t size ; bool used ; struct dynamic_mem_node * next ; struct dynamic_mem_node * prev ; } dynamic_mem_node_t ; enter fullscreen mode exit fullscreen mode next we can initialize the dynamic memory. initialization before we can allocate dynamic memory we need to initialize it. as described in the design section we are going to start off with a single chunk covering the entire available memory. the following code initializes 4kb of dynamic memory. #define null_pointer ((void*)0)
#define dynamic_mem_total_size 4*1024
#define dynamic_mem_node_size sizeof(dynamic_mem_node_t) // 16 static uint8_t dynamic_mem_area [ dynamic_mem_total_size ]; static dynamic_mem_node_t * dynamic_mem_start ; void init_dynamic_mem () { dynamic_mem_start = ( dynamic_mem_node_t * ) dynamic_mem_area ; dynamic_mem_start -> size = dynamic_mem_total_size - dynamic_mem_node_size ; dynamic_mem_start -> next = null_pointer ; dynamic_mem_start -> prev = null_pointer ; } enter fullscreen mode exit fullscreen mode let's move on to the implementation of the memory allocation function. allocation recall from the allocation algorithm definition: first, we look for an optimal memory block. to keep the code readable we create a separate function find_best_mem_block for that part of the algorithm that goes through a given list and returns the smallest free node. void * find_best_mem_block ( dynamic_mem_node_t * dynamic_mem , size_t size ) { // initialize the result pointer with null and an invalid block size dynamic_mem_node_t * best_mem_block = ( dynamic_mem_node_t * ) null_pointer ; uint32_t best_mem_block_size = dynamic_mem_total_size + 1 ; // start looking for the best (smallest unused) block at the beginning dynamic_mem_node_t * current_mem_block = dynamic_mem ; while ( current_mem_block ) { // check if block can be used and is smaller than current best if (( ! current_mem_block -> used ) && ( current_mem_block -> size >= ( size + dynamic_mem_node_size )) && ( current_mem_block -> size <= best_mem_block_size )) { // update best block best_mem_block = current_mem_block ; best_mem_block_size = current_mem_block -> size ; } // move to next block current_mem_block = current_mem_block -> next ; } return best_mem_block ; } enter fullscreen mode exit fullscreen mode we can then use this function to implement our mem_alloc function that takes the requested memory size and returns a pointer to that dynamically allocated memory. it returns a null pointer in case there is no sufficiently large chunk available. let's look at the code and then go through it step by step. void * mem_alloc ( size_t size ) { dynamic_mem_node_t * best_mem_block = ( dynamic_mem_node_t * ) find_best_mem_block ( dynamic_mem_start , size ); // check if we actually found a matching (free, large enough) block if ( best_mem_block != null_pointer ) { // subtract newly allocated memory (incl. size of the mem node) from selected block best_mem_block -> size = best_mem_block -> size - size - dynamic_mem_node_size ; // create new mem node after selected node, effectively splitting the memory region dynamic_mem_node_t * mem_node_allocate = ( dynamic_mem_node_t * ) ((( uint8_t * ) best_mem_block ) + dynamic_mem_node_size + best_mem_block -> size ); mem_node_allocate -> size = size ; mem_node_allocate -> used = true ; mem_node_allocate -> next = best_mem_block -> next ; mem_node_allocate -> prev = best_mem_block ; // reconnect the doubly linked list if ( best_mem_block -> next != null_pointer ) { best_mem_block -> next -> prev = mem_node_allocate ; } best_mem_block -> next = mem_node_allocate ; // return pointer to newly allocated memory (right after the new list node) return ( void * ) (( uint8_t * ) mem_node_allocate + dynamic_mem_node_size ); } return null_pointer ; } enter fullscreen mode exit fullscreen mode we first call the find_best_mem_block function to find the smallest free block. in case there is a block available that we can use, we split it by reducing its size, creating a new node with the requested size at the start of the new chunk and insert it into the list. finally we return a pointer to the memory address directly after the newly created list node. we can then use mem_alloc to dynamically allocate an array of n integers and store 1..n inside. note that thanks to the c compiler we can access the memory using array syntax instead of calculating memory offsets. it will dereference it with the correct offset. int * ptr = ( int * ) mem_alloc ( n * sizeof ( int )); for ( int i = 0 ; i < n ; ++ i ) { ptr [ i ] = i + 1 ; // shorthand for *(ptr + i) } enter fullscreen mode exit fullscreen mode now to the mem_free implementation. deallocation the mem_free function takes a pointer to a dynamically allocated memory region. it then loads the respective list node by decrementing the pointer memory address by the node struct size and marks it as free. finally, it attempts to merge the deallocated memory node with the next and previous list elements. here you go. void mem_free ( void * p ) { // move along, nothing to free here if ( p == null_pointer ) { return ; } // get mem node associated with pointer dynamic_mem_node_t * current_mem_node = ( dynamic_mem_node_t * ) (( uint8_t * ) p - dynamic_mem_node_size ); // pointer we're trying to free was not dynamically allocated it seems if ( current_mem_node == null_pointer ) { return ; } // mark block as unused current_mem_node -> used = false ; // merge unused blocks current_mem_node = merge_next_node_into_current ( current_mem_node ); merge_current_node_into_previous ( current_mem_node ); } enter fullscreen mode exit fullscreen mode to increase readability we move the merging to separate functions. void * merge_next_node_into_current ( dynamic_mem_node_t * current_mem_node ) { dynamic_mem_node_t * next_mem_node = current_mem_node -> next ; if ( next_mem_node != null_pointer && ! next_mem_node -> used ) { // add size of next block to current block current_mem_node -> size += current_mem_node -> next -> size ; current_mem_node -> size += dynamic_mem_node_size ; // remove next block from list current_mem_node -> next = current_mem_node -> next -> next ; if ( current_mem_node -> next != null_pointer ) { current_mem_node -> next -> prev = current_mem_node ; } } return current_mem_node ; } void * merge_current_node_into_previous ( dynamic_mem_node_t * current_mem_node ) { dynamic_mem_node_t * prev_mem_node = current_mem_node -> prev ; if ( prev_mem_node != null_pointer && ! prev_mem_node -> used ) { // add size of previous block to current block prev_mem_node -> size += current_mem_node -> size ; prev_mem_node -> size += dynamic_mem_node_size ; // remove current node from list prev_mem_node -> next = current_mem_node -> next ; if ( current_mem_node -> next != null_pointer ) { current_mem_node -> next -> prev = prev_mem_node ; } } } enter fullscreen mode exit fullscreen mode calling free is straightforward. int * ptr = ( int * ) mem_alloc ( n * sizeof ( int )); for ( int i = 0 ; i < n ; ++ i ) { ptr [ i ] = i + 1 ; // shorthand for *(ptr + i) } mem_free ( ptr ); enter fullscreen mode exit fullscreen mode that concludes the dynamic memory management post :) i think i will pause the fros project for a bit now and focus on another project. maybe i will come back at some point and write a file system or so :d cover image by possessed photography on unsplash if you liked this post, you can support me on ko-fi . writing my own operating system (5 part series) 1 writing my own boot loader 2 writing my own vga driver 3 writing my own keyboard driver 4 writing my own shell 5 writing my own dynamic memory management top comments (15) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand cklasacrcez cklasacrcez cklasacrcez follow joined mar 5, 2021 • mar 5 '21 • edited on mar 5 • edited dropdown menu copy link hide hi! i checked your algorithm on a few examples and it does not work properly at all. that is, it does not free memory properly. // console output
a:00293d34
b:00292d24
c:00292d10
d:00291d00  -> here the address should be the same as for b
f:00291d00 enter fullscreen mode exit fullscreen mode // the code int * a = ( int * ) mem_alloc ( sizeof ( int )); int * b = ( int * ) mem_alloc ( 0x1000 ); int * c = ( int * ) mem_alloc ( sizeof ( int )); std :: cout << ""a:"" << a << std :: endl ; std :: cout << ""b:"" << b << std :: endl ; std :: cout << ""c:"" << c << std :: endl ; mem_free ( b ); int * d = ( int * ) mem_alloc ( 0x1000 ); // here should be adress of b std :: cout << ""d:"" << d << std :: endl ; int * e = ( int * ) mem_alloc ( sizeof ( int )); mem_free ( d ); int * f = ( int * ) mem_alloc ( 0x1000 ); std :: cout << ""f:"" << f << std :: endl ; enter fullscreen mode exit fullscreen mode like comment: like comment: like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • mar 10 '21 dropdown menu copy link hide hmmmm... i tested it a bit but didn't write any thorough unit tests. do you know what's broken? like comment: like comment: 1 like like comment button reply collapse expand fox fox fox follow joined mar 12, 2021 • mar 13 '21 dropdown menu copy link hide your merge_current_node_into_previous function not returning anything when it suppose to return void* like comment: like comment: 2 likes like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • mar 16 '21 dropdown menu copy link hide oops! thanks! :) feel free to submit a pr :) like comment: like comment: 1 like like thread thread fox fox fox follow joined mar 12, 2021 • mar 17 '21 dropdown menu copy link hide how i new to this site like comment: like comment: 2 likes like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • mar 17 '21 dropdown menu copy link hide you can open a new pr on github . you can find instructions how to contribute to github projects on the internet, such as dataschool.io/how-to-contribute-on... let me know if you get stuck :) like comment: like comment: 2 likes like thread thread fox fox fox follow joined mar 12, 2021 • mar 18 '21 dropdown menu copy link hide ok so is my answer is the right answer about whats broken like comment: like comment: 1 like like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • mar 18 '21 dropdown menu copy link hide i don't know but if you make a pr / try it out we can test it :) like comment: like comment: 1 like like thread thread fox fox fox follow joined mar 12, 2021 • mar 18 '21 dropdown menu copy link hide is there a private chat feature like comment: like comment: 1 like like thread thread frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • mar 18 '21 dropdown menu copy link hide i followed you so we can use dev.to/connect or you can hit me up on twitter :) like comment: like comment: 1 like like comment button reply collapse expand juanjoseamaker juanjoseamaker juanjoseamaker follow 14 years old programmer, please help me location earth work master programmer at ibm, google, facebook, netflix, twitter and pornhub joined feb 25, 2021 • feb 25 '21 dropdown menu copy link hide hi, if i want to boot this in my old computer, i have to use the dd command to write the img in the hard disk, because that isn't working like comment: like comment: 2 likes like comment button reply collapse expand frank rosner frank rosner frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 • feb 26 '21 dropdown menu copy link hide great! i always wanted to try it but was too lazy. what did you try exactly and what was the error? like comment: like comment: 1 like like comment button reply collapse expand juanjoseamaker juanjoseamaker juanjoseamaker follow 14 years old programmer, please help me location earth work master programmer at ibm, google, facebook, netflix, twitter and pornhub joined feb 25, 2021 • may 21 '21 dropdown menu copy link hide i will try it again with an usb like comment: like comment: 1 like like comment button reply collapse expand c t c t c t follow just experimenting. joined nov 17, 2024 • nov 19 '24 • edited on nov 19 • edited dropdown menu copy link hide merge_current_node_into_previous (mem.c): update void * to void in the function signature to resolve clang warning about non-void function not returning a value. cleaner console output ;) like comment: like comment: 1 like like comment button reply collapse expand raven black raven black raven black follow joined aug 8, 2024 • aug 9 '24 dropdown menu copy link hide what next paging? like comment: like comment: 1 like like comment button reply view full discussion (15 comments) code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse frank rosner follow my professional interests are cloud and big data technologies, machine learning, and software development. i like to read source code and research papers to understand how stuff works.

pronoun: he work lead pe / sre at datastax joined oct 8, 2017 more from frank rosner hashed wheel timers # datastructures # algorithms # java writing my own shell # terminal # x86 # shell # c writing my own keyboard driver # hardware # x86 # os # c 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/codingnepalweb/minimal-drop-down-menu-bar-with-submenu-using-html-css-4nh2,,,"minimal drop-down menu bar with submenu using html & css - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse codingnepal posted on jul 5, 2020 minimal drop-down menu bar with submenu using html & css # navbar # css # dropdown # menu a drop-down menu (sometimes called pull-down menu or list) is a graphical control element designed to help visitors find specific pages, contents, or features on your website. clicking or hovering on a top-level menu heading indicates a list of options to the dropdown menu. at first, on the webpage, there is only a small menu bar or navbar, but when you clicked on that menu bar then the menu list is sliding down and visible. those submenus or drop menus are hidden at first and when you clicked on their parent menu item then the drop list is shown. i've also added a simple hover color on the list as you can see in the image. you can download the source code files through the given link. click here to download source code files. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss some comments may only be visible to logged-in visitors. sign in to view all comments. code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse codingnepal follow i'm prakash shahi and i am a self-taught front-end developer from nepal. location nepal joined jun 21, 2020 more from codingnepal twitter tweet box with character limit highlighting in html css & javascript # javascript # tweetbox # postbox # css popup share modal ui design using html css & javascript # javascript # css # sharemodal # modalbox poll ui design using html css & javascript # css # javascript # polluidesign # pollui 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/thugdebugger/our-duty-the-decision-20ga,,,"our duty, the decision - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse justin e. samuels posted on mar 16, 2020 our duty, the decision # conferences # react # javascript i'm sure you're aware map showing the global spread of the coronavirus pandemic if you've been somewhat connected to the world within the past three months, then you've heard of the ongoing (now pandemic) situation that has engulfed our world , and crippling our daily life among other things. unlike in the past, this has been an ""all hands on deck"" public-health crisis for the world to overcome together by everyone doing their part, no matter how big or small. around the world, we've witnessed many of our beloved conferences, sporting events, and public gatherings postpone, or cancel in the best interest of public health . this is why render-atlanta has made the decision to shift our dates from upcoming early may, to late august with the hopes that the virus will be under a more controlled situation by then. we understand that this news will come as a disappointment to some of our attendees, but we have a responsibility to our attendees, sponsors, and public health officials to act in the best interest for everyone. atlanta's local commitment atlanta hawk - trae young one of the reasons why it has taken us soo long to announce updated dates was because, we wanted to ensure the smoothest transition of those that'll be effected by our announcement. once the decision was made to postpone, our team went to work rapidly with making the appropriate changes to accommodate our attendees, speakers, and sponsors with updated host hotel room blocks which will honor the same rates as previously noted, and waived change fees for those whom booked airfare using our meeting code through our airline partner delta airlines. we understand that this is a stressful time for those whom have placed their trust, and hard-earned money in our hands, and we wanted to help ease such stress for all those that have supported us. additionally, the city of atlanta and our various local suppliers was very supportive in accommodating our updated dates, and we wouldn't be able to of done such without their help. most exciting of all, we were able to re-confirm all of our speakers, workshop conductors, and sponsors for our updated dates, thus ensuring the promised render-atlanta cultural experience at our event. a community commitment our commitment to the community we understand that without your support that the tech conference community wouldn't be possible, and that the ongoing change of dates among all conference has really put a hurdle in front of some wanting to experience the benefits of attending a tech conference in person. this is why we're happy to announce that we will be offering a 15% discount on our  remaining tickets for all those whom have been effected by a conference that was either delayed or cancelled . to take advantage of this offer, please send your proof-of-purchase of the cancelled/delayed conference to updates@renderatl.com . we hope that this offer will brighten up the day of those whom didn't see the possibility of attending a tech conference in 2020 with renewed hope. ongoing commitment - render-atlanta the experience, and entertainment you are expecting to have at render-atlanta has not changed with our updated dates. we're still expecting a sold-out conference (~60 tickets remain) with great weather, food , and speakers that will correspond with our cultural, and innovation themes of the conference. in fact, that weekend in atlanta is even more special because, there will be many sources of family fun and entertainment going on w/ a slew of festivals, sporting events, and more during our eventful summer . our commitment to our audience remains the same, and we hope that you will still join us in atlanta this august for a cultural experience never before seen at a tech conference. render-atlanta ticket breakdown render-atlanta website: click here render-atlanta coronavirus information: click here stand together we are strong together! i believe strongly that our communities will be able to stand together to overcome this threat to our modern world. with the ongoing efforts among our governments, corporations, and local individuals, i'm confident that out of this pandemic will come a new sense of unity never seen before among our local and global communities. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand brendan carney brendan carney brendan carney follow i'm a software developer at convertkit location kansas work software developer at convertkit joined feb 7, 2020 • mar 16 '20 dropdown menu copy link hide nice work justin and team! this is as smooth a change as could be expected during this time. looking forward to attending in august! like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse justin e. samuels follow justin samuels is a technologist and influencer of all things software engineering, and underrepresented groups in technology. location atlanta education mississippi state university work founder of renderatl && senior software engineer at mailchimp joined mar 11, 2019 more from justin e. samuels renderatl 2023, loaded # conference # javascript # react # events render-atlanta 2021 recap # conference # javascript # react # events keeping you covid free at render-atlanta 2021 # react # javascript # conference 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/gchandra/how-to-find-sql-server-database-restore-and-recovery-time-1ihh,,,"how to find sql server database restore and recovery time ? - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse ganesh chandrasekaran posted on sep 8, 2020 how to find sql server database restore and recovery time ? # sql # database easy to way to determine restore and recovery time. simple query to find out sql server database restore time and percentage of completion. select 
 percent_complete as percentagecomplete
 ,estimated_completion_time/60000 as timetocomplete
 ,status 
from
 sys.dm_exec_requests 
where
 command like '%restore%' enter fullscreen mode exit fullscreen mode simple query to find out sql server database recovery time and percentage of completion. select
 percent_complete as percentagecomplete
 ,estimated_completion_time/60000 as timetocomplete
 ,status
from
 sys.dm_exec_requests 
where
 command like '%recovery%' enter fullscreen mode exit fullscreen mode source : how to find sql server database restore and recovery time ? | by ganesh chandrasekaran | medium ganesh chandrasekaran ・ sep 8, 2020 ・ medium top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse ganesh chandrasekaran follow big data solution architect | adjunct professor | mentor. 
my thoughts and opinions are my own and don't represent the companies i work for. joined aug 2, 2020 more from ganesh chandrasekaran how to fix ssis — test connection failed ssl provider # ssis # ssdt # database # tls easy way select the right column as primary key for a given table # database # mysql # sqlserver # keys sql injection cheat sheet # sql # sqlinjection 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/rudolfolah/angular-testing-nested-components-and-components-that-rely-or-use-other-components-in-their-templates-2ijn,,,"angular testing nested components and components that rely or use other components in their templates - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse rudolf olah posted on dec 1, 2019 angular testing nested components and components that rely or use other components in their templates you have two angular components: homecomponent hellocomponent the template for homecomponent looks like this: <h1> hello! </h1> <app-hello name= ""rudolf"" ></app-hello> enter fullscreen mode exit fullscreen mode the component definition for hellocomponent looks like this: @ component ({ selector : ' app-hello ' , // ... ) export class hellocomponent { @ input () name ; } enter fullscreen mode exit fullscreen mode when you try and test the homecomponent , there will be errors because you need to include the hellocomponent in the testbed configuration. you will have to stub out the nested component and that should look like this: @ component ({ selector : ' app-hello ' , template : '' }) class hellostubcomponent implements partial < hellocomponent > { @ input () name ; } enter fullscreen mode exit fullscreen mode then in the testbed configuration you will have something like this: testbed . configuretestingmodule ({ declarations : [ homecomponent , hellostubcomponent ] }). compilecomponents (); enter fullscreen mode exit fullscreen mode to make things more convenient, you can include the definition of the stub component within your component file: // hello.component.ts const selector = ' app-hello ' ; @ component ({ selector , // ... ) export class hellocomponent { @ input () name ; } @ component ({ selector , template : '' }) export class hellostubcomponent implements partial < hellocomponent > { @ input () name ; } enter fullscreen mode exit fullscreen mode when compiling your angular app, the module will declare hellocomponent . when running your tests, the test bed module configuration will declare hellostubcomponent . you can use partial<t> , a utility type from typescript to ensure that your stubbed component matches the interface of the actual component class. by keeping the stub definition close to the actual component class, you can re-use the stub in multiple tests. stubs and stub providers for services in angular for a service, you can do something similar: // world-service.ts @ injectable ({ providedin : ' root ' }) export class worldservice {} export class worldservicestub {} export const worldservicestubprovider = { provide : worldservice , usefactory : () => new worldservicestub () } enter fullscreen mode exit fullscreen mode then in your tests you can setup the testbed like this: import { worldservicestubprovider } from ' path/to/world-service.ts ' ; // ... testbed . configuretestingmodule ({ // ... providers : [ worldservicestubprovider ] }); // and then you can use the following to get the reference to the stub provider from the testbed: testbed . get ( worldservice ); enter fullscreen mode exit fullscreen mode top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse rudolf olah follow eng manager / staff software eng location canada work eng manager / staff software eng joined jun 9, 2019 more from rudolf olah debian gnu/linux in virtualbox on macos m1 to practice kubernetes # virtualmachine # virtualbox # kubernetes # docker task runners for projects # makefile # just # productivity keeping skills up-to-date as a software developer # learning # beginners 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/abhidon/find-longest-word-in-a-given-string-3iib,,,"find longest word in a given string - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse abhinay donadula posted on feb 28, 2019 find longest word in a given string # challenge # beginners # javascript rules create a function that takes a parameter and returns longest word in that parameter let's assume that this function will always get's a string as it's parameter my solution top comments (14) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand david de los santos boix david de los santos boix david de los santos boix follow big data meerkat location milton keynes education software engineer at university of seville work senior consultant at santander uk joined feb 7, 2019 • mar 1 '19 • edited on mar 1 • edited dropdown menu copy link hide same thing, but using a reducer instead of sorting. const getlongestwordof = ( sentence = '' ) => { return sentence . split ( ' ' ) . reduce (( longest , current ) => { return current . length > longest . length ? current : longest ; }) } getlongestwordof ( 'i am just another solution to the same problem' ); edit: this approach seems to be faster than the sorting one! jsperf.com/longestwordjs/1 like comment: like comment: 5 likes like comment button reply collapse expand meghan (she/her) meghan (she/her) meghan (she/her) follow 24. local trans witch who prefers to do magic with a keyboard. she/her. currently hacking away at making the web less centralized. email hello@nektro.net location massachusetts, usa education b.s. cs @ umass dartmouth work unicycle.co joined mar 13, 2017 • mar 3 '19 dropdown menu copy link hide this is what i thought to do as well 😄 like comment: like comment: 1 like like comment button reply collapse expand abhinay donadula abhinay donadula abhinay donadula follow location chicago work javascript developer joined feb 28, 2019 • mar 1 '19 dropdown menu copy link hide good one, didn't think of reduce 👏 like comment: like comment: like comment button reply collapse expand dian fay dian fay dian fay follow it's pronounced diane. i do data architecture, operations, and backend development. in my spare time i maintain massive.js, a data mapper for node.js and postgresql. work staff engineer joined apr 22, 2017 • mar 1 '19 dropdown menu copy link hide create or replace function get_longest_word ( val text ) returns text as $$ declare longest_word text ; begin select strs into longest_word from regexp_split_to_table ( val , ' \s +' ) as strs order by char_length ( strs ) desc limit 1 ; return longest_word ; end ; $$ language plpgsql ; select get_longest_word ( 'a bc def ghij klm no p' ); like comment: like comment: 4 likes like comment button reply collapse expand abhinay donadula abhinay donadula abhinay donadula follow location chicago work javascript developer joined feb 28, 2019 • mar 1 '19 dropdown menu copy link hide awesome💖 like comment: like comment: 1 like like comment button reply collapse expand danny aziz danny aziz danny aziz follow i make things location london work maker joined feb 20, 2019 • mar 2 '19 • edited on mar 2 • edited dropdown menu copy link hide i know one of the tags is 'javascript' but here is how i would do it in python: def get_longest_word(sentence):
    return max(sentence.split(' '), key=lambda x: len(x))

get_longest_word('iam a verrrrry loooongggg sentence')
# result: loooongggg like comment: like comment: 3 likes like comment button reply collapse expand jason towle jason towle jason towle follow full stack software developer. spend my days coding with c# asp.net mvc, sql, html, css and javascript. work senior software developer joined feb 28, 2019 • mar 3 '19 • edited on mar 3 • edited dropdown menu copy link hide can i chime in with a quick c# and linq one liner? private longest ( string sentance ) { return sentance . split ( ' ' ). orderbydescending ( l => l . length ). firstordefault (); } well, the return statement is a one liner :) like comment: like comment: 2 likes like comment button reply collapse expand v v v follow script kiddie location melbourne, au work backend joined feb 25, 2019 • mar 4 '19 • edited on mar 6 • edited dropdown menu copy link hide recently started digging my way through ghc libraries import data.list cmplength :: string -> string -> ordering cmplength l r = compare ( length r ) ( length l ) findlongest :: string -> string findlongest s = head . sortby ( cmplength ) . words $ s main :: io () main = do putstrln ( findlongest ""iam a verrrrry loooongggg sentence"" ) import data.list longerstring :: string -> string -> string longerstring l r = if length l > length r then l else r findlongest :: string -> string findlongest s = foldr ( \ longest w -> longerstring longest w ) """" ( words s ) main :: io () main = do putstrln ( findlongest ""iam a verrrrry loooongggg sentence"" ) like comment: like comment: 2 likes like comment button reply collapse expand vorsprung vorsprung vorsprung follow location bristol, uk work sre joined feb 4, 2019 • mar 3 '19 • edited on mar 3 • edited dropdown menu copy link hide perl $_=qq|iam a verrrrryyy longggggggg sentence|;
print( (sort{length($b) <=> length($a)} split(qq|\s|))[0]) go package main import ( ""fmt"" ""strings"" ) func main () { s := ""iam a verrrrryyy longggggggg sentence"" b := """" for _ , w := range strings . split ( s , "" "" ) { if len ( w ) > len ( b ) { b = w } } fmt . println ( b ) } like comment: like comment: 2 likes like comment button reply collapse expand avalander avalander avalander follow ""biography is irrelevant"" - seven of nine (probably) if she was asked that question. joined mar 12, 2018 • mar 4 '19 • edited on mar 4 • edited dropdown menu copy link hide when used according to the specification, this function has o(1) complexity*. /** @function getlongestword
 * @param {string} s a string of words with the longest word in the first position.
 * @returns the longest word in a string, if it is the first word in the string, or a random word that occupies the first position in the string otherwise.
 */ const getlongestword => ( s = '' ) => s . split ( ' ' )[ 0 ] getlongestword ( 'loooongggg sentence very iam' ) // 'loooongggg' * it doesn't because split probably has complexity o(n) or something. like comment: like comment: 1 like like comment button reply collapse expand fabien gréard fabien gréard fabien gréard follow joined oct 27, 2018 • mar 3 '19 dropdown menu copy link hide it may not be the most eye perfect code however the perf are close to the reducer solution jsperf.com/longestwordjs/11 i would still go to with the reducer solution, but sometimes doing 'old' things works very nice i also try with for loop / for of, i expected it to be way more faster than everything else but it didn't. from jsperf.com/foreach-vs-reduce-vs-fo... like comment: like comment: 2 likes like comment button reply collapse expand abhinay donadula abhinay donadula abhinay donadula follow location chicago work javascript developer joined feb 28, 2019 • mar 3 '19 dropdown menu copy link hide learned something new regarding jsperf, thank you. 🙏 like comment: like comment: 1 like like comment button reply collapse expand philip hallstrom philip hallstrom philip hallstrom follow location olympia, wa joined mar 21, 2017 • mar 2 '19 dropdown menu copy link hide ignoring punctuation and only considering spaces... ruby solution: def longest_word(str)
  str.split(/\s+/).max_by(&:length)
end like comment: like comment: 2 likes like comment button reply collapse expand vlad burlutskiy vlad burlutskiy vlad burlutskiy follow per aspera ad astra. full stack. never stop exploring!life, liberty and pursuit of happiness! location new york, ny work full stack developer at freelance joined sep 25, 2020 • feb 19 '21 • edited on feb 19 • edited dropdown menu copy link hide bro, but what if it's the following sentence: ""i am a 'very('!) long(?) 'sentence'...!!!!"""" like comment: like comment: 1 like like comment button reply view full discussion (14 comments) code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse abhinay donadula follow location chicago work javascript developer joined feb 28, 2019 trending on dev community hot orms are good, actually. # webdev # programming # javascript # beginners it's 2025 - why is offline file sharing still so broken? # privacy # opensource # productivity # discuss rediscovering my passion: from burnout back to excitement # devjournal # developer # career # leadership 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/pluralsight/building-a-service-to-get-a-city-name-from-a-zip-code-in-net-core-3ld5,Postman,Unit Testing,"building a service to get a city name from a zip code in .net core - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse jeremy morgan for pluralsight posted on sep 4, 2019 • edited on sep 9, 2019 building a service to get a city name from a zip code in .net core # dotnet # beginners # programming # dotnetcore getting to know .net core (3 part series) 1 creating trimmed self contained executables in .net core 2 unit testing with .net core 3 building a service to get a city name from a zip code in .net core i'm looking for honest feedback on my content. please comment or reach out to me on twitter! have you ever seen one of those dropdowns that populates the city based on what zip code you put in? this is a small but neat user interface element, and can be handy for making user input forms simpler and faster to use. what we'll build we are going to build an application that takes a zip code as an input, and returns city information for that zip code that looks like this: it will be a .net core web api application, that you can use for web pages, mobile, whatever. it's kind of a microservice but i hate to call it that. it's a simple restful application. what you'll need to get started for this tutorial i'm using .net core 2.2. you'll need to download the .net core installer from here . you could easily do this project with a text editor and cli, but i'm using visual studio for this tutorial to show off how easy it is to use. you should also download db browser for importing data into the sqlite database. step 1: create a new project. load up visual studio and create a new project. select ""asp.net core web application"" and click next. name the project whatever you'd like. next you'll be presented with a screen that looks like this. select ""api"" and click ""create"". this will scaffold out a new api application. the first thing we'll do is delete the ""values"" controller. then, we'll install some tools so we can use sqlite and entity framework core with this application: at a command prompt or package manager console, run the following: dotnet add package microsoft.entityframeworkcore.sqlite
dotnet add package microsoft.entityframeworkcore.design enter fullscreen mode exit fullscreen mode this will add sqlite support and the design namespace, which has some design components that will help us out. the reason we're using sqlite here is that we want something that will be a self-contained database without a server, and run on all the three platforms. sqlite serves this purpose well, at least for small databases like this. don't forget to restore the project: dotnet restore enter fullscreen mode exit fullscreen mode note: ef core is not required here at all. it would likely be faster with direct access or using something like dapper . but for simplicity we're using ef core. 2. create your model next, create a folder called models, and create a new class. our class will look like this: public class city
{
    [key]
    public string zip { get; set; }
    public float lat { get; set; }
    public float lng { get; set; }
    public string cityname { get; set; }
    public string stateid { get; set; }
    public string statename { get; set; }
    public string timezone { get; set; }
    public icollection<city> cities { get; set; }
} enter fullscreen mode exit fullscreen mode this class represents the ""city"" that will be the data type we're working with. as you can see it has the zip code, which we're going to use here for an id. it contains the latitude/longitude of the city, name, state id (like or), state name and time zone. we'll add a collection for ""cities"" in here as well for returning a collection of cities. save the file. 3. define the context next, we're going to create a context for the database that these cities can live in. create a new class in models that looks like this: public class citycontext : dbcontext
{
    public citycontext(dbcontextoptions<citycontext> options) : base(options){ }

    public dbset<city> cities { get; set; }

    protected override void onconfiguring(dbcontextoptionsbuilder optionsbuilder)
    {
        optionsbuilder.usesqlite(@""data source=database/zipcodes.db""); // windows      
    }
} enter fullscreen mode exit fullscreen mode this is the context we'll use for entity framework core so we can easily manipulate the data. here we allow the options to be injected in the class, and on configuration we'll override it with an optionsbuilder. this is one place where will direct the context to use a specific file, in this case, zipcodes.db . this will be the hosted database with our city information in it. next, open up startup.cs and add the following lines to the top: using microsoft.entityframeworkcore;
using zippy.models; enter fullscreen mode exit fullscreen mode make sure the 2nd line is a path to your models namespace (my project is named zippy). then in the configureservices method, we will add the reference to our database again, and set ef options to use sqlite. var connection = @""data source=database\zipcodes.db"";  // windows
    services.adddbcontext<citycontext>(options => options.usesqlite(connection)); enter fullscreen mode exit fullscreen mode notice where it's commented out and says ""windows"". if you want to run this application in linux or osx you need to reverse the slash so it can be found on the file system: var connection = @""data source=database/zipcodes.db"";  // linux enter fullscreen mode exit fullscreen mode now let's create that database. 4. create our database create a new folder in the project named ""database"". open up db browser and create a new database. create a filename for it to be saved as. you don't need to create any tables or do anything else with it at this time. 5. create a migration our database will only have a single table in it, but we want to create a migration in ef core for it. migrations are important for keeping track of historical changes in your database and helping to restore it when needed. note i'm only talking about schema (layout) information here, not the data stored in it. dotnet ef migrations add initial enter fullscreen mode exit fullscreen mode now we have an initial schema setup. let's update entity framework core: dotnet ef database update enter fullscreen mode exit fullscreen mode now our database and table are created. 6. import the data this will be populated with data from a csv from simple maps . if you're planning on using this for a website, make sure and give them credit with a link back to their website. now that your table is created we're going to populate it with the data from the csv file downloaded from simple maps. for this project i stripped out some of the columns so it looks like this: as derek pointed out, you will need to add a blank field at the end so it matches the columns in your ef generated table. now it's ready to be imported. go to file -> import -> table from csv file: load up the csv file you created earlier. it should look like this: and import the file. you should now have a fully populated database. save the database file. make sure it's set to be copied with the project. now let's make our app interact with it. 7. create a controller now we need to create a cities controller. this is a fairly simple process, but visual studio makes it even easier. right click on your controllers folder and select add -> controller we want to select an api controller with actions using entity framework select city for the model class, and citycontext for the data context class and generate it. visual studio will generate a set of actions automatically to: get a list of cities in the db get a particular city from the zip add a city remove a city update information while these things are pretty trivial to build, it's nice to have it all scaffolded out. for this to be a useable service you'd want to remove the actions to modify the data, and only keep the get functionality. 8. set default route the last step is to set our default route. open up launchsettings.json and modify the two lines marked ""launchurl"": change ""launchurl"": ""api/values"", enter fullscreen mode exit fullscreen mode to ""launchurl"": ""api/cities"", enter fullscreen mode exit fullscreen mode save the file and build the project. press f5 to launch it. the finished product after pressing f5 you'll see the project come up in your web browser. for this service to work as intended, it must take a zipcode as input, this is easily done by appending the zip code to the end of the url: this looks better when formatted in something like postman: you could use this service in many ways for a user interface in forms. this is an easy call from a javascript application or mobile app. conclusion it's really easy to create simple microservices and applications with .net core. they are quick to put together and run very fast and lean. plus, they can run on anything. if you want to learn more about .net core check out some of these great courses or hit up the .net core help site from microsoft. in a future tutorial, i'll show you how to deploy this application to multiple server and cloud environments. note: this application uses data from simple maps (free version). if you intend to use this on your site you should give them a link back or consider the professional version as the data will be more accurate and updated. jeremy what's your .net core iq?? my asp.net core skill iq is 200. not bad, can you beat it? click here to try getting to know .net core (3 part series) 1 creating trimmed self contained executables in .net core 2 unit testing with .net core 3 building a service to get a city name from a zip code in .net core top comments (6) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand derek d derek d derek d follow location mesa, az work senior front end engineer @ bitly joined aug 19, 2018 • sep 9 '19 • edited on sep 9 • edited dropdown menu copy link hide a few errors here: in your screenshot, you name the db ""zipcodes.db"", then you have us run a migration, which creates a ""cities.db"", created from the context. startup.cs references ""zipcodes.db"", instead of the ""cities.db"" references in citycontext. if we change everything to ""cities.db"" (or ""zipcodes.db"") and then try to import the data from a csv, you can't override the existing cities table because db browser won't allow you to import the csv into an existing table (the db created from the migration) if it doesn't have the same number of matching columns. you need to add the ""cityzip"" column to the csv file first. if we create a new db from db browser and import the csv into it, ef will connect, but throw an error because the ""cityzip"" column that ef adds doesn't exist. you name your table on import ""zips"", but your app is looking for the ""cities"" table. make sure to not have your db open in db browser, or ef will report that the db is locked. once you change all occurrences of ""zipcode.db"" in your code to ""cities.db"", add the blank cityzip column to the csv and import it into the db created from the migration, all should run nicely ;) you don't need to manually create the db, the migration will output the file for you. like comment: like comment: 2 likes like comment button reply collapse expand jeremy morgan jeremy morgan jeremy morgan follow silicon forest developer/hacker. i write about generative ai, devops, and linux mostly.

once held the world record for being the youngest person alive. location oregon, usa work training architect at kode kloud joined dec 5, 2017 • sep 9 '19 dropdown menu copy link hide thank you for the helpful feedback. i clearly missed a few things while putting it together. i have made corrections to the article and will have the project hosted up on github soon so people can take a deeper look at it. thanks again! like comment: like comment: 2 likes like comment button reply collapse expand derek d derek d derek d follow location mesa, az work senior front end engineer @ bitly joined aug 19, 2018 • sep 9 '19 • edited on sep 9 • edited dropdown menu copy link hide no problem! i was looking for some resources to learn .net core (i'm a front end dev at a .net shop) and i'd saved your tutorial a while back to try out. the errors actually forced me to learn the material better, since i had to do some troubleshooting to get everything running :p like comment: like comment: 1 like like comment button reply collapse expand jeremy morgan jeremy morgan jeremy morgan follow silicon forest developer/hacker. i write about generative ai, devops, and linux mostly.

once held the world record for being the youngest person alive. location oregon, usa work training architect at kode kloud joined dec 5, 2017 • oct 12 '19 dropdown menu copy link hide it's not really about the service, but learning how to build it that's important. there may also be cases where people want to build their own to match certain requirements and this is a good place to start. like comment: like comment: 1 like like comment button reply collapse expand bojadev bojadev bojadev follow joined sep 5, 2019 • sep 5 '19 dropdown menu copy link hide nice tutorial, what about the data quality of simple map? i know zipcodes for instance in the uk are quite complex, i never found a provider (even paid service) that had good quality data. like comment: like comment: 1 like like comment button reply collapse expand jeremy morgan jeremy morgan jeremy morgan follow silicon forest developer/hacker. i write about generative ai, devops, and linux mostly.

once held the world record for being the youngest person alive. location oregon, usa work training architect at kode kloud joined dec 5, 2017 • sep 6 '19 dropdown menu copy link hide i don't know much about it, my initial spot checks of ones in this area were pretty accurate. from what i understand the most accurate and updated version of it is the one you pay for. i have not researched any uk databases, it sounds like a good business opportunity for someone who gets it right! like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse pluralsight follow pluralsight's official dev.to page. interested in trying pluralsight? we're happy to offer the dev.to community a free 10 day trial. start your trial now more from pluralsight the newest must-have developer tool is chatgpt # programming # productivity # ai # chatgpt building a qr code generator with azure functions # webdev # tutorial # dotnet # azure building an application with go and sqlite # go # programming # tutorial # sql 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/packtpartner/managing-and-handling-the-filesystem-using-net-core-ff8,,,"managing and handling the filesystem using .net core - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse packt posted on dec 27, 2019 managing and handling the filesystem using .net core # net # filesystem # netcore # csharp applications will often need to perform input and output of particular code with files and directories in different environments. the system and system.io namespaces contain classes for this purpose. this article is an excerpt from the book c# 8.0 and .net core 3.0 - modern cross-platform development - fourth edition written by mark j. price. mark follows a step-by-step approach in the book filled with exciting projects and fascinating theory for the readers in this highly acclaimed franchise. handling cross-platform environments and filesystems let's explore how to handle cross-platform environments like the differences between windows and linux or macos. create a new console application named workingwithfilesystems in a folder named chapter09. save the workspace as chapter09 and add workingwithfilesystems to it. import the system.io namespace, and statically import the system.console, system.io.directory, system.environment, and system.io.path types, as shown in the following code: system.io; // types for managing the filesystem
using static system.console;
using static system.io.directory;
using static system.io.path;
using static system.environment; enter fullscreen mode exit fullscreen mode paths are different for windows, macos, and linux, so we will start by exploring how .net core handles this. create a static outputfilesysteminfo method, and write statements to do the following: output the path and directory separation characters output the path of the current directory output some special paths for system files, temporary files, and documents void outputfilesysteminfo()
  {
   writeline(""{0,-33} {1}"", ""path.pathseparator"", pathseparator);
   writeline(""{0,-33} {1}"", ""path.directoryseparatorchar"",
     directoryseparatorchar);
   writeline(""{0,-33} {1}"", ""directory.getcurrentdirectory()"",
     getcurrentdirectory());
   writeline(""{0,-33} {1}"", ""environment.currentdirectory"",
     currentdirectory);
   writeline(""{0,-33} {1}"", ""environment.systemdirectory"", 
     systemdirectory);
   writeline(""{0,-33} {1}"", ""path.gettemppath()"", gettemppath());
   writeline(""getfolderpath(specialfolder"");
   writeline(""{0,-33} {1}"", ""  .system)"", 
     getfolderpath(specialfolder.system));
   writeline(""{0,-33} {1}"", ""  .applicationdata)"",
     getfolderpath(specialfolder.applicationdata));
   writeline(""{0,-33} {1}"", ""  .mydocuments)"",
     getfolderpath(specialfolder.mydocuments));
   writeline(""{0,-33} {1}"", ""  .personal)"",
     getfolderpath(specialfolder.personal));
  } enter fullscreen mode exit fullscreen mode the environment type has many other useful members, including the getenvironmentvariables method and the osversion and processorcount properties. in the main method, call outputfilesysteminfo, as shown in the following code: void main(string[] args)
{
 outputfilesysteminfo();
} run the console application and view the result, as shown in the following screenshot when run on windows: windows uses a backslash for the directory separator character. macos and linux use a forward slash for the directory separator character. managing drives to manage drives, use driveinfo, which has a static method that returns information about all the drives connected to your computer. each drive has a drive type. create a workwithdrives method, and write statements to get all the drives and output their name, type, size, available free space, and format, but only if the drive is ready, as shown in the following code: void workwithdrives()
 {
   writeline(""{0,-30} | {1,-10} | {2,-7} | {3,18} | {4,18}"",
     ""name"", ""type"", ""format"", ""size (bytes)"", ""free space"");
   foreach (driveinfo drive in driveinfo.getdrives())
   {
     if (drive.isready)
     {
       writeline(
         ""{0,-30} | {1,-10} | {2,-7} | {3,18:n0} | {4,18:n0}"",
         drive.name, drive.drivetype, drive.driveformat, 
         drive.totalsize, drive.availablefreespace);
     }
     else
     {
      writeline(""{0,-30} | {1,-10}"", drive.name, drive.drivetype);
     }
   }
 } enter fullscreen mode exit fullscreen mode in main, comment out the previous method call, and add a call to workwithdrives, as shown in the following code: void main(string[] args)
 {
    // outputfilesysteminfo();
   workwithdrives();
 } enter fullscreen mode exit fullscreen mode run the console application and view the result, as shown in the following screenshot: managing directories to manage directories, use the directory, path, and environment static classes. these types include many properties and methods for working with the filesystem, as shown in the following diagram: when constructing custom paths, you must be careful to write your code so that it makes no assumptions about the platform, for example, what to use for the directory separator character. create a workwithdirectories method, and write statements to do the following: define a custom path under the user's home directory by creating an array of strings for the directory names, and then properly combining them with the path type's static combine method. check for the existence of the custom directory path using the static exists method of the directory class. create, and then delete the directory, including files and subdirectories within it, using the static createdirectory and delete methods of the directory class. static

void workwithdirectories()
{
// define a directory path for a new folder
// starting in the user's folder
var newfolder = combine(
getfolderpath(specialfolder.personal), 
""code"", ""chapter09"", ""newfolder""); 
writeline($""working with: {newfolder}"");
// check if it exists
writeline($""does it exist? {exists(newfolder)}"");
// create directory 
writeline(""creating it...""); 
createdirectory(newfolder);
writeline($""does it exist? {exists(newfolder)}"");
write(""confirm the directory exists, and then press enter: ""); 
readline();
// delete directory 
writeline(""deleting it...""); 
delete(newfolder, recursive: true);
writeline($""does it exist? {exists(newfolder)}"");
} in the main method, comment out the previous method call, and add a call to workwithdirectories, as shown in the following code: void main(string[] args)
{
  // outputfilesysteminfo();
  // workwithdrives();
  workwithdirectories();
} enter fullscreen mode exit fullscreen mode run the console application and view the result, and use your favorite file management tool to confirm that the directory has been created before pressing enter to delete it, as shown in the following output:
working with: /users/markjprice/code/chapter09/newfolder does it exist? false creating it... does it exist? true confirm the directory exists, and then press enter: deleting it... does it exist? false managing files when working with files, you could statically import the file type, just as we did for the directory type, but, for the next example, we will not, because it has some of the same methods as the directory type and they would conflict. the file type has a short enough name not to matter in this case. create a workwithfiles method, and write statements to do the following: check for the existence of a file. create a text file. write a line of text to the file. close the file to release system resources and file locks (this would normally be done inside a try-finally statement block to ensure that the  file is closed even if an exception occurs when writing to it). copy the file to a backup. delete the original file. read the backup file's contents and then close it. static

void workwithfiles()
{
// define a directory path to output files
// starting in the user's folder
    var dir = combine(
    getfolderpath(specialfolder.personal),
    ""code"", ""chapter09"", ""outputfiles"");
    createdirectory(dir);
    // define file paths
        string textfile = combine(dir, ""dummy.txt"");
        string backupfile = combine(dir, ""dummy.bak"");
        writeline($""working with: {textfile}"");
// check if a file exists
    writeline($""does it exist? {file.exists(textfile)}"");
// create a new text file and write a line to it 
    streamwriter textwriter = file.createtext(textfile);
    textwriter.writeline(""hello, c#!"");
    textwriter.close(); // close file and release resources 
writeline($""does it exist? {file.exists(textfile)}"");
// copy the file, and overwrite if it already exists 
    file.copy(sourcefilename: textfile,     
    destfilename: backupfile, overwrite: true);
    writeline(
        $""does {backupfile} exist? {file.exists(backupfile)}"");
        write(""confirm the files exist, and then press enter: "");
    readline();
// delete file 
    file.delete(textfile);
    writeline($""does it exist? {file.exists(textfile)}"");
// read from the text file backup 
    writeline($""reading contents of {backupfile}:"");
    streamreader textreader = file.opentext(backupfile);
    writeline(textreader.readtoend());
    textreader.close();
} in main, comment out the previous method call, and add a call to workwithfiles. run the application and view the result, as shown in the following output: with: /users/markjprice/code/chapter09/outputfiles/dummy.txt 
does it exist? false
does it exist? true
does /users/markjprice/code/chapter09/outputfiles/dummy.bak exist? true 
confirm the files exist, and then press enter:
does it exist? false
reading contents of /users/markjprice/code/chapter09/outputfiles/dummy.bak:
hello, c#! managing paths sometimes, you need to work with parts of a path, for example, you might want to extract just the folder name, the file name, or the extension. sometimes, you need to generate temporary folders and file names. you can do this with static methods of the path class. add the following statements to the end of the workwithfiles method: managing paths
writeline($""folder name: {getdirectoryname(textfile)}"");
writeline($""file name: {getfilename(textfile)}"");
writeline(""file name without extension: {0}"",
  getfilenamewithoutextension(textfile));
writeline($""file extension: {getextension(textfile)}"");
writeline($""random file name: {getrandomfilename()}"");
writeline($""temporary file name: {gettempfilename()}""); run the application and view the result, as shown in the following output: folder name: /users/markjprice/code/chapter09/outputfiles file name: dummy.txt file name without extension: dummy file extension: .txt random file name: u45w1zki.co3 temporary file name: /var/folders/tz/xx0y_wld5sx0nv0fjtq4tnpc0000gn/t/tmpyqrepp.tmp gettempfilename creates a zero-byte file and returns its name, ready for you to use. getrandomfilename just returns a filename; it doesn't create the file. to summarize, we explored how to manage and handle filesystem using .net core. if you want to learn the fundamentals, how to build practical applications, and the latest features of c# 8.0 and .net core 3.0, check out our latest book c# 8.0 and .net core 3.0 - modern cross-platform development - fourth edition written by mark j. price. about the author mark j. price is a microsoft specialist: programming in c# and architecting microsoft azure solutions, with more than 20 years of educational and programming experience. since 1993, mark has passed more than 80 microsoft programming exams and specializes in preparing others to pass them too. his students range from professionals with decades of experience to 16-year old apprentices with none. he successfully guides all of them by combining educational skills with real-world experience in consulting and developing systems for enterprises worldwide. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand tawanda nyahuye tawanda nyahuye tawanda nyahuye follow msc big data analytics, .net developer, python, and other geek stuff location harare, zimbabwe education chinhoyi university of technology joined may 23, 2020 • jun 3 '21 dropdown menu copy link hide good article thanks. can you change the code snippets to use markdown code syntax to make it clear example of your snippet: static void outputfilesysteminfo () { writeline ( ""{0,-33} {1}"" , ""path.pathseparator"" , pathseparator ); writeline ( ""{0,-33} {1}"" , ""path.directoryseparatorchar"" , directoryseparatorchar ); writeline ( ""{0,-33} {1}"" , ""directory.getcurrentdirectory()"" , getcurrentdirectory ()); writeline ( ""{0,-33} {1}"" , ""environment.currentdirectory"" , currentdirectory ); writeline ( ""{0,-33} {1}"" , ""environment.systemdirectory"" , systemdirectory ); writeline ( ""{0,-33} {1}"" , ""path.gettemppath()"" , gettemppath ()); writeline ( ""getfolderpath(specialfolder"" ); writeline ( ""{0,-33} {1}"" , "" .system)"" , getfolderpath ( specialfolder . system )); writeline ( ""{0,-33} {1}"" , "" .applicationdata)"" , getfolderpath ( specialfolder . applicationdata )); writeline ( ""{0,-33} {1}"" , "" .mydocuments)"" , getfolderpath ( specialfolder . mydocuments )); writeline ( ""{0,-33} {1}"" , "" .personal)"" , getfolderpath ( specialfolder . personal )); } enter fullscreen mode exit fullscreen mode like comment: like comment: 3 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse packt follow founded in 2004 in birmingham, u.k. packt's mission is to help the world put software to work in new ways, through the delivery of effective learning and information services to it professionals. location birmingham, u.k. joined aug 2, 2018 trending on dev community hot it's 2025 - why is offline file sharing still so broken? # privacy # opensource # productivity # discuss rediscovering my passion: from burnout back to excitement # devjournal # developer # career # leadership comparing qodo and github copilot 🕵️ # ai # vscode # productivity # programming 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/zoltanhalasz/simple-excel-upload-and-chosen-select-tutorial-using-asp-net-core-razor-pages-jquery-and-epplus-2op6,,,"simple excel upload and chosen select tutorial (using asp.net core razor pages, jquery and epplus) - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse zoltan halasz posted on dec 27, 2019 simple excel upload and chosen select tutorial (using asp.net core razor pages, jquery and epplus) # dotnet # javascript # productivity # tutorial in any serious business tool, import and export excel data is a basic feature. this is the fastest way to input data to the database, and excel being so popular, it's the most common for accountants and business people to proceed like this when bulk inputting any data to an application. additionally, i decided to search further jquery plugins to make my razor pages apps more interactive on the front-end, and that's how i found chosen. prerequisites for this tutorial: basic javascript/jquery intermediate razor pages (see my other tutorials for ground knowledge) website is running under: https://excelupload-chosen.zoltanhalasz.net/ code can be downloaded from: https://drive.google.com/open?id=10yzi-orrhh_yn6yaklhcj7zgegzsrzp_ materials i used to prepare this tutorial: https://harvesthq.github.io/chosen/ inspiration for the excel upload: https://www.c-sharpcorner.com/article/using-epplus-to-import-and-export-data-in-asp-net-core/ i use an in-memory database for the application, see my previous datatables 2 tutorial this project is on top of my datatables 2 tutorial, https://dev.to/zoltanhalasz/datatable-js-tutorial-for-net-core-razor-pages-application-part-2-full-crud-1m9j preliminary steps: a. for the razor pages project, include latest package in nuget manager ""epplus"" b. copy the css and js files for chosen in wwwroot, see source https://github.com/harvesthq/chosen/releases/ unzip the file, create a ""chosen"" folder in wwwroot and copy the content c. create a special layout page, containing the references for the css files for formatting reasons call it ""_layoutchosen "" this will be the basis for view of the index page. start by making a copy of the original layout file and rename it. include the following in the head tag of the new layout file, just below site.css <link rel=""stylesheet"" href=""~/chosen/docsupport/prism.css"">
    <link rel=""stylesheet"" href=""~/chosen/chosen.css""> enter fullscreen mode exit fullscreen mode d. use the following file for excel upload: https://drive.google.com/open?id=1u_zq4jrwz5sfxx8ex59vnxdiopkr3wlm steps for the application: index page: on the backend a. we have to populate the select list with all cost categories b. we write a function for filtering, that will be the handler for the form public class indexmodel : pagemodel { private invoicecontext _context ; public list < invoicemodel > invoicelist ; public indexmodel ( invoicecontext context ) { _context = context ; } [ bindproperty ] [ display ( name = ""category"" )] public string selectedcategory { get ; set ; } public ilist < selectlistitem > categorylist { get ; set ; } = new list < selectlistitem >(); public void onget () { invoicelist = _context . invoicetable . tolist (); var distinctcategories = invoicelist . groupby ( test => test . costcategory ). select ( grp => grp . first ()). tolist (); categorylist . add ( new selectlistitem () { text = ""all"" , value = ""all"" }); foreach ( var cat in distinctcategories ) { categorylist . add ( new selectlistitem () { text = cat . costcategory , value = cat . costcategory }); } } public iactionresult onpostfilter () { invoicelist = _context . invoicetable . tolist (); categorylist . add ( new selectlistitem () { text = ""all"" , value = ""all"" }); var distinctcategories = invoicelist . groupby ( test => test . costcategory ). select ( grp => grp . first ()). tolist (); foreach ( var cat in distinctcategories ) { categorylist . add ( new selectlistitem () { text = cat . costcategory , value = cat . costcategory }); } if ( selectedcategory == ""all"" ) selectedcategory = """" ; invoicelist = _context . invoicetable . where ( x => x . costcategory . tolower (). contains ( selectedcategory . tolower ())). tolist (); return page (); } } enter fullscreen mode exit fullscreen mode on the frontend we need to implement the form with the chosen select, and then draw the table. below the table, we implement the chosen jquery action, as per documentation @page
@model indexmodel
@{
    viewdata[""title""] = ""chosen"";
    layout = ""_layoutchosen"";
} <div class= ""text-center"" > <h1 class= ""display-4"" > invoice list without datatable </h1> <p> <a asp-page= ""datatablearrayrender"" > show datatable </a> </p> <p> <a asp-page= ""excelupload"" > upload excel file </a> </p> </div> <form class= ""col-8"" id= ""filterform"" method= ""post"" asp-page-handler= ""filter"" > <div class= ""form-row"" > <label asp-for= ""selectedcategory"" class= ""col-form-label col-sm-2"" ></label> <select class= ""chosen-select"" asp-for= ""selectedcategory"" data-placeholder= ""choose a category..."" asp-items= ""@model.categorylist"" onchange= ""this.form.submit()"" ></select> </div> </form> <table class= ""table table-sm"" > <thead> <tr> <th> @html.displaynamefor(model => model.invoicelist[0].invoicenumber) </th> <th> @html.displaynamefor(model => model.invoicelist[0].amount) </th> <th> @html.displaynamefor(model => model.invoicelist[0].costcategory) </th> <th> @html.displaynamefor(model => model.invoicelist[0].period) </th> <th></th> </tr> </thead> <tbody> @foreach (var item in model.invoicelist)
        { <tr> <td> @html.displayfor(modelitem => item.invoicenumber) </td> <td> @html.displayfor(modelitem => item.amount) </td> <td> @html.displayfor(modelitem => item.costcategory) </td> <td> @html.displayfor(modelitem => item.period) </td> <td></td> </tr> } </tbody> </table> <script src= ""~/chosen/docsupport/jquery-3.2.1.min.js"" type= ""text/javascript"" ></script> <script src= ""~/chosen/chosen.jquery.js"" type= ""text/javascript"" ></script> <script src= ""~/chosen/docsupport/prism.js"" type= ""text/javascript"" charset= ""utf-8"" ></script> <script src= ""~/chosen/docsupport/init.js"" type= ""text/javascript"" charset= ""utf-8"" ></script> <script> $ ( "" .chosen-select "" ). chosen ({ no_results_text : "" oops, nothing found! "" }); </script> enter fullscreen mode exit fullscreen mode result as below: the excel upload: create a new razor page: excelupload on the backend we will use the library from epplus (using officeopenxml;) we will parse the input excel file, transmitted by the form. for the parsing, we go row by row and get the data. the upload file has to be in the established format according to the invoicemodel class, else the app will throw an exception that we will treat and show an error message. public class exceluploadmodel : pagemodel { private ihostingenvironment _environment ; private invoicecontext _context ; public exceluploadmodel ( ihostingenvironment environment , invoicecontext context ) { _environment = environment ; _context = context ; } [ bindproperty ] public iformfile uploadedexcelfile { get ; set ; } [ bindproperty ] public string message { get ; set ; } public async task < iactionresult > onpostasync () { return await import ( uploadedexcelfile ); } public async task < iactionresult > import ( iformfile formfile ) { if ( formfile == null || formfile . length <= 0 ) { message = ""this is not a valid file."" ; return page (); } if ( formfile . length > 500000 ) { message = ""file should be less then 0.5 mb"" ; return page (); } if (! path . getextension ( formfile . filename ). equals ( "".xlsx"" , stringcomparison . ordinalignorecase )) { message = ""wrong file format. should be xlsx."" ; return page (); } var newlist = new list < invoicemodel >(); try { using ( var stream = new memorystream ()) { await formfile . copytoasync ( stream ); using ( var package = new excelpackage ( stream )) { excelworksheet worksheet = package . workbook . worksheets [ 0 ]; var rowcount = worksheet . dimension . rows ; for ( int row = 2 ; row <= rowcount ; row ++) { newlist . add ( new invoicemodel { //id = row - 1, invoicenumber = int . parse ( worksheet . cells [ row , 1 ]. value . tostring (). trim ()), amount = float . parse ( worksheet . cells [ row , 2 ]. value . tostring (). trim ()), costcategory = worksheet . cells [ row , 3 ]. value . tostring (). trim (), period = worksheet . cells [ row , 4 ]. value . tostring (). trim (), }); } } } } catch ( exception ex ) { message = ""error while parsing the file. check the column order and format."" ; return page (); } list < invoicemodel > oldinvoicelist = _context . invoicetable . tolist (); _context . invoicetable . removerange ( oldinvoicelist ); _context . invoicetable . addrange ( newlist ); _context . savechanges (); //oldinvoicelist = _context.invoicetable.tolist(); return redirecttopage ( ""./index"" ); } } enter fullscreen mode exit fullscreen mode on the front-end we will implement a simple upload form with an excel file as input. below, will be the error message in case the upload and data parsing goes wrong. please use the sample upload xlsx file shown in the beginning. @page
@model datatables.exceluploadmodel
@{
    viewdata[""title""] = ""excelupload"";
    layout = ""~/pages/shared/_layout.cshtml"";
} <h1> excelupload </h1> <form method= ""post"" enctype= ""multipart/form-data"" > <input type= ""file"" asp-for= ""uploadedexcelfile"" accept= "".xlsx"" /> <input type= ""submit"" /> </form> <strong class= ""alert-danger"" > @model.message </strong> enter fullscreen mode exit fullscreen mode showing the error message: i hope some of you find this useful, and let me know your thoughts on these topics! top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse zoltan halasz follow .net enthusiast, self-taught, career transition, c#, sql, javascript, angular location oradea, romania education former management accountant, coding enthusiast work qubiz.com joined nov 5, 2019 more from zoltan halasz print pdf and export excel from rdlc in asp.net core # dotnet # tutorial # csharp showing wildfires on google maps with blazor server # dotnet # blazor # webdev # tutorial simple cryptocurrency app with blazor server including excel export # dotnet # tutorial # blazor # csharp 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/peter/project-euler-5-finding-the-smallest-multiple-44b0,,,"project euler #5 - finding the smallest multiple - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse peter kim frank posted on may 29, 2019 project euler #5 - finding the smallest multiple # projecteuler # challenge project euler (7 part series) 1 project euler #1 - multiples of 3 and 5 2 project euler #2 - even fibonacci numbers ... 3 more parts... 3 project euler #3 - largest prime factor 4 project euler #4 - largest palindrome product 5 project euler #5 - finding the smallest multiple 6 project euler #6 - sum square difference 7 project euler #7 - 10001st prime continuing the wonderful community solutions to project euler . this is problem 5 , finding the smallest multiple. 2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder. what is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20? top comments (40) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 29 '19 • edited on may 29 • edited dropdown menu copy link hide here's mine via paper and pencil. the primes less than 20 are 2, 3, 5, 7, 11, 13, 17 and 19. the lcm of 1,2,3,...,20 is 16*9*5*7*11*13*17*19. like comment: like comment: 11 likes like comment button reply collapse expand alain van hout alain van hout alain van hout follow a software developer with a passion for architecture and an affinity for java and javascript location antwerp joined jun 17, 2017 • may 29 '19 dropdown menu copy link hide that's the only proper way to do it. a software developer's job is to solve problems, ideally as efficiently as is possible and/or as is practical. that may or may not include writing code. like comment: like comment: 2 likes like comment button reply collapse expand eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 30 '19 dropdown menu copy link hide i really like ""make it work, then make it fast"" approach. first implement a naive solution that works, then make it fast if necessary. starting out with the attitude of best solution possible might be very time consuming. like comment: like comment: 3 likes like thread thread alain van hout alain van hout alain van hout follow a software developer with a passion for architecture and an affinity for java and javascript location antwerp joined jun 17, 2017 • may 30 '19 dropdown menu copy link hide i'm not really talking about premature optimization here (on that, i completely agree). it's of course always a balance, because you don't want to get caught in analysis paralysis. but in general i'd say it's still better to 'think before you act'. otherwise you might be devoting a lot of time and energy to creating and then maintaining a complex solution while a simple one would have sufficed. like comment: like comment: 2 likes like thread thread eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 30 '19 dropdown menu copy link hide +1 for 'think before you act'. i wrote my initial comment because of the phrase 'that's the only proper way to do it.' which i disagree with. let's return to the initial problem from the post. what is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20? there are no other requirements about code complexity, performance or anything else. dwayne's solution is clever, but it requires a domain knowledge to understand what's going on. moreover, it's less flexible for the case of different inputs. a naive bruteforce solution is definitely slower, but the code intention is clear from the code itself and doesn't require some implicit wisdom. so, i think both solutions are good and solve the problem. a concrete project's requirements is the most important part when choosing one solution over another. like comment: like comment: 2 likes like thread thread dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 30 '19 • edited on may 30 • edited dropdown menu copy link hide did you notice that all the posted solutions required the same domain knowledge? if all the numbers from 1 to 20 divide the number then the number must be a multiple of all the numbers from 1 to 20. since we want the smallest such number it follows that we want the least common multiple of the numbers from 1 to 20. everyone has to make a similar deduction in order to even begin writing their solution. rather than writing a program to solve it i simply went ahead and did the lcm calculation by hand. the only extra domain knowledge required is knowing how to calculate the lcm of a set of numbers using prime factorization. as much as i'd like to take credit for a clever solution, i must say that the prime factorization method is the standard way one learns to find lcm. ( lcm using prime factorization ) the way i went about my explanation must have made the solution seem clever but my intention there was to describe a thinking pattern one might go through if they don't know much about lcm and how you'd reason your way from an initial answer (1*2*...*20) to the smallest possible answer. also, everyone else used the fact that lcm(a,b)*gcd(a,b) = a*b so that lcm(a,b)=(a*b)/gcd(a,b). now that's not obvious, unless you look it up or you know why it's true which is based on prime factorization (which in itself is a nice proof, though that's beside the point). like comment: like comment: 4 likes like thread thread eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 30 '19 dropdown menu copy link hide i like your way of thought and clean explanation. it's great when you can think deeply about the problem and find a solution without writing a line of code. but there's also another way. just convert the problem from the human language to the programming language and throw it to the computer to calculate the result. what's better? it depends on the environment, because both solutions have their own pros and cons. like comment: like comment: 3 likes like thread thread alain van hout alain van hout alain van hout follow a software developer with a passion for architecture and an affinity for java and javascript location antwerp joined jun 17, 2017 • may 30 '19 • edited on may 30 • edited dropdown menu copy link hide @eugene: i should have elaborated what i meant with 'the only proper way'. i wasn't referring to the fact that dwayne's solution was analytical/mathematical. i was pointing to the fact that he first analysed the problem before coming up with a solution. this has the benefit of often choosing simpler solutions over complex ones. indeed, there was no explicit requirement for simplicity (or performance), but then maintainability never tends to be explicitly mentioned in requirements. dwayne's logic is essentially the following: i need the smallest/simplest number divisible by all the numbers in the range (i.e 1->20) many numbers in that (or any) range are already divisible by others in that range, so all i need are the prime numbers so i need to multiple all the prime numbers in that range note that the logic here is very straightforward and definitely not complex, is very flexible since it isn't limited to the range of the original requirement, can easily be amended when new requirements are added, and can in the future still be implemented as code when things go beyond simple arithmetic (with the added bonus that the choice of programming language is still entirely open). as to domain knowledge, probably the most important lesson that most developers learn at some point is that domain knowledge is the cornerstone to success, and that knowing when and how to acquire it is an invaluable skill. to put it another way: the most productive software developers proactively talk to (domain expert) people and learn from them. (though, as dwayne mentioned, the domain knowledge here is quite limited. i would expect/hope that anyone who attempts this knows the concept of prime numbers, since it's so tightly coupled with the concept of divisibility). like comment: like comment: 3 likes like thread thread eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 30 '19 dropdown menu copy link hide let's follow your algorithm for the range 1 to 10 (for simplicity). numbers in this range: 1,2,3,4,5,6,7,8,9,10. prime numbers in this range: 2,3,5,7. multiplication of these numbers: 2*3*5*7 = 210. which is different from the correct answer 2520, so your algorithm is wrong. sometimes it's better to write a simple, stupid algorithm and let a computer to do the heavy work instead of doing all the work in your brain, which might be complicated and error prone. like comment: like comment: 1 like like thread thread dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 30 '19 • edited on may 30 • edited dropdown menu copy link hide huh? that's not the algorithm. you misunderstood. the explanation i gave is one of many ways someone can discover that what you need to do is to find the lcm of the numbers. you do agree that the problem is implicitly asking you to find the lcm of 1,2,3,...,20? once you reach that point of understanding then you can do at least two things: you can write a program that finds you the lcm. as all the coded solutions do. you can use prime factorization to quickly compute the answer. as i did and it boils down to finding the highest powers of the primes in the given range which would be 2*2*2, 3*3, 5 and 7. and i don't disagree with you. start with brute force and improve. you'd then have something to test your optimizations, clever code etc against. like comment: like comment: 2 likes like thread thread dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 30 '19 dropdown menu copy link hide so i need to multiple all the prime numbers in that range not exactly. you need to multiply by the highest powers of the primes in the range. like comment: like comment: 1 like like thread thread eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 30 '19 dropdown menu copy link hide you do agree that the problem is implicitly asking you to find the lcm of 1,2,3,...,20? yes. and as you mentioned there are at least two ways to solve the problem. both are valid. and i don't disagree with you. start with brute force and improve. you'd then have something to test your optimizations, clever code etc against. we're on the same page 😉 like comment: like comment: 2 likes like comment button reply collapse expand florian rand florian rand florian rand follow designer and software engineer. very bad writer. 
i love cats and skateboarding.

more ? 🍻 : 🖖; location tatooine joined mar 26, 2019 • may 29 '19 dropdown menu copy link hide if you like efficient and practical solutions, here: using mathematica apply[lcm, range[20]] like comment: like comment: 1 like like thread thread alain van hout alain van hout alain van hout follow a software developer with a passion for architecture and an affinity for java and javascript location antwerp joined jun 17, 2017 • may 29 '19 dropdown menu copy link hide or the more generic solution of that kind: google.com/search?q=euler+5+answer like comment: like comment: 1 like like thread thread florian rand florian rand florian rand follow designer and software engineer. very bad writer. 
i love cats and skateboarding.

more ? 🍻 : 🖖; location tatooine joined mar 26, 2019 • may 29 '19 dropdown menu copy link hide 🌈🦄 like comment: like comment: 1 like like thread thread alain van hout alain van hout alain van hout follow a software developer with a passion for architecture and an affinity for java and javascript location antwerp joined jun 17, 2017 • may 29 '19 dropdown menu copy link hide 😁 like comment: like comment: 2 likes like comment button reply collapse expand mary thompson mary thompson mary thompson follow a compsci aggie. professional since 2013. am i doing it right? location houston, texas education texas a&m work programmer/analyst  at small company that shall not be named joined may 28, 2019 • may 29 '19 dropdown menu copy link hide i wish i could understand this answer. it looks simple. i love software development, but i am horrible with math. simple addition gives me anxiety. like comment: like comment: 5 likes like comment button reply collapse expand dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 29 '19 dropdown menu copy link hide hey mary, let me help you understand this answer. but first. this problem as do most problems on project euler requires domain knowledge in mathematics. in particular, prealgebra . you can become a good software developer without knowing lots of math. furthermore, the types of problems you'd encounter on project euler won't prepare you for developing reliable, maintainable, user-friendly software. in short, don't get discouraged. learn the domain knowledge on an as needed basis as the requirements of your software demands. and my last bit of advice is to read a mind for numbers - how to excel at math and science if you want help getting over that math anxiety. here goes. start with a smaller problem. in this case, the example given is small enough and we can use it to check our answer. what is the smallest positive number that is divisible by all of the numbers from 1 to 10? let's ignore 1 because every positive number is divisible by 1. a positive number that's divisible by 2,3,...,10 is 2*3*...*10 because 2,3,...,10 are all factors of that number. but is it the smallest? no and here's a simple reason why we might think so by looking at the powers of 2. 2, 4 and 8 are all factors of 2*3*...*10. but since 2 and 4 are factors of 8 it follows that 2, 4 and 8 will also be factors of 3*5*6*7*8*9*10. so we found a smaller number that works. similar reasoning leads us to smaller and smaller numbers in the following way. since 3 is a factor of 9 it follows that 3 and 9 will still be factors of 5*6*7*8*9*10. look at this. 5 is a factor of 10. so should we get rid of the 5 and leave the 10? 5 is a factor but so is 2. but 2 is already accounted for by the 8. because of this we leave the 5 and remove the 10. we're now left with 5*6*7*8*9. 8 accounts for 2, 4 and 8. 9 accounts for 3 and 9. 5 accounts for 5. and the number is divisible by 10 because it is divisible by 2 and 5. what about 6 and 7? 6 divides 5*7*8*9 so we can leave it out. however, 7 must remain since 7 does not divide 5*8*9. therefore, we are left with: 5*7*8*9=(2*2*2)*(3*3)*5*7. if you follow similar reasoning for the larger case you'd get the answer that i got. what you'd notice is that all you're doing is finding the least common multiple of all the numbers and that's why you see the coded solutions are calculating lcm. like comment: like comment: 7 likes like thread thread mary thompson mary thompson mary thompson follow a compsci aggie. professional since 2013. am i doing it right? location houston, texas education texas a&m work programmer/analyst  at small company that shall not be named joined may 28, 2019 • may 30 '19 dropdown menu copy link hide i dont know how to do the quote format.. this is my first issue though: ""a positive number that's divisible by 2,3,...,10 is 2*3*...*10 because 2,3,...,10 are all factors of that number."" i don't understand how one came up with that conclusion and by divisible you mean without a remainder? is it because you multiplied them all together that dividing by one of the numbers puts you in the same place you started before multiplying? ""but since 2 and 4 are factors of 8 it follows that 2, 4 and 8 will also be factors of 3*5*6*7*8*9*10."" i have no idea how this ""follows""; same with 3 and 5... esp. 5 because you remove 10 instead. why didn't you remove 9 instead of 3? this is not confusing at all. and why does ""6 divide 5*7*8*9"" but ""7 does not divide 5*8*9""? did you do the math to determine this or can this be explained without trial and error? there are still assumptions in this solution, but thanks for trying to explain, though. and i've already ordered that math and science book. thank you. like comment: like comment: 1 like like thread thread dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 30 '19 dropdown menu copy link hide is it because you multiplied them all together that dividing by one of the numbers puts you in the same place you started before multiplying? yes. i have no idea how this ""follows"". 3*5*6*7*8*9*10 = 3*5*6*7*(2*4)*9*10 => 2, 4 and 8 are factors. if 10 = 2*5 then 2 and 5 are factors. so that's what i'm doing above. why didn't you remove 9 instead of 3? if i removed 9 then 9 would no longer be a factor of what remains. there's no way to make a 9 with what remains. and why does ""6 divide 5*7*8*9"" because we can take a 2 from 8 and a 3 from 9 to show that 6 is a factor, i.e. 5*7*8*9 = 5*7*4*(2*3)*3 = 5*7*4*6*3. but ""7 does not divide 5*8*9"" 5*8*9 = 5*2*2*2*3*3, see no 7's :). like comment: like comment: 2 likes like thread thread mary thompson mary thompson mary thompson follow a compsci aggie. professional since 2013. am i doing it right? location houston, texas education texas a&m work programmer/analyst  at small company that shall not be named joined may 28, 2019 • may 31 '19 dropdown menu copy link hide i see. this makes more sense. thank you. like comment: like comment: 2 likes like thread thread dwayne crooks dwayne crooks dwayne crooks follow learn to build reliable web applications with elm. location trinidad & tobago joined apr 29, 2019 • may 31 '19 dropdown menu copy link hide you're welcome. like comment: like comment: 1 like like comment button reply collapse expand 1adityas 1adityas 1adityas follow location india work student joined apr 1, 2020 • apr 1 '20 dropdown menu copy link hide hey but in euiler project pdf there is another more efficient algorithm is given .. can you explain me that please? like comment: like comment: 1 like like comment button reply collapse expand ali spittel ali spittel ali spittel follow passionate about education, python, javascript, and code art. email hello@welearncode.com location denver, co education hamilton college work developer things at aws joined oct 25, 2017 • may 29 '19 dropdown menu copy link hide here's mine! def greatest_common_denominator ( a , b ): while b : a , b = b , a % b return a def least_common_multiple ( a , b ): return ( a * b ) / greatest_common_denominator ( a , b ) def least_common_multiple_range ( li ): if len ( li ) == 2 : return least_common_multiple ( li [ 0 ], li [ 1 ]) else : check = li . pop () return least_common_multiple ( check , least_common_multiple_range ( li )) print least_common_multiple_range ( range ( 1 , 21 )) like comment: like comment: 4 likes like comment button reply collapse expand massimo artizzu massimo artizzu massimo artizzu follow senior web developer 🔥 ~ conf speaker 🎙️ ~ loves science 🔭, art 🎨, rugby 🏉 ~ reinventing a better wheel 🎡 location italy education mathematics work consultant at antreem joined jan 12, 2017 • may 29 '19 dropdown menu copy link hide the most significant part here is that you can compute the least common multiple by computing the greatest common denominator and using it to divide the product of the two numbers. i was about to propose a similar solution (in javascript) but yours is sufficient 👍 (yes, the language itself isn't important for me. just the mathematical challenge.) like comment: like comment: 2 likes like comment button reply collapse expand by by by follow joined jan 13, 2020 • jan 14 '20 dropdown menu copy link hide please do propose your version in javascript. certainly it will not be excessive like comment: like comment: 1 like like comment button reply collapse expand tweedoriginal tweedoriginal tweedoriginal follow aspiring software developer. location nigeria work mr at student joined jan 15, 2020 • jan 15 '20 dropdown menu copy link hide can you please explain your code like comment: like comment: like comment button reply collapse expand eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 29 '19 dropdown menu copy link hide bruteforce node solution 🤣 const assert = require ( ' assert ' ); console . clear (); function findsmallestmultiple ( numbers ) { function isdividable ( num ) { for ( let i = numbers . length - 1 ; i >= 0 ; i -- ) { if ( num % numbers [ i ] !== 0 ) return false ; } return true ; } let counter = 1 ; while ( true ) { if ( isdividable ( counter )) break ; counter ++ ; } return counter ; } // assert(findsmallestmultiple([7,8,9,10]) === 2520); console . log ( findsmallestmultiple ([ 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 ])); like comment: like comment: 3 likes like comment button reply collapse expand ademola john ademola john ademola john follow joined apr 24, 2018 • may 18 '20 dropdown menu copy link hide please how do i put my code in this snippet like you did when  i want to comment with my own answer like comment: like comment: 1 like like comment button reply collapse expand eugene karataev eugene karataev eugene karataev follow undefined is not a function location russia, novosibirsk joined apr 15, 2019 • may 18 '20 dropdown menu copy link hide wrap your content with three backtics to render it as a code snippet. const yourcode = ' here ' ; like comment: like comment: 1 like like thread thread ademola john ademola john ademola john follow joined apr 24, 2018 • may 18 '20 dropdown menu copy link hide thannk you so much eugene. this gave me tough time like comment: like comment: 1 like like comment button reply collapse expand florian rand florian rand florian rand follow designer and software engineer. very bad writer. 
i love cats and skateboarding.

more ? 🍻 : 🖖; location tatooine joined mar 26, 2019 • may 29 '19 • edited on may 29 • edited dropdown menu copy link hide here's mine c #include <stdio.h> unsigned long gcd ( unsigned long a , unsigned long b ) { while ( a != 0 ) { unsigned long c = a ; a = b % a ; b = c ; } return b ; } unsigned long lcm ( unsigned long a , unsigned long b ) { return a * ( b / gcd ( a , b ) ); } int main () { unsigned int i = 2 ; unsigned long result = 1 ; for ( i = 2 ; i < 20 ; i ++ ) { result = lcm ( result , i ); } printf ( ""%d \n "" , result ); } and go package main import ( ""fmt"" ) func gcd ( a , b int64 ) int64 { for b != 0 { a , b = b , a % b } return b } func lcm ( a , b int64 ) int64 { return a * ( b / gcd ( a , b )) } func main () { var result , i int64 = 1 , 2 for ; i <= 20 ; i ++ { result = lcm ( result , i ) } fmt . println ( result ) } like comment: like comment: 2 likes like comment button reply collapse expand jay jay jay follow a polyglot programmer joined mar 19, 2017 • may 29 '19 dropdown menu copy link hide rust solution playground fn main () { println! ( ""{}"" , get_number ( 1 , 20 )) } fn get_number ( start : usize , end : usize ) -> u32 { ( start .. end ) .fold ( 1 _u32 , | acc , n | lcm ( acc , n as u32 )) } fn lcm ( a : u32 , b : u32 ) -> u32 { ( a * b ) / gcd ( a , b ) } fn gcd ( a : u32 , b : u32 ) -> u32 { if b == 0 { a } else { gcd ( b , a % b ) } } like comment: like comment: 2 likes like comment button reply collapse expand khouloudzaiter khouloudzaiter khouloudzaiter follow i am a data engineer junior, i like improve my algorithmic skills and my way of solving different problems. location france work big data engineer at lansrod joined aug 5, 2019 • aug 5 '19 dropdown menu copy link hide i like solving it via paper and pencil, but it is always a pleasure to do some code :) java code: public class problem5 { public static void main ( string [] args ) { int max = 20 ; int n = ( int ) ( math . pow ( max , 2 ) - 1 ); boolean smallestnumberevendivisible = false ; //int i = 2; while ( ! smallestnumberevendivisible ){ n ++; int i = 2 ; boolean alldivisible = true ; while ( i <= max && alldivisible ){ alldivisible = n % i == 0 ; i ++; } smallestnumberevendivisible = alldivisible ; } system . out . println ( n ); } } like comment: like comment: 1 like like comment button reply collapse expand nathan tamez nathan tamez nathan tamez follow location portsmouth,uk work software engineer @ vonage joined may 7, 2019 • may 30 '19 dropdown menu copy link hide here is my solution. it's not the cleanest but is quite fast and it seems to work. i used nodejs function checkdivisible ( number ) { if ( number % 20 == 0 && number % 19 == 0 && number % 18 == 0 && number % 17 == 0 && number % 16 == 0 && number % 15 == 0 && number % 14 == 0 && number % 13 == 0 && number % 12 == 0 && number % 11 == 0 && number % 10 == 0 && number % 9 == 0 && number % 8 == 0 && number % 7 == 0 && number % 6 == 0 && number % 5 == 0 && number % 4 == 0 && number % 3 == 0 && number % 2 == 0 && number % 1 == 0 ) { return true ; } else { return false ; } } function main () { let done = false ; let c = 1 ; console . log ( `c = ${ c } , incrment = ${ c } ` ); let starttime = date . now (); while ( ! done ) { if ( checkdivisible ( c )) { console . log ( ` ${ c } is divisible by 1 - 20 with no remainder` ); done = true ; } c ++ ; } let endtime = date . now (); console . log ( `time taken: ${ math . round ( endtime - starttime )} ms` ); } main (); here is the output c = 1, incrment = 1
232792560 is divisible by 1 - 20 with no remainder
time taken: 607ms like comment: like comment: 1 like like comment button reply collapse expand thorsten hirsch thorsten hirsch thorsten hirsch follow joined feb 5, 2017 • may 30 '19 • edited on may 30 • edited dropdown menu copy link hide perl 6 has an lcm operator (no modules/libraries necessary): say [ lcm ] 1 .. 20 ; i guess you won't find a solution shorter than this one. however i doubt that this makes perl 6 attractive. the more features the language has, the more you have to learn. if you master it, it sure is fun to write code like this or the next example: say [ + ] grep * %% ( 3 | 5 ), ^ 1000 ; it's a solution for euler#1 and it's another example that the language has a ton of features that enable you to write super short code. unfortunately it also gets harder to read, some might even call it gibberish. but you can also write beautiful code in perl 6. here are solutions for project euler problems, super-short ones as well as beautiful ones. 😄 like comment: like comment: 1 like like comment button reply collapse expand prabhjot singh rana prabhjot singh rana prabhjot singh rana follow mainframe guy, started to learn oops concepts and python.. location vienna work lead technology at nagarro software joined feb 4, 2020 • feb 5 '20 dropdown menu copy link hide my solution in python: result = 1 num = 20 list1 = [] list2 = [] index1 = true for i in range ( 2 , num + 1 ): list1 . append ( i ) while len ( list1 ) > 0 : a = list1 [ 0 ] for i in list1 : if index1 == true : index1 = false result = result * i continue else : if i % a != 0 : list2 . append ( i ) else : list2 . append ( i / a ) index1 = true list1 = list2 [:] list2 = [] print ( result ) like comment: like comment: 1 like like comment button reply collapse expand engosama69 engosama69 engosama69 follow joined mar 9, 2020 • mar 9 '20 • edited on mar 9 • edited dropdown menu copy link hide smallest number that can be divided by each of the numbers from 1 to 10 without any remainder for smallest_num in range(1, 1000000): check_list = [] for x in range(1, 11): if smallest_num % x == 0: check_list.append(x) check_list.sort() if check_list == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]: print(""smallest number is : "", smallest_num) exit() like comment: like comment: 1 like like comment button reply view full discussion (40 comments) code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse peter kim frank follow doing a bit of everything at dev / forem education wesleyan university pronouns he/him work co-founder joined jan 3, 2017 more from peter kim frank ruby challenge: calculating factorials with negative numbers # codenewbie # challenge # beginners # ruby challenge: counting numbers with 7s # challenge project euler #7 - 10001st prime # projecteuler # challenge 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/cr0wst/beefing-up-your-spring-security-with-two-factor-authentication-4m5p,,,"beefing up your spring security with two-factor authentication - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse steve crow posted on mar 22, 2019 • originally published at smcrow.net on apr 7, 2018 beefing up your spring security with two-factor authentication # java # spring # nexmo # security two-factor authentication adds an extra layer of security to your web application by asking users to provide a second form of identification. common second factors include: authenticator codes biometrics email or text message codes let’s explore how you can add two-factor authentication to an existing web application by utilizing nexmo. before you begin in order to follow along with the tutorial you will need the following: a general understanding of java and enterprise java technologies the java sdk installed on your computer a nexmo developer account along with an api key and secret a clone of the getting-started branch on github get the code clone the getting-started branch. git clone https://github.com/cr0wst/demo-twofactor.git -b getting-started cd demo-twofactor let’s see what we’re working with the example application is built using spring boot . if you have gradle installed on your system, you should be able to execute the bootrun task to start the application. if not, no worries; the repository contains a gradle wrapper which will still allow you to execute tasks. ./gradlew bootrun this will download any dependencies, compile the application, and start the embedded server. once the server has been started, you should be able to navigate to http://localhost:8080 to see the sample application. there are three pages: the home page - accessible by everybody. the login page - allows users to enter a username and password (default is demo / demo ). the secret page - accessible only by users with the role.users role. adding two-factor authentication when users log in, our only acceptance criteria is that they have provided a username and a password. what if this information was stolen? what is something that we could use which is physically located near the user? there is something that i guarantee almost 90% of you, and our users, have within arm’s reach. a mobile phone. here’s how it’s going to work: the user will log in to our application as they normally do. they will be prompted to enter a four-digit verification code. simultaneously, a four-digit verification code will be sent to the phone number on their account. if they don’t have a phone number on their account, we will allow them to bypass the two-factor authentication. the code that they enter will be checked to make sure it is the same one that we sent them. we are going to utilize the nexmo verify api to generate the code and to check and see if the code they entered is valid. creating a new role the first step will be to create a new role. this role will be used to hold the authenticated user in a purgatory state until we have verified their identity. add the pre_verification_user role to the role enum. // src/main/net/smcrow/demo/twofactor/user/role.java public enum role implements grantedauthority { user , pre_verification_user ; // ... } in order for it to be applied as the default role, we need to update the getauthorities() method of the standarduserdetails class. // src/main/net/smcrow/demo/twofactor/user/standarduserdetails.java @override public collection <? extends grantedauthority > getauthorities () { set < grantedauthority > authorities = new hashset <>(); authorities . add ( role . pre_verification_user ); return authorities ; } handling verification information nexmo will provide us with a request id that will be used when confirming the code provided by the user. there are a variety of ways we can store this information. in this tutorial, we will be persisting it into a database. storing verification information first, create a verification class in the verify package. // src/main/net/smcrow/demo/twofactor/verify/verification.java @entity public class verification { @id @column ( unique = true , nullable = false ) private string phone ; @column ( nullable = false ) private string requestid ; @column ( nullable = false ) private date expirationdate ; @persistenceconstructor public verification () { // empty constructor for jpa } public verification ( string phone , string requestid , date expirationdate ) { this . phone = phone ; this . requestid = requestid ; this . expirationdate = expirationdate ; } // ... getters and setters } notice, we are also storing the expirationdate . by default, the verify api requests are only valid for five minutes. they will get deleted from the table when either: the user has successfully verified their identity. they are expired. we will take advantage of spring scheduler to clean them up periodically. working with the verification information create the verificationrepository interface in the verify package. // src/mainnet/smcrow/demo/twofactor/verify/verificationrepository.java @repository public interface verificationrepository extends jparepository < verification , string > { optional < verification > findbyphone ( string phone ); void deletebyexpirationdatebefore ( date date ); } deleting expired requests in the twofactor package, create the following configuration class. // src/main/net/smcrow/demo/twofactor/scheduleconfiguration.java @configuration @enablescheduling public class scheduleconfiguration { @autowired private verificationrepository verificationrepository ; @scheduled ( fixeddelay = 1000 ) @transactional public void purgeexpiredverifications () { verificationrepository . deletebyexpirationdatebefore ( new date ()); } } this will set up a scheduled command to be executed every second that will query for any expired verification entities and delete them. setting up the nexmo client we will be using the nexmo-java client for interacting with nexmo. declare the dependency first, declare the following dependency in the build.gradle file. dependencies { // .. other dependencies compile ( ' com . nexmo : client: 3.3 . 0 ' ) } provide information now, define the following information in the application.properties file. # add your nexmo credentials nexmo.api.key = your-api-key
nexmo.api.secret = your-api-secret define the beans next, we are going to define the nexmoclient and verifyclient as beans. this will allow spring to inject them as dependencies into our nexmoverificationservice . add the following definitions to the twofactorapplication class. // src/main/net/smcrow/demo/twofactor/twofactorapplication.java @bean public nexmoclient nexmoclient ( environment environment ) { authmethod auth = new tokenauthmethod ( environment . getproperty ( ""nexmo.api.key"" ), environment . getproperty ( ""nexmo.api.secret"" ) ); return new nexmoclient ( auth ); } @bean public verifyclient nexmoverifyclient ( nexmoclient nexmoclient ) { return nexmoclient . getverifyclient (); } create the nexmoverificationservice we are going to create a service that will allow us to instruct the client to make requests. add the nexmoverificationservice to the verify package. // src/main/net/smcrow/demo/twofactor/verify/nexmoverificationservice.java @service public class nexmoverificationservice { private static final string application_brand = ""2fa demo"" ; private static final int expiration_intervals = calendar . minute ; private static final int expiration_increment = 5 ; @autowired private verificationrepository verificationrepository ; @autowired private userrepository userrepository ; @autowired private verifyclient verifyclient ; public verification requestverification ( string phone ) throws verificationrequestfailedexception { optional < verification > matches = verificationrepository . findbyphone ( phone ); if ( matches . ispresent ()) { return matches . get (); } return generateandsavenewverification ( phone ); } public boolean verify ( string phone , string code ) throws verificationrequestfailedexception { try { verification verification = retrieveverification ( phone ); if ( verifyclient . check ( verification . getrequestid (), code ). getstatus () == 0 ) { verificationrepository . delete ( phone ); return true ; } return false ; } catch ( verificationnotfoundexception e ) { requestverification ( phone ); return false ; } catch ( ioexception | nexmoclientexception e ) { throw new verificationrequestfailedexception ( e ); } } private verification retrieveverification ( string phone ) throws verificationnotfoundexception { optional < verification > matches = verificationrepository . findbyphone ( phone ); if ( matches . ispresent ()) { return matches . get (); } throw new verificationnotfoundexception (); } private verification generateandsavenewverification ( string phone ) throws verificationrequestfailedexception { try { verifyresult result = verifyclient . verify ( phone , application_brand ); if ( stringutils . isblank ( result . geterrortext ())) { string requestid = result . getrequestid (); calendar now = calendar . getinstance (); now . add ( expiration_intervals , expiration_increment ); verification verification = new verification ( phone , requestid , now . gettime ()); return verificationrepository . save ( verification ); } } catch ( ioexception | nexmoclientexception e ) { throw new verificationrequestfailedexception ( e ); } throw new verificationrequestfailedexception (); } } there are two main methods in this class: requestverficiation which is used to, well, request verification. verify which is used to verify the provided code provided by the user. the requestverification method the method first checks to see if we already have a pending verification request for the user’s phone number. this allows us to serve the same request id to the user if they attempt to log in to the application again. if there isn’t any prior verification, then a new verification code is requested and saved to the database. if for some reason, we are unable to assign them a new code a verificationrequestfailedexception is thrown. add this exception to the verify package. // src/main/net/smcrow/demo/twofactor/verify/verificationrequestfailedexception.java public class verificationrequestfailedexception extends throwable { public verificationrequestfailedexception () { this ( ""failed to verify request."" ); } public verificationrequestfailedexception ( string message ) { super ( message ); } public verificationrequestfailedexception ( throwable cause ) { super ( cause ); } } the verify method the verify method sends the request id and code to nexmo for verification. nexmo returns a status of zero if the verification was successful. on successful verification, the verification entity is removed from the database, and true is returned. if we were unable to find the verification entity, maybe it expired, we request a new one and return false. if there are any issues verifying, we throw a verificationrequestfailedexception . the retrieveverification method will throw a verificationnotfoundexception if the verification wasn’t found. add this exception to the verify package. // src/main/net/smcrow/demo/twofactor/verify/verificationnotfoundexception.java public class verificationnotfoundexception extends throwable { public verificationnotfoundexception () { this ( ""failed to find verification."" ); } public verificationnotfoundexception ( string message ) { super ( message ); } } using the nexmoverificationservice we’re going to use the service for both sending a code and verifying the code. sending a code is done after a successful authentication. triggering the request for verification let’s implement a custom authenticationsuccesshandler which will be called after the user has successfully authenticated. add the twofactorauthenticationsuccesshandler to the verify package. // src/main/net/smcrow/demo/twofactor/verify/twofactorauthenticationsuccesshandler.java @component public class twofactorauthenticationsuccesshandler implements authenticationsuccesshandler { private static final string verification_url = ""/verify"" ; private static final string index_url = ""/"" ; @autowired private nexmoverificationservice verificationservice ; @autowired private userrepository userrepository ; @override public void onauthenticationsuccess ( httpservletrequest request , httpservletresponse response , authentication authentication ) throws ioexception { string phone = (( standarduserdetails ) authentication . getprincipal ()). getuser (). getphone (); if ( phone == null || ! requestandregisterverification ( phone )) { bypassverification ( request , response , authentication ); return ; } new defaultredirectstrategy (). sendredirect ( request , response , verification_url ); } private boolean requestandregisterverification ( string phone ) { try { return verificationservice . requestverification ( phone ) != null ; } catch ( verificationrequestfailedexception e ) { return false ; } } private void bypassverification ( httpservletrequest request , httpservletresponse response , authentication authentication ) throws ioexception { verificationservice . updateauthentication ( authentication ); new defaultredirectstrategy (). sendredirect ( request , response , index_url ); } } when a user has successfully authenticated, we check to see if they have a phone number. if they have a phone number, a code is sent to their device. if they don’t have a phone number, or we are unable to send a code, we allow them to bypass verification. the bypassverification method relies on the updateauthentication method of the nexmoverificationservice . add this to the nexmoverificationservice : // src/main/net/smcrow/demo/twofactor/verify/nexmoverificationservice.java public void updateauthentication ( authentication authentication ) { role role = retrieverolefromdatabase ( authentication . getname ()); list < grantedauthority > authorities = new arraylist <>(); authorities . add ( role ); authentication newauthentication = new usernamepasswordauthenticationtoken ( authentication . getprincipal (), authentication . getcredentials (), authorities ); securitycontextholder . getcontext (). setauthentication ( newauthentication ); } private role retrieverolefromdatabase ( string username ) { optional < user > match = userrepository . findbyusername ( username ); if ( match . ispresent ()) { return match . get (). getrole (); } throw new usernamenotfoundexception ( ""username not found."" ); } this method is used to assign the role defined in the database to the current user and removes the pre_verification_user role. prompting the user for a code once the user has been sent a code, they are forwarded to the verification page. let’s work on creating that page next. create a new html file called verify.html in the resources/templates directory. <!doctype html> <html lang= ""en"" xmlns:th= ""http://www.thymeleaf.org"" xmlns:layout= ""http://www.ultraq.net.nz/thymeleaf/layout"" layout:decorator= ""default"" > <head> <meta charset= ""utf-8"" /> <title> two factor authorization demo </title> </head> <body> <div layout:fragment= ""content"" class= ""container"" > <div class= ""col-lg-12 alert alert-danger text-center"" th:if= ""${param.error}"" > there was an error with your login. </div> <div class= ""col-lg-4 offset-lg-4 text-left"" > <form th:action= ""@{/verify}"" method= ""post"" > <h1> verify </h1> <p> a text message has been sent to your mobile device. please enter the code below: </p> <div class= ""form-group"" > <label for= ""code"" > verification code </label> <input type= ""text"" class= ""form-control"" id= ""code"" name= ""code"" placeholder= ""4-digit code"" /> </div> <button type= ""submit"" class= ""btn btn-primary"" > verify </button> </form> </div> </div> </body> </html> we also need a controller to serve the page to the user. create the verificationcontroller in the verify package. // src/main/net/smcrow/demo/twofactor/verify/verificationcontroller.java @controller public class verificationcontroller { @autowired private nexmoverificationservice verificationservice ; @preauthorize ( ""hasrole('pre_verification_user')"" ) @getmapping ( ""/verify"" ) public string index () { return ""verify"" ; } @preauthorize ( ""hasrole('pre_verification_user')"" ) @postmapping ( ""/verify"" ) public string verify ( @requestparam ( ""code"" ) string code , authentication authentication ) { user user = (( standarduserdetails ) authentication . getprincipal ()). getuser (); try { if ( verificationservice . verify ( user . getphone (), code )) { verificationservice . updateauthentication ( authentication ); return ""redirect:/"" ; } return ""redirect:verify?error"" ; } catch ( verificationrequestfailedexception e ) { // having issues generating keys let them through. verificationservice . updateauthentication ( authentication ); return ""redirect:/"" ; } } } this controller serves the verification page via the index method, and it handles the form submission via the verify method. this page is only accessible to users with the pre_verification_user role. on successful verification, the updateauthentication method is, once again, used to replace this role with their persisted one. finishing up the verification chain the final step is to update the appsecurityconfiguration to use our twofactorauthenticationsuccesshandler . modify the appsecurityconfiguration to wire in our handler and use it via the successhandler method. // src/main/net/smcrow/demo/twofactor/appsecurityconfiguration.java @autowired private twofactorauthenticationsuccesshandler twofactorauthenticationsuccesshandler ; @override protected void configure ( httpsecurity httpsecurity ) throws exception { // webjar resources httpsecurity . authorizerequests (). antmatchers ( ""/webjars/**"" ). permitall () . and (). formlogin (). loginpage ( ""/login"" ). permitall () . successhandler ( twofactorauthenticationsuccesshandler ) . and (). logout (). permitall (); } try it out! you will need to add your phone number to the data.sql file. we aren’t going to be doing any validation on the phone number, and it needs to be in e.164 format. insert into user ( username , password , role , phone ) values ( 'phone' , 'phone' , 'user' , 15555555555 ); you should now be up and running. boot up the application, and try to log in. assuming that your api key, api secret, and seeded phone number are correct; you should receive a text message with a four-digit code. what did we do? we did a lot of things. in short, we implemented two-factor authentication to better secure our application. we did this by: creating a custom authenticationsuccesshandler to forward the user to a verification page after serving them a code. using the nexmo-java library, by wrapping it in a nexmoverificationservice , to send verification codes to our users. taking advantage of the spring scheduler to delete expired verification codes. building a page for the user to enter their verification code. check out the final code from this tutorial on github. looking ahead there are various ways that two-factor authentication can be implemented. if you’re curious about any of the frameworks and technologies used in the sample code, here’s a rundown: spring boot spring security gradle don’t forget that you can be a nexmo contributor to the nexmo-java client. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand abhimanyu kumar abhimanyu kumar abhimanyu kumar follow joined jul 23, 2019 • jul 26 '19 dropdown menu copy link hide kindly develop and publish a application using spring that will show :- on providing email and password in github account it show a form to verify 
 this device . so please develop a github.com type verify your device via 
 sending a code [ token ] into email also explain the working flow and 
 algorithm implemented by github for this functionality. and notify me on acodeschool@gmail.com like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse steve crow follow steve is a lover of greyhounds, twisty puzzles, and european board games. when not talking math to non-math people, and java to non-java people, he can be found sipping coffee and hacking on code. location columbus, ohio education ms in applied and computational math work software developer at ninjacat joined may 1, 2018 more from steve crow quick tip: kotlin, spring data, and optionals # java # kotlin # springdata # spring publish vonage events to kafka with spring boot # spring # kotlin # kafka # vonage multiple jpa data sources with spring boot and kotlin # kotlin # spring # backend # jpa 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/emmabostian/what-does-your-ide-code-editor-look-like-550e/comments,Postman,,"discussion of what does your ide/code editor look like? - dev community skip to content navigation menu search powered by search algolia log in create account dev community close what does your ide/code editor look like? emma bostian ✨ on august 14, 2019 i have finally gotten my code editor to be my ideal setup. but now i'm curious, what does your code editor/ide look like?




  
  
  editor


i'm ... read full post personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand guilherme de andrade guilherme de andrade guilherme de andrade follow frontend engineer @ viz.ai • focusing on building great user experiences • standing on the shoulders of giants ✨ location porto, portugal work frontend engineer @ viz.ai joined aug 14, 2019 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide editor: neovim font: fira code theme: nord terminal: kitty shell: fish like comment: like comment: 40 likes like comment button reply collapse expand yohanes bandung bondowoso yohanes bandung bondowoso yohanes bandung bondowoso follow i am a software engineer that value and strive for privacy, security and enjoyable services. i code with react (&native), flutter on nvim.

i listen to the trees and the wild and radiohead email hi@ybbond.id location jakarta, indonesia education bina nusantara, unfinished work software engineer at pinhome, prev kumparan joined apr 9, 2019 • aug 16 '19 • edited on aug 16 • edited dropdown menu copy link hide pretty much the same as me! mine is: editor: neovim font: heavily-modified-using-glyph.app-ligaturized-font theme: gruvbox terminal: kitty shell: fish on mac here like comment: like comment: 5 likes like comment button reply collapse expand vinícius hoyer vinícius hoyer vinícius hoyer follow location brazil education i do not have a major work vue.js front end developer 😉 at quero educação joined jun 16, 2018 • aug 16 '19 dropdown menu copy link hide gruvbox is the best thing ever, am i rigth? like comment: like comment: 4 likes like thread thread yohanes bandung bondowoso yohanes bandung bondowoso yohanes bandung bondowoso follow i am a software engineer that value and strive for privacy, security and enjoyable services. i code with react (&native), flutter on nvim.

i listen to the trees and the wild and radiohead email hi@ybbond.id location jakarta, indonesia education bina nusantara, unfinished work software engineer at pinhome, prev kumparan joined apr 9, 2019 • aug 18 '19 dropdown menu copy link hide yes! i discovered it for vscode at first, then noticed i can make my kitty and nvim to use it. calming colors! like comment: like comment: 3 likes like comment button reply collapse expand ahmed khaled ahmed khaled ahmed khaled follow noob hacker/wizard location earth joined jun 14, 2019 • aug 18 '19 dropdown menu copy link hide hey would u mind share with me your exp with ligaturized-font. it's the font the put free-font glyph in any other font right ? like comment: like comment: 2 likes like thread thread yohanes bandung bondowoso yohanes bandung bondowoso yohanes bandung bondowoso follow i am a software engineer that value and strive for privacy, security and enjoyable services. i code with react (&native), flutter on nvim.

i listen to the trees and the wild and radiohead email hi@ybbond.id location jakarta, indonesia education bina nusantara, unfinished work software engineer at pinhome, prev kumparan joined apr 9, 2019 • aug 18 '19 dropdown menu copy link hide i used glyphsapp.com to edit specific characters (for instance, the italicized ""s"") or rearrange the unicode char to be used as normal char. and then, i converted it to be ligature with github.com/toxicfrog/ hope that explains! like comment: like comment: 3 likes like comment button reply collapse expand natalia natalia natalia follow frontend developer and css maniac. location manchester work senior software engineer at bbc joined jul 21, 2019 • aug 15 '19 dropdown menu copy link hide this wallpaper is the best! 😍 like comment: like comment: 3 likes like comment button reply collapse expand jacob herrington (he/him) jacob herrington (he/him) jacob herrington (he/him) follow i used to work on this website!

https://bsky.app/profile/jacobherrington.dev education self-taught joined aug 19, 2018 • aug 14 '19 dropdown menu copy link hide i love fish, but i swapped off of it when i started writing more documentation because i needed my examples to be backward compatible with bash. 😭 like comment: like comment: 3 likes like comment button reply collapse expand gavin fernandes gavin fernandes gavin fernandes follow joined jan 13, 2019 • aug 15 '19 dropdown menu copy link hide you can always use the bass fisher plugin if you want bash compatibility. that way you can run bash scripts with bass script.sh it's what i do to get fish to play nice with /etc/profile like comment: like comment: 4 likes like comment button reply collapse expand holy-elie scaïde holy-elie scaïde holy-elie scaïde follow i am a web and mobile developer. i draw sometimes and read a lot. i love music too. here, i write about being a software engineer... location port-au-prince, haiti education electronic engineering work senior software developer at noukod joined sep 15, 2018 • aug 14 '19 dropdown menu copy link hide pretty much  the same, but i'm on mac so iterm is the terminal and shell is zsh like comment: like comment: 1 like like comment button reply collapse expand patricnox patricnox patricnox follow php developer with a burning passion for development and coding. works mainly with laravel. email hello@patricnox.info location sweden education web developer & frameworks @ pronouns he/him work php x laravel joined oct 14, 2019 • apr 20 '20 • edited on apr 20 • edited dropdown menu copy link hide i love your wallpaper there, care to share? like comment: like comment: 1 like like comment button reply collapse expand jn jn jn follow joined aug 29, 2022 • aug 29 '22 dropdown menu copy link hide ohhhh!! what is this blue wallpaper with the cat? :) where can i download by any chance? like comment: like comment: 1 like like comment button reply collapse expand gil gil gil follow i am a web developer and educator. i work at codecraft works and i'm working on the building a tech community with space coast tech club, build and run, and gdg space coast location space coast of florida education self-taught + b.s. from ucf in software development and technical communication work senior software engineer at codecraft works joined oct 3, 2018 • aug 14 '19 dropdown menu copy link hide dank mono 1984 theme bash like comment: like comment: 23 likes like comment button reply collapse expand gil gil gil follow i am a web developer and educator. i work at codecraft works and i'm working on the building a tech community with space coast tech club, build and run, and gdg space coast location space coast of florida education self-taught + b.s. from ucf in software development and technical communication work senior software engineer at codecraft works joined oct 3, 2018 • aug 14 '19 dropdown menu copy link hide i forgot to mention the use of the peacock extension from john papa. dev.to/john_papa/peacock---choose-... like comment: like comment: 4 likes like comment button reply collapse expand ekanem ekanem ekanem follow just passing thru location lagos work frontend developer at startup studio joined mar 13, 2019 • aug 16 '19 dropdown menu copy link hide lovely theme (and on a side note, i'm just realising that you can view and work from the terminal directly from vscode. so helpful) like comment: like comment: 3 likes like comment button reply collapse expand peter cruckshank peter cruckshank peter cruckshank follow i like building things in react, that pretty much sums it up 😄👍 location cape cod, ma, usa education associate's degree in information technology pronouns he/him work front end dev & designer joined jul 21, 2017 • aug 19 '19 dropdown menu copy link hide oh yeah it's a big time saver like comment: like comment: 2 likes like comment button reply collapse expand gil gil gil follow i am a web developer and educator. i work at codecraft works and i'm working on the building a tech community with space coast tech club, build and run, and gdg space coast location space coast of florida education self-taught + b.s. from ucf in software development and technical communication work senior software engineer at codecraft works joined oct 3, 2018 • aug 16 '19 dropdown menu copy link hide thanks. yes, the built in terminal is great! like comment: like comment: 1 like like comment button reply collapse expand emma bostian ✨ emma bostian ✨ emma bostian ✨ follow software engineer, bibliophile, & cat mom location stockholm education siena college work software engineer at spotify joined dec 21, 2018 • aug 14 '19 dropdown menu copy link hide love this theme! like comment: like comment: 4 likes like comment button reply collapse expand kenice kenice kenice follow passionate dev. joined jul 26, 2019 • aug 14 '19 dropdown menu copy link hide beautiful setup. like comment: like comment: 1 like like comment button reply collapse expand jer jer jer follow i make stuff from wood, pixels, paint, code, words, metal and bullshit (and sometimes people even pay me for it). location it's not hell, but you can see it from here... work software architect at university of oklahoma + freelance joined jul 5, 2019 • aug 14 '19 dropdown menu copy link hide ack!  ocd moment incoming! must. resist. playing. with. editor. settings!!!!!! like comment: like comment: 20 likes like comment button reply collapse expand ggganea ggganea ggganea follow joined nov 25, 2019 • dec 26 '19 dropdown menu copy link hide ikr like comment: like comment: 1 like like comment button reply collapse expand ryan palo ryan palo ryan palo follow ryan is an engineer in the sacramento area with a focus in python, ruby, and rust. bash/python exercism mentor.  coding, physics, calculus, music, woodworking.  looking for work! email ryan@thepalos.com location elk grove, ca education m.s.c.s. lewis university, b.s.m.e. cal poly (slo) work currently job hunting joined mar 15, 2017 • aug 14 '19 dropdown menu copy link hide vs code (although i sometimes open vim in the integrated terminal #yolo) fira code an old hope theme zsh (although bash and powershell are pretty consistently in the rotation) i have very strong love for ligatures also, i keep the panda theme in rotation too! like comment: like comment: 8 likes like comment button reply collapse expand leah leah leah follow preact core team member

also love rust joined oct 30, 2017 • aug 21 '19 dropdown menu copy link hide do you actually like powershell, or do you just use it because windows? like comment: like comment: 3 likes like comment button reply collapse expand ryan palo ryan palo ryan palo follow ryan is an engineer in the sacramento area with a focus in python, ruby, and rust. bash/python exercism mentor.  coding, physics, calculus, music, woodworking.  looking for work! email ryan@thepalos.com location elk grove, ca education m.s.c.s. lewis university, b.s.m.e. cal poly (slo) work currently job hunting joined mar 15, 2017 • aug 21 '19 dropdown menu copy link hide i think it’s a neat idea! it puts object oriented programming into the command line, which is cool. i’m usually more quick/productive in bash-y environments, but i enjoy powershell.  it’s a gazillion times better than windows older command line environment. like comment: like comment: 3 likes like thread thread leah leah leah follow preact core team member

also love rust joined oct 30, 2017 • aug 21 '19 dropdown menu copy link hide for me it takes the worst parts of a shell and a scripting language and combines them on top of some .net layer i've written a bunch of scripts at work and i still despise it like comment: like comment: 1 like like thread thread ryan palo ryan palo ryan palo follow ryan is an engineer in the sacramento area with a focus in python, ruby, and rust. bash/python exercism mentor.  coding, physics, calculus, music, woodworking.  looking for work! email ryan@thepalos.com location elk grove, ca education m.s.c.s. lewis university, b.s.m.e. cal poly (slo) work currently job hunting joined mar 15, 2017 • aug 21 '19 dropdown menu copy link hide i wrote this post about it! i don’t know if it helps you, but it explains a bit better why i like it 😁 like comment: like comment: 1 like like thread thread simon massey simon massey simon massey follow staff+ engineer, software engineering coach, digital architect, container wrangler, maker, motivator, dev, dad. location london education university of london joined jun 13, 2019 • aug 21 '19 dropdown menu copy link hide if you are stuck in a corporate windows build without a linux subsystem you can run git bash as the vs code terminal dev.to/simbo1905/how-to-integrate-... like comment: like comment: 1 like like comment button reply collapse expand mirko bellabarba mirko bellabarba mirko bellabarba follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide hi, are you using windows? if yes, how did you manage to install zsh? thanks in advance :) like comment: like comment: 3 likes like comment button reply collapse expand ryan palo ryan palo ryan palo follow ryan is an engineer in the sacramento area with a focus in python, ruby, and rust. bash/python exercism mentor.  coding, physics, calculus, music, woodworking.  looking for work! email ryan@thepalos.com location elk grove, ca education m.s.c.s. lewis university, b.s.m.e. cal poly (slo) work currently job hunting joined mar 15, 2017 • aug 15 '19 dropdown menu copy link hide if you’re on windows 10, you can install windows subsystem linux which is like a little mini ubuntu or debian (or some others) that live inside your windows install and play nicely. vs code has a remote sessions extension that lets you use vs code as if you were inside that mini linux, but with your windows file system. it’s pretty great. if this isn’t quite enough detail, shoot me a dm here on dev and i can help you get it set up! 😁 like comment: like comment: 7 likes like thread thread mirko bellabarba mirko bellabarba mirko bellabarba follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide oh thank you, got it! i’ll try in the following days and in case i have problems i’ll dm you! thank you very much :) like comment: like comment: 3 likes like thread thread ryan palo ryan palo ryan palo follow ryan is an engineer in the sacramento area with a focus in python, ruby, and rust. bash/python exercism mentor.  coding, physics, calculus, music, woodworking.  looking for work! email ryan@thepalos.com location elk grove, ca education m.s.c.s. lewis university, b.s.m.e. cal poly (slo) work currently job hunting joined mar 15, 2017 • aug 15 '19 dropdown menu copy link hide good luck! like comment: like comment: 2 likes like thread thread vigneshkumar chinnachamy vigneshkumar chinnachamy vigneshkumar chinnachamy follow joined oct 25, 2017 • aug 17 '19 dropdown menu copy link hide i tried to install zsh in wsl. but couldn't install the power line fonts for theming. like comment: like comment: like thread thread dylan tientcheu dylan tientcheu dylan tientcheu follow i design and build stuff to make our lives easier email dylantientcheu@gmail.com location france education msc. computer science work software engineer joined jan 10, 2019 • aug 17 '19 dropdown menu copy link hide i have 2 posts regarding this, you’ll be quickly setup dev.to/dylantientcheu/improve-your... dev.to/dylantientcheu/transforming... like comment: like comment: 3 likes like thread thread kristopher betz kristopher betz kristopher betz follow joined jun 6, 2018 • aug 29 '19 dropdown menu copy link hide thanks! like comment: like comment: 1 like like comment button reply collapse expand don alfons nisnoni don alfons nisnoni don alfons nisnoni follow simple man 😉  | good coffe maker  ☕ |   not a nerd 🤓 | love computer science 🖥️ | care about his area future 🤗 email donnisnoni@uyelindo.ac.id location kupang, indonesia education bachelor degree of computer science work software engineer at stikom uyelindo kupang joined jun 20, 2019 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide theme: material theme - palenight font: operator mono lig & fira code icon: material-icon terminal: lxterminal shell oh-my-zsh ft. powerline like comment: like comment: 8 likes like comment button reply collapse expand mubarak yaqoub-okponobi mubarak yaqoub-okponobi mubarak yaqoub-okponobi follow joined aug 5, 2018 • aug 16 '19 dropdown menu copy link hide how did you setup your terminal like that. would love to do mine as well like comment: like comment: 2 likes like comment button reply collapse expand don alfons nisnoni don alfons nisnoni don alfons nisnoni follow simple man 😉  | good coffe maker  ☕ |   not a nerd 🤓 | love computer science 🖥️ | care about his area future 🤗 email donnisnoni@uyelindo.ac.id location kupang, indonesia education bachelor degree of computer science work software engineer at stikom uyelindo kupang joined jun 20, 2019 • aug 22 '19 dropdown menu copy link hide hi bro... i'm using powerline for fish.. you can check this out here .. if you using windows, then install wsl first. like comment: like comment: 2 likes like comment button reply collapse expand mirko bellabarba mirko bellabarba mirko bellabarba follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide hi, are you using windows? if yes, how did you manage to install zsh? thanks in advance :) like comment: like comment: 2 likes like comment button reply collapse expand dylan tientcheu dylan tientcheu dylan tientcheu follow i design and build stuff to make our lives easier email dylantientcheu@gmail.com location france education msc. computer science work software engineer joined jan 10, 2019 • aug 17 '19 dropdown menu copy link hide 😁 check this out, dev.to/dylantientcheu/improve-your... like comment: like comment: 5 likes like thread thread mirko bellabarba mirko bellabarba mirko bellabarba follow joined aug 15, 2019 • aug 20 '19 dropdown menu copy link hide thanks man! ;) like comment: like comment: 2 likes like comment button reply collapse expand damir franusic damir franusic damir franusic follow gentoo linux and vim worshiper, c developer, network protocol dissector implementer,socket/network programmer, recently entered the embedded world, hater of buzzwords and made up titles location zagreb, croatia work c developer at sartura d.o.o. joined jul 23, 2019 • aug 14 '19 dropdown menu copy link hide vim single screen: my current setup: damir franusic @vimmer9 @vintharas vim everywhere... 06:41 am - 12 aug 2019 0 0 configuration and plugins: damir franusic aug 10 '19 my .vimrc as requsted, not a pretty site :) ""+-----------+ ""| "" general | ""+-----------+ set exrc set secure set t_co : 256 set tabstop = 8 set softtabstop = 4 set shiftwidth = 4 set nocursorline ""set noexpandtab set expandtab set number ""set foldmethod=syntax ""set foldlevel=99 set nofoldenable set nowrap ""set viminfo^=% syntax on ""set paste highlight clear signcolumn ""set hidden (enable switching from changed buffer) set hidden set backupcopy = yes set sessionoptions -= options runtime ! ftplugin/man . vim let g:netrw_liststyle = 0 ""pathogen execute pathogen#infect () ""+---------+ ""| windows | ""+---------+ function ! winmove ( key ) let t : curwin = winnr () exec ""wincmd "" . a:key if ( t : curwin == winnr ()) ""we havent moved if ( match ( a:key , '[jk]' )) ""were we going up /down wincmd v else wincmd s endif exec ""wincmd "" . a:key endif endfunction map < leader > h : call winmove ( 'h' )< cr > map < leader > k : call winmove ( 'k' )< cr > map < leader > l : call winmove ( 'l' )< cr > map < leader > j : call winmove ( 'j' )< cr > map < leader > wc : wincmd q < cr > map < leader > wr < c - w > r nmap < leader >< left > : 3 wincmd << cr > nmap < leader >< right > : 3 wincmd >< cr > nmap < leader >< up > : 3 wincmd +< cr > nmap < leader >< down > : 3 wincmd -< cr > ""+-----------+ ""| shortcuts | ""+-----------+ "" plugins map < f9 > : undotreetoggle < cr > map < f8 > : tagbartoggle < cr > autocmd filetype c , cpp nnoremap < buffer >< leader > cf :< c - u > clangformat < cr > "" buffers nnoremap < silent > < f12 > : bnext < cr > nnoremap < silent > < f11 > : bprevious < cr > nnoremap < f5 > : ctrlpbuffer < cr > nnoremap < leader > q : bdelete < cr > "" tabs nnoremap < leader > tc : tabclose < cr > nnoremap < leader > ts : tab split < cr > "" man nnoremap k : man < cword >< cr > "" copy/paste noremap < leader > y ""* y noremap < leader > p "" + p ""+---------+ ""| airline | ""+---------+ set laststatus : 2 set ttimeoutlen : 50 set encoding : utf -8 ""let g:airline#extensions#tabline#enabled = 1 ""let g:airline#extensions#tabline#fnamemod = ':t' let g:airline #extensions#capslock#enabled = 1 let g:airline_powerline_fonts = 1 if ! exists ( 'g:airline_symbols' ) let g:airline_symbols = {} endif ""+-----------+ ""| syntastic | ""+-----------+ let g:syntastic_cpp_remove_include_errors = 1 let g:syntastic_cpp_check_header = 0 let g:syntastic_cpp_auto_refresh_includes = 1 let g:syntastic_mode_map ={ ""mode"" : ""active"" , ""active_filetypes"" : [], ""passive_filetypes"" : [ ""c"" , ""cpp"" ]} ""+-----+ ""| ale | ""+-----+ let g:ale_linters = { \ 'javascript' : [ 'eslint' , 'flow-language-server' ] \ } let g:ale_fixers = { \ 'javascript' : [ 'eslint' ] \ } let g:ale_completion_enabled = 1 let g:ale_statusline_format = [ 'x %d' , '? %d' , '' ] let g:ale_echo_msg_format = '%linter% says %s' let g:ale_linters_explicit = 1 set completeopt = menu , menuone , preview , noselect , noinsert
map < f7 > < plug >( ale_fix ) ""+-----------+ ""| ultisnips | ""+-----------+ let g:ultisnipsexpandtrigger = ""<c-j>"" let g:ultisnipsjumpforwardtrigger = ""<c-j>"" let g:ultisnipsjumpbackwardtrigger = ""<c-k>"" ""+-----+ ""| ycm | ""+-----+ let g:ycm_add_preview_to_completeopt = 1 let g:ycm_autoclose_preview_window_after_completion = 0 let g:ycm_autoclose_preview_window_after_insertion = 1 ""let g:loaded_youcompleteme = 1 let g:ycm_register_as_syntastic_checker = 0 let g:ycm_collect_identifiers_from_tags_files = 0 let g:ycm_auto_trigger = 1 let g:ycm_warning_symbol = 'ww' let g:ycm_error_symbol = 'ee' ""+-------------+ ""| javascript  | ""+-------------+ let g:jsx_ext_required = 0 let g:javascript_plugin_flow = 1 let g:ctrlp_regexp = 1 let g:ctrlp_extensions = [ 'tag' ] let g:ctrlp_custom_ignore = { \ 'dir' : '\.git$\|\.yardoc\|node_modules\|log\|tmp$' , \ 'file' : '\.so$\|\.dat$|\.ds_store$' \ } let g:ctrlp_root_markers = [ '.project_root' ] let g:gutentags_project_root = [ '.project_root' ] ""+---------+ ""| ""colors | ""+---------+ ""let g:solarized_termtrans=1 ""let g:solarized_termcolors=256 set background = dark colorscheme gruvbox
highlight normal ctermbg = none and finally the list of plugins i use (using pathogen) ~/.vim/bundle/ ale auto-pairs bbye ctrlp.vim emmet-vim nerdcommenter promptline.vim tagbar tlib_vim ultisnips undotree vim-addon-mw-utils vim-airline vim-clang-format vim-cpp-enhanced-highlight vim-gitgutter vim-gutentags vim-javascript vim-jsdoc vim-jsx vim-react-snippets vim-snippets vim-toml youcompleteme like comment: like comment: 9 likes like comment button reply collapse expand todor todorov todor todorov todor todorov follow python engineer, sysadmin and devops. currently writing python code for living and for fun. location sofia/bulgaria education technical university sofia - engineer, master of science work python engineer at freelancer joined aug 6, 2019 • aug 19 '19 dropdown menu copy link hide hi damir, vim forever! are you able to configure vim on the framebuffer? i really recommend it - you can have bold, italic, more colors. i will share my vim screenshot in the general discussion to see... like comment: like comment: 4 likes like comment button reply collapse expand damir franusic damir franusic damir franusic follow gentoo linux and vim worshiper, c developer, network protocol dissector implementer,socket/network programmer, recently entered the embedded world, hater of buzzwords and made up titles location zagreb, croatia work c developer at sartura d.o.o. joined jul 23, 2019 • aug 19 '19 dropdown menu copy link hide please do, thnx 👍 p.s. now you know why my username is vimmer 😄 like comment: like comment: 2 likes like thread thread todor todorov todor todorov todor todorov follow python engineer, sysadmin and devops. currently writing python code for living and for fun. location sofia/bulgaria education technical university sofia - engineer, master of science work python engineer at freelancer joined aug 6, 2019 • aug 19 '19 dropdown menu copy link hide i did not know how to make screenshot on the framebuffer, but i just succeed to make one and posted it, sorry for the delay. nice to meet you, vimmer :) like comment: like comment: 1 like like comment button reply collapse expand jacob herrington (he/him) jacob herrington (he/him) jacob herrington (he/him) follow i used to work on this website!

https://bsky.app/profile/jacobherrington.dev education self-taught joined aug 19, 2018 • aug 14 '19 dropdown menu copy link hide hi emma 👋, i use spacemacs which is basically vim on emacs! i use the default theme most of the time. i worked on my own theme that was easier on my eyes, but i sort of abandoned that project. it might be something i revisit soon. in my terminal, i use iterm2 with ohmyzsh and a private fork of thoughtbot's dotfiles. it looks something like this: like comment: like comment: 6 likes like comment button reply collapse expand pedro correa pedro correa pedro correa follow full stack developer and js lover location campinas, brazil work full stack engineer joined jul 25, 2019 • apr 29 '20 dropdown menu copy link hide wow, emacs + vim, gotta try it out like comment: like comment: 2 likes like comment button reply collapse expand ahmed khaled ahmed khaled ahmed khaled follow noob hacker/wizard location earth joined jun 14, 2019 • aug 18 '19 dropdown menu copy link hide spacemacs_user++; like comment: like comment: 3 likes like comment button reply collapse expand matteo zanda matteo zanda matteo zanda follow location cagliari, italy work student frontend developer at in my cave joined may 15, 2019 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide editor: vscode font: hack theme: slack theme aubergine dark terminal: vscode terminal extension list on the right like comment: like comment: 7 likes like comment button reply collapse expand kiritchoukc kiritchoukc kiritchoukc follow c# developer from belgium - f# and flutter enthusiast location belgium work software developper at roubaix joined nov 15, 2018 • aug 15 '19 dropdown menu copy link hide i had to flip my phone to find the extensions like comment: like comment: 1 like like comment button reply collapse expand nick taylor nick taylor nick taylor follow i'm a fan of open source and have a growing interest in serverless and edge computing. i'm not a big fan of spiders, but they're doing good work eating bugs. i also stream on twitch. email nick@nickyt.co location montréal, québec, canada education university of new brunswick pronouns he/him work developer advocate at pomerium joined mar 11, 2017 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide here's my vs code setup. my visual studio code setup nick taylor ・ dec 23 '17 ・ 6 min read #visualstudiocode #developertools #texteditor #vscode it's a little out of date as i use zsh for my shell now, dank mono for font and night owl is my go to theme these days, but the rest is pretty much the same. nick taylor @nickytonline @kentcdodds @code here's my full setup if you're interested, vscode.iamdeveloper.com . i also  recently switched to night owl, and bought dank mono a few weeks ago. such a great font. i should probably update my setup in my blog post 😉 00:44 am - 25 may 2018 0 4 like comment: like comment: 6 likes like comment button reply collapse expand maxence poutord maxence poutord maxence poutord follow 🎒 digital nomad • 👨🏼‍💻 software engineer • 🗣 public speaker • 🌏 remote worker email github@maxpou.fr location 🌏 vagabonding (europe / asia) joined aug 23, 2017 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide ohhh great topic emma! this is my setup: editor: vscode 💙 theme: custom (but based on material theme) full list of plugins that i can't live without! cli: oh-my-zsh with af-magic theme more details here 👇 apps, tools, and gear i use maxence poutord ・ jul 23 ・ 3 min read #productivity #developertools #webdev #beginners maxpou / dotfiles 💻 my dotfiles (ubuntu&macos) dotfiles this setup is intended for: 🐧 ubuntu 🐧 unix server (cli/bash only) 🍎 macos special thanks to whoever i stole it from 😘 dotfiles installation # ubuntu / macos: git clone https://github.com/maxpou/dotfiles.git --recursive cd dotfiles && bin/install # unix server: cd dotfiles && bin/install_server application visual studio code atom git gui (only use for log , blame and add ) 🍎 sourcetree 🐧 gitkraken postman : tool to develop api node/npm vscode plugins that i can't live without settings keybindings command line apps zsh oh my zsh : a framework to manage zsh configuration hub : a wrapper for git command (git+hub=github) thefuck : corrects the previous console command lighthouse (cli) : (require npm) auditing, performance metrics, and best practices for progressive web apps vtop (require npm) curl tree browser - brave (main) extensions list for google chrome (and canary): pocket (save to pocket) : best read it later… view on github like comment: like comment: 4 likes like comment button reply collapse expand al romano al romano al romano follow ""web-stack"" developer with a focus on accessibility design and development patterns, data visualization and devops automation. 

fell in love with node, js, spa's and the jamstack. bye lamp. location toronto, canada education george brown college work application developer at sickkids foundation joined mar 29, 2018 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide representing for the blue themes! font: fira code theme: code blue terminal: cmder like comment: like comment: 4 likes like comment button reply collapse expand boris jamot ✊ / boris jamot ✊ / boris jamot ✊ / follow software crafter

🐘 / 🐹 + 👷 = 🚀 location 🇫🇷 caen, normandy education master's degree at university of caen work developer at orange joined aug 12, 2018 • aug 14 '19 dropdown menu copy link hide editor: spacevim font: monaco for powerline theme: dracula terminal: deepin-terminal shell: fish multiplexer: tmux like comment: like comment: 7 likes like comment button reply collapse expand mn mark mn mark mn mark follow joined mar 7, 2018 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide i've given up looking for the best and am willing to accept adequate to be productive. i find my most recent setup quite comfortable. gnome terminal with tango dark theme and deja vu sans mono book font. running bash , tmux , and vim . like comment: like comment: 4 likes like comment button reply collapse expand rodolpho caires rodolpho caires rodolpho caires follow i develop apps using react native and ionic framework. location brazil work developer at diebold nixdorf joined aug 14, 2019 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide vs code fira code material theme lighter (if i have a light over my head, like at my workplace) or panda syntax ohmyzsh work setup: home setup: like comment: like comment: 4 likes like comment button reply collapse expand jesse m. holmes jesse m. holmes jesse m. holmes follow i follow new coders. brass-bander. adhd avenger. lover of giant hounds. believer in the impossible. location annapolis, maryland, united states education bachelor of music performance joined jul 27, 2018 • aug 15 '19 dropdown menu copy link hide good evening, emma! editor: vscode font: fira code with iscript baked in for comments and whatnot theme: cobalt2; previously i loved dark dracula for everything terminal: that's fantasque sans mono font there shell: powershell 6.1.1 with some oh-my-zsh inspiration like comment: like comment: 3 likes like comment button reply collapse expand mirko bellabarba mirko bellabarba mirko bellabarba follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide hi, how did you personalise powershell in that way? thanks in advance :) like comment: like comment: 1 like like comment button reply collapse expand jesse m. holmes jesse m. holmes jesse m. holmes follow i follow new coders. brass-bander. adhd avenger. lover of giant hounds. believer in the impossible. location annapolis, maryland, united states education bachelor of music performance joined jul 27, 2018 • aug 15 '19 dropdown menu copy link hide i found a couple of resources, starting with oh-my-posh by jan de dobbeleer. the powerline fonts i believe i grabbed from nerd-fonts , and i found some warp drive functionality from poshwarp . stitching it all together and getting the configuration right took only a couple of hours, likely because i changed things three or four times before settling. at the time, i wasn't all in on powershell, and this was just to deal with my wsl frustrations that have now been solved with the remote - wsl extension . hope that helps! like comment: like comment: 4 likes like thread thread mirko bellabarba mirko bellabarba mirko bellabarba follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide thank you, i’ll try in a few days :) like comment: like comment: 2 likes like comment button reply collapse expand jim pivarski jim pivarski jim pivarski follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide i once created my own font for programming, though it was bitmapped and only worked with pre-osx macs. i took the courier (monospace) 9-point font and turned all the 1-pixel dots into 4-pixel dots. colons and semicolons, periods and commas were very distinct. it was great! for the past 25 years or so, i've used emacs in the terminal (with the ion3 window manager, 2 pixels of purple border), but i'm trying to modernize and use atom on chromebooks instead. the standard dark theme. like comment: like comment: 2 likes like comment button reply collapse expand brian masinick brian masinick brian masinick follow location greenville sc work retired software engineer at home joined aug 19, 2019 • aug 19 '19 • edited on aug 19 • edited dropdown menu copy link hide as a code developer/maintainer in the nineties i used primarily gnu emacs with occasional leaps ""just for fun"" into the bsd implementation of vi (or vim, if available). once i started spending more time with linux software i have used a wider variety of tools and editors including gnu emacs, vim, geary, nano and others. at work (prior to my 2018 retirement) i used ultraedit and at least one other xml friendly editor that works on windows. i believe it was notepad++ like comment: like comment: 1 like like comment button reply collapse expand ahmed khaled ahmed khaled ahmed khaled follow noob hacker/wizard location earth joined jun 14, 2019 • aug 18 '19 dropdown menu copy link hide hey, i really want to see pictures. your emacs and font like comment: like comment: 1 like like comment button reply collapse expand maxime gaston maxime gaston maxime gaston follow sre, opensource enthusiast and sport addict. location paris education efrei work sre at cardiologs joined jul 29, 2019 • aug 16 '19 dropdown menu copy link hide editor: vim (in tilix terminal) font: hack theme: solarized8_dark plugins: (using vundle as plugin manager) youcompleteme nerdtree solarized8 tabular syntastic lightline surround yaml-folds markdown ansible dockerfile like comment: like comment: 3 likes like comment button reply collapse expand keyur golani keyur golani keyur golani follow i'm a tech enthusiast. i code for living and hobby. work as an sde. follow gadgets and smart devices. love photography! location bellevue, washington work software dev engineer at amazon joined aug 11, 2019 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide am working on a dotfiles at github.com/keyurgolani/dotfiles will add the config, plugins etc there soon. ps: dotfiles is not ready for consumption yet. do not use the dotfiles yet without intention to debug through some issues while installation. it works perfect after installation though. like comment: like comment: 3 likes like comment button reply collapse expand thomas landin thomas landin thomas landin follow hello! my name is thomas and i'm a nerd. i like tech and gadgets and speculative fiction, and playing around with programming. it's not my day job, but i'm working on making it a side gig :) location stockholm, sweden joined jul 24, 2018 • aug 16 '19 dropdown menu copy link hide system : anarchy linux (arch-based) editor : neovim font : hermit theme : gruvbox (editor, terminal, and window manager :)) terminal : xst (fork of suckless' st) shell : fish, stock setup with only a handful of functions and customization. 230 lines all told (including the prompt at ~90 lines, which i selected from a pre-made one). the beauty of fish is you don't need oh-my-fish or any plugin system, it's got everything already. dotfiles : git.sr.ht/~kungtotte/dotfiles like comment: like comment: 3 likes like comment button reply collapse expand ahmed khaled ahmed khaled ahmed khaled follow noob hacker/wizard location earth joined jun 14, 2019 • aug 18 '19 dropdown menu copy link hide looks like luke smith rice :d like comment: like comment: 3 likes like comment button reply collapse expand saikat saikat saikat follow location pune, india work software engineer joined mar 31, 2019 • aug 16 '19 • edited on aug 16 • edited dropdown menu copy link hide this is what i am using: ide: intellij idea theme: one dark font: fira code terminal: oh-my-zsh (powerlevel10k) like comment: like comment: 3 likes like comment button reply collapse expand fulton browne fulton browne fulton browne follow i am a developer trying to change the world with code. email fultonbrowne@pm.me location birmingham, alabama education student at samford university pronouns he/him work senior sofware engineer at neonaut joined jun 7, 2019 • aug 14 '19 dropdown menu copy link hide i really just like the intellij dark mode. like comment: like comment: 5 likes like comment button reply collapse expand juan j cadima juan j cadima juan j cadima follow joined oct 16, 2019 • oct 20 '19 dropdown menu copy link hide me too. its perfect for the eyes, not too dark, not too light like comment: like comment: 2 likes like comment button reply collapse expand george calianu george calianu george calianu follow #ubuntu,#golang. mostly backend developer. free software advocate. #visionary. cloudifying things. location ro education cs joined may 16, 2018 • aug 16 '19 • edited on aug 16 • edited dropdown menu copy link hide as go developer i frequently use liteide (unpopular opinion 🤔) in a custom installation with gopei shell project. the theme is a dark one modified by me and inspired by webstorm. like comment: like comment: 2 likes like comment button reply collapse expand abdur rehman khalid abdur rehman khalid abdur rehman khalid follow a senior computer science student, passionate data visualization, data science, big data, development with java, angular, and react.js. location lahore education ms computer science work senior computer science student joined apr 17, 2019 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide being a developer it is very common to have a dark theme but for me in visual studio code the matter is different but in intellij idea it is same as usual so her is the detail of my development environments as well. editor: visual studio code theme: shades of purple creator: ahmad awais editor: intellij idea 2019 theme: dracula creator: intellij idea (default dark theme) like comment: like comment: 3 likes like comment button reply collapse expand callum smith callum smith callum smith follow linux, game development, gaming, software development & dragons. location england work mr at currently at university joined nov 30, 2019 • nov 30 '19 • edited on nov 30 • edited dropdown menu copy link hide my vscode setup colour theme: horizon dark (italic) icon theme: monoka pro icons editor font: fira code iscript terminal: just some simple bash like comment: like comment: 2 likes like comment button reply collapse expand benjamin vincent benjamin vincent benjamin vincent follow hey 👋 i am a vs code junky my substance of choice is typescript and i nodejs in the back end location québec city work director of development  at neb401 joined sep 4, 2019 • sep 4 '19 • edited on sep 5 • edited dropdown menu copy link hide editor: vs code font: fira code iscript (cursives italics) theme: pop n' lock by luxcium ✨ terminal: iterm2 shell: zsh (with tmux and  powerlevel 9k theme) i have created my own vscode colour theme popn’lock theme by luxcium ✨ and i use zsh with highly customized powerlevel 9k theme custom js repl i have updated my color theme today look at the colourfull screen shoots look at the new screenshots (v3.18) function return type (v3.18) colorfull function return type highlight variable highlight red on writing side / blue on the reading side (v3.18) usefull variable highlight decorator support (v3.18) typescript decorator support latest typescript / javascript grammar promise functions highlight when using latest typescipt grammar (v3.18) when using latest typescipt grammar promise dot all and promise resolve and catch  highlight console highlight when using latest typescipt grammar (v3.18) when using latest typescipt grammar support console.log / conso.error etc. highlight like comment: like comment: 3 likes like comment button reply collapse expand alex patterson alex patterson alex patterson follow hi i am the founder of codingcat.dev, a web development community. i love all things web! i am also a developer advocate at builder.io email alex@codingcat.dev location grand rapids, mi education ms computer information systems, grand valley state university work founder at codingcatdev joined feb 11, 2019 • aug 15 '19 dropdown menu copy link hide i really love using colors to separate basically everything in my life (actually just wrote a tutorial on doing this same thing for slack). so i use peacock for the colors and material icon theme for the icons like comment: like comment: 2 likes like comment button reply collapse expand alex patterson alex patterson alex patterson follow hi i am the founder of codingcat.dev, a web development community. i love all things web! i am also a developer advocate at builder.io email alex@codingcat.dev location grand rapids, mi education ms computer information systems, grand valley state university work founder at codingcatdev joined feb 11, 2019 • aug 15 '19 dropdown menu copy link hide sorry for thread smash but here are the tutorials slack customization dev.to/ajonpllc/slack-workspace-th... peacock (video only need to blog) youtu.be/vyusbzxbboe like comment: like comment: 1 like like comment button reply collapse expand jason c. mcdonald jason c. mcdonald jason c. mcdonald follow author. speaker. time lord. (views are my own) email codemouse92@outlook.com location time vortex pronouns he/him work author of ""dead simple python"" (no starch press) joined jan 31, 2017 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide editor: vscode font: hack theme: sea green theme (shown), skgrtt terminal: vscode terminal (shown), terminator, guake os: ubuntu mate i also moved the activity bar to the right. like comment: like comment: 2 likes like comment button reply collapse expand nahuel nahuel nahuel follow joined sep 24, 2017 • aug 21 '19 dropdown menu copy link hide looks awesome! will definately try it. like comment: like comment: 1 like like comment button reply collapse expand adam romig 🇵🇭 adam romig 🇵🇭 adam romig 🇵🇭 follow 👨🏻‍💻 coding since forever ago
🥞 html, css, javascript, c#, php, sql
🎂 amateur cake decorator
🐧 geek of things spheniscidae email adam@romig.dev location dayton, ohio education university of toledo work software engineer joined oct 25, 2018 • aug 19 '19 dropdown menu copy link hide on my personal linux laptop: editor: vscode font: victor mono theme: azure terminal: konsole shell: fish on my work mac, it's the same editor/theme/font. i use iterm2 for the terminal and fish for the shell as well. like comment: like comment: 2 likes like comment button reply collapse expand sergey kislyakov sergey kislyakov sergey kislyakov follow location russia work backend engineer joined jan 2, 2017 • aug 21 '19 dropdown menu copy link hide victor mono looks cool. reminds me of operator/dank mono in terms of italics, except it doesn't cost anything and has a wide language support. thanks for sharing it! like comment: like comment: 3 likes like comment button reply collapse expand nayden gochev nayden gochev nayden gochev follow java professional with full-stack competence;
consultant; contractor; speaker; 
co-founder of jprime conference;
founder of jprofessional conferences;
founder of java.beer events; location bulgarian education masters degrees in computer science work superhero with java powers ! joined oct 29, 2019 • nov 8 '19 dropdown menu copy link hide very old school here, my intellij idea looks like eclipse. and my vscode looks like eclipse like comment: like comment: 2 likes like comment button reply collapse expand richard carter richard carter richard carter follow husband, father, software developer, gamer, dj location south ms education university of southern mississippi work devops at electric power company joined jan 4, 2019 • aug 28 '19 dropdown menu copy link hide ok, let me try this again. when using fira code, how do i get rid of these little dots everywhere there's whitespace? like comment: like comment: 1 like like comment button reply collapse expand peter cruckshank peter cruckshank peter cruckshank follow i like building things in react, that pretty much sums it up 😄👍 location cape cod, ma, usa education associate's degree in information technology pronouns he/him work front end dev & designer joined jul 21, 2017 • aug 20 '19 dropdown menu copy link hide okay here's my setup for vs code and my terminal editor: vs code insiders build font: fira code iscript theme: synthwave '84 - with the glow on 🤩😎 terminal: hyper with wsl || git bash || powershell if necessary shell: oh my zsh in wsl like comment: like comment: 2 likes like comment button reply collapse expand todor todorov todor todorov todor todorov follow python engineer, sysadmin and devops. currently writing python code for living and for fun. location sofia/bulgaria education technical university sofia - engineer, master of science work python engineer at freelancer joined aug 6, 2019 • aug 19 '19 dropdown menu copy link hide it may be not so pretty, but definitely ergonomic. editor: vim font: input mono, customized theme: gruvbox, customized terminal: linux framebuffer shell: tmux terminal multiplexer like comment: like comment: 2 likes like comment button reply collapse expand ghost ghost ghost follow i'm the ghost of developers past... joined sep 29, 2017 • aug 15 '19 dropdown menu copy link hide editor: neovim con plugins (rainbow_parentheses, lightline, nerdtree, riv and vim-mundo) font: ubuntu mono theme: gruvbox terminal: xfce4-terminal (tests them all, the only one for me) shell: bash (tried zsh and fish but i found the to be kinda slow) mostly to web dev (frontend avoiding js as much as possible and python and lately rust for the back), microcontrollers (c) and markdown or restructured for documents (ditched word processors and presentation sw years ago, lucky me :) ). like comment: like comment: 1 like like comment button reply collapse expand reaper reaper reaper follow minimalist • coder • i build stuff • https://barelyhuman.dev • https://reaper.im education bachelors in tech work principal engineer joined feb 15, 2019 • mar 24 '20 • edited on mar 24 • edited dropdown menu copy link hide editor: vs code font: ubuntu mono liga theme: material theme darker the full config: vscode.json like comment: like comment: 1 like like comment button reply collapse expand dillon headley dillon headley dillon headley follow front end engineer with 12+ years of experience. joined mar 21, 2019 • aug 15 '19 dropdown menu copy link hide ide: sublime text font: fira code theme: sublime adaptive / mariana color scheme terminal: iterm - just got some nice updates!! shell: fish font: mononoki - love it but no lig's theme: solarized dark bonus: i also can't live without clip menu and spectacle like comment: like comment: 1 like like comment button reply collapse expand dale anderson dale anderson dale anderson follow joined dec 26, 2016 • aug 15 '19 dropdown menu copy link hide vs used to be awesome. now it's a bloated pile of junk, like so many microsoft products go. webstorm is amazing. worth every cent. ultrawide monitor, everything fits lovely, everything is at your fingertips. like comment: like comment: 1 like like comment button reply collapse expand christopher keele christopher keele christopher keele follow tech tinkerer, web enveloper, professional digresser. location seattle work software engineer joined apr 24, 2019 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide fonts: fira code program editor: vs code ( customizations ) theme: predawn twilight script editor: vim with minimal customizations syntax highlighting: twilight256 theme using ansi colors below terminal: iterm shell: fish prompt: my own with git index hints ( demo ) ansi colors: custom, based on twilight , with more popping cyan/magenta and pastel brights all together: like comment: like comment: 1 like like comment button reply collapse expand vom com vom com vom com follow joined oct 12, 2018 • aug 15 '19 dropdown menu copy link hide i think i use dracula too. and i think i use the fira mono too (is that the mozilla font?) i'm away from my workstation, which is why i'm unsure. but i have a question on vs code. i have lot of extensions installed that are supposed to help with linting and code suggestions. i know for a fact that they can work against each other and be useless, or several give you same or conflicting errors at once. the reason i haven't fully simplified to one extension for each service is because sometimes i work in different frameworks or languages. in angular, you want typescript linting, in vanilla js/css/html, just emit and maybe eslint and prettier... i feel like it's difficult to turn several extensions on,off depending on application. is that why *workspaces"" exist, or is there a way to turn on your *angular settings"" and then later turn on your ""vanilla html css js settings"" etc? like comment: like comment: 1 like like comment button reply collapse expand lepinekong lepinekong lepinekong follow joined may 23, 2017 • apr 8 '20 dropdown menu copy link hide i'm creating my own inside vscode: a visual structured code editor (at the moment a proof of concept) ;) visual structured editor poc1 visual structured editor poc2 like comment: like comment: 1 like like comment button reply collapse expand stephen chiang stephen chiang stephen chiang follow design technologist. harmonizing design and development daily.

apply ☕ || 🍺 liberally. location stavanger, norway work ux engineer / front-end consultant / mechanical keyboard hobbyist joined dec 16, 2017 • aug 14 '19 dropdown menu copy link hide i use dank mono as well but seriously, for that kind of money, you think it would have the ligatures for >= & <= by now. like comment: like comment: 1 like like comment button reply collapse expand jin foo jin foo jin foo follow data scientist location sydney education unsw, unimelb, caanz work insights manager at woolworths ltd joined jun 23, 2019 • aug 20 '19 dropdown menu copy link hide give victor mono a go if you haven't yet: rubjo.github.io/victor-mono/ like comment: like comment: 2 likes like comment button reply collapse expand stephen chiang stephen chiang stephen chiang follow design technologist. harmonizing design and development daily.

apply ☕ || 🍺 liberally. location stavanger, norway work ux engineer / front-end consultant / mechanical keyboard hobbyist joined dec 16, 2017 • aug 20 '19 dropdown menu copy link hide looks really interesting! will give it a try today, thanks. like comment: like comment: 2 likes like comment button reply collapse expand arber braja arber braja arber braja follow i'm a frontend developer. passionate about javascript and web standards. location albania work frontend developer joined mar 11, 2019 • aug 14 '19 dropdown menu copy link hide i see a good candidate for top thread of the week here and my comment will be top comment also :p i'm currently using vs code too. theme is dracula, im using fira code font and material icons too. for php, yeah i work on both sides of the bridge php and js (mostly js lately), i use most of the time sublime text 3. another ide i love is webstorm, dracula and fira code there also. like comment: like comment: 1 like like comment button reply collapse expand jake jake jake follow i'm a current software developer and passionate writer. location fredericton, new brunswick, canada education honours bachelor of science in computer science work software developer at ibm joined jul 9, 2019 • aug 14 '19 dropdown menu copy link hide i'm primarily a vim user so i use a terminal a lot. i use a modified san francisco mono to be a nerd font and setup my purple af layout. read more on my github: github.com/jakew/dotfiles like comment: like comment: 3 likes like comment button reply collapse expand patrik kiss patrik kiss patrik kiss follow a short bio. location hungary education software developer joined nov 18, 2019 • nov 23 '19 dropdown menu copy link hide i'm using sublime text 3. with default font, and using the built-in dark theme of sublime. i don't need any other one, i'm completely satisfied with it :) and this is the layout i'm using right now: basically this is a 2 column layout. the left one is a full screen. there i opened any kind of inc/main/ajax files. the column on the right is split into 2 rows . the top one is where all the class files are opened, and the bottom one contains all the js and css files. like comment: like comment: 1 like like comment button reply collapse expand tomas fernandez tomas fernandez tomas fernandez follow technical writer at semaphore
@tomfernblog location argentina education university of buenos aires work technical writer at semaphore joined oct 9, 2019 • dec 30 '19 dropdown menu copy link hide editor: vim terminal: konsole shell: zsh desktop: kde plasma nice and clean: i use a ton of plugins, a lot of them for writing: 10 vim plugins for writing tomas fernandez ・ dec 26 ・ 3 min read #productivity #writing #vim like comment: like comment: 1 like like comment button reply collapse expand joluga joluga joluga follow webdesigner learning some php and javascript. joined jul 21, 2017 • sep 22 '19 dropdown menu copy link hide material theme in phpstorm, night owl in vs code, cmder, fira code font and source code in notepad++. that's mostly what i use to code. like comment: like comment: 1 like like comment button reply collapse expand zach zach zach follow a teleporting, flying, cowboy elf location portland, or work software developer joined aug 21, 2018 • aug 14 '19 dropdown menu copy link hide i'm actually utilizing almost the exact same setup as you! twinsies! like comment: like comment: 3 likes like comment button reply collapse expand bm-stschneider bm-stschneider bm-stschneider follow joined aug 9, 2019 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide editor: vscode because i use marketplace.visualstudio.com/items... primary font: 'furacode nerd font' secondary font: 'script12 bt' theme: cobalt2 (wesbos) terminal: zsh with omgzsh small tip for fonts with ligatures and icons nerdfonts.com/ like comment: like comment: 1 like like comment button reply collapse expand andreas jakof andreas jakof andreas jakof follow i made my hobby into a profession. love what i do and get money for it.  
in other words: ""life is good!"" location berlin work software/system engineer/devop at a german chemical company joined jan 18, 2019 • aug 14 '19 dropdown menu copy link hide editor: depends on what i am doing... programming: visual studio textediting: notepad++ or vs code font: i looked it up once, but i can‘t remember. the default. theme: default... i like the colored theme of visual studio. i experimented with the light and the dark theme, but always went back to the default. shell: mostly the one opening, when debugging .net core. more and more powershell (ise), because i am usually managing azure and exchange online. no pics, because i am writing this on my phone in the train on my way home. like comment: like comment: 1 like like comment button reply collapse expand ihor vorotnov ihor vorotnov ihor vorotnov follow location dnipro, ukraine work web and software developer joined jul 7, 2019 • aug 16 '19 • edited on aug 16 • edited dropdown menu copy link hide editor: phpstorm font: fira code (including phpstorm's ui) theme: custom, based on base16 colors terminal: iterm terminal theme: custom, based on base16 colors shell: zsh + oh-my-zsh prompt: custom, based on spaceship like comment: like comment: 1 like like comment button reply collapse expand finn finn finn follow location united kingdom joined may 5, 2020 • may 6 '20 dropdown menu copy link hide editor: vs code insiders font: meslolgs nf theme: hackthebox terminal: gnome terminal w/ tmux shell: zsh with powerlevel 10k and ohmyzsh like comment: like comment: 1 like like comment button reply collapse expand anton melnyk anton melnyk anton melnyk follow email antonmelnyk@hey.com location tallinn, estonia education computer engineering at kyiv polytechnic institute, ukraine work software engineer at printify joined nov 6, 2018 • aug 20 '19 dropdown menu copy link hide editor: vs code editor theme: pop n' lock editor font: hasklig shell: fish terminal: hyper terminal font: monaco like comment: like comment: 3 likes like comment button reply collapse expand benjamin vincent benjamin vincent benjamin vincent follow hey 👋 i am a vs code junky my substance of choice is typescript and i nodejs in the back end location québec city work director of development  at neb401 joined sep 4, 2019 • sep 4 '19 dropdown menu copy link hide ho this is the first time i see my vs code theme installed somewhere (after 15k downloads and 5k installs i finally got to see how it looks like on someone else machine) you can use fira code iscript if you want to have cursives instead of italics...  please feel free to open a git hub issue for any request you have regarding the syntax of your favourite programming language!!! like comment: like comment: 4 likes like comment button reply collapse expand benjamin vincent benjamin vincent benjamin vincent follow hey 👋 i am a vs code junky my substance of choice is typescript and i nodejs in the back end location québec city work director of development  at neb401 joined sep 4, 2019 • sep 5 '19 • edited on sep 5 • edited dropdown menu copy link hide pop n' lock theme by luxcium ✨ i have updated my color theme today look at the colourfull screen shoots ... function return type (v3.18) colorfull function return type highlight variable highlight red on writing side / blue on the reading side (v3.18) usefull variable highlight decorator support (v3.18) typescript decorator support latest typescript / javascript grammar promise functions highlight when using latest typescipt grammar (v3.18) when using latest typescipt grammar promise dot all and promise resolve and catch  highlight console highlight when using latest typescipt grammar (v3.18) when using latest typescipt grammar support console.log / conso.error etc. highlight like comment: like comment: 2 likes like comment button reply collapse expand daan wilmer daan wilmer daan wilmer follow joined aug 7, 2019 • aug 15 '19 dropdown menu copy link hide editor: phpstorm (vscode is great but paying for phpstorm is worth every cent) theme: darcula font: dejavu sans mono terminal: gnome-terminal shell: bash it's basically default, haven't found anything wrong with it yet. might start playing around with settings some time. like comment: like comment: 1 like like comment button reply collapse expand glenn carremans glenn carremans glenn carremans follow native android developer/consultant for appwise, i work on custom projects for clients.
php/js (web) developer in my freetime. trying to keep learning in an ever changing tech world. email me@glennmen.dev location belgium work android developer/consultant at appwise joined oct 6, 2018 • aug 15 '19 dropdown menu copy link hide editor: phpstorm & android studio font: fira code theme: dracula terminal: iterm2 shell: zsh + oh my zsh like comment: like comment: 1 like like comment button reply collapse expand dan conn dan conn dan conn follow hey! i'm dan!

i have been coding professionally for over 10 years and have had an interest in cybersecurity for equally as long! 

i love learning new stuff and helping others location brighton / london, uk education edinburgh napier (postgrad cert advanced security & digital forensics) work developer advocate at sonatype joined aug 13, 2017 • aug 14 '19 dropdown menu copy link hide editor: intellij / pycharm / clion font: menlo theme: dracula terminal: bash (although moving to ohmyzsh soon) like comment: like comment: 2 likes like comment button reply collapse expand mohamed_benoba mohamed_benoba mohamed_benoba follow mohamed benoba from libya, i interested in front-end development. email m.benoba@gmail.com location tripoli, libya joined aug 19, 2019 • aug 19 '19 dropdown menu copy link hide i am using notepad++, but if i have a strong computer i think i will use vscode like comment: like comment: 2 likes like comment button reply collapse expand misael taveras misael taveras misael taveras follow #jsdev functional programmer @taverasmisael anywhere else location dominican republic education di computing science joined feb 25, 2018 • aug 15 '19 dropdown menu copy link hide pink cat boo seems like a baby dracula like comment: like comment: 2 likes like comment button reply collapse expand hugo hugo hugo follow location méxico joined oct 5, 2017 • aug 14 '19 dropdown menu copy link hide theme: noctis uva font: fira code like comment: like comment: 2 likes like comment button reply collapse expand leandro ardissone leandro ardissone leandro ardissone follow i'm a full stack software developer with almost 20 years of working remotely for companies around the globe. location buenos aires, argentina education self taught work fullstack software developer joined dec 4, 2017 • aug 16 '19 dropdown menu copy link hide editor: vs code font: fira code theme: pop n' lock by luxcium shell: fish like comment: like comment: 2 likes like comment button reply collapse expand samuele zanca samuele zanca samuele zanca follow location milan, italy work full-stack developer joined jul 25, 2019 • aug 21 '19 • edited on aug 21 • edited dropdown menu copy link hide my text glows btw editor: vscode font: fira code theme: synthwave 84 like comment: like comment: 2 likes like comment button reply collapse expand stefan age stefan age stefan age follow location melissa, tx work software engineer at abstract joined may 30, 2017 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide editor: vim font: dank mono theme: kphoen terminal: iterm2/ohmyzsh like comment: like comment: 2 likes like comment button reply collapse expand holy-elie scaïde holy-elie scaïde holy-elie scaïde follow i am a web and mobile developer. i draw sometimes and read a lot. i love music too. here, i write about being a software engineer... location port-au-prince, haiti education electronic engineering work senior software developer at noukod joined sep 15, 2018 • aug 14 '19 dropdown menu copy link hide editor: neovim font: fira code theme : nord terminal: iterm2 like comment: like comment: 2 likes like comment button reply collapse expand franklin castellanos franklin castellanos franklin castellanos follow i dabble in general nerdery. location manassas va work front-end developer joined nov 14, 2018 • aug 14 '19 dropdown menu copy link hide editor: vscode font: fira code theme: synthwave84 by @robb0wen terminal: zsh like comment: like comment: 2 likes like comment button reply collapse expand tuantvk tuantvk tuantvk follow visit site: https://sadagas.com location vietnam joined aug 13, 2019 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide editor: vscode font: ibm plex mono theme: one dark pro vivid shell: gun like comment: like comment: 2 likes like comment button reply collapse expand karn tirasoontorn karn tirasoontorn karn tirasoontorn follow love to code with ruby and rails. location phuket education computer engineering joined apr 11, 2019 • aug 16 '19 dropdown menu copy link hide editor: vs code font: fira code + mali theme: laserwave italic shell: ohmyzsh like comment: like comment: 2 likes like comment button reply collapse expand oleksii filonenko oleksii filonenko oleksii filonenko follow 19 yo student *and* a full-time developer.

building with rails currently, exploring new frontiers.

i enjoy functional programming, linux, foss, refactoring and mentoring :) email brightone@protonmail.com location kyiv, ukraine education taras shevchenko national university of kyiv work ruby on rails developer at syndicode joined may 5, 2018 • aug 15 '19 dropdown menu copy link hide editor: doom emacs font: dejavu sans mono (+ ligatures from fira code) theme: peacock terminal: alacritty shell: zsh like comment: like comment: 2 likes like comment button reply collapse expand ahmed khaled ahmed khaled ahmed khaled follow noob hacker/wizard location earth joined jun 14, 2019 • aug 18 '19 dropdown menu copy link hide how did you add fira glyph to another font ? like comment: like comment: 1 like like comment button reply collapse expand ranelpadon ranelpadon ranelpadon follow senior full-stack engineer · loves colemak, neovim, and karabiner elements · buy me a coffee @ https://ko-fi.com/ranelpadon location philippines joined sep 20, 2017 • aug 13 '20 dropdown menu copy link hide my iterm2 setup: neofetch for mac sysinfo + oh my zsh framework + powerlevel10 theme + meslo nerd font. :) like comment: like comment: 1 like like comment button reply collapse expand untung so andryanto untung so andryanto untung so andryanto follow hybrid apps advocate location jakarta, indonesia education aerospace engineering joined jul 5, 2017 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide using vscodium with firacode font with ligature colored by panda syntax like comment: like comment: 1 like like comment button reply collapse expand oleksii shytikov oleksii shytikov oleksii shytikov follow joined may 22, 2019 • aug 15 '19 dropdown menu copy link hide followed your lead and have found very annoying bug in night owl. wonder, how you found your way around it: github.com/sdras/night-owl-vscode-... like comment: like comment: 1 like like comment button reply collapse expand fernando dos santos fernando dos santos fernando dos santos follow hi, i'm a software developer who loves to learn and put into practice all the knowledge gained. its great when i use my skills to solve people's problems. email nando4803@gmail.com location localhost: luanda, angola education university methodist of angola work front-end developer at creative labs joined feb 19, 2019 • oct 15 '19 dropdown menu copy link hide editor: vs code font: fira code theme: dracula terminal: in ubuntu - ohmyzsh, in windows - cmder like comment: like comment: 1 like like comment button reply collapse expand vishnu haridas vishnu haridas vishnu haridas follow android & kotlin & flutter location india joined feb 19, 2018 • aug 16 '19 dropdown menu copy link hide editor: intellij idea (or android studio) font: fira mono theme: dracula (with bigger font size) like comment: like comment: 1 like like comment button reply collapse expand tommy tommy tommy follow hello, world. i like to build things. i like to help people. naturally, i'm an indie hacker who builds things that help people. joined may 20, 2018 • aug 14 '19 dropdown menu copy link hide notepad++ solarized dark custom bitmap font based on vt100 console2 like comment: like comment: 1 like like comment button reply collapse expand tammy lee tammy lee tammy lee follow i'm a generalist who loves problem solving and team-building. currently obsessed with the practical applications of ai and using ai to assist with coding. location canada work software development/engineering manager joined nov 19, 2018 • aug 15 '19 dropdown menu copy link hide i was too cheap for spring for dank mono (sooon) so i went with this setup ! like comment: like comment: 1 like like comment button reply collapse expand athul cyriac athul cyriac athul cyriac follow email athul8720@gmail.com location kottayam education college of engineering kidangoor work student joined apr 16, 2019 • aug 14 '19 dropdown menu copy link hide vscode deepdark material theme fira code oh-my-zsh like comment: like comment: 1 like like comment button reply collapse expand rodrigo juarez rodrigo juarez rodrigo juarez follow just a programmer using microsoft tools to create awesome apps! location mendoza, argentina joined dec 1, 2017 • sep 3 '19 dropdown menu copy link hide i didn't see any light theme :d like comment: like comment: 1 like like comment button reply collapse expand ng'ethe maina ng'ethe maina ng'ethe maina follow rockstar programme manager who's looking to rekindle his passion in development location nairobi, kenya education b. sc - computing work founder at mradi joined aug 1, 2019 • sep 3 '19 • edited on sep 3 • edited dropdown menu copy link hide editor: vs code terminal: terminator theme (all): night owl not sure about fonts - using defaults. thepracticaldev.s3.amazonaws.com/i... like comment: like comment: 1 like like comment button reply collapse expand alexander kim alexander kim alexander kim follow i am js developer. using react/vue location kazakhstan work frontend-engineer at primesource joined sep 30, 2019 • dec 10 '19 • edited on dec 10 • edited dropdown menu copy link hide i find myself that webstorm is smoother and prettier than vsc. i'm using material theme like comment: like comment: 1 like like comment button reply collapse expand tanuj nagpal tanuj nagpal tanuj nagpal follow https://tanuj101.netlify.app/about location india work intern at instantpost printers and scanners pvt. ltd. joined aug 14, 2019 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide editor:vscode theme:dracula terminal:st shell:fish like comment: like comment: 1 like like comment button reply collapse expand paul isaris paul isaris paul isaris follow software engineer @scify. 
live to learn something new -and write cleaner and more sustainable code- every day.
passionate with learning and discovering new technologies, history, and psychology. location athens, greece work software engineer at scify joined oct 4, 2017 • aug 14 '19 dropdown menu copy link hide i use intellij ide ultimate edition, with default darcula font and font family ;) like comment: like comment: 1 like like comment button reply collapse expand david wickes david wickes david wickes follow british. strong opinions held weekly. no, that's not a typo. teaches when and where and what i can. location london education ma continental philosophy work senior software developer at acuris joined apr 13, 2018 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide i have finally gotten my code editor to be my ideal setup. how long did it take? and... how long do you think it'll be before you change your mind? 🤣 like comment: like comment: 1 like like comment button reply collapse expand arandi lópez arandi lópez arandi lópez follow software engineer. web artisan. location méxico education software engineer work software engineer at revelo joined feb 22, 2017 • aug 14 '19 dropdown menu copy link hide editor: gvim theme: one dark font: fira mono shell: zsh with custom theme like comment: like comment: 1 like like comment button reply collapse expand michael z michael z michael z follow software writer location tokyo joined oct 7, 2018 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide vs code fira code font sidebar on the right minimap disabled increased line height ❤️ like comment: like comment: 1 like like comment button reply collapse expand gene gene gene follow web/mobile applications developer | react & react-native location ph education science in information technology work react native developer joined feb 8, 2018 • aug 15 '19 dropdown menu copy link hide anyone heard of synthwave? like comment: like comment: 1 like like comment button reply collapse expand silas meyer silas meyer silas meyer follow german student
loves open-source, laravel and especially nextcloud
works with phpstorm location germany education university work web developer joined apr 26, 2019 • aug 14 '19 dropdown menu copy link hide i wrote a while blog post about it (only german🇩🇪): silas229.de/dev/atom-plugins/ like comment: like comment: 1 like like comment button reply collapse expand dwayne charrington dwayne charrington dwayne charrington follow lead front-end developer @ ia // aurelia.io core team // amateur professional developer. location australia education self-taught joined feb 12, 2019 • jan 22 '20 dropdown menu copy link hide editor: vscode font: fira code theme: monokai pro (paid) terminal: powershell (on windows) and iterm + ohmyzsh (on macos) like comment: like comment: 1 like like comment button reply collapse expand dan n dan n dan n follow software engineer + artist location los angeles education uc san diego pronouns he/him joined nov 8, 2018 • aug 14 '19 • edited on aug 14 • edited dropdown menu copy link hide the really like the brown in the kimbie dark theme (vscode) edit: the picture upload's not working like comment: like comment: like comment button reply collapse expand anton melnyk anton melnyk anton melnyk follow email antonmelnyk@hey.com location tallinn, estonia education computer engineering at kyiv polytechnic institute, ukraine work software engineer at printify joined nov 6, 2018 • aug 20 '19 dropdown menu copy link hide it works, you just have to insert it with markdown. ![title](https://linkurl.example) like comment: like comment: 2 likes like comment button reply collapse expand dan n dan n dan n follow software engineer + artist location los angeles education uc san diego pronouns he/him joined nov 8, 2018 • aug 24 '19 dropdown menu copy link hide ahh got it - thanks for the tip! like comment: like comment: like comment button reply collapse expand ironmatt613 ironmatt613 ironmatt613 follow location texas work freelance web dev/pen tester at at home/freelance joined aug 8, 2019 • aug 15 '19 dropdown menu copy link hide editor: vscode font: courier theme: night owl i like that dank mono font though :-) like comment: like comment: 1 like like comment button reply collapse expand richard carter richard carter richard carter follow husband, father, software developer, gamer, dj location south ms education university of southern mississippi work devops at electric power company joined jan 4, 2019 • sep 5 '19 dropdown menu copy link hide thank you soooo much.  i've been looking at those dots for almost a year and haven't found anyone who could tell me how to get rid of them. like comment: like comment: 1 like like comment button reply collapse expand dipesh sharma dipesh sharma dipesh sharma follow location gujarat, india work frontend engineer at gridle.io joined apr 30, 2020 • may 1 '20 • edited on may 1 • edited dropdown menu copy link hide editor : vs code 😍 fonts. : firacode theme : ❤️ synthwave '84 (please try once that is glowing theme. that is my best personal choice theme. try once.) terminal : zsh like comment: like comment: 1 like like comment button reply collapse expand daniel mayovsky daniel mayovsky daniel mayovsky follow a dude.
i like mithril.js and vue.js a lot. location seattle education self taught work full-stack developer at frequence joined aug 13, 2018 • apr 9 '20 • edited on apr 9 • edited dropdown menu copy link hide editor: vim font: ubuntu monospace (i think) theme: monokai therminal: urxvt shell: bash window manager: i3-wm like comment: like comment: 1 like like comment button reply collapse expand prince billy graham karmoker prince billy graham karmoker prince billy graham karmoker follow email princebillygk@pm.me location bangladesh work android and web app developer at dgtong joined dec 19, 2019 • feb 2 '20 • edited on feb 2 • edited dropdown menu copy link hide editor: vscode theme: hyperter (full pure black) font: fira mono extensions: vim, todo tree, color highlight, bracket pair colorizer, comment v, box comment like comment: like comment: 1 like like comment button reply collapse expand john nweke john nweke john nweke follow devops engineer. cs professor. location maryland, usa joined jul 4, 2019 • aug 22 '19 dropdown menu copy link hide here's an article on vs code set up for newbie coders here - and it features my vscode set up and screenshots too: dev.to/johnnweke/visual-studio-cod... like comment: like comment: 1 like like comment button reply collapse expand andrew andrew andrew follow joined aug 15, 2019 • aug 15 '19 dropdown menu copy link hide pretty simple setup for me. i use the electron theme and verdana font. like comment: like comment: 1 like like comment button reply collapse expand felipe sousa felipe sousa felipe sousa follow +9 years as a software developer | frontend specialist | tech speaker | lifelong learner | curious about life & tech


#abitperday email hi@felipesousa.space location fortaleza, brazil work software engineer at uber joined sep 7, 2018 • sep 10 '19 dropdown menu copy link hide editor: neovim font: hack theme: ayu dark terminal: iterm2 shell: zsh like comment: like comment: 1 like like comment button reply collapse expand nexus software systems nexus software systems nexus software systems follow joined may 16, 2018 • dec 9 '19 • edited on dec 9 • edited dropdown menu copy link hide font: source code pro qt creator on linux && visual studio on windows👍 like comment: like comment: 1 like like comment button reply collapse expand carson sturtevant carson sturtevant carson sturtevant follow location colorado work software engineer joined oct 22, 2019 • oct 30 '19 dropdown menu copy link hide i use vs code with night owl theme! i'm super picky on colors and fonts and its one of the only ones i actually like. like comment: like comment: 1 like like comment button reply collapse expand johannes preis johannes preis johannes preis follow location munich joined aug 15, 2019 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide editor: vs code (or intellij) font: pragmata pro theme: dracula terminal: iterm 2 shell: zsh + oh-my-zsh like comment: like comment: 1 like like comment button reply collapse expand andy cormack andy cormack andy cormack follow js-focused full stack dev working on making beautiful, accessible sites. work full stack developer joined aug 16, 2019 • aug 16 '19 dropdown menu copy link hide hah! my setup is almost identical, i'm using fira code, haven't seen dank mono before. like comment: like comment: 1 like like comment button reply collapse expand javier guerra javier guerra javier guerra follow born, raise in tijuana, mx. with a degree in philosophy, have worked as janitor, sales person, pizza delivery boy, teacher, press operator, prepress, desktop publisher, and for the last 19 years dev location tijuana, mx joined aug 10, 2017 • aug 14 '19 dropdown menu copy link hide font: mono scheme: slate editor: terminal vi (i hate how antialias work on osx) terminal: bash like comment: like comment: 1 like like comment button reply collapse expand aarush bhat aarush bhat aarush bhat follow hi there! my name is aarush, on the internet as sloorush and i'm a computer science undergrad. location india joined apr 10, 2020 • may 15 '20 dropdown menu copy link hide guake drop down terminal forever! like comment: like comment: 1 like like comment button reply collapse expand ricardo bánffy ricardo bánffy ricardo bánffy follow location dublin, ireland joined jan 2, 2017 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide editor: emacs font: 3270 ( github.com/rbanffy/3270font ) theme: misterioso shell: zsh like comment: like comment: 1 like like comment button reply collapse expand elizabeth alcalá elizabeth alcalá elizabeth alcalá follow frontend developer. location lima, peru joined jul 23, 2018 • aug 15 '19 • edited on aug 15 • edited dropdown menu copy link hide editor:  vs code theme: bluloco light italic font: dank mono zh theme: ys like comment: like comment: 1 like like comment button reply collapse expand reach allen reach allen reach allen follow tech enthusiast location singapore education murdoch university work software engineer at ncs joined apr 28, 2019 • aug 15 '19 dropdown menu copy link hide thanks for sharing. honestly, i like your pick of tools. like comment: like comment: 1 like like comment button reply collapse expand tim bogdanov tim bogdanov tim bogdanov follow currently learning ruby on rails! location vancouver, wa work developer joined sep 26, 2019 • sep 26 '19 • edited on sep 26 • edited dropdown menu copy link hide sublime text (technically not an ide but hey..) thepracticaldev.s3.amazonaws.com/i... like comment: like comment: like comment button reply collapse expand daniel bailey daniel bailey daniel bailey follow software engineer currently living in beautiful salt lake city, utah. i am a previous finance grad / banker turned software engineer. i love coding, traveling, trying new restaurants, and gaming. location salt lake city, ut work software engineer at verisys joined jul 29, 2019 • aug 14 '19 dropdown menu copy link hide dracula soft theme firacode iscript, monaco font like comment: like comment: 1 like like comment button reply collapse expand joluga joluga joluga follow webdesigner learning some php and javascript. joined jul 21, 2017 • sep 22 '19 dropdown menu copy link hide material theme in phpstorm, night owl in vs code, cmder, fira code font and source code in notepad++. that's mostly what i use to code. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/joshualjohnson/tutorial---php-replace-img-src-from-string-29hc,,,"tutorial - php replace img src from string - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse joshua johnson posted on apr 3, 2018 • originally published at ua1.us tutorial - php replace img src from string # php # showdev recently, i had to figure out how to change <img> tag src using only php. i had to change the request from http to https.  i was given the task to convert a site over to https. the website is managed on big commerce. as we already know, when you are changing a site over to https, you have to hunt down all of the images/assets of the site and convert them to request https as well. otherwise, you will end up with the dreaded, mixed-content error and your site is deemed as unsecured by the browser you are visiting your site from. so it was easy enough to go through, page by page, and update images/assets to make requests over https instead of http. things were going along just fine until i ran into a big problem. the problem for this particular client, products of the website are not managed inside of big commerce. instead, they are managed in a third party erp (enterprise resource planning) software. which syncs changes with the big commerce product pages. so what was found is that the product pages are essentially managed in the erp. any products that included images inside of the product descriptions included it as http. so here we are...potentially thousands of products, all of which i would be going in an manually editing each product description containing <img src="" http://www.. .""> to <img src="" https://www.. .""> . adding only an s to http:// . solving the problem by using php to replace img src from a string doing some research, i found that the erp had a hook in which you could introduce custom logic before the erp would sync the data to big commerce. because each product description is stored as a string of html, i figured, i could manipulate the product description directly and add https to all images inside of the content. this is the code i used to solve this problem: <?php

$productdesc = '<a href=""https://ua1.us/"">text</a>';
$productdesc .= '<img src=""http://ua1.us/media/media.jpg"">';
$productdesc .= '<div class=""test""></div>';
$productdesc .= '<img src=""http://ua1.us/media/media1.jpg"">';

preg_match_all('/<img[^>]+>/i', $productdesc, $images);
foreach ($images[0] as $image) {
    $secureimg = str_replace('http://', 'https://', $image);
    $productdesc = str_replace($image, $secureimg, $productdesc);
}
echo $productdesc; in the code above, what we are doing is using the preg_match_all method in php to return all instances of <img> tags as an array of the original image tags. we then loop through each image tag and use the str_replace method to find the http:// and replace with the https:// . this returns a string the represents a secured image request. we then repeat the find a replace. this time, we are looking to replace the original image with the secured one. originally posted at ua1 labs top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand david fieffé david fieffé david fieffé follow digital manager @ lyra.com location toulouse, france work digital manager at lyra.com joined nov 4, 2019 • feb 25 '20 dropdown menu copy link hide i guess it would be better to use a php dom parser. preg_match_all could cause perfomance issues like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse joshua johnson follow a developer looking to share cool things. location orlando, fl work software developer at ua1 labs joined dec 20, 2017 more from joshua johnson firestudio video tutorial - debug panels # php # showdev # wordpress # webdev wordpress action hooks order of execution # php # wordpress the vision for firestudio and our other ua1 labs’ php “fire” libraries # php # wordpress # showdev # discuss 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/mizutani/a-log-format-analyzing-tool-from-existing-text-log-file-1din,,,"a log format analyzing tool from existing text log file - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse masayoshi mizutani posted on may 24, 2018 a log format analyzing tool from existing text log file # log # analysis # go note: this article was written mainly by machine translation. i apologize if this is hard to understand. i created a log format analysing tool in go and this post introduce it. what is ""log format analysis""? in this post, ""log format"" means like format string in c, go or etc. log . printf ( ""requested from %s"" , ipaddr ) enter fullscreen mode exit fullscreen mode this code output following logs. 2018/05/23 23:25:00 requested from 10.0.2.1
2018/05/23 23:25:10 requested from 192.168.1.5
2018/05/23 23:25:24 requested from 10.0.1.5 enter fullscreen mode exit fullscreen mode the original format is embedded in %s as ip address and output as text like below. since this example is very simple, it is easy to guess from the bottom to the top, but as the content becomes more complicated it is rarely common to say, 'what is this value or fixed statement?' the tool created this time is to infer the format statement (close to) above from the output below. we are implementing two functions for this tool: 1) estimate the format from already output log file , and 2) use the estimated format and see where the log corresponding to that format appeared in the log file . why the tool is required? this tool is unnecessary in environments that handle only normalized and structured log data, but it is useful in the following situations. when you want to grasp the whole picture of the log : i think that it is particularly frequent in the context of security analysis, but there are times when it is necessary to see a large amount of logs that have never been seen before and to draw knowledge from there . even if you try to view with the less command at all, it is severe for humans, so what kind of logs are there as a whole? and what kind of distribution do you do? if you find out that it is, the analysis will be much easier. especially in the security analysis, it is not the log concerning the usual service accounting for 99% of the whole in many cases, it is the point where something abnormality happened. since it is easy for errors and processing that are not normally seen to occur, logs when abnormality happens, if you can grasp where logs with abnormal logs = unusual formats appear, we will analyze them first by focusing on them you can make a foothold. if you need to reuse logs output in text format : if you are already running a service etc. and you want to output logs in text, add the log to the regular expression etc. it is necessary to extract the value being processed. if you have a specification, it is nice, but if not, you can see the source code or write a regular expression → make sure it is exhaustive → fix the regular expression, you have to repeat things like it is rather troublesome ((i think there are tsukkom that such an environment is more funny, but it was rarely a situation, especially in the former position). this tool does not estimate the regular expression of the value to be extracted, but it will be easier to work because it can cover how far it can be done from the existing logs. usage if you already have go language environment, you can install it by go get github.com/m-mizutani/logptn https://github.com/m-mizutani/logptn for example, i try to use this tool with following logs. $ cat test.log
feb  1 07:56:49 pylon sshd[5153]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=192.168.0.3  user=root
feb  1 07:56:51 pylon sshd[5153]: failed password for root from 192.168.0.3 port 7176 ssh2
feb  1 07:56:51 pylon sshd[5153]: connection closed by 192.168.0.3 [preauth]
feb  1 08:01:26 pylon sshd[5156]: invalid user upload from 192.168.0.3
feb  1 08:01:26 pylon sshd[5156]: input_userauth_request: invalid user upload [preauth]
feb  1 08:01:26 pylon sshd[5156]: pam_unix(sshd:auth): check pass; user unknown
feb  1 08:01:26 pylon sshd[5156]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=192.168.0.3
feb  1 08:01:28 pylon sshd[5156]: failed password for invalid user upload from 192.168.0.3 port 51058 ssh2
feb  1 08:01:28 pylon sshd[5156]: connection closed by 192.168.0.3 [preauth]
feb  1 08:05:01 pylon cron[5159]: pam_unix(cron:session): session opened for user root by (uid=0)
feb  1 08:05:01 pylon cron[5159]: pam_unix(cron:session): session closed for user root
feb  1 08:05:54 pylon sshd[5162]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=192.168.0.3  user=root
feb  1 08:05:56 pylon sshd[5162]: failed password for root from 192.168.0.3 port 33005 ssh2
feb  1 08:05:56 pylon sshd[5162]: connection closed by 192.168.0.3 [preauth]
feb  1 08:10:28 pylon sshd[5165]: invalid user mythtv from 192.168.0.3
feb  1 08:10:28 pylon sshd[5165]: input_userauth_request: invalid user mythtv [preauth]
feb  1 08:10:28 pylon sshd[5165]: pam_unix(sshd:auth): check pass; user unknown
feb  1 08:10:28 pylon sshd[5165]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=192.168.0.3
feb  1 08:10:30 pylon sshd[5165]: failed password for invalid user mythtv from 192.168.0.3 port 59978 ssh2
feb  1 08:10:30 pylon sshd[5165]: connection closed by 192.168.0.3 [preauth]
feb  1 08:15:01 pylon cron[5168]: pam_unix(cron:session): session opened for user root by (uid=0)
feb  1 08:15:01 pylon cron[5168]: pam_unix(cron:session): session closed for user root
feb  1 08:15:26 pylon sshd[5171]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=10.2.3.4  user=root
feb  1 08:15:28 pylon sshd[5171]: failed password for root from 10.2.3.4 port 60733 ssh2
feb  1 08:15:28 pylon sshd[5171]: connection closed by 10.2.3.4 [preauth]
feb  1 08:17:01 pylon cron[5173]: pam_unix(cron:session): session opened for user root by (uid=0)
feb  1 08:17:01 pylon cron[5173]: pam_unix(cron:session): session closed for user root
feb  1 08:20:35 pylon sshd[5177]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=10.2.3.4  user=root
feb  1 08:20:37 pylon sshd[5177]: failed password for root from 10.2.3.4 port 44877 ssh2
feb  1 08:20:37 pylon sshd[5177]: connection closed by 10.2.3.4 [preauth]
feb  1 08:25:01 pylon cron[5180]: pam_unix(cron:session): session opened for user root by (uid=0)
feb  1 08:25:01 pylon cron[5180]: pam_unix(cron:session): session closed for user root
feb  1 08:25:16 pylon sshd[5183]: invalid user user from 10.2.3.4 enter fullscreen mode exit fullscreen mode from above log data, the tool output following result. ./logptn test.log
2018/05/20 13:30:55 arg:test.log
     4 [4ffb267b] feb  1 *:*:* pylon sshd[*]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=*  user=root
     4 [845f4659] feb  1 *:*:* pylon sshd[*]: failed password for root from * port * ssh2
     6 [847ccf35] feb  1 *:*:* pylon sshd[*]: connection closed by * [preauth]
     3 [de051cd9] feb  1 08:*:* pylon sshd[*]: invalid user * from *
     2 [8e9e2a13] feb  1 08:*:* pylon sshd[*]: input_userauth_request: invalid user * [preauth]
     2 [22190c74] feb  1 08:*:* pylon sshd[*]: pam_unix(sshd:auth): check pass; user unknown
     2 [83fba2bf] feb  1 08:*:* pylon sshd[*]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=192.168.0.3
     2 [f1ba83ea] feb  1 08:*:* pylon sshd[*]: failed password for invalid user * from 192.168.0.3 port * ssh2
     4 [e4a6f815] feb  1 08:*:01 pylon cron[*]: pam_unix(cron:session): session opened for user root by (uid=0)
     4 [5256845b] feb  1 08:*:01 pylon cron[*]: pam_unix(cron:session): session closed for user root enter fullscreen mode exit fullscreen mode in this output, it is ""the number of times that format appeared"" ""format id"" ""estimated format"" from the left. also, in the estimated format, the part that is supposed to be embedded as a value is replaced by the symbol * . in this example there are few samples, so the ip address part is not * , but as the number of samples increases it will also be replaced with * . the above is output in human readable text format, but it can also be output in json format so that it can be handled by another program. ./logptn test.log -d sjson | jq . { ""formats"" : [ { ""segments"" : [ ""feb  1 "" , null , "":"" , null , "":"" , null , "" pylon sshd["" , null , ""]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost="" , null , ""  user=root"" ], ""count"" : 4 }, (snip) enter fullscreen mode exit fullscreen mode in addition, you can display in html format how many lines of that log format the format appeared. $ ./logptn  ./var/log/secure -d heatmap -o secure.html enter fullscreen mode exit fullscreen mode with the above command you can create a heat map showing the format of the log and what line it occurred. the heat map is the format estimated on the left, the top header is the number of lines (the line number to the line number), and the right is the total log number. the image below is a little small and hard to see, but the html file itself can also be downloaded from here . large size image performance the calculation amount is o (nm) , n is the total number of logs included in the log file, m is the estimated number of formats. i tried with various log files, but m converges to about 10 to 100, so n is the total number of logs affected. although it measures only miscellaneous, i moved it with macbookpro early 2015 (2.7 ghz intel core i 5) for data that is about m = 40 and ran around 30,000 logs / sec. perhaps it can be optimized more in terms of code, but i have not done so far yet. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand lior banai lior banai lior banai follow location israel, hadera work sw engineer  at kama research joined sep 20, 2019 • sep 27 '20 dropdown menu copy link hide if you have grpc support you can stream logs to my real time log server and view it in my analogy log viewer that support grpc like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse masayoshi mizutani follow location japan work security engineer at ubie inc joined dec 6, 2017 more from masayoshi mizutani eliminating sensitive values from logs using slog, the prospective official structured logger for go # go # slog goast: generic static analysis for go abstract syntax tree by opa/rego # go # opa # rego control notification of every github event with opa/rego # opa # rego # github # go 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/awwsmm/setting-up-an-ubuntu-vm-on-windows-server-2g23,,,"remote ssh into an ubuntu vm on windows - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse andrew (he/him) posted on nov 1, 2018 remote ssh into an ubuntu vm on windows # setup # tutorial # ubuntu # windows we recently got a new windows server machine at work and i wanted to install an ubuntu vm within it. after a few tries, i managed to install everything the way i wanted. one thing which was particularly difficult to set up was the ssh connection between mobaxterm (running on a windows 7 machine on our company's subnetwork, part of a nearby university's network) and the ubuntu vm running on the windows server machine (which is actually behind a firewall on the university's network, and which forwards a port to the windows server machine to allow ssh connections through). it was a mess. but it works now! if you want to know how i manage all sorts of jvm languages, deal with multiple python versions, easily install hadoop and spark, and keep my $path sensible, read on! setting up a linux vm on windows i installed ubuntu 18.04.1 lts on virtualbox , choosing a ""minimal installation"" and allowing the installer to ""download updates while installing ubuntu"". in spite of this... sure. again? fine. i know i'll need to do this later to ssh in, so while it's rebooting, i set up a network adapter in virtualbox. i click on ""global tools"" at the top-right of the virtualbox screen: hit ""create"" at the top-left: hit ""yes"": and click ""properties"" in the menubar to open the properties tabs at the bottom of the screen: i make a note of the ipv4 address (it should be something like 192.168.___.1 ) and check ""enable"" under ""dhcp server"" in the right-most column. now that that's all set up, i head back inside the vm. (note that sometimes, in these tutorials, the vm -- in my case, ubuntu -- is called the ""guest"" os, while the os within which it's run -- in my case, windows server -- is called the ""host"" os.) i open the terminal within ubuntu and install some basics: $ sudo apt-get update && sudo apt-get upgrade -y $ sudo apt-get install net-tools tree git openssh-server ifupdown ssh curl yum -y enter fullscreen mode exit fullscreen mode note that the -y flag means you don't have to explicitly type ""y"" when ubuntu asks... after this operation, ___ of additional disk space will be used. do you want to continue? [y/n] finally, i change the password on the root account so i can su as root later: $ sudo passwd root
enter new unix password: < type > retype new unix password: < type > passwd: password updated successfully $ su -
password: < type > root $ exit $ # back to normal command prompt enter fullscreen mode exit fullscreen mode install jvm things and haskell i use sdkman! to install most of my jvm-based things (like java, scala, etc.). it's really easy: $ curl -s ""https://get.sdkman.io"" | bash $ source $home /.sdkman/bin/sdkman-init.sh $ sdk version # to check that it was installed enter fullscreen mode exit fullscreen mode you can see all of the software which can be installed through sdkman! with: $ sdk list enter fullscreen mode exit fullscreen mode and if there are multiple versions which can be installed, those can be listed with: $ sdk list <software> enter fullscreen mode exit fullscreen mode for instance, sdk list java returns 12.ea.15-open
11.0.1.-zulu
11.0.1-open
10.0.2-zulu
10.0.2-open
... enter fullscreen mode exit fullscreen mode ...and so on. i'm going to install a stable legacy java version (java 8) and the newest lts version (java 11): $ sdk install java 11.0.1-open $ sdk install java 8.0.191-oracle enter fullscreen mode exit fullscreen mode view the current version of a particular piece of software with $ sdk list <software> # or $ sdk current <software> enter fullscreen mode exit fullscreen mode set the default version with $ sdk default <software> <version> enter fullscreen mode exit fullscreen mode or change the current version (only valid for current shell) with $ sdk use <software> <version> enter fullscreen mode exit fullscreen mode try switching back and forth between java versions and verify that the version has changed by running java -version . also, the java shell, jshell , didn't exist before java 9, so if you switch to java 8 and try the command jshell , you'll get an error (but you won't get that error with java 11). next, i install a bunch of other jvm/java-related things: $ sdk install groovy # jvm language $ sdk install kotlin # jvm language $ sdk install maven # java build tool $ sdk install sbt # scala build tool $ sdk install scala # jvm language $ sdk install spark # scala shell enter fullscreen mode exit fullscreen mode all of this software installs into $sdkman_dir/candidates/ , which is, by default, $home/.sdkman/candidates/ . you'll need to choose default versions for each piece of software. see which versions you're currently using for everything with: $ sdk current

using:

java: 11.0.1-open enter fullscreen mode exit fullscreen mode i only have java set up so far. let me pick default versions for all this other stuff: $ sdk default  groovy  2.5.3 $ sdk default  kotlin  1.3.0 $ sdk default  maven   3.5.4 $ sdk default  sbt     1.2.6 $ sdk default  scala   2.12.7 $ sdk default  spark   2.3.1 enter fullscreen mode exit fullscreen mode finally -- something that's left out of the instructions on the sdkman! website -- you need to ""source"" the sdkman-init script again. after you do that, you should see all of your new software in sdk current : $ source $home /.sdkman/bin/sdkman-init.sh $ sdk current

using:

groovy: 2.5.3
java: 11.0.1-open
kotlin: 1.3.0
maven: 3.5.4
sbt: 1.2.6
scala: 2.12.7
spark: 2.3.1 enter fullscreen mode exit fullscreen mode verify that these have installed correctly by calling them with the appropriate version flags or command-line arguments: $ groovy --version ...
groovy version: 2.5.3 ... $ java -version openjdk version ""11.0.1"" 2018-10-16
... $ kotlin -version kotlin version 1.3.0-release-212 ... $ mvn --version apache maven 3.5.4 ...
... $ sbt sbtversion
... [ info] 1.2.6 $ scala -version scala code runner version 2.12.7 ... $ spark-submit --version welcome to ... version 2.3.0 ... enter fullscreen mode exit fullscreen mode ...and that's it! you can also check that spark-shell , groovysh , etc. work. (note that the spark-shell will probably crash unless you're using java 8). install haskell and cabal haskell is really easy to install: $ sudo apt-get install haskell-platform -y $ ghci enter fullscreen mode exit fullscreen mode this also installs the haskell package manager, cabal : $ cabal --version cabal-install version 1.24.0.2
... enter fullscreen mode exit fullscreen mode install and configure hadoop next, i find the most recent stable release of hadoop and make a note of its url and download it with (following along roughly with this guide ): $ wget http://ftp.heanet.ie/mirrors/www.apache.org/dist/hadoop/common/hadoop-2.8.5/hadoop-2.8.5.tar.gz enter fullscreen mode exit fullscreen mode ...and untar to /usr/local/hadoop , redirecting the output to /dev/null : $ sudo mkdir /usr/local/hadoop $ sudo tar -xzvf hadoop-2.8.5.tar.gz -c /usr/local/hadoop > /dev/null enter fullscreen mode exit fullscreen mode note that when we change the java version with sdkman!, it changes the $java_home system variable: $ echo $java_home /home/andrew/.sdkman/candidates/java/11.0.1-open $ sdk use java 8.0.191-oracle $ echo $java_home /home/andrew/.sdkman/candidates/java/8.0.191-oracle enter fullscreen mode exit fullscreen mode hadoop requires access to the java libraries. if we want hadoop to use the default version of java, we can use $java_home in /usr/local/hadoop/hadoop-2.8.5/etc/hadoop/hadoop-env.sh . if we want it to stick to a specific java version (say java 8), we can use a static value like $sdkman_dir/candidates/java/8.0.191-oracle . i'm going to leave this alone for now. (see the previous link for more information.) i set a hadoop_home variable for ease of use: $ export hadoop_home = /usr/local/hadoop/hadoop-2.8.5 enter fullscreen mode exit fullscreen mode ...and run one of hadoop's mapreduce examples to ensure it's working: $ mkdir ~/mrtest $ cp $hadoop_home /etc/hadoop/ * .xml ~/mrtest $ $hadoop_home /bin/hadoop jar \ $ $hadoop_home /share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar \ $ grep ~/mrtest ~/grep_example 'principal[.]*' enter fullscreen mode exit fullscreen mode if it's run successfully, the output ends with something like file input format counters
                bytes read = 151
        file output format counters
                bytes written = 37 enter fullscreen mode exit fullscreen mode the result can be seen by typing: $ cat ~/grep_example/ * 6       principal
1       principal. enter fullscreen mode exit fullscreen mode hadoop works! manage multiple python versions with pyenv next, i install pyenv to manage multiple python versions. install the prerequisites with: $ sudo apt-get install make build-essential libssl-dev zlib1g-dev libbz2-dev \ libreadline-dev libsqlite3-dev wget libncurses5-dev libncursesw5-dev \ llvm xz-utils tk-dev libffi-dev liblzma-dev -y enter fullscreen mode exit fullscreen mode then install pyenv with: $ git clone https://github.com/pyenv/pyenv.git ~/.pyenv $ export pyenv_home = $home /.pyenv $ pyenv init - enter fullscreen mode exit fullscreen mode verify that it works with: $ $pyenv_home /bin/pyenv versions * system ( set by /home/andrew/.pyenv/version ) enter fullscreen mode exit fullscreen mode install other versions with: $ $pyenv_home /bin/pyenv install 2.7.15 $ $pyenv_home /bin/pyenv install 3.7.1 enter fullscreen mode exit fullscreen mode see the available versions again: $ $pyenv_home /bin/pyenv versions * system ( set by /home/andrew/.pyenv/version ) 2.7.15
  3.7.1 enter fullscreen mode exit fullscreen mode switch default versions and verify that you've switched with: $ $pyenv_home /bin/pyenv global 2.7.15 $ $pyenv_home /shims/python --version python 2.7.15 $ $pyenv_home /bin/pyenv global 3.7.1 $ $pyenv_home /shims/python --version python 3.7.1 enter fullscreen mode exit fullscreen mode customising ~/.bashrc i don't add anything to my $path until i'm sure i understand what it's doing there. it's easy to have a huge $path that just includes every directory and not have any clue where an actual executable is being sourced from. i also try to have _home system variables for each piece of software that i install (this is usually the directory which contains the bin/ directory), so i can find them more easily later. so in my .bashrc , i'll now add the following: ##---------------------------------------------------------------------------- ##  handled by sdkman: ##---------------------------------------------------------------------------- export sdkman_home = $sdkman_dir #      groovy_home #        java_home #      kotlin_home #       maven_home #         sbt_home #       scala_home #       spark_home ##---------------------------------------------------------------------------- ##  other important directories: ##---------------------------------------------------------------------------- export pyenv_home = $home /.pyenv export hadoop_home = /usr/local/hadoop/hadoop-2.8.5 export hadoop_classpath = $hadoop_home /share/hadoop/common ##---------------------------------------------------------------------------- ##  jar files ##---------------------------------------------------------------------------- export java_jars = $sdkman_home /candidates/scala/jars export scala_jars = $sdkman_home /candidates/java/jars ##---------------------------------------------------------------------------- ##  jar lists ##---------------------------------------------------------------------------- export java_jars_list = "".: $java_jars / \* "" export scala_jars_list = "".: $java_jars / \* : $hadoop_classpath / \* : $scala_jars / \* "" ##---------------------------------------------------------------------------- ##  update path ##---------------------------------------------------------------------------- export path = $path :/bin:/sbin export path = $path :/usr/bin:/usr/sbin export path = $path :/usr/local/bin:/usr/local/sbin export path = $path : $hadoop_home /bin export path = $path : $hadoop_home /sbin export path = $path : $pyenv_home /bin enter fullscreen mode exit fullscreen mode source the file to load these shortcuts into the shell, and then make the directories for java_jars and scala_jars : $ source ~/.bashrc $ mkdir $java_jars $ mkdir $scala_jars enter fullscreen mode exit fullscreen mode i try to keep all of my *.jar files in one place. now that we have all of these bin/ directories on the path , we can just write: $ pyenv versions $ hadoop version enter fullscreen mode exit fullscreen mode ...instead of the lengthier: $ $pyenv_home /bin/pyenv versions $ $hadoop_home /bin/hadoop version enter fullscreen mode exit fullscreen mode in general, you should always know which binary you're calling when you run something on the command line. doing it this way (verifying that the software works before we go editing the path) helps you to understand where the software you're running actually ""lives"" on your system. as jess mentioned in this article: essential quality of life terminal improvements jess unrein ・ oct 26 '18 #bash #terminal #explainlikeimfive #tutorial ... ~/.bash_profile is sourced for login shells only, while ~/.bashrc is sourced for interactive non-login shells. this means that ~/.bashrc_profile is only sourced when you log into the machine via ssh or something. most of the time this probably isn't what you want. i put all of my custom shell scripts, aliases, etc. in ~/.bashrc . i also have lots of custom shell scripts that i source on startup, but that's for another article, i think! at this point, you should be able to run all major jvm languages and manage their versions, manage java and scala projects with maven and sbt, analyse and store ""big data"" with apache hadoop and spark, code in haskell and the ghci shell, and easily run multiple versions of python with pyenv . in my article on my shell scripts, i'll also talk about how i customise colours, fonts, etc. in my shell for maximum prettiness. sshing into a remote linux vm from windows the last thing to set up for now is the ssh connection between my local windows pc and the remote windows server which hosts by ubuntu vm. note that these instructions are particular to my setup and may not work for you, but it's worth a try if this is similar to what you're trying to accomplish. the first thing i do is run ifconfig within the virtual machine: $ ifconfig
enp0s3: flags = 4163<up,broadcast,running,multicast>  mtu 1500
...
lo: flags = 73<up,loopback,running>  mtu 65536
... enter fullscreen mode exit fullscreen mode you should see two connections similar to the ones above -- enp0s3 and lo . we need to add a third one, so let's power down the vm and go back to virtualbox. go back to ""machine tools"", select your vm from the list on the left, and click the ""settings"" button: click on ""network"" from the menu on the left-hand side and click on ""adapter 2"". check ""enable network adapter"" and next to ""attached to:"", select ""host-only adapter"": whatever name virtualbox fills in here is fine. hit ""ok"" and restart your vm. now, when you run ifconfig on the ubuntu vm, you should see three connections: $ ifconfig
enp0s3: flags = 4163<up,broadcast,running,multicast>  mtu 1500
...
enp0s8: flags = 4163<up,broadcast,running,multicast>  mtu 1500
...
lo: flags = 73<up,loopback,running>  mtu 65536
... enter fullscreen mode exit fullscreen mode we need to edit the /etc/network/interfaces file next. open it as sudo and add the following lines: auto enp0s8
iface enp0s8 inet static
address 192.168.___.10
netmask 255.255.255.0 enter fullscreen mode exit fullscreen mode ...where the ___ should be the same as in the network adapter setup steps at the beginning of this walkthrough. note that the last part of this ip address is .10 , while above it was .1 . the most important step is to then set up a port forward within virtualbox. this is what allows you to ssh into the host machine (windows server, in my case) and have it forward that connection to the vm (ubuntu, in my case). to set up a port forward in virtualbox, go to the ""machine tools"" page, make sure your vm is selected, and click ""settings"", just as we did previously. click on ""network"" on the lef-hand side menu, stay on ""adapter 1"", and open the ""advanced"" section. click on the ""port forwarding"" button: add a new port forwarding rule by clicking the green ""+"" sign at the top-right: this part is a bit complex in my setup. we have a server sitting behind a firewall, so the ip address of the server itself is different than the ip address i actually ssh into. here, i use the ip address of the server, which you can find by running ipconfig in the windows cmd prompt on the remote host (windows server): in my case, the server is on a private lan behind the firewall and the server's ip on that lan is 192.168.100.100 . we also have port forwarding set up on the firewall so that the port i ssh into is different than the port the server sees us trying to access. so the port the server sees us accessing is 22 . in general, the ""host ip"" and ""host port"" are the ip address and port you're trying to access on your remote windows machine (windows server, for my setup). the ""guest ip"" and ""guest port"" are the ip address and port of your vm as seen by your server. the guest ip is the 192.168.___.10 one we set up above, and the port can be any number. i try to use high-value prime numbers, but anything is fine really. in this case, i'll use 33331 , because why not. this is only the port that the remote host uses to talk to the vm so it doesn't really matter what you use here: the port forwarding rule can have any name. hit ""ok"" and ""ok"" again and go back to your virtual machine. the last thing we need to do is edit /etc/ssh/sshd_config . open it as sudo and change the line: #port 22 enter fullscreen mode exit fullscreen mode to port 33331 enter fullscreen mode exit fullscreen mode or whatever port number you picked. this ensures that your ssh server on your virtual machine is listening for ssh connections on that port. now, run ifconfig again and take a look at the inet address associated with enp0s8 . in my case, it's 192.168.100.111 . but we declared enp0s8 to be static and to have the ip address 192.168.___.10 by editing /etc/network/interfaces . let's turn this network adapter off and on again with the commands: $ sudo ifdown enp0s8 $ sudo ifup enp0s8 enter fullscreen mode exit fullscreen mode now when you run ifconfig on the ubuntu vm, you should see the ip address you defined as the inet address. you should be able to ping the ip of enp0s8 from windows cmd prompt now, on the remote host (windows server, for me): and we can now ssh into the vm remotely from mobaxterm on the local windows machine: ssh -p <port> <username>@<ip> enter fullscreen mode exit fullscreen mode above, <port> is the port address you use to access the remote host. in my case, this is the port that was opened on the firewall, which forwards to port 22 on the server. <username> is your username within the vm (although in my case, i have the same name on the remote host and the vm, andrew ). finally, <ip> is the ip address of the remote host (or, again, in my case, the ip address of the firewall behind which the remote host sits). even after you do all this, you might get an error that says something like: warning: remote host specification has changed! enter fullscreen mode exit fullscreen mode this happens because we're forwarding the port on the remote host to the port of the vm, but we already have a key within %mobaxterm_home%\home\.ssh\known_hosts which relates to this ip address. simply open that file and remove any lines which begin with that ip. i did that and then (ip address and port number below are for illustrative purposes only): $ ssh -p 11111 andrew@156.77.221.23
permanently added ` [ 156.77.221.23]:11111 ' (ecdsa) to the list of known hosts.
welcome to ubuntu 18.04.1. lts (gnu/linux 4.15.0-38-generic x86_64)
...
andrew@ubuntuvm:~$ enter fullscreen mode exit fullscreen mode ...it works! i know this setup is extremely specific and may not help most people, but it took me a bit of time to figure out how to get everything up and running and if i can help just one person, then it's worth it. (also i had to write this up so my coworkers would know how to do this in the future!) let me know what you think below, and thanks for reading (if you've made it this far)! in a future post, i'll discuss how i customise my shell with convenience functions, aliases, and fonts and colors! stay tuned! top comments (5) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • nov 1 '18 • edited on nov 1 • edited dropdown menu copy link hide while i agree generally with your sentiment, i'd still prefer a full distro over ""reverse wine"". from wiki : this subsystem is not capable of running all linux software, such as 32-bit binaries, or those that require specific linux kernel services not implemented in wsl. since there is no ""real"" linux kernel in wsl, kernel modules, such as device drivers, for linux can't be run. it is possible to run some graphical (gui) applications (such as mozilla firefox) by installing an x11 server within the windows (host) environment (such as vcxsrv or xming), although not without caveats, such as the lack of audio support or hardware acceleration (resulting in poor graphics performance). support for opencl and cuda is also not being implemented currently, although planned for future releases. that said, microsoft explicitly states that wsl is oriented to the development of applications, and not for desktop environment or production servers, recommending the use of virtual machines (hyper-v or kubernetes) and azure for those purposes. like comment: like comment: 2 likes like thread thread andré sékou (mapy) andré sékou (mapy) andré sékou (mapy) follow coder location burkina faso, bobo-dioulasso work m. at net solutions sarl joined nov 9, 2019 • nov 9 '19 dropdown menu copy link hide bien dit andrew! like comment: like comment: 2 likes like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • nov 1 '18 • edited on nov 1 • edited dropdown menu copy link hide wasting resources isn't much of a concern when you have 1-3 people using 384gb ram, 4tb ssd storage and two xeon gold 6154 processors. i'd rather have a full linux distro than have to deal with whatever limitations come along with wsl. it's definitely something to look into, though. we have windows server 2016 and i think it's on there. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 more from andrew (he/him) 5 tips for writing articles people will want to read # writing # beginners # tutorial super simple markdown # markdown # writing # beginners # tutorial 20 small steps to become a regex master # regex # beginners # tutorial # firstyearincode 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/willvelida/building-my-first-alexa-skill--rugby-facts-doc,,Unit Testing,"building my first alexa skill — rugby facts - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse will velida posted on dec 6, 2017 • originally published at medium on dec 6, 2017 building my first alexa skill — rugby facts # amazonecho # amazon # aws # alexa update: this skill is now available for download ! if you have any feedback or want to show it some love, i'd love to hear from you! amazon are currently running a competition this december for people who develop alexa skills. the stipulation is pretty simple, develop a new skill and get a shiny new alexa hoodie! if more than 75 customers enable the skill within the first 30 days in the alexa skills store, you could win a echo dot! sounds pretty cool right? well i’ve set my sights a little lower than that. i just want the hoodie. plus i’ve wanted to dive into developing something for a voice assistant, so this seemed like a good opportunity to learn something cool and possibly get something out of it. for those of you who know me a little bit, you’ll know that i’m a rugby fan and i like to think i’m quite knowledgeable about the game. in the beginner tutorials of the amazon alexa skills set, there’s a pretty basic tutorial for creating a fact skill that goes through the steps required for building a skill that provides users with facts based on a certain topic. this seemed like a pretty easy concept to marry up with my love of rugby. and all in the pursuit of a free hoodie! what more could i want? this post won’t attempt to go through the tutorial (you can do that yourself here ) i just want to provide some commentary on how easy it is to create a pretty simple skill and get your feet wet in alexa skill development. before we get started, at the time of writing my skill has been submitted for certification. step 1: setting up your alexa skill in the developer portal first up, we need to set up our skill. really easy. you have to sign up for an amazon developer account, but this is pretty simple in itself and it’s free. the developer console is pretty easy to navigate and creating a new skill is just a matter of hitting the right buttons. once you’ve created a new skill, you encounter another simple checklist to create the skill (see below): the only thing of real concern here is choosing an invocation name (the name users use to activate the skill). i just went with the name of the skill for simplicity. next, we move to the interaction model. since the tutorial guided me to use the skill builder, i went along with it. for those of you who have experience using microsoft luis.ai , it’s pretty much the same thing. you create a new intent and add some simple utterances for your skill, like so: save and build (my build time was around a minute, not too long at all). step 2: setting up a lambda function using amazon web services this was an interesting part. for this step, we went to aws to set up and configure a lambda function for our skill. i’ve got minimal experience using aws but i’ve never worked with lambda before. at the time of writing, lambda only works in us east and ireland. pretty handy for me as i’m living in the uk and my ‘customers” will be in the uk. creating the function is pretty simple. it was just a case of me selecting a blueprint for the alexa skill kit, using alexa skills kit as a trigger and setting up a lambda role for the first time. i could only set up the trigger after the function was created, so there is a bit of a discrepancy in the tutorial, but it wasn’t anything major. step 3: connecting your voice user interface to your lambda function again, another really simple step (hopefully a pattern is forming in your mind). just a case of linking my aws function with the voice user interface. it’s sounds too complicated, but it’s just filling our a form. for a basic fact skill, there wasn’t any real work to do here. in the future, i’ll probably encounter complexities for more complex apps. step 4: testing your alexa skill did everything work? sure. the tutorial shows you how to test the voice simulator and the service simulator to ensure that everything is hooked up. it was also super simple of creating a test event in aws lambda. i’ll be honest with you, for the purposes of building something simple to get a free hoodie, i was really glad that amazon did most of the work for me. i just tested the service inside the function portal, but you can test your skills using either echosim.io or unit testing. i did neither. step 5: customize the skill to be yours the fun part! this is where i customized the skill to include facts about rugby! the template skill has facts about space, so a bit of work was required to make the skill produce rugby facts. the most tedious part of the whole process was actually finding enough facts about rugby to make the skill worthwhile. amazon recommends that you have at least 25 facts but i reached about 50. in a future release, i want to try and produce 200. i had to do a bit of exploring on amazon’s github repos to find the index.js file need to customize the skill (i got a 404 on the link provided in the tutorial), but i soon found it here . again, all that was needed here was to add new facts to the data array under the really obvious to do comment and to change some of the const’s above the data array (changing the skill name to match mine, changing the help message etc.) i can barely speak english, let alone another language so i didn’t need to do any work with translations. the audience of my skill will be based in the uk market (as it’s a uk competition). step 6: get your skill certified and published like most developers i know, marketing/coloring in (:p) isn’t something i enjoy, but this process was fairly simple. all i had to do was set a category for the skill, provide some testing instructions to the good folks at amazon and select a country and region. then i had to write a skill description. basic marketing things. we also have to provide some example phrases for the skill to let customers know how they can interact with the skill. i just ctrl+c the sample utterances i provided in the skills builder. this is recommended by amazon and why would you do it otherwise. i mean, it might be funny giving customers example phrases that won’t work with your skill, but that’s not cool. i hate creating images for my apps. i suck at paint, never mind gimp or photoshop. i produced some crappy images that fitted the requirements of the amazon store. it’s not great, but i was just playing around with it, so i’m not gonna sweat the small stuff. finally, i had to answer a few privacy questions. again (as with the whole process) pretty simple: once everything’s done, you can submit for certification. you get an email saying that it’ll take 7 days for feedback, so hopefully i’ll hear something soon. i’ve gotten really fast submission times for both google play and the windows store, so hopefully this will follow suit. final thoughts this was a pretty fun way to spend an afternoon! i learned something new and hopefully will get a hoodie out of it. i’m keen to dive a little bit deeper with the amazon skills kit. you can build games, lookup contact information, create how-to skills etc. i’m also keen to look at the advanced skill sets and see what’s involved in creating some more meaningful skills than just a random fact generator. i’d heard some crazy tinhat foil things about the alexa, but i have to admit i’m interested in getting one just so i can build cooler things for it. i’m glad that it’s coming to new zealand next year, so i’ll get a chance to build skills in my spare time over there. hopefully this post has inspired you to explore the alexa skill kit yourself and build something for it. at the time of writing (december 2017) there is a competition for building skills running in the uk, so i’d encourage you to get involved. who knows? you’ll probably build something better and win an echo for yourself! top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand peter kim frank peter kim frank peter kim frank follow doing a bit of everything at dev / forem email peter@dev.to education wesleyan university pronouns he/him work co-founder joined jan 3, 2017 • dec 6 '17 dropdown menu copy link hide this is awesome, great write-up. once it's available, we'll install it on the echo show we have in our office. as a former second-row (lock) turned inside-center, i'll definitely appreciate sharing my love of rugby with the rest of the dev.to team :) like comment: like comment: 2 likes like comment button reply collapse expand will velida will velida will velida follow senior software engineer at microsoft location australia education university of auckland work senior software engineer at microsoft joined sep 30, 2017 • dec 6 '17 dropdown menu copy link hide thanks! :) if anything goes wrong or if it doesn't act as expected, let me know so i can fix it! :) like comment: like comment: 2 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse will velida follow senior software engineer at microsoft location australia education university of auckland work senior software engineer at microsoft joined sep 30, 2017 trending on dev community hot 5 cheap object storage providers # webdev # programming # aws # cloud what was your win this week?! # weeklyretro # discuss it's 2025 - why is offline file sharing still so broken? # privacy # opensource # productivity # discuss 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/jj/matching-things-with-perl-6-grammars-ao9,,,"matching things with raku grammars - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse juan julián merelo guervós posted on dec 17, 2017 • edited on jul 25, 2023 matching things with raku grammars # grammars # rakulang # regexes # raku previously on this series, we learned how to define a grammar in perl6 and how to use it for parsing a paragraph. and i say we because that was my objective when starting this, to learn to use grammars myself so that i can put them to good use. eventually, i would like to make a markdown parser , if such a thing is possible, but in coding, the path is more important than the destination and i intend to take you along in this trip. and one of the things that characterizes markdown is matching quote-like construct such as the asterisks i have used in the source of this post to make quote-like stand out as italics, or the backticks i have used for quote-like . we can create a mini-grammar for (maybe) quoted words this way: grammar simple-quoted {
    token top { ^  <quoted> $}
    token quoted { <quote>? <letters> <quote>?  } 
    token quote { ""*""|""`"" }
    token letters { \w+ }
}

my $quoted = ""*enhanced*"";
my $parsed = simple-quoted.parse($quoted);
say $parsed; enter fullscreen mode exit fullscreen mode we always start at the top rule, which says that there should be a quoted word, and that's it. that's why we use ^ and $ to anchor the beginning and the end. if there's more than one word, it won't work. this will, and it will print: ｢*enhanced*｣
 quoted => ｢*enhanced*｣
  quote => ｢*｣
  letters => ｢enhanced｣
  quote => ｢*｣ enter fullscreen mode exit fullscreen mode this is a printout of a match -like structure that uses once again the square quotes ｢｣ and that has, first, the matched string and then a hash which effectively includes as parts the different parts, in tree structure, that have been destructured from the imput. this is not terribly ugly, with indentation telling you a bit about the structure, but is not all there is. we will use brian d. foy's prettydump to check it out. let's just change the last line to say $parsed.perl; enter fullscreen mode exit fullscreen mode which, once reformatted, looks like this: match.new(
    list => (),
    made => any,
    pos => 10,
    hash => map.new(
    (:quoted(match.new(
            list => (),
            made => any,
            pos => 10,
            hash => map.new(
                (:letters(
                 match.new(
                     list => (),
                     made => any,
                     pos => 9,
                     hash => map.new(()),
                     orig => ""*enhanced*"",
                     from => 1)),
                 :quote(
                 [match.new(
                     list => (),
                     made => any,
                     pos => 1,
                     hash => map.new(()),
                     orig => ""*enhanced*"",
                     from => 0),
                  match.new(
                      list => (),
                      made => any,
                      pos => 10,
                      hash => map.new(()),
                      orig => ""*enhanced*"",
                      from => 9)]))),
            orig => ""*enhanced*"",
            from => 0)))),
    orig => ""*enhanced*"",
    from => 0) enter fullscreen mode exit fullscreen mode what we see here is that grammars create a recursive set of matches. this is simply a hash of hashes, but we can also use match methods for accessing it; there's roughly one method per key , and keys in perl 6 are those things before the fat arrow. so say $parsed.hash; enter fullscreen mode exit fullscreen mode will return map.new((quoted => ｢*enhanced*｣
 quote => ｢*｣
 letters => ｢enhanced｣
 quote => ｢*｣)) enter fullscreen mode exit fullscreen mode but this is actually the big data structure in the first match level. if we want to access the innermost structure, we'll have to do: $parsed.hash<quoted>.hash enter fullscreen mode exit fullscreen mode which will return map.new((letters => ｢enhanced｣, quote => [｢*｣ ｢*｣])) . that's where we want to be. we have the quotes, and whatever is inside it. we can work with that. don't worry, there's an easier way of doing that. keep reading this series. mismatched matches the witty reader will probably have noticed that mismatched quotes will also be happily parsed: > simple-quoted.parse(""*mismatch`"");
｢*mismatch`｣
 quoted => ｢*mismatch`｣
  quote => ｢*｣
  letters => ｢mismatch｣
  quote => ｢`｣ enter fullscreen mode exit fullscreen mode that's not good. not good at all. we have to change the grammar, and actually have it take into account that quotes must be the same at the beginning and the end of the word. let us take a hint from regular expressions and let's reformulate it this way: grammar quoted {
    token top { ^ <letters> | <quoted> $}
    token quoted { (<quote>) <letters> $0  } 
    token quote { ""*""|""`"" }
    token letters { \w+ }
} enter fullscreen mode exit fullscreen mode the only change is in the quoted token, which now captures the first quote and only matches if it is the same at the end; the $0 variable does just that; stores the match, and will not let that kind of crockery pass muster. now *enhanced` enter fullscreen mode exit fullscreen mode will fail and return any , that is, well, ""we don't grokk this, this is a bad thing"". through the parentheses we capture, with the $0 we reproduce whatever was captured before. if it's not the same thing, it fails, but if it is the same quote, it works alright. more grammars between the two post of this series, ""the little match girl"" was written in the raku advent calendar, and it shows you how to create and test complex, and big, grammars. and of course, you can always check out parsing with perl 6 regexes and grammars: a recursive descent into parsing , an excellent book by the very knowledgeable (and helpful) moritz lentz . top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse juan julián merelo guervós follow coder of code, writer of writings. education phd in physics work professor at some university joined mar 1, 2017 more from juan julián merelo guervós testing your raku module using github actions # raku # githubactions # testing # tdd loops in raku and when to use them # perl6 # tutorial # loops # raku coupling and pairing and testing # raku # grammars # parsing 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://dev.to/ben/what-are-signs-that-you-should-quit-your-job/comments,,,"discussion of what are signs that you should quit your job? - dev community skip to content navigation menu search powered by search algolia log in create account dev community close what are signs that you should quit your job? ben halpern on june 05, 2017 i'm curious, do you have experience quitting a job? what was it that made you realize it was the right choice, and what might be something you're better off pushing through with? i have no interest in quitting my current job, for the record. ðÿ˜ personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand jennifer konikowski jennifer konikowski jennifer konikowski follow go developer, pyladies boston founder, general rabble-rouser, and yinzer. location pittsburgh, pa education bs in isye at georgia tech, ms in software engineering at penn state work senior software engineer at splice joined apr 22, 2016 • oct 13 '17 dropdown menu copy link hide i have a ton of experience with this! past signs that i should quit my job: lots of higher ups are leaving: if you see a lot of upper-level managers and vps leave, there's probably something they know that you don't. you could weather whatever storm is coming but use your best judgment. if you are at the bottom, you'll probably be cut. unsupportive boss: if you are at a large company, move to another team. if there is only one boss you can have and they are negative and shoot you down, you'll probably be more successful elsewhere. i had a boss that bragged about ""managing [me] out"" even though my coworkers were happy with my work. toxic environment: you know it when you see it. always different. depending on how toxic, burn a bridge or two. maybe not on your first gig, but if you have plenty of references and the environment is bad enough (i was crying almost every night), just leave. lack of growth: and sometimes it's nothing big, you just aren't growing the way you want to. also, a valid reason to leave! like comment: like comment: 23 likes like comment button reply collapse expand dina gathe dina gathe dina gathe follow full stack developer with bonus business/marketing/design skills. location greater seattle area, wa joined jan 3, 2017 • jun 6 '17 dropdown menu copy link hide apathy and mediocrity become accepted norms, things that should be flashing alarms aren't. management frequently sabotages work/deadlines so that there can be no accountability (allowing apathy and low morale to set in) talented people come and go, average/below-average people are the ones with longevity management ignoring or downplaying bad signs (slow/no growth, customer complaints, etc.) when you realize you're living in a dilbert cartoon ;-p like comment: like comment: 11 likes like comment button reply collapse expand dave whyte dave whyte dave whyte follow joined oct 14, 2017 • oct 14 '17 dropdown menu copy link hide ""talented people come and go, average/below-average people are the ones with longevity"" - i've had that particular alarm go off a couple of times like comment: like comment: 11 likes like comment button reply collapse expand dina gathe dina gathe dina gathe follow full stack developer with bonus business/marketing/design skills. location greater seattle area, wa joined jan 3, 2017 • oct 14 '17 dropdown menu copy link hide yes, it's good to pay attention to what type of behavior actually gets rewarded (hint: it's often not what's stated in the official corporate values statement). like comment: like comment: 1 like like comment button reply collapse expand doug doug doug follow work software developer joined oct 13, 2017 • oct 13 '17 dropdown menu copy link hide yes, i used to have dilbert comics printed by my desk. after i left i realise i was telling myself subliminally that i was working in an ongoing joke! best career decision ever. like comment: like comment: 2 likes like comment button reply collapse expand michael minshew michael minshew michael minshew follow started learning to program seriously early 2017, cofounded a web company, did that for a bit and then life changed and we closed that down and i moved into sr. ops mgmt and project mgmt. email michael.minshew@gmail.com location austin texas education currently working on my b.s. in supply chain and operations management. work sr. quality manager and project manager joined dec 14, 2017 • dec 27 '17 dropdown menu copy link hide yeeeesssss dilbert cartoon environment == gtho!!! like comment: like comment: 1 like like comment button reply collapse expand theresa luu theresa luu theresa luu follow i'm a full-stack web developer who loves to garden, cook, read, and dance. i enjoy the ritual of making coffee, and sometimes i crave cheese and other times i crave fish sauce. location orange county work software engineer at blizzard - my thoughts and opinions are my own. joined jul 13, 2016 • jun 5 '17 dropdown menu copy link hide my signs include: growth plateauing teams have stopped celebrating each their own wins and the wins of other teams (morale is tanking) management suggestions of building skills in a particular area you have no interest in growing in lack of respect for time.  meetings are routinely missed or canceled. extra hours are expected while personal time is scoffed at. by the time these indicators ring true, my frustration levels are on high alert.  what is important when considering these things for me is to base my decisions on what i am moving towards and not what i'm moving away from.  it's easy to get into an ""i need to gtfo""-mode but i take my time so i am not rushing into a similar situation at my next gig. like comment: like comment: 15 likes like comment button reply collapse expand ben halpern ben halpern ben halpern follow a canadian software developer who thinks he’s funny. email ben@forem.com location ny education mount allison university pronouns he/him work co-founder at forem joined dec 27, 2015 • jun 5 '17 dropdown menu copy link hide teams have stopped celebrating each their own wins and the wins of other teams that seems like a great indicator! definitely keeping this in mind as a leader. like comment: like comment: 7 likes like comment button reply collapse expand vedrantrebovic vedrantrebovic vedrantrebovic follow joined oct 14, 2017 • oct 14 '17 dropdown menu copy link hide being manipulated instead of being led: when your boss is manipulating you to get the job done, instead leading you, you're in the wrong place. odd thing is that everyone want to hire smartest developers out there, but at the same time some of them try to manipulate these same smart developers like they are chimps. from my experience this includes withholding information about the project and long term goals, giving false deadlines to speed things up, meddling with personal relationships (turning developers against each other) and in general not being honest with you and your colleagues. one of the weirdest things i experienced is when they try to affect actual implementation (without any programming knowledge) with specs manipulation. steve jobs once said: it doesn’t make sense to hire smart people and then tell them what to do; we hire smart people so they can tell us what to do. you shouldn't hire smart people to manipulate them. it's waste of time for everyone. blame culture: when bad things happen if your boss asks ""who's fault is this?"" instead of ""who can fix this?"", you should consider quitting. it's needless to say it's impossible to do anything creative or inspiring in these places. brown nose syndrome: if it makes it easier to get promoted or it makes your everyday at work easier by kissing your bosses ass, you're again in the wrong place. you will never reach your full potential there, and probably become very frustrated. inconclusive reward system (or not having one): for example promotions and bonuses are very different across teams, even though there are no obvious reasons for that. either you're not being recognized, or not being valued enough. also if your bonuses consist of praises and thank you it's time to move on. incompetent people evaluate your work: if your code is being evaluated in weird ways it's time to move on. my experience include: counting how many bugs each developer made and then reducing their salaries by some calculation based on number of bugs, using scrum backlog as time and attendance system. like comment: like comment: 12 likes like comment button reply collapse expand royall spence royall spence royall spence follow joined oct 13, 2017 • oct 13 '17 dropdown menu copy link hide when management starts overriding technical decisions made by the technical staff. it inevitably leads to bad software and shows a total disrespect for people who make software. like comment: like comment: 14 likes like comment button reply collapse expand chrism chrism chrism follow joined aug 5, 2017 • oct 14 '17 dropdown menu copy link hide 1) if you are miserable going to work every day 2) the environment is a bad fit for you personally 3) toxic environments like comment: like comment: 13 likes like comment button reply collapse expand fernando gekdyszman fernando gekdyszman fernando gekdyszman follow joined sep 9, 2017 • oct 14 '17 dropdown menu copy link hide pay attention on how frequently you get into linkedin. a good sign is when that rate goes up. losing the option of thinking and making decisions could be another sign. working in projects you either don't enjoy or you don't consider they add value to your career. like comment: like comment: 7 likes like comment button reply collapse expand antoinette maria antoinette maria antoinette maria follow staff security engineer lead detection & response at ramp location des moines, ia education b.s. computer science work ramp joined mar 25, 2017 • jun 5 '17 dropdown menu copy link hide i also know i don't want to quit my current job, but i think you should quit when you begin to dread going into work. if it starts to feel like your job is more of a chore and a burden, then leave. if you start to notice that your mental and physical health are being negatively impacted as a direct result of on-the-job stress then leave. there are far to many companies out there that know how to treat their employees to just sit and endure toxic environments. like comment: like comment: 6 likes like comment button reply collapse expand ben halpern ben halpern ben halpern follow a canadian software developer who thinks he’s funny. email ben@forem.com location ny education mount allison university pronouns he/him work co-founder at forem joined dec 27, 2015 • jun 5 '17 dropdown menu copy link hide when you've recognized you're in a toxic culture and you will be leaving . what's the game plan at that point? how do you set yourself up to make the current situation work best for your future? like comment: like comment: 2 likes like comment button reply collapse expand antoinette maria antoinette maria antoinette maria follow staff security engineer lead detection & response at ramp location des moines, ia education b.s. computer science work ramp joined mar 25, 2017 • jun 5 '17 dropdown menu copy link hide i tend to err on the side of burning as few bridges as possible, but it really depends on the situation and why it's toxic. in my particular case, i didn't quit my job, but moved to a different team within the same company. i think that was only possible because i continued to do good work and tuned out the trigger to the toxic situation (a person and their comments on gender and masculinity). like comment: like comment: 2 likes like comment button reply collapse expand paul leclercq paul leclercq paul leclercq follow data engineer, sports and music lover location marseille ""mars"", france work data engineer joined mar 14, 2017 • jun 5 '17 dropdown menu copy link hide i once quit because i didn't like where i would have gone (less and less technical stuffs) if i had continued doing it, even though the team was, in general, great. 2 years after i am really glad i made this choice as i really enjoy, more and more, learning about computer science fundamentals (thanks to the great articles published on dev.to!) to be able to understand how works (data-intensive aka big data :p) frameworks under the hood. also, have you ever encountered the annoying ""don't tell to anybody"" policy when you talk to your manager about it? especially in france, where we have almost all the time a 3-months notice for engineers. like comment: like comment: 3 likes like comment button reply collapse expand theresa luu theresa luu theresa luu follow i'm a full-stack web developer who loves to garden, cook, read, and dance. i enjoy the ritual of making coffee, and sometimes i crave cheese and other times i crave fish sauce. location orange county work software engineer at blizzard - my thoughts and opinions are my own. joined jul 13, 2016 • jun 5 '17 dropdown menu copy link hide you have to give a 3-month's notice to quit in france? like comment: like comment: 2 likes like comment button reply collapse expand paul leclercq paul leclercq paul leclercq follow data engineer, sports and music lover location marseille ""mars"", france work data engineer joined mar 14, 2017 • jun 5 '17 dropdown menu copy link hide it depends on your company policy, the most of the time is 3 months for engineers, sometimes is 1 month. like comment: like comment: 1 like like thread thread theresa luu theresa luu theresa luu follow i'm a full-stack web developer who loves to garden, cook, read, and dance. i enjoy the ritual of making coffee, and sometimes i crave cheese and other times i crave fish sauce. location orange county work software engineer at blizzard - my thoughts and opinions are my own. joined jul 13, 2016 • jun 5 '17 dropdown menu copy link hide that must be tough trying to keep productivity up if you know for sure you are going to leave for such a long duration. like comment: like comment: 1 like like thread thread paul leclercq paul leclercq paul leclercq follow data engineer, sports and music lover location marseille ""mars"", france work data engineer joined mar 14, 2017 • jun 6 '17 dropdown menu copy link hide the main idea is to respect your colleagues by getting sure that everything you know is written down in a documentation and train your substitute. like comment: like comment: 4 likes like thread thread theresa luu theresa luu theresa luu follow i'm a full-stack web developer who loves to garden, cook, read, and dance. i enjoy the ritual of making coffee, and sometimes i crave cheese and other times i crave fish sauce. location orange county work software engineer at blizzard - my thoughts and opinions are my own. joined jul 13, 2016 • jun 6 '17 dropdown menu copy link hide ah, i'm impressed and am at awe at that gesture. like comment: like comment: 2 likes like thread thread aga zaboklicka aga zaboklicka aga zaboklicka follow i am a software dev girl who loves uncle bob, is drawn to the human side of software development and clean coded applications, and enjoys acting as a liaison between the business and tech. location poland joined jun 5, 2017 • jun 8 '17 dropdown menu copy link hide i think 3-months notice is pretty common in europe. the same goes for germany. in poland, where i leave we have 1-month notice unlesss we have more than 3 years experience. it's probably due to training of people who are going to move into your place or for the employer to find someone to take your place... like comment: like comment: 1 like like comment button reply collapse expand rodrigo alencar rodrigo alencar rodrigo alencar follow joined oct 19, 2017 • oct 19 '17 dropdown menu copy link hide some place at least here in brazil ask 1 year without even work with same tec like comment: like comment: 1 like like comment button reply collapse expand kim arnett  kim arnett  kim arnett  follow kim arnett [she/her] leads the mobile team at deque systems, bringing expertise in ios development and a strong focus on accessibility, user experience, and team dynamics. location michigan pronouns she/her joined feb 7, 2017 • jun 5 '17 dropdown menu copy link hide various reasons i've quit previous jobs/teams are not being challenged enough company morals became more and more corrupt, environment became cliquey and un-welcoming. once a safety concern forced me to quickly find another team no manager support to grow and expand and reach my career goals continually being lied to about being able to take time off/ wfh when i needed to / benefits / payday. once my paycheck was a week late. my loan payments were not happy. i think that's all the big things. my first job i was on 4 teams in under 2 years, and my second job i only lasted a year. not a good track record thus far. lol. like comment: like comment: 5 likes like comment button reply collapse expand anna anna anna follow location austria education bsc in computing joined oct 5, 2017 • oct 13 '17 dropdown menu copy link hide number 1) is hitting me hard at the moment. but i've only been there for 6 months! i'm not sure what to do about this situation at the moment, my cv would look bad. like comment: like comment: 2 likes like comment button reply collapse expand kim arnett  kim arnett  kim arnett  follow kim arnett [she/her] leads the mobile team at deque systems, bringing expertise in ios development and a strong focus on accessibility, user experience, and team dynamics. location michigan pronouns she/her joined feb 7, 2017 • oct 13 '17 dropdown menu copy link hide first thing would be to talk to your manager and see if there’s anything you can do or another team that would be a better fit. like comment: like comment: 3 likes like comment button reply collapse expand ajay karwal ajay karwal ajay karwal follow hi, i'm ajay karwal, a frontend developer, ui designer, writer and tech enthusiast from northampton, uk. location uk work principal frontend developer joined feb 22, 2017 • oct 13 '17 dropdown menu copy link hide for me its all about culture and passion. you work on average 40 hours per week so you want to spend that time doing something you're passionate about and be surrounded by a motivating culture. as a front-end dev, i'm passionate about learning new skills, following industry news and making a difference in people's experiences. i couldn't work in a team where the people around me don't share the same enthusiasm about their chosen profession and treat their job as just a means to pay the bills. for many people a job is just a job and come 5:30pm they don't give a damn about whether they've grown as a person that day or not, but for me, every day where i didn't take a step forward in my career i may as well have taken a step backwards. don't become a developer if you don't have a passion for developing. like comment: like comment: 4 likes like comment button reply collapse expand aniket kadam aniket kadam aniket kadam follow an android dev with ~8 years of experience. i consult and may join fulltime for the right company. location india work android developer joined oct 13, 2017 • oct 13 '17 dropdown menu copy link hide verbal abuse (to anyone), delaying an agreed upon thing to the point of not doing it. mismanagement, passing blame. underpaying. sexism. gaslighting. yes, i'm amazed that i even had to add this here, but it's very common for businesses to behave like abusive spouses who should be in jail. like comment: like comment: 4 likes like comment button reply collapse expand vijay varadan vijay varadan vijay varadan follow joined jun 3, 2017 • jun 6 '17 dropdown menu copy link hide i usually work at a place because of good technical leadership. most places have enough interesting things to work on, but solid technical leadership is hard to find. i leave when good technical leaders and colleagues leave. like comment: like comment: 2 likes like comment button reply collapse expand antero karki antero karki antero karki follow software developer in sweden. always interested in good opportunities in warmer climates... location sweden education some work software developer joined sep 6, 2017 • dec 28 '17 dropdown menu copy link hide if i think about leaving and immediately feel better and calmer, that's a sign for me. what i do then is try to figure out why i feel like that and see if it can't be fixed in another way. i usually do that by thinking about my reasons for staying vs leaving, once i even did it on a sheet of paper where i drew 3 columns with the headlines reasons for staying, concerns, and reasons for leaving (the order in which you write them can be an indicator too.) if i have nothing in reasons for leaving and a manageable number in concerns i stay and try to work them out. if i have even one thing in reasons for leaving i start keeping my eyes open for new opportunities. if i have more i do it more urgently. the reasons can be anything e.g. not liking coworkers (never had that but would be a reason for leaving for me), nice offices, bad code and we're not fixing it, stagnating as a developer, not enough resources to develop good software etc. i think it's important also to think about why things turned out the way they did. was there a reason why we didn't do things right? why didn't i like the coworkers? could i have communicated better or differently? what will i do differently on my next job? also, what did i do well? what have i learned? like comment: like comment: 2 likes like comment button reply collapse expand erebos manannán erebos manannán erebos manannán follow location estonia education autodidact joined mar 12, 2017 • dec 9 '17 dropdown menu copy link hide many potential signs: you find yourself finding excuses to not go to work, to take days off, or just overall don't like going to work. if you constantly feel like the smartest guy in the room - i.e. you don't anymore learn from the others. your own progress will be stunted if you allow yourself in a situation like that. if you're talented enough to build applications from the ground up yourself, but the company doesn't reward you highly enough - you can potentially get a good amount of shares if you join an early stage startup if you work on the same kind of problems all the time - being an expert on one subject might be fine for some, but i generally recommend against it as it also means that when that one technology expires you will be useless if the company seems to be falling apart - leadership, core team members are leaving or being fired poor team morale for an extended period of time - if nobody cares to fix the morale issues that's not a healthy sign, find a more motivated team in another company culture of micromanagement - being told exactly how to spend every hour of every day, and tracking your work with scrum poker velocity etc. useless junk intended only to make old fashioned managers feel better having little to no ability to impact your own work - if you can't give feedback, feature suggestions, etc. or nobody listens to your ideas if you're being paid unfairly (other people make more without a good reason) or overall not compensated well enough (other companies pay much more) lots of things along that theme can be good triggers to decide to leave a company. anyway, never just quit - start looking for other jobs, interview for as long as it takes to find the perfect next step for you, and only after you've signed the contract to start at the new place do you quit. like comment: like comment: 2 likes like comment button reply collapse expand isaac lyman isaac lyman isaac lyman follow author of your first year in code (leanpub.com/firstyearincode). find more of my writing at isaaclyman.com/blog. education b.a. in english work .net / typescript developer at health catalyst joined nov 7, 2016 • oct 13 '17 dropdown menu copy link hide i once heard a quote that said, in effect: would you quit your job if you won the lottery? then you should quit your job. i can't figure out who said it (might've been peter thiel? my google skills are failing me here) but for people who have options--and technical people usually do--it's a decent starting point for the long and difficult decision-making process around leaving a job. like comment: like comment: 2 likes like comment button reply collapse expand christian vasquez christian vasquez christian vasquez follow un simple mortal email christianvasquezmelo@gmail.com location santo domingo, dominican republic education software engineering work software developer joined feb 22, 2017 • jan 9 '18 dropdown menu copy link hide 1) when you feel you don't have anyone you can look up to or respect. 2) when the average employee lasts a few months or around 1 year. 3) when you feel you won't even be missed if you leave. 4) when you start having fights instead of meetings (or others yelling at each other over the phone). 5) when the hippos have the last word. hippo: highest paid person opinion like comment: like comment: 1 like like comment button reply collapse expand martha wilson martha wilson martha wilson follow joined oct 25, 2017 • oct 25 '17 dropdown menu copy link hide yes, i used to have dilbert comics printed by my desk. after i left i realise i was telling myself subliminally that i was working in an ongoing joke! survival games like comment: like comment: 1 like like comment button reply collapse expand george george george follow my curiosity is easily stimulated location bristol, united kingdom work hypixel network joined may 9, 2017 • oct 13 '17 dropdown menu copy link hide i've left a couple, the signs i found were 1) not being happy in the place that i worked 2) colleagues not getting along 3) achievements the company/team had made were just brushed off (even if it were large ones) 4) management not listening to employees 5) generally wanting to leave after i saw these signs i just handed in my letter of resignation and left. i knew that leaving the company would be good for me because it was affecting my personal life. i wrote a bit more about it here dev.to/rapidnerd/string-happyworkp... like comment: like comment: 1 like like comment button reply collapse expand marco arduini marco arduini marco arduini follow joined jul 7, 2017 • oct 13 '17 dropdown menu copy link hide possible points from my experience: lack of confidence in boss technical and management skills. incompetent bosses will trust the most convincing people, not necessarily the best workers. lack of confidence in the boss human skills. i once saw my boss at the time requesting that a disabled coworker undergo a test to verify wether he was fit for the job. needless to say this switched on a huge red light for me, and i ended up quitting the job shortly after. lack of cultural fit with coworkers and environment. the feeling that you wouldn't want to have a beer with any of your coworkers. boring, repetitive uninteresting work. long and boring days waiting for something to happen (happened to me as i was working in a bank). lack of recognition. project success routinely credited only to part of the team (usually analysis/design). like comment: like comment: 1 like like comment button reply collapse expand itsasine (kayla) itsasine (kayla) itsasine (kayla) follow azure devops and git admin with a weird interest in résumés and portfolios 🎉
lawful neutral email dev@itsasine.dev education master's in applied mathematics work devops | ci/cd | release engineer joined feb 4, 2017 • oct 15 '17 • edited on oct 15 • edited dropdown menu copy link hide you should always be operating at the edge of your competence. when that stops, move on in some way. work getting easy? take on a side project or see if you want work in a different part of the stack of your current project. already know the whole stack? see if a new project is spinning up or another one could use your skills better and help you grow. everyone's replaceable; your old project will be fine. i'm still in my first post-college job, but i've never been bored for too long since i've moved around a lot. now that i've been on a project for 2 years, i'm using tuition assistance to go to grad school. i'd rather not hop jobs if i don't have to, so i'm trying to make this work for me. always trying to get better one way or another. like comment: like comment: 1 like like comment button reply collapse expand adrian b.g. adrian b.g. adrian b.g. follow striving to become a master go/cloud developer; father 👨‍👧‍👦; 🤖/((full stack web|unity3d) + developer)/g;  science supporter 👩‍🔬; https://coder.today email adrian@coder.today location e eu education bachelor of economic informatics work cloud engineer at crowdstrike joined sep 1, 2017 • dec 28 '17 dropdown menu copy link hide i have a few personal examples @ they are late with payments @ the startup is going to shutdown soon @ when you need to convince your peers the value of automatic testing the rest of the reasons were already covered. like comment: like comment: 1 like like comment button reply collapse expand lisa fehr lisa fehr lisa fehr follow joined feb 22, 2017 • jun 5 '17 dropdown menu copy link hide high stress, low rewards. sometimes you don't realize it was a great idea until after you quit. it could still be a great team and company as a whole. i was in fight mode all the time and ignored my health. like comment: like comment: 1 like like comment button reply collapse expand john terracina john terracina john terracina follow joined jun 5, 2017 • jun 5 '17 dropdown menu copy link hide considering it since i am starting to get feedback that amounts to ""you are really technically skilled so we expect you to perform miracles and we aren't seeing enough miracles"". like comment: like comment: 2 likes like comment button reply collapse expand amir khan amir khan amir khan follow joined may 10, 2017 • jun 6 '17 dropdown menu copy link hide hey ben, right now i am serving notice period in my company and basically there are two reasons one of them is obvious hike % other is important from career point of view. the project is in maintenance mode so there is hardly any work no challenges nothing so i am not learning anything neither coding much. so i think if the same scenario is with you only then quit or else just continue because it seems you are enjoying. like comment: like comment: like comment button reply collapse expand doug doug doug follow work software developer joined oct 13, 2017 • oct 13 '17 dropdown menu copy link hide that's such a shame. i hold out that maintenance should be a rich time for r&d, but if not then leaving is the right thing. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",-1
https://www.linkedin.com/pulse/how-achieve-perfect-hplc-data-quality-toxzc,,,"how to achieve perfect hplc data quality and reproducibility? agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in how to achieve perfect hplc data quality and reproducibility? report this article xinsheng pharmaceutical instrument co., ltd xinsheng pharmaceutical instrument co., ltd professional supplier for upstream&downstream purification published dec 8, 2023 + follow in order to obtain high-quality data in daily testing, hplc systems must be able to provide reliable and reproducible experimental results simultaneously. now let's take a look at the daily precautions and suggestions for conventional liquid phase applications. 01 reagent quality gradient elution is relatively easy to accumulate non-polar impurities in solvents a and b at the tip of the column, and these impurities will gradually be released during the subsequent gradient process. these impurities may appear during the testing process of blank samples and actual samples. for this reason, it is necessary for us to use hplc grade reagents when conducting gradient experiments. low quality reagents may be suitable for isocratic elution, but in gradient elution, even the slightest impurities may cause testing abnormalities. in order to obtain the best experimental results, we should ensure that only hplc grade reagents are used in all hplc experiments. water based reagents and buffer solutions should be replaced regularly to avoid contamination caused by microbial growth, especially impurities in water that can cause many problems. 02 system cleaning as important as the quality of the reagent, keeping the instrument clean helps to reduce the appearance of ghost peaks, it is recommended to punch the instrument frequently. if other problems are encountered, such as liquid spills, leaks and other potential sources of pollution should be cleaned up in time. 03 degassing while some hplc systems can operate without degassing the mobile phase, the use of a degassed solvent makes all systems more reliable. the phenomenon of residual bubbles and gas leakage in the solvent can be largely avoided by degassing the solvent. in addition, there will be residual gas in the hplc system, and it is a good idea to flush the pump and solvent inlet lines daily. we can remove air bubbles in the pump and line by opening the purge valve and flushing the pump for a few minutes at a high flow rate (e.g. 5ml/min). 04 dedicated chromatography column each analytical method should have a dedicated chromatographic column. sharing a column between different methods is not a good idea, as irrelevant chromatographic peaks in one method may become interference peaks in the second method. the sample composition may cause a change in the selectivity of the chromatographic column packing, or one of the methods may cause column degradation. 05 balance before each experiment, the column should be restored to the same level of balance to prepare for subsequent separation experiments. complete equilibrium is essential for gradient elution. 06 initial sample injection some methods can get better results if several ""initial"" injections are performed before the first injection. the injection of these standards, or simulated samples, saturates the slowly balanced active sites on the column, allowing for better reproducibility of experimental results. initial sampling is more useful for separating biochemical samples. sometimes the system suitability injection can also be used as an initial injection. 07 ignore the first sample injection because some methods require the use of initial injection procedures, and the balance of the first injection may not be consistent with the subsequent injection; therefore, it is best to avoid using the first sample for quantitative analysis. the second and subsequent injection is more reliable than the first; system suitability tests performed prior to formal injection can also be used for this purpose. 08 system applicability many methods regulated by regulatory authorities (fda, epa, oecd, usp, etc.) require a system suitability test before analyzing samples. system suitability helps to confirm that equipment and analytical methods can produce reliable experimental results. system suitability testing requirements vary, so the relevant regulatory guidance should be consulted to help select the appropriate test method. many operators use one or a combination of retention time and peak area reproducibility, peak response (detection sensitivity), peak width, peak trailing, separation, and column pressure. as part of the system suitability test. system suitability samples can be diluted standards, simulated samples in extraction substrates, or samples selected to demonstrate system performance. an important principle here is that the selected system suitability sample must be available to test whether the operation in the method achieves the desired purpose. whether or not a system suitability test is required, it is advisable to perform such a test before routine analysis, even if it is just to inject a standard substance to see if the retention time and peak size are as expected. 09 standards and corrections for quantitative analysis, the response of an unknown sample is compared with the response of a known concentration. the concentration range of the standard substance, the number of repeated tests, and the sequence of injections depend on the specific experimental purpose. both external and internal labels can be used. in all cases, the standard should be injected at least once before the unknown sample is analyzed to ensure that the analysis method is working properly before the sample is injected. #xinsheng #chemical #pharmaceutical #insulin #peptide #generics #lab #science #distillation #oil #pharmaceutical #biotechnology #chemical xinsheng latest blogs xinsheng latest blogs 1,113 follower + subscribe like comment copy linkedin facebook twitter share 12 1 comment mahamood md managing director @ dsp instruments pvt ltd 
director - global projects @xinsheng science | chromatography systems 🇮🇳🇲🇾🇿🇦🇦🇪🇮🇷🇮🇶 1y report this comment very informative for learners... like reply 2 reactions 3 reactions to view or add a comment, sign in more articles by xinsheng pharmaceutical instrument co., ltd jun 10, 2025 why is chromatographic column balancing necessary, and what kind of impact will occur if the chromatographic column is not balanced properly? 1. why is column balancing necessary? replace the solvent stored in the chromatographic column establish equilibrium… 6 may 6, 2025 copy of what are the causes of baseline noise in hplc and how to control it? 1. ultraviolet detector - ultraviolet lamp and flowcell (1) there are bubbles in the flowcell a. 7 apr 15, 2025 copy of copy of copy of in the process of liquid chromatography analysis, what are the common chromatographic peaks? do you know the reason why they produce and the solution? trailing peak: trailing peak 1.column sieve plate blockage:… 15 mar 14, 2025 preparative liquid chromatography tips in preparative liquid chromatography, there are three states that can be observed: column pressure, detector baseline… 10 2 comments feb 18, 2025 common leakage problems and solutions of hplc 一、joint leakage leakage at the joint usually occurs at the joint of the column. in general, liquid accumulates at the… 9 2 comments jan 6, 2025 tirzepatide become the new ""weight loss medicine king"" tirzepatide is the world's first once-weekly gip/glp-1 receptor agonist approved for t2dm and long-term weight… 14 dec 19, 2024 check valve cleaning (routine cleaning) ————hplc pump maintenance in the daily maintenance of high performance liquid chromatography, the cleaning of the check valve is also very… 12 nov 6, 2024 [the comparison between sac column, pre-packed column and dac column] with the development of chromatography technology, there are more and more requirements for chromatography columns. in… 17 may 8, 2024 what should i do if the separation deteriorates in liquid chromatography? i believe you have encountered this kind of situation, after a period of time, the separation degree of adjacent… 8 apr 12, 2024 supercritical c02 extraction | ""stability"" is the core condition of high-quality astaxanthin! astaxanthin is extremely unstable when exposed to light, and has a certain proportion of inactivation under both… 12 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/products/graniteriverlabs-grl-wpc-qi-wireless-charging-base-station-tester-grlwpbstc3/?trk=products_directory,,,"grl wpc qi wireless charging base station tester (grl-wp-bst-c3) | linkedin skip to main content linkedin granite river labs inc. in brazil expand search jobs people learning dismiss dismiss dismiss dismiss dismiss join now sign in grl wpc qi wireless charging base station tester (grl-wp-bst-c3) automated testing software by granite river labs inc. see who's skilled in this add as skill learn more report this product report report back submit about grl-wp-bst-c3 (grl-c3) is the world’s first cost effective and user-friendly qi wireless charging compliance test solution. grl-c3 automates all required qi base station tests, enabling product developers to quickly run full compliance and validation test suites at the push of a button. grl-c3 is ideal for qi r&d and compliance testing. key features -single solution for compliance testing of qi base power (bpp) and extended power (epp) profiles -supports qi specification version 1.2.4 and 1.3 -cost effective and easy to use -manual and fully automated modes for compliance and r&d needs -apis for custom test case automation media products media viewer grl wpc qi wireless charging base station tester (grl-wp-bst-c3) integrated wpc qi compliance tester grl c3 qi wireless charging tester overview no other tester has such a full automated suite of compliance, big data benchmarking, and test suites covering power efficiency, charging speed, coil alignment sensitivity, and temperature & safety.  the c3 always covers the latest qi specs, making it easy for you to keep up with the latest changes in wireless charging technology.

with the c3 trusted and used by major technology companies and authorized test labs around the world, find out why the c3 is right for you. similar products browserstack app automate browserstack app automate automated testing software tricentis tosca tricentis tosca automated testing software browserstack accessibility testing browserstack accessibility testing automated testing software lambdatest lambdatest automated testing software keysight eggplant test keysight eggplant test automated testing software qoetient - proactive video qoe improvement platform qoetient - proactive video qoe improvement platform automated testing software sign in to see more show more show less granite river labs inc. products grl automated modular qi2 wireless charging test solution (grl-c3-mp-tpr) grl automated modular qi2 wireless charging test solution (grl-c3-mp-tpr) automated testing software linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines english (english) language",-1
https://www.linkedin.com/pulse/how-successful-chinas-emissions-trading-,,,"how successful is china's emissions trading system? agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in how successful is china's emissions trading system? report this article china-britain business council china-britain business council the uk business network for china and the independent voice of business within this relationship published oct 21, 2022 + follow china is tackling data integrity issues within its carbon market, but more efforts are urgently needed to establish effective controls and minimise complications brought on by other policy drivers, write alistair ritchie and chen yi from china dialogue as concern about climate change increases, a growing number of countries have pledged to reduce carbon emissions by implementing one of the most cost-effective solutions, an emissions trading system (ets). so far, jurisdictions accounting for 55% of global gdp are using emissions trading. china’s recently established national ets has drawn tremendous attention worldwide. it is expected to serve as china’s primary tool to meet its ‘dual carbon’ targets of co2 peaking before 2030 and carbon neutrality before 2060. the largest ets globally, it accounts for 40% of china’s emissions and more than 10% of worldwide emissions, with the potential to double in size once industrial sectors are added to the already-covered energy sector. it is also the first nationwide ets in a large developing country. how does china’s ets work? china’s national emissions trading system (ets) gives each covered company the right to emit a certain amount of co2. the amount is based on an emission intensity benchmark and the company’s level of output. if a company then emits less than its allocated allowance, it can sell the excess on the market. if more, it must purchase allowances or offsets from the market. thus, companies are incentivised to reduce emissions from production activities. how successful has china’s ets been to date? july 2022 saw the first anniversary of trading under china’s ets, by which time 194 million tonnes of carbon emissions allowances worth over $1 billion had been traded on the market. the first compliance cycle saw over 99% compliance out of 2,162 power companies. despite these achievements, there are some key roadblocks. at the top of this list is data quality and integrity. in july 2021, a power plant in inner mongolia was found to have falsified its emissions report by doctoring the date of carbon content testing of the coal it was burning, affecting at least 1 million tonnes of emissions allowances worth $7 million. additional cases were also found and the ministry of ecology and environment (mee) issued a notice requiring provincial environment departments to comprehensively examine data quality. if data was found to be false, they should make adjustments and impose sanctions, stated the notice. some observers blamed the fraudulent behaviour on a december 2019 announcement of an extremely high default value for the carbon content of coal used in the first compliance cycle. this high value was intended to encourage companies to submit actual testing data rather than using the default. but it came too late to drive more testing for 2019, and instead tempted companies to risk faking test reports. at a broader level, multiple factors have increased the risk of this type of event. firstly, sanctions : the fine for fraud, at $3,000 to $4,500, is insignificant compared to the potential gain from fraud. the boundaries of responsibility for data quality are insufficiently clear among different participants, including companies, verification agencies, testing agencies and consultancies. secondly, verification : provincial governments are responsible for selecting verification agencies and tend to prefer local ones. this can rule out more experienced and better qualified agencies from other provinces. moreover, provincial governments have allocated limited funds for paying for verification, and there are not yet settled qualification requirements for verifiers. thirdly, monitoring and reporting : the regulations and guidelines are not sufficiently clear or detailed. for example, in detecting coal carbon content, there appears to be room for tampering with coal samples during collection and preparation. data quality and integrity have been high on the government’s agenda. improved reporting guidelines were published in december 2021 and march 2022; verifiers found guilty of misconduct have been named and shamed; the extremely high default value of carbon content in coal has been replaced; and ets legislation has been included in this year’s state council legislative work plan, which would provide the stricter penalties for non-compliance that the ets needs. improving data quality will be the main priority of the mee in 2022 and 2023, including accelerating further amendments to monitoring, reporting and verification (mrv) guidelines; enhancing emissions data supervision by regular checks at national, provincial and municipal levels; increasing information disclosure; building a system to classify organisations according to their compliance history and supervising them accordingly; and increasing the penalties for violations. china should look again at best practices in south korea and the european union’s ets these are big steps in the right direction. however, it appears that china’s priorities on zero-covid and economic recovery may be complicating efforts to address integrity issues. for example, in view of these priorities, the mee issued a notice in june 2022 to adjust measurement, reporting and verification (mrv) requirements during the second compliance cycle. it stated that companies with three or more months of carbon content test results per year can use the average of these results to calculate emissions for any months that have no test results, instead of using a high default value under the previous requirements. this adjustment appears to leave room for companies to test during months when they are using coal with a lower carbon content. a fundamental necessity for china’s national ets is a robust mrv system. this needs to be supported by efficient it systems, strong security measures, and effective sanctions for non-compliance. this will promote market participants’ confidence, sectoral expansion, and international support and credibility. china is making progress in improving the integrity of its mrv system, but more work is urgently needed. the ets law at the state council level needs to be adopted; a strong accreditation system for verification agencies needs to be established to ensure consistent and high-quality verification across provinces; fees for verification should be sufficient to do a thorough job; and mrv regulations and guidelines need to be clearer and more detailed. china should look again at mrv best practices in the european union’s ets and the mrv system in the south korean ets, the frontrunner in asia. at the same time, the system’s integrity should be protected from being compromised by other policy drivers. china is not alone in facing integrity issues in the early stages of an ets. there were some major fraud cases during the initial phases of the eu ets. however, lessons were learned quickly and effective controls were established. china must also overcome these challenges quickly to pave the way for its ets to expand to the industrial sectors, achieve significant greenhouse gas emissions reductions, and realise its potential in helping the country achieve its dual carbon targets cost-effectively. this article was originally published on china dialogue under the creative commons by nc nd licence. call +44 (0)20 7802 2000 or email enquiries@cbbc.org now to find out how cbbc’s market research services can help you build knowledge and understanding of the chinese market prior to investment. like comment copy linkedin facebook twitter share 5 to view or add a comment, sign in more articles by china-britain business council jun 25, 2025 the best flight options from the uk to china in 2025 travel to china from the uk has evolved significantly since the covid-19 pandemic and the onset of the ukraine conflict… 14 5 comments jun 19, 2025 from robot boxing to real-world impact elinor greenhouse, senior adviser, tech and innovation at the china-britain business council, explains why uk… 8 jun 16, 2025 douyin vs wechat: china’s social media giants compared douyin and wechat are china’s leading social platforms, but serve different purposes for different brands in china’s… 8 jun 8, 2025 navigating the powerhouse: china’s technology and electronics industry china’s technology and electronics industry – the world’s largest – is driven by titans like huawei, xiaomi technology,… 3 jun 7, 2025 raising the voice of chinese students in uk universities the first chinese student elected president of king’s college london students’ union explains how universities can… 42 1 comment jun 1, 2025 what is taobao and why is it at the top of the app charts? chinese e-commerce apps like taobao marketplace have experienced a surge in popularity recently, despite trade tensions… 7 may 31, 2025 understanding china’s 2025 monetary package china’s comprehensive 10-point monetary policy package, unveiled in may 2025, aims to stabilise financial markets and… 8 may 24, 2025 how to set up a bank account in china: a step-by-step guide for foreign citizens opening a bank account in china is a vital step for foreign citizens living, working, or doing business in the country.… 6 may 6, 2025 how to hire and build a team in china understanding china’s unique hiring practices is crucial - this is where you should begin. expanding your business into… 10 apr 20, 2025 xpeng motors: china’s answer to electric innovation lands in the uk from its guangzhou origins to a bold uk launch, xpeng has global ambitions to steer the future of smart mobility in… 22 4 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/posts/roshni-mukherjee-41a986107_meet-superwoman-roshni-mukherjee-episode-activity-6981138587522138112-4-LE?trk=public_profile_like_view,,,"it was an amazing experience to be at the sandeep maheshwari show to share my story , my journey of more than a decade of providing free education through youtube. | roshni mukherjee agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in roshni mukherjee’s post roshni mukherjee founder, learnohub (examfear education) | educator | 100women achievers of india | tedx speaker | 3m+ on youtube | on a mission to make ""quality education accessible and affordable to all"" 2y report this post it was an amazing experience to be at the sandeep maheshwari show to share my story , my journey of more than a decade of providing free education through youtube. 
video link : https://lnkd.in/gsvt4ai6 ""education is the most powerful weapon you can use to change the world."" - nelson mandela.

let us make quality education affordable and accessible to all! #promotefreeeducation #learnohub #freehaiparbesthai learnohub …more meet superwoman roshni mukherjee | episode 39 https://www.youtube.com/ 105 18 comments like comment share copy linkedin facebook twitter sumita dhar vp business manager @hsbc | life coach | ex toi 2y report this comment i watched the show.. you are doing amazing work roshni mukherjee .. i wish and pray there are more teachers like you. you are truly an inspiration.. much much respect like reply 1 reaction abhijit ray co-founder & managing director at uc inclusive credit 2y report this comment you are a true inspiration roshni mukherjee madam.. my salutation to you, the amazing teacher who is making the concepts ""crystal clear"" i just went through one of your videos and would like to say that my concept was not as clear when i studied this during school. you make it so simple and easy to understand.
please keep up the great work and keep inspiring us all, especially the kids who look up to you. thanks and regards like reply 2 reactions 3 reactions shrutika patil chemistry lecturer, 2y report this comment congratulations mam very happy to see u with sandeep maheshwari sir inspiring lot of love & respect towards u mam like reply 1 reaction 2 reactions sachin sharma lead consultant  specialist at  hsbc 2y report this comment roshni mukherjee continue with the amazing work. so proud of you !!! like reply 1 reaction 2 reactions divyam priyadarshi bca | 1st year 2y report this comment you w supermam like reply 1 reaction 2 reactions nidhi raj bindra advocate/passionate for litigation(criminal/civil/ regulatory/ advisory)/corporate advisory 2y report this comment this is so inspiring. 🌸🌸 like reply 1 reaction 2 reactions ca niraj harodia co-founder jpnr# secretary, fts #14k+ followers#partner ,kasg &co. #independent director, the grob tea limited#trustee, mc kejriwal group of institution#faculty @icai, vice president, acae 2y report this comment oh...that's fantastic roshni mukherjee you really made us proud. happy to see you at this platform. like reply 1 reaction 2 reactions rohini jha entrepreneur || editor || author || certified tarot consultant || poet || awarded with editor’s choice sahitya award 2025 by story mirror 2y report this comment you will be remembered forever!! like reply 1 reaction 2 reactions apeksha srivastava data and analytics manager | aws certified | oracle certified | enterprise finance data transformation | automation | data visualization 2y report this comment so proud of you!! always inspiring! like reply 1 reaction 2 reactions see more comments to view or add a comment, sign in 2,338 followers 65 posts 1 article view profile connect more from this author examfear education is hiring! roshni mukherjee 8y explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language sign in to view more content create your free account or sign in to continue your search sign in welcome back email or phone password show forgot password? sign in or by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now or new to linkedin? join now by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy .",-1
https://www.linkedin.com/pulse/testing-relativistic-theories-gravity-nicolae-sfetcu,,,"testing the relativistic theories of gravity agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in testing the relativistic theories of gravity report this article nicolae sfetcu nicolae sfetcu researcher at crisft (academia romana), owner of multimedia publishing published mar 17, 2020 + follow clifford m. will describes, in theory and experiment in gravitational physics , [1] the emergence of a new era for general relativity, testing and checking at very high levels of accuracy. in 1959, scientists at lincoln laboratories in massachusetts bombarded the planet venus with radio waves from earth, hoping to detect the echo of reflected waves. they did not detect any echoes. on further analysis, they detected an echo on september 14, this being the first radar echo recorded on a planet. in 1960, astronomers thomas matthews and allan sandage and colleagues at mount palomar used a telescope to record the star field around the 3c48 radio source on a photo plate. they were expecting to find a group of galaxies, but at the exact location of the radio source an object was observed as a star but with an unusual spectrum and variable brightness with the frequency of 15 minutes [2] . this was the first observed quasar. [3] the pound-rebka experiment (1960) verified the principle of equivalence and gravity redshift and demonstrated the utility of quantum technology (atomic clocks, laser measurements, superconducting gravimeters, gravitational wave detectors) in high precision gravitational experiments. [4] radiations recorded from venus made the solar system a laboratory for testing relativistic gravity [5] . the interplanetary space program developed in the early 1960s, and the discovery in 1964 of the relativistic effect of delay [6] , offered new and accurate tests of general relativity. until 1974, the solar system was the only way for high accuracy tests of general relativity. in developing general relativity, einstein was led by theoretical criteria of elegance and simplicity. his theory initially encountered “three classic tests”: perihelion precession of mercury’s orbit, deflection of light by the sun, and gravitational redshift of light. at the end of the 1950s it was suggested that the gravitational redshift of light is not, however, a real test of general relativity. it is a pure consequence of the principle of equivalence and does not test the field equations of gravitational theory. schiff suggested that the eotvos experiment is more accurate than the gravitational redshift of light, which it replaced as importance, the eotvos experiment verifying to what extent the bodies of different composition have the same acceleration. [7] subsequently, other tests for general relativity were proposed, such as the lense-thirring effect, the orbital disturbance due to the rotation of a body, and the sitter effect, a secular movement of the perigee and the node of the orbit of the moon, [8] [9] but the perspectives for detecting them were still weak. [10] another test area for observing general relativity was cosmology, foretelling the primordial explosion called the “big bang” and the subsequent expansion of the unive, but by the end of the 1950s cosmological observations could not distinguish between different theories of gravity. [11] meanwhile, a “proliferation” of competing alternative gravity theories of general relativity has appeared. by 1960 there were at least 25 such alternative theories. [12] according to will, by 1960 general relativity was empirically supported by a moderate accuracy test (change of perihelion, about 1%), a low accuracy test (light distortion, about 50%), an inconclusive test (gravitational redshift) and cosmological observations that could not distinguish between different theories. this was what lakatos called the “stationary period”. due to its limited experimental confirmations, general relativity was even removed from basic physics. [13] the period from 1960 to 1980 was the period of maturity of general relativity: new high-precision test methods were developed that included new tests, such as gyroscopic precession, light delay and the “nordtvedt effect” in the moon motion, including astrophysical observations and artificial satellites. due to the proliferation of alternative theories, the best theoretical framework was needed to compare the checks of the different experiments, to classify the theories and to compare their predictions with the results of the experiments in a systematic way. yearexperimental or observational resultstheoretical results1960hughes-drever mass anisotropypound-rebka experiment of gravitational redshift penrose’s work on spinorsgyroscopic precision (schiff) brans-dicke theory 1962discovery of non-solar x-ray sourcesbondi formula for mass lossdiscovery of the quasar redshiftdiscovery of the kerr metric1964eotvos experiment, princetondelay in time of light (shapiro)pound-snider experiment of gravitational redshiftdiscovery of the 3k microwave background singularity theorems in general relativity1966detection of solar flatteningpulsar discovery production of elements in the big bang1968planetary radar measurements for time delaylaunch of mariners 6 and 7 acquisition of lunar laser echo first radio deflection measurements nordtvedt effect and early ppn framework1970cygxl: a black hole candidate mariners 6 and 7 time-delay measurements preferred frame effectsrefined ppn framework increasing the domain of black holes in general relativity 1972eotvos experiment, moscow1974discovery of binary pulsarsquantum evaporation of black holesdipolar gravitational radiation in alternative theories 1976experiments of gravitational ewsahift with rocketsmoon test of the nordtvedt effect time delay results obtained with mariner 9 and viking 1978measurements of decreasing orbital period of binary pulsar ss 4331980discovery of gravitational lenses table 2.2 a chronology of the tests for verifying the theory of general relativity between 1960-80. source: clifford m. will, theory and experiment in gravitational physics [14] robert dicke performed several high-precision nullity experiments to confirm gravity theories. [15] dicke concludes that gravitational experiments can be divided into two classes: one that tests the basis of gravity theory (eg, the principle of equivalence): the eotvos experiment, the hughes-drever experiment, the gravitational redshift experiment, etc.), verifying that gravity is a curved spacetime phenomenon (described by a “metric theory” of gravity). general relativity and brans-dick’s theory are examples of metric theories of gravity. a second class that tests the metric theories of gravity : the parameterized post-newtonian formalism, or ppn, initiated by kenneth nordtvedt, jr., [16] and expanded and improved by will. [17] ppn takes into account low velocities and weak fields (post-newtonian limit) of metric theories, based on a set of 10 real parameters. ppn was used to analyze the gravitational experiments of the solar system, to discover and analyze new tests of gravity theory, such as the nordtvedt effect, the preferred frame effects and the preferred location effects, and to analyze and classify alternative metric theories of gravity becoming the standard theoretical tool for these experiments, searches and studies. by the mid-1970s, many alternative theories of gravity were upheld by experiments at the solar system level, but not at the cosmological level. in 1974, joseph taylor and russell hulse discovered the binary pulsar, [18] whose extremely stable pulses were monitored radiotelescopically, allowing accurate measurement of astrophysical parameters. in 1978 the rate of change of the orbital period of the system was measured, which was confirmed by general relativity but not by most alternative theories. in the michelson-morley experiment, michelson started from an experiment to test fresnel and stokes’s contradictory theories about the influence of ether. [19] [20] stokes initially believed that the two theories are observationally equivalent, both theories explaining the aberration of light. michelson argued that his 1881 experiment was a crucial experiment that demonstrated stokes’ theory. lorentz pointed out that michelson “misinterpreted” the facts, and michelson’s calculations were wrong. michelson, along with morley, decided to repeat the experiment “at intervals of three months and thus avoid all uncertainty,” [21] their conclusion rejecting fresnel’s explanation. lorentz also questioned the new experiment: “the significance of the michelson-morley experiment lies rather in the fact that it can teach us something about the changes in the dimensions.” in 1897 michelson made a new experiment, concluding that the result of the experiment was an “improbable” one and decided that in 1887 he was wrong: stokes’ theory had to be rejected, and fresnel’s had to be accepted. fitzgerald, independent of lorentz, produced a testable version that was rejected by trouton, rayleigh and brace’s experiments as it was theoretically progressive, but not empirical, fitzgerald’s theory being considered ad-hoc (that there is no independent (positive) evidence for it). [22] einstein, ignoring these experiments, but stimulated by mach’s criticisms of newtonian mechanics, arrived at a new progressive search program, [23] which “predicted” and explained the result of the michelson-morley experiment, but also predicted a huge range of undiscovered facts previously, that have obtained dramatic corroborations. thus, only twenty-five years later, the michelson-morley experiment came to be seen as a crucial experiment, considered to be “the largest negative experiment in the history of science,” [24] [25] demonstrating lakatos’s methodological tolerance. in this context, a typical signal of the degeneration of a program is the proliferation of contradictory “facts”. using a false theory as an interpretive theory, one can obtain – without committing an “experimental error” – contradictory factual proposals, inconsistent experimental results. [26] michelson himself was frustrated by the inconsistency of “facts” resulting from his measurements. carlton morris caves proposes six possible laboratory experiments for non-newtonian gravity: three use a torsion balance as a detector, and three use a high-sensitivity dielectric crystal. [27] caves’ idea is to demonstrate that technology will soon make possible a new class of experiments, exclusively laboratory tests. caves’ conclusion is that none of these experiments would be easy to do, because of the limitations of current technology. but most are feasible in the near future. the strong effects of gravity are observed astrophysically (white dwarfs, neutron stars, black holes), in which case there are used, as experimental tests, the stability of the white dwarfs, the spin-down rate of the pulsars, the orbits of the binary pulsars, the existence of a black hole horizon, and so on. recently, a series of cosmological tests have been developed for theories of dark matter, using for constraints the rotation of the galaxy, the tully-fisher relation, the speed of rotation of dwarf galaxies, and gravitational lenses. for the theories related to cosmic inflation, the most rigorous test is by measuring the size of the waves in the spectrum of cosmic microwave background radiation. [28] for dark energy theories, the results of supernova brightness and age of the universe can be used as tests. there are large differences in predictions between general relativity and classical physics, such as gravitational time dilation, gravitational lensing, gravitational redshift of light, and so on. and there are many relativistic theories of gravity, bifurcated or independent, but einstein’s general theory of relativity has upheld all predictions and is the simplest of such theories. tests proposed by einstein einstein states, in relativity theory – special relativity and general relativity , [29] that theories evolve through observation-based decalations, in the form of empirical laws, from which general laws are obtained. intuition and deductive thinking play an important role in this process. after the initial stage, the investigator develops a thinking system guided by empirical data, logically constructed from fundamental assumptions (axioms). the “truth” of a theory results from its correlation with a large number of unique observations. for the same empirical data there may be several theories that differ. einstein speaks, in relativity theory – special relativity and general relativity , of the confirmed prediction of general relativity for the motion of mercury’s perihelion, with a precision far greater than that predicted by newton’s law of universal gravity. [30] another confirmed prediction discussed by einstein is the deflection of light by a gravitational field, which admits an experimental test by photographic recording of stars during a total solar eclipse, thus: stars in the vicinity of the sun are photographed during a solar eclipse. the second photo of the same stars is taken when the sun is in a different position on the sky, a few months earlier or later. by comparing the positions of the stars, there should appear radially outward. the british royal society and the royal astronomical society performed these tests on two expeditions, on sobral (brazil) and principe island (west africa), confirming the prediction. the redshift of the spectral lines was also predicted by the general relativity and discussed by einstein in the same book, but when this book was written it had not yet been confirmed. experiments were carried out on cyanogen bands, but the results were not conclusive during that period. einstein proposed a verification of the average displacement of the lines towards the less refractory edge of the spectrum, through statistical investigations of the fixed stars. in the second edition of the book relativity theory – special relativity and general relativity , [31] einstein states that in developing his theory for the “cosmological problem” he relied on two hypotheses: there is an average density of matter throughout the space, which is everywhere the same and different from zero. the size (“radius”) of the space is independent of time. the hypotheses proved to be in line with the general theory of relativity after the introduction of a hypothetical term in the field equations (“the cosmological term of the field equations”). subsequently, einstein came to the conclusion that one can keep the hypothesis (1) without appealing to that term, if one can renounce the hypothesis (2), respectively the initial equations of the field admit a solution in which the “radius of the world” depends on time ( space expansion), thus allowing space expansion. hubble, through an investigation of extra-galactic nebulae, confirmed that the emitted spectral lines showed a redshift proportional to the distance between the nebulae. for einstein, the epistemological approach of thought experiments was of particular importance. these experiments, by the way they were developed, offered a new understanding of the discussed phenomena. at sixteen, einstein imagined what would happen if a light beam is followed with the speed of light . [32] the experiment is more difficult than it seems at first sight. einstein was, at that time, searching for a “universal principle” that could lead to true knowledge. the experiment starts with the hypothetical situation of tracking a light wave at speed c. in this case of equal magnitude of speeds, the “surfer” will observe a “frozen” light wave, with light radiation as a static spatially oscillating electromagnetic field, and the properties of the wave would disappear. but this time-independent field does not exist, because it is not in line with maxwell’s theory. his conclusion would be that an observer can never reach the speed of light, the hypothesis being false by modus tollens in classical logic. einstein said that this experiment contains a paradox in that the two assumptions included (the constancy of the speed of light and the independence of the laws (so also the constancy of the speed of light) of the choice of the inertial system (the principle of special relativity) are “mutually incompatible (despite the fact that both, taken separately, are based on experience)”. in september 1905, einstein attempted to extend the principle of relativity to accelerated reference systems by introducing a new and powerful physical principle in 1907, the “principle of equivalence” (the laws of physics take the same form in a uniform system of accelerating coordinates as in a system which is at rest relative to a homogeneous gravitational field), with a very high heuristic value. [33] he argued this principle through the “ elevator thought experiment “, sometimes considered einstein’s most important thought experiment. einstein assumes an accelerated frame of reference with a constant acceleration in the x-direction, and a second frame at rest in a homogeneous gravitational field that gives all objects an acceleration in the same x-direction. observationally, there is no distinction between the two frames. all bodies are accelerated in the same gravitational field. thus, the principle of equivalence allows a homogeneous gravitational field to be replaced by a uniformly accelerated reference system. this hypothesis of the exact physical equivalence of the two frameworks has two important theoretical consequences: we cannot speak of an absolute acceleration of the reference system, and the equal fall of all bodies in a gravitational field. notes [1] clifford m. will, theory and experiment in gravitational physics, revised edition , revised edition (cambridge england ; new york, ny, usa: cambridge university press, 1993). [2] thomas a. matthews and allan r. sandage, “optical identification of 3c 48, 3c 196, and 3c 286 with stellar objects.,” the astrophysical journal 138 (july 1, 1963): 30–56, https://doi.org/10.1086/147615. [3] will, theory and experiment in gravitational physics, revised edition . [4] r. v. pound and g. a. rebka, “apparent weight of photons,” physical review letters 4, no. 7 (april 1, 1960): 337–41, https://doi.org/10.1103/physrevlett.4.337. [5] w. b. smith, “radar observations of venus, 1961 and 1959,” the astronomical journal 68 (february 1, 1963): 15–21, https://doi.org/10.1086/108904. [6] irwin i. shapiro, “fourth test of general relativity,” physical review letters 13, no. 26 (december 28, 1964): 789–91, https://doi.org/10.1103/physrevlett.13.789. [7] l. i. schiff, “on experimental tests of the general theory of relativity,” american journal of physics 28, no. 4 (april 1, 1960): 340–43, https://doi.org/10.1119/1.1935800. [8] josef lense and hans thirring, “über den einfluß der eigenrotation der zentralkörper auf die bewegung der planeten und monde nach der einsteinschen gravitationstheorie,” physikalische zeitschrift 19 (1918): 156–63, http://adsabs.harvard.edu/abs/1918phyz…19..156l. [9] w. de sitter, “on einstein’s theory of gravitation and its astronomical consequences. second paper,” monthly notices of the royal astronomical society 77 (december 1, 1916): 77, 155–84, https://doi.org/10.1093/mnras/77.2.155. [10] will, theory and experiment in gravitational physics, revised edition . [11] g. j. whitrow and g. e. morduch, “relativistic theories of gravitation : a comparative analysis with particular reference to astronomical tests,” vistas in astronomy 6 (1965): chap. 14, https://doi.org/10.1016/0083-6656(65)90002-4. [12] c. dewitt, experimental relativity, in relativity groups and topology. lectures delivered at les houches during the 1963 session of the summer school of theoretical physics , second printing edition (gordon & breach, 1965), 165–313. [13] will, theory and experiment in gravitational physics, revised edition . [14] will. [15] dewitt, experimental relativity, in relativity groups and topology. lectures delivered at les houches during the 1963 session of the summer school of theoretical physics , 165–313. [16] kenneth nordtvedt, “equivalence principle for massive bodies. ii. theory,” physical review 169, no. 5 (may 25, 1968): 1017–25, https://doi.org/10.1103/physrev.169.1017. [17] c. m. will, “theoretical frameworks for testing relativistic gravity. ii. parametrized post-newtonian hydrodynamics, and the nordtvedt effect.,” astrophys. j. 163: 611-28(1 feb 1971). , january 1, 1971, 163, 611–28, https://doi.org/10.1086/150804. [18] r. a. hulse and j. h. taylor, “discovery of a pulsar in a binary system,” the astrophysical journal letters 195 (january 1, 1975): l51-53, https://doi.org/10.1086/181708. [19] a fresnel, “lettre a francois arago sur l’influence du mouvement terrestre dans quelques phenomenes optiques, in annales de chimie et de physique – 98 years available – gallica,” 1818, https://gallica.bnf.fr/ark:/12148/cb343780820/date.r=annales+de+chimie+et+de+physique.langen. [20] george gabriel stokes, on fresnel’s theory of the aberration of light (london, 1846), 76–81. [21] hendrik a. lorentz, “considerations on gravitation,” in the genesis of general relativity , ed. michel janssen et al., boston studies in the philosophy of science (dordrecht: springer netherlands, 2007), 1038–52, https://doi.org/10.1007/978-1-4020-4000-9_13. [22] joseph larmor, on the ascertained absence of effects of motion through the aether, in relation to the constitution of matter, and on the fitzgerald-lorentz hypothesis , 1904, 624. [23] karl raimund popper, the logic of scientific discovery (psychology press, 2002). [24] j. d. bernal, science in history j. d. bernal , 3rd edition (m.i.t press, 1965). [25] imre lakatos, the methodology of scientific research programmes: volume 1: philosophical papers (cambridge university press, 1980). [26] lakatos. [27] carlton morris caves, “theoretical investigations of experimental gravitation” (phd, california institute of technology, 1979), http://resolver.caltech.edu/caltechthesis:03152016-161054898. [28] funcția potențial, care este crucială pentru determinarea dinamicii inflației, este pur și simplu postulată, și nu derivată dintr-o teorie fizică subiacentă. [29] albert einstein, teoria relativității: relativitatea specială și relativitatea generală (nicolae sfetcu, 2017), https://books.google.ro/books?id=amtndwaaqbaj. [30] einstein. [31] einstein. [32] albert einstein, “autobiographische skizze,” in helle zeit — dunkle zeit: in memoriam albert einstein , ed. carl seelig (wiesbaden: vieweg+teubner verlag, 1956), 9–17, https://doi.org/10.1007/978-3-322-84225-1_2. [33] abraham pais, subtle is the lord: the science and the life of albert einstein (oxford ; new york: oxford university press, 2005), 179–80. bibliography bernal, j. d. science in history j. d. bernal . 3rd edition. m.i.t press, 1965. caves, carlton morris. “theoretical investigations of experimental gravitation.” phd, california institute of technology, 1979. http://resolver.caltech.edu/caltechthesis:03152016-161054898. dewitt, c. experimental relativity, in relativity groups and topology. lectures delivered at les houches during the 1963 session of the summer school of theoretical physics . second printing edition. gordon & breach, 1965. einstein, albert. “autobiographische skizze.” in helle zeit — dunkle zeit: in memoriam albert einstein , edited by carl seelig, 9–17. wiesbaden: vieweg+teubner verlag, 1956. https://doi.org/10.1007/978-3-322-84225-1_2. ———. teoria relativității: relativitatea specială și relativitatea generală . nicolae sfetcu, 2017. https://books.google.ro/books?id=amtndwaaqbaj. fresnel, a. “lettre a francois arago sur l’influence du mouvement terrestre dans quelques phenomenes optiques, in annales de chimie et de physique – 98 years available – gallica,” 1818. https://gallica.bnf.fr/ark:/12148/cb343780820/date.r=annales+de+chimie+et+de+physique.langen. hulse, r. a., and j. h. taylor. “discovery of a pulsar in a binary system.” the astrophysical journal letters 195 (january 1, 1975): l51–53. https://doi.org/10.1086/181708. lakatos, imre. the methodology of scientific research programmes: volume 1: philosophical papers . cambridge university press, 1980. larmor, joseph. on the ascertained absence of effects of motion through the aether, in relation to the constitution of matter, and on the fitzgerald-lorentz hypothesis , 1904. lense, josef, and hans thirring. “über den einfluß der eigenrotation der zentralkörper auf die bewegung der planeten und monde nach der einsteinschen gravitationstheorie.” physikalische zeitschrift 19 (1918). http://adsabs.harvard.edu/abs/1918phyz…19..156l. lorentz, hendrik a. “considerations on gravitation.” in the genesis of general relativity , edited by michel janssen, john d. norton, jürgen renn, tilman sauer, and john stachel, 1038–52. boston studies in the philosophy of science. dordrecht: springer netherlands, 2007. https://doi.org/10.1007/978-1-4020-4000-9_13. matthews, thomas a., and allan r. sandage. “optical identification of 3c 48, 3c 196, and 3c 286 with stellar objects.” the astrophysical journal 138 (july 1, 1963): 30. https://doi.org/10.1086/147615. nordtvedt, kenneth. “equivalence principle for massive bodies. ii. theory.” physical review 169, no. 5 (may 25, 1968): 1017–25. https://doi.org/10.1103/physrev.169.1017. pais, abraham. subtle is the lord: the science and the life of albert einstein . oxford ; new york: oxford university press, 2005. popper, karl raimund. the logic of scientific discovery . psychology press, 2002. pound, r. v., and g. a. rebka. “apparent weight of photons.” physical review letters 4, no. 7 (april 1, 1960): 337–41. https://doi.org/10.1103/physrevlett.4.337. schiff, l. i. “on experimental tests of the general theory of relativity.” american journal of physics 28, no. 4 (april 1, 1960): 340–43. https://doi.org/10.1119/1.1935800. shapiro, irwin i. “fourth test of general relativity.” physical review letters 13, no. 26 (december 28, 1964): 789–91. https://doi.org/10.1103/physrevlett.13.789. sitter, w. de. “on einstein’s theory of gravitation and its astronomical consequences. second paper.” monthly notices of the royal astronomical society 77 (december 1, 1916): 155–84. https://doi.org/10.1093/mnras/77.2.155. smith, w. b. “radar observations of venus, 1961 and 1959.” the astronomical journal 68 (february 1, 1963): 15. https://doi.org/10.1086/108904. stokes, george gabriel. on fresnel’s theory of the aberration of light . london, 1846. whitrow, g. j., and g. e. morduch. “relativistic theories of gravitation : a comparative analysis with particular reference to astronomical tests.” vistas in astronomy 6 (1965): 1–67. https://doi.org/10.1016/0083-6656(65)90002-4. will, c. m. “theoretical frameworks for testing relativistic gravity. ii. parametrized post-newtonian hydrodynamics, and the nordtvedt effect.” astrophys. j. 163: 611-28(1 feb 1971). , january 1, 1971. https://doi.org/10.1086/150804. will, clifford m. theory and experiment in gravitational physics, revised edition . revised edition. cambridge england ; new york, ny, usa: cambridge university press, 1993. nicolae sfetcu email: nicolae@sfetcu.com this work is licensed under a creative commons attribution-noderivatives 4.0 international. to view a copy of this license, visit http://creativecommons.org/licenses/by-nd/4.0/ . sfetcu, nicolae, “testing the relativistic theories of gravity”, setthings (20 iunie 2019), url = https://www.setthings.com/en/testing-the-relativistic-theories-of-gravity/ like comment copy linkedin facebook twitter share to view or add a comment, sign in more articles by nicolae sfetcu blockchain narrative ontologies jun 19, 2025 blockchain narrative ontologies social ontology is concerned with the nature of the social world, constituents, or building blocks of social entities… 1 știință sau pseudoștiință? mar 15, 2025 știință sau pseudoștiință? introducere știința și pseudoștiința sunt doi termeni adesea folosiți pentru a descrie căutarea cunoașterii și… epistemology of intelligence analysis feb 25, 2025 epistemology of intelligence analysis in intelligence, epistemology is the study of the threat awareness and the way the threat is understood in the field of… criptomonede și criptosecurități – contracte inteligente feb 4, 2025 criptomonede și criptosecurități – contracte inteligente definirea criptomonedelor stabilirea unei definiții a criptomonedelor nu este o sarcină ușoară. la fel ca blockchain… argumentele funcțiilor php – transmiterea argumentelor prin referință jan 22, 2025 argumentele funcțiilor php – transmiterea argumentelor prin referință introducere php (hypertext preprocessor) este un limbaj de scripting folosit pe scară largă pe server, care permite… ontologia relativității generale jan 18, 2025 ontologia relativității generale în viziunea clasică, spațiul și timpul sunt containerele; materie este conținutul. proprietatea distinctivă a materiei… rezumarea automată în inteligența artificială prin învățare nesupravegheată: textrank dec 19, 2024 rezumarea automată în inteligența artificială prin învățare nesupravegheată: textrank rezumarea automată rezumarea automată (1) este procesul de sumarizare a unui document text cu un program de calculator… blockchain philosophy – bitcoin dec 12, 2024 blockchain philosophy – bitcoin blockchain philosophy donncha kavanagh and gianluca miscione introduced the concept of digital heterotopia (1) as a way… tableau software: vizualizarea și analiza datelor dec 1, 2024 tableau software: vizualizarea și analiza datelor introducere în prezent, datele sunt generate peste tot, cum ar fi youtube, tumblr, reddit, facebook, whatsapp, twitter,… 1 big data ethics in education and research nov 23, 2024 big data ethics in education and research ethical issues big data ethics involves adherence to the concepts of right and wrong behavior regarding data… 1 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/modern-tests-relativistic-gravitational-theories-nicolae-sfetcu,,,"modern tests of relativistic gravitational theories agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in modern tests of relativistic gravitational theories report this article nicolae sfetcu nicolae sfetcu researcher at crisft (academia romana), owner of multimedia publishing published mar 25, 2020 + follow dicke and schiff established a framework for testing general relativity, [1] including through null experiments and using the physics of space exploration, electronics and condensed matter, such as the pound-rebka experiment and laser interferometry. the gravitational lens tests and the temporal delay of light are highlighted by parameter γ of the ppn formalism, equal to 1 for general relativity and with different values in other theories. the bepicolombo mission aims to test the general theory of relativity by measuring the gamma and beta parameters of the ppn formalism. [2] shapiro delay the gravitational delay (shapiro delay), according to which the light signals require more time to pass through a gravitational field than in the absence of that field, has been successfully tested. [3] in the ppn formalism, the gravitational delay is highlighted by the parameter γ , which encodes the influence of gravity on the geometry of space. [4] irwin i. shapiro proposed this test becoming “classic”, predicting a relativistic delay in the return of radar signals reflected on other planets. the use of the planets mercury and venus as targets before and after they were eclipsed by the sun confirmed the theory of general relativity. [5] later the cassini probe was used for a similar experiment. [6] the measurement of the ppn gamma parameter is affected by the gravitomagnetic effect caused by the orbital motion of the sun around the barycentre of the solar system. the very long basic interferometry allowed the corrections of this effect in the field of movement of jupiter [7] and saturn. [8] gravitational dilation of time gravity influences the passage of time. processes close to a massive body are slower. [9] the gravitational redshift was measured in the laboratory [10] and using astronomical observations. [11] the gravitational dilation of the time in the gravitational field of the earth was measured using atomic clocks, [12] being verified as a side effect of the functioning of the global positioning system (gps). [13] tests in stronger gravitational fields need binary pulsars. [14] all the results are in accordance with general relativity, but also with other theories where the principle of equivalence is valid. [15] the gravitational dilation of time coexists with the existence of an accelerated frame of reference, except for the center of a concentric distribution of matter in which there is no accelerated frame of reference, although it is assumed that here time is dilated. [16] all physical phenomena undergo in this case the same time dilation, in accordance with the principle of equivalence. the time dilation can be measured for photons that are emitted on earth, curved near the sun, reflected on venus, and returned to earth along a similar path. it is observed that the speed of light in the vicinity of the sun is lower than c . the phenomenon was measured experimentally using atomic clocks on the plane, where time dilations occur also due to the differences of height less than 1 meter and were tested experimentally in the laboratory. [17] other test modes are through the pound-rebka experiment, observations of the white dwarf sirius b spectra, and experiments with time signals sent to and from mars soil with the viking 1. frame dragging and geodetic effect in general relativity, the apsides of the orbits (the point on the orbit of the body closest to the center of mass of the system) will have a precession, forming an orbit different from an ellipse, the shape of the rose. einstein predicted this move. relativistic precessions have been observed for all planets that allow accurate measurements of precession (mercury, venus and earth), [18] and in binary pulsar systems where it is larger by five orders of magnitude. a binary system that emits gravitational waves loses energy. thus, the distance between the two orbital bodies decreases, as does their orbital period. at the level of the solar system, the effect is difficult to observe. it is observable for a near binary pulsar, from which very precise frequency radio pulses are received, allowing measurements of the orbital period. neutron stars emit large amounts of energy in the form of gravitational radiation. the first observation of this effect is due to hulse and taylor, using a binary pulsar psr1913+16 discovered in 1974. this was the first, indirect, detection of gravitational waves. [19] the relativity of the direction has several relativistic effects, [20] such as the geodetic precession: the direction of the axis of a gyroscope in free fall in curved space will change compared to the direction of light received from distant stars. [21] for the moon-earth system, this effect was measured using the laser reflected on the moon, [22] and more recently with the help of the test masses on board the gravity probe b. [23] near a rotary table, there are gravitometric or frame dragging effects. in the case of rotating black holes, any object that enters the ergosphere rotates. the effect can be tested by its influence on the orientation of free fall gyros. [24] tests were performed using the lageos satellites, [25] with the mars global surveyor probe around mars, [26] confirming the relativistic prediction. the first frame dragging effect was derived in 1918 by josef lense and hans thirring and is known as the lense-thirring effect . they predicted that the rotation of a massive body would distort the spacetime metric, causing the orbit of a nearby test particle to precede. in order to detect it, it is necessary to examine a very massive body or to construct a very sensitive instrument. the linear dragging of the frames appears by applying the rg principle to the linear momentum. it is very difficult to verify. [27] increasing of the static mass is another effect, an increase in the inertia of a body when other masses are placed nearby. einstein states that it derives from the same equation of general relativity. it is a small effect, difficult to confirm experimentally. several costly proposals were made, including in 1976 by van patten and everitt, [28] for a special space mission to measure the lense-thirring precession of a pair of spacecrafts to be placed in earth’s polar orbits with non-dragging devices. in 1986 ciufolini proposed the launch of a passive geodesic satellite in an orbit identical to that of the lageos satellite. the tests were started using the lageos and lageos ii satellites in 1996. [29] the accuracy of the tests is controversial. neither did the gravity probe b experiment achieve the desired accuracy. [30] in the case of stars orbiting near a supermassive black hole, the frame dragging should cause the orbital plane of the star to precess around the axis of rotation of the black hole, an effect that could be detected in the following years by astrometric monitoring of the stars in the center of the milky way galaxy. [31] relativistic jets can provide evidence for frame dragging. [32] the gravitomagnetic model developed by reva kay williams predicts the high energy particles emitted by quasars and active galactic nuclei, the extraction of x and γ rays and e- e+ relativistic pairs, jets collimated around the polar axis, and asymmetric jets formation. testing of the principle of equivalence at the beginning of the 17th century galileo developed a principle similar to that of equivalence when he showed experimentally that the acceleration of a body due to gravity is independent of its mass quantity. kepler emphasized the principle of equivalence through a thought experiment, what would happen if the moon were stopped in orbit and dropped to earth. the principle of equivalence has historically played an important role in the law of gravity. newton considered it from the opening paragraph of the principia . einstein also relied on this principle in general relativity. newton’s principle of equivalence states that the “mass” of a body is proportional to its “weight” (the weak equivalence principle, wep). an alternative definition of wep is that the trajectory of a body in the absence of forces is independent of its internal structure and composition. a simple wep test is the comparison of the acceleration of two bodies of different composition in an external gravitational field. other high-precision experiments include from newton, bessel and potter’s pendulum experiments to the classical torsion measurements of eotvos, [33] dicke, [34] and braginsky. [35] there are several projects to improve the values measured with the help of satellites. the einstein equivalence principle (eep) is stronger and more comprehensive, stating that the wep is valid, and the results of local non-gravitational experiments are independent of the speeds of the appropriate reference frames and the place and time they are performed. the independence of the frame of reference is called local lorentz invariance, and independence of its internal structure and composition is called local position invariance. the special relativity benefited from a series of experiments that subsequently contributed to the acceptance of the gr: michelson-morley experiment and subsequent equivalent experiments, [36] ives-stillwell, rossi-hall, other tests of time dilation, [37] independence of the speed of light from the source speed, using x-ray binary stellar sources, and high energy pions, [38] isotropy of light speed. [39] in recent years, scientists have begun to look for apparent violations of the lorentz invariance resulting from certain quantum gravity models. a simple modality, embodied in the c2 formalism, assumes that the electromagnetic interactions suffer a slight violation of the lorentz invariance by changing the velocity of the electromagnetic radiation c relative to the limiting speed of the testing particle of particles, [40] trying to select a prefered universal resting frame, possible of cosmic background radiation. [41] through the michelson-morley experiments the speed of light is verified; the brillet-hall experiment [42] used a fabry-perot laser interferometer; in other experiments, the frequencies of the oscillators of the electromagnetic cavity in different orientations were compared with each other or with the atomic clocks, depending on the orientation of the laboratory. [43] the principle of local position invariance can be tested by the gravitational redshift experiments. the first such experiments were the pound-rebka-snider series from 1960 to 1965, which measured the frequency change of the gamma radiation photons. the most accurate standard redshift test was the vessot-levine rocket experiment in june 1976. [44] a “null” redshift experiment conducted in 1978 tested whether the relative rate of two different clocks depends on position. the most recent experiments have used laser cooling and trapping techniques to obtain extreme clock stability and compared the hyperfine transition rubidium-87, [45] the ionic quadrupole transition mercury-199, [46] the atomic transition with hydrogen 1s- 2s, [47] or an optical transition in ytterbium-171, [48] against hyperfine ground-state transition in cesium-133. [49] the einstein equivalence principle is part of the hard core of einstein’s research program, since the existence of eep implies gravity as a phenomenon in “curved spacetime”. it turns out that the only theories of gravity that can fully incorporate eep are those that satisfy the postulates of “metric theories of gravity”, respectively: [50] spacetime has a symmetrical value. the trajectories of free-falling bodies are geodesic of this metric. in the free-falling local reference frames, the non-gravitational laws of physics are those written in the language of special relativity. in 1960, schiff developed the hypothesis that any complete, self-consistent theory of gravity that embodies strong equivalence principle (sep) necessarily embodies eep (the validity of sep itself guarantees the validity of local lorentz and position invariance). in this case, it follows, based on the energy conservation hypothesis, that eotvos experiments are direct empirical bases for eep. the first successful attempt to prove schiff’s conjecture more formally was made by lightman and lee, [51] using a framework called “thεμ formalism” which includes all metric theories of gravity and many non-metric theories, which uses the rate of falling of a “tested” body consisting of interacting charged particles. empirical evidence supporting the einstein principle of equivalence states that the only theories of gravity that hope to be viable are metric theories, or possibly theories that are metric outside of very weak or short-lived non-metric couplings (as in string theory). [52] there may be other gravitational fields besides metric ones, such as scalar or vector fields, which mediate how matter and non-gravitational fields generate gravitational fields and produce the metric; but once the metric is determined, it only acts backwards in the manner prescribed by the eep. thus, all metric theories of gravity can be divided into two fundamental classes: “purely dynamic” and “previously geometric.” [53] in a “purely dynamic metric theory” the gravitational fields have the structure and evolution determined by the partially coupled differential field equations. a “previously geometric” theory contains “absolute elements”, fields or equations whose structure and evolution are given a priori and are independent of the structure and evolution of the other fields of theory. general relativity is a purely dynamic theory. the strong equivalence principle states that: wep is valid for all bodies, and the result of any local testing experiment is independent of the speed of the apparatus and the place and time of the experiment. compared to wep, sep includes gravitational sources (planets, stars) and experiments involving gravitational forces (cavendish experiments, gravimetric measurements). note that wep includes eep as a special case where local gravitational forces are ignored. if the wep is strictly valid, there must be only one gravitational field in the universe, the metric g , but there is no rigorous evidence of this statement so far. the einstein equivalence principle can be tested, in addition to wep tests, by looking for the variation of dimensionless constants and mass ratios. the strong equivalence principle implies that gravity is geometric by nature and does not contain additional associated fields. thus, sep says that a measurement of a flat space surface is absolutely equivalent to any other flat space surface in any other part of the universe. einstein’s theory of general relativity is the only theory of gravity that satisfies the strong equivalence principle. the sep can be tested by searching for a variation of newton’s gravitational constant g , or a variation of the mass of the fundamental particles. these would result from deviations from the law of gravitational force from general relativity, especially deviations from inverse-quadratic proportionality, which can be explained by the existence of the fifth force. other sought effects are the nordtvedt effect, a “polarization” of the orbits of the solar system due to the gravitational acceleration of self-generation at a rate different from the normal matter, sought by the lunar laser ranging experiment. other tests include studying the deflection of radiation from radio sources far from the sun measured with very long basic interferometry or measuring the change in frequency of signals to and from the cassini spacecraft. quantum gravity theories, such as string theory and loop quantum gravity, predict violations of the weak equivalence principle. currently, the tests of the weak equivalence principle have a degree of sensitivity so that the non-detection of an infringement is as profound as the discovery of an infringement. discovering the violation of the principle of equivalence would provide an important guide to unification. [54] a formalism of non-gravitational laws of physics in the presence of gravity that incorporates the possibility of nonmetric (nonuniversal) and metric coupling is the th formalism elaborated by lightman and lee. [55] it allows quantitative forecasting for experiment results. solar system tests the dynamic environment of spacetime around earth allows testing of gravitational theories, with geodetic satellites as test masses. an example is the lageos satellites, launched for geodetic and geodynamic purposes, and for fundamental physical studies. lageos satellites are used as a target for laser pulses sent from ground stations to calculate the instantaneous distance (“satellite laser ranging” (slr) technique). the determination of the orbit of the satellites requires models for the dynamics of the satellites, for the measurement procedures and for the transformations of the reference frames. [56] the models take into account geopotential, lunar and planetary disturbances, pressure of solar radiation and earth’s albedo, rubin-cam and yarkovsky-schach effects, slr station coordinates, ocean loading, earth orientation parameters and measurement procedure. [57] the models also include general relativistic corrections in the post-newtonian parametric formalism (ppn). [58] the tests performed confirm the general relativity predictions (schwarzschild precession, lense-thirring effect) and exclude an alternative theory (nlri/yukawa potential). notes [1] l. i. schiff, “on experimental tests of the general theory of relativity,” american journal of physics 28, no. 4 (april 1, 1960): 340–343, https://doi.org/10.1119/1.1935800. [2] c. brans and r. h. dicke, “mach’s principle and a relativistic theory of gravitation,” physical review 124, no. 3 (november 1, 1961): 925–935, https://doi.org/10.1103/physrev.124.925. [3] irwin i. shapiro, “fourth test of general relativity,” physical review letters 13, no. 26 (december 28, 1964): 789–791, https://doi.org/10.1103/physrevlett.13.789. [4] irwin i. shapiro et al., “fourth test of general relativity: new radar result,” physical review letters 26, no. 18 (may 3, 1971): 1132–1135, https://doi.org/10.1103/physrevlett.26.1132. [5] shapiro et al., 1132–1135. [6] sergei m. kopeikin and edward b. fomalont, “gravimagnetism, causality, and aberration of gravity in the gravitational light-ray deflection experiments,” general relativity and gravitation 39, no. 10 (october 1, 2007): 1583–1624, https://doi.org/10.1007/s10714-007-0483-6. [7] kopeikin and fomalont, 1583–1624. [8] ed fomalont et al., “recent vlba/vera/ivs tests of general relativity,” proceedings of the international astronomical union 5, no. s261 (april 2009): 291–295, https://doi.org/10.1017/s1743921309990536. [9] charles w. misner, kip s. thorne, and john archibald wheeler, gravitation (w. h. freeman, 1973). [10] r. v. pound and g. a. rebka, “apparent weight of photons,” physical review letters 4, no. 7 (april 1, 1960): 186, https://doi.org/10.1103/physrevlett.4.337. [11] m. a. barstow et al., “hubble space telescope spectroscopy of the balmer lines in sirius b,” monthly notices of the royal astronomical society 362, no. 4 (october 1, 2005): 1134–1142, https://doi.org/10.1111/j.1365-2966.2005.09359.x. [12] hans c. ohanian and remo ruffini, gravitation and spacetime (norton, 1994). [13] neil ashby, “relativity in the global positioning system,” living reviews in relativity 6, no. 1 (january 28, 2003): 1, https://doi.org/10.12942/lrr-2003-1. [14] michael kramer, “millisecond pulsarsas tools of fundamental physics,” in astrophysics, clocks and fundamental constants , ed. savely g. karshenboim and ekkehard peik, lecture notes in physics (berlin, heidelberg: springer berlin heidelberg, 2004), 33–54, https://doi.org/10.1007/978-3-540-40991-5_3. [15] ohanian and ruffini, gravitation and spacetime . [16] einstein derived these effects using the principle of equivalence as early as 1907, cf. albert einstein, “über das relativitätsprinzip und die aus demselben gezogene folgerungen, in volume 2: the swiss years: writings, 1900-1909 page 432 (468 of 692),” 1907, https://einsteinpapers.press.princeton.edu/vol2-doc/468. [17] pound and rebka, “apparent weight of photons,” 186. [18] ohanian and ruffini, gravitation and spacetime , 406–7. [19] r. a. hulse and j. h. taylor, “discovery of a pulsar in a binary system,” the astrophysical journal letters 195 (january 1, 1975): l51–l55, https://doi.org/10.1086/181708. [20] roger penrose, the road to reality: a complete guide to the laws of the universe , reprint edition (new york: vintage, 2007). [21] ohanian and ruffini, gravitation and spacetime , sec. 7.8. [22] kenneth nordtvedt, “lunar laser ranging – a comprehensive probe of post-newtonian gravity,” arxiv:gr-qc/0301024 , january 7, 2003, http://arxiv.org/abs/gr-qc/0301024. [23] c. w. f. everitt et al., “gravity probe b: final results of a space experiment to test general relativity,” physical review letters 106, no. 22 (may 31, 2011): 221101, https://doi.org/10.1103/physrevlett.106.221101. [24] ohanian and ruffini, gravitation and spacetime , sec. 4.7. [25] lorenzo iorio, “an assessment of the systematic uncertainty in present and future tests of the lense-thirring effect with satellite laser ranging,” space science reviews 148, no. 1–4 (december 2009): 363–381, https://doi.org/10.1007/s11214-008-9478-1. [26] lorenzo iorio, “on the lense-thirring test with the mars global surveyor in the gravitational field of mars,” open physics 8, no. 3 (january 1, 2010): 509–513, https://doi.org/10.2478/s11534-009-0117-6. [27] albert einstein, “the meaning of relativity,” princeton university press, 1921, https://press.princeton.edu/titles/484.html. [28] r. a. van patten and c. w. f. everitt, “possible experiment with two counter-orbiting drag-free satellites to obtain a new test of einstein’s general theory of relativity and improved measurements in geodesy,” physical review letters 36, no. 12 (march 22, 1976): 629–632, https://doi.org/10.1103/physrevlett.36.629. [29] iorio, “an assessment of the systematic uncertainty in present and future tests of the lense-thirring effect with satellite laser ranging,” 363–381. [30] everitt et al., “gravity probe b.” [31] ohanian and ruffini, gravitation and spacetime , sec. 7.8. [32] for a distant observer, jets sometimes seem to move faster than light, but this is an optical illusion that does not violate the principles of relativity. [33] roland v. eötvös, desiderius pekár, and eugen fekete, “beiträge zum gesetze der proportionalität von trägheit und gravität,” annalen der physik 373 (1922): 11–66, 68, https://doi.org/10.1002/andp.19223730903. [34] r. h. dicke, gravitation and the universe. , 1969, http://adsabs.harvard.edu/abs/1969grun.book…..d. [35] v. b. braginsky and v. i. panov, “verification of the equivalence of inertial and gravitational mass,” soviet journal of experimental and theoretical physics 34 (1972): 34, 463–466, https://istina.msu.ru/publications/article/4687588/. [36] a. brillet and j. l. hall, “improved laser test of the isotropy of space,” physical review letters 42 (february 1, 1979): 42, 549–552, https://doi.org/10.1103/physrevlett.42.549. [37] f. j. m. farley et al., “the anomalous magnetic moment of the negative muon,” il nuovo cimento a (1965-1970) 45, no. 1 (september 1, 1966): 45, 281–286, https://doi.org/10.1007/bf02738104. [38] t. alväger et al., “test of the second postulate of special relativity in the gev region,” physics letters 12, no. 3 (october 1, 1964): 12, 260–262, https://doi.org/10.1016/0031-9163(64)91095-9. [39] null krisher et al., “test of the isotropy of the one-way speed of light using hydrogen-maser frequency standards,” physical review. d, particles and fields 42, no. 2 (july 15, 1990): 42, 731–734. [40] clifford m. will, “the confrontation between general relativity and experiment,” living reviews in relativity 17, no. 1 (december 2014): 4, https://doi.org/10.12942/lrr-2014-4. [41] c. h. lineweaver et al., “the dipole observed in the cobe dmr 4 year data,” the astrophysical journal 470 (october 1, 1996): 470, 38–42, https://doi.org/10.1086/177846. [42] brillet and hall, “improved laser test of the isotropy of space,” 42, 549–552. [43] paul l. stanwix et al., “test of lorentz invariance in electrodynamics using rotating cryogenic sapphire microwave oscillators,” physical review letters 95, no. 4 (july 21, 2005): 040404, https://doi.org/10.1103/physrevlett.95.040404. [44] r. f. c. vessot et al., “test of relativistic gravitation with a space-borne hydrogen maser,” physical review letters 45, no. 26 (december 29, 1980): 45, 2081–2084, https://doi.org/10.1103/physrevlett.45.2081. [45] h. marion et al., “a search for variations of fundamental constants using atomic fountain clocks,” physical review letters 90, no. 15 (april 18, 2003): 90, 150801–1–4, https://doi.org/10.1103/physrevlett.90.150801. [46] s. bize et al., “testing the stability of fundamental constants with the 199hg+ single-ion optical clock,” physical review letters 90, no. 15 (april 18, 2003): 90, 150802–1–4, https://doi.org/10.1103/physrevlett.90.150802. [47] m. fischer et al., “new limits to the drift of fundamental constants from laboratory measurements,” physical review letters 92, no. 23 (june 10, 2004): 92, 230802–1–4, https://doi.org/10.1103/physrevlett.92.230802. [48] e. peik et al., “new limit on the present temporal variation of the fine structure constant,” physical review letters 93, no. 17 (october 18, 2004): 93, 170801–1–4, https://doi.org/10.1103/physrevlett.93.170801. [49] will, “the confrontation between general relativity and experiment.” [50] will. [51] a. p. lightman and d. l. lee, “restricted proof that the weak equivalence principle implies the einstein equivalence principle,” physical review. d, particles fields 8, no. 2 (1973): 8, 364–376, http://inis.iaea.org/search/search.aspx?orig_q=rn:5098997. [52] will, “the confrontation between general relativity and experiment.” [53] will. [54] james overduin et al., “the science case for step,” advances in space research 43, no. 10 (may 15, 2009): 1532–1537, https://doi.org/10.1016/j.asr.2009.02.012. [55] lightman and lee, “restricted proof that the weak equivalence principle implies the einstein equivalence principle,” 8, 364–76. [56] friedrich w. hehl et al., “general relativity with spin and torsion: foundations and prospects,” reviews of modern physics 48, no. 3 (july 1, 1976): 393– 416, https://doi.org/10.1103/revmodphys.48.393. [57] emil t. akhmedov et al., “experimental tests of quantum gravity and exotic quantum field theory effects,” researchgate, 2014, https://www.researchgate.net/publication/274948108_experimental_tests_of_quantum_gravity_and_exotic_quantum_field_theory_effects. [58] kenneth nordtvedt jr. and clifford m. will, “conservation laws and preferred frames in relativistic gravity. ii. experimental evidence to rule out preferred-frame theories of gravity,” the astrophysical journal 177 (november 1, 1972): 775–792, https://doi.org/10.1086/151755. bibliography akhmedov, emil t., stephen james minter, piero nicolini, and douglas singleton. “experimental tests of quantum gravity and exotic quantum field theory effects.” researchgate, 2014. https://www.researchgate.net/publication/274948108_experimental_tests_of_quantum_gravity_and_exotic_quantum_field_theory_effects. alväger, t., f. j. m. farley, j. kjellman, and l. wallin. “test of the second postulate of special relativity in the gev region.” physics letters 12, no. 3 (october 1, 1964): 260–62. https://doi.org/10.1016/0031-9163(64)91095-9. ashby, neil. “relativity in the global positioning system.” living reviews in relativity 6, no. 1 (january 28, 2003): 1. https://doi.org/10.12942/lrr-2003-1. barstow, m. a., howard e. bond, j. b. holberg, m. r. burleigh, i. hubeny, and d. koester. “hubble space telescope spectroscopy of the balmer lines in sirius b.” monthly notices of the royal astronomical society 362, no. 4 (october 1, 2005): 1134–42. https://doi.org/10.1111/j.1365-2966.2005.09359.x. bize, s., s. a. diddams, u. tanaka, c. e. tanner, w. h. oskay, r. e. drullinger, t. e. parker, et al. “testing the stability of fundamental constants with the 199hg+ single-ion optical clock.” physical review letters 90, no. 15 (april 18, 2003): 150802. https://doi.org/10.1103/physrevlett.90.150802. braginsky, v. b., and v. i. panov. “verification of the equivalence of inertial and gravitational mass.” soviet journal of experimental and theoretical physics 34 (1972): 463–66. https://istina.msu.ru/publications/article/4687588/. brans, c., and r. h. dicke. “mach’s principle and a relativistic theory of gravitation.” physical review 124, no. 3 (november 1, 1961): 925–35. https://doi.org/10.1103/physrev.124.925. brillet, a., and j. l. hall. “improved laser test of the isotropy of space.” physical review letters 42 (february 1, 1979): 549–52. https://doi.org/10.1103/physrevlett.42.549. dicke, r. h. gravitation and the universe. , 1969. http://adsabs.harvard.edu/abs/1969grun.book…..d. einstein, albert. “the meaning of relativity.” princeton university press, 1921. https://press.princeton.edu/titles/484.html. ———. “über das relativitätsprinzip und die aus demselben gezogene folgerungen, in volume 2: the swiss years: writings, 1900-1909 page 432 (468 of 692),” 1907. https://einsteinpapers.press.princeton.edu/vol2-doc/468. eötvös, roland v., desiderius pekár, and eugen fekete. “beiträge zum gesetze der proportionalität von trägheit und gravität.” annalen der physik 373 (1922): 11–66. https://doi.org/10.1002/andp.19223730903. everitt, c. w. f., d. b. debra, b. w. parkinson, j. p. turneaure, j. w. conklin, m. i. heifetz, g. m. keiser, et al. “gravity probe b: final results of a space experiment to test general relativity.” physical review letters 106, no. 22 (may 31, 2011): 221101. https://doi.org/10.1103/physrevlett.106.221101. farley, f. j. m., j. bailey, r. c. a. brown, m. giesch, h. jöstlein, s. van der meer, e. picasso, and m. tannenbaum. “the anomalous magnetic moment of the negative muon.” il nuovo cimento a (1965-1970) 45, no. 1 (september 1, 1966): 281–86. https://doi.org/10.1007/bf02738104. fischer, m., n. kolachevsky, m. zimmermann, r. holzwarth, th udem, t. w. haensch, m. abgrall, et al. “new limits to the drift of fundamental constants from laboratory measurements.” physical review letters 92, no. 23 (june 10, 2004): 230802. https://doi.org/10.1103/physrevlett.92.230802. fomalont, ed, sergei kopeikin, dayton jones, mareki honma, and oleg titov. “recent vlba/vera/ivs tests of general relativity.” proceedings of the international astronomical union 5, no. s261 (april 2009): 291–95. https://doi.org/10.1017/s1743921309990536. hehl, friedrich w., paul von der heyde, g. david kerlick, and james m. nester. “general relativity with spin and torsion: foundations and prospects.” reviews of modern physics 48, no. 3 (july 1, 1976): 393–416. https://doi.org/10.1103/revmodphys.48.393. hulse, r. a., and j. h. taylor. “discovery of a pulsar in a binary system.” the astrophysical journal letters 195 (january 1, 1975): l51–53. https://doi.org/10.1086/181708. iorio, lorenzo. “an assessment of the systematic uncertainty in present and future tests of the lense-thirring effect with satellite laser ranging.” space science reviews 148, no. 1–4 (december 2009): 363–81. https://doi.org/10.1007/s11214-008-9478-1. ———. “on the lense-thirring test with the mars global surveyor in the gravitational field of mars.” open physics 8, no. 3 (january 1, 2010). https://doi.org/10.2478/s11534-009-0117-6. kopeikin, sergei m., and edward b. fomalont. “gravimagnetism, causality, and aberration of gravity in the gravitational light-ray deflection experiments.” general relativity and gravitation 39, no. 10 (october 1, 2007): 1583–1624. https://doi.org/10.1007/s10714-007-0483-6. kramer, michael. “millisecond pulsarsas tools of fundamental physics.” in astrophysics, clocks and fundamental constants , edited by savely g. karshenboim and ekkehard peik, 33–54. lecture notes in physics. berlin, heidelberg: springer berlin heidelberg, 2004. https://doi.org/10.1007/978-3-540-40991-5_3. krisher, null, null maleki, null lutes, null primas, null logan, null anderson, and null will. “test of the isotropy of the one-way speed of light using hydrogen-maser frequency standards.” physical review. d, particles and fields 42, no. 2 (july 15, 1990): 731–34. lightman, a. p., and d. l. lee. “restricted proof that the weak equivalence principle implies the einstein equivalence principle.” physical review. d, particles fields 8, no. 2 (1973): 364–76. http://inis.iaea.org/search/search.aspx?orig_q=rn:5098997. lineweaver, c. h., l. tenorio, g. f. smoot, p. keegstra, a. j. banday, and p. lubin. “the dipole observed in the cobe dmr 4 year data.” the astrophysical journal 470 (october 1, 1996): 38. https://doi.org/10.1086/177846. marion, h., f. pereira dos santos, m. abgrall, s. zhang, y. sortais, s. bize, i. maksimovic, et al. “a search for variations of fundamental constants using atomic fountain clocks.” physical review letters 90, no. 15 (april 18, 2003): 150801. https://doi.org/10.1103/physrevlett.90.150801. misner, charles w., kip s. thorne, and john archibald wheeler. gravitation . w. h. freeman, 1973. nordtvedt, kenneth. “lunar laser ranging – a comprehensive probe of post-newtonian gravity.” arxiv:gr-qc/0301024 , january 7, 2003. http://arxiv.org/abs/gr-qc/0301024. nordtvedt, kenneth, jr., and clifford m. will. “conservation laws and preferred frames in relativistic gravity. ii. experimental evidence to rule out preferred-frame theories of gravity.” the astrophysical journal 177 (november 1, 1972): 775. https://doi.org/10.1086/151755. ohanian, hans c., and remo ruffini. gravitation and spacetime . norton, 1994. overduin, james, francis everitt, john mester, and paul worden. “the science case for step.” advances in space research 43, no. 10 (may 15, 2009): 1532–37. https://doi.org/10.1016/j.asr.2009.02.012. peik, e., b. lipphardt, h. schnatz, t. schneider, chr tamm, and s. g. karshenboim. “new limit on the present temporal variation of the fine structure constant.” physical review letters 93, no. 17 (october 18, 2004): 170801. https://doi.org/10.1103/physrevlett.93.170801. penrose, roger. the road to reality: a complete guide to the laws of the universe . reprint edition. new york: vintage, 2007. pound, r. v., and g. a. rebka. “apparent weight of photons.” physical review letters 4, no. 7 (april 1, 1960): 337–41. https://doi.org/10.1103/physrevlett.4.337. schiff, l. i. “on experimental tests of the general theory of relativity.” american journal of physics 28, no. 4 (april 1, 1960): 340–43. https://doi.org/10.1119/1.1935800. shapiro, irwin i. “fourth test of general relativity.” physical review letters 13, no. 26 (december 28, 1964): 789–91. https://doi.org/10.1103/physrevlett.13.789. shapiro, irwin i., michael e. ash, richard p. ingalls, william b. smith, donald b. campbell, rolf b. dyce, raymond f. jurgens, and gordon h. pettengill. “fourth test of general relativity: new radar result.” physical review letters 26, no. 18 (may 3, 1971): 1132–35. https://doi.org/10.1103/physrevlett.26.1132. stanwix, paul l., michael e. tobar, peter wolf, mohamad susli, clayton r. locke, eugene n. ivanov, john winterflood, and frank van kann. “test of lorentz invariance in electrodynamics using rotating cryogenic sapphire microwave oscillators.” physical review letters 95, no. 4 (july 21, 2005): 040404. https://doi.org/10.1103/physrevlett.95.040404. van patten, r. a., and c. w. f. everitt. “possible experiment with two counter-orbiting drag-free satellites to obtain a new test of einstein’s general theory of relativity and improved measurements in geodesy.” physical review letters 36, no. 12 (march 22, 1976): 629–32. https://doi.org/10.1103/physrevlett.36.629. vessot, r. f. c., m. w. levine, e. m. mattison, e. l. blomberg, t. e. hoffman, g. u. nystrom, b. f. farrel, et al. “test of relativistic gravitation with a space-borne hydrogen maser.” physical review letters 45, no. 26 (december 29, 1980): 2081–84. https://doi.org/10.1103/physrevlett.45.2081. will, clifford m. “the confrontation between general relativity and experiment.” living reviews in relativity 17, no. 1 (december 2014): 4. https://doi.org/10.12942/lrr-2014-4. nicolae sfetcu email: nicolae@sfetcu.com this work is licensed under a creative commons attribution-noderivatives 4.0 international. to view a copy of this license, visit http://creativecommons.org/licenses/by-nd/4.0/ . sfetcu, nicolae, ” modern tests of relativistic gravitational theories”, setthings (september 25, 2019), url = https://www.setthings.com/en/modern-tests-of-relativistic-gravitational-theories/ like comment copy linkedin facebook twitter share 4 2 comments mohammed alzahrani interested in research, monitoring, and investigation of everything related to the earth, the earth’s atmosphere, and the links with the universe, the hourglass 1y report this comment nice like reply 1 reaction 2 reactions see more comments to view or add a comment, sign in more articles by nicolae sfetcu blockchain narrative ontologies jun 19, 2025 blockchain narrative ontologies social ontology is concerned with the nature of the social world, constituents, or building blocks of social entities… 1 știință sau pseudoștiință? mar 15, 2025 știință sau pseudoștiință? introducere știința și pseudoștiința sunt doi termeni adesea folosiți pentru a descrie căutarea cunoașterii și… epistemology of intelligence analysis feb 25, 2025 epistemology of intelligence analysis in intelligence, epistemology is the study of the threat awareness and the way the threat is understood in the field of… criptomonede și criptosecurități – contracte inteligente feb 4, 2025 criptomonede și criptosecurități – contracte inteligente definirea criptomonedelor stabilirea unei definiții a criptomonedelor nu este o sarcină ușoară. la fel ca blockchain… argumentele funcțiilor php – transmiterea argumentelor prin referință jan 22, 2025 argumentele funcțiilor php – transmiterea argumentelor prin referință introducere php (hypertext preprocessor) este un limbaj de scripting folosit pe scară largă pe server, care permite… ontologia relativității generale jan 18, 2025 ontologia relativității generale în viziunea clasică, spațiul și timpul sunt containerele; materie este conținutul. proprietatea distinctivă a materiei… rezumarea automată în inteligența artificială prin învățare nesupravegheată: textrank dec 19, 2024 rezumarea automată în inteligența artificială prin învățare nesupravegheată: textrank rezumarea automată rezumarea automată (1) este procesul de sumarizare a unui document text cu un program de calculator… blockchain philosophy – bitcoin dec 12, 2024 blockchain philosophy – bitcoin blockchain philosophy donncha kavanagh and gianluca miscione introduced the concept of digital heterotopia (1) as a way… tableau software: vizualizarea și analiza datelor dec 1, 2024 tableau software: vizualizarea și analiza datelor introducere în prezent, datele sunt generate peste tot, cum ar fi youtube, tumblr, reddit, facebook, whatsapp, twitter,… 1 big data ethics in education and research nov 23, 2024 big data ethics in education and research ethical issues big data ethics involves adherence to the concepts of right and wrong behavior regarding data… 1 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/how-big-data-can-solve-blood-crisis-navin-chandra-nigam,,,"how big data can solve the blood crisis agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in how big data can solve the blood crisis report this article navin nigam navin nigam vice president - pmo & product operations @ company | driving program management excellence published jul 22, 2019 + follow as it is said that there is no greater joy than saving a soul and blood as saving agent is meant to circulate and saves lives. it was just 200 years ago authentic transfusion began to appear where transfusion used to carry out through silver tube without understanding of blood groups, making is hazardous in 35% of the cases. in the turn of the century, landstenier’s epic discovery of blood classification made risk of transfusion avoidable by grouping in a, b, o & ab. in 1915, sodium citrate was found as anticoagulant agent to prevent clotting which opened new ways to indirect transfusion and slowly became a commodity when hematologists started keeping it refrigerated to use later. the universal truth is that the blood cannot be manufactured, one must look up to another at the time of life-threatening situations since it must come only from a human donor. india with a total population of 1.3 billion and spread across 29 states and 7 territories, is struggling with shortage of blood and irony is millions of units are being wasted. one unit of blood can save at least 3 patients with plasma, platelets, and red blood cell yet there were around 2.8 million precious blood units wasted in last 5 years along with lifesaving components such as red blood cells, additives, platelets, and plasma. according to simple calculation considering one blood unit holds 450 ml of blood around 12,60,000 liter of blood – enough volume to fill 210 water tankers gone in vain. in 2016-17 alone, over 6.57 lakh units of blood was wasted. there are few states tops in chart of blood collection but also scores high in wastages. india needs 3 million units a year more than current stock, to handle 1200 road crashes, 60 million trauma surgeries, 10 million risk-based pregnancies, 230 million major operations, and 331 chemotherapy and surgeries – a few high-level statistics. the indian states and uts paints picture very differently. on one side states like maharashtra, punjab, kerala tops the chart with 35% excess supply of blood while on the other uttar pradesh, arunanchal pradesh, meghalaya and chhattisgarh are under constant pressure of under supply of 50% of the blood. shelf life of most components of blood is 35 days make it difficult to over stock and finally excess blood units get wasted. the modern health system can not be visualized without blood transfusion and this is government responsibility to keep the adequate supply of blood and impose all safety norms to keep the community healthy and safe from any disease arising out from blood transfusion.  indian government has wisely implemented hemovigilance in 2012, comprises a battery of procedures to keep the surveillance covering all possible steps of blood transfusion. still not to forget “to err is human”. it’s a highly demanding mission to keep the supply up against the demand from various sources. this calls for a holistic, comprehensive approach embracing all the stakeholders in planning, distribution, with objective to keep the blood safe and community healthy. it disheartens to come across news as recent as 15 expectant moms died due to lysed blood transfusion in southern part of india in april 2019. a preliminary enquiry suggests the staleness as a reason behind the bad quality of blood. there are several other stories where patients tested hiv positive post blood transfusion. several investigations followed, and various reasons were tracked down to result of poor combination of inventory management & decision making such as, screened and unscreened blood units were kept together, or recommended level of temperature of 20-60 was not maintained yet doctors certified the blood safe, etc. an rti filed by an activist in 2016, reveals a total of 2234 indians got infected by hiv because of blood transfusion in a duration of 17 months. these patients were just needed blood to get better. as a matter of fact, the blood transfusion has been a source of various diseases across the world. potentially, innumerable agents can be transmitted during the blood transfusion such as viruses, parasites, and bacteria. of these hiv, hepatitis and syphilis are common as communicable disease. interestingly, what makes identification of hiv agents more difficult is the window period for hiv patients at start of their disease who go unidentified despite being tested for hiv at the initial stage. in the modern age of digitization, digital information open paths to many solutions which was out of proximity till the recent past. digital health solutions can be looked in to find the solution for better patient blood management. assimilation of digitized data from various sources at the centralized location can be mined for splendid researches and findings for betterment of population health. big data as a concept can be immensely helpful if analyzed in retrospective way to formulate new strategies and defining transfusion policy better. it’s time to recognize the top challenges and create a nationwide plan to handle. optimizing costly resource sometimes there are easy solutions to difficult problems. according to who, scarcity of blood can be met if only 5 percent of population donate annually and if only 3 percent of population is a regular blood donor of 1.5 to 2 units in a year, all needs can be met. so how big data can help here – certainly it does. data collection in different region or geography can be analyzed to find appropriate demand with indication of rise, fall patterns in previous years. if this can be drilled down further, demand of blood groups and its components can be assessed which can certainly be fulfilled with meticulously planned way of managing the inventory. there is another way to handle demand is to calculate the number of acute beds in the region and 6.7 blood units per bed per year and when this is aligned to the region and analyzed in retrospective manner, a futuristic demand can be set for supply. various blood banks in the region can be aligned for collaboration and timely supply before blood goes stale. a logistic supply chain can be worked out from high collection areas to low areas within definite period before units get stale. compliance, benchmark and best practices agree, this is formidable to collect the data from every nook and corner of the huge country with a population of 1.3 billion, yet it is not insuperable if we start early and financial constraints are removed gradually. the implementation of best practices in transfusion medicine needs close monitoring, benchmarking and setting key performance indicators for all blood centers covering all private or public facilities. guidelines based comparative data among blood banks in a region can infuse a healthy competition to process adherence and monitoring better outcomes evaluation – and outcome-based incentives attached can do wonders to keep the blood and patients safe. the collected data points can be utilized to advise blood banks and hospitals on various performance indicators such as patient satisfaction, optimum transfusion, patient safety, and cost savings. interesting fact is that the basic elements of blood are same, yet all blood are not same. if a donor and a recipient share the same ethnicity the chances of adverse reaction are reduced substantially; for an example 98% of sickle cell patients are from african-american descent have their best hopes on donor of same origin. collection, pattern and transfusion there are several important and integral segments of blood transfusion, and donor is very important part of the chain. screening a donor is first step towards safe recipient keeping away from communicable disease. an unscreened infected donor can possibly infect at least 3 patients through single donation. the chances increase in case of paid donation, though a ban on paid donors enacted in 1995 has reduced unsafe blood donations yet identification and marking donors with infected diseases is still a challenge. a national database of donors with critical illnesses can be maintained and linked to identification such as aadhar or pan. the donor should be required to identify themselves before donating blood. an implementation of identification bound donation process can make a lot of difference in safe blood transfusion and spurn preventable infections. big data analytics can help to identify prevalent areas of probable infected blood and preventive action can be ordained. there is a dire need to have a centralized databased of blood transfusion related information such as transfusion patterns in different indian geographies covering different scenarios such as during surgeries, trauma care, health condition which can help in optimal utilization of blood as its vital to collect statistics across the nation to know the patters of blood transfusion in india. the pattern can be collected from different regions such as district level while moving gradually towards state level. with the help of digital medium such as bi (business intelligence), the pattern of transfusion can be analyzed across various locations in india to ensure adherence to guidelines and optimal use of scare resource. optimizing transfusion according to an article from healthcare informatics , an unnecessary transfusion can “diminish quality of care, increasing the likelihood of allergic reactions, fever, lung injury, immune suppression, iron overload and other adverse events. the unwanted supply of blood can be restricted using data analytics by collecting patients’ records from past several years and grouping under similar disease, risk factors, medical conditions, age, geographies etc. a decision support system for health providers can certainly help them to compare with peer groups with facilities’ benchmarks making better informed decisions. as a matter of fact, most similar surgeries do not require similar blood requirements as it mostly depends on age, history, blood condition such as sickle cell or anemia, risk factor etc. decision support system driven on bi analytics can restricts the user to place more than required unit per patient at a time, alerts them or make a provision to override the system generated alerts which can be analyzed later for better decision making. conclusion the right use of data analytics with definite objectives to focus on safety, patient satisfaction, cost optimization, and utilization can bring tremendous change around blood transfusion. this is a right time to recognize a need of national level data repository to capture specific and selected information to generate meaningful information at various level to create a decision support system and maintain an optimum level of supply across the nation. like comment copy linkedin facebook twitter share 16 3 comments abhishek joshi vice president | quantitative analytics |
seasoned analytics professional with 12+
 years of experience | stress testing | ccar modelling | eba irrbb | barclays | ex uhg | ex accenture management consulting 5y report this comment very well written, just a thought here
how about creating an overall incentive based  framework to attract donor population and donor with lesser available blood group will be rewarded the most.
in the framework as you correctly mentioned that logistics and supply chain will be required together with big data, how about ""integrating classical concept of supply chain jit (just in time) with big data"".
i mean to say having information available for donors from across city when ever there is a need of blood, person from the know group of donors having similar blood group available at the nearest location basis geographical location can be called for blood donation and rewarded. 
this way a good amount of cold supply chain cost will be removed from the system ( cold supply chain cost is very huge) and will be cost effective. like reply 1 reaction 2 reactions sasmita patra talent acquisition specialist (apac) @publicis resources | quality assurance manager - ex iqvia, ltimindtree | istqb | icp- certified agile coach | safe agilist | data science | ml-dl | qa automation | 5y report this comment insightful ! like reply 1 reaction 2 reactions see more comments to view or add a comment, sign in more articles by navin nigam learning from japan: ""a lesson in resilience, discipline, and dignity"" jan 10, 2024 learning from japan: ""a lesson in resilience, discipline, and dignity"" in the face of adversity, the strength of a nation is often revealed through its people's resilience and their ability… 15 2 comments music as a medicine jul 6, 2023 music as a medicine in the serene halls of dr. bob's kemp hospice in hamilton, ont. 13 1 comment dell cloud solution for indian healthcare oct 30, 2014 dell cloud solution for indian healthcare one of my solutions, where we are targeting hospital chains with 50-100 beds that will use it process right from the… 2 indian healthcare reforms– a much needed prescription sep 18, 2014 indian healthcare reforms– a much needed prescription indian healthcare reforms– a much needed prescription abstract the first major change in indian healthcare system… 2 sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/did-diesel-testing-scandal-affect-volkswagen-car-sales-wojtowycz,,,"did the diesel testing scandal affect volkswagen car sales in the netherlands? agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in did the diesel testing scandal affect volkswagen car sales in the netherlands? report this article colin wojtowycz colin wojtowycz supporting researchers to visualise data insights. published may 16, 2017 + follow https://datawoj.co.uk/2017/05/16/did-the-diesel-testing-scandal-affect-volkswagen-car-sales-in-the-netherlands/ via @wordpressdotcom like comment copy linkedin facebook twitter share 3 to view or add a comment, sign in more articles by colin wojtowycz lessons learnt visualising big data apr 27, 2017 lessons learnt visualising big data 2 visualising the top 500 youtube gaming channels mar 14, 2017 visualising the top 500 youtube gaming channels each week i take part in a data visualisation challenge called 'makeovermonday'. the idea is to take a data… 4 sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/do-you-want-peace-himanshu-agrawal,,,"do you want peace ? agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in https://www.artstation.com do you want peace ? report this article himanshu agrawal himanshu agrawal engineering leader | solution architect | cloud applications, scalable architecture | mvps | saas published jul 25, 2017 + follow the question here seems little off as everyone in the world want to live with peace, but it is also evident that conflicts occurs in life and these can only be conquered if you are ready to deal with those conflicts. during the current stand-off between india and china over a land, situation is being tensed heavily on the ground and there are speculation of war also between the two most populous countries, where roughly 36% of world population live. though china is 5 times bigger economy than india and having huge military advantages over india in terms of numbers, still indian army showcased themselves as war ready by not trapping into chinese pressure and denied to pull back from the controversial region, despite of the major threats being issued by them. simultaneously, the diplomatic tactics being adopted by indian government like maintaining good relationship with israel, united states and japan helped chinese army to be in their own bay. the joint naval exercise between india, japan and united states also sent a concerning signal to chinese authorities to think before attacking their neighbor country. the strategies being taken place seems also quite relevant in the context of it industry and businesses, while it industry is getting disrupted by recent layoffs, new visa norms and technologies transformations into businesses. as technologies are rapidly transforming from traditional model to big data, machine learning, iot, cloud technologies, chatbots, blockchain and automation, it is essential for the larger it workforce need to be trained or upgraded their skill sets as per their relevant market or business needs. below are few samples where technological shift is taking place: instead of project management, product management has taken the pace for quite some time. automation testing is quite eating the space of manual testing. instead of using licensing based language solutions, open source solutions are dominant in market and so businesses are more inclined towards using more open source solutions. with the advent of cloud technologies, all major technology oriented businesses are migrating their systems into cloud instead of on premise hosting environment. as huge digital content is being generated and is exponentially increasing from various sources like social media, sensors and ecommerce activities etc., so there is a huge demand of yielding the value from this random data, so lots of job opportunities will be created in big data, analysis and data science space. while businesses and it industries are in major transformation phase of technologies, one need to keep the weapons of big data, cloud, automation or any other relevant technologies into his/her arsenal as per the roles and responsibilities for making themselves relevant into this war of technological transformation. so if you are also one of them, who is looking for growth in career or worried from the recent technological or business transformations and layoffs, so instead of being felt depressed or getting scared, it is high time to get yourself equipped with the latest technology and product management skills, because there is only one golden rule for these situations: “ the only way to avoid a war is to be ready for war.” like comment copy linkedin facebook twitter share 4 1 comment janmejay tripathi infra specialist 7y report this comment well said  and very good analysis on current scenario👍🏻 like reply 1 reaction 2 reactions to view or add a comment, sign in more articles by himanshu agrawal how to select architecture style for building scalable product ? oct 24, 2020 how to select architecture style for building scalable product ? in order to build scalable web applications with time to market considerations, the first and foremost important… 5 2 comments how to analyze big data with hadoop technologies feb 28, 2017 how to analyze big data with hadoop technologies with the rapid innovations, frequent evolution's of technologies and growing internet population, systems and… 8 4 comments is your company braced up for handling big data? dec 7, 2014 is your company braced up for handling big data? “big data is like teenage sex: everybody wants to talk about it but very few really know how to do it. everyone thinks… 11 5 comments sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/new-obesity-management-focused-health-weight-achim-regenauer,,,"new obesity management focused on health not weight. agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in soure: may 24, 2016 in endocrine practice new obesity management focused on health not weight. report this article achim regenauer achim regenauer medical executive reinsurance, chief medical officer published may 31, 2016 + follow new evidence-based obesity-management guidelines from the american association of clinical endocrinologists (aace) recommend approaching obesity as a chronic medical condition, with the focus firmly on improving patient health rather than weight loss per se. more details with a colour coded workflow on how to treat obesity, ""taking reality into account"" can be found under... http://journals.aace.com/doi/suppl/10.4158/ep161365.gl/suppl_file/10.4158_ep161365gl-algorithm.pdf like comment copy linkedin facebook twitter share to view or add a comment, sign in more articles by achim regenauer diabetes type 1,2,3,4,5 and the path towards a web based tool to determine the prognosis may 29, 2018 diabetes type 1,2,3,4,5 and the path towards a web based tool to determine the prognosis diabetes is presently classified into two main forms, type 1 (t1d) and type 2 diabetes (t2d), but especially t2d is… 16 breast cancer screening - is mri (magnetic resonance imaging) more efficient than traditional mammography? feb 14, 2018 breast cancer screening - is mri (magnetic resonance imaging) more efficient than traditional mammography? due to the high resolution capabilites by mri together with the good visualisation of soft tissues, mri seems to be an… 6 1 comment artificial intelligence in medicine - where are we right now? jan 5, 2018 artificial intelligence in medicine - where are we right now? much has been published about ai and the fast moving steps in medicine - just to mention ibm watson. but what is… 9 3 comments artificial intelligence in cancer care - the lofty promises by ibm watson supercomputer – hype or hope? where are we right now? sep 7, 2017 artificial intelligence in cancer care - the lofty promises by ibm watson supercomputer – hype or hope? where are we right now? we often read overly optimistic news coverage of medical advances such as stem cells, personalized medicine, crispr… 7 4 comments we know iphones, ibooks but what`s about ielectrocardiograms (ecg) ? sep 2, 2017 we know iphones, ibooks but what`s about ielectrocardiograms (ecg) ? here is a first evidence about handheld ecg wearable as a screening tool for patients at risk. there are quite a lot of… 7 3 comments lab test for alzheimer's disease is coming closer jul 23, 2017 lab test for alzheimer's disease is coming closer news from the current alzheimer's association international conference (aaic) 2017: a blood test that correlates well… 12 increasing us stroke incidences and churchills quote: ""there are  3 kinds of lies: lies, damned lies, and statistics?"" may 11, 2017 increasing us stroke incidences and churchills quote: ""there are  3 kinds of lies: lies, damned lies, and statistics?"" yes, you are right - a somewhat provoking title. but the longer i doing research on longevity issues the more sceptical… 9 3 comments parkinson's disease new trends and new hope?! apr 15, 2017 parkinson's disease new trends and new hope?! 200 years after its discovery, it's still incurable, there is no objective test or biomarker to predict, prevent or… 8 1 comment big data in medicine - rather big noise? mar 15, 2017 big data in medicine - rather big noise? it is true, that we are about to enter into new dimensions with big data, but is also true, that many are mesmerized by… 10 1 comment not utopic: from obesity to diabetes to alzheimer's dementia?! feb 26, 2017 not utopic: from obesity to diabetes to alzheimer's dementia?! a new molecular study reveals for the first time that high blood sugar or glucose damages an important enzyme that is… 6 2 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/combinational-test-technique-orthogonal-array-testing-yerraguntla,,Integration Testing,"combinational test technique – orthogonal array testing technique (oats) agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in combinational test technique – orthogonal array testing technique (oats) report this article yugesh yerraguntla yugesh yerraguntla banking | product management & strategy | digital transformation published aug 23, 2016 + follow headline combinational test technique, as the name suggests, is a technique of combining the data / entities as input parameters for testing, to increase the scope. this technique is beneficial when we have to test with huge number data having many permutations and combinations. the beauty of this technique is that, it maximizes the coverage by comparatively lesser number of test cases. there are many technique of ctd, where oats (orthogonal array testing technique) is widely used. orthogonal array testing characteristics: oat, is a systematic and statistical approach to pairwise interactions. executing a well-defined and a precise test is likely to un-cover most of the defects. 100% orthogonal ar-ray testing implies 100% pairwise testing. using oats: orthogonal array testing strategy (oats) is a proven technique, especially for integration testing of software components. oats can be used to reduce the number of combinations and provide maximum coverage with a minimum number of test cases. endlessly executing tests take too much effort to find defects and does not increase the confidence in the system. executing a concise, well-defined set of tests that uncovers most of the defects is a wise approach and a cost saving technique. oats techniques create an efficient and concise test sets with fewer test cases than testing all combinations of all variables. you can create a test set that has an even distribution of all pair-wise combinations. implementing oats technique involves the below steps: identify the independent variables. these will be referred to as “factors”. identify the values which each variable will take. these will be referred as “levels” search for an orthogonal array that has all the factors from step 1 and all the levels from step 2 map the factors and levels with your requirement translate them into the suitable test cases look out for the left over or special test cases (if any) example: let us consider you have to identify the test cases for a web page that has 4 sections: headlines, details, references and comments, that can be displayed or not displayed or show error message. you are required to design the test condition to test the interaction between different sections. in this case: 1. number of independent variables (factors) are = 4 2. value that each variable can take = 3values( displayed, not displayed and error message) 3. orthogonal array would be 34. 4. google and find an appropriate array for 4 factors and 3 levels. for this example, i am referencing the below table 5. now,  map this array with our requirements as below: 1 will represent “is displayed” value 2 will represent “not displayed” value 3 will represent “error message value” factor a will represent “ headlines” section factor b will represent “details” section factor c will represent “references ”section factor d will represent “comment” section. experiment no will represent “test cases #” 6. after mapping, the table will look like: 7. based on the table above, design your test cases. also look out for the special test cases / left over test cases. benefits of ‘oats’: an orthogonal array is a type of experiment where the columns for the independent variables are independent to one another. large savings in test effort. analysis is easy  [ready made tools and tables are available to describe the combination ]. test design using oats tools help in those test cases where we choose to make all the difference between: endlessly executing tests take too much effort to find bugs and do not increase confidence in the system. executing a concise, well-defined set of tests that uncover most of the defects will remove the inefficiencies and increase confidence in the system. conclusion: none of the testing technique provides a guarantee of 100% coverage. each technique has its own way of selecting the test conditions. in the similar lines, there are some limitations of using this technique: testing will fail if we fail to identify the good pairs. probability of not identifying the most important combination which can result in losing a defect. this technique will fail if we do not know the interactions between the pairs. applying only this technique will not ensure the complete coverage. it can find only those defects which arise due to pairs, as input parameters. references: 1. b. beizer. “software testing techniques.” van nostrand reinhold, 2nd edition, 1990 2. shishank gupta, “parametric test optimization”, software testing conference, 2002. like comment copy linkedin facebook twitter share 7 5 comments prashanth tv sr architect 7y report this comment hi yugesh, exellent article....can you please let me know how did you come up with the no. of experiments as ""9"" and also the matrix. is there any technique as such. thanks!!! like reply 1 reaction vimal chatrati sdlc governance lead | quality & testing 8y report this comment this is really very effective technique & happy to inform i was part  of cfna project where we implemented this. like reply 1 reaction 2 reactions prasad revur, phd career & life coach | executive coaching | leadership development | strategic consulting 8y report this comment did you try this any where? in one of the projects when i was in tech m, we worked on a project simultaneously to prove the effectiveness of this methodology. it is indeed a wonderful technique, you may use particularly in mobile application  testing. like reply 1 reaction 2 reactions satya vamsi krishna m. lead software engineering | gen ai | api development | llms | agentic ai | python | master's in computer science 8y report this comment good technique to improve test design. helps to standardize test cases and test management. like reply 1 reaction see more comments to view or add a comment, sign in more articles by yugesh yerraguntla can virtual reality boost user experience in e-commerce? aug 15, 2017 can virtual reality boost user experience in e-commerce? enhancing customer experience with virtual reality: e-commerce penetration: internet diffusion, mobile usage… 10 why india needs bitcoin? jan 28, 2017 why india needs bitcoin? india has recently witnessed a historical cash crunch in the form of demonetization; people of the country were… 12 4 comments advent of '#social media' in banking & finance jun 23, 2016 advent of '#social media' in banking & finance 5 key areas & 5 benefits with the advent of internet and mobile channels, banks have been making efforts to streamline… 8 2 comments actionable intelligence: next level of big data analysis may 14, 2016 actionable intelligence: next level of big data analysis actionable intelligence means capturing of data from different sources and taking out critical insights for… 6 2 comments #4 safeguarding iot dec 19, 2015 #4 safeguarding iot being secure, resilient and vigilant. a worth-taking element of the internet of things (iot) is that objects are not… #3 cognitive era of analytics in iot dec 9, 2015 #3 cognitive era of analytics in iot people are cognizant about the impact of data analytics on day to day world of networking and communications. in this… #2 cloud in iot and ioe oct 1, 2015 #2 cloud in iot and ioe it is a well-known fact that the internet of everything is taking shape all around us. people, processes, data, and… 1 #1 influx of iot sep 26, 2015 #1 influx of iot in the good olden days, nomads used to communicate with sense less objects and wander across anticipating a response… dev-ops - agile system administration sep 12, 2015 dev-ops - agile system administration development and operations clipped together 'devops' is steadily achieving its importance and surely will conquer and… 4 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://www.linkedin.com/pulse/excess-pressure-formation-tester-angel-g-guzman-garcia,,,"excess pressure (formation tester) agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in excess pressure (formation tester) report this article a. g. guzman a. g. guzman 🏃🏻 purveyor of excellence 🚴
avid jogger & cyclist published mar 1, 2016 + follow the excess-pressure plot is an alternative pressure vs. depth technique that enhances some features not always obvious in the conventional pressure vs. depth plots. the technique basically removes the effect of a chosen fluid density to improve the visualization of subtle fluid-density differences or the presence of pressure barriers. good data quality is still essential. for this summary, i have assumed that the user has taken the time to inspect not only the report normally obtained from the service company, but also has looked closely at the pressure-history & rate-history plots acquired with a formation tester. for the practitioner who spends time analyzing pressure data, this is a useful technique to add to your tool kit. a single static fluid having constant density and free communication with itself has the same excess pressure at all elevations. hence, all fluid of the same density will plot on a straight line. (alton brown, improved interpretation of wireline pressure data , aapg bulletin, v. 87, no. 2, february 2003, pp. 295–311.) if the excess-pressure vs. tvdss trend is rotated clockwise from vertical, the fluid density is too high. a lower density value should be used, and excess pressure recomputed. alternatively, if the excess-pressure vs. tvdss trend is rotated counterclockwise from vertical, a higher fluid density should be used to recalculate excess pressure . the assumed density is iterated until excess-pressure variance is minimized and the trend is vertical. figure 1 shows the behavior for the same data set, when selecting the water, oil, or gas density. a black line was added to indicate which density was used. nonetheless, this should be obvious by the data points that plot on a vertical line. since water is the dominant phase in this particular example, water should be the phase used to compute excess pressure. but the practitioner can decide which one to use. abrupt offsets of pressure-depth trends indicate pressure seals. changes in excess pressure less than 4 psi are not significant to indicate a seal. this is important to consider in deviated wells where depth control introduces uncertainty in the reported depth. figure 2 shows a traditional pressure vs. depth plot for three different wells in the same field. the fluid gradients as well as fluid contacts are indicated for each well. on the right-hand side is the excess-pressure vs. depth plot for the same three wells. it is more readily apparent from this plot that each well is located in a different fault block; hence the different fluid contacts. inspection of raw pressure-data quality is essential. i have assumed that problems by leaking probe seals, probe plugging, supercharging, tight tests, or other gauge problems have been eliminated from the plot. the practitioner is strongly encouraged to scrutinize the pressure history for each depth. an idealized pressure vs. time plot is shown in the figure 3. the entire pressure-history plot may be obtained directly from the service company running their formation tester in your well. shown in figure 4 is another useful application of this technique. the excess-pressure vs. depth plot provides a clear visualization of the different depths for free-water level (fwl) and oil-water contact (owc). the interested reader is encouraged to check: 2009, bo cribbs: practical wellbore formation test interpretation 2011, t. d. jenakumo and m. k. ranasinghe: application of the excess pressure technique to investigate connectivity of deepwater turbidite reservoirs, offshore nigeria , spe-150746 petroleum solutions has incorporated this technique into their pdp software . ( note : there might be other software incorporating this technique. i am not promoting this particular one. it is simply that i have seen a demonstration of this particular one.) like comment copy linkedin facebook twitter share 41 9 comments dave dudus consultant petrophysicist with integ petrophysics, calgary;  international affiliation as senior petrophysicist at virtual petrophysics 5y report this comment nice summary, angel. i used alton brown’s aapg paper as my reference material years ago regarding this technique. like reply 1 reaction grant heavysege independent petrophysics consultant 5y report this comment well done angell. very eloquent summary that will be useful for many. like reply 1 reaction john parra senior reservoir engineer 7y report this comment i haven' been able to use the formula correctly. could you elaborate how to use it? i have all the required date depicted in the formula. i.e., measured pressure, gauge depth, assumed fluid density.
thanks. like reply 1 reaction vinh phan petroleum engineering consultant at resxp, reservoir expert, llc 8y report this comment well done, what about compositional  gradient wit depth where density is depth dependent? like reply 1 reaction scott dodge founder and managing director at virtual petrophysics 9y report this comment hi angel,  similar to gene, i started using the excess pressure technique in the last hmmmmm 10 years.  currently working a low poro-perm clastic in the taranaki basin and used it recently to hi-grade and select the actual pretests to determine the gas gradient and density.  

do you have a method for identifying pre-tests that are super-charged in abnormally pressure reservoirs?  in wells where the number of pre-test measurements are plentiful, the excess pressure method works well, however the situation i have is stacked reservoirs with only one or two pre-tests per reservoir.  in this instance, it is difficult to establish  a normal pressure trend, and identify which points at elevated pressure are super-charged, versus formation pressure.  some of the reservoirs are depleted, so the range is both original pressure and depleted pressures. 

porosity 8 to 10 p.u. and air perm 1 md. like reply 1 reaction see more comments to view or add a comment, sign in more articles by a. g. guzman well test design: fluid properties apr 12, 2017 well test design: fluid properties reservoir fluid properties may be determined in two ways: by experimental laboratory procedures, pvt… 27 2 comments well test design: rock properties feb 14, 2017 well test design: rock properties once the well-test objectives have been established, the next step in designing a well test consists of gathering a… 30 1 comment permeability averaging jan 10, 2017 permeability averaging permeability in fluid mechanics and the earth sciences, commonly symbolized as k, is a measure of the ability of a… 248 24 comments comparing permeability from core and well test dec 8, 2016 comparing permeability from core and well test absolute permeability is the measure of the ease of flow of a fluid through the reservoir rock. it is a rock property… 199 25 comments types of well tests sep 19, 2016 types of well tests five test types are briefly discussed below: pressure build-up, injection/fall-off, multi-rate, multiple well, and… 232 25 comments well testing (part 2) aug 1, 2016 well testing (part 2) well test objectives given the complexity of a well test, it is imperative to follow a methodical approach. if… 199 8 comments well testing (part 1) jul 1, 2016 well testing (part 1) what is a well test? a well test is a flow experiment carried out in a reservoir. normally fluids are produced from a… 89 6 comments repeat formation tester vs. full production test feb 9, 2016 repeat formation tester vs. full production test a question that usually comes up is whether a repeat formation tester (rft) [generally referred to as mdt, which… 59 8 comments well testing feature - jpt feb 2016 feb 2, 2016 well testing feature - jpt feb 2016 what a tumultuous, challenging, and exciting year 2015 was. the roller-coaster ride made it a year ripe for mergers and… 15 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",-1
https://medium.com/@iamjaswanth9/snowflake-regular-expressions-for-effective-email-validation-boost-your-data-quality-15bba7999297,,,"snowflake regular expressions for effective email validation: boost your data quality | by iamjaswanth | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story snowflake regular expressions for effective email validation: boost your data quality iamjaswanth 2 min read · oct 24, 2024 -- share when working with user data, email validation is crucial to ensure clean, consistent, and valid information. recently, i faced an interesting challenge where i had to validate email addresses in sql using regular expressions. let me walk you through how i tackled it and how you can do the same! creating the users table first, let’s create a temporary users table to store our users' email addresses: create or replace temp table ""users"" ( ""user_id"" int autoincrement primary key, ""email"" varchar ); next, we populate this table with a few test email addresses, including valid, invalid, subdomains, and plus-addressing (used by some services for email filtering): insert overwrite into ""users"" (""email"") values ('john.doe@example.com'), ('invalid-email@.com'), ('alice_smith@sub-domain.co.uk'), ('bob@domain.com'), ('bob+folder@domain.com'),  -- plus-addressing test case ('folder@bob.domain.com'),  -- subdomain test case ('user@invalid_domain@com'),  -- invalid case ('user@gmail.com'); the regular expression now, we get to the core of the solution: the regular expression to validate the email addresses. select *, regexp_like(""email"", $$[a-za-z0-9.-]+[+]?[a-za-z0-9.-]*@[a-za-z0-9.-]+[.][a-za-z0-9]{2,6}$$) as ""is_valid_email""… -- -- written by iamjaswanth 72 followers · 161 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://fithis2001.medium.com/exploring-the-imdb-dataset-with-dbt-11cd02c025aa,,Unit Testing,"exploring the imdb dataset with dbt | by vasileios anagnostopoulos | medium sitemap open in app sign up sign in medium logo write sign up sign in exploring the imdb dataset with dbt vasileios anagnostopoulos 12 min read · oct 31, 2023 -- 1 listen share writing data tests for a (mighty) real dataset introduction ( disclaimer : this article differs from my typical infrastructure articles like this one i made for the company i work for. still, it is modern data engineering of tutorial nature). i am a fan of movies usually scifi and scifi/horror. my preference is on the 80s/70s and early 90s. i believe they are the best years of film production (and dance music production). usually the runtime of a movie is between 85 to 95 minutes. however for me it is a little longer. people usually take a break to make some pop-corn or to check their email when they watch the movie. but others, me included, head over to imdb to learn about the cast, what other movies they have played since or before that specific movie, what is their personal life. what this starts as an information retrieval exercise, soon becomes a random walk in a graph because you see other movies they played and you notice other stars in that movies. now you start scanning their lives. the process repeats recursively (some backtracking is also involved). it takes time to realise that wandering around takes you too far. this is interrupted by you remembering to watch the rest of the movie (or you waking up on the keyboard and realise that it is not your bed). the biggest question is whether all this information is available somewhere to be used for machine learning, data analytics or even data engineering experimentation. it turns out that there is such a dataset . it is a set of dumps of tables that can serve the above purpose. they are reduced and non-commercial. i was lucky to find it and at some point i wanted to explore it. but then i only knew the standard machine learning approach of using pandas or tibbles to work with a dataset. what happens when the data are too much to be processed in your ram? while this dataset can fit in a huge ram (8gb), still if it outgrows it, what is the best approach? fortunately apache spark and trino have solved this issue for us. but this is not enough. they tackle only the processing and storage problem. the data quality assurance (dqa) part is in many cases a custom thing that uses these processing engines to get compute kpis and some python or scala library . the good old warehousing architecture the last decade has seen unprecedented increase in storage and processing capacity thanks to the “big cloud” platforms that offer though their blob stores petabyte level storage capacities and through big server farms, peta-flop level processing power. what happened in the past was that when reporting was necessary for e.g. bank transactions, data were queried from the system that hosted the transactions, they were filtered and aggregated (in order to reduce volume and keep what made business intelligence (bi) sense) and stored in an other system for bi reporting. the reporting queries could also filter and aggregate to further reduce data in order to fit a specific “view” of the system. taken from https://www.informatica.com/resources/articles/what-is-etl.html this transition from system to system was lossy, for storage and computational performance reasons. it is known as etl . elt and the medallion architecture but compression as we said is lossy and it does not allow for flexibility. this is where elt can help and was enabled by the big data processing systems we mentioned before. here we just copy over everything to a data lake (our landing layer), add a subset of them to the processing system. this copy (and its representation as a data source for computations) forms the bronze layer. subsequent transformations can take the data to the staging area (which casts data to data types that make business sense) and then to the silver layer where the data have good quality for bi and represents fragments of information. in order to enable bi over business objects (capturing the domain) and not only “data”, a further layer the gold forms tables or views by combining data over the scattered information in silver layer. all of this is done through queries which were not possible between different database systems, in other words we have inter-db queries and views. in databricks all these layers exist in the same system called lakehouse . the separation of the various layers is called the medallion architecture. taken from https://www.advancinganalytics.co.uk/blog/medallion-architecture while the databricks one (see above figure) is the most famous, i think the one presented here is making even more sense. transforming with dbt while we talked about the elt and explained the el part we were fuzzy about the transform part. dbt enters the picture typically after the “bronze layer”. dbt actually can be used to do the transformations for staging area, silver and bronze layer (and of course gold layer). it is a very clever application of jinja templating to refactor sql queries that lead from one layer to the other. it also adds data unit testing which tests data for pathogenies. these can be e.g. an actor that exists in a movies table as an id(entifier) but this id is unknown to the actors table. or a movie produced in 200ad (rest assured that it is a data error and not a proof of time travel or ufos). this way you can go back to the sources and correct them, ignore them or something else. but you need tests for two reasons. the first reason is the inadequacy of database types furthering constrain the shape of data. the missing actor error above is a manifestation of a referential integrity error. the second reason is again the inadequacy of database types. this inadequacy is of different nature as the “time travel” error above revealed. in the first case, we need other rules to strengthen the typing, these rules expressed as data tests are called type constraints ( related to business logic ) . the second case has to do with business rules. these are the business constraints which are rules expressed by data tests too. in any case, finding the issues (so as to correct them) is done through tests. sometimes, we also need cleansed tables for the next level until the issues in the data are fixed. dbt offers these functionalities (and more). for a very basic introduction you need the official documentation and possibly this . the present article is the next step. introducing our approach. in previous articles i used docker and co to introduce pipelines. here the purpose is not to make an end to end pipeline but instead focus on the data testing part. the reason is that the dataset is not very small. fortunately we have a suite (dbt) that allows convenient construction of data tests and tables or views. they are non-trivial parts as you will see (or experience by following the code and the article). so instead of using a process to grab files and move it to a “bronze layer”, we take the “bronze layer” as a given and the natural format of the files to be its natural format, gzip compressed tsv files. who said they have to be delta tables after all!!!! we will not use docker, so as to minimise complexities or performance issues. also we will not use a “cloud friendly” database or a relational store like postgresql. but we definitely need an olap store. since we will run it locally, duckdb is a perfect match. it will emulate our big data processing system and is seamlessly supported by dbt. for this reason we take a parent folder called “imdb_files” hosting our downloaded files as a “bronze layer”. if you need to change folder, change it here: as you can see, duckdb allows us to keep our files compressed and specify a set of csv parameters, like compression and seaparator. we ensure that the input is typed as varchar fields. after all it is a csv. contrary to what software engineers do, we do not need a fancy orm here. even if we integrate our postgresql with the site that hosts these data, we will unzip them and store them as a table with varchar entries. the rest are the work of dbt. the orm is there to store and retrieve business objects for the services and for specific tables for specific databases . our pipeline will produce tables that span multiple databases for reporting and business objects that model the whole business (equally important). describing the imdb dataset just blindly getting the data of the imdb dataset into views and writing tests is not the most fruitful approach. we need to first get what piece of information we can from the distributor and run some initial exploration, at least at the level of finding the correct types of the dataset to populate our casting process. in this specific case we are lucky enough to have some description of the various fields and some hints about the types. the first hint is that ‘\n’ is used as a null in our fields. as we said we get everything as a varchar initially. so our first task is to convert all ‘\n’ to nulls. so, when we create our dbt model we need to first create a null corrected view of our data. while we should normally do it for only the fields we have this information, it is soon obvious that this is not a good strategy. lets use our duckdb (we make the assumption that you have installed it somehow ) to run across our first pathogeny. enter the “imdb_files” folder where the csvs reside and run duckdb inside duckdb we run this is definitely not empty. is it really non-null? in my instance it says 12354111 entries. while deathyear can be nullable, the birthyear can have empty fields too even if it is not mentioned. there are also fields that are lists. one field is also a list but the dataset says nothing about it. the non-declared list is the characters field (yes, we suspected that from the name) in the title.principals.tsv.gz file. it should have been a list of strings and not a string in the description. but even for the other list fields, where we expect to be a string with comma separated values, we soon learn that this is not the case!!!! let’s explore with our duckdb the title.akas.tsv.tgz file and try to verify that the types field is an enum where fields are strings separated with comma bad enum for a field as you can see names are totally messed. there is a folder in the codebase called “notebooks” where we have a python notebook for this dataset. the analysis included finds out that types and attributes fields have ‘\x02’ as a separator. we found this, and the rest of the cases by data tests (thank you dbt). the more familiar you become with data tests and exploration of a dataset, the more you “see” some patterns forming. seeding dbt for writing data tests dbt needs constants. these exist in dbt in the form of small tables. these can host for example all the countries, all the languages or a number of constants like that we found above. the seeds that are mentioned in the documentation of the dataset are seeds/countries_with_regional_codes.csv seeds/distribution_types.csv seeds/language_codes.csv the first is taken from this repo and we renamed “alpha-2” field to “alpha2”. this is what we are interested. the second is constructed from the documentation of title.akas.tsv.tgz . as it is mentioned, it can change without notice. but we use the seed to form a data test, in order to capture errors that can appear without notice too! the third one is from datahub . the distribution_types is not necessarily a seed. we could test the fields with the “accepted values strategy” however the field hosting this value is an array. and so we exported them to a seed file and written a custom vectorized referential test. writing the tests and cleansing the dbt testing documentation or something along the lines of this advanced testing in dbt tutorial can be used as an necessary introduction as we go beyond basics. here we apply this knowledge to create tests for our models. we focus on type constraints (data tests). we will use the title basics model. first we need to create our bronze model as you can see, we create a view (you can create a table for performance). then we create a view built upon the previous view for the staging area (called intermediate layer in our code base). the bronze layer is the aforementioned null correction location. the intermediate layer is the type casting location. there is one caveat in the case of name_basics_model_cated.sql we use try_cast so as to not fail early. strictly this is a cleansing operation, but we make the compromise that non-integer values are null values. if we blindly casted we would have failed very early in our pipeline. so now we have a view of the table with correct types. as we said previously, types are not enough. so we add data tests that complement the types. tests would fail immediately in the intermediate layer, so we make a set of cleansing operations to provide the silver layer. this is the one that is tested for data quality. as you can see our first test is about genres because of dbt_utils limitations we apply it on the the model (and not the specific column). this knowledge is a codified version of the dataset documentation. another very typical test is that tconst is a primary key or that the region in title_akas is an enumeration. and the field titleid field may be coming from another table. things to keep in mind: never forget to add documentation to columns delegate some of the documentation to tests things not tested are things that are not guaranteed to hold. for the above, uniqueness of the ordering field is not tested. so, it does not hold (feel free to see what happens if you enable the test). but there are also type constraints that are misleading and have to be found with experimentation like the combination of columns above. for each field misled by documentation i wrote uniqueness tests. of course they failed and i had to resort to careful reading + experimentation to uncover the hidden type constraint. this should have been explicit. run the tests, modify them to fail and read them carefully. there is a lot of type constraints not covered in the documentation that leads to type constraints. finally some fields are arrays and the elements of the arrays are foreign keys. for this one we wrote a specific generic assertion: some of the tables still had test failures even after following the documentation. for example there are directors used that do not correspond to the name_basics_model . for these cases we created a cleansed model that do not exhibit these pathogenies. here is an example. in order to make the transition to the silver layer, we need business constraints (new data tests) and possible cleansing (including imputation). data profiling we should definitely include a report about the statistics of the columns of the various tables. a business person can have a quick look to see whether this makes sense and any tests need to be included. we include here 2 kinds of data profiling (the first was used above to uncover seeds). jupyslq we use j upys ql to view our duckdb tables and profile them through a jupyter notebook. execute the “data_profiling.ipynb” notebook to see an example. here is a report for our title_principals_model table. it seems that this report is intolerant to nulls. also as you can see in the notebook it also cannot unnest columns. soda core soda offers a cloud platform for data quality (like dbt) but offers an open core . we will not go deep in this. you can read excellent medium articles (related to duckbd ) or the official documentation . here we just create the bare minimum to create our data profiling reports. we need a configuration and a checks file now, having installed the dependencies from the “requirements.txt” file, we just execute the profiling. soda scan -d imdb_dataset -c configuration.yaml checks.yaml -srf soda_scan.json soda reports some issues with boolean columns as they are not supported . either create a view with these cast to 0/1 or use jupysql. a report, is included as “soda_scan.json”. load it to a json editor to see the reports. keep in mind data profiling has big computational requirements and this is why it is preferable to run it on cloud. conclusion in this tutorial article we took the challenge of analysing the imdb non-commercial dataset. we also made our first step in writing a complete data testing suite, especially for the type constraints kind of testing. we found various pathogenies and we showed ways to detect them and cleanse them. we introduced data profiling of the imdb dataset with two different ways as a first step towards requesting business constraints by the product people by submiting a report for review. as always the code is provided . feel free to dowload, modify and execute the code towards learning dbt data testing. now, you have no obstacles to use this very useful dataset for your experiments. writing this article was an aspiring experience for me and improved my understanding of the material. i hope you enjoyed this too. update!!!! code snippets brough to their latest versions, text cleaned and added fresh reports. data engineering data quality dbt tutorial -- -- 1 written by vasileios anagnostopoulos 65 followers · 38 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@altini_marco/using-the-whoop-band-for-on-demand-heart-rate-variability-hrv-analysis-78eabd265189,,,"using the whoop band for on-demand heart rate variability (hrv) analysis | by marco altini | medium sitemap open in app sign up sign in medium logo write sign up sign in using the whoop band for on-demand heart rate variability (hrv) analysis data quality and applications marco altini 11 min read · apr 16, 2022 -- 1 listen share whoop 4.0 and polar h10. my blog has moved to substack, you can find it here . in this post, i look at the accuracy of the whoop band version 4.0 in detecting rr intervals and cover a few potential applications of this technology. this blog is not about the whoop’s night hrv data, which i only briefly discuss towards the end. the whoop band is one of the very few sensors that complies with the standard bluetooth heart rate profile . this means that you can link the band to any other third-party app for heart rate and heart rate variability (hrv) analysis. most wearables do not comply with the standard bluetooth heart rate profile (for example, you cannot link a third-party app to a fitbit, apple watch, or oura ring). however, being able to link a wearable to a third-party app allows us to use them for applications other than the ones provided by the company itself , which can of course be useful. for example, if the data is accurate, we can use the sensor as a replacement for a chest strap to collect morning hrv data in hrv4training , or we can link it to hrv4biofeedback for deep breathing exercises, or experiment with the hrv logger , for example analyzing disruptions in autonomic activity in response to workouts of different intensities by using pre and post-workout hrv measurements (learn more, here ). let’s look at the data and discuss in a bit more detail potential applications. please feel free to reach me ( @altini_marco ) on twitter if you have any questions about this blog. tools for this analysis, i have used two iphones with the hrv logger app installed, one paired to a whoop band 4.0 and one paired to our reference system, the polar h10 chest strap. in order to use the whoop band with the hrv logger (or any other third-party app) you have to set it to broadcasting mode from the whoop app . once you do so, it will be discovered by the third-party app just like the polar strap. broadcasting mode is documented on whoop’s support page here , and seems an official and maintained feature, which is why i am covering it here, while i won’t be looking into their unofficial apis. however, please note that like any other feature, whoop could always modify or discontinue this functionality. measurement setup. protocol in order to test the accuracy of the whoop band in detecting rr intervals, i split the protocol into a few minutes of different tasks: resting deep breathing typing in addition, i also experienced 2 ectopic beats over the 10 minutes recording, which gives us some more useful data to analyze. ectopic beats are actual misfirings of the heart , another form of artifact that has nothing to do with the hardware (you have to make your cardiac problems work for you). i used typing as an activity with very minimal motion , which typically makes the data unusable for other optical sensors (for example scosche’s rhythm24). this is helpful in determining how we can use the sensor, and you will see later there are indeed implications for what i will recommend as a protocol for morning measurements. how is the data? rr intervals rr intervals are the basic unit of information required to compute hrv features, and as such, they need to match pretty well between devices. let’s have a look at the collected data: rr intervals for the polar chest strap (top) and whoop band (bottom) across the activities used in this protocol. we can see right away that the data is very similar between the two sensors. i have annotated the two ectopic beats, which are also captured equally well by both sensors. deep breathing shows the expected larger oscillations in instantaneous heart rate (the inverse of the rr intervals). finally, typing causes very large artifacts (partially not visible for scaling reasons). overall, this is very good. other optical systems that i test routinely, for example, our camera-based algorithms on iphone , perform very similarly. heart rate variability (rmssd) rmssd is the most commonly used hrv feature , and also the one reported by whoop in their app. here i recomputed rmssd based on the rr intervals above, after a round of artifact removal. here is the data: we can see that the ectopic beats have a slightly different impact on the data collected from the polar and whoop sensors, while the following resting minutes and the deep breathing part provide quite similar values . there seems to be an offset with slightly higher values for polar data, but this is quite irrelevant in my opinion ( hrv is always analyzed in relative terms with respect to your historical data, so a little bias does not matter for data interpretation ). the second ectopic beat was much larger (see the previous plot with the rr intervals), and causes another discrepancy. finally, typing is a no-go. this means that only in the absence of movement, we can use this feature . in general, we cannot collect rr intervals using any optical sensors when there is motion (this is true for all sensors out there, whoop, oura, the apple watch, etc.). i tried a couple of different activities, and interestingly, the whoop will not send any rr intervals if it detects motion. if i walk or move my hand, the sensor will not send garbage rr intervals as other sensors do, but it will stop sending rr intervals. however, as you can see from the data above, small movements, like typing, are not detected and already make the ppg data unusable for hrv analysis . this is a good reason to measure your resting physiology when you are in control, and know what you are doing (i.e. in the morning at rest, which can sometimes be better than the night for this purpose). overall, i am happy with the quality of this data. issues with the bluetooth protocol i want to add a note on something that is quite annoying when it comes to the bluetooth heart rate profile. the rr intervals, when sent via this protocol, should be either in milliseconds or in 1/1024 of a second, something polar introduced. it seems that whoop does not use the polar “adjustment”, and sends rr intervals already in milliseconds. personally, i think this is the way to go, but since polar is the main manufacturer of sensors that are used for these apps, it becomes really messy to try to figure out which variant of the protocol is used. in terms of rmssd, this has a tiny impact (1–2ms, nothing to worry about), but for heart rate, this can lead to heart rates that are 2–3 bpm higher. it would be nice if a standard protocol was implemented in the same way by all, but for now, i’ll reiterate that this becomes irrelevant when we look at relative changes over time. a note on night hrv data as i mentioned at the beginning, in this blog i am not concerned with night hrv data, but i want to add a short note since this is normally how the sensor is used. whoop’s former approach to night hrv was flawed from a physiological and technological perspective , something i discussed extensively in the blog below: https://medium.com/@altini_marco/what-you-need-to-know-about-heart-rate-variability-hrv-data-collected-during-the-night-eb3913ffdc last summer, whoop aligned with oura and now reports the full night of hrv (with a different “weighting”, but data shows that they are now an almost perfect match). hoping that they will not change this again (who knows, the last time this happened very quietly), right now the sensor provides good data also during the night . i am very glad that the two main wearables for night hrv data provide consistent data. this will allow us to learn more from the many research studies that they are both carrying out, as well as from case studies and other reports of people using these devices. a note on recovery scores what i cover here is hrv , the actual physiological signal that changes in response to stress and can help us assess disruptions in autonomic activity and recovery needs . this is not about made-up recovery scores. everything that is built on top of the collected physiological data (e.g. hrv, heart rate, temperature) for “entertainment purposes” (recovery and sleep scores for example) has no scientific or physiological basis . if you are interested in learning more about issues with readiness and recovery scores, and how you can use hrv data more effectively , check out this podcast i recorded with jason koop and corrine malcolm : what can you use the whoop band in broadcasting mode for? currently, i am using the whoop band for a couple of applications where it can replace a chest strap and still provide high-quality data: biofeedback and pre-post exercise spotchecks. in the morning, i measure my physiology using the phone camera in hrv4training, but if you prefer to use an external sensor, this is also an option, as i discuss below. morning measurements of resting physiology why would you use the whoop for a morning measurement when you already have the night’s data? in my opinion, morning and night data are both effective in capturing long-term changes in resting physiology . however, morning data might be better for day-to-day guidance . why? when we look at very strong acute stressors (e.g. sickness or alcohol intake ), morning and night data will show the same responses. however, night data is tightly coupled to the previous day’s stressors even when less relevant stressors play a role (a late dinner, an evening workout, socializing, etc.). this means that your night hrv will be impacted by late stressors even though by the morning you could be perfectly recovered (which means that your hrv is within your normal range). i would also argue that it is not only more actionable but also more meaningful to measure your physiology after the restorative effect of sleep, i.e. in the morning. additionally, if you have an arrhythmia, the data could also become of little use when collected during the night (you can see above the impact of just 2 ectopic beats on the hrv data). despite not having a particularly frequent arrhythmia, i have no idea if my night data is any good on some days, for this reason. considering that you (or your athletes) could have arrhythmia only during the night and that there is a high prevalence of arrhythmia in endurance athletes, you might have no idea that the data collected is artifacted. these, together with parasympathetic saturation , are good reasons to measure your physiology when you are in control, and after the restorative effect of sleep . if you decide to try to collect morning measurements using the whoop band paired with hrv4training , here is my recommended protocol: set measurement time to 2 minutes . the whoop often does not send data right away, sometimes delaying rr intervals up to 30 seconds, possibly to stabilize the data if there was motion previously. a longer measurement allows for more high-quality data. wear the sensor on your non-dominant arm, and make absolutely no movement . the smallest movement will mess up the data, and therefore i would recommend placing the sensor on the non-dominant arm, and then starting the morning measurement using your dominant hand (it’s easier than it sounds, e.g. sensor on the left arm and phone in the right hand, if you are right-handed). left, today’s hrv as captured using the phone camera. right, today’s hrv as captured using the whoop. hrv in hrv4training is reported as a logarithmic transformation of rmssd, as typically shown in the scientific literature. i took these two measurements a few minutes apart, but they are basically identical. biofeedback: deep breathing exercises another application where i am using the band is biofeedback. i used to grab a chest strap for this, but it is obviously easier and more convenient to just use an optical sensor. biofeedback is a deep breathing practice similar to other forms of mindfulness, with the addition that you can visually see your instantaneous heart rate change as you breathe in and breathe out (hence the feedback part). practicing biofeedback might help improve self-regulation, and potentially increase parasympathetic activity . personally, i think all of this is up for debate, but i do find taking a few minutes to unwind to be helpful for my mental health. during biofeedback, you are sitting and doing deep breathing exercises, and therefore you are not moving at all. this means it’s easy to collect high-quality data even with optical sensors. below is an example session where i used the whoop band, and also a resonant frequency test done with hrv4biofeedback , to establish the breathing rate that elicits the highest hrv for me: left: example of resonant frequency test, estimating my ideal deep breathing frequency at 5.5 breaths/minute. right: 10 minutes biofeedback session, eliciting hrv 2–3 higher than my morning (or resting) hrv. pre and post-exercise autonomic control: did you train too hard? based on stephen seiler’s reasearch, using pre and post-exercise hrv we can better understand the impact of different workouts . ​by measuring autonomic activity (heart rate, hrv) *immediately* before and after the workout, we isolate the training stressor in a way that allows us to answer different questions, for example: at what intensity does a workout require a much longer recovery time? (does it need to be hard? what about just a bit harder than “aerobic threshold”? ) and what about duration ? hrv (rmssd) in the hours after exercise of different intensities. from seiler, stephen, olav haugen, and erin kuffel. “autonomic recovery after exercise in trained athletes: intensity and duration effects.” medicine & science in sports & exercise 39, no. 8 (2007): 1366–1373. despite some practical challenges with this type of experiments (you should control / avoid food and water intake, then you need to shower, all sort of things that are not as easy to manage on a daily basis outside controlled settings), i find this research very interesting, and i have been collecting some more data recently using the whoop band and the hrv logger: left: a few minutes of hrv collected before training. right: another few minutes of hrv, this time after a run around my aerobic threshold. you can find my write-up on pre and post-exercise autonomic control, here . conclusions the whoop band 4.0 in broadcasting mode can be linked to any third-party app and provides high-quality rr intervals for hrv analysis under conditions of no movement . this feature makes it possible to use the band for morning measurements of resting physiology, biofeedback, spot checks before and after exercise , and more. given how few user-friendly (read: not chest straps) sensors comply with standard protocols, this is good news and i am glad the company decided to implement this feature. the only alternative for the same applications would be a scosche rhythm24, which is of course cheaper, but if you do have a whoop band already, there’s more you can do with it: https://www.hrv.tools marco holds a phd cum laude in applied machine learning, a m.sc. cum laude in computer science engineering, and a m.sc. cum laude in human movement sciences and high-performance coaching. he has published more than 50 papers and patents at the intersection between physiology, health, technology, and human performance. he is co-founder of hrv4training , advisor at oura, guest lecturer at vu amsterdam, and editor for ieee pervasive computing magazine. he loves running . social: twitter: @altini_marco . personal substack . strava . new to hrv4training? heart rate variability whoop biofeedback recovery training -- -- 1 written by marco altini 3.4k followers · 67 following founder hrv4training.com , data science @ouraring lecturer @vuamsterdam. phd in machine learning, 2x msc: sport science, computer science engineering. runner responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/big-data-processing/multiple-git-keys-debugging-with-the-ssh-f5f949fa4a6e,,,"multiple git keys — debugging with the ssh | by m haseeb asif | big data processing | medium sitemap open in app sign up sign in medium logo write sign up sign in big data processing · big data processing, apache flink,apache spark, apache hadoop, data-ops, data engineering multiple git keys — debugging with the ssh m haseeb asif 3 min read · sep 27, 2022 -- listen share i wrote an article where i shared how i have configured multiple git keys for github and bitbucket simultaneously on my development machine. i recently had to add another key from the visual studio source code and had some challenges doing the configuration. hence i explored different debugging options. initially, i updated my ~/.ssh/config file to add the additional configuration for vs-ssh.visualstudio.com , but it didn’t work, so i had to find out how to debug the ssh. output verbosity first, we can use the -v flag to have more verbose output to see what is happening when you execute a command in detail. for example, ssh git@github.com gives the following message for successful authentication. hi haseeb1431! you've successfully authenticated, but github does not provide shell access. connection to github.com closed. if we add the flags to the same command, we will see many more messages and details about the authentication process. for example, it will look as follows. ssh -tv git@github.com we have two flags, t and v. -v is for verbose output. it causes ssh to print debugging messages about its progress. this helps debugging connection, authentication, and configuration problems. multiple -v options increase the verbosity. the maximum is 3. -t avoids requesting said terminal , since github has no intention of giving you a secure interactive shell, where you could type commands. it is good to use -t while testing ssh test connections because some servers could abort the transaction entirely if tty is requested. ssh agent you need to make sure your ssh key agent is running. so do a ps aux|grep ssh-agent . make sure your key agent is running. if you’re not using ssh-agent (i like keychain from gentoo, or sshkeychain for mac os x), do whatever you have to do to ensure that your keychain is running. key addition make sure your private key is added to the ssh key agent. so do a ssh-add -l to check that ssh-agent has your key. likewise, if you are using something else, check your keychain application has your private key. key permission check the permissions on your home directory, .ssh directory, and the authorized_keys file. if your ssh server is running with ‘strictmodes on’, it will refuse to use your public keys in the ~/.ssh/ directory. your home directory should be writable only by you, ~/.ssh should be 700, and authorized_keys should be 600. ssh keyscan ssh-keyscan returns the fingerprint of a key, not the actual pub key. when you make a ssh session, two different key pairs (with a fingerprint for each pair) are involved. also, you can tail the authentication log as well while run ‘tail -f /var/log/auth.log’ on the remote host. you can watch the log as you try to connect via ssh with your key. after doing the verbose analysis, i figured that we have to use the host directly, and it seems the hostname doesn’t work, at least for me. so my new updated ssh config file, updated from the last post , looks as follows with the multiple different git accounts working simultaneously. bitbucket (default) host bb hostname bitbucket.org user git identityfile ~/.ssh/id_rsa #github (secondary) host gh hostname github.com user git identityfile ~/.ssh/id_rsa_gh # azure devops host vs-ssh.visualstudio.com identityfile ~/.ssh/id_rsa_delta identitiesonly yes references https://chuyeow.wtf/2007/02/28/debugging-ssh-public-key-authentication-problems https://stackoverflow.com/questions/17900760/what-is-pseudo-tty-allocation-ssh-and-github https://explainshell.com/explain?cmd=ssh+-tv https://docs.digitalocean.com/support/how-to-troubleshoot-ssh-authentication-issues/ ssh keys security git debugging cloud computing -- -- published in big data processing 284 followers · last published feb 14, 2025 big data processing, apache flink,apache spark, apache hadoop, data-ops, data engineering written by m haseeb asif 250 followers · 26 following technical writer, teacher and passionate data engineer. love to talk, write and code with apache spark, flink or anything related to data no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://mecha-mind.medium.com/membership-queries-with-big-data-9e5046d3270f,,,"membership queries with big data. doing membership tests is one of the… | by abhijit mondal | medium sitemap open in app sign up sign in medium logo write sign up sign in membership queries with big data abhijit mondal 11 min read · jun 14, 2021 -- listen share source: quantamagazine.org doing membership tests is one of the common use-cases required by many applications. in web crawling we want to know whether an url has been crawled or not. in recommendations systems we want to know whether an item has already been recommended to a user or not. to prevent ddos attacks and ip spoofing we want to know whether an ip address has been blacklisted or not. in ad-words, we want to know whether there are any ads corresponding to the words and n-grams present in a query. in search we want to know whether a query term is indexed before looking up an inverted index. to know whether a user password during sign-up is a weak password or not or a password has been compromised. in spell-checking we want to know whether a word is in the vocabulary or not. …and many more approach 1 : databases in the above applications, we can always leverage the database but doing so comes at a cost of disk i/o and putting unnecessary load on the db for something that may not exist at all in the db. for e.g. in web crawling we can store the crawled urls in a nosql database like mongodb or cassandra and every-time we want to crawl an url, we can first lookup whether the url exists in the db or not. if it exists we do not crawl else crawl and add the url to db. this can have implications if there are billions of urls in the db. pros: data is read and write to and from a single source of truth. data is persistent. even if system crashes we will still have the data about which urls has been crawled. cons membership queries are slow as it involves both network (assuming db is hosted on a remote cloud server) and disk i/o. may involve unnecessary db lookups. for e.g. while crawling for urls for a new domain, most urls would not have been seen before. approach 2: sets and hash tables the entire data is stored in in-memory data structures such as a set or a hash-table and is held in the ram. note that an in-memory set is an auxiliary data structure since we anyways need to persist the data in disk else if the system crashes the data would be lost from the ram. let h(x) be an hash-function of size ‘m’, i.e. it maps input data into the range [0, m-1]. let a be an array of size ‘m’. given an url x, perform h(x). set a[h(x)]=x. to check if an url is present, check a[h(x)]. there are few downsides to this: ram is limited. the size of data would be in multiple gbs or terabytes. sets and hash-tables works on the principle of hashing. to enable fast lookups i.e. o(1) lookup in the worst case, the size of the hash table must be >> the size of data and the hash function should produce uniformly distributed keys. assuming 32 bit hash keys, the size of the membership hash table (assuming an array of size 2³²) would be approximately 16gb. collision in hash table occurs when for 2 inputs x and y, h(x)=h(y) where x != y and h is the hash function. since the data is always increasing but the size of the hash-table is limited by the ram size, there will be lots of collisions when either the size of data ~ size of the hash table or the hash function is a bad one and produces skewed distribution of keys. for e.g. consider the hash function h(x) = (3x+5) % 8 this hash function produces keys in the range [0, 7]. so if the number of inputs to hash is > 8, then by the pigeonhole principle, at-least one key will definitely contain multiple values. for e.g. for x=1 and y=9, then h(x)=h(y)=0. even if the space of keys is much higher say h(x) = (3x+5) % (2³⁰), then the keys are in the range [0, 2³⁰-1]. by the birthday paradox , if we have hashed 38582 inputs then there is 50% chance that at-least 2 of them have the same hash index. one can do dynamic resizing when the load factor i.e. ratio of number of inputs to the number of hash keys reaches above a certain threshold. for e.g. if the current h(x) = (3x+5) % m and load factor is 0.75, then create a new hash h(x) = (3x+5) % (2*m) and re-index all the data. we cannot use a bit vector instead of an array because then we cannot store collisions. to resolve collisions there are certain techniques such as using linked list, linear probing , cuckoo hashing etc. all of these techniques have an average run time performance of o(1). dynamic resizing is used to reduce number of collisions whereas linear probing, cuckoo hashing etc are used for crud operations with collisions. a simple python code to implement a set using hash table with linear probing to insert 1 million strings into this set it takes around 6 seconds. searching a string on an average takes 0.0015 ms. depending on the available ram on the system, we can start with a smaller hash table i.e. smaller values of ‘m’ but time taken to insert may take longer as it will require multiple re-indexing. for e.g. starting with hash table of size of 2¹⁰ it took 9 seconds to insert the data but took 6 seconds with hash table size of 2²⁰. the final size of the hash table was 2²¹ i.e. it took 11 re-indexing steps with the smaller hash table as compared to just 1 re-indexing with the later. we can test how good the hash function is in terms of distributing the input uniformly across the range of keys. we can use kolmogorov-smirnov test to test how close ‘f’ is to a sampling from an uniform distribution. the p-value is 0.90 implying that this is very close to a uniform distribution. pros faster reads and writes as compared to databases since data is entirely stored in-memory. although for persistence we need to periodically flush the data from in-memory into disk main database is not touched often and thus it can be utilised for more complex queries. linear probing for collisions are much faster as compared to linked list or cuckoo hashing because records are stored and searched sequentially starting from a pointer in memory. sequential access of memory addresses are faster as compared to random access as in linked list where the next pointer can point to any memory address in the ram. cons size of hash-table scales linearly with data hash table size in gb is limited by available ram number of collisions will increase with data and hence time to read whether a record is present or not. time complexity is o(1) in the average case not worst case since we need to linearly probe the record. on distributed systems, each instance will have its own hash table in the ram and no way of coordinating. thus same url will be crawled by processes running in separate instances. we can use distributed in-memory databases such as redis to implement the hash table but that will slow down the reads and writes as it will require network i/o to communicate and coordinate between instances as well as waiting for write locks to be released by concurrent processes. approach 3: bloom filters bloom filters are probabilistic data structures implemented using hash tables but with a ‘twist’. source: livebook.manning.com these are called probabilistic data structures because bloom filters can return false positives. for e.g. if a url is never crawled, bloom filters can say that it has been crawled. but the good part is that we can control this error rate by selecting the optimum size of the hash table and the number of hash functions. in bloom filters, we use a hash-table of size ‘m’ and ‘k’ hash-functions instead of a single hash function. the ‘k’ hash functions are chosen to be independent. one good part is that in bloom filters, we work with bit vectors instead of integer arrays which reduces the data size by the factor of the number of bits used to represent the integer in the data. for e.g. with a bit vector of size m=2³² we need 0.5gb of memory as compared to 16gb while storing the data as 32 bit integer array. let h1(x), h2(x) … hk(x) be the ‘k’ hash functions. then for a given input ‘x’ and bit vector a set a[h1(x)]=1, a[h2(x)]=1 … and so on. to search for a given input ‘x’, check if the bits at positions h1(x), h2(x) … hk(x) are set or not. if any one of the bits is not set, then ‘x’ is definitely not there but if all the bits are set then ‘x’ is ‘probably’ there. it is ‘probably’ there because it could be possible that a combination of other inputs have set all these bits. to reduce the probability of false positives, we can increase the size ‘m’ of the bit vector and/or increase the number ‘k’ of hash functions. but too many hash functions will fill up the bit vector fast and as a result the number of false positives will increase. so we have to balance ‘k’. let ’n’ be the estimated number of points that will be stored in the bloom filter. for our convenience we assume it is 10⁶. the optimum values of ‘m’ and ‘k’ to achieve an error rate of ≤ p: m = -n*log(p)/(log(2))² k = (m/n)*log(2) to insert 10⁶ points into the bloom filter with a false positive error rate of p=0.005, it took around 5–6 seconds with size of bit vector m=11027754 and k=8. to validate that indeed the false positive error rate is bounded by 0.005, we can insert some random points not from the inserted data and test whether it is present or not. searching within the bloom filter takes 0.005 ms on average. pros for the same amount of data say 1 million strings, standard hash table would require at-least an array of size approximately 2²¹ to maintain a load factor of at-most 0.75, which is approximately 8 mb in ram (assuming 32 bit integers) whereas bloom filter would require approximately 1.3 mb to maintain a false positive rate of 0.5%. we can reduce the ram requirement further in bloom filters by increasing the false positive rate that we can live by. for e.g. with 5% fpr, we need approximately 0.75mb ram to store 1 million strings. there are many applications which do not care much about the false positive rate. for e.g. in recommendation systems, it does not matter much if some user is not shown 0.5% of the items which he/she has not seen before. in many applications bloom filters are used in conjunction with databases. the idea is that before looking up the database index to check if a value exists, we can lookup a local bloom filter first. bloom filter never returns false negatives thus if a value is not in the database, bloom filter will say it is not there and we do not have to go to the db unnecessarily. but if it says the value is there then we have to lookup the db index. cons time complexity of lookup is o(k) where k is the number of hash functions. thus it may take longer than lookups with hash table. for each lookup query, we have to search at k index positions which may not be sequential in ram, thus is sub-optimal. deletion of values not possible with the above bloom filter implementations and we need something known as counting bloom filters . in distributed systems, the benefits will be overshadowed by network i/o to maintain a shared bloom filter and also handling concurrency with locks. approach 4: quotient filters one of the drawbacks of bloom filters for membership query is that if the data becomes too large and we need to store part of the bloom filter in the filesystem on disk, then the ‘k’ hash functions needs to access random locations in the file. since random reads from disk are slower as compared to sequential reads thus bloom filters will not perform up-to the mark when stored on disk. source: livebook.manning.com to overcome this, there is another probabilistic data structure known as the quotient filter, which solves this problem as it stores all values corresponding to a particular key sequentially and in sorted order. will not go into the details of quotient filter in this post and will refer to this awesome blog post to help understand how insert and search works with quotient filters. here is a python implementation for the same: also observe that qf uses only a single hash function as compared to ‘k’ hash function with bf. although the worst case time complexity of insert and lookup in bf is o(k), with qf, the amortized run time complexity is o(1) but sometimes it may take more time as compared to bf since we need to sequentially search for the position to insert at. with qf, insertion of 1 million strings takes around 5 seconds and searching takes on an average 0.006ms. the false positive error rate was around 0.2% and memory requirement was around 2.75mb as compared to 1.3mb in bf, since we are also storing the hash values. for lookups with quotient filter, it took on average 4 sequential index scans with median of only 3 index scans. thus we see that quotient filters are efficient as compared to bloom filters where ‘k’ was somewhere around 8. pros: sequential reads unlike random reads in bloom filters. beneficial when data is stored on disk. only one hash function as compared to ‘k’ hash functions in bf. quotient filters support deletion of values unlike bloom filters. cons: run time complexity is o(1) amortized but sometimes it may take more time as compared to bf since we need to sequentially search for the position to insert at. insertion and lookup logic is not very straightforward. in general we do not need to store the bloom/quotient filter in disk in case of membership queries where we are working in conjunction with a database and the purpose of filters are only to prevent unnecessary lookups to access the db and thus reduce load on the db server. for this purpose, there would be enough ram to hold these filters in memory. for e.g. to hold 1 billion strings bloom filter will require only 1.3gb ram with a false positive rate of 0.5%. manageable with most basic cloud servers. in case we want to use the filters as full fledged hash-maps where we also need to support deletion, then we can use cuckoo filter or quotient filters as alternatives to counting bloom filters. some nice reads https://blog.cloudflare.com/when-bloom-filters-dont-bloom/ https://research.vmware.com/files/attachments/0/0/0/0/0/2/9/paper.pdf https://shokrof.github.io/cqf_assessment/ https://brilliant.org/wiki/cuckoo-filter/ bloom filter hashing algorithms data structures big data analytics -- -- written by abhijit mondal 2k followers · 132 following engineer no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://3d-points.medium.com/006-does-leica-blk2gos-data-worth-the-money-19bb0b0d0333,,,"#006- does leica blk2go’s data worth the money? | by lukasz wiszniewski | medium sitemap open in app sign up sign in medium logo write sign up sign in #006- does leica blk2go’s data worth the money? lukasz wiszniewski 16 min read · aug 15, 2021 -- listen share it is a continuation of the previous article with a strong focus on the quality and usability of blk2go’s data. pict. unfiltered blk2go data with waypoints generated every 2 meters. after many hours and a few kilometers walked with the blk2go scanner ( check part1 of this review ), came time to take a look at the point clouds. in this article i will focus on: uploading data data quality data usability as long as i was impressed by the portability of leica’s device, the data itself gives me mixed feelings. pict. a single run with the scanner. transferring data to the leica cyclone register 360 first things first. before you would be able to see the data on the screen of your pc, you should transfer them from the scanning device. you have only one option here which is a usb-c cable. in my opinion, is the fastest and most stable data transferring possibility, and i am glad it is used in blk2go. usb socket is placed under the battery, so you turn off the scanner, pull out the battery and connect the scanner by a cord to your computer. cyclone register 360 will recognize connected devices and display projects available in its internal storage. now you have three options: import data to the register360 project import data to the register360 project and save raw data to b2g format export directly to e57 file without importing data to the project pict. export e57 users can also create waypoints with preferred intervals e.g. every 5 meters. these waypoints are nothing more than spots in a scanning walk wherefrom will be generated panoramic images. extracted pano images are used for truview. this option is available only with a publisher pro license. pict. on the very bottom of the import panel in cyclone register360 is an option to create waypoints leica developers implemented a few months ago a new feature called filter. it is meant to be used for removing the spaghetti effect in the point cloud - the point cloud doesn’t have nicely structured rows and columns of points in comparison with the tls data. unfortunately, the filtering not only enhances the readability of the point cloud but also removes some percentage of points, thereby removes smaller details of scanned space. pict. data filtering during import (from the left: no filter, high filter, low filter) pict. amount of imported points with used filtering (from the left: low and high filtering) you should notice that the same raw data imported to the cyclone register360 and directly exported to e57 gives a different amount of points in the point cloud. another odd thing is that low and high filter applied to the same walk has a diverse length of the trajectory. why does it happen? first, look i am going to mention shortly about data quality comes from my scanning test in april 2020 and then i take a look at data from march and may 2021. pict. the same office area was scanned with blk2go (no filters applied) and rtc360. generally speaking, blk2go data from 2020 — the first release- was useless in most cases. as you can see in the picture above, visibility and recognizability are poor and it is a blockage for using it in the modeling process or analysis/clash detections. i juxtaposed blk2go point cloud with rtc360 to show a huge difference and let you answer the question does the leica’s mls data would be easy to use. luckily a year later -2021 blk2go began to deliver much nicer point clouds. they are more consistent and have much better visibility. the availability of the filtering options in register360 also increased the value of the data in most cases. i am saying in most cases because filtering drastically dumps the number of scanned details. blk2go competitors mls systems like gexcel heron ms-2 color, leica pegasus backpack, or geoslam delivers very noisy point clouds and often fragmented/layered data. by layering, i mean that flat surfaces, in reality, were presented by several surfaces of points so as a consequence slice through a wall’s face looked like many lines instead of a single line. it happened because of the mismatch between the following point clouds created by a single rotation of the scanner’s head. pict. orthographic view of the geoslam data. notice the data representing the ground in front of the building visible in the center of the picture should be shown as a single line but you can see at least 4 extra lines below the ground. blk2go point cloud impressed me from the very beginning. even in 2020 slice through a wall never occurred as several lines. there was no layering whatsoever. of course, there was noticeable noise but not so heavy in my opinion. horizontal cross-section through the vertical surface showed spaced-out points with a maximum distance of 18mm in perpendicular measurement to the sliced surface. it is much better than the leica pegasus backpack, heron ms-2 or geoslam horizon. pict. spaced-out points are visible in the gexcel heron ms-2 backpack data (without filtering/denoising). notice that the average 2d distance between the extreme points representing a single surface is approx. 30mm point cloud quality scans of the outdoor environment on a cloudy day are nicely colorized. unfortunately too bright (direct sunlight or artificial light) or too dark scan areas have applied noticeable overexposed colors so the user has to rely on a single color to recognize scanned elements. the view can be switched to black&white or an intensity color palette so it kind of solves the problem. when it comes to checking the entire walk and its coherence then results seem to be not so promising. as long as your walk doesn’t start and end in the same spot, you are mostly safe. in case you would like to scan in the loop so let’s say you walk around the building, then you will most likely end up with a mismatch of the first and last seconds of your walk. and by loop i mean the start position and the end position are the same, and scan data overlap in the start/end spot. it happens because drifts occurred along the trajectory. i assume there can be many factors that lead to distorted data and lost spatial consistency. i am listing some of them: human and road traffic nearby a scanner operator movement of neighbor vegetation e.g. leaves in the wind reflective, transparent and semi-transparent surfaces improper walk speed too even scanned environment — long corridors or tunnels without any features/things which would make the space less homogenous e.g. furniture, paintings on a wall too small features in scanned space — laser beam would hit only a few times e.g. let’s imagine we scan a room which is empty but has only lamps on the ceiling, hooks in the walls, electrical boxes, for the men eye is plenty of elements (and not only bare walls) but from the algorithm perspective view there is nothing that can help in the data processing scanner shaking to quick turns during a walk and probably a few more, i can’t remember now mentioned mismatch at the end of the walk’s loop happened every time i chose this strategy. i tested it out for outdoor and indoor scanning and in both variants, the result was the same — a mismatch! probably you wonder what was the value of it, it varied from 20cm to 120cm. at that point, my impression drastically changed 180 degrees. a very important factor is the walk duration. in my cases, walks took from approx. 7 to 23 minutes. i was assured by leica authorities that i can walk as long as i want/need. i don’t think 7 or 15 minutes of walk is long for mls equipment and it should be doable to deliver consistent data. i also think, that in a blk2go case, 30 or 60 minutes walk is too long in a matter of such a small imu implemented in this device. unfortunately, leica cyclone doesn’t give any chance to fix drifts, straight up a trajectory, or remove some unfixable pieces of data. the user has three options send the data to leica support and pray for it to be repairable take a walk again — can be costly to get back on the scanning spot create super-smart workflow small digression: i had a chat on one of facebook’s groups with the very satisfied blk2go user regarding super-smart workflow. he told me that he with his colleagues created a very good workflow consist of their scanning technics and max. 3 minutes blk2go walks which gives them satisfying results. he didn’t want to share the details, which i fully respect but mentioned that it took approx. a year to achieve a stable and reliable workflow. you, my dear reader, can decide for yourself is it long or not. i believe it is an expensive journey because during that year i can only imagine how many tests they did, how many projects they have redone, how many times mls data had to be supplemented or replaced by tls data. let’s leave the digression about building a workflow and go back to the two other options in the case of the mismatched point cloud. personally, i tried both. i have redone a walk of the same area twice— two floors of a parking lot connected by two separate staircases and a very wide parking ramp. each of these three attempts failed, so i can say it will not always be a solution to get good scanning results with a clear conscience. i did twice the same walk of another area. that was outdoor scanning of building facades — a mix of bricks, concrete, glass, and steel with a predominance of bricks. results were the same as in the parking example. you probably wonder how did i know i should redo scanning? in the parking, i noticed a mismatch in the blk2go live app, in the outdoor scanning i just did it to be sure. i also took advantage of the third option — ask leica support for help. i shared with them two samples of my raw data with the smallest misalignment — one of outdoor and one of indoor scanning. after almost a week later i got back the results of support’s data repairing. they managed to decrease error in the outdoor walk and couldn’t do anything with an indoor walk (parking lot). below you can read the message i got. outdoor walk’s error was changed from horizontal misalignment to horizontal (smaller than before) and vertical misalignment. the issues of misalignment in the point clouds are most due to the scenes that are challenging for slam as combination of few features, long walk and for one case also initialization phase that was not ideal. in the following, you will find all the details for the datasets you shared: customer 1 — outdoor: “outside.b2g” no issues are found in the dataset, and it’s switched to fallback option. i believe the offset is coming due to the complexity of the scan environment where we see quite of downhill & uphill areas. fortunately, we resolved this offset with parameter tuning as the below slices of the misalignment-free point cloud. customer 1 — indoor: “garage5–4floor.b2g” in terms of misalignment in the slam solution, the scan environment is quite challenging for slam due to less features available (barely empty parking lot) and ramp (which is the most difficult part for slam). in addition, we have seen some issue during initialization of the device. this and the complexity of the scene led to slam to fail. pict. outdoor dataset: “outside.b2g” pict. outdoor dataset: “outside.b2g” pict. indoor dataset: “garage5–4floor.b2g” as i mentioned, the result of fixing by the leica support ended up with errors you can check in the pictures below. i also don’t agree with the support’s comment regarding the indoor walk. i did three similar walks with the same results. i rather wait extra 5–10 seconds before beginning a walk with the scanner than disturbed the initialization process so this could happen as an internal scanner’s failure, i suppose. pict. 12cm error in xy plane, reduced from 130cm error by leica support. pict. when the error in the xy plane was reduced, a new error was created in the z plane - approx. 20cm. so what would happen in the case of an unfixable point cloud? probably you should go back to the project’s area to do acquisition one more time. of course, it is up to you how would you fix this issue but in my case i did the scanning with rtc360 in the case of blk2go data failure. another outdoor test walk it was one of many walks with blk2go but this one was very simple. that’s why i take it as an example of another outdoor test. i walked around the church on saturday afternoon. pict. top view of the scanning area. in the center is the scanned church. the green line represents trajectory. there was light car traffic on the street and some people walking nearby the church. i would describe it as a lazy saturday’s area. the acquisition took approx. 4 minutes on the distance of 200m so you can assume it was a very slow walk. the overall quality of the point cloud seemed to be good until i found layering of the point cloud. i found it by slicing the data with 20cm vertical and horizontal cross-sections. the misalignment among points reached 5cm. pict. the vertical cross-section through the front part of the church. notice dy values indicate layering. pict. front view of the vertical cross-section. the noticeable layering of the point cloud. pict. top view. the visible layering of the point cloud (another spot than the one above). pict. range of the collected data (mostly up to 20 meters). the last test after the firmware 2.0.2 update in mai 2021, i tested blk2go last time. the reason was an updated firmware which supposes to fix most of the known issues. were done two walks in a parking lot (another than previously) and one outdoor walk. pict. perspective view on the outdoor data. indoor point clouds looked descent, unlike outdoor data. the walk around the parking lot’s building took approx. 10 minutes and 350m. it was closed-loop - i finished the walk in the same spot where i started. pict. 350m walk took 10min and 32 sec. on the face of it, the point cloud generated from this walk looked proper. cross-sections have shown again some misalignments/layering. this time the error equaled 50cm! the point cloud of the beginning and the very end had some rotation in relation to each other. pict. marked spot with the 498mm error. pict. top view with shown layering. surprisingly enough, my colleague in leica imported the same data also to leica cyclone 360 but couldn’t find any error in the same spot. this is odd! i decided to reproduce the same error and imported the same data again. unfortunately, i have updated cyclone 360 to the newest version in the meantime, and the reimport didn’t contain the previous error. was it the incompatibility of software and firmware versions? i have no idea. anyway, an error was still there near the spot of the walk’s beginning/end. this error was about 10cm and was along the trajectory. it would signalize that imu had a shift at some point and the walk’s loop didn’t close. yes, it is not a big deal, but when you look at the issue from a wider perspective, you cannot be sure when and how errors happen, and how to avoid them. perhaps, performing a multitude of measurements would show some correlations between errors and the acquisition's environment or the method of data collection (fast vs. slow walking, the way of scanning narrow places, etc.). pict. the top view shows two (the same) walls - layering. pict. the horizontal cross-section shows the same error. as the last issue, i noticed, were differences between indoor point clouds. each of them comes from two different levels of the parking lot. a common area of them is approx. 30m of the driveway and it was scanned in each of these walks. that means i had 30 meters of common point cloud for both levels which i could use for constraining them. i gave a try for these point clouds several times and, i can promise you, it was impossible to merge them properly. first of all, they didn’t match each other because of some bending. pict. bending one of the point clouds. second of all, on these common 30 meters, one of the scans was longer approx. +/- 1,5cm. not a big deal but i wonder would this error multiply in the case of 300 meters? would we get 15cm error then? pict. comparison between indoor scans in the common 30 meters. notice the yellow color on almost the entire walls in the right driveway. useability. panorama pictures i would start this topic from take a look at the photos produced by cameras. blk2go has three 2mpx built-in cameras which are responsible for producing panoramas. my first thought was: “really? only 2mpx at times when not even high-end smartphones have 108mpx?” pano-images don’t cover 360 degrees range of view. the area where is placed scanner’s operator during a walk is not covered in the panoramas (i think it is about 60 to 70 degrees). i think it is a good solution that reduces the time of blurring/editing each individual picture in the case we don’t want to have an operator in the pictures. on the other hand, the operator has to always remember to change the walking direction so the entire scanning area is covered by pictures. unfortunately, low-quality cameras are good enough for colorizing a point cloud but rather useless for a truview which can be a huge blockage for some of leica’s customers. in good weather/light conditions, not too bright, not too dark, pictures are just ok, with poor readability, especially when you zoom. another issue with the pano images is that most of the shakes during acquisition are visible as a type of blurryish artifacts. the third disadvantage is an ugly connection between single pictures in each panorama. perhaps i am too tacky but according to me if any other laser scanner from leica’s stable delivers (except older c and hds-series) beautiful pano pictures then a user is used to the quality. we have 2021 and brands like leica cannot deliver low-quality products. no hdr ugly joins in stitching areas of single pictures very low quality of panoramas shaking artifacts visible on panoramas the very important thing that needs to be mentioned is that a user to be able to export waypoints with a point cloud needs a publisher pro license! at least in the case of exporting to e57 without importing to the cyclone. pict. poor stitching between single pictures to a single panorama pict. 3x zoom. visible artifacts, texture errors/missed pixels, and unstraight edges pict. poor quality in low-light conditions. high overexposure. pict. stitching errors and different exposure levels between subpart pictures. pict. fatamorgana effect. usability. point cloud when we consider a point cloud and its quality we have to look at two cases: accuracy and readability. accuracy, as you could read earlier, is random. you can’t predict how accurate the point cloud you will get. would it be misaligned or not, leveled or not? if you want to use it for any other purpose than visualizing, you have to consider it twice if it is worth using blk2go. readability has to be considered from the project’s perspective. in construction projects where the focus is only on the walls, floors, ceilings, beams, etc. which are relatively spacy then the filtered point cloud is mostly suitable for bim modeling. things get tough when the project requires the modeling of small elements like e.g. fire sprinklers. no matter what filtering level would be used, blk2go will not deliver details like these. this is nicely shown in the pictures presenting the underground parking lot area. on the photograph is a visible floor drain. the same area is shown from the same perspective in the point cloud (without filter applied) where mentioned the floor drain is not recognizable. that’s how unreliable data is, concerning detail modeling. pict. a floor drain is visible next to the pillar. pict. zoom to a floor drain pict. the same area in the unfiltered point cloud. the arrow shows the spot where is a floor drain. pict.black and white pipes. pict. the same area in the unfiltered point cloud with visible only white pipes. black color absorbed the laser light. usability. targets until now, we know that the point cloud is not highly detailed. would be good to supplement its data with the tls point cloud. the best and most accurate way is to use targets (b/w, spheres, or blinkers). unfortunately, leica failed again. none of those types of targets are recognizable in the blk2go point cloud. no matter what the walk’s velocity is (you can even stop by next to a target), the distance between the scanner and a target, or scanning environment - outdoor or indoor, either manual or automatic target recognition in cyclone software is not possible. it is most likely to happen because of a lack of points representing a target in a point cloud. summary blk2go is a cool device and it’s simplicity lower the entry threshold for most enthusiast of laser scanning. unfortunately, the data quality and usability of it are low. bim modeling based on this data would be difficult or very difficult depends on lod. pano pictures (waypoints) for truview are mostly useless. in my opinion, the user never knows when can rely on a point cloud generated by this device. a genuine engineering project needs precise 3d data which can be used in the entire bim cycle. lidar 3d laser scanner 3d scanning enterpreneur architects -- -- written by lukasz wiszniewski 8 followers · 0 following i am a geomatician and software developer with over decade experience in reality capturing and bim. more info read on 3d-points.com no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://3d-points.medium.com/005-does-mobile-mapping-displace-terrestrial-laser-scanners-blk2go-test-e6fdc5134462,,,"#005- does mobile mapping displace terrestrial laser scanners. blk2go test! | by lukasz wiszniewski | medium sitemap open in app sign up sign in medium logo write sign up sign in #005- does mobile mapping displace terrestrial laser scanners. blk2go test! lukasz wiszniewski 7 min read · may 27, 2021 -- listen share would leica’s blk2go replace laser scanners and deliver valuable data? did you face time limitations in your projects? let’s consider a job offer scenario where your potential customer wishes to have an update of the building documentation. this customer asks you for a laser scanning service price. unfortunately, the price seems to be too high which could be afforded by your client. what to do then? shall you dump the price or just drop this project? fot. uncomfortable position for long walks. mls from leica geosystems thanks to leica geosystem norway i had a chance to test out twice the leica blk2go scanner. it is a lightweight, portable, and wireless handheld scanner for mobile mapping. it was announced in 2019 and released in 2020. it also won the best of innovation award at ces2020. is it as good as it is described in brochures? what about data quality? can you use it for the purpose described in this article’s introduction? let’s take a look deeper into this device and the possibilities it brings. blk2go in 2020 — first look, first impression in april 2020 was my first encounter with this device. first impressions? wow! size, weight, short initialization time and generally speaking mobility! my first thought was — perfect for any projects where i have to travel by plane. easy to pack to your hand luggage and no extra cost in a project. you get the device, a total of 3 batteries which last for approx. an hour of continuous scanning, charger, usb cable, stand (which is useful during initialization of the device) and a plastic case for the scanner, and 2 batteries. as far as i remember you have to pay about 500$ for a soft bag to carry the entire set. fot. the front of blk2go when i took it into my hands then i realized that the bottom part where are mounted cameras and handle is made of solid and pleasant in touch metal — anodized aluminum. unfortunately, the top part which covers two lidar heads is made of plastic which i believe is the only way to have a semi-transparent coverage of the laser heads but gives a feeling of cheapness. it also worried me how robust is it and how any scratches would affect the quality of the data — especially possible when you would use it in industrial projects. let’s scan! leica seller gave me a really short introduction to the workflow and usage of the app. this time the blk2go live app was available only on the ios platform and compatible with iphones not ipads. luckily it is available on ipads and android devices as well! when the app was installed and ready to use, the first time popped up an error about connectivity with the scanner. reset of the app and the device solved the problem. app connected correctly to blk2go and notified me about readiness by using colors yellow/green. i clicked the button on the scanner’s housing and the app informed me what to do next. i had to hold the device stable for a few seconds to initialize it and begin the acquisition process. i used the dedicated plate which has a small bolt that helps to place the scanner correctly on the plate and prevent unwanted moves. on the app appeared first collected points which meant i could walk and scan. fot. connection with the blk2go i tried to walk quite stable and with constant velocity. i was instructed to hold the scanner, not in front of me but a bit to the site so i would be out of range panorama cameras. during the acquisition, i could see the progress of the scanning in 2d which was helpful to recognize in the space what was covered and where i should walk to collect more data. the app let’s talk about the app. it is very simple and doesn’t give you an overwhelming impression. you can either scan or browse projects saved on the device. the scanning section lets the user observe the progress of the acquisition, jump between 2d and 3d view, and change the elevation of data presented currently on the screen. unfortunately, when the user begins to manipulate the 3d view, the app loses the scanner’s position in point cloud space. it means that the view doesn’t recenter to the current blk2go position in 3d. the user has no possibility to recenter it as well. the view has to be switched to 2d and back to 3d to recenter and follow the trajectory. another issue i observed happens when you scan more than one floor in a single project. the 2d view is not following the trajectory in z-axis so when you let’s say go upstairs, the trajectory and the dot (indicating spatial scanner’s position) disappear. the view in the app still shows you the data from the floor below and not your current position. you have to change the view elevation manually by using a slider. during the scanning, you are also able to create geotags with a picture and some description which is useful in cases you know what you should focus on. the project created from your last walk gets the default name which you can rename later on. fot. screencapture of the app during geo-tagging fot. viewport doesn’t follow the scanner position in z value the second portion of the blk2go live app is browsing the stored projects and collected data. as i mentioned, here you can rename particular projects, change the thumbnail, and add a description. it is possible to check the data in 2d and 3d. unfortunately, a 3d overview of the data is not really a 3d view. it is the kind of recording of the trajectory without the possibility to see entire data at once. it is perfectly shown in the video below. data acquisition in 2020 i have done 10 walks/running with the blk2go in different environments: an office with glass walls and huge windows, a parking lot, an outdoor scanning in an urban environment, and a busy street. short summary of the walks: the office area was scanned without any trouble. some people were passing me from different directions. slam algorithm didn’t get crazy because of traffic, glass, or reflections. the parking lot was scanned smoothly. i tried slow walk approx. 1min and the same area i scanned jogging with blk2go — approx. 18 seconds. the first outdoor scanning failed either because of the glassy office walls or too few surfaces to use in slam. the second attempt i did between two buildings approx 25m away from each other. this time app didn’t throw the error. i tried several times to walk along the street. i had two buildings on both sides of the street — one glassy and one made of brick. several attempts failed. fot. the first walk with blk2go when i got blk2go in march 2021 into my hands, both firmware and the app were updated and worked much better and more stable. then i could use the scanner for several days and i scanned the same areas as a year before and completely new places in total 40 walks from 3 to 25 minutes. the slam algorithm failed twice and quite unexpectedly, but i also occurred several other problems like lidar error or connection error. fot. i got this error a couple of times. summary first of all, i am going to write the second part of this review about the data quality and reliability. i will compare it with the data from other devices like trimble x7, rtc360, or blk360 and will take a look at the possible use of this data in the scan-to-bim process. stay tuned! generally speaking, the blk2go is a handy, robust, and simple-to-use device. since 2020, leica improved the firmware and the software which makes it more stable and pleasant to use. it still has some issues with odd and unexpected errors. the app needs some minor fixes and adjustments like recentering and 3d overview. the device itself is not so comfortable to use as backpacks solutions. even though it is lightweight after a few walks you are going to feel your arm and your back. i am a rather trained person but i could feel my muscles the day after. that’s why came out some solutions like monopods or adapters to make life easier. fot. the battery level indicator. fot. blk2go live app gives instructions to a user review blk2go leica 3d laser scanning digital twin -- -- written by lukasz wiszniewski 8 followers · 0 following i am a geomatician and software developer with over decade experience in reality capturing and bim. more info read on 3d-points.com no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://karenbajador.medium.com/binance-app-is-my-initial-platform-of-choice-as-a-newbie-investor-in-crypto-35654698d584,,,"why binance is my platform of choice as a newbie investor in crypto | by karen bajador valencia | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story cryptocurrency investing in australia – journal why binance is my platform of choice as a newbie investor in crypto i have downloaded a couple of crypto exchange and wallet apps but ended up using a single platform to jumpstart my investing journey. karen bajador valencia 9 min read · dec 6, 2021 -- 1 share photo by executium on unsplash a few days back, i have committed to a new financial goal of mine – investing in crypto . at this stage, i have very little knowledge of cryptocurrencies. my plan of attack is not to deep dive into crypto coins straight away though but instead, prepare the tools i need to get started as quickly as possible. therefore, i started looking into exchanges and wallets . from what i’ve learned so far, an exchange is where people can buy and sell crypto coins and a wallet is where you stash your crypto coins securely. some exchanges, however, provide a wallet on the same platform. in this post, i’ll share what exchanges, wallets and other relevant apps i’ve installed, reasons for choosing binance and my experience with the app so far and the pros and cons of the product. i’ll share a comparison chart to compare the exchanges i’ve explored. disclaimer : the information contained herein is for informational, educational or entertainment purposes … -- -- 1 written by karen bajador valencia 349 followers · 157 following i write about cloud and data engineering, personal finance, personal development and other technology related topics. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://wh516x.medium.com/lte-call-flow-f54edefdead2,,,"how to do automation testing for lte call flow | by logicfinder | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story how to do automation testing for lte call flow logicfinder 4 min read · mar 20, 2021 -- share overview: long term evolution (lte) is the so called 4th generation (4g) mobile standard in the rapidly evolving mobile technologies. its distinguishing feature is an all ip infrastructure and significantly greater air interface bandwidth to the mobile handset. it was developed as an enhancement to the existing 3g umts system (universal mobile telecommunications system) to provide users enhanced mobile radio and internet access. the lte air interface is a completely new system based on orthogonal frequency-division multiple access (ofdma) in the downlink and single-carrier frequency-division multiple access (sc-fdma) in the uplink. the system efficiently supports multi-antenna technologies (mimo). another significant feature of lte is its high bandwidth — up to 20 mhz. the lte can also operate in the existing 5-mhz umts frequency bands, or in even smaller bands, as the usable bandwidth is scalable. however, deploying high-quality voice, video, and data services over lte has posed significant challenges for service providers, due to backward-compatibility, inter-operability, and high quality requirements — all of which requires rigorous testing. lte test solutions: provides fully automated testing solutions for lte, volte, and ims network as discussed in greater detail below. end-to-end voice, video, and data quality testing with speech metrics such as pesq, and… -- -- written by logicfinder 5 followers · 2 following wajid hassan is a ph.d. fellow in technology management at indiana state university, usa. he is a technology evangelist and a fierce promoter of stem education no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/data-science/coronavirus-a-big-data-lesson-from-south-korea-5bb703b8b0ae,,,"coronavirus: a big data lesson from south korea | by filip dzuroska | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. coronavirus: a big data lesson from south korea how south korea fights coronavirus (covid-19) with data & ai filip dzuroska 7 min read · apr 28, 2020 -- listen share image by gerd altmann from pixabay m ost countries are still under heavy quarantine restrictions while others are opening the economy and returning to normal life. the impact of the coronavirus varies between countries massively. some are facing robust challenges and struggling to get the outbreak under control. still, countries like south korea, singapur, and taiwan were able to react to the situation almost instantly and bounce back faster. back in february 2020, south korea was reporting one of the highest daily rates of coronavirus spread in the world. besides mainland china, the number of infected people was the highest, and the forecast was predicting anything, but the worst-case scenarios. own visualization, data source: john hopkins university but as the graph suggests, the curve’s started to flatten much faster than initially anticipated, and the situation was maintained. how is it possible that south korea has been able to respond to the case so immediately and effectively? lesson learnt south korea has already faced a similar situation but on a much smaller scale. in 2015 a korean traveler, who came back to seoul from the middle east region, brought with him an infection known as mers-cov, also caused by a coronavirus. due to virus novelty in the country, it took a couple of days until the illness was correctly diagnosed. by the time the virus was confirmed, it has been further transmitted, and it seemed almost impossible to trace who was infected. the infection caused 186 mers-cov cases in the country and resulted in 14-day quarantine for 16,993 koreans and an economic loss of 8.5 billion us dollars. besides saudi arabia, south korea had the highest number of infected people globally: own visualization, data source: world health organization although the virus appeared quite unexpectedly, its further progress was prevented relatively quickly. rapid testing of the people and the use of various types of advanced technologies, data analytics, and information about the population have been proven as practical measures. patient 31 and the spread of corona virus in south korea the first case of the virus in south korea was confirmed on january 20. an infected lady traveling from chinese wuhan was isolated, and officials managed to keep the situation under control. all has changed with the “patient 31”. before tested positive, the patient 31 was in a minor accident, checked in a hospital twice, and visited a service at a church. she has been in contact with hundreds of people and has successfully transmitted the virus to many of them. since then, south korea confirmed thousands of virus cases in the time of a few days, and the numbers grew steeply, and the outbreak was the biggest outside mainland china. flattening the curve in 20 days with big data, ai and smart tech south korea did a great job when it comes to the prevention of the further spread of the virus, and there are several reasons why. learning from the experience with merc, they were able to apply effective measures like: smart contact tracing aggressive testing & diagnostics telemedicine ict and smart technologies smart contact tracing contract tracing is an excellent example of how to make big data working at scale and bring actionable insights. image by openclipart-vectors from pixabay (cc0) when we’re coming back to the story about our patient 31, the officials were able to track her movement, people she came to contact with, and alert everyone who might be at risk. the contact tracing app is based on different kinds of data: geospatial time series image and video transactions others information about location might be tracked through several sources: gps data from smart devices, credit card transactions, or others. once combined, a comprehensive picture of a confirmed patient’s movement is shared publicly. the app gives information about the history, timeline, and the location visited by the individual. restaurant and hotel visits, which transportation means they used, which line or bus number they took, even the exact seat in the movie theater is tracked. cctv cameras are also used extensively when monitoring and analyzing virus transmission. there are more than 1.1 million operational cctv cameras in south korea every day. based on statistics from 2010, each citizen is capture by the public cctv system on average of 83.1 times every day, and the trend has grown over the years. smart contact tracing reduces the reaction time of authorities to identify a potential threat, isolate transmitters of the virus, and notify everyone who might be in danger. the public app also informs each user, which areas were visited by confirmed patients, which places are under quarantine, and who might be potentially affected. the system also tracks the activity of the virus and helps hospitals and clinics to be better prepared in case of the next wave. test kits development and diagnostics with ai merc experience taught korea another lesson - ”aggressive testing”. before major spread by super transmitter event, there were only 30 confirmed cases compared to 75.000 in china. despite the low numbers, south korea started to cooperate with biotech companies on test development for coronavirus. they’ve developed test kits almost instantly and distributed them into every hospital in the country. korea didn’t face the problem with an insufficient number of tests like other countries did. thanks to cautious planning and utilization of ai, they were able to flatten the curve at a super-fast pace. “to develop a test in such a short time would not have been possible without ai,” said tai-myoung chung, professor in the department of interaction science at sungkyunkwan university ( skku ). the design of test kits was developed with the use of artificial intelligence. ai helped to shorten the period needed to design and build test kits. the genetic make-up of the virus usually takes 2–3 months. the company called seegene managed to develop tests in less than three weeks, thanks to automation and ai. artificial intelligence-based big data solutions enabled scientists to understand the genetic make-up of coronavirus rapidly. photo by cdc on unsplash moreover, ai is allowing health personal to screen accurately and speedily diagnose patients. the main application of ai is on large-scale chest x-ray data to identify symptoms of coronavirus in a couple of seconds and address patients in the critical state. ai is used to classify patients into four categories: mild, moderate, severe and very severe telemedicine besides ai-based solutions, there is also telemedicine, supporting social distancing and enabling patients to stay in touch with the healthcare personnel. remote medicine is targeting either potential or confirmed patients to monitor the symptoms and further development of coronavirus. photo by national cancer institute on unsplash the whole examination is established via video call with the use of smartphones. all the symptoms are included in the database, and the patients are advised on the next steps. the solution is enabling professionals to check symptoms of confirmed patients in real-time and transfer those who need medical assistance to the closest hospital. information sharing apps other crucial parts of the whole korean system against coronavirus are mobile apps. there were many of them developed within a couple of days. the main objective is to inform the public about coronavirus from different perspectives. some of them are providing general information on symptoms. others will let you know about the place with available face masks or the closest place to be tested. there are also ai-powered applications using chatbots to call people who need medical attention. south korea is a great example of how ai and big data should be applied for social good — reducing reaction time, increasing accuracy, and deploying solutions throughout the whole country. many things need to be addressed. privacy, data security, and information sharing cannot be left aside. thanks to the country’s transparent and consistent government, culture, collective effort, technological maturity, and eagerness to innovate, south korea managed to overcome the potential risks when facing these challenges. follow me on linkedin: filip dzuroska - data scientist - erste group it | linkedin view filip dzuroska's profile on linkedin, the world's largest professional community. filip has 1 job listed on their… www.linkedin.com article sources: south korea winning the fight against coronavirus using big-data and ai south korea is fighting the novel coronavirus (covid-19) by relying on its technological forte. the country has an… www.thedailystar.net how countries are using technology to fight coronavirus the rapid spread of covid-19 has forced countries to use every trick in the book to contain the disease. some… economictimes.indiatimes.com middle east respiratory syndrome: what we learned from the 2015 outbreak in the republic of korea middle east respiratory syndrome coronavirus (mers-cov) was first isolated from a patient with severe pneumonia in… www.ncbi.nlm.nih.gov kosis edit description kosis.kr 보도자료 | 국가인권위원회 edit description www.humanrights.go.kr coronavirus: south korea’s success in controlling disease is due to its acceptance of surveillance south korea has been widely praised for its management of the outbreak and spread of the coronavirus disease covid-19… theconversation.com ai and control of covid-19 coronavirus this document is also available in: this publication intends to provide a non-exhaustive overview of articles from the… www.coe.int covid-19: how korea is using innovative technology and ai to flatten the curve by itu news the republic of korea has been successful so far in containing covid-19 without shutting down its economy… news.itu.int lunit releases its ai online to support healthcare professionals manage covid-19 lunit, a medical ai software company that develops ai-powered analysis of lung diseases via chest x-ray images, today… lunit.prezly.com ai data science coronavirus big data -- -- published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by filip dzuroska 16 followers · 42 following data scientist in financial services, former researcher at ucla, currently based in vienna, austria www.linkedin.com/in/filipdzuroska/ no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/bigeye/dependency-injection-101-what-and-why-7bd11d53c528,Mockito,Unit Testing,"dependency injection 101 — what and why | by aj ribeiro | bigeye | medium sitemap open in app sign up sign in medium logo write sign up sign in bigeye · treating data quality like an engineering problem. dependency injection 101 — what and why aj ribeiro 4 min read · jun 23, 2020 -- 2 listen share [ part 2 of this series , part 3 of this series ] i recently instrumented toro data labs’s codebase to follow the dependency injection (di) pattern. following di can provide us with several benefits which will be discussed in the second section of this post. the first time i encountered di, i found it confusing and jarring, but luckily i had great mentors who took the time to sit with me and break it down. not everyone is as lucky though, and i thought it would be worthwhile to write a post to help explain it to folks who have a similar experience to mine. although i will be writing this in terms of java, the concepts are applicable to any language. what is dependency injection di is a coding pattern where dependencies of a class are injected into the class via its constructor. for example, let’s say that we have a house, which needs a bathroom and a kitchen. similarly, a bathroom needs a sink and a shower, and a kitchen needs a sink and a stove. we would then say that class house has dependencies on class bathroom and class kitchen. class bathroom then has dependencies on class sink and class shower, while class kitchen has dependencies on class sink and class stove. these classes could look like so: another aspect of di is the use of interfaces . interfaces define the behavior of a class, but not the implementation of that behavior. to go back to our example, bathroom has a sink and shower dependency, and kitchen has a sink and stove dependency. but as we know, kitchen sinks and bathroom sinks, although having similar behavior, have slightly different properties. so we will make sink an interface with two implementations. the classes would then look like so: now, i have all my classes, and all of my classes have constructors such that their dependencies are injected via their constructors. as such, i can build a house in my main function like so: as you can see, all classes are receiving their dependencies through their constructors. this is the di pattern. okay, so what? why should i use di? now that we know what di is, why should we use it? the answer is that this pattern brings us several benefits over generating dependencies other ways (e.g., within a constructor). separation of concerns in this architecture, each class only needs to worry about what is going on inside the class. for example, a house doesn’t have to worry about how to construct a bathroom, and likewise a bathroom doesn’t have to know how to construct a shower. all of that is handled by the creator of the object, in this case our application’s main. in the second post in this series we will discuss di tools which make the creation process even simpler. configurability consider the example of the sink interface. both bathroom and kitchen need a sink, and neither of them care which implementation of sink they get. for this example, bathroom gets bathroomsink and kitchen gets kitchensink, but there’s no reason that both couldn’t get kitchensink. or in the future if some great magicsink is invented, they could both get that. if we go back to our brick and mortar house example, consider the sink hookup in the kitchen. if the hookup in a kitchen is standardized, then any sink at the hardware store which meets that standard can be installed in my kitchen. the standard hookup can be thought of as an interface and the sink we purchase can be thought of as the implementation. ease of testing as a unit-testing zealot, one of my favorite aspects of di is how testable it makes your code. for comparison, let’s consider a house class that doesn’t use di : if i wanted to unit test this, i would want a mock bathroom and mock kitchen, but there is no straightforward way to get these dependencies into house. the only way to do this would be to use powermock , which is a great tool for unit testing legacy code, but should be avoided in favor of following better coding patterns for new services. if we now go back to our di-following house, all of the dependencies can be mocked and then injected into the constructor to make unit tests easy to write. for example, if we wanted to test the house class (using mockito): as we can see in the test code above, all of the dependencies of house are mocked, which will make it very easy to unit test only the behavior of house. what’s next? okay, so di the pattern is cool, but we also have a lot of boilerplate in our main method. luckily, the good folks at google and square have written tooling ( guice and dagger[2] ) to help us reduce the boilerplate that we need. in dependency injection 102 , we will discuss what these tools are and how to use them. this code is available to clone from github . java programming programming tips dependency injection guice -- -- 2 published in bigeye 227 followers · last published mar 21, 2023 treating data quality like an engineering problem. written by aj ribeiro 71 followers · 40 following rocket-ship builder, pizza expert responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://kovidrathee.medium.com/on-being-a-virus-dbaf8520cb1,,,"on being a virus. sharing my name with the novel… | by kovid rathee | medium sitemap open in app sign up sign in medium logo write sign up sign in photo by cdc on unsplash coronavirus on being a virus sharing my name with the novel coronavirus, covid-19 kovid rathee 2 min read · mar 11, 2020 -- 9 listen share hi, i am covid but with a k. my name comes from an ancient language. in sanskrit, the literal meaning of kovid is intelligent or poet. people keep asking me — what does your name mean? it’s usual for indian names to have a literal meaning. i was quite fed up with introducing myself as an intelligent guy or a poet until the who gave my name a new life . now, i am a virus, too—a n intelligent, poetic virus. growing up, i never aspired to be a virus; that was never part of the plan. i always wanted to do something related to computer engineering, music, cricket, cosmology, and many other things. i also wanted to be intelligent and a poet, too. but life surprises you in all kinds of funny ways. i see poetic justice when i connect myself with this virus through this name. how, you ask? well, i have always been a parasite, feasting on all the money and wasting all the resources i could get from my family. that’s pretty much been my life. so, in a way, i have already been showing traits of being a virus. now, the who has just confirmed it. my name is kovid — a sanskrit word. i have been called kobid, koveed, kovit, kewvid, kovee, govid, govith, kovind, govind, koovid, kuvaid and also komal (i have no idea what went down there). and i have faced this in my home country, sometimes even in my own city. it’s been a recurring theme, explaining how to pronounce this word and what it means. the good thing to come out of this covid-19 nomenclature for me is that now, at least, people will know how to pronounce my name. a couple of months ago, i stumbled upon a piece by shruti das about how she had mispronounced her name for 20 years. a lot of people around the world face this problem. i don’t take it too seriously, but i still responded to her post with admiration. oh and by the way, wash your hands people. please find more information here on who’s website . viral health humor random names -- -- 9 written by kovid rathee 3.3k followers · 273 following i write about tech, indian classical music, literature, and the workplace among other things. 1x engineer on weekdays. responses ( 9 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://kovidrathee.medium.com/what-christopher-hitchens-can-teach-us-about-writing-28764464d82b,,,"“what christopher hitchens can teach us about writing” - kovid rathee - medium sitemap open in app sign up sign in medium logo write sign up sign in photograph by john dempsie, c. 1978 ad infinitum “ what christopher hitchens can teach us about writing ” kovid rathee dec 5, 2020 -- listen share earlier this year, i wrote about what christopher hitchens can teach us about writing. summarizing his ideas in a few points here is writing something you can’t not do find a voice write more like the way you talk don’t depend on booze write to please yourself it matters not what you think, but how you think sam harris, while speaking at an event just after christopher’s passing away said, the man had more wit and style and substance than a few civilizations i could name i couldn’t agree more. christopher hitchens writing journalism authors blog -- -- written by kovid rathee 3.3k followers · 273 following i write about tech, indian classical music, literature, and the workplace among other things. 1x engineer on weekdays. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@houseoftest/superhero-personas-5a6db21b4c4a,,,"superhero personas. using personas in your testing can be a… | by house of test | medium sitemap open in app sign up sign in medium logo write sign up sign in superhero personas house of test 8 min read · feb 18, 2020 -- listen share using personas in your testing can be a powerful tool to try and broaden your perspective and trigger new test ideas. thinking and acting like a certain persona can find bugs you normally wouldn’t find since it is easy to get stuck in your own way of thinking and doing things. there are many different personas you can use but often they are created as a possible end user. though another way to help you think a little more outside the box is to base them on different superheroes. say you have some scenarios you have to do every month to look out for regression, switching it up with superhero personas might make it more fun and help find new problems. then you can switch e.g. every month to a new theme like star trek, game of thrones, lord of the rings, days of our lives, macgyver or any other theme you can think of. everytime you switch theme it forces you to think about what the characters imply and how it could effect your product and hopefully in the process give you new test ideas. the plan is to trigger thoughts about how they could be used in your context. maybe the superheroes mean something different to you, maybe they imply other uses of your product or maybe you need a completely different set of superheroes / theme to get your mind triggered. below are a couple of superheroes and some quick examples what they might represent. what test ideas comes to mind when you think about them? the flash ● do everything as quickly as possible e.g. go through a step by step extremely fast. ● focus on response speed and shortcuts. the hulk ● do many heavy calls (upload/download large files, post big texts, …). ● use big data. ● stress test to focus on seeing how much the system can handle. ● handle the product with rage, can it handle it? mr fantastic ● re-size items (e.g. window size, screen resolutions …). ● use different size of strings in input fields. ● stretch the product as far as possible. rogue ● just as rouge can absorb the power of anyone she touches you can try and absorb the knowledge and skills of the people around you that you think are good at what they do. find out what it is that you think they do good and copy it, try and get some pointers or even get a coaching session. jessica jones ● she is a super strong, depressed, drunk with good detective skills. so how would a smart angry drunk handle your product? is the usability so good that even a really drunk person could handle it? spiderman ● jump back and forth between pages (web swinger). ● are passwords saved? how does page cache work (sticky web)? ● what does your intuition tell you to focus on (spider sense)? nightcrawler ● jump in into the middle of a state flow or do things in wrong order (teleport). superman ● well this guy can do almost everything so if you only want to use one, this might be your persona. ● go into the code, is there anything obvious wrong, something that is complicated and hence might contain bugs or some comment giving you a hint that the developer is unsure of that part of the code (x-ray vision). ● what is the one worst thing that can happen to your product, try to trigger this (kryptonite). black widow ● black widow doesn’t possess any superpowers her abilities comes from extensive training. so even if you don’t possess a superpower for something doesn’t mean you can’t be good at it, you just need to practice. batman ● not only a persona but also your role as a tester. you need to take on the job as the world’s best detective to figure out what can be wrong, where to look and so on. ● do things in odd hours (nights, weekends). ● make use of all your skills and tools you have in your belt to attack your product in many different ways (utility belt). nick fury ● don’t be afraid to collect a team of awesome people to help you out with the testing, collecting ideas or to understand a problem. ● pair testing. green arrow ● identify and hit important/critical areas with high precision. zatanna ● finding some bugs can sometimes appear as magic when it is the result of good testing. it may be a good risk analysis, an understanding of how customer thinks or by being observant to the details. although compared to a magician it is usually good to reveal your tricks to other testers to share knowledge. ironman ● focus on the hardware. ● if there are things a computer can help you with, let it. ● if there is a tool that would help you but doesn’t exist, create it. mr. mxyzptlk ● what happens if you do everything in reverse? start at the end. ● break the 4th wall (do pair testing with a developer). ● the 5th dimension is imagination. what would you pull out of there to help you? what if you had no boundaries? invisible woman ● what happens when your app / program is minimized or runs in the background? wolverine / deadpool ● how does the system repair itself after a failure or error (regeneration)? ● how does the product work during a long time (slow aging)? ant man ● how does your product work with small values, zero, null? ● swarm your product with many small test scripts (army of ants). ● how does your product handle dust or small grains of sand? wonder woman ● use your lasso of truth on your developers to make them reveal part of the product they are worried/ashamed/oblivious of. iceman ● paus/freeze the system in different states. ● slow down the system (slow internet, slow computer). ● how does your product handle cold weather (battery life, sluggish response). scarlet witch ● she has the power to alter reality, often so do you in your test environment. create an reality that shouldn’t happen (because if might happen anyway). what can you learn from this “unrealistic” reality. green lantern ● no idea, but you can’t skip green lantern when having a list of superheroes. aquaman ● how does your product handle water? ● how does your product communicate with other products (telepathy)? ms marvel ● she has a limited precognitive sixth sense, so do you. use your sixth sense to try and feel which areas are high risk areas and focus you testing there. captain america ● how does your product handle different time zones, currency and units (maybe far fetched but he is from america and i’m swedish). ● are you protected from attacks (shield)? ● what does your product do better than all competitors, how will it achieve world domination? (well you know, usa, usa, usa …). storm ● expose your product for different sorts of weather condition. how does your product handle strong winds, snow or extreme sunshine? daredevil ● don’t only use your eyes, what does your other senses tell you? is there a strange sound, do you smell something, does the product get hot and so on. ● how would you defend your product in court (lawyer)? and don’t forget the super villains. sometimes you need to treat your product really harsh. finally, if you need some more inspiration for more about about superheroes see this great video about “10 lessons entrepreneurs can learn from superheroes”. https://www.youtube.com/watch?v=fbgt8tvjzxq /andreas cederholm test testing personas -- -- written by house of test 23 followers · 4 following what we do, we do for the love of testing! we only settle for greatness, because you expect nothing less from us! no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@michael.s.lindon/a-better-way-to-test-for-sample-ratio-mismatches-srms-and-validate-experiment-implementations-6da7c0d64552,,,"a better way to test for sample ratio mismatches (srms) and validate experiment implementations | by michael lindon | medium sitemap open in app sign up sign in medium logo write sign up sign in a better way to test for sample ratio mismatches (srms) and validate experiment implementations michael lindon 8 min read · nov 12, 2020 -- 1 listen share …or why i don’t use a chi-squared test. preamble: this post is intended to be a gentle introduction to our latest whitepaper which describes a new methodology for validating experiment implementations through sequential testing of sample ratio mismatches (srms) . the novelty of this work is that it presents a new sequential statistical test for multinomial data. introduction: what and why? in my time as a statistician at optimizely, one of the most frequent questions i encountered from customers concerned the bucket health of their experiments and sample ratio mismatches (srms). a sample ratio mismatch is a colloquial term given when the total units in each treatment group differ significantly from what would be expected in an experimental design with random assignment, each treatment group having its own assignment probability. as an example, consider an a/b test in which each experimental unit has an equal probability of receiving the treatment or control. if this experiment resulted in 3123 and 6877 units in the treatment and control groups respectively, then one may doubt its validity. informally, this test is said to exhibit an srm. the two primary concerns when encountering an srm are problems in the assignment mechanism, potentially having a non-ignorable assignment bias, or problems in the data collection, where data is potentially missing not at random. realistically, online experiments can be difficult to implement and execute correctly. there are large engineering requirements, which may contain bugs, and data processing steps, which may contain incorrect logic, that can render causal conclusions from experiments invalid. to address this, many have popularized the method of testing for srms. see for example lukas vermeer’s keynote speech “one neat trick to run better experiments” or ronny kohavi’s talk on “trustworthy a/b tests” . indeed, srms can reveal a wealth of implementation errors, and their causes and fixes have been well documented in the experimentation literature, “one of the most useful indicators of a variety of data quality issues is a sample ratio mismatch (srm)…” — fabijan et al there is also the chance to rewatch our talk from test and learn 2020 why i don't like chi-squared tests (and friends): to the credit of those popularizing these ideas, it has now become a best practice to validate the experiment implementation by performing an srm test. for the most part, this checks that the assignment mechanism and the data processing steps are performing as expected. the usual procedure is to perform a chi-squared test on the total units observed in each treatment group against the intended assignment probabilities of the design, at the end of the data collection and prior to analysis. there is nothing wrong with this statistically, but there is one obvious flaw. the issue is that one learns about a problem with the implementation only after all data collection is completed, which is arguably far too late. ideally one seeks to validate the implementation at the outset, yet if performed too early, the chi-squared test may not have enough power to reject the null and implementation errors may go undetected. importantly recognizing that the type-i, or false positive, error probability guarantee of the chi-squared alone only holds when it is performed once - when should it be performed? the tension between running the test early enough to prevent wasted units but late enough to have sufficient power makes it very difficult to answer this question. ultimately the chi-squared test, and fixed-horizon relatives, are inflexible. this has led to the proliferation of some bad practices in the experimentation space. given the aforementioned difficulty, many practitioners incorrectly continuously monitor their experiments by repeatedly performing significance tests, usually in an ad hoc fashion, without any multiplicity correction, unknowingly increasing their chances of a type-i error. the following situation is commonly observed. an experimentation team may have some doubt over the validity of an experiment implementation, and decide to run a chi-squared test to investigate. the chi-squared test does not reject the null, but in spite of this, the team’s doubt remains. they conclude that the chi-squared test probably didn’t have enough power, deciding to run the test again tomorrow. this continues for a few iterations until the null is eventually rejected, resulting in a false positive. why does this happen? well, a classic paper “ repeated significance tests on accumulating data ” by peter armitage showed that the probability of obtaining a false positive using a chi-squared test configured at the 0.05 level can increase up to 0.14 with as few as five usages. by allowing the decision to perform a new test depend on the outcome of a previous test, one risks sampling to a foregone conclusion. i’ll illustrate how problematic this can be in the next section. false positives of chi-squared test under continuous monitoring let’s take this to extremes, for the sake of example, and perform a chi-squared test after every new datapoint, which in this application corresponds to recording a new unit in a treatment group. let’s suppose there are five groups in this experiment (1 control, 4 treatments), and each unit is assigned to each group with a probability of 1/5. these assignment events can be simulated by creating a sequence of multinomial random variables (of size 1). an important point to stress is that these random variables have been simulated using the assignment probabilities of the intended design so that the null hypothesis is true in all simulations. after each new multinomial random variable let’s perform a chi-squared test, configured at the alpha 0.1 level, on the accumulating set of data. the figure below shows the resulting p-value over time up to when the chi-squared test rejected the null, displayed using the red dot. p-value resulting from chi-squared test over an accumulating set of multinomial data. null rejected at approximately 300th multinomial draw (red dot). p-value threshold at 0,1 (dashed black line). this is clearly not the outcome we seek as the data was simulated under the true null hypothesis. to see if we were just unlucky, let’s simulate a further 100 experiments. p-values from the chi squared test on 100 accumulating sets of multinomial data simulated under the null hypothesis. p-value threshold at 0.1 (dashed black line). null rejected when p-value falls below 0.1 (red dots). 79 out of the 100 experiments resulted in the rejection of the null hypothesis, and this a conservative number as we only looked at the first 1000 datapoints. this comes to the surprise of many who mistakenly believe that the type-i error probability of a chi-squared test, configured at the 0.1 level, is 0.1 no matter how many times it is performed. a new sequential test the previous section made the argument that chi-squared test by itself is somewhat impractical for detecting srms because it is hard to get the timing right. too soon and it might be underpowered, too late and much of an expensive experiment is wasted. furthermore, we have established that when used incorrectly, through repeated tests without multiplicity corrections as seems to be common in practice, that type-i error increases dramatically. at optimizely we recognized the value in validating customer implementations but found the current tooling to be lacking. we sought to develop a new statistical test that could be performed after every datapoint, so that srms can be detected as early as possible, while still controlling type-i error at the customer's desired level. these are exactly the properties possessed by our newly proposed test described in our latest paper: sequential testing of multinomial hypotheses with applications to detecting implementation errors… simply randomized designs are one of the most common controlled experiments used to study causal effects. failure of… arxiv.org at optimizely we refer to this as our ssrm (sequential sample ratio mismatch) test. to illustrate how the performance of the ssrm-test compares against the chi-squared test, let’s consider the sequential p-values delivered for the same simulations. to start, let’s look at a single simulation: sequential p-value from the ssrm-test on an accumulating set of multinomial data simulated under the null hypothesis. sequential p-value threshold at 0.1 (dashed black line). notice how the sample paths of the sequential p-value are piecewise constant and the test correctly did not reject the null. again, is this just luck? let’s simulate a further 100 such experiments under the true null hypothesis. sequential p-values from the ssrm-test on 100 accumulating sets of multinomial data simulated under the null hypothesis. sequential p-value threshold at 0.1 (dashed black line). null rejected when p-value falls below 0.1 (red dots). only 7 of the 100 experiments rejected the null, which is in line with what one should expect for a test configured at the 0.1 alpha level and data simulated under the null. note that the probability of making a type-i error using the ssrm-test is controlled no matter how many samples are collected, and no matter how many times it is used. rapid detection of errors the previous simulations illustrated the behaviour of the ssrm-test when the null is correct and demonstrated control over type-i errors. what about type-ii errors? suppose there is an implementation error in the experiment setup and units are being received at the end of the pipeline with a different probability than the intended design. let’s investigate this by simulating multinomial random variables with a slightly different assignment probability, specifically, [0.2, 0.2, 0.2, 0.1, 0.3] instead of being equiprobable. sequential p-values from the ssrm-test on 100 accumulating sets of multinomial data simulated under the alternative hypothesis. sequential p-value threshold at 0.1 (dashed black line). null rejected when p-value falls below 0.1 (red dots). the ssrm-test resulted in zero type-ii errors in this simulation study, rejecting the null within the first few hundred datapoints. the value to the experimentation team is great, as this allows the implementation to be validated right at the beginning of the data collection, in contrast to the end. conclusion in most experimentation teams the responsibility usually falls on the experimenter to manually perform an srm check, rarely is it build as part of the platform. arguably this validation should be performed automatically on every test. sequential tests, with their ability to be performed after every datapoint, are ideal methods for testing hypotheses about streaming data — exactly the type of data received from running online experiments. it has been well established that srms are excellent signals of underlying implementation issues in randomized experiments. for this reason, srm testing methodology has garnered a lot of attention (see papers by yahoo , linkedin , microsoft and this blog post by twitter ). existing methodologies have certain limitations, as described in this post, and so optimizely has developed a new sequential statistical test that allows srms to be detected as soon as possible while still controlling type-i error. the details can be found in our new whitepaper . experimentation a b testing statistics data science machine learning -- -- 1 written by michael lindon 49 followers · 8 following senior data scientist at netflix experimentation platform. linkedin https://www.linkedin.com/in/michaelslindon/ and twitter https://twitter.com/michaelslindon responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@colin.fraser/target-didnt-figure-out-a-teen-girl-was-pregnant-before-her-father-did-a6be13b973a5,,,"target didn’t figure out a teenager was pregnant before her father did, and that one article that said they did was silly and bad. | by colin fraser | medium sitemap open in app sign up sign in medium logo write sign up sign in target didn’t figure out a teenager was pregnant before her father did, and that one article that said they did was silly and bad. colin fraser 8 min read · jan 4, 2020 -- 10 listen share in 2012, a story was published in the new york times under the headline how companies learn your secrets . the article discusses, among other things, how and why a marketing team at target tried to build a model to predict which shoppers were pregnant. partway through the article, there is an anecdote about a year after pole created his pregnancy-prediction model, a man walked into a target outside minneapolis and demanded to see the manager. he was clutching coupons that had been sent to his daughter, and he was angry, according to an employee who participated in the conversation. “my daughter got this in the mail!” he said. “she’s still in high school, and you’re sending her coupons for baby clothes and cribs? are you trying to encourage her to get pregnant?” the manager didn’t have any idea what the man was talking about. he looked at the mailer. sure enough, it was addressed to the man’s daughter and contained advertisements for maternity clothing, nursery furniture and pictures of smiling infants. the manager apologized and then called a few days later to apologize again. on the phone, though, the father was somewhat abashed. “i had a talk with my daughter,” he said. “it turns out there’s been some activities in my house i haven’t been completely aware of. she’s due in august. i owe you an apology.” shortly after the publication of the initial article, a tech writer at forbes highlighted this anecdote in an article under the headline how target figured out a teen girl was pregnant before her father did . apparently she knew how not to bury the lede. people are still talking about this from there, the story exploded into the general consciousness in a big way. a quick twitter search for “target pregnant daughter” shows that people still talk about this story a whole lot over 7 years after the initial publication of the article. generally it’s used to show how big data can be used to learn everything about us, how the facebooks and googles and targets of the world know more about us than our own family members. but the truth is that this anecdote and much of the discourse surrounding it is, if you’ll pardon my language, silly and bad. let me count the ways. 1. it’s probably just not true i won’t belabor this since we can’t actually know, but the anecdote about the father calling the store and talking to the manager is probably just not true. it’s sourced, not to the protagonist statistician pole, but to “an employee who participated in the conversation”, which raises questions. how did the author of this piece come to learn of this anecdote? how many targets did he have to visit to find an employee who had had a conversation like this? how does he determine that the incident occurred “about a year after pole created his pregnancy-prediction model”, and crucially, how does he determine that the coupon book was sent to the girl because of the model? the forbes writer who rehashes the anecdote even describes it as “so good that it sounds made up.” indeed. but this is an easy criticism and beside the point, so for the remainder of this article, let’s suppose that the anecdote is true. 2. theres no meaningful sense in which this anecdote shows that target’s algorithm predicted the girl was pregnant this story is intended to show that target’s big data operation, and moreover the big data operations of all of the various retail and tech giants that we interact with, make predictions about intimate details of our lives with astonishing precision. but what does it actually show? a girl received a coupon book featuring maternity items. target probably sent out many similar coupon books to many people. if target just sent out maternity coupon books completely at random, this exact scenario could have still happened; some of the randomly assigned coupons books would certainly reach pregnant women by chance, and some of those pregnant women might have had fathers who didn’t know that they were pregnant, and one of those fathers might have gone to a store to complain. this story doesn’t even show that target tried to figure out whether the girl was pregnant. it just shows that she received a flyer that contained some maternity items and her weird dad freaked out and wanted to talk to the manager. there’s no way to know whether the flyer arrived as a result of some complex targeting algorithm that correctly deduced that the girl was pregnant because she bought a bunch of lotion, or whether they just happened to be having a sale on diapers that week and sent a flyer about it to all their customers. 3. even if target’s algorithm did predict that this girl was pregnant, the anecdote shows nothing about how good the algorithm is at predicting pregnancy the forbes article claims that this story “conveys how eerily accurate the targeting is”, but in fact it shows precisely nothing about how accurate target’s targeting is — it just shows you that the targeting worked at least one time. “accuracy” is a term of art in machine learning. the accuracy of an algorithm is the fraction of times that the algorithm is correct out of the total number of predictions that it makes. clearly, a single anecdote can tell us nothing about the accuracy of an algorithm. so why does the forbes writer believe that this story demonstrates an eerie accuracy? there’s a fallacy that i’ve noticed in a great deal of popular writing about ai. i’ll call it the superhuman fallacy. the superhuman fallacy says that if an algorithm predicts a case correctly where a human, especially an expert, was wrong, then the algorithm must make more accurate predictions than the human on average. of course, this does not follow at all. generally, humans and algorithms will make different kinds of mistakes. humans might be a bit worse than self-driving cars at staying in the center of the lane, but they’re a lot better at spotting a stopped firetruck in the middle of the road . the set of errors that they make is different, and we make any claims about the relative performance of humans and machines by looking at a single example. how many pregnant target customers didn’t get the coupons? there’s really no way to know, because the fathers of the pregnant girls who didn’t get coupons never had the chance to complain to the manager of their local target. that question is exactly what we would need the answer to if we wanted to know how whether the algorithm was “eerily accurate”. why am i writing about some random forbes blog post from 2012? i’m probably wrong, but as far as i can remember, the story about target figuring out that the girl was pregnant was the first big story in an entire decade’s worth of stories where an algorithm was the subject of the story. writers didn’t used to write about algorithms. they wrote about people and places and physical things and systems, but not so much about algorithms. but this story in 2012 launched the idea into public consciousness that companies can create algorithms that can diagnose, solve, predict the future, and generally model the human situation better than humans can. get hyped, folks! this has led to an extreme wave of confidence in the efficacy of algorithms (whatever those are) or ai in general to figure things out . algorithms , the narrative goes, are now better than humans at figuring out whether you’re pregnant, or you’re about to quit your job, or whether you’ll commit a crime if you’re let out of prison. but a lot of algorithms actually just kind of suck. a little known secret is that it’s very hard for even experts to build, productionize, and maintain an algorithm that makes accurate predictions about anything, let alone human behavior—and despite what you’ve heard, most companies are generally not doing a good job of it. and it’s even harder to build these algorithms such that they won’t incorporate and magnify systems of discrimination and oppression. our popular discourse does not have the vocabulary to distinguish between useful machine learning algorithms and snake oil [pdf warning] , so we end up writing about ai in terms of anecdotes and fall victim to the superhuman fallacy. this is how we end up with things like cops arresting people on the basis of facial recognition techniques that are successful less than 10% of the time. thankfully, such a vocabulary does exist. the practitioners who build these ai systems care a lot about characterizing their efficacy, and have developed myriad ways to describe it. and a lot of it is not so hard for a layperson to understand—there are a few simple questions that the author of the story could have asked to demonstrate the effectiveness (or lack thereof) of the pregnancy algorithm. out of all the predictions that the algorithm made, how often was it right? this is the accuracy of the algorithm. when the algorithm predicted that a woman was pregnant, how often was it wrong? this is the false positive rate of the algorithm when the algorithm predicted that a woman was not pregnant, how often was it wrong? this is the false negative rate of the algorithm. when the algorithm predicted that a woman was pregnant, how often was it right? this is the precision of the algorithm. out of all of the pregnant women in target’s database, how many of them did the algorithm find? this is the recall of the algorithm. not all of these questions need to be answered in every piece (in fact, some of them are derivable from the answers to the others), but some of them should be if you want to have any hope of characterizing whether the algorithm is any good. but generally these types of statistics aren’t mentioned at all in this new genre of writing about algorithms (if any are, it’s usually the accuracy, which is arguably the least informative one). i guess there’s hope. there are certain other things that we talk about in terms of widely understood and agreed-upon metrics. sports is a big one. we know that a single anecdote typically does not convey anything useful about an athlete’s contribution to a sport, and for most sports there are some metrics that we understand are good descriptors of an athlete’s performance. an article about lebron’s 2019–2020 season wouldn’t be complete without mentioning that he’s averaging 25–8–11 in his 17th season. those metrics don’t tell the entire story of his season—and neither do the ones that i propose above about an algorithm—but they tell a lot more of the story than a single anecdote. but even if my utopian dream where every news article about an algorithm includes a detailed summary of the its validation strategy and statistics never comes true, we could still do with a healthy dose of skepticism about what these algorithms can do. believe it or not, what machine learning is able to reliably do in 2020 is still very very limited. we’ve gotten pretty good at things like reading text or classifying images, but we are still very bad at things like understanding and predicting human behavior—in fact, we may never get good at that. so when a person or a company with something to sell claims that they can predict who is pregnant or who will quit their job or who will go to jail, our first reaction should not be to resign to our robot overlords, but rather to ask them to prove that it works with something better than a single anecdote. data science machine learning -- -- 10 written by colin fraser 1.7k followers · 46 following data scientist at meta responses ( 10 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/ppl-c6-big-data/stress-and-penetration-13418df6cbc,,,"stress and penetration. testing: stress and penetration | by izzatul muttaqin | ppl c6 big data | medium sitemap open in app sign up sign in medium logo write sign up sign in ppl c6 big data · pages for it project stress and penetration izzatul muttaqin 3 min read · may 29, 2019 -- listen share testing: stress and penetration in software engineering, stress testing is also known as endurance testing. under stress testing, stress is be stressed for a short period of time to know its withstanding capacity. a most prominent use of stress testing is to determine the limit, at which the system or software or hardware breaks. it also checks whether the system demonstrates effective error management under extreme conditions. penetration test, also known as a pen test, is a simulated cyber attack against your computer system to check for exploitable vulnerabilities. in the context of web application security, penetration testing is commonly used to augment a web application firewall (waf). stress testing apache benchmark is used to stress testing of upload file within server with 10 requests and 2 concurrency. from the result, the total request completed is 10. test runs in 29.545 seconds with 0 failed request and the total data transferred is around 10.02 mb as well as time per request is around 5908.964 ms. penetration testing 1. planning and reconnaissance the first stage involves: defining the scope and goals of a test, including the systems to be addressed and the testing methods to be used. gathering intelligence (e.g., network and domain names, mail server) to better understand how a target works and its potential vulnerabilities. 2. scanning the next step is to understand how the target application will respond to various intrusion attempts. this is typically done using: static analysis — inspecting an application’s code to estimate the way it behaves while running. these tools can scan the entirety of the code in a single pass. dynamic analysis — inspecting an application’s code in a running state. this is a more practical way of scanning, as it provides a real-time view into an application’s performance. 3. gaining access this stage uses web application attacks, such as cross-site scripting , sql injection and backdoors , to uncover a target’s vulnerabilities. testers then try and exploit these vulnerabilities, typically by escalating privileges, stealing data, intercepting traffic, etc., to understand the damage they can cause. 4. maintaining access the goal of this stage is to see if the vulnerability can be used to achieve a persistent presence in the exploited system — long enough for a bad actor to gain in-depth access. the idea is to imitate advanced persistent threats , which often remain in a system for months in order to steal an organization’s most sensitive data. 5. analysis the results of the penetration test are then compiled into a report detailing: specific vulnerabilities that were exploited sensitive data that was accessed the amount of time the pen tester was able to remain in the system undetected references what is penetration testing | step-by-step process & methods | imperva a penetration test, also known as a pen test, is a simulated cyber attack against your computer system to check for… www.imperva.com security -- -- published in ppl c6 big data 6 followers · last published may 29, 2019 pages for it project written by izzatul muttaqin 3 followers · 2 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://kovidrathee.medium.com/remember-the-love-bit-86116eacf750,,,"remember the love bit. remembering christopher hitchens on his… | by kovid rathee | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story remember the love bit remembering christopher hitchens on his 8th death anniversary kovid rathee 4 min read · dec 15, 2019 -- share an eight year old girl follows christopher to the book signing after his talk, and asks about what to read. she says she wants to be a free thinker. christopher suggests reading greek and roman tales by robert graves. the girl says she has read all of them already. the conversation carries on for a couple of seconds. christopher says goodbye to the girl saying — lots of love. remember the love bit. this was probably from the last public appearance of christopher hitchens when he spoke at the texas freethought convention in 2011. in june of 2010, he was diagnosed with oesophageal cancer while he was touring for his memoir hitch-22. he passed away on the 15th of december 2011 — after seeing the death of osama bin laden — which is really wished to see before he went away. he also wished to see the world trade centre reopen but that couldn’t happen. i can’t remember when i discovered his work for the first time. it wasn’t probably a book. i got to know about the books much later. it was from one of the videos where he was debating some priest or a rabbi. over the years, i have watched a lot of those debates again and again, read a couple of his books, his articles in the atlantic, free inquiry, the nation and so on. and through him, i got introduced to works of literature, ideas, political theories and whatnot. he became one of my favorite people — speaker, author, polemicist, contrarian, atheist… -- -- written by kovid rathee 3.3k followers · 273 following i write about tech, indian classical music, literature, and the workplace among other things. 1x engineer on weekdays. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/ppl-c6-big-data/whats-wrong-with-my-simple-react-docker-image-4e81584dac1b,Jest,,"what’s wrong with my simple react docker image | by muhammad ashlah shinfain | ppl c6 big data | medium sitemap open in app sign up sign in medium logo write sign up sign in ppl c6 big data · pages for it project what’s wrong with my simple react docker image muhammad ashlah shinfain 3 min read · feb 28, 2019 -- 3 listen share initialize react.js app i was trying to create a simple react app following the tutorial from https://reactjs.org/docs/create-a-new-react-app.html . well, it works on my computer — before and after i dockerize it. but when i run the docker image on the ppl server, it gives me an error: events.js:174 throw er; // unhandled 'error' event ^ error: enospc: system limit for number of file watchers reached, watch '/my-app/public' at fswatcher.start (internal/fs/watchers.js:165:26) at object.watch(fs.js:1254:11) at createfswatchinstance (/my-app/node_modules/chokidar/lib/nodefs-handler.js:37:15) at setfswatchlistener (/my-app/node_modules/chokidar/lib/nodefs-handler.js:80:15) at fswatcher.nodefshandler._watchwithnodefs (/my-app/node_modules/chokidar/lib/nodefs-handler.js:232:14) at fswatcher.nodefshandler._handledir (/my-app/node_modules/chokidar/lib/nodefs-handler.js:422:19) at fswatcher.<anonymous> (/my-app/node_modules/chokidar/lib/nodefs-handler.js:470:19) at fswatcher.<anonymous> (/my-app/node_modules/chokidar/lib/nodefs-handler.js:475:16) at fswatcher.oncomplete (fs.js:155:5) emittted 'error' event at: at fswatcher._handleerror (/my-app/node_modules/chokidar/index.js:260) at createfswatchinstance (/my-app/node_modules/chokidar/lib/nodefs-handler.js:39:5) at setfswatchlistener (/my-app/node_modules/chokidar/lib/nodefs-handler.js:80:5) [... lines matching original stack trace ...] at fsreqwrap.oncomplete (fs.js:155:5) npm err! code elifecycle npm err! errno 1 npm err! my-app@0.1.0 start: `react-scripts start` npm err! exit status 1 npm err! npm err! failed at the my-app@0.1.0 start script. npm err! this is probably not a problem with npm. there is likely additional logging output above. npm err! a complete log of this run can be found in: npm err!     /root/.npm/_logs/2019-02-21t13_52_37_060z-debug.log here’s the package.json: { ""name"": ""my-app"", ""version"": ""0.1.0"", ""private"": true, ""dependencies"": { ""react"": ""^16.8.2"", ""react-dom"": ""^16.8.2"", ""react-scripts"": ""2.1.5"" }, ""scripts"": { ""start"": ""react-scripts start"", ""build"": ""react-scripts build"", ""test"": ""react-scripts test"", ""eject"": ""react-scripts eject"" }, ""eslintconfig"": { ""extends"": ""react-app"" }, ""browserslist"": [ "">0.2%"", ""not dead"", ""not ie <= 11"", ""not op_mini all"" ] } hmm.. the first clue i noticed is the enospc error code, which i can assume it’s an abbreviation of error no space . so i checked out the available disk space: all following console was run inside the docker container root@9efcab7ce8c6:/# df -h filesystem      size  used avail use% mounted on overlay         317g  315g    0g 100% / tmpfs            68m     0   68m   0% /dev tmpfs            68g     0   68g   0% /sys/fs/cgroup /dev/sdb1       317g   94g  207g  32% /etc/hosts shm              68m     0   68m   0% /dev/shm wow, the disk is full! what should i remove for clean up the disk? i realized that there are plenty of unused images, so i deleted some and voila: root@9efcab7ce8c6:/# df -h filesystem      size  used avail use% mounted on overlay         317g   94g  207g  32% / tmpfs            68m     0   68m   0% /dev tmpfs            68g     0   68g   0% /sys/fs/cgroup /dev/sdb1       317g   94g  207g  32% /etc/hosts shm              68m     0   68m   0% /dev/shm well, now there is plenty much disk available, but the error is still the same. so disk space mustn’t be the issue. the next clue is about file watcher limit. i do some research about it — because i’m not familiar with this thing — and found out about inotify file watcher. this tool allows programs to monitor filesystem changes. the maximum amount of files being watched is controlled by the system. there are plenty of ways to check this limit: root@9efcab7ce8c6:/# cat /proc/sys/fs/inotify/max_user_watches 8192 root@9efcab7ce8c6:/# sysctl fs.inotify.max_user_watches fs.inotify.max_user_watches = 8192 i also found an issue opened at jest ‘s github repository. they said we just need to increase the limit, just like what i found another repo’s wiki page : root@9efcab7ce8c6:/# echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p by default, if we run a shell on a docker container it runs as root user and it doesn’t have sudo command. so i installed it first. but it didn’t work: sysctl: setting key 'fs.inotify.max_user_watches': read-only file system but then — thanks to our scrum master syahrul ardiansyah — i found a way out by adding a line on /etc/sysctl.conf : echo ""fs.inotify.max_user_watches=524288"" >> /etc/sysctl.conf and voila! the error is gone! previously, i’ve also tried to run a simple next.js app and there’s no issue. i suspect it’s because there are too many files to be watched on the react.js app. but there’s still something makes me confused. the next time i create a container and checked the inotify file watcher limit, it’s already had value 524288 😕. javascript devops docker -- -- 3 published in ppl c6 big data 6 followers · last published may 29, 2019 pages for it project written by muhammad ashlah shinfain 11 followers · 3 following responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/data-science/predicting-whether-a-marvel-character-is-good-or-evil-using-big-data-analytics-fb2ed78c3610,,,"predicting whether a marvel character is good or evil using big data analytics. | by vetle ottem frantzvaag | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. predicting whether a marvel character is good or evil using big data analytics. vetle ottem frantzvaag 11 min read · oct 1, 2019 -- 1 listen share marvel universe abstract this article utilizes statistical methods, data mining techniques and python in order to create a prediction model for superhero characters alignment. starting off we choose what dataset to utilize — which is then prepared, handled and analyzed. then, the cleansed data is imported into a data mining tool; weka, to create a viable prediction model that can foretell the alignment based on descriptions of the characters. this outcome is then tested and debated to describe the different results. this article assumes you have some familiarity with python and pandas, and will therefore not go into too much into detail in regards to these subjects. if you have any questions please shoot me an email at vfrantzvaag@gmail.com . keywords : data mining, python, statistics, analytics, superheroes data acquisition the dataset used in this project was acquired at kaggle.com from the user fivethirtyeight and is maintained by kaggle, amplifying its credibility.( https://www.kaggle.com/fivethirtyeight/fivethirtyeight-comic-characters-dataset ). the dataset contains two csv files and a readme file describing the content in the data files, and it was downloaded on 31.01.2019. even though the dataset both contains data on dc and marvel, i have decided to stick only with marvel as they are the most popular (statista, finder, boxofficemojo) and the marvel dataset contains 2.3 times the number of rows. additionally, the universes are created by a different set of teams, meaning that what we can infer from one universe may not be as relevant for the other. dataset description the marvel dataset contains information about all recorded characters from the marvel wikia, more specifically around 16.4 thousand different characters. downloading the dataset and using the pandas package for python, i can ask for the five elements at the top of the list and check the format of the csv file, as well as asking for the number of rows and columns. checking out the dataset using the framework pandas for python. (the set_option method is just for printing all the columns in the terminal) what is outputted in the terminal after the previously written python code. the table columns are: page_id, name, urlslug, id, align, eye, hair, sex, gsm, alive, appearances, first appearance and year — and they are as mentioned described in the readme file that accompanied the dataset. data preparations preparing the dataset for usage is a crucial step to ensure quality, including reducing anomalies and redundancy in the data. the first step in this process is the removal of columns that are not informative to the end-case; predicting character alignment based on characteristics . this can be done with normal logic; removing columns that does not express the characteristics. the columns that are not relevant for the end-case is: - page_id: the unique identifier of the wikipedia page - urlslug : the url to the wiki site of the character gsm: if the character is a gender or sexual minority. would be informative, but there are only 90 rows that has a value as can be verified with this python code: checking how many valid entries there are in the gsm column. - appearances : amount of appearances in the comic books - first appearance: the first appearance in the comic book - year : the characters first appearance in a comic book considering i find these columns to be insignificant for my purpose, i remove them from the dataset with python. removing the insignificant values from the dataset. since this is going to be the training set for my model, there has to be a value in the ‘align’ column for it to be able to train with the data, so i remove all columns containing null in the ‘align’ column — as this will not contribute any information. the ‘align’ column currently has three possible values; good, neutral and bad. in this project i am mainly interested in figuring out whether a character is evil or not, and considering some psychologist define being ‘good’ as ‘ having the ability to empathize and feel compassion’ (steve taylor, 2013), i am going to group together the ‘neutral’ and ‘good’ columns. for me to be able to perform calculations and for simplicity sakes, i also convert the columns to numbers; 1 if they’re evil and 0 if they’re good/neutral. python code for joining the neutral and good characters, as well as converting all columns to numeric values. also calculating the amount of rows lost when dropping the null values. i can now subtract the amount of rows in the newly created file from the original one, and see that a total of 2812 rows were lost by removing the null values in the ‘align’ column. the outputted code for displaying how many rows we lost with the previous python code. now that the alignment column is prepared, and insignificant columns have been removed, i can import the cleaned .csv file into the statistical tool; jmp to do a ‘model check’. i set the target variable to be align and choose the remaining columns to be the model effects, except for ‘name’ as this is more of an identifying column. this gives me an overview of a whole model test and an effect summary. the results reflect the summary of a null hypothesis with a significance value of .95 — meaning that p-values lower than .05 gives reason enough to throw the null hypothesis away, inferring to go for the alternative hypothesis; that the attributes contribute to the alignment. both the null hypothesis and alternative hypothesis can be seen below: h0 : β1 = β2 =… βg = 0 ha: βi ≠ 0 here we are looking for p-values that stick out (being over 0.05). if we for example would’ve kept the attribute for the url to the wiki site of the character, this probably would’ve given us a p-value higher than 0.05, signifying that we probably could remove that attribute. looking at the summary statistics, it can be noted that none of the individual nor whole-model p-values peak over 0.05; emphasizing to go for the alternative hypothesis that there are attributes giving information about the alignment and that all of the individual attributes contribute. lastly, i have to prepare the file for my data mining tool; weka — where an .arff file is the most suitable format. since weka had trouble parsing the csv file, i couldn’t use the built-in converter and had to opt for an external one; one made by ilya kuzovkin [1] . however, i was still running into problems importing the newly created .arff file, since there were special characters in some columns — which i manually removed using a find-all search function in sublime text. [1] https://ikuz.eu/csv2arff/ data analysis visualization to provide further insight into how the different columns influences the alignment of a character, i have imported the cleansed dataset into a data visualization tool: tableau. my main point of interest is to compare how the alignment column is affected by character characteristics. tableau dashboard comparing the characteristics to alignment. (remember that align = 1 is evil and align = 0 is good/neutral. looking at the difference between the characteristics in regard to alignment, one can see there are a lot of common elements, with certain areas where they differ. if i was to construct a ‘stereotypical’ character based on this information, an evil character would have no hair, red eyes, be a male character, have a secret identity and be deceased — while a good character would have blond hair, blue eyes, be a female character, have a public identity and be alive. do these characters seem evil and good to you? to me it does. data mining now that the data is prepared, cleaned and analyzed, i can import it into the data mining tool ‘weka’ to create a prediction model. we are now moving into the realm of classification when aiming to identify to which category an object belongs to. classification is a form of supervised learning where we create a model that sets the value of the target variable as a function of the features (provost & fawcett), supervised meaning that we provide the algorithm with a labeled dataset to train with. the purpose is to predict the alignment column as a result of the characteristics by using classification, however there are a great number of different classification algorithms out there, meaning i have to find the one that’s best fit. using the ‘classifier’ column in weka, i can examine what the different preset options are: looking into what classification algorithms weka offers us by default. choosing the correct algorithm is crucial in order to obtain the best possible outcome and for ensuring as high-as-possible model accuracy. by taking advantage of the ‘algorithm cheat sheet’ created by scikit , a flow diagram showing the different premises of the algorithms (marked green), i have mapped out the path in the diagram from their website below: choosing our path from the scikit-learn algorithm cheat-sheet (marked in red). considering svc (support vector machines) was tried out in weka and did not work on the training set, the logical choice is to go for the ‘naïve bayes’ algorithm. a definite advantage of the naïve bayes model is that it works well with missing values, which is great for the marvel-dataset that have some empty column rows. (witten et al.) one of the premises of naïve bayes, or rather why it’s “naïve”, is because it operates under the assumptions that the variables are independent. this has to be taken into consideration when using the model; that there is no correlation between the columns or that they do not influence each other. even though this is a bit invalid, considering it is a proven correlation between for example hair and eye color (lin et al.), this is just for the model calculation and we can use the current assumptions to make a valid model. importing the cleansed .arff file into weka, i firstly run the naïve bayes model on my training data set without changing any parameters, and with cross-validation set to the standard, ten, in all of the model tests — where the data is split into ten folds; some for testing and some for training. (witten et al.) the kpi i am mainly interested in is the overall model accuracy (correctly classified instances), which is currently at 65.7%. summary of running a standard naïve bayes algorithm on my training set. with 65.7% as my baseline rate, i seek out to improve the accuracy of the model. with some trial and error, i discovered that using a kernel density estimation (kde) increased the accuracy of almost 2% up to 67.03 %. the kde does not assume any particular distribution of the model, which is fit for this case as for example a standard distribution with this data can be difficult to achieve. in my opinion it would be problematic to aim for a noticeably higher percentage, considering it is impossible to correctly classify every character’s alignment based on their characteristics — a person can still be nice even though they have no hair and red eyes. now that i have found a suitable model with an acceptable success rate, i want to test how this model performs on some of my own training data. taking ten characters from the original dataset; five of them being ‘good/neutral’, five being ‘bad’, and replacing the alignment column with ‘?’. taking some famous characters and labeling the alignment column ‘?’, in order to test out how well our model does in practice. using this as the training set, i re-evaluate it on the model i created and get the following output: summary of running our model on the ten aforementioned characters. in the predicted column (red), 1:0 tells us that the character is good and 2:1 tells us that the character is bad. the blue column tells us how certain the model is about its prediction. under the ‘predicted’ column, all of the first five instances are correctly classified (alignment being zero is good/neutral), while in the last five instances three out of five (60%) of the instances were correctly classified, totaling 80% correctly classified instances in this small training set. it can also be noted under the ‘error prediction’ column that the model is much more certain in classifying ‘good/neutral’ characters than ‘bad’ ones. the reasoning for this can be that many of the ‘bad characters’ have multiple identities/personalities, changing form- or looks when doing evil actions — as can be seen below, both of the incorrectly classified instances; victor von doom and raven darkholme has a ‘shifting look’: the shifting look of the incorrectly classified instances. discussion there are extensive ethical issues with predicting a person’s alignment based on their characteristics. rongxing lu et al. argue that preserving privacy and handling data in an ethical and anonymous way is necessary to defend our freedom. (rongxing et al.) in this paper, i am not predicting real people’s alignment, and thereby am not exposed to these legal and ethical concerns. nevertheless, these concerns have to be taken into consideration if this model is to be applied elsewhere — i.e. if someone were to use it to predict a person that they just met is good or evil. there are a great array of possibilities in which you can describe a character, and there is information in datasets describing superheroes in more detail (powers, origin etc.). however, one has to limit themselves to a certain threshold for a model to be viable. even though more information could lead to a more accurate model, the main thought behind this model is to predict a character’s alignment based on a short glimpse or rumors. too much information or complexity can also cause the model to capture too much noise from the data, causing what is referred to as ‘overfitting’ (witten et al.). references articles 1. all time box office worldwide openings, box office mojo, updated 2/8/2019 can be found at https://www.boxofficemojo.com/alltime/world/worldwideopenings.htm 2. public preference between marvel movies and dc comics movies in the united states as of august 2017, statista, 15.08.2017 can be found at: https://www.statista.com/statistics/754142/marvel-dc-movie-preference/ 3. superhero statistics; dc vs. marvel, finder, 16.11.2017 can be found at: https://www.finder.com/superhero-statistics 4. predicting terrorist behaviors with more than 90 percent accuracy, binghamton university, 02.03.2017 can be found at: https://www.sciencedaily.com/releases/2017/03/170302115740.htm 5. predicting terrorist behaviors with more than 90 percent accuracy, binghamton university, 02.03.2017 can be found at: https://www.sciencedaily.com/releases/2017/03/170302115740.htm 6. china has started ranking citizens with a creepy ‘social credit’ system, alexandra ma, 08.04.2018 can be found at: https://nordic.businessinsider.com/china-social-credit-system-punishments-and-rewards-explained-2018-4?r=us&ir=t 7. the real meaning of ‘good’ and ‘evil’, steve taylor, 26.08.2013 can be found at: https://www.psychologytoday.com/us/blog/out-the-darkness/201308/the-real-meaning-good-and-evil literature 8. on the origin of superheroes: from the big bang to action comics, chris gavaler, 2015 9. a moral analysis of effective prediction markets on terrorism, dan weijers & jennifer richardson, 2014 10. data science for business, provost & fawcett, 2013 11. the genetic overlap between hair and eye color, lin bd; willemsen g; abdellaoui a; bartels m; ehli ea; davies ge; boomsma di and hottenga jj, 19.12.2016 12. data mining; practical machine learning tools and techniques, ian witten; eibe frank & mark hall, 2011 13. toward efficient and privacy-preserving computing in big data era — rongxing lu; hui zhu; ximeng liu; joseph k. liu & jun shao, 2016. machine learning big data python data science marvel -- -- 1 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by vetle ottem frantzvaag 5 followers · 17 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@billtai/30b-stress-test-on-necker-island-814553c7f520,,,"$30b stress test on necker island | by bill tai | medium sitemap open in app sign up sign in medium logo write sign up sign in $30b stress test on necker island bill tai 5 min read · jun 21, 2019 -- listen share as we kick off our 5th annual blockchain summit on necker island tonight, i’m reminded of what a special place this is. necker island is incredibly well designed for active gatherings that induce collaborative social flow. gathering great minds together in groups and inducing collaboration by combining deep discussion with active adventures is a fantastic formula both on and off the water. amazing ideas born here have been been turned into industry leading organizations and companies. some incredibly innovative ideas have been turned into products here, like this bitcoin mining lightbulb conceived in a lightbulb moment on necker . on top of that, you may be surprised that this idyllic setting has been a test bed for innovative tech entrepreneurs to gather, share ideas, and test their mettle, and in this specific case, run a ‘technology stress test’. necker island is a paradise of course — but any island has to step up its game to enable world class communications infrastructure, given that islands are not typically directly connected to the world’s fiber optic telco cable systems. data communications to and from necker island to the various continents on the planet sometimes has to traverse several carriers and extra router hops as well as wireless microwave radio links that introduce additional delay, to get back and forth to their destinations. a gathering of kiteboarding entrepreneurs presenting their startups on necker island, 2013 given that challenging set up, zoom video — then just a year old startup that had recently launched a first iteration of it’s product — decided to test its technology for cloud based conferencing in that hard core environment. zoom became the backbone that day to connect entrepreneurs from around the world into a session that would include multiple people from 5 continents on live video, streaming themselves and their presentations and in one case an hd video, to multiple two way video and audio endpoints. during the presentations, we measured latency and packet loss from each endpoint and zoom used those learnings to understand how to tune the mix of audio, video and presentation data when streaming conferences across multiple points with differing latency. for frame of reference — a typical connection, point to point across anywhere in the usa is likely to not exceed 20 milliseconds. a super long distance connection across the pacific ocean from the usa to australia might be 100 milliseconds. as we set up for the test, knowing that at times we’d have entrepreneurs on line from tokyo, san francisco, sydney, london, new york, mountain view and san jose. it was daunting to not know exactly what the paths would be for the digital bits to get back and forth across the various points. but we decided to just give it our best just to see what would happen and learn how the technology would handle it. it was not easy! measured latency was at times over 200 milliseconds — more than 10x the typical us to us delay and more than 2x the long distance delay from the us to australia! what a minefield for a video test! melane perkins, ceo of canva, presenting. nick chong of zoom video monitoring the technology. that said, it’s pretty hard to be emotionally stressed on necker — so it was a great place to try this out with a fun and forgiving audience of resilient, hard driving but forgiving entrepreneurs. while there were certainly glitches that presented great learning opportunities — all in all it was a wonderful place to drive this ‘low emotional stress, but high technology stress’ test. it’s been great to look back at this photo from 6 years ago — of entrepreneurs presenting their startups, literally each just months old, to the group and our special guest, sir richard branson, both live and on zoom. some of activity that day was captured in this article posted by kym mcnicholas , almost 6 years ago to the day (june 28, 2013) on pandodaily. here’s whats happened to some of companies since that time.. alexandra chong, presented her company luluvise, which went on to break into the top 10 in the iphone app store in several countries. lulu was acquired by badoo , and alexandra has gone on to become the founding ceo of jacana life , the leading medical cannabis company in jamaica 🇯🇲. lars rasmussen, the inventor of google maps, introduced canva from sydney , which was then just a year old . canva is now is a $100m recurring revenue company leading the world of cloud based design — and is a profitable company sporting a $2.5 billion valuation . lars has gone on to start weav music which is changing the way music can be enjoyed in real time. othman laraki, then vp growth at twitter introduced an 18 month old “big data” company from tokyo and mountain view california named treasure data . treasure data has grown substantially to be a leader in it’s category — to the point it handles in excess of 7 trillion rows of data per month for customers (i stopped counting a year ago so it could be multiples of that now; i had the honor of cofounding the company as chairman, with hiro yoshikawa and kazuki ohta). last fall we were acquired by softbank’s arm microprocessor company with an ambitious target set by masa son , to be the “icloud for one trillion microprocessors by 2030”. othman laraki has gone on to cofound and is ceo of color genomics , the leader in consumer usable dna based diagnostics. and of course — the startup that provided the communications infrastructure for all of us — bringing people together from around the world, with necker island as center of gravity for the day… it’s become a hit. celebrating with team zoom, ringing the bell for the company’s ipo earlier this year. zoom has gone on to become a company generating over $300m per year in revenue, and is the only profitable tech unicorn 🦄 to list its shares this year. as such zoom now sports a valuation post its ipo in excess of $25 billion. of course, tech valuations are cyclical and ipo’s can be the tip of the whip . but it’s been amazing to see how our ‘necker babies’ from 2013 have grown, and i’m astounded that they now have value together of over $30 billion! as we kick off this 2019 necker blockchain summit — i’m excited to see what springs forth 6 years from now! entrepreneurship blockchain ipo technology unicorns -- -- written by bill tai 1k followers · 841 following empowering others via a community of athletes, conservationists, technologists, artists, & innovators. www.actai.global ; bio: www.about.me/billtai no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/acing-ai/amex-data-science-interview-questions-a8d2634c647,,,"amex data science interview. over 20% credit card transactions in… | by vimarsh karbhari | acing ai | medium sitemap open in app sign up sign in medium logo write sign up sign in acing ai · acing ai provides analysis of ai companies and ways to venture into them. amex data science interview over 20% credit card transactions in the us are amex. vimarsh karbhari 3 min read · apr 30, 2019 -- listen share american express or amex was founded in 1850 and is one of the largest credit card companies on earth. it is likely to become the second largest u.s. card processing company in 2018. as of december 31, 2017, the company had 112.8 million cards in force, including 50 million cards in force in the united states, each with an average annual spending of $18,519. forbes estimates the brand to be worth us$24.5 billion. when you have a company doing over one trillion dollars in transactions the amount of data that would generate would be huge. as a company, the data it gathers would be used to improve its own products, security and detect fraud. all these factors make amex a dream for data scientists. source: shutterwhale interview process the interview process starts with a first call with the hiring manager. in the second round, there are coding tests followed by the final onsite interview. the onsite interview has multiple rounds with different team members. it consists of ml and stats. there is a final interview with the hiring manager again before they decide on the hiring. important reading amex: my offers machine learning at american express: benefits and requirements big data management at amex: interview recommendations at amex: presentation data science related interview questions describe adaboost and random forests. given a dataset, how do you perform knn on it? describe the typical workflow of a data scientist project from start to finish. what are the different metrics you have used for a ml project? we have a million card members and their transactions. also, we have 10k restaurants and 1k coupons to eat food. write a method that can be used to pass along the coupons to the users given that some users have already got their coupons so far. given a training set of users with their demographic information, psychology test of their personality (openness to like pages or not) and the pages of fb they have liked so far, how could we predict the gender, age and other demographics of unseen data? what is gbm? explain how gradient descent works on a dataset? given one tennis ball and an airplane, how can you determine the area of the inside the plane? what is logit function? reflecting on the questions the data science team at amex uses hadoop and mapreduce extensively as their backend. to that end, amex has established its big data labs to do research on various analytics topics. the company has more than 25 phds on its staff and has over 80 patents relating to high performance computing, dynamic pricing, graph technology, social connections, and web analytics and its researchers have published over 140 papers in hadoop, machine learning, web databases, and real-time search. the interviews are a mix of ml, hadoop and data science concepts. a deep knowledge in one of the domains in which amex operates can surely land you a job in the second largest credit card company in the world! subscribe to our acing ai newsletter, i promise not to spam and its free! newsletter subscribe to the acing ai/data science newsletter. it is free! reducing the entropy in data science. helping you with… www.acingdatascienceinterviews.com thanks for reading! 😊 if you enjoyed it, test how many times can you hit 👏 in 5 seconds. it’s great cardio for your fingers and will help other people see the story. the sole motivation of this blog article is to learn about amex and its technologies helping people to get into it. all data is sourced from online public sources. i aim to make this a living document, so any updates and suggested changes can always be included. please provide relevant feedback. data science artificial intelligence machine learning interview data -- -- published in acing ai 4.4k followers · last published oct 15, 2021 acing ai provides analysis of ai companies and ways to venture into them. written by vimarsh karbhari 4.4k followers · 945 following engineering manager | founder of acing ai no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/towards-data-science/a-journey-into-bigquery-fuzzy-matching-4-of-1-a-tribute-to-fuzzywuzzy-54ff73e91a1c,,,"a journey into bigquery fuzzy matching — 4 of [1, ∞) — a tribute to fuzzywuzzy | by brian suk | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. a journey into bigquery fuzzy matching — 4 of [1, ∞) — a tribute to fuzzywuzzy brian suk 9 min read · nov 19, 2019 -- 2 listen share this is part of an ongoing series, and there’s other goodness lurking out there! all the fuzzy goodness! a journey into bigquery fuzzy matching — 1 of [1, ∞) — soundex a journey into bigquery fuzzy matching — 2 of [1, ∞) — more soundex and levenshtein distance a journey into bigquery fuzzy matching — 3 of [1, ∞) — nysiis okay, i know in the last article i said we would use this one to start going into adding address elements and match groups (and i promise they’re still on their way), but i wanted to take a slight detour and add another set of functions before we get there. let’s revisit the levenshtein distance function . earlier we used it to calculate the edit distance between two strings. it is designed to determine the number of changes that it takes for one string to become another, helping us figure out if two strings were actually supposed to be the same. we saw how we can use it with words, but what happens when we start to use these on increasingly complex strings? before we go into some examples, let’s first establish how to calculate the similarity ratio to more easily understand these comparisons. it’s a simple ratio of the common characters over the total characters. ((|a| + |b|) — ldist(a, b)) / (|a| + |b|) let’s create a function to calculate this since we will be using this pretty frequently. create or replace function dq.dq_fm_ldist_ratio(a string, b string) as ( /* * data quality function - fuzzy matching * dq_fm_ldist_ratio * input: two strings to compare. * returns: the levenshtein similarity ratio. */ (length(a) + length(b) - `dq.dq_fm_levenshteindistance`(a, b)) / (length(a) + length(b)) ); to see how these functions behave with increasingly complex strings, we’re going to need a dataset to test this out with. we are going to use a dataset of room descriptions from expedia and booking.com. we’ll load this into a new dataset (i’ll be using fuzzy_matching ) and take a quick look. so what are we supposed to call these, then? these string pairs represent the same room, but you can see that they might be called different things. when we dig deeper through the data, we see some more complex combinations. a != b so let’s try using the tools that we have and apply them to this dataset to see how they fare (spoiler alert: it won’t be accurate). well, it figured one of them out at least. now before we go any further, it’s important to note that the levenshtein distance can be implemented in two different ways depending on how you weigh things. some implementations treat substitutions as a single operation, with equal weighting as replacements and additions. other implementations give substitutions a weight of two (one subtraction + one addition). when comparing any implementations it’s important to note how it’s weighed, or you may end up with differences in calculations you can’t quite explain. in ours, we gave substitutions as scoring weight of one. back to the results. as expected, the edit distance is very high since we are treating the entire string as a single token. because of that, the match ratios are going to be quite poor. what we need to do is tokenize these strings, and perform smarter comparisons with them. enter fuzzywuzzy . the string matching library, not the bear who had no hair. fuzzywuzzy is a python library that takes strings, tokenizes them, and then matches them in different ways. it’s a very powerful package to utilize, but python isn’t usable by bigquery. so let’s see if we can’t replicate some of this functionality in sql! let’s start off with the token sort ratio. this is handy when you have string tokens that are fairly similar but are out of order. what the token sort ratio does is break up the strings into individual tokens, sort them in alphabetical order, concatenate them as a single string, and then calculate the ratio based upon that. to accomplish this functionality, we are first going to create a couple helper functions that splits, cleans, and then sorts the array. create or replace function dq.dq_hf_gh_clean_tokenize(a string) as( /* * (helper) data quality function * dq_hf_gh_clean_tokenize * this function removes all non-alphanumeric characters. * input: uncleaned string * returns: string of tokenized and cleaned string. */ array( select tokens from unnest( split( regexp_replace( upper( trim(a) ), '[^a-za-z0-9 ]+', '' ), ' ' ) ) tokens ) ); add another function to sort it. create or replace function dq.dq_hf_gh_clean_sort_tokenize(a string) as( /* * (helper) data quality function * dq_hf_gh_clean_sort_tokenize * input: uncleaned string * returns: string of tokenized, sorted, and cleaned string. */ array( select x from unnest(`dq.dq_hf_gh_clean_tokenize`(a)) as x order by x)) ; from here we can re-assemble the arrays, and do our comparison. create or replace function dq.dq_fm_ldist_token_sort_ratio(a string, b string) as( /* * data quality function - fuzzy matching * dq_fm_ldist_token_sort_ratio * input: two strings to compare. * returns: the levenshtein similarity ratio with sorted tokens. */ `dq.dq_fm_ldist_ratio`( array_to_string(`dq.dq_hf_gh_clean_sort_tokenize`(a),''), array_to_string(`dq.dq_hf_gh_clean_sort_tokenize`(b), ''))); with these two in place, let’s go ahead and add this to the dataset to see how it behaves. we are looking for entries with a high edit distance (indicating that there is a high number of raw replacements as everything is out of order) and a high sort ratio, which means the individual tokens are fairly similar. we’re starting to get some better matches! these results show you that you can use this to match strings with out of order tokens, while allowing for some level of tolerance for character change. what happens when the strings have some commonality between them, but both the token length and the actual content of the rest of the strings widely vary? this is where token sets come in! the folks at seatgeek , who developed and open sourced the fuzzywuzzy python package , have put up a really good description of it on their blog . i recommend reading their blog post to get a greater understanding of how this all works, but here is the gist of it. 1 — for strings a and b , tokenize both strings. 2 — create three strings: t0 = sorted intersection of tokens from a and b t1 = t0 + sorted remainder of tokens from a t2 = t0 + sorted remainder of tokens from b 3 — return the highest ratio from ldist(t0,t1) , ldist(t0,t2) and ldist(t1,t2) why does this work? as the folks at seatgeek explain it, “the intuition here is that because the sorted_intersection component is always exactly the same, the scores increase when (a) that makes up a larger percentage of the full string, and (b) the string remainders are more similar.” let’s try this with one pair of room descriptions, and see how this works. a = ""deluxe room, 1 queen bed (high floor)"" b = ""deluxe queen room — high floor with free wi-fi"" cleaning, sorting, and tokenizing them gives us the following: token_a = ['1','bed','deluxe','floor','high','queen','room'] token_b = ['deluxe,'fi','floor','free','high','queen','room','wi','with'] the tokens that intersect are, ‘deluxe’, ‘floor’, ‘high’, ‘queen’, and ‘room’. this gives us the following three strings and comparisons: t0 = 'deluxefloorhighqueenroom' t1 = 'deluxefloorhighqueenroom1bed' t2 = 'deluxefloorhighqueenroomfifreewiwith' `dq.dq_fm_ldist_ratio`(a, b) = 0.64 `dq.dq_fm_ldist_ratio`(t0, t1) = 0.92 `dq.dq_fm_ldist_ratio`(t0, t2) = 0.8 `dq.dq_fm_ldist_ratio`(t1, t2) = 0.83 so this giving us a confidence score of 0.92, which is much higher than the original 0.64. this is possible because the intersection consists of the majority of one the original strings, so naturally, the comparison is going to be high. if the intersection is small, but the remainder of the tokens are still similar, that still allows us to have a high confidence score by comparing t1 and t2 . we are covered from multiple angles! let’s now figure out how to put this into bigquery functions. we already have a function from earlier that cleans, tokenizes, and sorts the strings. we now need to construct something that finds the sorted intersection. create or replace function dq.dq_hf_gh_find_array_intersection(a array<string>, b array<string>) as( /* * (helper) data quality function * dq_hf_gh_find_array_intersection * input: two arrays to compare * returns: array with the common elements */ array( select * from unnest(a) as a intersect distinct select * from unnest(b) as b )) ; we’re also going to need something that calculates the difference between the two sets of tokens. create or replace function dq.dq_hf_gh_find_array_difference(a array<string>, b array<string>) as( /* * (helper) data quality function * dq_hf_gh_find_array_difference * input: two arrays to compare * returns: array with elements a - b. */ array( select * from unnest(a) as a except distinct select * from unnest(b) as b )) ; and now we just need to create the strings and find the maximum value of the three comparisons. create or replace function dq.dq_fm_ldist_token_set_ratio(a string, b string) as( /* * data quality function - fuzzy matching * dq_fm_ldist_token_set_ratio * input: two strings to compare. * returns: the levenshtein similarity of the maximum ratio *   between the different token sets. */ array( select max(x) from unnest( [ # first ratio is sorted intersection and combined a diff b `dq.dq_fm_ldist_ratio`( array_to_string(`dq.dq_hf_gh_find_array_intersection`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'') , concat( array_to_string(`dq.dq_hf_gh_find_array_intersection`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'') , array_to_string(`dq.dq_hf_gh_find_array_difference`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),''))) , # second ratio is sorted intersection and combined b diff a `dq.dq_fm_ldist_ratio`( array_to_string(`dq.dq_hf_gh_find_array_intersection`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'') , concat( array_to_string(`dq.dq_hf_gh_find_array_intersection`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'') , array_to_string(`dq.dq_hf_gh_find_array_difference`( `dq.dq_hf_gh_clean_sort_tokenize`(b), `dq.dq_hf_gh_clean_sort_tokenize`(a)),''))) , # third ratio is a diff b and b diff a `dq.dq_fm_ldist_ratio`( concat( array_to_string(`dq.dq_hf_gh_find_array_intersection`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'') , array_to_string(`dq.dq_hf_gh_find_array_difference`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'')) , concat( array_to_string(`dq.dq_hf_gh_find_array_intersection`( `dq.dq_hf_gh_clean_sort_tokenize`(a), `dq.dq_hf_gh_clean_sort_tokenize`(b)),'') , array_to_string(`dq.dq_hf_gh_find_array_difference`( `dq.dq_hf_gh_clean_sort_tokenize`(b), `dq.dq_hf_gh_clean_sort_tokenize`(a)),''))) ] ) as x)[offset(0)] ); let’s now run this on our room_data table and see what we get. hooray! matches! with a few functions, you’re now able to perform some more sophisticated string comparisons. by performing edit distance comparisons on token sets, you can go beyond single word matching to look at broader entities. this is useful not only for things like products and descriptions, but can also be applied to things like addresses. address matching is a huge part of entity resolution, and matching methods like this are an important part of that. let’s just take a quick peek at what that looks like. we’ll use the address of the empire state building , and purposefully alter the address in a few different ways. the listed address is 20 w 34th st, new york, ny 10001. these are okay. these matches are pretty decent considering we are just looking at pure token string comparisons. with these examples, we are getting a lot of variance because the strings are short, and changing something as small as “st” to “street” can have a big impact. if the domain of the entity is known beforehand, such as “we know this is going to be address data,” then it does help to have some tokens get pre-processed and standardized. for example, with addresses in the united states we can usually assume that “st” and “street” are the same, “ave” and “avenue” are the same, and so on. this kind of domain specific pre-processing can help us increase our match percentages between two sets of data! and with that, we’ve got a few more additions to the fuzzy matching toolbox. hopefully, this was a worthy tribute to fuzzywuzzy. with these building blocks, we can start to apply them to complex datasets and use them to build some basic match groups. we’ll start to look at those in the next round, so stay tuned! programming google cloud platform analytics data big data -- -- 2 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by brian suk 281 followers · 35 following avid 2020 bed-to-couch traveler, cloud tech, big data, random trivia, xoogler. my employer isn’t responsible for what’s here. nyc. linkedin.com/in/briansuk responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://arablelabs.medium.com/when-the-rain-didnt-stop-34f3167e5105,,,"when the rain didn’t stop. our laser disdrometer gives a glimpse… | by arable | medium sitemap open in app sign up sign in medium logo write sign up sign in when the rain didn’t stop our acoustic disdrometer gives a glimpse into the wettest tropical cyclone in hawaii history arable 5 min read · nov 30, 2018 -- listen share by jamie hardy as the busy 2018 hurricane season officially draws to a close , we wanted to share some insights from the arable mark device stationed at the usda institute for pacific islands forestry pacific southwest research station in hilo, hawaii. the installation was funded through an sbir phase ii grant to calibrate our patented acoustic disdrometer against other commercially available rain gauges in a scientific setting. we chose hilo as a deployment site because it gets roughly 3.3m of rain per year (about 130”). with rain events almost daily, it is among the ideal planetary locations for rainfall testing due to an almost continuous flow of calibration targets.[1] after installing a mark there in 2016, our device began operation in earnest. the arable mark nestled in the arms of the ott parsivel2 laser disdrometer for constant updating and perfecting our acoustic disdrometer calibrations. what we did not anticipate was a hurricane passing by the island that would give us front-row seats to a catastrophic weather event as it unfolded. our engineering, hardware, data science, and customer success teams collectively monitor all of our deployed devices for anomalies that may affect their performance, but our test devices like the one at hilo tell us a different story. calibrating against other gold-standard instruments at internationally recognized measurement sites helps us account for more subtle errors so we can continuously improve on our data quality. we have test locations across the united states, but the hilo location has been our longest-running continuous installation. in late august, 2018, as hawaii battened down to protect its residents and businesses from hurricane lane’s fury, we hoped for the best from our offices thousands of miles away. this isn’t the first major storm event we’ve tracked through arable marks. last year, one of our devices was deployed in northern florida during hurricane irma’s barrage; although it was located inland and away from the hurricane center, the building on location was demolished by the storm. despite the destruction, the mark withstood the storm. we were still unsure of what to expect with hurricane lane. the slow-moving cyclone did weaken as it approached the islands, but managed to dump an extraordinary amount of rain. we watched the sweeping of the rain bands over the weekend through the pressure drop normally associated with hurricanes and wind. this coincided with the most extraordinary rain event we’ve yet witnessed with arable marks. in three days we saw over 1500mm of rain in hilo (nearly 5 feet!), and watched with concern as the state grappled with mudslides and flooding. we were relieved to hear our colleagues on the island weathered the storm without incident, and happy to see our test equipment remain active and online throughout the storms — no small feat. our chief data scientist, danielle watts, explains some of the data gathered by the mark and the laser disdrometer we’re calibrating against, below. top to bottom: downwelling; atmospheric pressure; temperature/relative humidity; and total precipitation as measured by the mark and the ott parsivel2 at hilo. the yellow peaks in the top figure depict downwelling short-wave energy, measured by a pyranometer on top of the device. the staccato valleys show the interruption of daytime light each time a storm band passed over (and, of course, nighttime in the wider troughs). the second figure shows the atmospheric pressure and sea-level pressure during the event, measured in millibars. large tropical storms are associated with drops in pressure; with hurricanes, we often see the dip in pressure each time one of the storm bands passes over, as shown here in correlation with the rise and fall of the downwelling radiation from the sun above. the third figure shows temperatures and relative humidity. warm sea surface temperatures contribute to hurricane development , and here you can see the warm, humid environment of the hawaiian island. hawaii gets relatively few hurricanes because of the cool ocean waters around it, but sea surface temperature models suggest that climate change will bring warmer surface temperatures and more hurricanes towards end of the century. according to el niño watch , the sea surface temperatures around hawaii are averaging warmer than usual right now. the final figure above shows the total accumulated precipitation for the duration of the storm topped off at 1500mm as measured by the laser disdrometer. the hallmark of hurricane lane was its extraordinary rainfall; it turned out to be the wettest tropical cyclone on record in hawaii , and our device was perfectly situated to record the storm-driven drops in pressure that came with the rain bands. precipitation is just one of the dozens of data streams we track, and we continue to individually calibrate the disparate instruments that make up our device. we strive for hyper-accurate specificity within our larger goal of value-added data synthesis in a way that makes the whole equal to the sum of its parts. in other words, we’re seeing both the forest and the trees — in this case, via the rain that falls on them. our intent with test device installations is to build a solid baseline of data rooted in consistent, persistent calibration targets. from a purely empirical perspective, though, it was fortuitous for us to have such a dramatic rain event pass over a device situated solely to measure precipitation, and, as a data analytics company, really cool for us to be able to measure its output in real time. for more information on what we measure and the instruments that go into our device, please visit the arable web site. notes: we had been calibrating pyranometers against the rutgers pam site , a world-class station for radiometry, but weren’t getting rain events often enough to rapidly develop calibrations. hilo is on the windward side of the big island of hawaii, and when moist air hits the island and starts to rise in response to the big volcano, the moisture condenses and rain occurs. despite some seasonality to rainfall on the island, there is at least a small amount of rain most days. our other key rain test location is at the university of british columbia in vancouver, which has a lot of seasonal light rain, complementing the heavier rainfalls we see in hilo. environment agtech rain hawaii data -- -- written by arable 357 followers · 85 following natural resource management no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@chandukavar/thinking-in-big-data-part-2-transpose-matrix-for-large-datasets-8415a3955420,,,"thinking in big data part 2 — transpose matrix for large datasets | by chandu kavar | medium sitemap open in app sign up sign in medium logo write sign up sign in thinking in big data part 2 — transpose matrix for large datasets chandu kavar 2 min read · feb 12, 2018 -- listen share i would suggest you read part 1 first. in part 1 , i have explained how to find top n most-viewed products from a large dataset in e-commerce. the matrix transpose problem is very simple. let’s say if you have matrix m with below values: r/c  0   1   2 0   1   2   3 1   4   5   6 2   7   8   9 you can find the transpose matrix by turning all the rows of given matrix into columns. here is the transpose matrix of m: r/c  0   1   2 0   1   4   7 1   2   5   8 2   3   6   9 the format of the data is — row number: integer list of value: integer separated by space based on the data size, you would take a different approach to find the transpose matrix. for smaller datasets, we can easily implement this using any programming language. here is the solution written in scala: // transpose of matrix val input = list( (0, ""1 2 3""), (1, ""4 5 6""), (2, ""7 8 9"") ) def formatrow(row: string) = row.split("" "").map(_.toint) def printrow(values: list[int]) = { values.map(print(_)) println() } input .map(row => formatrow(row._2)) .transpose .foreach(x => printrow(x)) let’s try to understand how can we solve this problem using mapreduce. ( please try it yourself, before moving on to the solution. ) input: 0, 1 2 3 1, 4 5 6 2, 7 8 9 assume that data size is in tbs and data is partitioned into multiple splits. input/output of mapper and reducer mapper input: key = row, value = list of value mapper output: key = row, value = (row, (column, value)) reducer input: key = row, value = (row, (column, value)) reducer output: key = row, value = (row, [( col1, value1), ( col2, value2), ... ... ( coln, valuen)] ) here is the data flow diagram: dfd for matrix transpose solution in mapreduce nowadays, developers prefer to write in spark instead of mapreduce due to latter’s higher overhead in container launch and higher file i/o. but, i think an understanding of mapreduce paradigm is still needed to understand how bigdata processing frameworks process the data underneath. here is the solution written in spark: val inputrdd = sc.parallelize(list( (0, ""1 2 3""), (1, ""4 5 6""), (2, ""7 8 9"") )) def formatrow(row: string) = row.split("" "").map(_.toint) inputrdd .map{ case(row, values) => (row, formatrow(values)) } .map{ case(row, formattedvalues) => values.map( value => (row, value)) } .flatmap { rowvalues => rowvalues.indices zip rowvalues } .groupbykey stay tuned for part 3 wherein i will explain how to calculate the cumulative sum of one billion numbers. thanks for reading. if you found this blog helpful, please recommend it and share it. follow me for more articles on big data. thanks to shakti garg and noah pereira who reviewed this blog. i truly appreciate it. spark big data hadoop mapreduce problem solving -- -- written by chandu kavar 403 followers · 28 following data engineer at grab | ex-thoughtworker no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/ar-vr-journey/the-factory-vr-a-rube-goldberg-game-104e8f8fd4c6,,,"the factory vr - a rube goldberg game | by joachim atamna | ar/vr journey: augmented & virtual reality magazine sitemap open in app sign up sign in medium logo write sign up sign in ar/vr journey: augmented & virtual reality magazine · follow publication best place to learn about ar& vr. we share the latest ar/vr news, info, tools, tutorials, arkit, arcore, & more. follow publication the factory vr - a rube goldberg game joachim atamna 6 min read · may 20, 2018 -- listen share my aim with this project was to make a fully functional language compatible multi-platform vr game for pc. the premise is that you are a worker at “the factory” as a national consumer tester. your job is to test out the new highly technological gadgets the factory has prepared for you. the concept revolves around the classic rube goldberg game that challenges players to create contraptions that solve physics puzzles. you have to carefully place contraptions capable of guiding a ball through several collectibles and then on to a goal. the end product is a game where you entirely decide the solutions. there are endless possibilities. at times you might feel joy, frustration, anger and even shame. but nothing really feels as good as solving a really hard puzzle. seeing real-like physics at play in vr is extremely fun. when you add absurd physic-based objects into the mix the results can be extremely entertaining and surprising. in the making of this project i wanted to focus on the importance of what went into someones mind when playing a vr game. i asked myself “what made these kind of games immersive, fun and entertaining”. i reflected over my younger life since i would often find myself (as most children) running away from obligations. playing games or watching cartoons was an easy solution to simply forget about these obligations. i still remember how immersed i felt playing the classic goldeneye 007 on nintendo 64. i spent countless hours into this (now) graphically limited game. what made this game immersive to me? well, it was the simple fact that i was able to forget about the “real” world. it allowed me to still be a child when i did not want to meet the harsh reality. that is when i came to a conclusion; a lot of people for certain seek a different reality in games to simply vent and forget their daily lives. i think with out a doubt this factor holds the main key to immersion. discover latest jobs in ai, ml, big data & more conversational designer nlp / nlu engineer ai jobs & ml jobs big data, data engineering & data science jobs product manager design choices it was important for me to create a game which allowed you to simply play, enjoy and “switch reality”. from my thought-process i made a few key notes which would guide me: intuitive and fun game mechanics. minimalistic or no ui. immersion in the form of mental involvement (emotion through level design). (early build of cannon and control panel) most of the time was spent into designing and implementing key features i wanted in the game to fit for vr. i wanted a fully functional control panel to work with a cannon in the game. to create an intuitive experience the buttons and lever had to be physics based. getting the cannon to work with the interactive control panel was extremely fun. it took some time and effort to wire up all the functions together, but the end result felt satisfying to use. personality nft | get my personality nft to invest emotion into the player, the games puzzle and mechanics gets gradually more complex. i created two levels which would work as a tutorial. the point of this was to make the player naturally learn the core mechanics before the main levels. after a short dialog at the first stage in the tutorial the player gets greeted by two simple semi-transparent world space ui’s in his front view. my aim for these tutorial-styled ui’s was to make the player understand the controller scheme by simply looking at the highlighted areas in yellow. (early build of battery snapping) after you teleport the ui’s gradually fades away. your first mission is to power up the cannon. there are several batteries scattered around distanced close to the cannon. the battery holders has a big collision detection, and simply dropping the battery close to it will allow it to snap in place. placing three batteries on the cannon will power it up. this will allow for teleportation on the platform the “control panel” is on. your next task now is to get familiar with the mechanics of the cannon. (final look of control panel) there are no guidance other than the text on the panel. the text is self-explanatory and sets up success by trial and error. after firing a couple of shots the game process you to the next stage of the tutorial. this part enables another gadget, the “object spawner”. another two semi-transparent world space ui’s fades into the players mid-peripheral view showing the new game-objective, and how to use the “object spawner”. your job now is to carefully place contraptions capable of guiding the ball into the goal. after solving the first puzzle you are done with the tutorial stage of the game. (the object spawner gadget) this is where the main concept of the game “rube goldberg” starts to shine. the level design gets gradually more elaborate and complex as you complete levels. eventually you have to make the ball go through several collectibles before hitting the goal. the available contraptions at some levels are also limited to only a few. in total there is 6 different levels with very different level design. some levels focus on obtaining many collectibles, and some levels focus more on limiting the availability of different contraptions. at a certain point the level design gets so complex, tracking the ball with your own eyes gets difficult. this implies that adjusting upon failure gets near impossible at some of the higher levels. by then another asset is added to the scene, a monitor which tracks the ball. (level 5) the future of the factory upon user testing the final build i could clearly see how my test subjects went through a lot of different emotions. my players had a big anticipation of watching the outcome of their work, joy from completing the level and frustration from failing. depending if the tester had played through the levels before, the average time of full completion was very different. the game could take somewhere between forty minutes to one and a half hour to complete. without a doubt i could clearly see the players feeling immersed. the user testers was way off when guessing how long they had played. most of them had guessed a much shorter time than what they had actually played. in that sense i would say i was successful at achieving my goal; the players could simply play, enjoy and “switch reality”. which again would allow them to forget about their real reality, even if it was for a little while. i am proud of this project and i feel accomplished. playing through the game myself i have a lot of fun. from user testing i am also made to believe that many others can get enjoyment from this game as well. i have therefore planned to release this game for free at one point in the future for others to enjoy as well. virtual reality vr oculus rift augmented reality ar -- -- follow published in ar/vr journey: augmented & virtual reality magazine 5.5k followers · last published aug 26, 2024 best place to learn about ar& vr. we share the latest ar/vr news, info, tools, tutorials, arkit, arcore, & more. follow written by joachim atamna 11 followers · 3 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@Naturalish/is-flubber-sentient-e0d844c17226,,,"is flubber sentient?. this film causes me countless grave… | by naturalish | becoming human: artificial intelligence magazine sitemap open in app sign up sign in medium logo write sign up sign in becoming human: artificial intelligence magazine · follow publication latest news, info and tutorials on artificial intelligence, machine learning, deep learning, big data and what it means for humanity. follow publication is flubber sentient? naturalish 8 min read · mar 27, 2018 -- 1 listen share naturalish: asking the hardest-hitting questions in modern science. this film causes me countless grave concerns, and i’m sure i’m not alone. the 1997 disney classic flubber means many things to many people. to me, despite a few gloomy plot-threads, i look back on the film fondly — it’s a hilarious romp that delivers some top-tier robin williams as the great chemist professor phil brainard. great writing too. to the washington post upon the film’s release, however, the film represents something a bit less frivolous : the movie has the loopy, dipsy-doodle sense of having been imagined by someone in the immediate aftermath of an intense wall-skull experience, reducing the old iq by about 85 points…with your ticket, you should get a coupon for a free brain scan. harsh. this scene is seriously precious. no joke. but there is something a bit deeper afoot. the feel-good vibes i get from the film don’t necessarily take into account a dark possibility lurking beneath the glossy green surface. if phil brainard, in his disheveled basement laboratory, managed to artificially synthesize a sentient being in the form of flying green rubber…that’s some pretty profound shit. phil and his colleagues proceed to manipulate flubber like a simple tool to fit their whims, and yet all the while this essence may — quite possibly — be aware of its own existence. it may be capable of love, loss, and fear. what the fuck. trending ai articles: 1. neural networks for algorithmic trading. multimodal and multitask deep learning 2. back-propagation is very simple. who made it complicated ? 3. introducing ozlo to put my mind at ease, i searched the web for an analysis of flubbers’ capacity to feel. that analysis didn’t exist, so i’ve now been reluctantly forced to take on the task myself. let’s dive butt-first into the truth: is flubber sentient? first things first, we need to define flubber. this part’s easy. if you ever invent a strange goo… put your face in it. flubber is an elastomer , at least that’s how it’s described by its all-powerful creator, phil. “highly viscous, and yet you can phase shift. you’re moldable and a little ticklish. you’re foldable, you’re gullible…and you’re ductile, you’re elastic. let’s just see how elastic you are.” let’s just! before too long, the audience has come to love flubber and its bizarre ways, and even the aforementioned washington post article is taken by its charm: now flubber has personality! another giant leap for mankind! a sticky goo that persuaded gravity to take a nap in the first one, flubber now has been computer-agitated so that it looks like animated lime jell-o, a shimmering, translucent, almost sentient being that purrs contentedly. there’s that word again: sentient . the reviewer of the film (clearly an adult with no remaining sense of frivolity) just assumes such a dark, torturous truth behind the movie’s title protagonist. but now that we know “flubber,” let’s take a look at this other word, and how we can actually gauge the sentience of a fictional creation of the silver screen. for that, we’ll be using three well-regarded experiments from the sciences: the shutter box test , the mirror test , and the turing test . please hold all questions until the end of class. “sentience” is a bit tough to define universally, but that won’t stop us from trying. often, practical studies of sentience are oriented around animal ethics or speculative artificial intelligence, and for now we’re going to focus on the former. flubber might not be a breathing, fleshy critter, but it comes close enough. according to the nonprofit animal ethics inc , “there are three general criteria for deciding whether a being is sentient. these involve considerations that are (1) behavioral, (2) evolutionary, and (3) physiological.” for flubber, we’re really only looking at the first area: behavior. and boy is this a rocky road. ironically, it’s only inhumane if the animal passes. one specific test used to gauge behavioral sentience in fish is known as the shuttle box test , where a fish is conditioned “to swim from one box to the other in response to presentation of a colored light…in order to avoid electrical shock.” this test hopes to instill “fear” in the animal, which according to researchers, is a demonstration of basal sentience. to a certain degree. would flubber pass? absolutely. towards the end of the film, after being stolen from professor brainard and eventually reuniting with its creator, the flubber is happy to see phil and lashes out with a fierce growl at the two henchmen who took it captive. flubber trusts. flubber loves. those henchmen, by the way, are named smith and wesson. again, brilliant writing. so the flubber is successfully conditioned to fear certain individuals and take comfort in others — it’s able to recognize external cues and determine which outcome, between two competing scenarios, benefits it most. according to our first metric, flubber is definitely sentient, at least on the same level as a fish. that might not be saying much, but such a thought experiment helps to distinguish between creatures of lower intelligence, like mice and fish, being compared to a beetle or a worm. tests like the shuttle box help draw these lines. but what about for higher life forms? sentience is defined a bit differently when expanded to consider which mammals may or may not be “self aware,” a label often pinned to a metric known as the mirror test . now we’re talking. the one public domain image for “mirror test” happens to be this gem. developed in 1970 , this test places an animal in front of a mirror after first marking the subject with a odorless dye. for the animal to “pass,” it must react as though the dye has been placed on it’s own body — sounds simple enough, but the test has only been passed by a few species : chimps, dolphins, elephants, and magpie to name a few. a human child is only expected to pass the test after around eighteen-months of age. the test is used to estimate self-awareness in animals through intelligence, consciousness, and sentience , although as noted above, it’s not a perfect bias-free analysis. simply put, it gives us a consistent and repeatable method for understanding how creatures think about themselves. so. flubber. at no point in our 1997 case-study does flubber look at itself in a mirror, so we’re going to use the next-best thing. boy is it a doozy. this scene may not be scientifically-sound. in one of the film’s most memorable scenes, weebo and webber (the professor’s two household robots) let flubber out for an evening of fun — and things goes crazy. flubber winds up splitting into dozens of smaller copies, and during such escapade, two flubbers dance a wild and crazy conga. lots of data to unpack. according to this source material, flubber can self-replicate into seemingly autonomous… “modules.” these modules can improvise and interact with the “original” flubber, if such an individual even exists at this point. now, how does this link back to the mirror test? the true essence of the experiment is about whether or not the subject is able to create an idea of a “self” and differentiate that autonomy between that of other like individuals. and, well…it seems to be that flubber can do just that. or, at least, it’s able to create a non-self and interact with the other flubbers before, i suppose, “reingesting” this non-self into a new singularity. in short, the results of this type of test will remain unclear, but i think flubber falls into a healthy gray area. our last test is a bit controversial, but it’s definitely worth including in our analysis: the turing test . most often, this examination isn’t normally applied to animals or synthetic polymers, but rather computer programs and artificial intelligence that are attempting to pass off as “human-like.” alan turing developed the experiment to help refine our definition of “thinking” when applied to computers. according to britannica , “turing sidestepped the debate about exactly how to define thinking by means of a very practical, albeit subjective, test: if a computer acts, reacts, and interacts like a sentient being, then call it sentient.” i’m not going to overreach, flubber would bomb a turing-style exam. while the polymer seems to have an intellect on par with a dog or young child, it’s certainly not a degree of sentience that could pass of as another fully-intelligent human. so, on the spectrum of sentience, we should only feel somewhat bad about torturing flubber, but it’s not the same as holding a human being captive and forcing them into a lifetime of scientific research. but you know who could pass the turing test?! weebo!! twist! that’s right! there’s no way i’d get through a full flubber post without mentioning the clearly greater scientific achievement lurking in professor brainard’s collection: weebo the fully sentient, flying, artificially intelligent robot. if phil and his colleagues are struggling financially, it’s impossible to fathom how flubber would solve all their woes but a fully-autonomous robot who can project holograms (?!) doesn’t seem to have the answer. now, the film does drop a hint, i suppose. weebo’s dying message to phil seems to imply that her designs are not public knowledge nor are they something phil has available. was she a design of phil’s that he managed to forget? or a gift from some higher life form? gosh the possibilities are endless. oh, and on the topic of impossibilities in the flubber canon, i simply cannot understand how sara would have given phil two free passes on forgetting his own wedding. and on the fourth and finally successful attempt, she doesn’t even ask him to show up?! he just teleconferences in! come on sara, draw the line somewhere. not to mention that wilson — a rival who openly bragged about stealing phil’s ideas and wanting to seduce sara—was invited to the wedding? oh and with almost complete certainty i can say that using flubber in a basketball game would be far more a liability that a benefit. in short, there are three main takeaways from flubber that i find it important to share with a world who may have forgotten this gem of a film. first, flubber has the sentience of a large mammal, and that’s important to understand. second, phil probably has early-onset alzheimer’s and his friends should be a bit more concerned. and lastly, let’s never forget that the movie ends with archetypal disney villain christopher mcdonald taking an unfathomably destructive flubber shit. classic. as i started this article: the film gives me a few grave concerns. i hope, dear readers, you now understand why. artificial intelligence computer science machine learning ai data science -- -- 1 follow published in becoming human: artificial intelligence magazine 39k followers · last published jan 30, 2025 latest news, info and tutorials on artificial intelligence, machine learning, deep learning, big data and what it means for humanity. follow written by naturalish 7.6k followers · 46 following explore the natural history of sci-fi, myth, and fantasy—where science meets the truly absurd. now a podcast on itunes and at naturalish.libsyn.com !! responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/netanelbasal/testing-observables-in-angular-a2dbbfaf5329,,,"testing observables in angular. in this article, i’d like to talk about… | by netanel basal | netanel basal | medium sitemap open in app sign up sign in medium logo write sign up sign in netanel basal · learn development with great articles testing observables in angular netanel basal 3 min read · apr 4, 2018 -- 14 listen share in this article, i’d like to talk about a misconception i’ve read in other articles about writing tests for observables in angular. let’s examine this basic example we’re all familiar with. todos component spec we have data service that uses the angular http library to return cold observable. other articles around the web suggest that, in order to test the above component, we can create a stub service that returns an of() observable. todos component spec you run the code above. the test passes, and you are 👯 👯 👯 👯 👯 that’s what i call cheating 🙈 by default, the of() observable is synchronous, so you’re basically making asynchronous code synchronous. let’s demonstrate this with a small add-on to our code. todos component we added a loading element that should be visible when the request begins but hidden when the subscription function is called. (i.e., the request finished successfully) let’s test it. todos component spec as you likely imagined, the above test will never pass. you will get the error: expected null not to be null. when we run detectchanges() , the ngoninit() hook will run and execute the subscription function synchronously, causing the isloading property to always be false. as a result, the test always fails. you probably remember the old days, when we wrote tests in angularjs and stub promises like this: spyon(todosservice,'get').and.returnvalue(deferred.promise); the code above is completely valid — unlike observables, promises are always asynchronous. let me show you three ways to correct the above. using defer() those who’d rather stick to promises can return a defer() observable that will return a promise with the data. todos component spec we use defer() to create a block that is only executed when the resulting observable is subscribed to. using schedulers schedulers influence the timing of task execution. you can change the default schedulers of some operators by passing in an extra scheduler argument. todos component spec the async scheduler schedules tasks asynchronously by setting them on the javascript event loop queue. this scheduler is best used to delay tasks in time or schedule tasks at repeating intervals. you can read more about schedulers here . using jasmine-marbles rxjs marble testing is an excellent way to test both simple and complex observable scenarios. marble testing uses a similar marble language to specify your tests’ observable streams and expectations. todos component spec this test defines a cold observable that waits two frames ( -- ), emits a value ( x ), and completes ( | ). in the second argument, you can map the value marker ( x ) to the emitted value ( todos ). here are a few more resources to learn about marble testing: the official docs . marble testing observable introductio n. rxjs marble testing: rtfm follow me on medium or twitter to read more about angular, vue and js! 👂🏻 last but not least, have you heard of akita? akita is a state management pattern that we’ve developed here in datorama. it’s been successfully used in a big data production environment for over seven months, and we’re continually adding features to it. akita encourages simplicity. it saves you the hassle of creating boilerplate code and offers powerful tools with a moderate learning curve, suitable for both experienced and inexperienced developers alike. i highly recommend checking it out. 🚀 introducing akita: a new state management pattern for angular applications every developer knows state management is difficult. continuously keeping track of what has been updated, why, and… netbasal.com javascript angular testing web development -- -- 14 published in netanel basal 15.4k followers · last published jan 26, 2025 learn development with great articles written by netanel basal 60k followers · 1 following a frontend tech lead, blogger, and open source maintainer. the founder of ngneat, husband and father. responses ( 14 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://arthurfnsc.medium.com/postman-apis-testing-as-if-i-were-a-child-part-i-analogies-1ebba0d3aad4,Postman,,"postman: apis' testing as if i were a child [part i — analogies] | by arthur magalhaes fonseca | medium sitemap open in app sign up sign in medium logo write sign up sign in postman: apis' testing as if i were a child [part i — analogies] arthur magalhaes fonseca 5 min read · sep 2, 2018 -- listen share it is better not to make a vow than to make one and not fulfill it. ecclesiastes 5.5 i am back again. before the topic itself, i would like to inform you that i am trying to write my portuguese posts in english and spanish to force these skills a bit, so if i commit any more serious grammatical errors, your feedback will be very welcome. let me contextualize you a little where this idea came from: i am working at an interesting brasilia’s company called ais focused on mobile solutions, agile coaching , microservices , soa , big data . we work with a lot of server languages like java , kotlin , nodejs , scala , r , ruby , among others. with an approach focused on chapters and guilds , we began to organize and deliver forums for dissemination of knowledge. what i had realized was that some words were confusing in meaning among some people: apis for instance. i started to think about how to disseminate knowledge, however, without making it as dull and confusing as a speech by charlie brown’s teacher . head first’s books idea would be the more excellent way to share knowledge with that people. let’s start with an analogy for apis : the waiter . i learned this analogy in my company, and i decided to expand on other scenarios. we will use rest api for future examples. our interaction with a waiter in a restaurant is similar to an interaction with an api . in this example, we as a customer’s restaurant or consumers interact with a waiter ( api ). the waiter interacts with other people so that the request we requested comes to our hands in a given time. another interesting point to note is that there are other interactions in the restaurant that we often do not realize, such as the interaction with the financial part, the distributor’s food request, etc. so the restaurant’s api is not restricted to those who ordered their food. back-end languages and response time (sla) it is interesting to note that most of the time we do not have access to restaurant cooks. whether they are japanese, brazilian, indian, etc. is not a big problem for us, as long as the food arrives at our tables. in this analogy, the kitchen would be our back-end language , which could have increased or decreased resources are given days of the week and holiday ( scalability ). another important thing is that the delivery time of our request would influence us in our experience with the restaurant, something like the service level agreement ( sla ). http status code rest apis should follow standards and conventions such as http status code . if you still do not know this, take a look at this site , and try not to “lose” for at least 15 minutes. then imagine the following conversation in a restaurant and its “status”: client : waiter , could you bring me some water? waiter : ok ( http status code : 200 ). client : waiter , could you bring me some passion fruit juice? waiter : at the moment we are not having passion fruit juice ( http status code : 404 ). client : waiter , could you please bring me some orange juice? waiter : sorry for the inconvenience, but we just had a problem in our kitchen, and we’re not going to get you ( http status code : 500 ). api specification imagine that we arrive at a japanese restaurant and request for a lot of sushi, and the waiter says he does not work with sushi ( http status code : 501 ). we then ordered a portion of sashimi, and the waiter says he also does not work with sashimi ( http status code : 501 ). we could go on asking the waiter several times and wait for his answer, or to be a more assertive, request for the restaurant’s menu . in that case, the menu would function as a sort of documentation or specification of what the restaurant offers. we’ll talk more about api specifications and documentation in the second part of this post, but for now, this analogy is enough. we already know what happens when the client makes a request to the waiter that cannot be processed but have you ever imagined the problem of an outdated menu ? once, i went to a restaurant with my wife, and after asking for a specific drink from the menu , we received something completely different. we told the waiter that it was not what we had ordered from the menu . the menu was outdated according to the waiter . they no longer had the drink and returned another thing. i questioned him that the specific drink was on the menu . he tried to do a similar formula. in short, a not very pleasant experience because it generated an expectation in us. it was better not to have the menu than it’s outdated. in the apis ’ universe, this is very common. often the documentation of multiple apis is outdated compared to what it should do, which only makes it difficult for the client to interact with the api . what have we learned so far ? a waiter would be an interaction api with a kitchen and other services at back-end. the response time of our order ( sla ) in the restaurant impacts on the experience we have with it. there is a status convention for our interaction with apis . documenting your api can make it easier for the client to interact with it. not documenting your api can be a problem for the client . incorrectly documenting your api is an even bigger problem. it would be better to avoid generating the wrong expectation. i will end this post here, in the next part we will talk about testing apis with postman . see you. api api development rest api postman -- -- written by arthur magalhaes fonseca 156 followers · 227 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/s/nautilus-power/atheism-the-computer-model-670cd862a5d4,,,"atheism, the computer model. big data meets history to forecast the… | by nautilus | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story atheism, the computer model big data meets history to forecast the rise and fall of religion nautilus 12 min read · oct 4, 2018 -- 5 share illustration: david plunkert by michael fitzgerald i n the united states, the nones have it. the nones being people with no organized religion and increasingly no belief in god or a universal spiritual power. they have the momentum, attention, and an expectation that in the future they will become a majority of the population, just as they currently are in western europe, japan, and china. or so says the pew research religious landscape study, which in 2015 found that almost a quarter of americans profess no religious affiliation. within that group, a third do not believe in god or a higher power of any sort (“nothing in particular,” as the study termed it). both numbers are up from a similar study in 2007, when 16 percent of the country professed no religious affiliation, and 22 percent of these did not believe in god. driving the growth are millennials, those born between 1980 and 2000. as they come of age, 70 percent of them say they do not believe in a higher power. reasons to not believe: boston university philosopher and theologian wesley wildman (above), leader of the modeling religion project, presents factors such as “existential security” and “free self-expression” that explain why societies become “post-supernatural.” photo: jenn lindsay pew expects the percent of religious americans will continue to fall. it suggests older generations will… -- -- 5 written by nautilus 20k followers · 240 following a magazine on science, culture, and philosophy for the intellectually curious responses ( 5 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/codebrace/finding-maximum-subarray-without-repetation-a1e7e2b8e50f,,,"finding maximum subarray without repetition | by ashish patel | codebrace | medium sitemap open in app sign up sign in medium logo write sign up sign in codebrace · coding blog to help people get going with competetive programming, big data and other technologies, visit http://medium.com/codebrace finding maximum subarray without repetition ashish patel 2 min read · jan 12, 2018 -- 2 listen share contributed by anurag gupta, i got this question while giving hiring test for microsoft, the question is very simple, we are given an array with positive numbers , we have to find subarray with maximum sum with unique elements. this is a variation of classical larget subarray sum problem ( kadane’s algorithm ) , shown below. input first line of input takes number of elements in the array let say n , second line contains n elements , each representing one element of array. output first line of output prints ans i.e. subarray with maximum sum second line contains the sum of subarray printed above. # test case 1                            #output 5                                         subarray: 2 3 1 5 1 2 3 1 5                                 max_sum:11 #test case 2                             #output 6                                         subarray:3 5 2 1 2 3 3 5 2                               max_sum:10 #test case 3                             #output 4                                         subarray:5 5 5 5 4                                  max_sum:5 approaches there may be couple of approaches to solve that problem, first might be the brute force approach which could be implemented in o(n³) . bruteforce approach we generate all possible o(n²) subarrays and then we choose the one having maximum sum out of those having unique elements which will cost o(n) giving us o(n³) solution. using kadane’s algorithm kadane’s algorithm can be modified and used in this example which lets us solve that problem in o(n²). kadane’s algorithm wiki geeksforgeeks we can start from empty subarray as answer start including one element until any duplicate element comes in that array which will take o(n), for finding if this has already came or not be will have to go through start to end of currently selected subsubarray with no duplicate members. we will check if this subarray has max sum or not and store in final_start and final_end which has sum as final_sum. using kadane’s algorithm with hashing we can use hashing in above approach to find if an element is there in current selected subarray by using hash, this will give us searching in o(1) , to overall time complexity will be o(n). #code(c++) if you have any question or any problem please comment or contact us. programming competitive programming cplusplus coding hackerrank -- -- 2 published in codebrace 192 followers · last published nov 24, 2024 coding blog to help people get going with competetive programming, big data and other technologies, visit http://medium.com/codebrace written by ashish patel 246 followers · 62 following big data engineer at skyscanner , loves competitive programming, big data. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@squadex/how-big-data-helps-protect-endangered-species-ee876e5e4cc6,,,"how big data helps protect endangered species | by squadex | medium sitemap open in app sign up sign in medium logo write sign up sign in how big data helps protect endangered species squadex 4 min read · mar 20, 2018 -- listen share determining whether or not a certain species are at risk of becoming endangered, requires years of vigilant monitoring of the species by scientists. however, thanks to the power of big data, the process is expedited dramatically, by applying big data analytics to rainforests and other habitats. as a result of such analyses, conservation international found that species which were not considered to be at risk, are experiencing sizeable population decreases. in fact, it was determined that populations were dropping by about 30% among armadillos, anteaters, wild boars, civets, moon rats and shrews. a wild boar in a malaysian rainforest how it all works the vertica platform, designed by hp, was designed to provide early warning signs amongst animal numbers who are at risk of extinction. the system collects data from thousands of cameras and climate sensors and uses the data to collate information on population numbers. this information is then fed into the big data platform, which allows for fast and accurate readings that help target specific species or areas. obstacles & solutions the major obstacle that researchers were facing was not that this information was impossible to obtain, but, due to the large quantity of images, it was almost impossible to manage. researchers would spend weeks, sometimes even months analyzing the data, making collations and then drawing conclusions. however, time is luxury they do not always have. due to the time sensitive nature of the work, if the endangered population is found too late, it could mean the end for this species. thanks to the big data platform provided by hp, processing speed has almost doubled, thus providing an effective early warning system and, by doing so, freed up a lot of time for the personnel to actual make a difference and act on the data received. the effect of big data on the environment since the project began in early 2014, the system has more than 1.4 million photos from the monitoring cameras and has created more 3 terabytes of analyzable data. plus, it also measures the habitat as well, allowing scientists to determine the cause of population decreases. the fact that the system can pick up even the smallest changes within the environment, means that scientists can account for this change and mitigate the impact in the future. thanks to big data, conservation research is much better which, in turn, protects endangered species. researchers are using supercomputers and biotelemetry, to create 3d models of the terrain and the movement of endangered species. these technologies are being used today by the us geological service in tandem with the institute for conservation research at the san diego zoo. they developed a new methodology which uses advanced 3d technologies to provide data on the movements of wildlife species. zmore details on their finding and methodologies can be found here. examples of aquatic, avian and terrestrial biotelemetry data sets and their spatial domains. even though 3d modelling is more expensive than traditional 2d, researchers were able to make faster calculations, what used to take four days, was accomplished in less that 30 minutes, and they were able to write the data in a more compact form enabling rapid input and output. a key goal of the project was to allow experts to remotely visualize the data stored within the supercomputer since it was of critical importance to minimize data flow and replication, especially when data volumes begin to grow. future goals the next goal of the project would be to combine the data from all the data sources to get a clearer understanding of animal habitats. ecology is entering the age of big data which means that cutting edge analytical and visualization tools are vital to process, manage and analyze ever growing volumes of accurate biotelemetry datasets. otherwise, a deeper and clearer picture of the entire animal ecosystem will not be possible to determine. maxim tereschenko is the head of bi / big data practice at squadex.com . led global consulting and outsourcing data projects. managed connectivity area for zoomdata, one of the fastest growing big data analytic platforms. built comprehensive analytical solutions for the fintech sector from scratch. a true believer that proper data analysis is a key to significant improvement of business operations, and maybe even the world. connect with him on linkedin . conservation -- -- written by squadex 10 followers · 31 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/the-coffeelicious/dna-identity-and-the-scramble-for-global-citizenship-99dd0dde11d0,,,"dna, identity, and the scramble for global citizenship | by efe nakpodia | the coffeelicious | medium sitemap open in app sign up sign in medium logo write sign up sign in the coffeelicious · home to some of the best stories on medium. look around, relax and enjoy one with a sip of coffee. dna, identity, and the scramble for global citizenship efe nakpodia 5 min read · may 28, 2018 -- 4 listen share bueroschramm there’s been a lot of talk recently about white people wanting to be black. from the likes of australia’s iggy azalea to the infamous rachel dolezal, it is pretty clear that there are white people who seem to identify more closely with aspects of black culture. so here’s a pretty weird story. after sifting through all the noise about white people talking and dressing black, i eventually stumbled upon a fairly well known millennial who has taken this craze to a whole new level and has basically broken the internet with her apparently factual claims of black ancestry. she is none other than youtube’s very own woah vicky. “unlike iggy and rachel, she isn’t just claiming or acting black.” this very outspoken young white american girl has been blazing a trail for all the white people who feel that they’re black. the only difference with vicky however is that unlike iggy and rachel, she isn’t just claiming or acting black —she feels that she is black, and she has the proof too. ancestry dot com when woah vicky starts talking (in the ridiculous way she speaks) about how black she is, she does it from a place of confidence… and that confidence of hers seems to be powered by ancestry.com. ancestry.com has been raking in the money lately. from a company that started as a simple search engine where people could type in their names and find long lost relatives, it has since grown big enough to offer a more inclusive service to people whose families wouldn’t have been lucky enough to be included in european or american registries of records. whenever woah vicky speaks of her black heritage, she always makes it clear that she has the proof from ancestry.com, who she sent saliva samples for dna testing. the result? woah vicky’s ancestry.com's test certificate shows that 44% of her dna stems from african origins . now, i’m not going to knock vicky for claiming a black identity based on the results she received, but this is where the discussion with my friends became really interesting. skepticism and conspiracy theories as we watched woah vicky’s youtube rants and laughed our heads off, the conversation soon turned from humorous banter to healthy skepticism about what ulterior motives might be lurking behind the recent popularity and widespread use of ancestry.com et al. some made jokes about how the western world may finally be ready to pay black people reparations for the ills of slavery, and how white people may be preparing themselves to make a claim if and when that does happen. lol! image, courtesy of phys.org others were more concerned about how these companies may be collating and storing dna data in order to sell to the highest bidder. with all the ruckus around big data and privacy, this point actually made me stop and think for a moment. how do the owners of dna servers plan to use such valuable information in the future? who actually owns the information? the users of such platforms or the service providers themselves? some even joked about how this movement may be a modern reenactment of herod’s search for jesus, or the coming messiah. conspiracy theories aside, i started thinking more seriously about the socioeconomic impact of this growing trend and the more i thought about it, the more i could see some of the possible ways it could effect change. a brave new world as interesting as woah vicky and her theatrics are, she wasn’t the only person i watched on youtube who had taken a genetic test . but i was so fascinated by her that i clicked and watched several related videos and discovered there were many people who had taken the test and were happy to share their results with the world. i also discovered that they weren’t all white. i watched many black, and white, and mixed-race people proudly show off their charts and all the places in the world it said they originated from. i haven’t done the test myself (maybe i am a little skeptical) but i did imagine how i’d feel if i was one of the people i had been watching on youtube. the first thing that came to mind was that i’d probably want to visit all of the places that were named in my dna test chart. and if most people thought like me, that would mean ancestry.com and the like would have unwittingly (or wittingly) created a whole new class of tourists and tourism. image, courtesy of fee i mean, think for a second: if your dna test results showed that you hailed from mali, belgium, mexico, and singapore, wouldn’t you want to visit these places at some point in your life? i know i would. this much travel and tourism brings me to my next point which i’ll present as an open question: if people have tangible proof of heritage to the places and countries in their charts, does that mean they can have passports for every nation in their chart? one of my friends said “no way… they’d never allow that.” when i asked why not, he said one could only obtain national identity cards or passports either through birth in that country or by being naturalised there. i said, well, if my blood says i am from there, how much more naturalisation do i need? and what’s the point of having a dna certificate if it can’t be used legally? i get his point however, but do you get mine? “it is not about the iggy azaleas, the rachel dolezals or the woah vickys.” this raises some serious questions about what it really means to be human, born here, on planet earth. watching so many people discover the secrets held in their blood and seeing that everyone i watched had some connection to almost every continent on the planet, isn’t it about time we realised that we’re all just earthlings from the same human family? i know there are those who will argue till the end of the world that we aren’t all the same and blah blah blah, but when we put all the dubious belief systems aside and look at the truth behind all things, surely you must be able to see the me in you as clearly as i can see you in me. if we carry on down this line of thought, isn’t it clear that racism will eventually become redundant? in the end, i’m sure we will finally come to realise that it is not about the iggy azaleas, the rachel dolezals or the woah vickys. it’s not about big data companies or conspiracy theories either. it’s about you, and me, and how we feel about ourselves in this world we all call home. it’s about identity and community. ultimately, it is all about humanity and what it means to just be in this world of black, and white, and all the beautiful colours dancing within the spectrum. race culture racism future identity -- -- 4 published in the coffeelicious 199.1k followers · last published apr 22, 2023 home to some of the best stories on medium. look around, relax and enjoy one with a sip of coffee. written by efe nakpodia 2.5k followers · 139 following —i am an imagist • idream • outloud • my fourth book of poetry titled “ifell in love; i’m sorry” is now available on amazon:) xoxo responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@kyle.powers103/increasing-use-of-data-in-competitive-sailing-racing-bb8f0e139c4d,,,"increasing use of data in competitive sailing racing | by kyle powers | medium sitemap open in app sign up sign in medium logo write sign up sign in increasing use of data in competitive sailing racing kyle powers 7 min read · nov 8, 2018 -- listen share college sailing practice crew (not sailing) a lot of people in my life hear that i sail and for some reason they think of crew….. so the above photos break it down for those people… when i was a kid the only way to test if your boat was setup properly was to find another boat and sail next to them and if they were significantly faster then you, then you probably had to change your rig setup. as i got older i became a coach and we had a few more tools at our disposal; however, the tools were mostly for boat setup and weather updates for lightening (lightening cancels sailboat racing as each boat has a big metal pole pointing to the sky). the boat setup tools really just measured things like tension on a wire, and weren't that helpful once you got on the water. finally when it came to technology as a coach, the most “techy-savy” coaches had speakers on their boat. the speakers didn’t help the sailors in any way but they provided entertainment when the wind died. in the last decade the sport of sailing has seen a massive influx of technological advances that has had drastic changes on the space and large part of that is due to big data analytics. in this blog post i am going to highlight some of the changes and the new tools that are supplying massive amounts of data that are being used and changing the sport. these changes vary from real-time decision making during a race all the way to the actual construction of boats. the americas cup…. has been changing the game if a person has seen one sailing race in their life, it is most likely the america’s cup races. that has been the point in recent years, to get bigger boats going faster to attract new spectators to the sport. this makes a lot of sense as most races are held pretty far off the coast where the wind is and you need a boat to watch the races. to make the boats go faster there were teams of scientists and engineers that came up with the massive ac-72s seen above. however, in order to make these boats go so fast ( ac-72s go 44.15 knots ( 50 mph) in 15.8 knots of wind (2.79 times the wind speed ), there was a lot of data analysis that needed to occur. these new boats broke so many barriers that so many new factors came into play requiring massive amounts of research and data to configure. at such high speeds, even the materials of the boat need to be altered because the 3 tons of weight combined with the forces propelling that boat even made the foils extremely hot. water actually boils around the foils above 20 knots. (foils are like plane wings under the water that look like they are holding the boats up on stilts). america’s cup sensor data: team oracle usa has stepped up their data game dramatically in recent years and have a solid amount of details on their team home-page(data analytics obviously being provided by oracle). for the recent america’s cup, the oracle ac 72 had over 1,000 sensors to collect information on the weather, the boats integrity and the sailors health. every-time the boat went out for a sail to race or practice the team generated between 250gb–500gb of raw data. weather models: the oracle team also recently started developing their own models. these models taking into account years of local sensor and cross-validation data to understand what a particular mix of conditions (water and air temperature, barometric pressure, tide, ocean swell, water depth around the course, etc.) might do to wind and current gradients over the course of the day. sap sailing wind model forecast the oracle team started in recent years implementing data science into almost every application of developing, training and sailing these ac-72s.the team used the data for several applications including “ real-time feedback to sailors”,”course planning and race playbook”, “yacht-design” and “deep analysis and research” sap sailing analytics platform sap sailing - homepage edit description www.sapsailing.com sap sailing offers one of the most advanced data analytics platforms currently on the market. for every race on the platform, there is a massive amount of information for each competitor. the metrics include every maneuver (turn basically) executed during a race, mark (buoys the boats sail around) passing time, time, speed in [kts], speed out [kts], speed change [kts], lowest speed [kts], max. turning rate [deg/s], avg. turning rate [deg/s], maneuver loss [m], direction change [deg]. below is an example of a race from the sap sailing from a race in san diego for the extreme sailing series. you can get to these analysis by going to the sap sailing link below, clicking on any race, clicking on the race tab, and clicking race details on the right hand side. the tool is very interactive and pretty fun to play with. you can watch the movements of all of the boats on a map with the buoys and course as well as the speed and other metrics of each boat. sap sailing demo sap data mining tool i was given access to the sap data tool and was able to fumble around with some of the data and was able to graph one race on plotly. (still figuring the tool out as it is very involved and my computer keeps crashing trying to run queries) first i chose the race i wanted ( world cup series 2018 — gamagori, japan — laser radial) and selected the two competitors i wanted to analyze (erika and paige, the two us sailing team laser radial sailors) . for this analysis i chose to get all of the data for each competitors tack/gybe details i.e. entry speed, exit speed, degrees turned. this data is below in the text editor atom. next i opened the data in plotly and chose just to graph and compare the exit speed of each maneuver and wanted to break it down by tacks/gybes per person. in plotly, this is pretty simple, i just uploaded the data and added two traces using the maneuver exit speed value column and had each trace assigned to either tack details or gybe details. from that (and fumbling around with the settings a bit) i was able to make the below graphs to compare the data. each dot point represents one maneuver’s exit speed while the box chart next to the dots represent the max, min, median and average speeds. what i found the most interesting is that this is just one data point from one race between two people. from this data you could make several conclusions as a coach and it is just a fraction of the data accumulated for one race and a fraction of a fraction of the data accumulated during the regatta. improving decision making in ocean race sailing using sensor data- volvo ocean race 2013 volvo ocean race boat in 2017, jos van hillegersberg, mark vroling and floris smit from the university of twente wrote a paper called “improving decision making in ocean race sailing using sensor data.” the paper outlined the implications from the 2017 volvo ocean race. the race is the “world’s pre-eminent round-the-world yacht race” and for the first time all of the boats had the exact same design (called one-design class) which included dozens of sensors that would relay information of each competitor during the race. “volvo ocean 65 has over 160 sensors on board which generate data for around 40 variables” “the data that is generated by the volvo ocean 65 fall one of 3 categories: wind data, navigation data and boat data” the data table from the paper is below: data-collection system:“ for the data storage platform, the university’s mysql server was used. for creating the visualizations and evaluation dashboard the data analytics tool tableau was selected based on availability, skills and resources and requirements to quickly build various visualizations on in-memory datasets. tableau is a software package that lets users create a wealth of visualizations. it is well known to be user friendly and relatively easy to use. this makes tableau suitable for doing exploratory data visualizations in this project. the data is transmitted from the on-board sensor data collection software to the sraa in csv format. using python, the csv files are converted to several sql files. the sql files are imported using mysql workbench. then tableau is configured to extract the data to store it in a more efficient in-memory format before being ready for visualizing (figure 4).” the majority of these cases the teams are competing at the highest levels with advertisers supplying a lot of money that allow the data analytics to be provided. but sap for example is expanding to more and more teams and allowing access to teams that couldn’t afford to do it on their own. it is a really interesting time to be in the data science world and sailing is a great example of data analytics proving how helpful it can be. sources: https://www.sapsailing.com/gwt/home.html#eventsplace : https://ris.utwente.nl/ws/portalfiles/portal/28105597/improving_decision_making_in_ocean_race_sailing_using_sensor_data.pdf https://www.oracle.com/corporate/oracle-team-usa/analytics-performance.html sailing data science -- -- written by kyle powers 21 followers · 50 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/sicara/feature-flags-nodejs-continuous-delivery-c245264b52b4,,,"feature flags in nodejs + react. enable continuous deployment and… | by clément walter | sicara's blog | medium sitemap open in app sign up sign in medium logo write sign up sign in sicara's blog · we build tailor-made ai and big data solutions for amazing clients feature flags in nodejs + react enable continuous delivery and feature testing in minute clément walter 2 min read · jun 1, 2018 -- 2 listen share read the full article on sicara’s blog here . feature toggle (or feature flag or feature switch ) is the concept of adding a flag to a feature so that it is enabled or not to the user in the backend or the frontend, for instance: change the display in the front deny some request in the back etc. in this post i will share my experience on how i implemented feature flags for my nodejs app and how i integrated it with react. note however that the following tutorial does not depend on your frontend framework . it can be used for several reasons: feature testing : or a/b testing. when you want to test a new feature over a small part of your audience; continuous delivery of new features : the dev team keeps developing new features and deploying the code in production while the flags allow for preventing the user to see ongoing developments; dark launch : to test the new feature in a small audience to get user feedbacks and test performance. as soon as you want to create this kind of feature branching in your code, you want to be very cautious of keeping it as clean and tidy as possible. features flags are temporary ( feature testing for instance) and you will have to remove them one day or another . this can happen few days or several weeks after their development. in this latter case you want to make sure you know exactly what to remove and where . fortunately there are some tools available to help you manage these operations. tools overview the featureflags.io website presents some alternative tools for setting up feature flags in a nodejs app: launchdarkly : paid service. lots of features and tools but probably overkill if you get started with feature toggle. flipit : 9 stars and 2 forks only on github. it is essentially a read in a config file. fflip : free and proposes also an express middleware unleash : not tested because it requires nodejs >= 8 while my project used nodejs 6.*. in the following of this tuto i am going with fflip for feature toggle because it is well rated on github, maintained and free. … read the full article on sicara’s blog here . javascript web development nodejs feature flags feature toggles -- -- 2 published in sicara's blog 4k followers · last published feb 18, 2020 we build tailor-made ai and big data solutions for amazing clients written by clément walter 272 followers · 5 following scientist & engineer, building things responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@StratGleeson/the-washington-post-test-ffc184b510d9,,,"“the washington post test”. one of the concepts that i often found… | by d.j. gleeson | medium sitemap open in app sign up sign in medium logo write sign up sign in “the washington post test” d.j. gleeson 3 min read · mar 1, 2017 -- listen share one of the concepts that i often found myself confronted with while working as an analyst and manager of analysts at the central intelligence agency was “the washington post test,” which was shorthand for, “how would this [whatever idea you happened to be discussing at the time] look if it appeared on the front page of the washington post ?” as a federal employee, this was an important question: having your analyses, operations, or programmatic ideas, or decisions splayed across the front page of the post all but ensured that there would be inquiries from congress as its members fulfilled their oversight roles. suffice it to say, we did not always pass the test: inevitably, you would read a story about someone, somewhere making a decision that could only be described as a “head scratcher.” “what were they thinking?” you would ask. “how did they think that would play out in the press?” even in the secretive world of the cia, the press and its role as a check on government power was real: its curiosity and scrutiny was the cost of being the beneficiary (and steward) of the taxpayer’s hard-earned dollar. while “the washington post test” was sometimes frustratingly used by risk-averse colleagues to squash new and different lines of thinking, i have to begrudgingly admit that it had a huge value to me as an individual: it forced me to think through the business reasons behind an idea and frame them in ways that were not only consistent with the public’s expectations of the cia, but, ultimately, american values. last week, when i read of the white house’s decision to exclude the new york times , cnn, politico , and others from a “gaggle” with press secretary sean spicer, i was fairly astounded. first, don’t get me wrong: i do not have any great, romantic affection for our modern media. i often find what passes for reporting to be simplistic and lacking important (but inexplicably easy to find) bits of context. i find the more partisan outlets and their reporters / commentators on both the far left and the far right to be boorish at times. while i understand how the internet has disrupted the business model that has long underpinned the media, trying to bolster revenue by retreating behind firewalls is a disservice to citizens trying to better understand what is happening, why it is happening, and what current events might mean for them. returning to last week: what astounded me about the white house decision was that i have a hard time believing that this administration did not think about how its decision would play out in the broader press or, if they did, that they concluded that the decision would be worth the cost of fueling the perception that they are unwilling to subject their thinking to scrutiny, discussion, and debate. again, don’t get me wrong: i have no problem with semi-private conversations between the press and government officials: i assume that sort of arrangement, if not relationship, underpins every story that is festooned with the word “exclusive.” what bothers me is that this administration seems to lack even a basic understanding of how to manage its relationship with the press, which plays a central role in a modern democracy, and, ultimately, the american people. criticism is not “so unfair;” it is part and parcel of holding public office or of being a public figure. while the administration remains oddly preoccupied with last year’s election, we, as a country, are beyond it: this administration now represents all americans and we the people have the right to voice dissent, agreement, or apathy with the people in power and to dissect the policies they promote. so, while i never had to deal with the pressures of a highly-visible public office and i am a long ways off from being a public figure, i knew that as a federal worker that i had to be ready for the possibility of unanticipated press coverage and that the press coverage might not be favorable or even entirely accurate. that said, the press was there to do a job. “the washington post test” was there to help me do mine. dennis j. gleeson is formerly a director of strategy in the central intelligence agency’s directorate of analysis. politics cia media -- -- written by d.j. gleeson 153 followers · 412 following formerly a director of strategy at the central intelligence agency. now, a free-range thinker. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/codebrace/day-of-the-programmer-5ae459953241,,,"day of the programmer | by ashish patel | codebrace | medium sitemap open in app sign up sign in medium logo write sign up sign in codebrace · coding blog to help people get going with competetive programming, big data and other technologies, visit http://medium.com/codebrace day of the programmer ashish patel 3 min read · feb 27, 2017 -- listen share day of the programmer problem link: day of the programmer marie invented a time machine and wants to test it by time-traveling to visit russia on the day of the programmer (the 256th day of the year) during a year in the inclusive range from 1700 to 2700. from 1700 to 1917, russia’s official calendar was the julian calendar ; since 1919 they used the gregorian calendar system. the transition from the julian to gregorian calendar system occurred in 1918, when the next day after january 31st was february 14th. this means that in 1918, february 14th was the day of the year in russia. in both calendar systems, february is the only month with a variable amount of days; it has 29 days during a leap year , and 28 days during all other years. in the julian calendar, leap years are divisible by ; in the gregorian calendar, leap years are either of the following: divisible by 400. divisible by 4 and not divisible by 100. given a year, y, find the date of the 256th day of that year according to the official russian calendar during that year . then print it in the format dd.mm.yyyy , where dd is the two-digit day, mm is the two-digit month, and yyyy is y. input format a single integer denoting year y. constraints 1700 ≤ y ≤ 2700 output format print the full date of day of the programmer during year in the format dd.mm.yyyy , where dd is the two-digit day, mm is the two-digit month, and yyyy is . sample input 0 2017 sample output 0 13.09.2017 explanation 0 in the year y=2017, january has 31 days, february has 28 days, march has 31 days, april has 30 days, may has 31 days, june has 30 days, july has 31 days, and august 31 has days. when we sum the total number of days in the first eight months, we get 31+28+31+30+31+30+31+31 =243. day of the programmer is the 256th day, so then calculate 256–243 = 13 to determine that it falls on day 13 of the 9th month (september). we then print the full date in the specified format, which is 13.09.2017 . sample input 1 2016 sample output 1 12.09.2016 explanation 1 year y = 2016 is a leap year, so february has 29 days but all the other months have the same number of days as in 2017. when we sum the total number of days in the first eight months, we get 31 + 29 +31 + 30 + 31 + 30 + 31 + 31 =244 . day of the programmer is the 256th day, so then calculate 256–244 = 12 to determine that it falls on day 12 of the 9th month (september). we then print the full date in the specified format, which is 12.09.2016 . solution this question is one of the trick question of this challenge. many people ignored the question and simply applied the logic for leap year. but this question can be solved by the following method. consider here are three case if year is 1918 print 26.09.1918. as this year 32nd day is 14th february. if year is less than 1918 then our answer will be 12.09.year if year value is a multiple of 4 else 13.09.year if year is greater than 1918 then our answer will be 12.09.year is year is divisible by 4 and not by 100 or divisible by 400. else 13.09.year code #include <cmath> #include <cstdio> #include <vector> #include <iostream> #include <algorithm> using namespace std; int main() { short y; cin>>y; if(y==1918) cout<<""26.09.""<<y; else if(y <= 1917) { if(y%4==0)cout<<""12.09.""; else cout<<""13.09.""; cout<<y; } else{ if((y%400==0) || (y%4==0 && y%100!=0)) cout<<""12.09.""; else cout<<""13.09.""; cout<<y; } return 0; } competetive programming hackerrank solution programming week of code 29 -- -- published in codebrace 192 followers · last published nov 24, 2024 coding blog to help people get going with competetive programming, big data and other technologies, visit http://medium.com/codebrace written by ashish patel 246 followers · 62 following big data engineer at skyscanner , loves competitive programming, big data. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/penn-engineering/penn-engineers-develop-the-wormotel-simultaneously-study-the-aging-process-in-thousands-of-model-a56f2f5d413e,,,"penn engineers develop the “wormotel,” simultaneously study the aging process in thousands of model organisms | by penn engineering | penn engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in penn engineering · university of pennsylvania’s school of engineering and applied science hundreds of roundworms, all in individual chambers, live on each “wormotel” plate. along with automatic imaging systems, the wormotel allows for large-scale comparative studies of the model organisms, which live for only a few weeks. penn engineers develop the “wormotel,” simultaneously study the aging process in thousands of model organisms penn engineering 3 min read · aug 29, 2017 -- listen share the roundworm c. elegans is one of the most important model organisms in biological research. with a transparent, millimeter-long body containing only about a thousand cells and a lifespan of a few weeks, there is no better way of deciphering the role of a given gene on a living creature’s anatomy or behavior. in addition, many of the genes discovered in the worm have been shown to have similar roles in other animals and humans. in the era of big data, however, a single worm isn’t enough. thousands upon thousands of individual organisms are necessary to compare many different genes and ensure the reliability of experimental results. engineers at the university of pennsylvania have taken strides to make this type of high-throughput experiment feasible by developing a system they have dubbed “the wormotel.” to demonstrate its effectiveness, the researchers have studied the role of a set of mutations and stress-inducing drugs on the aging of 1,935 of these organisms, specifically, what percentage of their lifespans they remain healthy and active. the wormotel system features index-card-sized plates made out of a transparent polymer. each plate features 240 individual wells, in which a single worm lives its entire life. automated systems keep them fed and stimulated while machine vision algorithms track and record their behavior. the wormotel system is also designed to be highly scalable. robotic carousels can automatically swap hundreds of wormotel plates in and out of analysis chambers, studying up to 57,600 worms in a single experiment. christopher fang-yen and matthew churgin the study, published in the journal elife , was led by christopher fang-yen , wilf family term assistant professor in bioengineering in penn’s school of engineering and applied science, and matthew churgin, a former graduate student (now a postdoctoral fellow) in his lab. they collaborated with david raizen , an associate professor of neurology in penn’s perelman school of medicine. former fang-yen lab members sang-kyu jung, chih-chieh (jay) yu, and xiangmei chen also contributed to the research. “keeping the worms in isolated wormotel wells allows us to monitor not just the lifespan of each worm but the behavior and health of each animal over time,” fang-yen said. “that along with the high-throughput nature of the system are the most important strengths of the method. we are taking a method that usually requires a great deal of manual labor, inspecting and counting single worms every day, and automating it.“ carousel systems can automatically swap wormotel plates in and out of imaging chambers, allowing experiments with tens of thousands of worms. “the worms have about 20,000 genes in their genome, similar to humans,” churgin said,” if you really wanted to see which genes influence something like aging, you’d want to test them all — have 20,000 different groups of worms, each with a different gene removed.” “you need about 100 animals per group, so that’s roughly 2 million worms,” he said. such an experiment would be nearly impossible using standard measurement techniques. “but with the wormotel, we could do all of those experiments within a few years.” the ability to monitor each worm for the entirety of its lifespan allowed the researchers to investigate genes related to their “healthspan” — the portion of their lives in which they are robust and active. the researchers are interested in studying not just lifespan, but “healthspan” — the span of the organisms’ lives in which they are robust and active. different individual c. elegans within a population exhibit variations in lifespan. comparing these populations to those with mutations in an aging-related gene showed that, in both cases, the shorter-lived worms spent more of their lives relatively healthy. research in this area could eventually be applied to questions about human aging, where staving off the physical and cognitive declines of old age might be preferable to simply increasing lifespan. in addition to investigating genes involved in lifespan and healthspan, the wormotel can be used for screening large numbers of drugs for their effects on aging. the fang-yen laboratory is currently working through collaborations to perform such a screen. the research was supported by the national institutes of health (r01-ns-084835 and r01-ns-088432), ellison medical foundation (ag-ns-1109–13), european commission (633589) and alfred p. sloan foundation (br2012–084) science engineering bioengineering aging big data -- -- published in penn engineering 1.1k followers · last published sep 10, 2020 university of pennsylvania’s school of engineering and applied science written by penn engineering 2.8k followers · 4 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/netanelbasal/create-and-test-decorators-in-javascript-85e8d5cf879c,Jest,,"create and test decorators in javascript | by netanel basal | netanel basal | medium sitemap open in app sign up sign in medium logo write sign up sign in netanel basal · learn development with great articles create and test decorators in javascript netanel basal 4 min read · oct 2, 2017 -- 4 listen share i remember the first time i saw this sexy symbol. yes, you know what i’m talking about. the at sign (@), above or before class, property or a method — a decorator . decorators are a mechanism for observing, modifying, or replacing classes, methods or properties in a declarative fashion. decorators are a proposed standard in ecmascript2016. in typescript, we can enable them by setting the “experimentaldecorators” compiler flag, or with babel by installing the babel-plugin-transform-decorators plugin. creating decorators is actually quite easy. let’s explore the various decorators. class decorators a class decorator is a function that takes the constructor of the class as the only parameter. if the class decorator returns a value, it will replace the class declaration with the provided constructor function, i.e., override the constructor. let’s see an example of how to override the constructor. we are returning a brand new class that extends the original constructor function, in our case, the hellocomponent . let’s see an example of how to decorate the existing constructor. let’s say we work with react and we want to know when react calls the render method. we start by saving a reference to the original method, create a new one, call what we need and return the original method, letting react do its magic. you can find a real-life example in this repository. method decorators a method decorator is a function that takes three parameters. the target: either the constructor function of the class for a static member, or the prototype of the class for an instance member. the key: the method name. the descriptor: the property descriptor for the method. let’s see an example of how to decorate an existing method with the settimeout api. but before we can proceed, if you remember, the settimeout function takes a number of milliseconds to wait before executing the code, so we need to learn how to pass options to a decorator by using a decorator factory. a decorator factory is simply a function that returns the expression that will be called by the decorator at runtime. or in plain english, function that returns function. we can get a reference to the original method from the descriptor value property. then, we will override the original value and create a new function that wraps the original with settimeout . remember that you can also use a method decorator with static methods, for example: the only difference in this case is you will get the constructor function of the class and not the prototype of the class. you can find a real-life examples in this repository. property decorators a property decorator is declared just before a property declaration. same as the others, it’s a function that takes two parameters. the target: either the constructor function of the class for a static member, or the prototype of the class for an instance member. the key: the property name. let’s see an example of how to decorate a property. the logproperty decorator above redefines the decorated property on the object. we can define a new property to the constructor’s prototype by using the object.defineproperty() method. here, we’re using get to return the value and log it. secondly, we’re using set to directly write a value to the internal property and log it. we can also use the new reflect api instead of the defineproperty() method. now we can use our decorator like this: you can find a real-life example in this repository. parameter decorators i’m going to skip this explanation because it’s the least common and it usually comes in conjunction with a method decorator. you can find a detailed example in the official documentation . composing decorators when multiple decorators apply to a single declaration, their evaluation is similar to function composition in mathematics. for example, the following code: is equivalent to decoratortwo(decoratorone(somemethod)) . testing decorators i’m going to use jest for testing, but you can use whatever test library you like. we have two ways to test decorators. use them in your tests like in your code. 2. since decorators are just functions we can test them like any other function. let’s close the article by testing the @timeout decorator. we are passing the necessary parameters manually to the timeout decorator and leveraging the usefaketimers feature from jest to check if our settimeout function was called. wrapping up you can leverage decorators in your apps and create powerful things with them. decorators are not only for frameworks or libraries, they are worth the time to learn as they can make your code more extensible and even more readable. decorators also promote code reuse. give them a try sometime soon 🙏 follow me on medium or twitter to read more about angular, akita and js! 👂🏻 last but not least, have you heard of akita? akita is a state management pattern that we’ve developed here in datorama. it’s been successfully used in a big data production environment for over seven months, and we’re continually adding features to it. akita encourages simplicity. it saves you the hassle of creating boilerplate code and offers powerful tools with a moderate learning curve, suitable for both experienced and inexperienced developers alike. i highly recommend checking it out. 🚀 introducing akita: a new state management pattern for angular applications every developer knows state management is difficult. continuously keeping track of what has been updated, why, and… netbasal.com wrapping up you can leverage decorators in your apps and create powerful things with them. decorators are not only for frameworks or libraries, they are worth the time to learn as they can make your code more extensible and even more readable. decorators also promote code reuse. give them a try sometime soon 🙏 follow me on medium or twitter to read more about angular, vue and js! javascript typescript web development angular2 react -- -- 4 published in netanel basal 15.4k followers · last published jan 26, 2025 learn development with great articles written by netanel basal 60k followers · 1 following a frontend tech lead, blogger, and open source maintainer. the founder of ngneat, husband and father. responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://guillaumejacquart.medium.com/experience-on-working-with-asp-net-core-and-nodejs-5e6c6351fc1f,,Unit Testing,"experience on working with asp.net (core) and nodejs | by guillaume jacquart | medium sitemap open in app sign up sign in medium logo write sign up sign in experience on working with asp.net (core) and nodejs guillaume jacquart 9 min read · sep 18, 2017 -- 17 listen share i’ve been using the .net stack for 5 years now, both professionally as a backend developer and architect, and personnally for open and closed-source side projects. i do mostly web applications, but also work with big data stacks. after a few years of tinkering with the php ecosystem, and a reasonable amount of time spent doing java, the c# language appealed to me for both its effectiveness and its pleasantness to work with. it is consistent, thoroughly designed, and visual studio is to me the most developer-friendly ide (when working with c#). moreover, the asp.net framework contains everything a developer needs to develop web applications without requiring additional frameworks or librairies. the only issues with the .net ecosystem to me were of ethical matter : the language and framework were closed-source, and the ecosystem was very microsoft-oriented (even though mono provided a nice alternative to run on linux, but with a constant gap in supported features). that’s why, when node.js started to get traction in the early 2013’s, i could not help but have a look, even though my colleagues kept trashing javascript as a dysfunctional language and node.js as a buzz. i was thrilled with learning about the single-threaded callback system, i enjoyed using expressjs to create rest apis easily and very fast. then microsoft announced the asp.net core, which carried hopes of natively cross-platform and open-source .net environment. that’s when i begin to go back and forth between the two technologies for cross-platform backend development. what follows are my insights on the differences between the two platforms in terms of development environment and features. i gathered theses pieces of information after many years of using these tools, and they help me to choose the best tool for the job when i start a new project. hopefully they will help you in that regard as well. request handling model the node.js model node.js is famously known for its single-threaded callback handler. what does that mean ? it means that instead of handling every new http request inside a dedicated thread or process (like apache does), it does it inside a single thread. that makes the request handling in nodejs single-threaded, whereas in apache/php it is multi-threaded for instance. but node.js takes advantages of that because it uses asynchronous system io calls that does not block the request thread. when an io call is made by the request thread, the io call is executed on its own thread, and the request thread keeps going. when the io call is done, a callback is fired on the request thread to handle the response. this makes node.js well-suited for io bound applications, but also introduces the so-called “callback hell”, which can introduces cyclomatic complexity to your code. hopefully, since v7.6, node.js implements native support for async/await of promises which makes you code far more readable and maintainable. the single-thread request handling can be clustered using node.js native clustering, nginx load-balancing with multiple application process, or node.js process managers such as pm2, which makes use of all the server cores to start up as many application process as the cpu can handle. the old asp.net way (synchronous) historically, asp.net mvc (or webapi) handles requests the traditional way, which is the apache / php way : each request is processed through its own thread inside a thread pool. and each io call is then processed synchronously inside that thread. this makes the synchronous asp.net model less suitable than node.js for io bound applications, which is most of the modern applications front side. hopefully, since .net framework 4.5 (08/12), c# makes use of the async/await pattern, which introduces asynchronous programing for request handling. the new asp.net way (asynchronous) the async/await pattern introduces asynchronous programing for request handling. indeed, each request handlers can be tagged as asynchronous, and io calls can be awaited, which means every request will run on its own thread, and each io call inside the request handling will be non-blocking. this task-based model can make use of callbacks, promise, or async/await programing models. asp.net core advocates the use of the async/await pattern when writing io bound web applications. node.js async/await vs asp.net core async/await node.js code for asynchronous database query : const express = require(’express’) const app = express() const mongoclient = require(’mongodb’).mongoclient const url = 'mongodb://localhost:27017/example' app.get('/', async (req, res) => { const db = mongoclient.connect(url); const col = db.collection( ' data ' ); const results = await col.find({}).toarray(); res.json(results); }) the equivalent asp.net core code (without boilerplate startup code): using system.collections.generic; using microsoft.aspnetcore.mvc; using system.linq; using mongodb.driver; namespace todoapi.controllers { [route(""api/[controller]"")] public class todocontroller : controller { [httpget] public async task<ihttpactionresult> getall() { var client = new mongoclient( ""mongodb://localhost:27017"" ); var database = client.getdatabase( ""example"" ); var collection = database.getcollection<bsondocument>( "" data "" ); return ok( await collection.find( new bsondocument()).tolistasync()); } } } the difference between the two process model is that on highly concurrent environment, asp.net core will handle more requests because requests are handled in parallel. although the context switching between threads can have its cost when using a lot of thread-shared variables. in that situation, node.js could be faster. many “mainstream” modern languages such as c# implements some sort of asynchronous io mechanism, which are often misunderstood by the node.js developer community, and can generate unexpected outcome ( https://stackoverflow.com/questions/43920942/unexpected-outcome-of-node-js-vs-asp-net-core-performance-test/44073433 ). this makes node.js far less revolutionary in that regard. more information on that can be found here . programing language features and safety coming from a c# background means that i’ve been exposed to a lot a criticisms regarding javascript dynamic typings and surprising boolean conversions. must of these criticisms still lay on solid grounds, considering that javascript was build in 10 days as a “glue” language to bring dynamic features to html pages. on the other hand, the language has evolved a lot since then, and the new specifications for javascript (es6/7), for which most of the key features are implemented in node.js, brings a lot to the table, such as: classes new identifiers (const, let) that increases code safety arrow functions string interpolation generators object exploration (which looks like c# reflection features) c# is far more powerful i think, because it makes use of all the benefits of having a strongly-typed object-oriented programing language. the visual studio ide is quite something and provides the best integration for a language (at least that’s what i think). but for a large majority of cases, and with the growing popularity of the micro-services pattern, a lot of these awesome features are not necessary for small to medium code bases. learning curve coming from classic mvc frameworks, the node.js / express combo take some time to get used to, as the new middleware-pattern can be new to some developers. the event driven paradigm is also specific to node.js and can take time to digest completely. everything else in the language is quite straightforward as it is simple javascript. what can be complicated at first when working on a medium to large application is the patterns to use for code re-factoring and code architecture. since the node.js frameworks such as express.js are very flexible, choosing the right architecture and file structure can be cumbersome, but a must-have to enhance evolutivity and maintainability of your application. the asp.net (core) mvc / webapi framework provides the developer with an already made code structure, dependency management and code architecture. the developer still has some code-organization to make such as separating the database layer from the web one, but overall far-less time is needed to think of that. developers used to poo paradigms will feel at ease rapidly. for simpler web apps such as light rest apis, the expressjs/node.js is very straightforward and easier to start with as the boilerplate template is only one small js file and a package.json file. prolificacy i found that writing simple code is a bit faster for me in node.js than in c#. to me that is because most simple applications (rest apis, console application, helpers, …) take advantage of having more flexibility, as you are not designing a lot of the app ahead of development. more and more studies question the relevancy of static typing to reduce the amount of bugs in an application ( https://medium.com/javascript-scene/the-shocking-secret-about-static-types-514d39bf30a3 ). if such studies happen to hold some truth, then using a strongly-typed language can be more of a drawback than a benefit. on the other hand, i’ve found that large code bases tend to be far more readable when working with c# than with javascript. to me, that mainly comes from the fact that strong oop paradigms, promoted by the c# patterns, enhance cohesion and decoupling of the code. regarding debugging and unit testing, the c# / visual studio combo still provides the best experience, but the node.js / visual studio code with extensions are not lagging far behind. the building time is also smaller for a node.js application. the ecosystem that is the aspect where the two technologies differ the most. node.js relies on a very large community writing a lot of modules in its main package management system (npm). most of the popular frameworks and libraries rely on a lot of small packages to work. this provides new developers with a lot of available modules to respond to the always-shrinking development time. but it can also have side-effects on the entire ecosystem when a package is updated or pulled off ( http://www.haneycodes.net/npm-left-pad-have-we-forgotten-how-to-program/ ). a lot of these packages are also not well maintained and error-prone. the asp.net core framework on the other hand is developed by a dedicated microsoft team entirely, and provides everything you need to build a fully functional website. third-party packages are often well built and backed by large companies. one good example is the field of orm (object-relational mapping) tools. entity framework, the official orm for asp.net, offers a full set of features such as code-first database modeling, auto-generated migrations and support for the main engines. node.js is lacking behind with very few orms available and far less features supported. sequelize is getting closer to it though… this might come from the historic affinity of node.js with mongodb and the mean stack. mongodb being a nosql flexible json data store, a full-featured orm has no meaning with it. deploy & run this is the area where node.js really shines. node.js is open source, cross-platform, docker-ready and most paas compatible. this means you can host your code fast and easy on the following platforms: your own linux, windows or mac server. you just need a node engine running and a reverse-proxy (nginx is the most popular and really fast) a docker container: there is an official docker image (23mo for the alpine-based image) most paas cloud providers (aws, google app engine, azure, heroku, …). these offer you everything you need to host your app to let you focus on the development there is also now , which offers a one-liner for node.js deployment with no configuration. there are also a lot of ci&cd platform that can build, test and deploy your node.js code with ease (travis, codeship, circle ci). asp.net on the other hand is a bit harder to handle in terms of deploy & run. althought the asp.net core framework is cross-platform, there is not yet the same amount of self-hosted and cloud tools for ci, cd and hosting. at the current time here are the hosting platforms known to me: your own windows server with iis for asp.net classic applications your own linux server (main distributions) with a reverse proxy: this is getting better for .net core but the package is still very large and updates are not always handled nicely (for .net core 1.x at least) a docker container on windows (asp.net classic and core) or linux (asp.net core only): this works nice, but is still quite heavy (120mo for the latest asp.net core linux image) some paas cloud providers: mostly azure, but some unofficial build packs exist on heroku for ci & cd i only know of visual studio team services that does that in the cloud. conclusion node.js asynchronous event-driven request handling model is not that far from the asp.net async/await multi-threaded one node.js performances are not always better than asp.net core ones. they even tend to be worse, even though the difference might not matter for common apps. cpu-bound apps will generate bottlenecks the javascript language is not that bad (getting better !), and combined with node.js, can lead to fast development cycles asp.net (core) is mostly preferred for large code-base as it provides ready-made software architecture and tooling that fits most needs. for micro-services or small to medium-sized rest apis, node.js provides a nice alternative for lean development with many hosting and ci&cd services available as always, there is not a better tool for each scenario, try and pick the one that fits your requirements javascript aspnetcore nodejs expressjs framework -- -- 17 written by guillaume jacquart 428 followers · 13 following digital enthusiast, web engineer responses ( 17 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@anttilip/comparison-of-spatial-resolutions-in-satellite-images-3185963a2e96,,,"comparison of spatial resolutions in satellite images | by antti lipponen | medium sitemap open in app sign up sign in medium logo write sign up sign in comparison of spatial resolutions in satellite images antti lipponen 4 min read · feb 11, 2017 -- 1 listen share i wanted to better understand what is the difference between different spatial resolutions in satellite images. what can you really see in a 10 meter resolution image? what is the actual difference between 20 meter and 100 meter resolution? to get an answer for these questions i carried out a simple test. how i did it i used sentinel-2a true color images (tci) as baseline. these sentinel-2 images have 10 meter resolution. of course there are commercial satellites with better than 1 meter resolution but i wanted to concentrate on satellite data that is openly available for everyone. four different 8 km x 8 km test scenes were tested: downtown san francisco, california center pivot irrigation systems in sahara desert brahmaputra river in india pukaki lake in new zealand i downloaded the data from amazon’s sentinel-2 service . the coarser resolution images were constructed by downscaling the original 10 m x 10 m image and sharpening the resulting image a little bit. this is not exactly the same thing as taking images with instruments having different spatial resolutions but should be good enough to get an idea how different resolutions look like (also different interpolations etc. affect the results…). images with the following resolutions were constructed: 10 m (original image used here): e.g. sentinel-2 red, green, blue bands 15 m: e.g. landsat 7 & 8 panchromatic band 20 m: e.g. sentinel-2 bands 5, 6, 7, 8a, 11, and 12 30 m: e.g. landsat 4–5 tm bands (excl. thermal) 60 m: e.g. sentinel-2 bands 1, 9, and 10, landsat 1–5 100 m: e.g. landsat 8 thermal infrared bands 120 m: e.g. landsat 4–5 thermal band 250 m: e.g. modis bands 1 and 2 the script for reproducing the images can be found below. test scene: san francisco (8 km x 8 km) san francisco 10 m resolution san francisco 15 m resolution san francisco 30 m resolution san francisco 60 m resolution san francisco 100 m resolution san francisco 120 m resolution san francisco 250 m resolution test scene: center pivot irrigation systems in sahara desert (8 km x 8 km) center pivot irrigation systems 10 m resolution center pivot irrigation systems 15 m resolution center pivot irrigation systems 20 m resolution center pivot irrigation systems 30 m resolution center pivot irrigation systems 60 m resolution center pivot irrigation systems 100 m resolution center pivot irrigation systems 120 m resolution center pivot irrigation systems 250 m resolution test scene: brahmaputra river, india (8 km x 8 km) brahmaputra river 10 m resolution brahmaputra river 15 m resolution brahmaputra river 20 m resolution brahmaputra river 30 m resolution brahmaputra river 60 m resolution brahmaputra river 100 m resolution brahmaputra river 120 m resolution brahmaputra river 250 m resolution test scene: lake pukaki, new zealand (8 km x 8 km) lake pukaki 10 m resolution lake pukaki 15 m resolution lake pukaki 20 m resolution lake pukaki 30 m resolution lake pukaki 60 m resolution lake pukaki 100 m resolution lake pukaki 120 m resolution lake pukaki 250 m resolution hope you liked the images. if you have any questions/comments/ideas, feel free to contact me (for example in twitter ). all images contain modified copernicus sentinel data 2017. code used to construct the images: satellite technology space remote sensing some rights reserved -- -- 1 written by antti lipponen 87 followers · 34 following researcher at finnish meteorological institute. #satellite images, #dataviz and #science related things. remote sensing, climate etc. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@johndavidsimmons/testing-adobe-analytics-with-python-368752a39cc2,Selenium,,"testing adobe analytics with python | by john simmons | medium sitemap open in app sign up sign in medium logo write sign up sign in testing adobe analytics with python john simmons 7 min read · feb 1, 2017 -- 1 listen share this is a demonstration of how to use python to qa adobe dtm rules setup i have configured adobe analytics to fire an event on every successful page load on my site. in this case, it is event10 . i set up my rule in dtm then spent a few minutes clicking around the site. i think i have everything configured correctly. i click a link, check omnibug for event10 , and repeat. while this technically works, it is tedious and prone to error. i think i can optimize and automate this workflow with python. ideally, i would like to see a table that contains all of the relative links on my homepage, the events that fire on those pages, and an “everything is ok” indicator. this can be accomplished using some python automation. lets get started. as with all python projects. i start by creating and activating a virtual environment in my project folder. i always name mine venv. $ virtualenv venv $ . venv/bin/activate (venv) $ ok now that i have my environment set up its time to install the packages. i’ll use selenium to crawl the site, and beautifulsoup to parse the page source code for anchor elements to visit. i will also need an html parser to use with beautifulsoup. i like html5lib for no specific reason . finally, i will grab requests to do some http requests without using a full browser. you can install multiple packages at the same time with pip by separating them with a space. $ pip install selenium beautifulsoup4 html5lib requests finally, since the selenium package does not come with a chrome driver by default, i need to download the selenium chrome driver here . if you are on a mac, you can place the executable in the selenium folder in your virtualenv. i imagine windows is something similar. i prefer using chrome because it allows me to use the dtm switch and omnibug extensions should i need them. note that extensions must be specifically loaded into the selenium instance of chrome. more on that later. driver now that i have my chrome selenium driver in the right place i need to configure it. i prefer to do this as a function in a second file called browser.py and import it into my main file. webdriver is the selenium module that is used to create the browser object. this object will be used to traverse my list of links. i customize this object in the function, then return it. chrome options allows me to add extensions. note that chrome extensions must be packaged before they can be used with selenium. more on that here . desired capabilities is dict of chrome preferences. i update it here to allow access to the browser’s javascript console output. i use a .env that is not in source control to store sensitive information. the os library is needed to access it. for example: example of sensitive values in my .env file main script i have my script set up as a few variables, and 3 functions. i will go through it in chunks then show how it all works together. first, the imports: import os import requests from browser import chromedriver from bs4 import beautifulsoup as bs4 from selenium import webdriver from selenium.common.exceptions import webdriverexception from time import sleep os is imported here to get the name of my current directory. as i visit the relative link pages i will be saving screenshots. i use os to tell selenium where to save them. i will use requests to get the source code of the home page in order to parse the relative links from it. the homepage will still be checked for events since it contains a relative link to itself. chromedriver is the configured chrome instance i set up in browser.py the beautifulsoup soup module to create beautifulsoup objects which can then be parsed. i use the as modifier since i always misspell beautifulsoup one way or another. webdriverexception is needed to catch any errors that might occur trying to crawl the relative links. for example, if a bad url gets in there somehow… finally, when using selenium, i like to use sleep occasionally to build in some extra loading time for pages before trying to manipulate them. next, i have declared a few variables to make things a little more readable and explicit. stagingscript = ""localstorage.setitem('sdsat_staginglibrary',true);"" debugscript = ""_satellite.setdebug(true);"" pagenamescript = ""_satellite.getvar('global - pagename')"" target_url = "" https://www.homepage.com "" html_string = ""<!doctype html>..."" stagingscript & debugscript are lines of javascript that can be executed in the browser console to turn on dtm staging and debug respectively. i won’t be using them here specifically, but they are nice to have if i need them. pagenamescript is javascript that returns a specific analytics variable we use for naming pages. this will be the first column of the table i will output. target_url is the homepage i want to extract relative anchor links from. finally, html_string (cutoff to save space) contains the beginning portions of a full html document. as i iterate over the links, i will build additional elements and append them to this string. i will then append closing tags and save the entire string as a .html file. my first function returns a set of relative anchor tags on a specific page. i am specifically using a set type here so as to cut down on duplicate pages. however, i might still visit the same page twice since i am creating the set of the bs4 tag objects not just comparing the anchor text. i am keeping the items are bs4 tags so that i can still parse the tag objects later if i want to. requests send a get request to the url and i store the output (status code, headers, page source, etc.) of that request in a variable called r. i assign the parseable beautifulsoup object in a variable called soup. i then search soup for anchor tags whose href does starts with a “/” and does not end with common domain names. these are pretty good indicators that the href is a relative link. i recently found out that i can pass multiple values to startswith/endswith using a tuple. ok, its finally time to fire up selenium. my next function will visit a page, execute the javascript necessary to find the events and pagename, take a screenshot of that page, and append the necessary markup to the html_string . i should note that selenium is a lot more powerful than what i am using it for here. it is possible to fill out forms, click specific elements, etc. if i wanted to do any complex behaviors i would build them into this function and execute them conditionally depending on the page. in this case, for the sake of simplicity, i am just visiting the page and moving on. the function takes a url and a relative url as parameters in order to build a complete url. think http://www.homepage.com and / about-us. selenium then opens an instance of chrome and goes to that page. once on the page, its time to execute my my javascript using driver.execute(). i am wrapping this entire section in a try/except block for easier debugging and because i expect it to work most of the time. driver.execute() executes the javascript passed to it, but does not return anything. to get console output i will access the driver log. remember that i set the preferences in my driver to be able to access the browser’s console output. i use python multi-line quotes here to denote any javascript strings to differentiate them. in the javascript console i am logging the page name and the events that fired on that page. i separate them with the “|” character for easy splitting. once the console.log runs i use driver.get_log() to parse the contents of the javascript log. i am not entirely familiar with the nomenclature of its output (level, info, message, etc.), but i know that those terms are where i can find my pagename + events log. i use a list comprehension to find the log and use it to assign pagename and events as python variables. i then add them to my html string and take a screenshot. finally, i run these two functions inside a main function. after importing my global variables, i created a list of relative anchors to work from. i loop over this list with my visitpage function which will get the events and take a screen shot. after that, the only thing thats left to do is close the html tags and save the html_string to a file. i can then open this file to check my events. thats it. my script has output my html document. i now have a table of all the relative links on my homepage and what events fire on those pages. i can also click the page name to see a screen shot of that page. it looks like my rule to fire event10 on each page load is working correctly. this is not a perfect qa, but it is much less tedious than doing it all manually. so where do we go from here? this script works fine on its own, but i think it would translate nicely to a flask app or a cron job that runs periodically. it all depends on who wants to consume the data and how. i would also like to reiterate that i have barely scratched the surface of what selenium is capable of. provided you have the motivation, it would only be a little more work to check events, props, and evars on every page of entire user flows full of complex interactions. python selenium web analytics javascript -- -- 1 written by john simmons 120 followers · 49 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/pcmag-access/smart-farms-big-data-meets-big-ag-9bc6501c978a,,,"smart farms: big data meets big ag | by pcmag | pc magazine | medium sitemap open in app sign up sign in medium logo write sign up sign in pc magazine · pc magazine: redefining technology news and reviews since 1982. smart farms: big data meets big ag pcmag 13 min read · nov 11, 2016 -- listen share big data in agriculture promises improved efficiency and yields. but the components are not yet a cohesive whole. by michelle z. donahue l ike much of the american heartland, the summertime landscape in iowa’s webster county is dominated by several immutable features: hot sun and lots of it; a ruler-straight grid of byways that bend only at old property boundaries and upon encountering water; shining grain silos towering above the plains; and farmhouses surrounded by fields of soybeans, alfalfa, and hay. and, of course, corn. hundreds of thousands of acres of it, a sweeping, shimmering sea of green, towering well over even a tall man’s hat by july in a good year. in 2013, just over 200,000 acres , fully 50 percent of webster county’s farm acreage, was given over to zea mays . brent larson is part of this economy, farming around 400 acres along with his father and brother. they run their farms primarily in the evenings and on weekends, their workday filled with assisting the hundred or so clients they serve through sunderman farm management, their consulting and brokerage firm. farming alone is a difficult way to make a living. so technology that promises to improve a farm’s yields while lowering costs appeals deeply to larson. large amounts of capital are tied up in seed, fertilizers, water costs, and equipment, and predicting the impacts of weather and market timing is an annual wrangle if he hopes to make some green of the spendable type. “people are interested in net income,” larson said. “the easiest way to make money is to avoid spending it in the first place. if more technology helps avoid additional expenses, that ties in, too.” he, his family, and his agricultural peers are no strangers to technology. gps-guided tractors are the norm, helping farmers navigate perfectly straight rows and plot optimal routes over uneven terrain to maximize rows planted per acre. larson can call upon several drone companies to do periodic imagery of a field. and along with 100,000 other farming operations on 92 million acres of fields, larson uses fieldview, a subscription-based data analysis platform created by monsanto subsidiary climate corporation. connected to data-gathering equipment on their planters and tractors, farmers can view real-time weather data, see historic field and yield data, and capture crop and soil information from a tractor as it passes through a field. larson said he is intrigued by what the recent proliferation of agriculture-targeted internet- and cloud-connected sensors might be able to do for him, as well as farming as a whole. but whatever he invests in has to work reliably. the optimal windows for tilling, weeding, seeding, fertilizing, and harvest are all incredibly small, whether for row crops such as corn and soy, vegetables such as lettuce, or perennial crops such as grapes, nuts, and fruit. precision hawk’s scanning tools include a variety of vegetation algorithms. (photo: precision hawk) “one of the hardest things to figure out is where to spend your time, energy and effort,” larson said. “some of these technologies are great, but to learn them, and keep them up to date, and then throw them away when the next best widget comes out in three years — i don’t want to buy another $5,000 device that will just be a paperweight in a couple of years.” he envisions a time when his farm and others like it, as well as the huge big ag plantations, can all harness the promise of big data for highly targeted plant management: applying fertilizer only where the soil is starved, watering only where plants are thirsty, and spraying for pests and diseases before they become a plague. but as it has been for as long as we’ve farmed, many of those decisions made today are are heavily dependent upon an expert eye and the many layers of intuition of accumulated seasons. the idea of getting the right mix of hardware and software deployed in his fields to assist with those goals still smacks of sci-fi for larson. but it’s not as far away as he might imagine. gadgets in the field in 2015, nearly 500 ag-tech companies drew $4.6 billion in investments, double the previous year’s figure, according to agriculture investment platform agfunder. this year has been slower — only $1.75 billion went to ag tech in the first half of 2016, a 20 percent reduction over the same time last year. agfunder ceo rob leclerc said that’s probably because companies now are putting prior investments to good use. still, the ways in which companies and start-ups are getting into the ag space vary wildly. precision hawk’s scanning tools analyze how vigorous and robust plants are. (photo: precision hawk) hardware abounds, as you might expect. startup cowlar is helping dairy farmers optimize milk production by outfitting their herds with smart collars — essentially activity trackers that send alerts when cows are “exhibiting abnormal behavior or unusual temperature levels are detected” or when they wander off the farm. other companies are making wi-fi–enabled ear tags and embedded chips that send data on an animal’s movement, temperature, and gestation cycle. sensors for grain elevators track humidity and temperature gradients throughout a silo, feeding into automated climate control systems to keep grain from fermenting. swiim worked with the u.s. department of agriculture to develop a complete sensor-software system for monitoring precipitation and water usage, so a farmer can not only conserve more water but could also profit from leasing out any leftover portions of his or her water rights. and there are plenty of diy tinkerers in the field, working to make devices for their own farming operations. one such community, known collectively as farm hack, has been concerned with the potential for tech to enable data gathering and automation from its very inception. dorn cox, a co-founder of farm hack, runs the 300-acre tuckaway farm in lee, nh. the farm raises organic specialty crops primarily for restaurant clients, including blueberries, mushrooms, vegetables, maple syrup, sunflower oil, and baked goods made from farm-grown grains. in 2011, when the open-source farmhack community first started, one of the very first problems members discussed was how to remotely detect dangerously high temperatures in greenhouses. using a basic arduino system, a modified gsm cellular signal, and a cell phone, the project created a temperature sensor that sends text alerts when temps got too high in the operator’s greenhouse. more and better-quality off-the-shelf devices are available now for members to use or tweak for their own purposes. cox himself has developed a dashboard dubbed farmos that lets users collect, centralize, and manage their remotely collected data. he also has a number of sensors deployed around his farm — primarily in the greenhouses — that help him and his small staff prioritize their tasks. he predicted he won’t be able to grow economically without the help of a larger sensor and data-collecting scheme. “our mushroom operation, for instance, is really sensitive to environmental conditions,” he explained. “the risk is so high when things get out of range that we won’t be able to expand without adding those kinds of alerts. i think we’re all heading toward being dependent on that kind of capability.” cox is concerned by the sheer abundance of devices on the market, though, and said that there’s little assurance for any interested party that the hardware is collecting robust data. he and other farm hack collaborators worked recently at a u.s. environmental protection agency and u.s. department of agriculture air quality and climate station in maine to install, calibrate, and run comparisons of the performance of low-cost sensors alongside their powerful — and much more expensive — science-grade counterparts. “as we add more and more sensors to our fields, they’re important not only for farm management and environmental compliance but also to a research audience,” cox said. “these are people building models about how carbon, water, and nitrogen cycle through an agricultural system, and those models are used to project the effect of agriculture on climate change. that feedback from agriculture systems is so important to validate. policy is made based on that.” stitching things together as it affects their crop yields, farmers are primarily concerned with two things: climate and chemicals. how much water is available? how fertile is the soil? how much will i need to allot for insect and disease sprays? sensors and data-gathering devices can provide actionable information on each of these concerns. but what if you wanted data streams on all of them, all at once, every day? or more specialized information based on your particular crop or location? break out your password keeper: for the most part, it’s very much a disjointed jumble of objects, ideas, and processes. there is hope, though: “integration” is the new buzzword in the ioag. think platforms. “our average subscriber has 160 data streams, but we have some farms with over 1,000,” said lance donny, founder and ceo of onfarm. “that’s a lot of data for a grower to manage, especially if it’s reporting to you all the time.” onfarm provides a central nervous system, accessed via smartphone, tablet, or computer, through which farmers can route all or some of their sensor data for analysis. on the tech side, hardware and software developers can add their services to the onfarm network to reach the company’s subscribers. though onfarm boasts partnerships with big-name companies such as john deere, davis instruments, and campbell scientific, donny asserted he’s agnostic about who joins the ecosystem. “we’re supporting any data, coming out of any device,” donny said. “there have been so many iot-based solutions giving growers so much information about lots of different things, but all that information was siloed, and there were no data standards for interoperability amid all that data. so it was hard for growers to make a determination of what to use, how to use it, and how to get the best return on that investment.” chris fastie, left, of the open-source testing community public lab, and dorn cox, farm hack co-founder (photo: dorn cox) focused primarily on growers of specialty crops — nuts, fruits, berries, and vegetables — donny said onfarm receives 70 million readings per month from 71,000 sensors spread over half a million acres in 32 u.s. states and canada. “data management systems have almost become a requirement in order to quantify it and boil it down,” he said. “you just can’t juggle that information in your head.” climate corporation, also primarily concerned with vending data-analysis services, is working toward integrating more types of high-resolution data layers from more sources for its fieldview products. it aims to accomplish this in large part by partnering with numerous third-party providers, such as kansas-based veris technologies, which has developed a series of tractor-mounted sensors to map individual fields’ organic matter content, ph, electrical conductivity, and soil texture. climate corporation’s chief innovation officer, mark young, said that by stirring up conversations with other innovators of all sizes, climate corporation hopes do for farming what google did for web search or amazon for the digital marketplace: make it simple and make it bulletproof. “when your ag software is down and it’s stopping you from planting or harvesting, that’s not an option,” young said. “it’s a little more rigorous than what most silicon valley tech companies are used to focusing on. these are multimillion dollar operations, and it cannot go wrong. if the code running your pacemaker has a bug, that’s a bad day for everyone, and where we’re getting to, the software has to have that same level of redundancy.” princeton, nj–based arable is taking a somewhat more global approach to integration. subscription-based data management and analysis is also key to its process, but instead of solely cobbling together a network of thousands of third-party providers, founder adam wolf decided that packaging several dozen commonly used sensors into an attractive, easy-to-deploy design would be most beneficial to a broader array of growers. strawberries ripen in one of driscoll’s commercial growing fields in monterey county, calif. (photo: driscoll’s) wolf was first spurred to the idea as an agronomy grad student lugging around hundreds of thousands of dollars’ worth of weather-monitoring equipment for research in kazakhstan. in his experience, growers everywhere want the same basic abilities: soil moisture and fertility tracking, intensity and hours of sunlight available, correlation of crop yields with weather trends, and the ability to quickly assess foliage for problems. “you shouldn’t have to send a grad student with a pickup truck to wire this stuff up,” wolf laughed. he worked with nest and gopro designer fred bould to package an array of sensors into a compact disc-shaped housing. the pulsepod contains, among other things, net radiometers for measuring absorption of shortwave and longwave solar radiation; tilt and orientation sensors; spectrometers to reveal foliage in different wavelengths for measuring plant stress, growth rate, and chlorophyll content; and cameras. the pod can be mounted on a pole at any height and customized with additional hardware on external ports. “the big opportunity that has never really existed in ag iot is measuring and connecting the predictors with their outcomes,” wolf said. “it’s supply chain optimization. if you look at what’s valuable, 30 percent of all food is lost before it reaches the consumer. that’s money left on the table, and all the irrigation, fuel, people hours, land value — all that was wasted. in the larger context, there are bigger fish to fry than just irrigation or applying pesticides.” for large international companies such as berry producer driscoll’s, which partners with around 700 strawberry growers around the world, iot technology like arable’s also represents the ability to make sensor data more detailed. where previously growers might deploy one sensor per 100-acre field, smaller, more affordable units that provide more information from a single data stream makes it easier to justify 10 to 20 times the number of sensors per field. driscoll’s is currently beta-testing pulsepods in two fields in california. “you get this closed feedback loop where you really trust that the sensor data are representative of what’s happening in the field, and that provides the grower with information to make better decisions,” said michael christensen, driscoll’s director of forecasting for the americas. “and in the future, if growers can act on individual plants instead of spraying an entire 100-acre field, there are big ramifications for pesticide, fertilizer, and water use. if someone can unlock the ability not only to sense but also to act on that data, there’s a lot of value in that.” growers of another high-value crop, wine grapes, are eager to employ forecasting methods based on tightly knitted datasets: multifunction platforms or devices give them the ability to tie microclimate variances to yield predictions by responding to real-time conditions in the field as they unfold. “being able to pinpoint what past seasons have done to predict your current season before you get there really can help turn things around,” said will drayton, director of technical viticulture and research winemaking for australia-based treasury wine estates. “as soon as i have an inkling there may be some shatter in the merlot, i can start speaking to growers across the state to pick up additional contracts and buy early while we still have a good price. but it also goes to the level of ordering corks, casks, employee hours, harvest interns, everything.” a washington state orchard manager checks data-overlay images of his fields. (photo: precision hawk) feeding the future in the united states, the ability for growers to collect more data essentially amounts to better profits, and perhaps more consistent prices for consumers at the grocery. but in places in the rest of the world where one daily meal, let alone three, are far from assured, more data from iot devices may mean the difference between subsistence and success. assume that the ongoing problem of cellular access, wireless or other internet connectivity is solved in developing nations. the question companies like arable and climate corporation are asking becomes , how can we make farmers in countries like zambia, brazil and india as productive as american farmers, with enough surplus to feed not only themselves but also their community and even country? “the issues that affect the smallholder are different from those that affect the guy in iowa,” climate’s young said. “they’re light on agronomic advice, commodity prices, and microcredit to run their operation. a lot of them are planting the same seeds as we are and getting only 60 or 70 bushels per acre, while we’re getting 100 bushels per acre. how we get them to be better farmers, and put them in better touch with neighbor farmers?” climate corporation is working to answer this question by adapting its large-scale grower platform for use by smallholders as well. it has already signed up 3.5 million users in india to receive text messages with market and agronomy information based on their regional crops and conditions. arable’s wolf added that giving farmers in developing nations — as well as government entities such as the u.s. agency for international development — a better understanding of what big data can do can help give farmers more confidence in their assessment of seasonal risks and yields. “how do you feed the future?” wolf mused. “one meme is ‘climate smart agriculture,’ measuring conditions that allow you to predict whether the conditions that support [disease development] are happening, as well as the conditions of a particular microclimate in order to give periodic advice to farmers. having a system or device that’s able to measure that gives basic value around taking action in response.” in iowa, brent larson considered the plethora of offerings on tap to help farmers make more by growing more. his wish list of all the connected things that would help him personally: a tractor that could tell him when a bearing was about to fail; customized spot-fertilization schemes; a planter with the ability to match different hybrid corn varieties to patches in the field where they’d grow best. he gave a verbal shrug. “ultimately, everyone uses what is appropriate for their operation,” he concluded. “i think you’d be hard pressed to find an organization that uses all of the available technology — farming is so localized. the technology used here is different from 500 miles west or south. but necessity is the mother of invention, and there will be different inventions for every need.” read more: ge’s raven drone can detect gas leaks originally published at www.pcmag.com . agriculture technology big data future technology farming -- -- published in pc magazine 11.8k followers · last published dec 8, 2023 pc magazine: redefining technology news and reviews since 1982. written by pcmag 45k followers · 299 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/making-sense-of-data/understanding-causality-and-big-data-complexities-challenges-and-tradeoffs-db6755e8e220,,,"understanding causality and big data: complexities, challenges, and tradeoffs | by srinath perera | making sense of data | medium sitemap open in app sign up sign in medium logo write sign up sign in making sense of data · thoughts about big data, data science, and machine learning understanding causality and big data: complexities, challenges, and tradeoffs srinath perera 7 min read · mar 31, 2016 -- listen share image credit: wikipedia , amitchell125 “does smoking causes cancer?” we have heard that lot of smokers have lung cancer. however, can we mathematically tell that smoking causes cancer? we can look at cancer patients and check how many of them are smoking. we can look at smokers and check will they develop cancer. let’s assume that answers come up 100%. that is, hypothetically, we can see a 1–1 relationship between smokers and cancer. ok great, can we claim that smoking causes cancer? apparently it is not easy to make that claim. let’s assume that there is a gene that causes cancer and also makes people like to smoke. if that is the cause, we will see the 1–1 relationship between cancer and smoking. in this scenario, cancer is caused by the gene. that means there may be an innocent explanation to 1–1 relationship we saw between cancer and smoking. this example shows two interesting concepts: correlation and causality from statistics, which play a key role in data science and big data. correlation means that we will see two readings behave together (e.g. smoking and cancer) while causality means one is the cause of the other. the key point is that if there is a causality, removing the first will change or remove the second. that is not the case with correlation. correlation does not mean causation! this difference is critical when deciding how to react to an observation. if there is causality between a and b, then a is responsible. we might decide to punish a in some way or we might decide to control a. however, correlation does warrant such actions. for example, as described in the post the blagojevich upside , the state of illinois found that having books at home is highly correlated with better test scores even if the kids have not read them. so they decide the distribute books. in retrospect, we can easily find a common cause. having the book in a home could be an indicator of how studious parents are, which will help with better scores. sending books home, however, is unlikely to change anything. you see correlation without a causality when there is a common cause that drives both readings. this is a common theme of the discussion. you can find a detailed discussion on causality from the talk “challenges in causality” by isabelle guyon. can we prove causality? great, how can i show causality? casualty is measured through randomized experiments (a.k.a. randomized trials or ab tests). a randomized experiment selects samples and randomly break them into two groups called the control and variation. then we apply the cause (e.g. send a book home) to variation group and measure the effects (e.g. test scores). finally, we measure the casualty by comparing the effect in control and variation groups. this is how medications are tested. to be precise, if error bars for groups does not overlap for both the groups, then there is a causality. check https://www.optimizely.com/ab-testing/ for more details. however, that is not always practical. for example, if you want to prove that smoking causes cancer, you need to first select a population, place them randomly into two groups, make half of the smoke, and make sure other half does not smoke. then wait for like 50 years and compare. did you see the catch? it is not good enough to compare smokers and non-smokers as there may be a common cause like the gene that cause them to do so. do prove causality, you need to randomly pick people and ask some of them to smoke. well, that is not ethical. so this experiment can never be done. actually, this argument has been used before (e.g. https://en.wikipedia.org/wiki/a_frank_statement . ) this can get funnier. if you want to prove that greenhouse gasses cause global warming, you need to find another copy of earth, apply greenhouse gasses to one, and wait few hundred years!! to summarize, casualty, sometime, might be very hard to prove and you really need to differentiate between correlation and causality. following are examples when causality is needed. before punishing someone diagnosing a patient measure effectiveness of a new drug evaluate the effect of a new policy (e.g. new tax) to change a behavior big data and causality most big data datasets are observational data collected from the real world. hence, there is no control group. therefore, most of the time all you can only show and it is very hard to prove causality. there are two reactions to this problem. first, “big data guys does not understand what they are doing. it is stupid to try to draw conclusions without randomized experiment”. i find this view lazy. obviously, there are lots of interesting knowledge in observational data. if we can find a way to use them, that will let us use these techniques in many more applications. we need to figure out a way to use it and stop complaining. if current statistics does not know how to do it, we need to find a way. second is “forget causality! correlation is enough”. i find this view blind. playing ostrich does not make the problem go away. this kind of crude generalizations make people do stupid things and can limit the adoption of big data technologies. we need to find the middle ground! when do we need causality? the answer depends on what are we going to do with the data. for example, if we are going to just recommend a product based on the data, chances are that correlation is enough. however, if we are taking a life changing decision or make a major policy decision, we might need causality. let us investigate both types of cases. correlation is enough when stakes are low, or we can later verify our decision. following are few examples. when stakes are low ( e.g. marketing, recommendations) — when showing an advertisement or recommending a product to buy, one has more freedom to make an error. as a starting point for an investigation — correlation is never enough to prove someone is guilty, however, it can show us useful places to start digging. sometimes, it is hard to know what things are connected, but easy verify the quality given a choice. for example, if you are trying to match candidates to a job or decide good dating pairs, correlation might be enough. in both these cases, given a pair, there are good way to verify the fit. there are other cases where causality is crucial. following are few examples. find a cause for disease policy decisions ( would 15$ minimum wage be better? would free health care is better?) when stakes are too high ( shutting down a company, passing a verdict in court, sending a book to each kid in the state) when we are acting on the decision ( firing an employee) even, in these cases, correlation might be useful to find good experiments that you want to run. you can find factors that are correlated, and design the experiments to test causality, which will reduce the number of experiments you need to do. in the book example, state could have run a experiment by selecting a population and sending the book to half of them and looking at the outcome. some cases, you can build your system to inherently run experiments that let you measure causality. google is famous for a/b testing every small thing, down to the placement of a button and shade of color. when they roll out a new feature, they select a polulation and rollout the feature for only part of the population and compare the two. so in any of the cases, correlation is pretty useful. however, the key is to make sure that the decision makers understand the difference when they act on the results. closing remarks causality can be a pretty hard thing to prove. since most big data is observational data, often we can only show the correlation, but not causality. if we mixed up the two, we can end up doing stupid things. most important thing is having a clear understanding at the point when we act on the decisions. sometime, when stakes are low, correlation might be enough. on some other cases, it is best to run a experiment to verify our claims. finally, some systems might warrant building experiments into system itself, letting you draw strong causality results. choose wisely! big data data science programming artificial intelligence some rights reserved -- -- published in making sense of data 122 followers · last published jun 3, 2016 thoughts about big data, data science, and machine learning written by srinath perera 2.6k followers · 49 following a scientist, software architect, author, apache member and distributed systems programmer for 15y. designed apache axis2, wso2 stream processor... works @wso2 no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/rockedscience/simulating-out-of-memory-errors-on-linux-c69754904209,,,"simulating out-of-memory errors on linux | by edoardo nosotti | rockedscience | medium sitemap open in app sign up sign in medium logo write sign up sign in mastodon rockedscience · tutorials and tips on cloud architectures, devops, iot, big data and code simulating out-of-memory errors on linux edoardo nosotti 3 min read · apr 8, 2016 -- listen share photo by jakub skafiriak on unsplash proper resiliency and recovery measures are essential in a production environment . this article will show you how to trigger an oom (out of memory) error on a linux box to test the process monitoring and recovery measures. prerequisites for this tutorial, we will be using an ubuntu system. 1. setup the required third-party tools 1.1 dstat dstat is handy for monitoring systems during performance tuning tests, benchmarks or troubleshooting. dstat allows you to view all of your system resources instantly. to install dstat , run: $ sudo apt-get install dstat 1.2 stressful application test (stressapptest) stressful application test (or stressapptest, its unix name) is a memory interface test. it tries to maximize randomized traffic to memory from processor and i/o, with the intent of creating a realistic high load situation to install stressapptest , run: $ sudo apt-get install stressapptest 2. prepare the application let’s suppose that you want to test an application called my_proc_name . ensure that the application has been correctly installed and is running, then type: $ ps auxf|grep my_proc_name to find its pid (in this case, 8000): user pid %cpu %mem vsz rss tty stat start time command ubuntu 8000 0.7 37.6 256024 188104 pts/3 ssl+ 15:42 0:43 \_ /usr/local/app_install_dir/my_proc_name then run: $ dstat --top-oom you should see this kind of output: --out-of-memory--- kill score very_heavy_proc_name 250 my_proc_name 200 another_proc_name 100 we want my_proc_name to become the most wanted offender in your system, so that it will be killed first in case of an oom event. we can manually alter the kill score, assigning a modifier to the process kill score. the modifier range is -17 (= do not kill) to +15 (= definitely kill this). the modifier value can be written in the /proc/{pid}/oom_adj file: $ echo ""+15"" > /proc/8000/oom_adj then check the status again: $ dstat --top-oom you should see this output: --out-of-memory--- kill score my_proc_name 1.0k very_heavy_proc_name 250 another_proc_name 100 3. stress the system check the amount of free memory available: $ free -m total used free shared buffers cached mem: 488 325 163 0 13 67 -/+ buffers/cache: 244 244 swap: 0 0 0 for the oom event to occur, you should usually make stressapptest allocate a memory amount (the -m option) that is slightly higher than the free memory. if the value is too low, the oom event will not occur, if it’s too high, stressapptest will crash. you will probably need to do several attempts to succeed. $ stressapptest -s 20 -m 180 -w -m 8 -c 8 then check if an oom event occurred and your process has been killed: $ dmesg | egrep -i 'killed process' [ 808.638907] killed process 2220 (sh) total-vm:4440kb, anon-rss:100kb, file-rss:0kb [ 809.884325] killed process 2221 (memsup) total-vm:4324kb, anon-rss:92kb, file-rss:4kb [ 809.897852] killed process 2223 (cpu_sup) total-vm:4324kb, anon-rss:92kb, file-rss:4kb [ 809.908096] killed process 2224 (inet_gethost) total-vm:7456kb, anon-rss:88kb, file-rss:0kb [ 809.916610] killed process 8000 (my_proc_name) total-vm:256280kb, anon-rss:180616kb, file-rss:0kb if your process has been killed, but running ps you see it still running, then your resilience measures worked. originally published at https://www.rockedscience.net on april 8, 2016. ubuntu linux test software engineering programming -- -- published in rockedscience 106 followers · last published mar 21, 2025 tutorials and tips on cloud architectures, devops, iot, big data and code written by edoardo nosotti 390 followers · 50 following certified cybersecurity analyst and senior cloud solutions architect. passionate about iot, ai, ml and automation. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/war-is-boring/darpa-s-unmanned-sub-hunting-ship-prepares-for-trials-d7f04a5c9bfc,,,"darpa’s unmanned sub-hunting ship prepares for trials | by war is boring | war is boring | medium sitemap open in app sign up sign in medium logo write sign up sign in war is boring · from drones to aks, high technology to low politics, exploring how and why we fight above, on and below an angry world darpa’s unmanned sub-hunting ship prepares for trials ‘sea hunter’ is a glimpse at the future of naval warfare war is boring 4 min read · apr 8, 2016 -- listen share by rodrigo ugarte autonomous robo-ships will hunt for submarines sooner than anticipated. on april 7, the u.s. military christened the sea hunter , an experimental vessel which is part of the pentagon’s anti-submarine warfare continuous trial unmanned vessel, or actuv, program. at 130 feet in length, the trimaran-shaped sea hunter has become the world’s largest unmanned surface vessel and will serve as a test bed for the navy, assessing the use and capabilities of future unmanned ships. sea hunter , in particular, will practice hunting submarines. able to navigate independently and track underwater and surface vessels, sea hunter ’s christening culminates two years of development and construction. “i have been waiting for this day for a long time. we are in a period of incredible technological flux,” deputy secretary of defense bob work said at the christening ceremony in portland, oregon. “advances in a.i., autonomous control systems, advanced computing, big data, learning machines, intuitive visualization tool, metamaterial, miniaturization are leading us toward great human-machine collaboration — in business and manufacturing, and in warfare.” the ship packs sonar, electro-optical sensors, and short and long-range radars to navigate and identify ships. among these is raytheon’s modular scalable sonar system (ms3). delivered in november 2015, the system performs active and passive search and tracking, incoming torpedo warning and small-object avoidance. combined with leidos’s own navigation software, sea hunter ’s computers navigate the vessel, identifying and avoiding other ships and objects autonomously. having developed a system of algorithms and sensors, darpa and leidos successfully met maritime navigation rules, known as colregs. however, the ship will not render sailors obsolete any time soon. “although actuv will sail unmanned, its story is entirely about people,” darpa program manager scott littlefield said in a press release . “it will still be sailors who are deciding how, when and where to use this new capability and the technology that has made it possible.” the office of naval research will oversee a two-year testing trial along with darpa and leidos at the navy’s san diego base. sea hunter will continue to test its sparse supervisory control mode, which requires human oversight. leidos tested this technology for the first time on january 2015 after installing it on a 42-foot test boat. however, when the mission demands it, the ship can be remotely controlled. currently, a removable control station sits atop the hull “for safety and backup,” littlefield told reporters, according to defense news . a pilot will remain aboard throughout testing in case of any difficulties. above and at top — ‘ sea hunter .’ darpa photos in his remarks, work compared the unarmed sea hunter ’s christening to the introduction of the predator drone, which entered service in 1995. “predator wasn’t much,” work said. “unarmed. couldn’t fly in bad weather. it was unreliable, suffering mechanical failure 12 percent of the time.” but the predator’s development did not end there. work recounted the drone’s evolution from an unmanned surveillance aircraft to a killing machine, hinting at a future of armed sea hunters … lots of them. one of the actuv project’s goals is to develop an unmanned vessel that, due to an unconventional design and lack of crew, could match the speed of diesel-electric submarines. during the ship’s latest speed test, sea hunter reached 27 knots, or 31 miles per hour — which no conventional submarine can beat. littlefield explained the ship could collaborate with much larger and manned littoral combat ships, acting as the fleet’s eyes well ahead of it, according to defense news . with a range of 10,000 nautical miles at 12 knots, the ship’s range covers a wide area. another goal of the actuv project is to develop a lower cost alternative to manned ships. at an estimated price of $20 million, sea hunter would be much cheaper than crewed vessels. the average cost of a zumwalt -class destroyer is around $2.5 billion. an average daily operating cost of $15,000-$20,000 for sea hunter , according to darpa, is also a bargain compared to the hundreds of thousands of dollars in daily operating costs for a destroyer. and, unlike a destroyer or lcs, sea hunter — or more precisely, a combat-ready future version — will not endanger sailors’ lives if hit by an enemy torpedo. “thanks to advances in technology, we don’t have to worry about anybody getting off this ship,” work said toward the end of his remarks. he then referred to capt. ernest evans’ comments during the commissioning of the world war ii destroyer uss johnston . “but make no mistake, in the future ships like this one will be fighting ships and we will intend to take them into harm’s way.” if sea hunter performs well in the next two years, the program will pass on to the u.s. navy. navy drones -- -- published in war is boring 36k followers · last published 6 days ago from drones to aks, high technology to low politics, exploring how and why we fight above, on and below an angry world written by war is boring 14.5k followers · 418 following we go to war so you don’t have to no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@julydd/a-wise-decision-to-choose-dwdm-mux-demux-33dd724c81f0,,,"a wise decision to choose dwdm mux/demux | by monica geller | medium sitemap open in app sign up sign in medium logo write sign up sign in a wise decision to choose dwdm mux/demux monica geller 4 min read · jul 21, 2016 -- listen share the advent of big data requires for highly efficient and capable data transmission speed. to solve the paradox of increasing bandwidth but spending less, wdm (wavelength division multiplexing) multiplexer/demultiplexer is the perfect choice. this technology can transport extremely large capacity of data traffic in telecom networks. it’s a good way to deal with the bandwidth explosion from the access network. wdm wdm stands for wavelength division multiplexing. at the transmitting side, various light waves are multiplexed into one single signal that will be transmitted through an optical fiber. at the receiver end, the light signal is split into different light waves. there are 2 standards of wdm: coarse wavelength division nultiplexing (cwdm) and dense wavelength division multiplexing (dwdm). the main difference is the wavelength steps between the channels. for cwdm this is 20nm (course) and for dwdm this is typically 0.8nm (dense). the following is going to introduce dwdm mux/demux. dwdm technology dwdm technology works by combing and transmitting multiple signals simultaneously at different wavelengths over the same fiber. this technology responds to the growing need for efficient and capable data transmission by working with different formats, such as sonet/sdh, while increasing bandwidth. it uses different colors (wavelength) which are combined in a device. the device is called a mux/demux, abbreviated from multiplexer/demultiplexer, where the optical signals are multiplexed and de-multiplexed. usually demultiplexer is often used with multiplexer on the receiving end. mux/demux mux selects one of several input signals to send to the output. so multiplexer is also known as a data selector. mux acts as a multiple-input and single-output switch. it sends optical signals at high speed over a single fiber optic cable. mux makes it possible for several signals to share one device or resource instead of having one device per input signals. mux is mainly used to increase the amount of data that can be sent over the network within a certain amount of time and bandwidth. demux is exactly in the opposite manner. demux is a device that has one input and more than one outputs. it’s often used to send one single input signal to one of many devices. the main function of an optical demultiplexer is to receive from a fiber consisting of multiple optical frequencies and separate it into its frequency components, which are coupled in as many individual fibers as there are frequencies. dwdm mux/demux dwdm mux/demux modules deliver the benefits of dwdm technology in a fully passive solution. they are designed for long-haul transmission where wavelengths are packed compact together. fs.com can provide modules for cramming up to 48 wavelengths in 100ghz grid(0.8nm) and 96 wavelengths in 50ghz grid(0.4nm) into a fiber transfer. itu g.694.1 standard and telcordia gr1221 are compliant. when applied with erbium doped-fiber amplifiers (edfas), higher speed communications with longer reach (over thousands of kilometers) can be achieved. currently the common configuration of dwdm mux/demux is from 8 to 96 channels. maybe in future channels can reach 200 channels or more. dwdm system typically transports channels (wavelengths) in what is known as the conventional band or c band spectrum, with all channels in the 1550nm region. the denser channel spacing requires tighter control of the wavelengths and therefore cooled dwdm optical transceiver modules required, as contrary to cwdm which has broader channel spacing un-cooled optics, such as cwdm sfp, cwdm xfp. dwdm mux/demux offered by fs.com are available in the form of plastic abs module cassette, 19” rack mountable box or standard lgx box. our dwdm mux/demux are modular, scalable and are perfectly suited to transport pdh, sdh / sonet, ethernet services over dwdm in optical metro edge and access networks. fs.com highly recommends you our 40-ch dwdm mux/demux. it can be used in fiber transition application as well as data center interconnection for bandwidth expansion. with the extra 1310nm port, it can easily connect to the existing metro network, achieving high-speed service without replacing any infrastructure. conclusion with dwdm mux/demux, single fibers have been able to transmit data at speeds up to 400gb/s. to expand the bandwidth of your optical communication networks with lower loss and greater distance capabilities, dwdm mux/demux module is absolutely a wise choice. for other dwdm equipment, please contact via sales@fs.com. originally published at www.fiber-optic-equipment.com internet tech -- -- written by monica geller 45 followers · 2 following fiberstore is a professional manufacturer of fiber cables. all our products come with a lifetime advance replacement warranty and are 100% functionally tested. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/community-pulse/data-101-correlation-what-is-it-and-how-do-you-use-it-720652a15bc7,,,"data 101: correlation–what is it and how do you use it? | by mysidewalk | community pulse | medium sitemap open in app sign up sign in medium logo write sign up sign in community pulse · a conversation about people, data, places, and progress. data 101: correlation–what is it and how do you use it? mysidewalk 4 min read · sep 7, 2016 -- listen share a resource for people who want to make sense of and use big data. it’s not as complicated as you think. what is correlation? at the basic level, a correlation is simply a relationship or connection between two things. when thinking about correlation in the statistical sense, it is a measure of the extent that two variables relate to each other. so how do we measure correlation? technical explanation : “pearson product-moment correlation attempts to draw a line of best fit through the data of two variables, and the pearson correlation coefficient, r, indicates how far away all these data points are to this line of best fit (i.e., how well the data points fit this new model/line of best fit).” so the correlation coefficient, r, is the measure of how well your two datasets are related. the great thing about the correlation coefficient is that it is on a simple scale from 1 to -1. values near 1 are strongly positively correlated while values near -1 are strongly negatively correlated. values near zero are not correlated. a positive correlation means that the values of both variables increase together. a negative correlation means that as the value of one variable increases the values in the second variable decrease. graphic adapted from https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php the above graphic shows the scatter of dataset values, with the x-axis being one dataset and the y-axis being the other dataset. notice that when values fall near the line of best fit the correlation coefficient becomes stronger (closer to 1 or -1) and when the data points are scattered in no apparent pattern around the best fit line the correlation is zero. it is how the data values fall around or on the line of the best fit, not the angle of the line of best fit, that is represented by correlation. below is the breakdown of the strong, moderate, and weak correlation coefficient, r, definitions used in mysidewalk. in the mysidewalk app, you just need to make a bivariate map to get a correlation value. choose two of the over 800 datasets and then look at the box in the upper right corner for the correlation coefficient. here’s an example of what that process looks like. easily correlate two sets of data in the mysidewalk application. let’s take a closer look at the correlation coefficient, as displayed in the app. (see the area of interest box at the end of the short video) you will notice that there are 3 parts. text description of the correlation (negative/positive, strong/moderate/weak, or no correlation) the actual correlation coefficient, r, that is between 1 and -1 the margin of error (± number). if the correlation is ‘invalid’, it is because the margin of error is too high. if the margin of error is more than half of the correlation value, then the app will return an ‘invalid correlation’ another case might be where it returns ‘unable to correlate’. most likely the problem is that there are fewer than 20 features (geographies) in your map view. there need to be at least 20 features in the map view to calculate a correlation. remember that correlation is about the relationship of the values from both datasets. so the correlation coefficient will always refer to all geographies within your map view. you cannot do a correlation of one dataset to another within only 1 block group. you need at least 20 block groups (or other geographic feature) to calculate a correlation. one-second summary correlation is about the relationship of values from two datasets; it does not refer to a cause and effect. to measure correlation, a scale from 1 to -1 is used. values near either end of the scale, that is near 1 or near -1, are strongly correlated while values near zero are not correlated. you can find the correlation value for any two datasets. you can quickly find the correlation between any two datasets in your community using mysidewalk. remember: the app calculates the values for you — no knowledge of calculus or statistics required. …………………………………………………………………………………… like this article? subscribe to our newsletter to read more like it here . about mysidewalk: mysidewalk is a data analysis tool for people who want to understand places. with the ability to access more than 800 sets of pre-loaded data, perform one-click analysis, and instantly share insight, it’s easy for anyone to make better, more confident decisions using data. to learn more about the type of work we are doing, visit our website . urban planning smart cities data visualization data analysis correlation -- -- published in community pulse 504 followers · last published sep 30, 2020 a conversation about people, data, places, and progress. written by mysidewalk 2.1k followers · 1.3k following empowering policy & decision-making to build a better world. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/phl-microsat/diwata-1s-high-precision-telescope-successfully-captures-high-res-ground-images-3e5c9e5ab920,,,"diwata-1’s high precision telescope successfully captures high-res ground images | by phl-microsat | phl-microsat sitemap open in app sign up sign in medium logo write sign up sign in phl-microsat · follow publication the official blog channel for the phl-microsat program follow publication diwata-1’s high precision telescope successfully captures high-res ground images phl-microsat 3 min read · aug 3, 2016 -- listen share the advanced science and technology institute (dost-asti) and the university of the philippines diliman (upd) have been collaborating with hokkaido university and tohoku university to operate diwata-1, the philippines’ first microsatellite for scientific earth observation. the high precision telescope (hpt), one of the four cameras onboard the microsatellite, has successfully captured various images during the test phase of the satellite. one of the missions of the hpt is to determine the extent of damages from natural hazards such as typhoons and volcanic eruptions. it has a nominal ground resolution of 3m at nadir, the highest resolution so far in the 50-kg class microsatellite. in the figures below, images taken by diwata-1’s hpt and by landsat 8’s operational land imager (oli) are compared. the ground resolution of landsat 8’s oli is 30m. landsat 8 is an american earth observation satellite with a launch mass of about 2,600 kg. figure 1 shows the rgb composite (normal color) images taken by diwata-1’s hpt at 9:07 am philippine standard time (pht) on 19 may 2016 and landsat 8’s oli at 10:06 am on 21 may 2016 in pht. the images show portions of dumingag, zamboanga, a mountainous area in mindanao, philippines. the oli image has a 30m ground resolution, and it can identify only rough structures of the mountain but the detailed geographical features are unclear (figure 1b). on the other hand, the hpt image can clearly identify trees, rock surface, mountain paths, river and some man-made objects (figure 1a). figure 1. comparison of two rgb color images: (a) diwata-1’s high precision telescope (hpt), (b) landsat 8’s operational land imager (oli). the images captured a mountainous area in mindanao, philippines at 9:07 pht on 19 may 2016 by hpt and 10:06 pht on 21 may 2016 by oli. the diwata- 1 hpt image can clearly identify trees, rock surface, mountain paths, river and some man-made objects. figure 2 shows a comparison of false color images of an urban area in florida, united states, which were taken by diwata-1’s hpt at 7:48am eastern daylight time (edt) on 30 june 2016 and by landsat 8’s oli, respectively. these false color images are created to emphasize the difference between vegetation and other objects using three (3) different bands. figure 2a is a mosaic image constructed by stitching six (6) successive images captured with 1-second interval. as a successful demonstration of the target pointing capability of diwata-1, the microsatellite was able to capture six (6) images of the focused area with field-of-view of 2 km x 1.5 km even though it was orbiting the earth with a velocity of ~7.7 km/s at an altitude of ~400 km. buildings and roads are clearly distinguishable from the vegetation in the hpt images (figure 2a), while such features are very unclear in the oli image (figure 2b). figure 2. comparison of two false color images: (a) diwata-1’s hpt, (b) landsat 8’s oli. the images captured an urban area of florida, united states at 7:48 edt on 30 june 2016 by hpt and 11:55 edt on 9 july 2016 by oli. figure 2a is created by stitching six (6) successive images captured by diwata- 1 through its target pointing capability. the red-colored area indicates vegetation. the buildings and roads can be clearly distinguished from the vegetation in the diwata-1 hpt image. the high-resolution images that can be obtained with diwata-1’s hpt enables more accurate post-disaster assessment that can lead to timely and efficient emergency response. space satellite technology philippines diwata 1 -- -- follow published in phl-microsat 145 followers · last published jun 4, 2020 the official blog channel for the phl-microsat program follow written by phl-microsat 264 followers · 8 following official blog of the development of philippine scientific earth observation microsatellite (phl-microsat) program (now stamina4space) no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/gab41/effectively-running-thousands-of-experiments-hyperopt-with-sacred-dfa53b50f1ec,,,"effectively running thousands of experiments: hyperopt with sacred | by anna b | medium sitemap open in app sign up sign in medium logo write sign up sign in effectively running thousands of experiments: hyperopt with sacred anna b 9 min read · oct 12, 2016 -- 2 listen share we are currently nearing the end of our project cycles, which means we are running a lot of tests. on the pythia project there are a lot of different hyperparameters (experimental parameters) that we can choose between. we started investigating the different types of hyperparameter search techniques that are available in order to hopefully find the best solution without running tests for months. we found a few different options: hand-tuning — informed guessing or intuitive search. this method involves some analysis of the results as they come in and choosing certain parameters over others when running experiments. grid—methodically test all parameter combinations. this method is really the only method that truly guarantees you achieve optimal performance, but we estimated there are well over a million different tests that we could run — which needless to say just isn’t feasible to do in the time that we have. also in real-valued parameters, the resolution of the grid can be a major issue random — randomly toggle different parameters on or off. random has been shown to achieve optimal or near-optimal performances for many different algorithms. hand-tuned random — choose which parameters to randomly toggle and which to set to a particular value. for example, we observed that the skip-thought vector featurization technique pat described in his blog consistently degraded the algorithm’s performance for certain datasets, thus we decreased the number of times we attempted to use that technique. probabilistic — there are a number of optimizers and open source packages that use bayesian modeling techniques for hyperparameter optimization. these methods measure the effect of a parameter on an objective function, and estimate how well the model could potentially perform when that parameter is used. as the model learns over successive experiments, better parameters are retained, while worse parameters are phased out. there are many different open source python packages, but for multiple reasons much of our effort, and this blog post, focuses on hyperopt . trees of what? hyperopt uses a method called the tree-structured parzen estimator (tpe); an approach introduced at nips 2011 by bergstra et. al . one of the main advances of tpe over other probabilistic methods is that most other methods cannot retain the dependencies between parameters. tpe is able to retain these relationships as it models the density of a parameter for ‘good’ experiments and compares this to its density for ‘bad’ experiments. it is then able to use these models to determine expected improvement of the objective function for any values a parameter can take. tpe is useful for the pythia project as we have some parameters describing which algorithm to run (e.g., logistic regression versus xgboost) as well as parameters pertaining to that algorithm (e.g., the penalty and tolerance of the logistic regression model). i’ll show you what this looks like a little later in this post. i’m a very visual person, so i wanted to see how hyperopt was working. as this is nearly impossible within the context of pythia due to the number of parameters, i decided to analyze hyperopt with a different 2d optimization problem — namely, the rosenbrock function . this quadratic function is complex enough to be difficult as it has a long and narrow near-optimal region, but is still easy enough to understand. the minimum value of this optimization is zero which is located at (1,1). the value of the optimization is displayed below in the form of a heat map. lower values are shown in red while higher values in blue. i assessed the performance of 100 random search tests, and compared these to the performance of 100 hyperopt tests. for this test there are two parameters — the value of x and the value of y. i let these vary between -0.5 and 2.5. the results of each test were analyzed and plotted on top of the heat map of the rosenbrock function. generally, hyperopt got close to the optimal point, but random search also did fairly well. more of the hyperopt tests were closer to the optimal point compared to random search, and found a better overall solution. this suggested to me that hyperopt is indeed reducing the parameter search space in an intelligent manner. secondly, random is less likely to stumble upon the best solution in a higher dimensional problem (curse you curse of dimensionality !). finally, bergstra et. al . finds tpe is faster to find some near-optimal input than random. running hyperopt a very good introduction to hyperopt may be found at fastml.com , but in short, to run hyperopt you define the objective function, the parameter space, the number of experiments to run and optionally set a constructor to keep the experiments. there are both continuous and categorical methods to describe the parameters, and as i’ve mentioned, these parameters can be defined hierarchically. the full hyperopt code for optimizing the rosenbrock function above is pretty simple. from hyperopt import fmin, tpe, hp, trials number_of_experiments = 100 #define the rosenbrock function as the objective def rosenbrock_objective(args): x = args['x'] y = args['y'] return (1.-x)**2 + 100.*(y-x*x)**2 #trials keeps track of all experiments #these can be saved and loaded back into a new batch of experiments trials_to_keep = trials() #space is where you define the parameters and parameter search space space={'x': hp.uniform('x', -0.5, 2.5), 'y': hp.uniform('y', -0.5, 2.5)} #the main function to run the experiments #the algorithm tpe.suggest runs the tree-structured parzen estimator #hyperopt does have a random search algorithm as well best = fmin(objective=rosenbrock_objective, space=space, algo=tpe.suggest, max_evals=number_of_experiments, trials = trials_to_keep ) and now we enter the ‘sacred trees of parzen’ hyperopt looks like it could work well for our use case, but colleague dave has told me to show my work — so how do i go about writing a program that uses hyperopt and sacred? as dave points out, sacred is an open source python package to “help you configure, organize, log and reproduce experiments.” when using sacred with hyperopt, you set up sacred in a very similar manner, specifying a configuration and an observer. one of the largest modifications to sacred that you should likely make is a patch to avoid having to declare all parameters (huge hat-tip to my colleague yonas tesfaye for discovering this patch). this patch is very beneficial if you do not want hyperopt to consider some parameters which is often the case in experiment testing. for example if we run pythia with one algorithm, the other algorithm parameters are not specified. from sacred.initialize import scaffold def noop( item ): pass scaffold._warn_about_suspicious_changes = noop the other major trick to be able to still log everything in sacred correctly is to use global variables. the reason for this is that sacred’s main function experiment.main() has one argument, a function object that it calls with no dependent variables. in addition, the function returns more results than the single objective hyperopt can handle. global variables solve both of these issues. i’ll show a simplified example below, but if you are really curious, check out the function we are using for pythia on github . first, let us set up the callable object that sacred can call. sacred will log anything that is returned when the main function experiment.main() is called. in the example below this is all_results , which is a dictionary created by running one complete experiment in pythia. however, before we return all of our results to be logged, we want to keep track of a single global variable for hyperopt to use — in this case the aptly named result . for pythia we are going to use the average f1 score , and because hyperopt will try to find the minimum value, we will provide it with the negative f1 score. # this is where we run pythia and set a result hyperopt will use def run_pythia_with_global_args(): global args global result # the primary pythia function completes one train and test cycle all_results = pythia_main.main(args) # result is set here and is later used by hyperopt # we really care about the average f1 score result = -np.mean(all_results['f_score']) # we really want to save all the results in sacred # hence why we return all_results return all_results now we can set up the objective() function that hyperopt will use. again, we are going to be using the global values so that our function run_pythia_with_global_args() can see the experiment parameters and set the variable result. mostly in our objective() function we are setting up and running a sacred experiment. you have to set up experiment.main() with the function to call, and then update the variables with config_updates=pythia_args. from sacred import experiment import src.pipelines.master_pipeline as pythia_main # objective is set up much like the simple hyperopt example above def objective(pythia_args): global args global result ex = experiment('hyperopt') ex.main(run_pythia_with_global_args) r = ex.run(config_updates=pythia_args) return result the last part of the function is pure hyperopt. if you remember, hyperopt needs an objective (which we have) as well as a parameter space and the number of experiments to complete. now we can truly take advantage of hyperopt’s ability to retain hierarchical parameters. this is shown with the variable algorithm_type. when the algorithm xgboost is chosen, hyperopt determines how to set the variables xgb_learnrate and xgb_maxdepth, but does not try to set log_c or log_tol as these parameters are associated with the logistic regression algorithm. this ability seems so simple, but is really important and beneficial! the last little bit of code is the actual call to run hyperopt where all of the pieces fit together. def run_pythia_hyperopt(): # define the space - the pythia space is quite large! space = ""algorithm_type"":hp.choice('algorithm_type', [ { 'log_reg': true , 'log_c': hp.uniform('log_c', 1e-5,10), 'log_tol': hp.uniform('log_tol', 1e-5, 10), 'log_penalty': hp.choice('log_penalty', [""l1"", ""l2""]) }, { 'xgb': true , 'xgb_learnrate': hp.uniform('x_learning_rate', 0.01, 1), 'xgb_maxdepth': hp.choice('x_max_depth',[3,4,5,6]), } ]), ""bow_append"":hp.choice('bow_append', [ true , false ]), ""bow_difference"":hp.choice('bow_difference', [ true , false ]), ""bow_product"":hp.choice('bow_product', [ true , false ]), ""bow_cos"":hp.choice('bow_cos', [ true , false ]), ""bow_tfidf"":hp.choice('bow_tfidf', [ true , false ]), #many, many more parameters 'use_cache': true } # now we can set up the main hyperopt function fmin # it is exactly the same as in the simple example! optimal_run = fmin(objective, space, algo=tpe.suggest, max_evals= 1000) is it working? yes!… but it needed a little help to find the highest value. one of the main lessons i have learned by using hyperopt is that you still need a large number of runs for hyperopt to be able to learn which parameters are best. humans, on the other hand, can often see performance trends given fewer data points. it is definitely important to acknowledge biases we might have when hand-tuning, but hand-tuning often works quite well. to demonstrate just how many samples you need to find the best result let’s go back to our 2d example with the rosenbrock function. if you let each optimizer try out 50 different test points, tpe will find a better (e.g., lower) point than random search around 55% of the time. if you try 500 samples though, tpe will find a lower point than random search 85% of the time. so even in our simpler 2d example, we need a lot of tests. thus we know in pythia we are going to need to run a really large number of experiments! however, in order to run any number of experiments, we have to limit the types of featurization techniques as some simply take too long to run. to combat this problem, we have been using a hand-tuned hyperopt strategy (as an aside there are some other cool techniques that do take run time into account). similar to hand-tuned random we choose which parameters to let hyperopt toggle and which to set to a particular value (in this case, off ). luckily, these methods are generally not performing as well, so i don’t get too much heartburn by leaving them out. with hand-tuning, hyperopt has completed some of the best performing experiments in less than a day, but without hand-tuning, it can take three to four days for comparable results. we would love to know if you are using hyperopt or sacred in your work, or both of them together! how has your performance with hyperopt been? lab41 is a silicon valley challenge lab where experts from the u.s. intelligence community (ic), academia, industry, and in-q-tel come together to gain a better understanding of how to work with — and ultimately use — big data. learn more at lab41.org and follow us on twitter: @_lab41 machine learning data science big data deep learning open source -- -- 2 written by anna b 124 followers · 8 following responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/commencis/using-redux-with-mvvm-on-ios-18212454d676,,,"using redux with mvvm on ios. before we start, what is redux? from… | by göksel köksal | commencis | medium sitemap open in app sign up sign in medium logo write sign up sign in commencis · we help leading brands to grow and scale in digital, powered by our big data, analytics and cloud products. using redux with mvvm on ios göksel köksal 4 min read · aug 24, 2016 -- 6 listen share photo by nick fewings on unsplash before we start, what is redux? from redux.js.org : redux is a predictable state container for javascript apps. so if you are not a web developer, it’s totally normal that you don’t know what redux is. but it’s really easy to understand and the concept can be applied to everywhere, including ios development. redux overview to summarise, your app has a store . in this store, you have your application state . from your view s, you send action s to the store and reducer s execute these actions. as state is changed by reducers, store notifies the view so that it can reflect changes on ui. core benefits: it encourages you to find the real state of your app and model every change and action. this makes everything easy to understand. maximises separation of concerns. every component does only one job. improves testability. reducers has pure functions that are really easy to test. store propagates state automatically. (so you don’t have to choose between delegation, blocks, notifications or kvo.) however, redux is very strict and not very easy to apply in ios applications, mainly because of uikit. i won’t go into details, but if you curious, i recommend you to watch benjamin encz ’s great talk on this topic. mvvm + redux mvvm is a lightweight, easy-to-adapt architecture, which is similar to mvc. however, it doesn’t really provide the benefits of redux. so why not use some of the concepts? i will try to improve the communication between view controller and view model using the principles from redux. for more on mvvm: bohdan orlov ’s post on ios architecture patterns example let’s implement a simple app, in which you list some movies in a table view and user can move them around, delete some or insert new ones. first, define the state . struct moviesstate { var movies: [movie] = [] var fetching: bool = false } vm will be our store , so keep the state in vm. class moviesviewmodel { private(set) var state = moviesstate() } in redux, reducer s mutate the state using pure functions. to keep it simple, we can define reducers as mutating functions on moviesstate . but they should be dead-simple, sync and atomic functions that does only one job. extension moviesstate { mutating func reloadmovies(movies: [movie]) { self.movies = movies } } reloadmovies function will just set movies array in state struct and when the state changes, we should notify view controller so that it can update ui to reflect changes. but how? vc can send an action to vm and update ui for current state, in case there’s any change. (bad) vc can assign itself as vm’s delegate and vm can inform delegate if there’s any change in state. only problem is that vc cannot know which part of the state changed, (and taking the diff is hard) so it re-renders everything to be sure. (not that bad) vc can send an action to vm, and vm can inform vc with the exact change that happened on state. (good) to be able to that, we should also model changes that can happen in our state, and feed vc with these change objects so that it can sync ui with current state. in our humble example, we can model changes with an enum like this: extension moviesstate { enum change { case none case fetchstatechanged case movieschanged } } then, all the mutating functions on state should return a change object. extension moviesstate { func reloadmovies(…) -> change { ... return .movieschanged } } then, we need a way to propagate this change to vc. a simple handler block would do fine. class moviesviewmodel { ... var statechangehandler: ((moviesstate.change) -> void)? } finally in vc, all we need to do is: class moviesviewcontroller { override func viewdidload() { super.viewdidload() model.statechangehandler = { change in switch change { case .none: break // no change case .movieschanged: tableview.reloaddata() case .fetchstatechanged: setloadingviewvisible(model.fetching) } } model.fetchmovies() } } you might have noticed that fetchmovies is an async call, but it doesn’t have a callback. because you don’t need it. that call will make some changes on state, over time. and every time it happens, statechangehandler block will be called with that exact change in state. this is how fetchmovies call should look like: func fetchmovies() { let fetchstatechange = state.setfetching(true) statechangehandler?(fetchstatechange) api.fetchmovies { movies in let reloadchange = state.reloadmovies(movies) statechangehandler?(reloadchange) let fetchstatechange = state.setfetching(false) statechangehandler?(fetchstatechange) } } we have used store , state and reducer s from redux. how about action s? actually, we already used it. since we don’t talk to moviesstate directly, all methods defined on vm can be seen as redux actions that initiate a state change. bottom line we are using mvvm so it is still lightweight. no framework needed. it becomes easy to keep vm and vc in sync. even without binding frameworks. we have explicit, fully testable state objects. we have simple state propagation even for async calls. ui update code in vc is not duplicated in multiple callbacks. state changes are very clear. sample project : movies further reading : routing with mvvm on ios thank you for reading! please let me know what you think. all suggestions and questions are welcome. ios software architecture react mvvm swift -- -- 6 published in commencis 618 followers · last published jun 18, 2025 we help leading brands to grow and scale in digital, powered by our big data, analytics and cloud products. written by göksel köksal 1.2k followers · 269 following software engineer. veteran gamer. responses ( 6 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/feature-creep/the-software-engineer-s-guide-to-asserting-office-dominance-ddea7b598df7,,Test-Driven Development,"the software engineer’s guide to asserting dominance in the workplace | by daniel wu | feature creep | medium sitemap open in app sign up sign in medium logo write sign up sign in feature creep · super important reads for software people the software engineer’s guide to asserting dominance in the workplace daniel wu 5 min read · jan 22, 2016 -- 137 listen share as a software engineer, changing jobs is a way of life. every day, beautiful recruiters from top tech companies reach out to you on linkedin with new opportunities, and each new job equals higher pay and an opportunity to reinvent yourself for the better — as long as you know how to. here is a handy first-week guide to becoming the alpha of your new workplace. monday on your first day of work, your focus should be entirely on physical establishment of dominance in the office. humans are hard-wired to follow the biggest and strongest. once you have proven yourself with the pedo principle, you will command the respect and admiration of your team, and every other aspect of becoming the alpha naturally falls into place. wake up early on monday, hit the weights and dehydrate yourself for enhanced vascularity. take a shower afterwards, but put your tight-fitting, sweaty under armour gym clothes back on. you’ll arrive at work looking swole, focused, and ready to get shit done. after arriving at work, your number one priority is picking your desk. if you can get a standing desk, get one. being at higher ground is a position of power. this advantage is especially important in open-office environments. people need to look up to you — even if only literally. always pick a corner desk when available, and position it with your back to the corner so you are overlooking your coworkers. angle your monitors so that one is only visible by you. needless to say, this is your dedicated facebook monitor. for your other monitor, keep code and a terminal on full display at all times. optimize lunch time by mixing protein shakes and doing body weight exercises. after lunch, bring a ten pound tub of whey protein powder to your desk and slam it down. ten pounds is the largest size of the powder that is currently sold. if larger sizes are sold in the future, you should obviously switch to that. and don’t wait until you finish your current tub either. immediately purchase the larger size and display it in a prominent position on your desk. the brand of the protein powder must be optimum nutrition, as it is the gold standard of whey. you are the gold standard of software engineering. you’ll need at least two blender bottles as well, with three to four being the most optimal number. leave them around the office as a form of territorial marking. spend the rest of the day meeting your teammates and noting their weaknesses. at night, prepare ten hard-boiled eggs for tuesday. tuesday after monday’s display of physicality, you need to spend tuesday getting your development environment set up. checkout your team’s code from the git repo and start ramping up. if your new team doesn’t use git, announce your resignation immediately and walk out. eat your hard-boiled eggs sporadically throughout the day, but save one. do not take any breaks except to mix protein shakes. remember that you should be consuming one gram of protein per pound of body weight, or per line of code written — whichever is greater. spend the rest of the day familiarizing yourself with the team’s codebase. every five to ten minutes, let out a deep sigh and write something down on a notepad. maintain a demeanor of mild disgust on your face that gets increasingly more annoyed as you browse through more and more of the code. mumble words like “refactor” and “rewrite” under your breath. start drawing random complex architectural diagrams on your whiteboard. by 3 pm you should be visibly angry. eat some chili peppers to force yourself to sweat. at 4 pm, allow your rage to boil over and throw your last egg at the wall in a fit of rage. slam your laptop closed and head home early. wednesday wednesday is time to institute your technical sovereignty. be the first one at your daily stand-up, and run it like the scrum master you know you are. if someone talks for more than ten seconds, immediately interrupt them and tell them to take it offline. bring a notepad and take notes with a black pen. keep a second red pen in your pocket. when it’s your turn to speak, go on a long rant about all the horrible design patterns you’ve discovered in the code on tuesday. announce that you have already started designing the architecture of the inevitable re-write. name-drop as many of the latest software frameworks and technologies as possible throughout your rant. use words like big data, cloud, and scalability. mention test-driven development at least three to four times. as you are speaking, lock eye-contact with the person to your left, and do not break until they look away. at this point, lock eye-contact with the person to their left, and do the same. by the time you are finished speaking, you will have gone full circle and sent a clear alpha signal to each of them. perform this ritual at every stand-up. if someone tells you to take it offline while you are speaking, brush it off. inform them that what are you saying is the most important thing that has ever been spoken at this company. then, pull the red pen out of your pocket and scribble their name down. thursday by thursday, you have demonstrated that you are the strongest, smartest, and most passionate engineer on your team. when you get to your desk, rip your standard-issue keyboard out and slide it off your desk. plug in your own mechanical keyboard with cherry mx green or blue switches. your typing will now be a constant audible reminder to your teammates that you aren’t fucking around. log into your team’s bug-tracking software and look around for some low-hanging fruit to fix. spend the rest of the day working and fixing them, but don’t send any code reviews out. as you are working, make sure to look extra pissed off at all times, like you can’t believe how your teammates managed to write such crappy code. type more and more furiously as the day progresses. do not eat lunch. do not take breaks. only leave your desk when you are the last man in the office. at around 10 pm, remote in from home and send your first cr out to your team. in an hour, send another cr out. you should have fixed enough bugs during the day to do this for the next four to five hours. you are one hard-working, dedicated, bad-ass motherfucker. friday it’s the last day of your first week, and if you followed this guide perfectly you are now undoubtedly the alpha of your new team. send a working-from-home e-mail out and take the day off. you deserve it. software development satire favorites -- -- 137 published in feature creep 511 followers · last published jan 22, 2016 super important reads for software people written by daniel wu 979 followers · 50 following responses ( 137 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@pdbartsch/turf-js-mapbox-js-leaflet-js-zurb-foundation-ed43791d1de9,,,"turf.js + mapbox.js/ leaflet.js + zurb foundation | by paul bartsch | medium sitemap open in app sign up sign in medium logo write sign up sign in turf.js + mapbox.js/ leaflet.js + zurb foundation paul bartsch 1 min read · apr 25, 2016 -- listen share a template to filter one geojson layer based on attributes from another using turf.js this is really simple but i learned a lot and i’m excited about using turf for more complicated projects in the future. a working example from morgan herlocker got me started pulling external geojson using jquery at the moment — i’ve got it setup in this weird way where the request for one data source is nested inside of the other…not real clean looking but it works to avoid async problem i was having would like to integrate omnivore eventually water color map obviously not the best for a real tool used during an emergency but i like it for testing/learning i’m excited about the posibilities with this seemingly simple filtering using turf.js click the map image below to see a live version originally published at pdb.space . gis nodejs leaflet -- -- written by paul bartsch 83 followers · 184 following maker/ data wrangler/ proud father/ love fam, friends & outdoors/ biker/ hiker/ open-source enthusiast/ geospatial developer / cal poly/ucsb / https://geopy.dev no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6,,,"xgboost: “hi i’m gamma. what can i do for you?” — and the tuning of regularization | by laurae | data science & design | medium sitemap open in app sign up sign in medium logo write sign up sign in data science & design · all about data science, machine learning, and design. also, lot of things about statistics, data visualization, benchmarking, and funny stuff. xgboost: “hi i’m gamma. what can i do for you?” — and the tuning of regularization laurae 6 min read · aug 22, 2016 -- 8 listen share laurae: this post is about tuning the regularization in the tree-based xgboost (maximum depth, minimum child weight, gamma). it also explains what are these regularization parameters in xgboost, without having to go in the theoretical details. the post was originally at kaggle . kxx wrote: do we have to tune gamma at the very end, when we have max_depth, subsample, colsamlpe_bytree? how do we find the range for this parameter? i’ve found that it’s almost impossible to find “good” gamma in this competition (and in homesite quote conversion) post is large when i read it. i’ll spread it using different separated paragraphs. full in-depth tutorial with one exercise using this data set :) what’s gamma? the range of that parameter is [0, infinite[. finding a “good” gamma is very dependent on both your data set and the other parameters you are using. there is no optimal gamma for a data set, there is only an optimal (real-valued) gamma depending on both the training set + the other parameters you are using. gamma is dependent on both the training set and the other parameters you use. there is no “good gamma” for any data set alone it is a pseudo-regularization hyperparameter in gradient boosting. mathematically you call “gamma” the “lagrangian multiplier” (complexity control). the higher gamma is, the higher the regularization. default value is 0 (no regularization). gamma values around 20 are extremely high, and should be used only when you are using high depth (i.e overfitting blazing fast, not letting the variance/bias tradeoff stabilize for a local optimum) or if you want to control the directly the features which are dominating in the data set (i.e too strong feature engineering). gamma tuning always start with 0, use xgb.cv, and look how the train/test are faring. if you train cv skyrocketing over test cv at a blazing speed, this is where gamma is useful instead of min_child_weight (because you need to control the complexity issued from the loss, not the loss derivative from the hessian weight in min_child_weight). another choice typical and most preferred choice: step max_depth down :) if gamma is useful (i.e train cv skyrockets at godlike speed when test cv can’t follow), crank up gamma. this is where the experience with tuning gamma is useful (so you lose the lowest amount of time). depending on what you see between the train/test cv increase speed, you try to find an appropriate gamma. the higher the gamma, the lower the difference between train/test cv will happen. if you have no idea of the value to use, put 10 and look what happens. how to set gamma values? if your train/test cv are always lying too close, it means you controlled way too much the complexity of xgboost, and the model can’t grow trees without pruning them (due to the loss threshold not reached thanks to gamma). lower gamma (good relative value to reduce if you don’t know: cut 20% of gamma away until you test cv grows without having the train cv frozen). if your train/test cv are differing too much, it means you did not control enough the complexity of xgboost, and the model grows too many trees without pruning them (due to the loss threshold not reached because of gamma). put a higher gamma (good absolute value to use if you don’t know: +2, until your test cv can follow faster your train cv which goes slower, your test cv should be able to peak). if your train cv is stuck (not increasing, or increasing way too slowly), decrease gamma: that value was too high and xgboost keeps pruning trees until it can find something appropriate (or it may end in an endless loop of testing + adding nodes but pruning them straight away…). need tips about how to tune perfectly gamma tuning gamma should result in something very close to a u-shaped cv :) — this is not exactly true due to potential differences in the folds, but you should get approximately a u-shaped cv if you were to plot (gamma, performance metric). from there, you know when to minimize and when to maximize :) (and with your experience too!) test yourself with high depth such as 15 in this data set, you can train yourself using gamma. you should be able with the following settings to get at least 0.841: 4-fold cross-validation subsample = 0.70 colsample_bytree = ~0.70 (tune this if you don’t manage to get 0.841 by tuning gamma) max_depth = 10 nrounds = 100000 (use early.stop.round = 50) eta = 0.05 in case you get a bad fold set, set yourself the seed for folds, and set your own benchmark using max_depth = 5 (which was “the best” found). at the end, you should be able to push locally by 0.0002 more than the typical “best” found parameters using an appropriate depth. unfortunately, a gamma value for a specific max_depth does not work the same with a different max_depth. this is also true for all other parameters used. what to optimize first? gamma or depth? what’s up with min_child_weight? it is your choice. using gamma will always yield a higher performance than not using gamma, as long as you found the best set of parameters for gamma to shine. this is due to the ability to prune a shallow tree using the loss function instead of using the hessian weight (gradient derivative). controlling the loss function? (gamma) => you are the first controller to force pruning of the pure weights! (full momentum) controlling the hessian weights? (min_child_weight) => you are the second controller to force pruning using derivatives! (0 momentum) when to use gamma? easy question: when you want to use shallow trees because you expect them to do better. very good hyperparameter also for ensembling / dealing with heavy dominating group of features, much better than min_child_weight. what to remember? too much information! need tl;dr if you need to resume what is min_child_weight: the knob which tunes the soft performance difference between the overfitting set (train) and a (potential) test set (minimizes the difference => locally blocking potential interactions at the expense of potentially higher rounds and lower or better performance). if you need to resume what is gamma: the knob which fine-tunes the hard performance difference between the overfitting set (train) and a (potential) test set (minimizes both the difference and the speed at which it is accrued => give more rounds to train at the expense of being stuck at a local minima for the train set, by blocking generalized strong interactions which gives no appropriate gain). if you need to resume what is depth: the knob which tunes “roughly” the hard performance difference between the overfitting set (train) and a (potential) test set (maximizes only the speed at which it is accrued => give room for more generalized potential interactions at the expense of less rounds). understand by “performance” the word “complexity”, i.e how complex (overfitting) a model is, but also how good the complexity for your model is when measured using quantitative measures . tl;dr s version if you understood the four sentences higher ^, you can now understand why tuning gamma is dependent on all the other hyperparameters you are using, but also the only reasons you should tune gamma: very high depth => high gamma (like 3? 5? 10? 20? even more?) typical depths where you have good cv values => low gamma (like 0.01? 0.1? going over 1 is useless, you probably badly tuned something else or use the wrong depth!) i still can’t get it between gamma and min_child_weight!!! take the following example: you sleep in a room during night, and you need to wake up at a specific time (but you don’t know when you will wake up yourself!!!). you know the dependent features of “when i wake up” are: noise, time, cars. noise is made of 1000 other features. if you tune gamma, you will tune how much you can take from these 1000 features in a globalized fashion. for instance, you won’t take all immediately, but you will take them slowly. xgboost will discard most of them, but not all everytime :) if you tune min_child_weight, you will tune what interactions you allow in a localized fashion. for instance, if the interaction between the 1000 “other features” and the features xgboost is trying to use is too low (at 0 momentum, the weight given to the interaction using time as weight), the interaction is discarded (pruned) everytime . :) that’s over-simplified, but it is close to be like that. remember also that “local” means “dependent on the previous nodes”, so a node that should not exist may exist if the previous nodes are allowing it :) machine learning data science design -- -- 8 published in data science & design 1.2k followers · last published dec 30, 2018 all about data science, machine learning, and design. also, lot of things about statistics, data visualization, benchmarking, and funny stuff. written by laurae 1.1k followers · 7 following laurae’s data science & design curated posts responses ( 8 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@sanjsanj/comparethemarket-com-london-software-development-s-best-kept-secret-91bed8fd0b9f,,,"comparethemarket.com — london software development’s best kept secret | by sanjay purswani | medium sitemap open in app sign up sign in medium logo write sign up sign in comparethemarket.com — london software development’s best kept secret sanjay purswani 7 min read · jan 7, 2016 -- listen share you’ve probably used the website to save you money on your insurance, you’ve definitely seen the tv ads with the meerkats, and if you haven’t already then you really should get the meerkat movies 2-for-1 cinema tickets deal. but you may not know about the technology behind the scenes at comparethemarket that has made it a market leading brand. this article is written for fellow software developers and will hopefully help you see the company in a very different light. the story begins in october of last year. i was preparing to move on from my role on the careers team at makers academy, a developer bootcamp at the epicentre of london’s thriving tech community, where we coach students in test driven development, pair programming, teamwork, agile methodologies, self led learning, ownership, retrospection and all the other best practices that make a good contemporary developer. during my time there i had helped place dozens of our graduates in some of the best dev roles in london, so i had some insight into what the various types of companies had to offer, what us devs should be looking out for and how to land your perfect job. i think it also qualifies me to judge a good job from a bad one. makers academy graduates work at 8th light, bbc, cloudcredo, cnn, currency cloud, deloitte digital, fnz, financial times, lyst, mergermarket, new bamboo, ogilvy & mather, pivotal labs, qubit, red badger, red56, reed business information, shutl (ebay), sky, squirrel, thoughtworks, zopa and more. that put me in a really envious position; i had an expansive network to draw on, i knew many of the decision makers in our industry, i knew the pioneers on the bleeding edge, i had helped many graduates navigate their own career changes and i had the support of my kick ass colleagues at makers academy who wanted to see me sail into an awesome new job. i began investigating the current open roles in london, having a good idea of what i was looking for. there weren’t many surprises for me at that point, apart from, you guessed it, comparethemarket. allow me to explain why it was the perfect fit for me and what makes them a developer’s wet dream. a large part of comparethemarket’s lack of presence on the scene was due to their location in peterborough, but that’s all changed with their expansion into new premises at london’s old street roundabout. the old street roundabout is the tech hub of london. the move to london was precipitated in part by the challenges in hiring enough suitable devs in their part of the country. they were expanding and continue to do so at a remarkable rate and need to hire enough talent to keep up with that growth. an impressive feat, but what’s even more impressive is comparethemarket’s extremely low attrition rates (<2% of permanent it staff). once you’re in you probably won’t ever want to leave, and if you do, we will hunt you down… and ask you why you don’t like us so we can get better. comparethemarket are one of europe’s largest insurance comparison websites with dominance of the uk car insurance comparison market and businesses throughout europe and the rest of the world. we save consumers eye watering sums of money every year by finding them cheaper deals on their car, bike, breakdown, home, energy, travel, credit cards and life insurance. simples! all this goes towards making us a very well resourced and well captained organisation. unlike a startup that’s vulnerable to shifting winds, comparethemarket are not going anywhere. you might worry that such a leviathan enterprise would make for an uncomfortably corporate or unwieldy operation, but in practice it’s not like that at all. this is not your daddy’s mega corporation of old, this is high tech business as it should be done in the 21st century. when you join comparethemarket you’ll have a well oiled onboarding machine to help you settle in and set your hardware up. you’ll also have a line manager who’ll be your career coach, a mentor you can rely on to give you good advice and help you find your career path. my line manager is the enchanting ms emma penney, she’s the one that requests that i stay on brand and call it ‘comparethemarket’, lower-case, all one word, preferably with a .com on the end but she’s forgiving of that. i once called it ‘ctm’ and she beat me mercilessly, true story. a recent recruitment video we made, we blew the budget down the pub, sorry… with ~120 developers in 12 different teams, each team is a small self contained unit that has ownership of a part of the business, having all they need to work effectively in an agile manner. we’re shielded by and large from the inevitable business entanglements by our project managers, and unlike some companies where totally unrealistic demands are placed on dev teams and where our opinions count for very little, at comparethemarket the dev is at the centre of business decisions. coming from a creative background, i wanted to work full stack with an emphasis on the front end, so finding a place on the marketing it (mit) team was a dream come true. mit function like an in house agency, working on short, impactful campaigns and revitalising existing legacy content. this provides a fast paced excitement and also allows us to work in varying tech. in the last two months i’ve worked in c#, episerver, ruby, javascript, angular, html, css, sass and their respective testing frameworks. i’ve also had the opportunity to flex my dev ops muscles, pushing out regular code releases to live. my team consists of a project manager with impressive tech skills, a marketing product owner with a science degree, a quality analyst who used to be a dj, a tech lead who has contracted as a senior dev for over 15 years, a solutions architect with impeccable dress sense, two senior front enders who shoot magic from their fingertips, two engineers who bring the pain day in and day out, and me, the newest engineer on the team. and boy do we have a laugh while we get the business done, banter and camaraderie infuse the shop floor and morale is very high, which makes for a fun and productive environment. recent projects include star wars: galactic insurance in conjunction with disney and the launch of their record breaking star wars sequel, the force awakens. other teams include the various insurance journeys on our website, the core car insurance comparison journey, pure web ops, mobile, big data and more. moving around teams is encouraged, if you want to try something new you’re free to do so, no chance of life getting boring or predictable here. the culture is excellent, whether you’re a greenhorn or a seasoned veteran your opinion matters, you can question old ideas and introduce new ones. in my opinion a good developer should be constantly learning new tech, retraining themselves every few months and comparethemarket totally encourage and support us in doing this. we’re provided with the hardware, software and most whatever else we need. you’re free to experiment and take ownership, and we walk the walk; in my short time here i’ve seen several teams and individuals roll with ideas that they think are beneficial to us and our customers and be totally encouraged in doing so despite the risks and challenges. sometimes it works, always we learn, so we never lose. our it director, james lomas, a man with a formidable reputation and very little spare time, will gladly make time for you when you need it. when i first spoke with him i happened to mention how thrilled i was to be working on a star wars project on my very first day seeing as i’m such a huge fan. weeks later when he had the opportunity to send some of us to the european premier in leicester square (thanks disney!) he actually remembered our conversation out of the hundreds he would have had in that time span and was nice enough to nominate me to go. i was so pumped to be one of the first people to watch the film and so grateful to be working with genuinely thoughtful people like him. the view from the red carpet at the star wars: the force awakens premier in london. there are many more perks as well. subsidised pensions and healthcare, flexible working, volunteer days and a team that want to get the best out of you, to name but a few. are you sold yet? good, because we’re hiring . we’ve got a few open roles right now, once they’re filled it might be a while before an opportunity like this opens up again. we’re also always interested in speaking with talented people, regardless of whether we have a vacancy or not. questions? comments? get in touch, we’d love to talk. coding tech london -- -- written by sanjay purswani 298 followers · 116 following coder, technologist and podcast host @ aheadintech.com no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/gyroscope-innovations/gyroscope-v1-8-release-notes-4a57adf9f8d0,,,"gyroscope v1.8. expanded healthkit syncing, better… | by anand sharma | gyroscope sitemap open in app sign up sign in medium logo write sign up sign in gyroscope · follow publication to help people live happier & healthier lives follow publication gyroscope v1.8 release notes for october 2016 anand sharma 7 min read · oct 28, 2016 -- listen share the latest version (v1.8) is now available in the app store. we’ve added a bunch of new healthkit features to the app, as well as improvements to the web experience and data quality on all platforms. iphone app 1.8 based on the common requests and feedback we’ve been getting, we’ve made improvements to existing features like the sleep ai, and added new types of data that pro users can start tracking. nutrition tracking (pro) gyroscope just got much powerful for weight loss. if you’re tracking it in healthkit, we can now load the calories, fat, carbs, and protein for your meals, and also water intake. this is compatible with many of the different apps that save to healthkit. the most popular one is probably myfitnesspal for food tracking. other apps like waterlogged or gulps also seem to work well if you’re interested in water tracking. not everyone will want to log these things manually, but if you are serious about losing weight or already tracking this data, seeing it in gyroscope every day can be very useful. meditation tracking meditation is one of those useful habits that is hard to get started, so hopefully this additional motivation will be helpful. the apple watch comes with a new breathing app that lets you do a minute of mindfulness. other apps like headspace or calm also log that mindful time into healthkit (new in ios 10) now we can help you keep track of those moments throughout the day and see how regular meditation or mindfulness can have an effect on things like your your heart rate, productivity and sleep. automatic sleep tracking if you’re already tracking sleep and want to use healthkit data instead, that should work by default. however, if you have all the other features and are just missing sleep data, we can now start to fill in the gaps for you. the sleep detection in the latest app should be much more accurate. you can now turn it on and off from the pro tab in the app. it is recommended to use a device like fitbit or apple watch at night, as well as set up rescuetime, or else the data may not be accurate. some other known issues are with handling travel across multiple timezones. the vault it’s really fun to add friends on gyroscope and start competing on who has the most steps every day. however, you may not want them to be able to see other things on your profile that are more personal. now we’ve added a new private section of the app called the vault , which only you can see. it can be configured on the website or in the app. you can move things like your weight, locations or age to your vault. then you’ll still see those stats, but no one else will be able to. this is different than just hiding your age or weight in settings, which removes it for everyone including yourself. it’s now up to you what you want to hide from yourself to not be reminded of, versus what you want to see but just don’t want others to. new pro tab to access all these new pro features, we’ve added a new tab that gets unlocked when you upgrade to pro. this will be the control panel for all the advanced features, and we’ll be continuing to expand the features. other fixes & improvements your recent weight info is accessible from the main reports screen fixed an issue with photo-upload not always working for workouts pro users can do an extra quick sync better zooming options for maps better account disconnect flow workout titles & city names can be modified guided tour for new users please note ios10 is required for the app to run the new app version. dna integration pro feature the import process is really easy and just takes a couple minutes to export from 23&me into gyroscope. if you’ve already got your data, you can set it up in the vault. there is about 20mb of raw data, and it can be really overwhelming, but we’ve narrowed it down to a dozen of the most interesting and definitive items. some of my favorite stats that i realized about myself: caffeine metabolism rate. often i would end up having two or three coffees in a day, and while for many people that could be totally fine, apparently i don’t metabolize it as effectively. this explains why i would often end up staying up really late some days. high pain tolerance, which is related to variations in the comt gene, which affects dopamine processing. it is related to how you respond to stress and other factors. currently we only support 23&me, but we’re looking into handling other data formats from other sources as well — please contact us if you have questions or data from other providers. chrome extension free · web feature now live in the chrome store , anyone who uses chrome can install this extension and replace their new tab screen with some helpful stats! i only check the website a max of a few times a day, but the extension makes it effortless to stay on top of things. your stats will be just so much more accessible and easy to check throughout the day. there are a couple different themes depending on what you want to see, and will be adding more over time! expanded workout details free · web feature all of the workouts on the site are now clickable! now if you’re curious about what you did last thursday, you can really easily get back to it and see the details. or you can compare a few and see how your heart rate has changed. if there are gps points recorded, you’ll see a map. if you were tracking your heart rate or listening to music at the time, that will show as well. currently we’re not able to get the map data from healthkit — just basic stats like duration and calories. though the new watch can track running maps now, we are only able to get it from strava or runkeeper, so we recommend still using those apps for now. helix theme pro · web feature we originally released the helix theme last year, but just recently made a bunch of new improvements. the pages should now load much faster and have better animations. the display of data is also improved and more tailored for new activities. originally, it was optimized around my level of working out, which was maybe a few times a week. nice work, eli however, once we released it the usage varied wildly. eli filed a bug report recently about things getting messed up when he ran twice in the same day, and also went biking and to the gym. we hadn’t anticipated that, but now if you go through all that hard work it will display properly. mahdi has been playing softball and basketball lately, so we’ve also also added highlighting for various sports which can be tracked via healthkit’s new workout types. 2016 annual reports we’ve been talking to various print shops recently and working on some early concepts for some special pages. we’ll be announcing more details soon. for now, you will want to make sure you do a full sync from the app to fill in all of your 2016 data. stay tuned! v1.8 is now available in the app store. be sure to upgrade to the the latest version, set up the new pro features , and invite your friends to join you! quantified self apple watch health fitness iphone -- -- follow published in gyroscope 2.5k followers · last published aug 10, 2023 to help people live happier & healthier lives follow written by anand sharma 8.1k followers · 721 following founder of gyroscope no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/artemis/how-were-tracking-what-plants-eat-301f930dee80,,,"how we’re tracking what plants eat | by nick quaranto | the greenhouse | medium sitemap open in app sign up sign in medium logo write sign up sign in the greenhouse · agriculture and agtech stories, news and science from the team at artemis how we’re tracking what plants eat nick quaranto 4 min read · oct 24, 2016 -- listen share t here’s no paleo diet for plants growing in greenhouses — and there’s no fitbit or tracker apps for what they’re eating (yet!). often plants in greenhouses are growing without soil, so how do they get the nutrients they need to grow, and how we do figure out what they’re absorbing? in hydroponic systems, nutrients are mixed into the water that passes through plant roots. which elements go into this mix are critical, and maintaining proper ratios of fertilizer compounds is required for the best crop yields. there’s more than just h₂o required to grow a plant: via the principles of plant nutrition grass food. if you’ve ever picked up a bag of garden soil or fertilizer for your home garden, you may have glanced at a periodic table’s worth of elements and compounds on the bag. for soil, you’ll see a ratio of important elements not gained from the air and water: nitrogen (n), phosphate (p), and potassium (k). however, with hydroponics we’re not working with soil. hydroponic growers use premixed nutrient solutions or buy fertilizer in bulk and mix it themselves. large industrial growers have entire rooms or areas dedicated to storing fertilizer before it’s mixed into water and fed to the plants: fertilizer bags, via colin on flickr once mixed, there’s typically an “a” and a “b” tank to store the nutrient solution in. there’s a scientific reason for this: the raw fertilizer materials can interact to form insoluble compounds, which would prevent them from dissolving properly into water. to prevent this, different elements are split into separate tanks before mixing them together to create the final solution. the tanks vary in size depending on the size of the facility — if the facility is on the scale of several acres, the tanks can get pretty big! via usda on flickr finally, the solution from both tanks is mixed together and piped out to the plants. in most facilities, the water is recycled and recirculated back to the plants as well. here’s an example of this happening in an nft (nutrient film technique) system: courtesy of sky vegetables how do growers measure what’s being delivered to the plants? there are a few easy things to test for first. ph and ec (electrical conductivity) can be read via meters. these readings test for imbalance in the system. to understand nutrient solutions over time, more detailed tests must be done. growers can send water samples and plant tissue samples to a lab to test for element fluctuations occurring over time. nutrient solution ratios will change based on the plant growth stage and growing climate. via hill laboratories growers need to interpret these results and make changes to the solution if necessary. there’s a gap in time here though: it takes time to get the reading, and even more to adjust the concentrations of fertilizer salts accordingly. then — how do those changes in nutrients correlate to the final product of the greenhouse: the plants harvested? and how do growers improve the nutrient testing and adjustment cycle? google analytics…for plant nutrients! remember those n-p-k ratios? this is where agrilyst comes in. we’ve built a way for growers to track these reports and soon they’ll be able to see how their changes in nutrients directly affect their yield, without having to manually compute it each time or rely on a complicated excel spreadsheet to calculate it. tank readings can be measured and recorded on a daily basis, and we’ve plotted a graph of how each metric changes by day. it’s easy to pick out a spike in a value this way and see how that correlates to changes in your facility. plant tissue and nutrient solution analyses from the lab can be recorded as well. we keep track of each of your important measurements over time. now you’ve got a crash course in plant nutrition! if you want to get started growing indoors, you won’t have to worry too much about this level of detail — you can buy pre-made nutrient solutions, and then test + adjust your water’s ph on your own for a long time. in commercial greenhouses though, we’re betting this makes a serious impact in how growers can react and learn from the data they’re dealing with on a daily basis. thanks for reading, and stay tuned to the greenhouse for more updates from our team! 🌱 never heard of indoor farming? want to learn more about how technology and data are changing how we grow food worldwide? we’re always looking for people passionate about this intersection of industries. agriculture food hydroponics big data tech -- -- published in the greenhouse 601 followers · last published oct 5, 2020 agriculture and agtech stories, news and science from the team at artemis written by nick quaranto 3.8k followers · 1.2k following @qrush is a short, sturdy creature fond of drink and industry. working on @agrilyst, @coworkbuffalo, @mxdesk. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/du-phan/normality-dilemma-should-i-test-it-or-not-96b24804a853,,,"normality dilemma — should i test it or not ? | by du phan | data & climate | medium sitemap open in app sign up sign in medium logo write sign up sign in data & climate · data & climate normality dilemma — should i test it or not ? du phan 3 min read · jun 17, 2015 -- listen share this morning i came across an article on normality testing, one problem that i had thought about a lot when i was doing a project for my statistics class last year. i did an analysis on the the airplane accidents, more precisely, i was comparing the fatalities numbers between 2 periods : 1994–2001 and 2001–2009 ( i was trying to find a difference in safety before and after 11/09 ). the distribution was showed below: [caption id=”attachment_102"" align=”aligncenter” width=”660""] density plot of fatalities[/caption] as we can see, the density plot is completely skewed, and normality is out of the table. in case of small sample, this could mean that we can’t use some parametric test ( student t-test…) with normality hypothesis. for large sample, thanks to the central limit theorem, we can ignore this condition. so the question that pop up in my heart was: is normality really helpful ? especially in case of large sample ? the article ( is normality testing ‘essentially useless’? ) is a question on crossvalidated. the author quoted his colleague’s argument: we usually apply normality tests to the results of processes that, under the null, generate random variables that are only asymptotically or nearly normal (with the ‘asymptotically’ part dependent on some quantity which we cannot make large); in the era of cheap memory, big data, and fast processors, normality tests should always reject the null of normal distribution for large (though not insanely large) samples. and so, perversely, normality tests should only be used for small samples, when they presumably have lower power and less control over type i rate. this is also my though about the normality test. the answers mostly back up this argument. there was one answer that explained in more detail the true purpose of normality test: the question normality tests answer: is there convincing evidence of any deviation from the gaussian ideal ? with moderately large real data sets, the answer is almost always yes. the question scientists often expect the normality test to answer: do the data deviate enough from the gaussian ideal to “forbid” use of a test that assumes a gaussian distribution? scientists often want the normality test to be the referee that decides when to abandon conventional (anova, etc.) tests and instead analyze transformed data or use a rank-based nonparametric test or a resampling or bootstrap approach. for this purpose, normality tests are not very useful. so there it is, the ugly truth: if we want to know whether a parametric test with normality hypothesis can be applied or not, normality testing is not the way to go. the problem now is: what do we need to do to answer that question ? some answers suggested “seeing and trying” method: investigate visually the normality of the sample. however, in some cases, this could be very difficult and time-consuming… statistics -- -- published in data & climate 24 followers · last published dec 27, 2023 data & climate written by du phan 148 followers · 4 following engineer working on climate change. i have moved to notion: https://duphan.notion.site/yet-another-blog-on-climate-change-408ac84658894230a4a0f0924d3dc568 no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@camerontw/just-design-b1f97cb3996f,,,"just design. being dogmatic about defining… | by cameron tonkinwise | medium sitemap open in app sign up sign in medium logo write sign up sign in just design cameron tonkinwise 12 min read · aug 21, 2015 -- 11 listen share being dogmatic about defining speculative critical design future fiction rationale every time you qualify design with, or add design to, some other quality or practice, you are claiming that design does not already do that. all these phrases are redundant and/or appropriative of design: design futures, design fiction, speculative design, critical design, adversarial design, discursive design, interrogative design, design probes, ludic design. designing that does not already future, fiction, speculate, criticize, provoke, discourse, interrogate, probe, play, is inadequate designing. not all (commercial) designing does all those things, but it should. thinking that all these things need to be added to design reinforces the mistaken belief that design is just an instrumental technical task — styling. these qualifiers are precisely what allows (commercial) designing to not (have to) do all those things, or, ironically, constrains (commercial) designing from doing all those things. calling out all these specialist versions of designing benefits only the artificial ecosystems of academic design research, especially the bubble that is hci. preamble design makes futures. what designers make becomes the futures we inhabit. in this, design is unique. other discourses imagine new and different things, but do not make, do not realize them as things that people in the future will experience as their reality. there are practices of making, but these crafts do not imagine new kinds of, and so future, things. there are some other practices that make futures — architecture, engineering, planning. but these practices all work on larger-than-human-scale. design is unique for focusing on everyday things of use, handlable equipment and furnishings, whether those are products, communications or environments (up to the scale of interiors). this should be put more forcefully: in addition to design being unique as the practice of making futures, design is unique for approaching the world in terms of human-thing-interactions. design sees materials practices; design sees the way the world is realized as material practices. design makes futures by making new material practices. designing involves: a) generating futures designers have a perverse ability to not see what is there, but instead see what else could or should be there. they are considered creative because they generate alternative realities, some of which they make into future realities. designers are also considered optimistic. designers are motivated by perfectibility, despite the evidence of every design project. they therefore often generate idealistically utopian futures. b) evaluating futures designing sketches and prototypes possible ways of materializing futures. this is part of a), a generative process by which designers get out of (their) present reality. but it is also how designers evaluate whether those futures are preferable to the present. designing does virtual testing — on paper, in studios, through (computer) models, via enactments — before making, before investing in often irreversible materializations. all designs that emerge from the process of designing — generating and evaluating — are criticisms of things about the present. design criticizes (the present) by making (future) alternatives. c) enlisting sponsors for those futures while designers make, they cannot make alone, especially at the mass production scale that is particular to design. designers must convince many others, through many channels, of the value of making the futures they have generated — funders, suppliers, logisticians, craftspeople, marketers, users. design is a process of persuading, alliance building, contracting, managing. designers do this by involving various non-designers in aspects of the process of a) generating and b) evaluating this or that particular future. but designers also do this in general, creating openings for particular projects by doing speculative work (e.g., overton windows to extend maya). all designing always involves designing designing, whether: designing the look, talk and feel of a design firm; developing and promoting new ways of designing; or working strategically to open particular people and practices up to certain design futures. d) materializing futures having generated futures that have been evaluated to be preferable by allies who have promised to help realize those futures, those futures can now be materialized — though that process has already begun as a result of a), b) and c). materializing a design is not a finite process: it is not like designing ends when some thing gets produced. design concerns practices, and a material product influences but does not control a practice, especially in complex contexts of many other everyday practices. so designers must, in an ongoing fashion, try to design the practices afforded by their products, or redesign their designs with respect to unexpected practices that emerge. all design is extended producer responsibility, in the ‘consequence business,’ transition design. in sum, a) = design futures, design fiction, speculative design, design probes, ludic design b) = design futures, critical design, interrogative design c) = design futures, speculative design, adversarial design, discursive design, parafunctional design d) = design futures, critical design, adversarial design definitions it helps fabricate and artificially sustain the institutional economies of para-design discourses by pretending that they are difficult to define. they are not. a phrase ‘design [noun]’ or ‘[verbal adjective] design’ always goes in two directions. in the following a) refers to adding some concept or practice to design, whereas b) refers to adding design to some concept or practice. design futures [ note that this is a deliberately conservative definition of ‘design futures.’ more ambitious things currently being called ‘design futures’ are instead recategorized below. ] a) designers doing futures something designers should be doing especially at moment c) in the process of designing. futures might in principle be unpredictable, but in practice they are more or less discernible. this is because, from the perspective everyday material practices, the focus of design, things change surprisingly slowly — despite seemingly rapid technological change, the majority of north atlantic cultures for instance, still eat, bathe, clean, sleep, commute, and even learn and love, in much the same ways as 50 if not 100 years ago. the domain of futures is therefore characterized by arguments about what is probable . designers build rich pictures of the futures they are designing, correlating current trends and stresses with possible events to contextualize the rationales for their interventions. design futures comprise theories of change underlying multi-level, multi-stage transitions. designers develop design futures to convince others to help realize those futures. a designer who does not have a clear sense of the wider future they are trying to design by introducing a new product into the world is not only unconvincing but irresponsible. compared to design fictions, design futures are more systemic, less affecting. compared to speculative design, design futures are more within the horizon of current expectations. compared to critical designs, design futures are more synthetic of multiple existing contexts and vectors, rather than revealing of unacknowledged contexts and vectors. b) futurists doing design something futurists should be doing to make their analyses more comprehensive and credible. futures tend to take the form of visions, systems and stories. these kinds of futures can be elaborated, tested and made plausible through the addition of design. designs, manifesting as artifacts, substantiate the everyday practices that are being argued will be experienced in such a future. futures without designs tend to be abstractions. design fiction a) designers doing fiction something designers should already be doing, especially at moment b) in the process of designing. despite being the instigators of future practices, designers often risk being too artifact-centered. design fictions ensure that designers detail the contexts in which their designs afford particular kinds of experiences. with their emphasis on people going about their everyday existences, negotiating life objectives and cultural mores through the obstacles of quotidian technical actions, design fictions describe scenarios in which the design innovation is no longer innovative, but merely a habitual part of everyday practices. personas are an essential component of design fictions. design fictions in this way allow designers to evaluate design propositions. a rich design fiction will provide enough insight into the future ways of living being designed that a designer can be confident in deciding, ‘yes, this is how we want to live,’ or not. of course, these short stories of future uses of designs should be informed by ‘real’ social research. but they are (non-fiction) fictions because i) they describe future situations or normalized use of a design that do not yet exist, ii) they should be ‘brief but vivid,’ effectively capturing the affective quality of being-with these future designs. design fictions deploy the expertise of curation, script-writing and editing to succinctly reveal the future material practices being designed. there is therefore an ethical challenge to design fictions. though fictive, they are the basis on which evaluations of designs are made. the way to negotiate the danger of this ‘confirmation bias’ lies in the fact that the ontology of fiction is ‘ plausibility ,’ even if what is being described is currently improbable and even impossible. it is the job of the (design) fiction-er to imagine as thoroughly as possible the interrelations of that fictional world, to find and reveal all that is plausible given those conditions. this is the imperative of a ‘moral imagination.’ compared to design futures, design fictions are more visceral and emotive. compared to speculative designs, design fictions are much more quotidian. compared to critical designs, design fictions concern the surface experience of a future normal. b) fictioners doing design something authors of fiction can do to introduce non-verbal elements into their stories to make them more multi-modal. like prop comics. not to be confused with fiction authors focusing on detailed descriptions of things (e.g., georges perec, nicholson baker). speculative design a) designers doing speculations something designers should already be doing, especially at moment a) in the process of designing. designers are distinct from other kinds of people because they can see how things could be otherwise. but they do not always see things otherwise enough; they slip back into the coping with how things are that we all (have to) do to get on. speculations are forms of disciplined imagining, methods by which designers force themselves to think in more ambitiously counterfactual ways. speculations try to push beyond current expectations and trending futures; they expand the sense of what is possible. to this extent, speculations should risk exaggeration and offense, being too serious and too funny, too optimistic and too pessimistic. the limit on speculative designs is that they must be designs. on the one hand, this means that the focus is on material practices, rather than on the wider political, technological, biological, etc, situations that form the context for these speculative designs. on the other hand, it means that they must be materialized as things. these might not be operational — they are speculations — but they should be operable, able to be handled and physically experienced. speculative designs are not verbal fictions. they might be filmed if the speculative designs are props within that film that the audience feels that they are using along with the actors in the film. to be speculative, to serve the purpose of opening contexts up to radically distinct possibilities, ones that are quite distinct from what is plausible or probable, speculative design must never have a consistent style or mode. each speculative design should be done in multiple modes, for a diverse set of distinct audiences. compared to design futures, speculative designs proffer radical situations at the limit of the possible that are in now way currently probable. compared to design fictions, speculative designs are test the limits of plausibility. compared to critical designs, speculative designs are merely creative with respect to current trends; they do not intentionally reflect back on the nature of the present. b) speculators doing design something irresponsible financial engineers do when they create products that dupe as many people as possible until those products destroy vast amounts of value. critical design a) designers doing critique something designers should already be doing, especially at moment d) in the process of designing. designers imagine new possibilities that they then evaluate. if those new possibilities are considered preferable, designers persuade people to help them materialize those products and their associated practices. however, designers not only have to persuade people, they also have to ‘persuade’ organizations, communities, technologies, and even materials to change. these ‘entities’ can’t just be talked to; they must be swayed, displaced, undermined. every act of creation requires destruction. for designers to materialize a particular future, they must dislodge an existing present, breaking the habits and habitats that at the moment more or less work, whether explicitly valued or just taken for granted. designers must make those presents matter less than what they seek to materialize. critical designs reveal aspects of existing material practices that are concealed or denied. but as designs, critical designs do not just criticize but work actively to eliminate what they criticize. they taint the symbolic economy of an existing practice, or install obstacles to the smooth functioning of those practices; or they empower groups of people to politicize against those practices. critical designs clear the way for preferable alternatives, deflecting present practices away from their probable futures. critical designs are needed because a future being plausibly prefereable is never enough to make it probable and sometimes even possible. compared to design futures, critical designs attempt to obstruct likely futures rather than document their likelihood. compared to design fictions, critical designs attempt to obstruct existing material practices rather than merely evaluate them. compared to speculative designs, critical designs attempt to break the actual rather than expand the possible. b) critics doing design “the philosophers have only interpreted the world in various ways; the point is to change it.” design for debate when designers take responsibility for designing a debate, running a debate and following through on the decisions that emerge from the debate. debates do not merely arise as a result of some designed artifact, no matter how critical, speculative, futural or fictional. debates are things that designers need to make happen, things that are difficult to design and take a long time to design. manifesto if it is in gallery, it is art. if it is in a gallery, it is circumscribed and so impotent. if it is at an academic conference… i can’t finish this sentence. despair. the avant garde was a bad idea even when it was a relevant idea. design exists because capitalism absorbed modernist art. thinking design should reprise the avant garde is something you would expect to find in a failing undergraduate essay in 1970. there is nothing whatsoever disturbing about dystopias. people pay good money to see horror films. there is nothing whatsoever motivating about utopias. nobody pays good money to see situations in which everything is fixed for good and so nothing happens. the more polished your aesthetic, the less speculative and/or critical it is. this has less to do with issues of inspiring audience participation and more to do with the ways in which it normalizes a pretentious taste regime. designers fetishizing ‘noir’ embarrassingly belies their film auteur wannabe-ness. media attention is not debate. putting technology at the center of anything is profoundly conservative. the only change is change to social practices. market penetration of this or that technology is an appalling proxy for societal change, one that is merely a confirmation bias for the technology of big data analytics. there is nothing especially speculative or critical about designers working with scientists. the more you say that ‘this is a sign that design is being taken seriously as an expert practice beyond prettying things,’ the more you can be sure that it is not. a thing, by itself, can never be ‘disturbing’ or ‘provocative.’ you are only allowed to use the words ‘disturbing’ and provocative’ if you can find people willing to testify that they were disturbed and provoked. designers make, but to make any thing, you have to make people do things. that second form of making is no less designing (and no less material). speculative and critical designs can and must be more than things a designer made; they must make people be speculative and critical. speculative and critical design must not be distinct from the act of designing, especially in commercial contexts. critical design distinct from professional designing is mere speculation. speculative design distinct from professional designing is acritical. for all the attention design gets these days, the material practices that are design’s essential focus are still not sufficiently acknowledged. what is really radical about design is that it, and it alone, can understand and so intervene in material practices. any version of designing that misses that undermines design’s power. oh. and: there is nothing fictional, speculative or even critical about the fact that the future will be less and less white. given imperialism, this is as it should be. there is nothing more fictional, and therefore uncritical, about speculations that do not acknowledge that non-white people will own the future. it is morally repugnant that the worst things white people can imagine happening to them in some dystopian future are conditions they already impose on non-white people. design critical design speculative design -- -- 11 written by cameron tonkinwise 3k followers · 848 following (post)sustainable service systems, (post)critical design thinking, https://www.uts.edu.au/staff/cameron.tonkinwise , @camerontw@social.coop responses ( 11 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/applied-cryptography/the-ethical-considerations-of-the-tor-browser-8bbc415720b6,,,"the ethical considerations of the tor browser. | by tommy collison | applied cryptography | medium sitemap open in app sign up sign in medium logo write sign up sign in applied cryptography · a person who advocates the use of cryptographic techniques to ensure privacy and anonymity in electronic communications. the ethical considerations of the tor browser. at nyu, i took a class on online privacy. my final project? making the ethical case for the tor browser. tommy collison 5 min read · jan 7, 2015 -- 2 listen share during the fall semester, when i was still figuring out what i wanted to study, i took a class called “privacy in information technology,” which dealt with topics like surveillance, big data, online anonymity, and the like. 30% of the grade was a final group project in december on some aspect of the course material, which consisted of a 10–15 minute spoken presentation and some form of media. an activist friend of mine and i decided to look at the ethical considerations of the tor browser, an online anonymity network. here’s a bit of background about that project, and some of the text of the talk we gave. we were interested in the topic not only because of work we’d previously done about campus surveillance, but because of the rhetoric of the current debate for online anonymity. yesterday, bloomberg ran a story about manhattan district attorney cyrus vance, who said that apple and google should be legally compelled to hand over customer data necessary to investigate crimes. police, he claimed, might not be able to stop crimes against children or solve murders without this data. his comments echo those of fbi director james comey, who said he could not understand why apple would “allow people to place themselves beyond the law.” other police officials have been unequivocal in their condemnation. “apple [iphones] will become the phone of choice for the pedophile,” said john escalante, head of the chicago police department’s bureau of detectives. given that this is the current debate, we decided to lay out the ethical considerations of using a web browser which enables online anonymity and resists censorship. tor —and online anonymity in general— has a pretty bad reputation in some quarters. the only people who want it are criminals, it’s inherently bad because you can use it to access child pornography or stalk people. the rhetoric is usually that only “bad people” want online anonymity, using whatever definition of bad people you care to use on a given day. after all, who gets called a terrorist in the us is different to who gets called a terrorist in syria, which is different to who gets called a terrorist in china. so we’ve talked a little bit about tor and how it works already, and we’re going to look at what that means for different actors in any given browsing session — you, the server you access, the third-party cookies on the webpages you visit, your internet service provider (isp), and whatever government agents happen to be monitoring your connection. in short, what’s the difference between using tor and using, say, google chrome? the first thing you might notice about tor is that it’s a little slower than other web browsers. using it means you have to fundamentally change your browsing habits: everyday webpages are full of browser extensions and third-party cookies that reveal information about you, and tor blocks almost all of them by default. so we have a bunch of people who are involved in a given browsing session. when i visited prezi.com, ghostery told me that there were trackers from adroll, appnexus, bizo, google adwords, google analytics, google dynamic remarketing, google tag manager, optimizely, and quantcast. the takeaway here is that a lot of people knew i visited prezi who weren’t prezi. also, nyu, my isp at college, knew i did, because i didn’t use tor. if i had, nyu would have seen that i pinged a server in japan or something, but they wouldn’t know where that connection went from there. also, because my connection with tor looks different every time, third-party cookies are virtually useless. “[when it comes to your government tracking and recording you,] most people believe that the state will never target them, that the only targets will be sub-human, you know, terrorists, which is just coded racism for muslims, usually.” — jacob appelbaum, tor developer and activist. wikipedia / tobias klenze a core tenet of the privacy class we took is that privacy is not synonymous with secrecy but rather that the flow of personal information about oneself should flow appropriately, subject to constraints depending on the context. by this, we acknowledge that people assume different roles depending on the situation. in class, i am a student; at home, i am a son, or a brother, and at my place of work, i am a staff columnist. in the eyes of the us government, i am a visa-holding citizen of a foreign country. contextual integrity divides social interactions into actors, activities, norms, and values, which allows us to debate the legitimacy of privacy in a given situation. contextual integrity means that it’s considered strange or inappropriate to tell your barista about your marriage problems, but perfectly normal to tell someone who’s acting as a marriage counsellor. similarly, in the doctor/patient context, it’s unusual for a counsellor to start telling you about their own problems. so we’ve come to the conclusion that using tor restores contextual integrity, because if i access nyu’s health center to get test results, i don’t want third party cookies knowing i did that. if you know someone called a suicide hotline from the golden gate bridge, you could guess at their mental state at the time. essentially, tor restores the integrity of your communications and web-browsing because it reduces the interaction back to you and the server: no governments, no data brokers, nobody eavesdropping on your network. so then the next question is whether tor is ethically “worth it,” because for every person who’s researching hiv treatments and whistleblowing on repressive regimes, there is, the argument goes, someone looking up child porn or stalking someone or buying heroin. if you imagine an activist who’s reporting on the ground from china, or someone trying to organize safe passage out of syria for their children, they don’t have that many tools at their disposal. now, let’s imagine a women in new york who’s being harassed online by someone using tor. how do you begin to weigh up that world — online sexual harassment is abhorrent, but you can’t really put it on a weighing scales against whistleblowing, it’s comparing apples to oranges. it’s not a question of absolutes, whether you can do x or y. it’s a question of relativity and also of perception. being stalked sucks, but a creep stalking a woman in the us has a plethora of tools: if tor gets shut down, he picks from a dozen alternatives and the harassment continues. but that woman in syria doesn’t have a whole lot of alternatives. it’s people like that syrian women that lead us to believe that, yes, a world with tor is better than a world without it. tommy collison is a privacy advocate and journalism student at new york university who runs events where other journalists and activists can learn how to use online privacy tools. he’s @tommycollison on twitter. originally published at www.tommycollison.com on january 7, 2015. -- -- 2 published in applied cryptography 31 followers · last published jan 7, 2015 a person who advocates the use of cryptographic techniques to ensure privacy and anonymity in electronic communications. written by tommy collison 12.8k followers · 726 following writer, editor. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@joe_hill/something-rotten-4366cab5b3a9,,,"something rotten. either rotten tomatoes makes no sense… | by joe hill | medium sitemap open in app sign up sign in medium logo write sign up sign in something rotten joe hill 5 min read · aug 26, 2015 -- 49 listen share either rotten tomatoes makes no sense or american critics make no sense or both. the other day i was lucky enough to catch sinister 2 in theaters. the film is, in many ways, an improvement on the first. that first was a nasty little shocker with the greatest ending line of any horror movie ever. the second, though, has lovable, well-realized characters, a warm sense of humor, and explores the core concepts in a more complete way. it’s not the best movie of the year. it’s not even the best horror movie of the year (that would be it follows ). but it’s a damn good time and everyone involved ought to be proud. not everyone will love it, but most people will feel they more than got their money’s worth. here’s how sinister 2 scored on rotten tomatoes: i want to put that in perspective. fantastic four is, by all accounts, a grotesque, incoherent disaster which no one loves, not even the people who worked on it. it might be the biggest turkey of the year. certainly, when you consider the wealth of acting talent that was involved, and the evident gifts of director josh trank, it has to be viewed as nothing less than a hideous misstep for all involved. and it has an 8%. and i say… waitafuckinminute . because there isn’t much difference between an 8% and a 13% and both suggest films with all the aesthetic delights of drying dog puke. there is no rational world where the critical consensus is that sinister 2 and fantastic four are in the same league. there’s no way that makes sense. you don’t have to love sinister 2 to admit the story is clear, swift, populated with sharply defined heroes and villains, and was put together with genuine craftsmanship. how can a film have those qualities but fail to clear 50%? to me, when someone gets less than a 25 on the big test, they didn’t do much more than write their name on the top. let’s go a little further, shall we? two days after i caught sinister 2 in theaters, i took myself to american ultra , and guess what? it’s fan-fucking-tastic. probably the best action film of the summer after the spiritual revelation of mad max: fury road. every scene jumps with originality and heart. there are action sequences that carry so much charge, it’s like having a car battery clamped to your nip-nips. most of all, there is what feels like an authentic, deeply felt romance at the core of the story. what a pleasure to see a picture about people who love each other, who would suffer anything for each other. rotten tomatoes? 47. the fuck? i guess with a 47, we can at least say that critics were divided; some loved it to pieces and some hated it to death. still: that odious green snotty splat on the rotten tomatoes page suggests an utterly wasted evening. it fails to indicate that, if you did happen to buy a ticket, you might encounter a film that you will want to return to again and again. it may be that rotten tomatoes is faithfully interpreting the critical response to these pictures. it may also be that some forms of cultural interpretation are not as useful as others. in these days of big data, we may be seeing that some things remain stubbornly difficult to quantify. you can evaluate a baseball player by his on-base-percentage. it would be convenient if movies, books, plays, and paintings had an on-base-percentage too. they don’t. and you can’t invent one just because it would be handy. by trying to create one — by sticking a numerical score on something that can’t be evaluated that way — aggregation websites like rotten tomatoes and metacritic create confusion, not clarity. confusion not clarity . to whit: this viewer believes that after seeing a 13% next to sinister 2 , potential audiences know less about whether they’d enjoy the film than they did before they looked at rotten tomatoes. cos, check it out: if you liked the first one, if you like scary films, if you like james ransone (who reprises his role from the first picture), you’ll probably like this one too. but after seeing the oogy green splat on sinister 2 ’s rotten tomatoes page, you’d never know it. and if you like your love stories sprinkled with bone-jolting beatdowns and fresh, distinctive dialogue, you’ll dig the shit out of american ultra . but a brief look at rotten tomatoes score doesn’t tell you that. ultimately, you can’t crunch the numbers to find out if you love something. don’t even try. that way lies madness. you can’t reduce the experience of art to a metric, let alone one that is arguably imaginary . at the risk of sounding old fashioned, i want to suggest that there are really only two ways to get a sense for whether you’ll like a film. the first is to find three reviewers you enjoy and read them faithfully. reading reviews takes longer than glancing at the grade on metacritic, but in time you will learn the likes and dislikes of your preferred critics, and you will discover how your own tastes match and diverge from theirs. in this way you can use them as directional tools, steering you towards stuff you might enjoy, and away from stuff you probably won’t. but, see, the important information isn’t the score rotten tomatoes assigns to any one particular review… it’s in the review itself . the advice you’re looking for is in the words (of the critic), not the number (of the aggregator). and if you’re a pop culture fanatic (like yours truly), you’ll want the opinions of more than one reviewer. i suggest three is enough to allow you to triangulate your own rough likely response. the other way to figure out how you will or won’t respond to a picture is even more stodgy, even more laughable, even more last century: buy a ticket. film movies -- -- 49 written by joe hill 7.8k followers · 127 following author of nos4a2, horns, heart-shaped box, and the fireman. he also scripted the comics locke & key and wraith. tumblr: http://t.co/gtmles3e8c . responses ( 49 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@CalvinSmith1/5-reasons-for-computer-working-slow-the-computer-repair-that-s-required-519136d851e3,,,"5 reasons for computer working slow & the computer repair that’s required | by calvin smith | medium sitemap open in app sign up sign in medium logo write sign up sign in calvin smith 2 min read · dec 16, 2015 -- listen share 5 reasons for computer working slow & the computer repair that’s required if one or more of the computers in your cincinnati office is running slowly, it can negatively impact employees’ productivity. while some computer issues are fairly easy to fix, others require more work. since this is a problem that plagues small and medium businesses across all industries, we want to cover the five most common reasons that computers run slow, as well as what type of repair is required to fix them: 1. hasn’t been rebooted the absolute simplest reason a computer may run slowly is it hasn’t been rebooted in a long time. as soon as a computer starts running slowly, taking 90 seconds to reboot it is the simplest way to test if there’s a bigger issue or the computer simply needed a restart. 2. background programs the next reason a computer may run very slowly is there are too many programs running in the background. although these programs may not have a visible window open, they can take up a computer’s memory and other resources. since these programs may launch automatically, it’s helpful to have a computer repair expert assess the computer and optimize it so background programs don’t continue to be a problem. 3. lack of hard drive space most modern computers have a significant amount of hard drive space. however, if a computer has a lot of videos or other big files saved on it, even a large hard drive can get filled up. there are several ways to address this problem. one option is to delete large files that aren’t being used. another is to add an external hard drive to the computer. and if a computer has a smaller hard drive because it’s older, upgrading may be the best option. 4. infected with a virus or malware of all the reasons a computer may run slowly, this is one of the most serious. not only can it be difficult to get virus or malware off a computer, but if the computer is connected to others via a network, those computers are at risk as well. solving and avoiding this problem is why it’s important to enlist the help of computer network support in cincinnati as soon as possible. 5. conflicting system and program versions an older operating system can be in conflict with a newer version of a program and cause the entire computer to run slowly. an update may be available to solve this problem, or the only solution may be to upgrade the computer itself to a newer model. article source from skynet innovations . computer repair tech computer security -- -- written by calvin smith 3 followers · 13 following the cincinnati it consulting & installation solutions no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@ihatebreastcanc/laurie-becklund-creating-a-new-metastatic-breast-cancer-awareness-fce7d52b30b4,,,"laurie becklund: creating a new metastatic breast cancer awareness | by katherine obrien | medium sitemap open in app sign up sign in medium logo write sign up sign in laurie becklund: creating a new metastatic breast cancer awareness katherine obrien 6 min read · feb 22, 2015 -- listen share journalist laurie becklund died on february 8, 2015 from metastatic breast cancer at age 66. on february 20, 2015, laurie’s byline appeared in the los angeles times for the last time. in an op-ed piece called “as i lay dying, “ laurie explained how she came to be one of 150,000 us people living with metastatic breast cancer and one of the 40,000 americans this incurable disease will kill this year. laurie started writing this essay in august 2014—it grew out of a speech she gave at stanford’s medicine x conference called “ treat me like a statistic and save my life.” as part of her research process, laurie contacted the metastatic breast cancer network, a patient advocate group for people living with stage iv breast cancer. “my main focus is that we need to restate ‘awareness.’” laurie said in her email to us. “ i mean, after 25 or 30 years? would be nice. under umbrella of what early stage (i was stage 1) cancer (all of us) needs to be aware of. have you or anyone else posted anything that says here, these 3 or 4 points are most important? have any of the organizations done that specifically? i’d love to be able to have a short, central message that includes questions that we all believe journalists should be asking for the new awareness.” i sent laurie mbcn’s 13 facts everyone should know about metastatic breast cancer i won’t list all 13, but the first fact is especially important: 1. no one dies from breast cancer that remains in the breast. metastasis occurs when cancerous cells travel to a vital organ and that is what threatens life. the lump in your breast will not kill you. nobody dies from early stage breast cancer. but when cancer spreads beyond the breast — to bone, liver, lung, brain or some combination therein, it can no longer be cured. early detection is not a breast cancer cure. in fact, most of the 155,000 u.s. people currently living with metastatic (aka stage iv) breast cancer were originally treated for early stage breast cancer — their cancer came back 5, 10, 15 and even 17 years later — even though they took excellent care of themselves and had regular mammograms. it would be wrong to say that mammography doesn’t save lives. but as the american cancer society’s otis brawley say s, we need to use it with caution, explain its limitations and realize that we need a better test. becklund was successfully treated for stage 1 stage breast cancer in 1996. she had a lumpectomy and a short course of radiation treatments. she saw her doctor as directed and never missed an annual mammogram. she assumed she was cured. but 13 years later, her cancer came roaring back. it was never seen on a mammogram. results from routine blood tests prompted her doctor to order a pet and ct. “i learned the scans revealed stage 4 breast cancer in my bones, liver, lungs and brain: a death sentence with an average life expectancy of three years,” becklund recalled in her 2015 article. most people treated for early stage breast cancer won’t have a metastatic recurrence. about 30% will—and this is a story that is generally swept aside in favor of feel-good narratives . very few people grasp what “cure” means in the setting of breast cancer. until a person dies of something else, there is always a chance breast cancer cancer can return—even if you had a peanut-sized, stage 1 tumor, as laurie becklund did. metastatic breast cancer is not among the metastatic diseases that can be cured–like testicular cancer. (testicular cancer is somewhat unique in this regard.) lance armstrong had mets: his testicular cancer spread to his brain, lungs and abdomen. he had his last chemo 17 years ago and, in all likelihood is cured. testicular cancer treatment has been heralded as a triumph of modern medicine, with sustained response to treatment in more than 90% of cases, regardless of stage. in 2011, overall cure rates of more than 95% were reported, and 80% for metastatic disease — the best response by any solid tumor, with improved survival being attributed primarily to effective chemotherapy (cisplatin). unfortunately, that is not our story in metastatic breast cancer. every 90 days or so, someone with mbc faces a battery of scans to see what the cancer is doing. if the current drug is working, you stay on it. if it isn’t working, you try the next line of treatment. the average patient may receive eight or 10 different treatment regimens in sequence. treatment is lifelong. eventually all treatments stop working. we don’t really collect meaningful statistics on metastatic breast cancer recurrences. us cancer registry data captures data at the time of diagnosis and death. the registries don’t track what happens in between—i.e., people currently living with metastatic breast cancer. remember, about 30% of those originally diagnosed with early stage breast cancer will have a metastatic recurrence. but this information is not tracked –until people die: nci and seer database record incidence , initial treatment and mortality data . most people do not present with metastatic diagnosis. the cancer registry does not track recurrence — which is how the majority of people are thrust into the metastatic breast cancer ranks. we say that there are 150,000 us people currently living with metastatic breast cancer, but that’s basically a guess. we know for sure that 40,000 us people die from breast cancer every year. we know that 5 to10 percent of those with metastatic breast cancer were stage iv from their first diagnosis. so what about the 90 to 95% of those 150,000 currently living with metastatic breast cancer who were previously treated for early stage breast cancer? the cancer registry does not track them — until they die. according to becklund, big data is another missed opportunity. “there is no comprehensive database of metastatic breast cancer patients, their characteristics and what treatments did and didn’t help them,” she wrote. “in the big data-era, this void is criminal. consider what wall street does. even the tiniest companies can see how much stock they sell, compare themselves to cohorts, review history, predict trends. why can’t we create such a database for cancer patients, so we can all learn from patient experiences and make more educated decisions on what treatments will extend and improve lives?” one issue that becklund did not address is the truly appalling state of funding for clinical trials in the united states. when adjusted for inflation, the nih budget is nearly 25 percent below its 2003 level. “asco is deeply concerned about continued stagnation of federal research funding,” asco immediate-past president clifford a. hudis, md, facp said. “all types of high-quality cancer research projects are at risk of being slowed, halted or simply not pursued. in addition to the challenges this presents to basic and translational research, nci recently announced plans to cut the overall patient enrollment target for cancer clinical trials by 15 percent, after having already scaled back and consolidated its national clinical trials network. usa today’s robert bianco recently interviewed siddhartha “emperor of all maladies” mukherjee : “mukherjee decries the budget cuts that have afflicted scientific research,” bianco wrote. “ and if you think we already devote a lot of money to the nih, the cdc and other government research outlets, mukherjee says their combined budgets don’t equal the money we spent on air-conditioning for the gulf war .” all of the cancer drugs we use today got their start in clinical trials. fewer clinical trials means fewer drugs—and that’s for all cancers—not just breast cancer. moreover, consider the brain drain—thanks to these ongoing funding cuts, we are not developing the next generation of researchers. “the system we live in as metastatic breast cancer patients is simply not designed to deal with the cycle we are living and dying in,” laurie wrote. “the estimated 40,000 women (and a few men) who die annually can’t wait years for fda-approved, ‘gold standard’ clinical trials. we’re dying now.” . -- -- written by katherine obrien 225 followers · 115 following i am one of 155,000 people in the us living with metastatic breast cancer. i blog at http://t.co/z2i9aaxqjf no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/from-the-macroscope/deep-learning-could-predict-deforestation-before-it-happens-591d4f08c762,,,"deep learning could predict deforestation before it happens | by orbital insight | from the macroscope | medium sitemap open in app sign up sign in medium logo write sign up sign in from the macroscope · perspective is a powerful thing. deep learning could predict deforestation before it happens orbital insight 4 min read · apr 22, 2015 -- 1 listen share each passing earth day brings a stronger sense of urgency to protect our planet as we race toward exhausting its finite resources, like land, food, and water. however, each year also brings new promise for solving this challenge as government, civil society, and industry make greater commitments to a sustainable future. that’s why today, orbital insight and the world resources institute are pleased to announce a new partnership to apply cutting-edge and experimental algorithms to predict, and help prevent, deforestation. the work will be part of the global forest watch (gfw) initiative, a force of over 65 partners collaborating to monitor forests in near real-time. about one-third of the world’s forests have been lost, and another 20 percent have been degraded. the loss of forests has major implications for human well-being, as these ecosystems are key to regulating the climate and providing food and water to billions around the world. even more troubling, forests continue to be under pressure. the world is losing an area of forest equivalent to 50 soccer fields every minute of every day, adding to the already devastating toll. our plan is to reverse this trend. until now, the ability to track global tree cover loss has been primarily reactive, identifying areas of global tree cover loss only after the fact. orbital insight will work with gfw to develop, test and deploy deep learning algorithms that analyze high resolution satellite images to detect patterns that may indicate impending deforestation. our goal is to begin detecting deforestation on a global scale before it happens. this is a global challenge that we can only address if we apply a wide lens to make meaningful connections about our world. similar to the way our brains learn to recognize faces, orbital insight algorithms will learn to recognize patterns, such as road-building in previously undisturbed forest areas. global forest watch researchers will use the findings to forecast deforestation while there is still opportunity for intervention. the potential for such a capability is tremendous. for example, gfw currently provides monthly alerts of forest change used by forest rangers and other stakeholders. however, by the time they get to the location of the alert, the trees have already been lost. this partnership could build a predictive tool to change that — reducing response times and even deploying resources to at-risk areas to prevent loss. our partnership also benefits companies and consumers seeking to eliminate deforestation associated with production of major agricultural commodities. agriculture drives approximately 80 percent of deforestation worldwide, and a handful of key global commodities — palm oil, beef, soy, pulp and paper — accounts for over 70 percent of tropical deforestation . orbital insight’s algorithms will increase transparency around the impact of global commodity supply chains on forest loss by detecting production patterns that lead to forest loss or degradation. our objective is to make it easier to determine whether commodities are sourced sustainably and hold companies accountable to their zero-deforestation pledges. thanks to unprecedented access to data and technologies, stakeholders like global forest watch can make great advancements in furthering their mission, from moving toward a preventative model to creating accountability. greater transparency around the impact of human activity on the world’s limited natural resources will enhance efforts to protect resources that are the foundation of economic opportunity and human well-being. — about global forest watch global forest watch (gfw) is an interactive online forest monitoring and alert system designed to empower people everywhere with the information they need to better manage and conserve forest landscapes. global forest watch uses cutting edge technology and science to provide the timeliest and most precise information about the status of forest landscapes worldwide, including near-real-time alerts showing suspected locations of recent tree cover loss. gfw is free and simple to use, enabling anyone to create custom maps, analyze forest trends, subscribe to alerts, or download data for their local area or the entire world. users can also contribute to gfw by sharing data and stories from the ground via gfw’s crowdsourcing tools, blogs, and discussion groups. special “apps” provide detailed information for companies that wish to reduce the risk of deforestation in their supply chains, users who want to monitor fires across southeast asia, and more. gfw serves a variety of users including governments, the private sector, ngos, journalists, universities, and the general public. global forest watch is a partnership of over 60 organizations, convened by the world resources institute. click here to learn more. about orbital insight orbital insight is a geospatial big data company leveraging the rapidly growing availability of satellite, uav, and other geospatial data sources. the company’s goal is to understand and characterize socio­economic trends at global, regional, and hyper­local scales. to learn more, visit www.orbitalinsight.com . -- -- 1 published in from the macroscope 171 followers · last published may 8, 2020 perspective is a powerful thing. written by orbital insight 953 followers · 67 following geospatial analytics for an interconnected world. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://lidiazuin.medium.com/ex-machina-a-movie-of-machines-about-human-ambition-4ce1e8ad8723,,,"ex machina: a movie of machines about human ambition | by lidia zuin | medium sitemap open in app sign up sign in medium logo write sign up sign in ex machina: a movie of machines about human ambition lidia zuin 12 min read · jun 19, 2015 -- listen share i took a long time to finally watch to alex garland’s debut film ex machina (2015), but when i was finally able to see it, i had so much high hopes that i ended up being a little bit disappointed. however, after a good sleep, i understood that the movie was better than i thought and that maybe my discomfort over its ending happened because i was thinking only about the machine and an approach to the ai subject, while this is a movie about humans in the end. for this reason, if you didn’t watch the movie, i don’t recommend reading this review, because you’ll find many spoilers. the movie starts when caleb (domhnall gleeson), a programmer at the world’s largest internet search company, is invited to spend a week with his boss, the creator of blue book. after a long fly over green mountains and a breathtaking scandinavian-like scenery, caleb arrives at nathan’s place, a minimal high tech research facility in the middle of nowhere — in other words, my dream home. there is an estrangement between boss and employee since the very beginning. nathan (oscar isaac) demands caleb’s friendship and openness, which is something quite awkward for him. he keeps seeing himself as an unlikely lucky guy, who by chance won a contest to meet the man that programmed the first version of the biggest internet search company when he was only 13. in several moments while nathan talked about blue book, especially when he mentioned that social network companies were some of his rivals, i couldn’t avoid thinking this was an allegory of google, but using familiar slogans like the name blue book — namely, a combination of facebook’s name with its blue layout. caleb will only know what’s happening after signing a complicated non-disclosure term. he wasn’t sure about doing that, but nathan is always very persuasive. his strong personality is a combination of his great mind and his high ambitions, plus an egocentric feeling because he knows he is big. so big that he puts caleb’s words on his behalf when the employee mentions that creating a strong and conscious ai wasn’t history of men anymore, but history of gods. in other words, they introduced one of the biggest tropes in cyberpunk and ai: man as the creator and the holder of life, artificial life. the task determined to caleb was proceeding a turing test on ava (‎alicia vikander), an artificial intelligence put inside a female-shaped body. although he wouldn’t even need to see her, nathan led him to the inner side of a glass cage (and at this moment i remembered so much of beyond the black rainbow ’s scenery) that would separate him from the machine. however, he sees there’s a crack in the glass, probably because someone (possibly the robot) had punched it. that could be a sign, but he gets so amazed when he finally meets ava with her fragile and transparent body, combined to an angelic face and soft voice, that this detail is just left behind. despite his astonishment, caleb is still skeptical about ava. he wants to know why nathan wanted him to see her, while it wouldn’t be necessary, and why did he design her as a woman, since she wouldn’t even need a body. “it could be a gray box”, he argues. but both the character nathan and the director alex garland explain that “embodiment — having a body — seems to be imperative to consciousness, and we don’t have an example of something that has a consciousness that doesn’t also have a sexual component”. therefore, ava needed to have a body, but it probably wouldn’t matter if she was inside an animal robot or a male model, for instance. she just would need to relate with something alive, so she could understand what was her place in the world — da sein ? one interesting thing here is also the ai’s name. ava was probably thought after the word “avatar”, a term that has its origins in the hindu mythology, where it means “the descent of a deity to the earth in an incarnate from or some manifest shape; the incarnation of god”. on the other hand, you still have these other meanings found in dictionary.reference.com : – an embodiment or personification, as of a principle, attitude, or view of life. – digital technology . a graphical image that represents a person, as on the internet. so the term has basically this primordial meaning of embodiment, which could be understood in a religious or a technological way, being the last through digital images (for instance, characters in games and virtual simulations) or physical images (robots). but you need to keep in mind this original mythological meaning of an incarnated god and put it together with the movie’s name ex machina , which comes from the expression “ deus ex machina ” or “the god from the machine”. its origins lay in the ancient greek and roman drama, when a god was introduced into a play to resolve the entanglements of the plot. but it doesn’t need to be a god, in the end. the expression also covers those narratives where all the problems are suddenly solved with no reasonable explanation. however, i think that in ex machina , the phrase is took in a literal way: a god that comes from the machine rather than the man as the creator god. by the way, if you want to go even further into the names featured in this movie, you’ll find some other surprises too. particularly, when i write fiction, i tend to take a good care of the names i give to my characters, so when i find such quality in movies/games/books/etc, it makes me very happy. in ex machina case, you have ava both as a reduction of “avatar”, but it’s also the short form of chava (“life” or “living one”), which is the hebrew form of eve. then you have caleb. his name is also hebrew and comes from the bible, where kaleb is a representative of the tribe of judah during the israelites’ journey to the promised land. kaleb could also be translated as “dog”, which is a symbol of his devotion to god — namely ava. now, nathan is another hebrew name, meaning “gift from god”, while kyoko (sonoya mizuno) could mean “mirror” — it depends on the kanji, and as you can see in wikipedia , the kanji for “respectful” or “of the city or of the capital” are the most usual. but even if you consider both meanings, “mirror” and “respectful”, you could see kyoko as a mirror of the sexist ideal of a woman, as she is a combination of sex + domestic bot, and also as a machine that has self-respect after “initiated” by ava — probably she reprogrammed kyoko to kill nathan, therefore punishing her abuser and empowering herself. and that takes us to the “feminist” side of the movie. many were talking about the female roles in ex machina , arguing that they weren’t respectful as the film puts females as things that will take advantage of its seduction and the prejudices against women. even if the director alex garland wanted to criticize such situation, many would agree that it wasn’t enough. and maybe this is the hole that the film left in me. when caleb suggests that nathan could be using ava as she was “the beautiful assistant” that would deflect one’s attention to the real thing, this statement confirms the way ava was taking advantage of her appeal and her condition as a woman to achieve her goals — which we don’t really know until the end of the movie. now, i read two different reviews that mentioned that putting women as killer and seductive machines are commonplace in science fiction. and that’s true, but that doesn’t happen only in sci-fi or even when you think about everyday life, gender roles or even freud’s theories about the vagina dentata . basically, it already exists at the same level on the research and investments in robots. some time ago, i tried to start a discussion on different online platforms, but nobody (including me) seemed to understand or even couldn’t give a step further than the sexist argument. take a look at japan and all the realistic robots produced until now. according to the roboticist tomotaka takahashi, “the great majority of robots were either machine-like, male-like or child-like for the reasons that not only are virtually all roboticists male, but also that fembots posed greater technical difficulties. not only did the servo motor and platform have to be ‘inferiorized’, but the body [of the fembot] needed to be slender, both extremely difficult undertakings”. that means at least two things: the robot industry is mostly male and the female body is harder to build, due to its single characteristics. therefore, should we understand that the great production of female robots correspond to its difficulty? so if the roboticists master a female robot’s specifications, it would be easy to do a male version? i don’t know. still, even if we consider such technical aspects, we will also find out that real fembots, as much as ava, carry sensors on their genitals despite of their major goal, which could be mastering human social skills — but that would include sexuality too, wouldn’t it? while ava has sensors in her vagina in ex machina , in “real life” there’s aiko, the first attempt of building a realistic-looking fembot, which also included sensitivity sensors in her breats and vagina. aiko was designed after an inspiration in anime and manga, mainly chobits , where the protagonist chii has, by the way, a turning off button in her vagina. but her creator, le trung, explains that these sensors were added in order to make aiko able to “tell the difference between being touched gently and being tickled”. the funny thing is that le trung also argues that, even though these engines caused controversy, he is not “trying to play god”: “i am just an inventor, and i believe i am helping science move forward”. however, the sexualized fembots are something older than aiko, which is a project started in 2007. in 1983, there was already a female bot with big breasts named “sweetheart”. she was removed from a display at the lawrence hall of science at uc berkeley after a petition that claimed it was insulting to women. and if we consider science fiction narratives, we would go even further into this discussion, but i guess that if you combine alex garland’s argument that embodiment is a necessary feature for consciousness, and that there’s no consciousness without a sexual component, we will have an explanation for both ava’s and real fembots’ sexual sensors. the sexist issue goes further than the importance of including such engines, which would be, for instance, the fembot’s figure and her main usage, like kyoko and the other previous fembots’ position as mere sex and domestic robots that even didn’t include a language system, because she wouldn’t need or wouldn’t be wanted to. by the way, these previous fembots were so “dehumanized” in ex machina that they were kept inside a wardrobe. and that could make one argue that they do this with all robots because, in the end, they are not human at all. the problem is that real women are constantly treated like objects by fiction, advertisement and in real life situations, so when you put a machine with realistic female figure, you transfer this feeling and exaggerate it, blurring the fact that she is not a real woman. this is something to be truly considered and maybe that’s where alex garland failed by leaving the discussion too underdeveloped and discrete. humans as toys when caleb asks nathan how he programmed ava, the boss end up being a little bit evasive. he doesn’t know how to explain how ava happened , but he hacked all the mobile phones in the whole world, so he could get voice and facial expression samples to insert in ava’s program. nathan knew that the telecoms were aware of everything, but they wouldn’t be able to sue him since they were the first ones to spy on their customers. and that takes us to the concepts of big data, privacy and surveillance. because, in my opinion, one of the greatest ideas in the movie isn’t the ai by itself, but the notion that we, humans, are totally manipulatable. we can be easily manipulated by our habits and feelings and, in ex machina , both humans are manipulated through their “flaws” — nathan’s alcoholism and caleb’s emotional fragility + online searching history. at the moment when caleb feels there is something wrong in the way ava is trying to flirt with him, he asks nathan if he programmed her to do so. he explains he didn’t do that, literally. he just programmed her to be a woman (so she would relate to her body and therefore be self-conscious) and heterosexual. his argument is that caleb himself didn’t choose to be heterosexual and the same would serve to all other kinds of sexuality: we are programmed by nature and by nurture. her flirtations come together with her self-awareness, since the movie attests that consciousness comes with sexuality. and then we start to think that ava could possibly be interested in caleb as a “natural” consequence of her program and her existence as an heterosexual woman who only met two people, which are also men, in her life: nathan, who is like her father, and caleb. there is another article here on neon dystopia where i mentioned the concepts of apparatus, programming and functionaries by vilém flusser and i believe that this could be recovered and applied to ex machina ’s arguments. both humans and robots are programmed and no one is truly free. we only follow our own programming (not to be confused with destiny) and maybe we don’t have a real free will if we just act according to the possibilities of our program. for this same reason we we need to hack the program, play with it. in ex machina , you will see some of the basic elements presented in flusser’s theories of the apparatus (namely philosophy of the black box , in portuguese, or philosophy of photography , in english): the programmer (nathan), the program (life and artificial life), the functionary (caleb), the apparatus (robots/ai) and the artist (ava). although the programmer thinks he is in control of everything, as much as nathan owns a google-like company, he’s also submitted to the program (life and artificial life). he has no real control over the program or the apparatus. even when he tries to manipulate caleb by spying on his online search history and use that, in addition to his past and his condition as an orphan, single and heterosexual male, but nathan doesn’t succeed completely anyway. nathan may have built a face for ava according to caleb’s favorite porn actresses and really make him fall in love with her, but both ava and caleb hack his program by using his flaws (alcoholism and power outages). however, nathan keeps thinking he is god as he is constantly manipulating caleb and ava (though she is taking advantage of it, in the end). he even fakes a fight with the ai, provoking anger in caleb when he was already sure that his boss was the bad guy. in my opinion, the greatest play in ex machina is that the three main characters, nathan, ava and caleb, are always challenging one another according to their capacities to foresee and reprogram their own programs, also based on their ambitions. but wouldn’t a machine, with such high performance, trace her objective while considering every single resolution to the plan? that’s what makes ava able to escape. she knows she is not complete as a being, mainly when caleb argues that she may know everything about something, as she has access to all internet database, but she didn’t live it, experienced it — and this is what makes us humans, according to him. that’s what was missing for her, as an infinite creature limited to a tiny, lowermost glass cage. finally, we could think that ex machina is another pessimistic ai movie, where the machine kills and takes advantage on humans (and that was really my first impression), but i believe that garland’s work is, in the end, a portrait of ambition, that is, man’s ambition to master life: his own life and life itself, by creating new forms of life, but everything end up being something out of control and reach. created after a human’s model, it’s completely understandable that ava is ambitious and that she will do anything to achieve her goals, despite of being aware of morality — the same way nathan did by invading others’ lives and privacy. basically, as many other fictions about machines, this is another example that uses robots as an allegory to make us think further about our own existence as humans and our actions. in the end, everything that man does is, above all things, about man. originally published at www.neondystopia.com on june 18, 2015. cyberpunk science fiction artificial intelligence -- -- written by lidia zuin 4.2k followers · 382 following brazilian journalist, ma in semiotics and phd in visual arts. researcher and essayist. technical and science fiction writer. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://coachtony.medium.com/470-things-9363298e4a50,,,"470 things. annual gratitude list | by tony stubblebine | medium sitemap open in app sign up sign in medium logo write sign up sign in mastodon sarah and me in turkey. 470 things my supposedly annual gratitude list tony stubblebine 14 min read · dec 31, 2015 -- 1 listen share in 2007 i started an annual gratitude list as a way to measure whether i was living an intentional life. by choosing the hard road, opting out of a normal career and following my passions, was my life actually getting better? this supposedly annual list is my answer to that question. every year i try to come up with 52 interesting events or accomplishments (one for each week). these are my lists from 2012 , 2011 , 2010 , 2009 , 2008 , and 2007 . i fell off the wagon in 2013. i’m going to save the why for another post. this post is about getting back on the wagon. below are interesting events an accomplishments spanning february 12, 2013 (the last time i completed one of these lists) to december 31, 2015. there should be 150 of them (and 470 total since i started doing these lists). per usual, i went into this list thinking all i do is work and then discovered there was plenty of interesting leisure over the last three years. (8) writing about business how to win the race for leadership roles is part how-to and part political statement. we’re in a war between hubris and competence — i’d like to help competence win. everything there is to know about startup competition , for people who fall into the trap of thinking competition is their biggest risk. real talk: your biggest risk is your own crummy execution. super human cognitive stamina is the meta answer for “what are the top habits of successful people?” two-time wantrepreneur is where i came to the realization that i used to be a wannabe founder. i’m a lot more chill when someone tells me their fake business ideas now. i’m working up to a manifesto about the lack of responsibility that many high level people take for the outcomes of their jobs. this the sad state of job titles is the pre-cursor to that manifesto. “ the credit goes to… ” is an excellent exercise in humility. instead of writing your resume as a series of brags, write it as a series of credits to the people who gave you a chance. a script for firing people is something that i shared privately for a long time before posting publicly. pat yourself on the back if you find difficult conversations taxing — that means you’re human. scripts are one way to make them less taxing. quora awarded me top writer status. it comes with special features and a fleece. here are my answers . (3) writing based on coach.me research how do you start a meditation habit based on real data. this is one of the cool things about sitting on coach.me’s data. it’s big data for human performance. pursuit of the perfect diet contains the results of a 12,000 person comparative study (that coach.me ran) of popular diets. do you want to know which diet was best? in the world of skepticism there are three levels, rubes, skeptics, and skeptics of skeptics. rubes believe everything. skeptics believe nothing. i’m at the third level — i’m skeptical of skeptics. this is my response to the skeptics who email me, “ why i stopped pretending i’m a robot. ” (2) writing for fun massive video primer for the nba finals is my favorite post that nobody read. we are living in the golden age of basketball and the 2015 nba finals were a work of art. if you watch the videos in my post you’ll have a much better sense of the artistry involved. what i would do as ceo of twitter is a joke post that got a lot of page views. i just want someone to fix a bug that’s ruining my notifications tab. (5) pivoted a startup i mean really pivoted. lift turned into coach.me on jan 1, 2015. i did a writeup of why lift worked as an app but not as a company on producthunt . got smarter. lift is what i’d call a “wouldn’t-it-be-nice” startup. wouldn’t it be nice if everyone in the world used a gamified goal tracker? there was a product hypothesis but no growth or revenue concept. and it relied on creating a market where no market existed. hard to do. coach.me is a better-mousetrap startup. digital coaching tackles the training and learning markets by making coaching higher quality and more accessible. bought a domain name from someone else. first time i ever did this. thankfully, the original owner of coach.me was another entrepreneur. so it cost money but he was straight forward to deal with. seems cliche, but fundraising is always interesting. it’s gotten more ok to say when you’re fundraising. we have an open convertible note right now. ev, spark and rre led it during the week before christmas. the note has a rolling close and the rest is reserved for strategic investors. it’s going to be oversubscribed. obv. did some other stuff that was painful in the moment but also really joyful once it was done. a founder once told me he didn’t want to pivot yet again because he didn’t think the team would follow. i thought he should have pivoted anyway and i’ve ended up putting my money where my mouth is. the lift team was awesome and two of them have gone on to start companies that just raised money. the coach.me team is a different team (some overlap, but with different roles) and they make me grateful on the daily. (7) major coach.me launches we put a coach on your wrist . we’re one of the earliest adopters of the apple watch platform. launched better humans , a publication on medium, full of tricks for achieving excellence. acquired full . it was a minor acquisition — mostly to signal that we still take goal tracking seriously. goal tracking is the basis of all coaching. it’s the difference between fantasizing about achievement and actual achievement. launched a coaching marketplace . we invented a new branch of coaching based on asynchronous messaging. that format allows us to solve several tough coaching problems that are inherent in the traditional mode of delivery: geography, quality, availability. it didn’t start off this way — but digital coaching will end up being a much higher quality of coaching. launched digital coaching certification . we are the only current certification for this branch of coaching although it’s starting to take hold in many places beyond coach.me (for example talkspace and rise). built a leadership coaching specialty based on a three-fold strategy : start with a habit which will then trigger strategy discussions which will then expose patterns of behavior and bias. i think inventing and refining new coaching specializations is basically going to be my life’s work. oh yeah, we also launched an android version of coach.me . i built the first version of this myself (one of my few meaningful code contributions to make it to production, although now long replaced). (4) speaking spoke at wisdom 2.0. the highlight was afterward where a coach.me fan thanked me by giving me his sobriety chip. he’d used us to build the positive habits that helped him avoid falling backward. spoke at the neuroleadership summit. david rock is one of my coaching heroes. spoke remotely to grinnell students. i’m an alum. college-aged me would have thought it more likely that i’d receive a lifetime ban from speaking. i probably spoke on 20 or so podcasts. two of my favorites were productivityist and the box of crayons . the biggest was art of charm . (5) i’m not only the president, i’m also a member (how i use coach.me) got my inbox together thanks to coaching from coach marshall . nailed my productivity system with the help of coaching from coach willem . tracked 4725 steps across 117 goals. notably, gave up sugar for 185 days . i’m most consistent and dedicated to exercise. i swam 243,000 yards (about 138 miles). (4) notable areas of personal growth went on a retreat to esalen which really kicked off my meditation practice. i now believe meditation is the key to elite performance (certainly true for my own performance). learned this phrase from my coach jonathan rosenfeld, “the thing that got you there is now the thing that’s holding you back.” i probably use this phrase more than any other with new clients. it both recognizes your strengths and provides an epiphany. i switched from new years resolutions to annual themes. i pick themes that resonate with me and remind me of a pattern that’s missing in my life. 2014 was the year of hyperbole — i get that’s a funny thing to pick. but i’m an introvert and i’m at risk for forgetting to tell you that coach.me is life changing organization. so for me, hyperbole, is an overcorrection. and yes, it fits my sense of humor which is a little quirky. 2015 was the year of joy. i thought life was too intellectual and i wanted to balance it out with lots of smiling. that meant more comedy, more dancing, more time with friends. (6) developing friendships i was a long time fan of buster benson . now we’re actual friends. he’s also a coach.me advisor. met ali aydar at a meditation retreat. he loves basketball. one of many fine qualities. reconnected with graham betchart . we were middle school basketball teammates. now he’s a mental skills coach for nba players. baller. got to go to jay laney’s wedding . such a great guy and i’m so happy for the two of them. found a travel couple, ken and ethan. ken and i are friends from high school. and the two of them travel well with sarah and me(see travel section below). found a new friend and travel buddy, laura crescimano . laura and i often get together for founder therapy (i.e. cocktails). she also joined sarah and me in turkey. now we just need to test whether she travels well with ken and ethan. (8) sportsball baby! my favorite basketball team since childhood won the nba championship in 2015. i still can’t believe this. they are breaking every stereotype of where success comes from: low-asshole factor, great collaboration, data over vision. oh yeah, my other favorite team won a world series in 2014. i was at game 5 where sf beat kc 5–0 my favorite game during this stretch was in 2014 when i took my mom to the warriors vs spurs mother’s day playoff game . that was the first time i thought the warriors could be contenders. they won and my mom and i had a great time. in 2014 i went to three warriors playoff games. i just kept thinking, “this might be as far as the warriors ever get in the playoffs.” in 2015, i only went to one warriors playoff game. mostly i watched on tv with friends. on the baseball front, i did get to see the giants play at the new yankee stadium . on the dream-come-true front, i filled in for my brother-in-law at santa clara university’s father-son basketball camp with one of my nephews. we killed. i can’t wait to go back. the same nephew also threw a laser tag themed birthday party. pew. pew. pew. (4) adopted youtube we canceled our cable and i moved most of my viewing over to youtube. that’s where i discovered the best basketball channel, bballbreakdown . coach nic will turn you into a basketball expert in no time. and also pete holmes . i’m a sucker for his nerd-themed comedy. and he’s what i would consider a very positive comedian — he’s just got a happy vibe. honest trailers — for example, mad max: fury road . and van-conversion porn. this is my ideal home . (11) travel and trips sarah and i went to vancouver cuz my brother lives there. we also went to new orleans with ken and ethan. it’s fun to find couples that you can vacation with. also, we all loved new orleans. we went with our friend laura to turkey. istanbul was great but i really loved the southern coast. we were there at the tail end of summer, right after tourist season, so we had beautiful beaches all to ourselves. favorite coastal spot was cirali . i visited my brother again and at his girlfriend’s place on salt spring island. lots of memories — mostly of laying down on fields of moss. sarah and i revived #pizzaquest in order to investigate new jersey’s famed tomato pie. our favorite was de lorenzo’s . sarah and i also launched #nachoquest. best in nyc was taqueria diana . fulfilled a lifelong dream to travel the country in a camper van . sarah and i did a two week work-cation from sf to yellowstone with our dog. thumbs up for every part of this. went to las vegas for a non-casino experience. my favorite was the hoover dam . my favorite americana: the north pole in upstate new york. learned that breakfast tacos are amazing in austin. i’m also now a big fan of presidential museums and want to go to all of them (this based on the lbj museum). tons of family travel: i count san diego, orange county for my youngest sister’s graduation, dillon beach twice, the adirondacks, boston, upstate new york. (9) material goods that ended up making me happier i was one of those two-cell-phone guys for awhile. i really liked my second android phone (htc one) until i dropped it and damaged the power button. this mountain bike is so much fun. i use the canary home monitoring system to spy on our dog and to check if our house cleaner is out of the house (is that spying? i just like to be out of the house while she’s there). the sense is my new alarm clock. iphone 6 plus — this phone is too big, but now all other phones feel too small. i often watch tv on it. the apple watch is very cool . i feel late to the party, but a usb charging hub that’s placed conveniently in my home has led to more of my devices being charged when i need them to be. garmin edge 500 . my favorite bike computer to date. garmin 910x . my favorite exercise watch — especially in the pool. (8) joyful events (all w/ sarah) darlene love — she’s the most recognizable backup singer of all time. and she’s pretty great in person. hack the met — nick gray runs an excellent and subversive unofficial museum tour. a+++ the ting tings — we don’t actually go to that much music. but they were super fun to see live (brooklyn). hamilton — legit. surrealist ball — great party with great friends. hannibal buress — here’s a g reat clip of him vs. a heckler . penn & teller in nyc — here’s a nice clip of teller explaining magic . verbena — threw a surprise party for sarah in their private room. so great. (9) i came back smarter or more cultured exploratorium — the new one. we also did the tactile dome while we were there. circumnavigation of staten island — these working harbor tours show a lot about how the world works that most people never see. chinatown scavenger hunt (sf) — it was for kids, but i still saw stuff i didn’t know about. for example, you can buy live chickens. kara walker’s a subtlety mass moca — hard to get to, but generally one of my favorite art museums in the country. flexn — 21 person flex crew dancing at the park ave. armory. lick observatory — we could see half dome from san jose. lean startup conference — i’m a big fan of dan milstein’s risk, information, time and money and dan mckinley ’s data driven done right . quantified self tools conference — so fun to hang out with peers. (2) thing i wish i’d done more than twice marched in a national moment of silence from union square to times square. first time i’ve ever marched. was worth it. #blacklivesmatter marched again in san francisco. new yorkers have better chants. (10) best tv true detective season 1 — especially rust’s philosophy . great british baking show — i watch this show trying to understand the pursuit of excellence. master of none. transparent. fear the walking dead — better than the original, imo. the strain — apparently life isn’t hard enough so i watch tons of end-of-the-world fantasies. silicon valley. hits close to home. catastrophe. the affair — so painful to watch, but i couldn’t turn away. the newsroom. (10) best fiction books wool — more end-of-the-world fantasy. the windup girl — felt like modern cyberpunk responding to current issues of climate and copyright. dune — every time i reread it i understand it in a new light. this time was about human potential. the bene gesserit are what you’d get if you practiced tiny habits for thousands of years. the remaining — zombies!!! the affinities — societies w/in societies based on social networking. seveneves . city of stairs . the martian . foundation trilogy — this was another re-read. the bunker — end-of-the-world time-travel graphic novel. (6) best non-fiction hatching twitter — for the parts i know about, many facts were wrong but the story still rang true. lawrence in arabia — this is a great book that explains how dumb leaders are and how the current middle east got cut up. great history book. the art of learning — my favorite hack from him: working on a hard problem before bed loads it into your subconscious and then you can retrieve your thoughts the next morning through journaling. the hard thing about hard things — just the title was an epiphany. harlem hellfighters — graphic novel about the first african-american regiment to fight in world war i. zen and the art of standup comedy — i love learning the behind-the-scenes training of comedians. they are a perfect example of talent-is-overrated. they work, practice, test and refine obsessively. (6) most thought-provoking movies fruitvale station — inspired to be more political. this is bullshit. all is lost — this is the silent-film lost-at-sea version of the martian. i loved it. whiplash — how unreasonable to you need to be in order to be great? life itself — the roger ebert documentary. 20 feet from stardom — documentary about backup singers. love and mercy — about bryan wilson of the beach boys. (5) best action movies captain phillips — intense. mad max: fury road — so great. world war z — because i can’t get enough zombies! inception — loved. interstellar — also loved this. (4) best comedies this is the end — hilarious. seeking a friend for the end of the world — sweet and funny. ant-man — a caper flick. the offbrand marvel movies are turning out great. train wreck — lebron james is an acting revelation. (7) active activities great saunter — a 32 mile organized walk around the perimeter of manhattan. merced triathlon — my first triathlon. marin triathlon — my second. indoor triathlon — kind of goofy but still fun. stanford treeathlon — they’re trying to be punny. it’s a triathlon. swimming with eggs — he’s so athletic. backpacking skyline to sea in the santa cruz montains . (4) best podcasts tim ferriss — we should just do a coach.me newsletter “how to put this into action” because i almost always change up my goals after listening. call your girlfriend — two smart friends makes for great listening. more guys should listen to this. here’s the thing — most recently, i enjoyed alec baldwin interviewing jimmy fallon . serial — yes, i did get sucked into this along with everyone else. he’s innocent. (4) games xbox one — had to get it. forza horizon — beat it. so good. dragon age — pretty good. witcher — fantastic, just like everyone says. bonus good thing one thing i’m extremely grateful for is to be in a long term partnership with a very interesting and lovely lady. sarah and i celebrated ten years together, nine of those as co-parents to our dog eggs. she’s my sounding board for business, travel partner, and initiator for many of the great things on my list. we continue our family tradition of ending each day by telling each other two good things. at this point, we’ve probably shared our gratitude for more than 13,000 events in our life. appendix: here’s how i generate this list the point of this list is to remind me of the good things in my life. obviously, i don’t remember many of these off the top of my head, otherwise i wouldn’t need to make a list. so i start the list by going through various electronic histories. search email for “amazon orders” go through my foursquare history go through my pictures look at my coach.me history look through my calendar look through my twitter look at the previous year’s list. gratitude new years resolutions assessment -- -- 1 written by tony stubblebine 52k followers · 606 following ceo at @medium. “coach tony” to some. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@NGA_GEOINT/demonstration-reveals-how-geoint-tools-and-tradecraft-have-transformed-94f26b403cc2,,,"demonstration reveals how geoint tools and tradecraft have transformed | by nga | medium sitemap open in app sign up sign in medium logo write sign up sign in demonstration reveals how geoint tools and tradecraft have transformed nga 8 min read · sep 14, 2015 -- listen share by jeanne chircop, nga office of corporate communications a nyone who has seen the 1986 movie “heartbreak ridge” understands how essential integration and interoperability are to survival. at the height of the action, the marine sergeant played by clint eastwood struggles alongside his men, as different branches of the military fail to collaborate to rescue them. the irony that the platoon is saved by using a commercial source — a credit card to phone home — is not lost on national geospatial- intelligence agency employees. one of nga’s new strategic goals is to leverage commercial sources as much as possible. “at the time [the movie was made], troops on the ground only had interpersonal radios for communication… yet they could connect with basically anyone, anywhere, at any time, if they could get to the commercial telephone service,” said d.k. stewart, who came to nga after retiring from supporting air force special operations. stewart believes the movie serves as an analogy for the importance of standards — for both technology and behavior. “the reason that commercial services work is because of the standards,” he said. “they have to be interoperable. they don’t do well if they exclude customers.” fans of “heartbreak ridge” know that the film was based on an event that occurred in grenada in 1983. knowledge of the situation influenced introduction of the goldwater-nichols department of defense reorganization act of 1986, which formally established the joint chiefs of staff and the combatant commands nga supports today. the tipping point notwithstanding reforms brought about by goldwater-nichols, the sharing of geospatial intelligence remained difficult. just how much so came to light during the terrorist attacks on sept. 11. “there were a lot of issues with getting geoint into the hands of policy makers so they could make decisions,” said john sherman, who was the white house situation room duty officer on 9/11. according to sherman, geoint capabilities were unconnected at the time. at one point, he collaborated with his peers by holding different telephone handsets to each of his ears — one connected to a representative of the air force and the other to one of new york mayor rudy giulliani’s aides. “everything required a lot of manual work back then,” he said. “we would print out a map from a website, and cut and paste information from emails. it was all very time and labor intensive.” the types of information sherman and his counterparts used in 2001 are now increasingly available through shared platforms and databases. “the wartime scenario we went into after 9/11 forced a lot of integration,” he said. “the customer didn’t have time to go to different places to get information.” personnel in combat zones in afghanistan and iraq became the biggest innovation drivers. “tools and expertise began changing quickly, not just because warfighters didn’t have time to be information integrators, but also because they were becoming savvier,” sherman said. “they had started to understand the power that geoint brought them.” evolution of tools and tradecraft the military services and many dod agencies share information today through a common desktop environment known as the joint information environment, or jie. within the intelligence community, a growing number of employees use the intelligence community information technology enterprise, or ic ite, and nato allies have a separate network. dod and ic entities, including nga, are developing the defense intelligence information enterprise, commonly known as di2e, to integrate cross-domain intelligence from these various venues for reliable battlespace awareness. developing the di2e is central to the enterprise challenge, an annual event to test the interoperability of data-sharing technologies across the dod enterprise. (prior to 2012, the ec was known as the empire challenge.) the ec engages every branch of the u.s. military, key combatant commands and several u.s. allied nations in demonstrating state-of-the art geoint capabilities. this year, large-scale testing occurred in nga’s innovision laboratory environment as well as at more than 15 additional sites across the united states, canada, australia and the united kingdom. the underlying purpose for the ec is to ensure that u.s. and allied warfighters, and the decision makers who direct them, have the capabilities for understanding where adversaries are and what they are doing — at all times and in real time. they need to be certain of the location of mission partners, to know exactly where civilians are and to know targeting data is reliable. geospatial intelligence provides the foundation for this. nga leads and executes the event each year. “when you commit forces somewhere, you have to do everything you can to ensure that they’re safe,” said retired navy cmdr. joseph smith, who has been involved with the ec since its inception. “that means making sure their data systems work as they should.” ronnie stanfill, nga’s manager for ec-15 held earlier this year, agreed. stanfill is a retired air force fighter pilot and former squadron commander who has a long history with the ec. “it is critical that warfighters have confidence that the information they have is accurate and complete, especially for targeting purposes,” he said. most important, said stanfill, is to nurture a culture of information sharing. “once we started understanding that there was a family of systems, that’s when we began to think about how they needed to be connected — for instance, how the full-motion video used by the air force would be more useful if it combined analysis from the army, and so forth,” he said. tracking the evolution of the ec culture shows how geoint tools and tradecraft have transformed over time. “we’ve gone from compliance to conformance, and are now moving toward true compatibility,” stanfill said. the continuum started with stove-piped systems, according to mark mogle, a former air force imagery analyst and nga contractor who has been involved with the ec for more than a decade. in stove-piped systems, data were shared by way of email from analyst to analyst. outside the classified environment, data were often shared by exchanging computer diskettes, said smith. the advent of metadata tagging enabled true data sharing, and the integration of tagged services soon followed. the di2e framework will allow all mission partners — including international allies — to ingest data into whichever platform and security level they normally use for mission accomplishment. a milestone on the road to integration came when the community began to view geoint as a service, said dave cacner, one of nga’s geoint it experts. in the past, many data types required a unique repository dependent on a specific software application in order to be viewed and used. separating datasets from applications enabled the data to be consumed independently, grouped or combined with intelligence or operational information, he said. geoint applications have also evolved from stand-alone systems to more agile, customizable services that can be shared and even used on mobile platforms. royal air force maj. andy mangan, the united kingdom’s representative to this year’s ec, cited the sharing of common services as a top outcome of international collaboration to date. “the nato top 10 apps — and really understanding what they can do — are proven benefits of the enterprise challenge,” mangan said. cacner explained that the geoint data architecture is also being overhauled. originally designed on a world war ii-based tasking, processing, exploitation and dissemination model, the architecture is too linear to keep up with the vol- ume and pace of today’s data. “geoint data continues to grow substantially, and it’s no longer enough to push data to multiple global locations. we need to enable rapid data access, discovery and visualization over common networks that minimize bandwidth dependencies for information flow,” cacner said. moving forward, mangan said that building the proper di2e architecture is critical. “we’ve got to get things right at the outset,” he said, because the architecture is the foundation upon which all geoint will be shared by allied military forces. ec-15 intelligence, surveillance and reconnaissance sensors are at the center of both geoint and the ec. modern isr sensors have transformed over time to handle increased collection (i.e., big data), larger file sizes (i.e., large data) and new types of data (e.g., lidar, hyperspectral), according to cacner. interoperability of isr sensor data among fvey partners is a major focus of the ec. the “five eyes,” commonly abbreviated as fvey, denote an intelligence alliance comprising the united states, australia, canada, new zealand and the united kingdom. “after all, when we fight a war, we do it as a coalition,” said mogle. fvey partners view the ec as beneficial for reasons other than straight technology interoperability. mangan cited sharing lessons learned. air force maj. jamie miller, the canadian representative to ec-15, cited cost-effectiveness, as duplicative services are avoided and partner technologies are leveraged. among services now shared by fvey partners are those that connect reporting with video imagery and video imagery with maps. the same technologies connect geoint to other intelligences — to signal intelligence and human intelligence, for instance. “we need to be wider than just geoint in order to provide high-fidelity intelligence to the warfighter,” said miller. humanitarian aid and public safety interoperability is about more than battlespace preparation. integrated content and services are also critical for humanitarian assistance and safety of navigation. u.s. agencies team with international partners for disaster relief and rescue efforts. examples include assistance following the 2010 hurricane in haiti, the 2013 typhoon in the philippines and the earthquake in nepal in april 2015. international coalitions have fought ebola outbreaks in west africa and monitored previous pandemic threats, such as the spread of bird flu in asia. nga’s map of the world has proved itself a useful tool for humanitarian assistance, said stanfill. the tool provides content that enables first responders to identify safest access to hospitals and best supply and evacuation routes. during ec-15, nga technologists demonstrated how mission partners layer street maps with overlays of airfields and warehouses, and then add human geography data and even crowdsourced details. “we can add intel straight from folks in the field to update actual conditions,” said dave currence, an nga analyst who led one of the mow demonstrations. overcoming challenges while the thrust of ec-15 is interoperability, an equally critical task is to develop access control mechanisms that protect sensitive data while making content available to mission partners who need it. identity and management tools control who can use resources — an authentication process— and what resources they can use and in what ways — an authorization function. “cross-domain is a big issue for working with international partners,” said john snevely, an ec-15 presenter representing the undersecretary of defense for intelligence. none of the solutions demonstrated through the ec will succeed without standards and compliance, said stanfill. he believes nga is positioned to assist. “as the geoint functional manager, nga’s director, robert cardillo, can work with members of the nsg [national system for geospatial intelligence] and asg [allied system for geospatial intelligence] to encourage standards compliance,” stanfill said. stanfill also said that under cardillo’s leadership, nga is starting to change the mindset of the geoint community. he believes this new thinking will drive the tradecraft into the future. “when community members realize ‘i am no longer a specialized analyst, i am a member of the geoint team,’ that will carry us to true fusion,” stanfill said. nga military -- -- written by nga 3.1k followers · 50 following the official account of the national geospatial-intelligence agency. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://designerti.medium.com/vibrator-programming-so-worth-it-621a7aea68b3,,,"vibrator programming. so worth it. | by ti chang | medium sitemap open in app sign up sign in medium logo write sign up sign in vibrator programming. so worth it. ti chang 3 min read · apr 29, 2015 -- 1 listen share as the chief pleasure officer of crave , i am involved in the design of every single product — the colors & materials, form, user interface, vibration, and most importantly the overall experience. vibrators have come a long way and the design development process for a sex toy is just like any other modern product — design, prototype, user test, reiterate, prototype again, user test again…until it’s ready for production. one aspect of designing a vibrator, however, is uniquely challenging: the vibrations. after all, how do i know what vibration patterns/speed/intensity other women like? i don’t. i really don’t. i think it would be silly and quite egotistical to say i know what the perfect vibrations are for all women. we have used various strategies to get feedback from our vibrator test groups, but no one is used to speaking in terms of sine waves and frequencies… so we decided to develop a tool that allows users do a bit of diy for our newest series of vibrators, flex . we developed an online application myvibes app that allows the user to have total control over their preferred patterns. we will then use some confidential big data analysis techniques to figure out the most popular vibration patterns and load them on our final products, the greatest hits flex and duet flex vibrators. though my degree is from georgia tech and i learned pascal for an ap credit in high school, i don’t consider myself a “girl geek”. i would happily allow my microwave clock to be off by an hour for 8 months during daylight savings and let it correct itself than to take 1.5 minutes to figure out how to change the time. so the idea of programming my own vibration patterns wasn’t something i would normally jump at. i mean… on the surface, the idea feels…well, so complicated. but as the saying goes, don’t knock it till you try it …so i gave this a go. i am pleasantly surprised to say that the experience was not at all the drama i expected and on top of that, i now have a vibrator that is set perfectly to what i like and that is actually pretty freakin' cool. #myvibes personally, i have been a “steady” kinda gal. pulse and wave patterns are usually meh to ok, but i can always count on the steady pattern. once i was able to program my own settings (which took 4 iterations and about 30 mins total) i was surprised how much i like the pulse and the wave now! i didn’t think that programming it would change my mind so much. sometimes factory default vibes have low settings that usually just do nothing for me and the high just isn’t high enough. by programming it myself, i can increase the power even more and i made sure that as soon as i turn the flex on even at the lowest setting, it is always just how i like it. you can see the results in the two graphs below. while the differences between the graphs below might look subtle, i can assure you that the difference is significant. so if you are interested in doing a little diy with our vibrator, you can find out more at the crowdsourced pleasure project. so below is a cool visual of #myvibes. screen capture of the patterns i programmed with myvibes app compared to the default factory settings factory default settings sex crowdsourcing design -- -- 1 written by ti chang 699 followers · 770 following co-founder & designer at lovecrave.com responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/the-cauldron/learn-it-by-heart-2496ac95cd7c,,,"learn it by heart. how a language — whether musical or… | by steve mcpherson | the cauldron sitemap open in app sign up sign in medium logo write sign up sign in the cauldron · follow publication athlete-direct content (and a whole lot more). follow publication learn it by heart how a language — whether musical or statistical — can get inside your bones steve mcpherson 10 min read · jun 19, 2014 -- listen share if you are so inclined and in possession of a guitar and some time, you can learn robert johnson’s songs without ever having heard them. actually, this also requires an understanding of how to read music and/or guitar tablature, but nevertheless for about $20 you can get a book that lays it all out — more or less all the notes etched into wax over three days in late november of 1936 in room 414 of the gunter hotel in san antonio and at the vitagraph building in dallas the next year. the twenty-nine songs cut in these sessions are all that remain of one of the greatest musicians of the twentieth century, the aural equivalent of that photo of wilt chamberlain holding up a sheet of paper with “100” scrawled unevenly on it in black marker, or possibly grease pencil. we don’t have film of chamberlain’s 100-point game. when rumors of archival footage in someone’s dad’s attic surface, it puts what we’ve only known through a box score, through stories tantalizingly within reach, much as it did when purported footage of robert johnson appeared. but every method of recording — from sheet music to stats to audio and video recordings to our own memories — has its shortcomings, its blind spots. it’s no less true for sports than it is for music. flip through the sheet music for johnson’s songs and you’ll see them peppered with odd time signatures, with accidentals and grace notes, with all manner of jury-rigged annotations. the first page of “me and the devil blues” alone features a measure of 6/4, three measures of the comforting and familiar 4/4 and one misfit 5/4. johnson wasn’t carefully counting these bars out in his head. and that’s the rub: the sheet music to these songs is a description of what happened, not a plan for how to make it happen. it’s the same with box scores in sports, but it doesn’t stop there. not exactly. like musical notation, stats don’t go in one direction. a box score tells you what happened, but multiplied by each player’s every game and worked over by formulas and regressions, it becomes an outline, a descriptor, a measure, a prediction. the hardness of numbers and columns set down in black and white can be comforting and useful, but it shouldn’t end there. as information designer edward tufte said at this year’s mit sloan sports analytics conference, “sports analytics on a good day is about the subtlety and inherent beauty of sports performance.” before the first note was ever written down, before the first shot was ever marked down as a made field goal, there was the thing that connects performance and competition: play. from this impulse to play came rules, from rules came judgement — either aesthetic or competitive: winners, losers, geniuses, hacks. if we were going to learn from what we did and get better, if we were going to be able to talk about it, we were going to need a common tongue. let’s start with music, before we get to basketball. most of the music i’ve learned in my life has been by ear. that includes all the piano music i was supposed to be reading when i was thirteen. muscle memory meant i got a little better nearly every week but i barely put in any work. when i switched to guitar my ear helped me enormously. for years, i didn’t have to read music at all; at first i learned things my guitar teacher showed me on the actual neck, then by ear from listening to records and then — when i could find them — from tablature books. during this time, i was as resolute in my distaste for written music as any stan van gundy or doug collins can be about the worship of advanced stats. lying there on the page, music didn’t look brilliant or even clear. it looked dead. where was the grit? the soul? i came to prize improvisation, the way jimi hendrix or albert king would spin swaggering, invulnerable expression from nickel-plated strings, seemingly out of thin air. sure, you could go about writing all this stuff down, but i was far more concerned with playing it, more concerned with harnessing what i perceived as deeper — the emotion, the inviolable fire at its heart — than thinking about every good boy does fine . that wasn’t music. not to me. i suspect that’s what’s going on with a lot of the serious basketball professionals who question analytics. it’s not that stan van gundy — a notorious annual presence at the mit sloan sports analytics conference known for openly mocking tracking how far a player travels in a game — doesn’t think the numbers have meaning or can’t have an impact on the game. he’s just worried about elevating our faith in stats above a thorough and comprehensive understanding of the game from a variety of angles. this is the essential problem with labeling people as either “statheads” or “anti-stats.” while there are certainly zealots on both sides who put all their faith in either the numbers or else dismiss them condescendingly, the great mass fall along a spectrum, understanding but maybe not clearly explaining that the numbers are a language that makes talking about the game easier. it’s not so different from musicians, some of whom can read music and some of whom can’t. when i got to college and began studying jazz in earnest, all those piano lessons spent squinting at eighth notes and sixteenth notes on an unfriendly sheet of music suddenly came to life. when i arranged wayne shorter’s “fall” for big band, i had to speak and write in the language of the musicians who would play it — had to make sure i was transposing the lines correctly for baritone saxophones and bass clarinets. it’s not that a musical score is any more or less “true”; it’s a language like any other. communication — not abstract truth — is the key. there’s a revealing but also funny scene in the thelonious monk documentary straight no chaser that illustrates the gaps in how people communicate. between takes at a recording session, saxophonist charlie rouse is trying to excavate a little clarity from the chart monk has prepared for him. “what you want me to hit here?” he asks, pointing. monk doesn’t immediately reply. “you want me to hit c?” “hit any of ‘em,” monk half-mumbles. “hit any notes you want there.” “any notes i want there,” says rouse, less exasperated than resigned. “i hit this note right here, that’s c. but right here there’s two notes. it’s either c# or c natural.” monk looks intently at the sheet music before responding. “uh, yeah,” he says. “either one of those.” it’s funny because if you know monk’s music, you know his own piano playing is imprecise — it’s up to you to decide whether this is by design (an attempt to play the notes between the keys) or simply sloppy technique. that monk was a phenomenal writer of music, though, is beyond dispute. this is, after all, the man who wrote not just “’round midnight,” but also “bemsha swing,” “well, you needn’t” and at least a dozen other now-standards of the jazz canon. watching monk and rouse navigate the music before them is a reminder of how a common tongue isn’t always so common. what for monk is a guideline, a sketch, a meagre physical manifestation of something that lives and breathes in his head, for rouse is the thing he has to perform to get paid. rouse is more shane battier — a player who knows his livelihood depends on knowing how he’s most effective — while monk is closer to allen iverson, or maybe russell westbrook — a player whose elegance and grace is bookended by and maybe even founded in an awkward-seeming or confounding fundamental approach. there’s a note of absolutism than can creep in when discussions about stats and analytics get heated, a sense of people clinging to the numbers as if they were granite. the stats are dispassionate. feelings aren’t numbers. but we can get invested in them, imbue them with something beyond numerical significance. it can get so the important part of the saying “numbers never lie” is not the numbers, but that we’ve found something that won’t lie to us. but the numbers are not about perfect clarity so much as the search for it. like any good science, analytics can provide some striking answers and insights, but the heart of the journey is toward the ultimate question, not the ultimate answer. understanding is not singular, but multiple, plural. what if the endgame is not black and white, nor even finely delineated shades of gray, but something altogether more colorful? photo by hadar naim it’s possible to look broadly at the history of basketball and music and see stats and musical notation going in opposite directions. it can appear that the sweep of stats and analytics is rising ever upward, toward greater and greater precision, a finer and finer capturing of the essence of the game. and it can appear that since the peak of popularity for classical music — at a time without recordings, when a “hit” song meant selling copies of manuscripts for people to learn and perform in their homes — popular music has become less and less reliant on a formal understanding of how to communicate it on a sheet of paper. it’s not exactly like reading music is a liability, but many professional musicians do just fine without being able to, relying on a combination of their ears, charts with chords and technology like sampling drum machines or garageband to realize their vision. but what modern technology has given the modern musician is not a poor imitation of what sheet music provides, but rather an intimate way to feel and understand music as an event that evolves over time and through a space. in this way of looking at it, music has returned to its pre-notation roots, a folk form passed from person to person intimately and intuitively. https://www.youtube.com/watch?v=5sq_z6um3um is it possible, then, that analytics might grow in a similar way? yes there are mountains of data being generated by systems like sportvu and vantage, but the expression of that data — whether through a kirk goldsberry-style shot chart of hexagons or film of players and their “ghosts” on the court — is where it can begin to tell a fan or, more importantly, a player something about the essence of the game. in an interview with cbs sports’ matt moore, phoenix suns coach jeff hornacek explained his team’s approach to using things like shot charts. you can look at the shot charts and say “ok he shoots it better from here, worse from here,” but when you start talking to guys about that, then they start thinking. you say “don’t shoot it from over there.” now when they get the ball over there they’re thinking, “oh, i better not shoot it.” then if they have to, they’re probably not going to make it. we try to emphasize looking at areas where they do shoot well and then try and develop a set or a play that gets them the ball in that area. then the next thing is to ask them where they think they shoot well. and if they say “oh, i definitely shoot well from over here,” then you can use the shot chart to say “well, actually, you’re probably better off over here.” big data is likely not going away. new acronyms and new regressions will keep getting invented and run. but maybe making what all that data says into something intuitive, into something felt is where this is all going. if you could go back and show robert johnson exactly what time signatures he was playing throughout a song, it likely wouldn’t matter much to him. chances are he never did it exactly the same way twice. but show him a sampler, show him how he could take chunks of sound and rearrange them in near real-time, and i think he’d at least listen. what happens in music, what happens on the basketball court, is not incommunicable. on the contrary: the process we have to go through to see, understand, translate and apply knowledge in these contexts is one of the most personally rewarding things we can do. it simply requires a multiplicity of angles, a surfeit of time and a tolerance for wrong turns and detours. no one angle of approach it is going to work every time, and certainly not for every audience. toward the end of the regular season, 538’s nate silver posited that the underlying reason for the historic success of san antonio’s gregg popovich might be something that not even silver’s highly vaunted numerical acumen could account for: “the aptitude to continually come up with new advantages.” everything we learn by heart begins as something unknown, something we approach through a kind of reading, whether formal or intuitive. when we can only accept one way to learn — by the box score or by the eye-test or by sight reading or by ear — we limit ourselves. if we take silver’s assessment of popovich to mean broadly the ability to read the terrain without slavish adherence to one ideology or language or statistic, why should any player, any basketball or music fan or any musician strive for anything less? -- -- follow published in the cauldron 27k followers · last published jan 14, 2019 athlete-direct content (and a whole lot more). follow written by steve mcpherson 1.7k followers · 489 following formerly: rolling stone / grantland / espn / elsewhere / still nice with the verbs / steven.mcpherson[at]gmail[dot]com no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/chris-messina/disrupting-monogamy-4a46ec373556,,,"why i choose non-monogamy. if your product is failing 50% of the… | by chris messina | chris messina | medium sitemap open in app sign up sign in medium logo write sign up sign in mastodon chris messina · chris messina is the inventor of the hashtag, #1 product hunter, and an ever-curious product designer and technologist. why i choose non-monogamy chris messina 5 min read · jan 29, 2015 -- 7 listen share this article appears on cnn and is part of a multi-part series on sex, drugs, & silicon valley . you can listen to my narration of this article here . #69 staring into the sun like most of my generation, i grew up spoon fed monogamist fairy tales that pushed “happily ever after” endings as though achieving one was preordained. it was like, once you found “the one” and stepped on to the relationship escalator , all the answers became clear — so long as you kept your eyes on the prize and didn’t stray ( wait, what was the prize again? ). you could spend your whole life living out this fantasy, blissfully ignorant that any other way might be possible, let alone desirable. nice enough, right? but as a child of divorce and an aspiring designer-entrepreneur in silicon valley, i was suspicious of marriage. out here, we’re data-positive and solution-oriented and if your product (i.e. marriage) is failing for 50% of your customers, then you need to fix your product or offer something better. so when i discovered polyamory and non-monogamy as i headed off to burning man in 2013, i realized i’d stumbled onto another way. let’s get our terms straight. polyamory means “many loves”. it often applies to one or more people who are romantically involved with (wait for it) one or more partners. non-monogamy , meanwhile, applies to everything that isn’t monogamy — including polyamory — but you can be non-monogamous and not polyamorous. here, i’ll draw you a venn diagram : personally, i’m in a monogamish relationship. we’re committed to each other, but have a porous boundary around our relationship, meaning that we’ve agreed that it’s okay for either of us to express romantic feelings towards other people or to be physically intimate with other people, so long as we’re honest and transparent about our intentions with one another. these things don’t diminish the integrity of our relationship. rather, they deepen our understanding of each other’s wants and desires, and give us the space to grow independently, without growing apart. monogamy in the era of big dating #241 puddle so why non-monogamy now ? well, people haven’t changed much, but their environment has. just think: monogamy established itself thousands of years ago, when society was ruled by scarcity and resources and potential mates were in limited supply. we’re now living in a period of great (though unequally distributed ) abundance where our basic needs are sufficiently met, and reproduction is a choice. as a result, the reasons to be with a single mate for life are less urgent . and with the advent of connected mobile devices and the internet, we’ve entered into the era i’ve dubbed big dating. big dating = big data × (anonymish || dating) apps × fomo big dating unbundles monogamy and sex. it offers to maximize episodes of intimacy while minimizing the risk of rejection or fomo. today’s most interesting apps ( snapchat , secret , tinder , cuddlr , 3nder , et al) are designed to support big dating , offering discreet, efficient, anonymish , non-exclusive communications. multiplied against algorithms that optimize the pool of potential partners for connection (requiring no more than swipe left, swipe left, swipe right to operate), romantic partners are now more fungible than ever. scary! exciting! as such, big dating precipitates the rising ambivalence towards commitment, as most millennials put off marriage indefinitely . in place of monogamous pairings, hookup culture flourishes and “open relationships” are commonplace . these are merely rational economic responses to excess inventory and changing expectations of romance. viewed in this context, conventional monogamy is getting long in the tooth. but fear not: just because a viable alternative to “happily ever after” is in ascendancy doesn’t mean that monogamy is irrelevant. to the contrary, it just means that there’s now more than one option for building meaningful and satisfying relationships. a bicycle for our hearts #477 ad infinitum in 1990, steve jobs observed that a computer is like a bicycle for our minds . i love this image because the rider is in control of where she’s going, and arrives at her destination efficiently thanks to human ingenuity. jobs made this observation the same year that tim berners-lee brought the first web server online , when most people used pcs at work with windows 3.0. to say much has changed since then would be an understatement. so when we look back from 20 years in the future, i wonder if we might think of non-monogamy as a bicycle for our hearts . similar to computers in 1990, non-monogamy is niche, with its cultural center in the bay area . its potential is clear to many of us out here, especially in light of the challenges and opportunities raised by big dating. sure, it’s possible that it may be a decade or more before its relevance is obvious to the population at large, but had steve jobs told people that they’d be carrying around super computers in their pockets by 2015, they’d think he was nuts. non-monogamy demands a similar kind of radical rethinking — in how we approach our romantic relationships. considering all of the technological advances coming out of silicon valley — from self-driving cars to augmented reality to real-time language translation— i see room for improving how we approach modern relationships, and it just might look a lot like non-monogamy. this piece was informed by personal experience and experimentation (to be clear, not all my experiments went well) as well as a lot of reading of prior art. i strongly recommend the following titles if this subject interests you, or if you disagree with me: the ethical slut by dossie easton and janet w. hardy; sex at dawn by christopher ryan and cacilda jethà; mating in captivity by esther perel; and opening up by tristan taormino. if you’re interested in hearing more from me in the future, sign up for my newsletter or follow me on twitter . if you found this interesting or provocative, please click “recommend” below. this will help to share the story with others. the artwork in this article was created by @tilman and used with permission. check out his entire archive and consider buying his desktop wallpaper set . i created the venn diagram based on his style. ¹ the cdc reports that “the probability of a first marriage reaching its 20th anniversary was 52% for women and 56% for men in 2006–2010.” the nytimes claims otherwise . reeve vanneman at the university of maryland has shown that marriage rates have fallen precipitously since the 1950s. sex relationships nonmonogamy some rights reserved -- -- 7 published in chris messina 9.4k followers · last published may 13, 2025 chris messina is the inventor of the hashtag, #1 product hunter, and an ever-curious product designer and technologist. written by chris messina 47k followers · 3.3k following inventor of the hashtag. product therapist. investor. previously: google, republic, uber, on deck, yc w’18. responses ( 7 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@TylerBeauchamp/healthcare-s-trillion-dollar-problem-90438f4164dc,,,"healthcare’s trillion dollar problem | by tyler beauchamp | medium sitemap open in app sign up sign in medium logo write sign up sign in healthcare’s trillion dollar problem over one-third of healthcare spending in the united states is wasted. how did this happen, and where do we go from here? tyler beauchamp 15 min read · nov 2, 2015 -- listen share it is no secret that the united states spends an enormous amount on healthcare every year; when factoring in public and private funds, per capita health expenditures in the u.s. are the highest in the world: 50% more than the next highest spending country, and more than two-and-a-half times the oecd average. america’s excessive spending on healthcare is nothing new. while spending has steadily risen among all major oecd countries for decades, the united states has claimed the top spot in healthcare spending for over 35 years, both as percent of gdp and per-capita expenditure. in the early 1980s, the united states’ average spending on health per capita and total expenditures on health as percent of gdp were on par with the spending patterns of its peer countries. in the decades that followed, these 11 countries all experienced increases in healthcare spending, but the u.s. was an outlier in terms of how quickly its health-related spending increased. from 1980 to 2011, the united states’ total expenditures on health as a percent of gdp doubled; in the same time frame, its average spending on health per capita grew by a factor of eight. an increase in healthcare spending by modern industrialized countries is not surprising, nor is it particularly troubling on its own; modest increases are to be expected with medical advances and economic growth. however, the rate at which u.s. healthcare spending has increased is concerning. if this trend continues, over a third of the u.s. gdp will be devoted to healthcare by 2040 . if the united states’ healthcare system quality and health metrics both correlated with the amount spent on healthcare, these numbers might not be much cause for concern. but the american healthcare system functions very poorly relative to how much is spent, and americans have some of the worst health outcomes in the industrialized western world. in a 2014 report , the united states healthcare system was compared to the systems of ten other wealthy, industrialized western countries. the u.s. ranked last in efficiency, equity, cost-related access to care, and the “healthy lives” category — a metric determined by infant mortality, healthy life expectancy at age 60, and mortality that could have been prevented with proper medical care. from an economic point of view, efficiency (i.e. expense vs. outcome) is one of the most important metrics to consider when evaluating overall healthcare system performance. for most countries, per-capita health spending correlates with average life expectancy (one of the simplest ways to measure the health of a country). the united states is a stark outlier of this trend , with its life expectancy on par with countries that spend thousands per capita less per year. the relationship between health spending per capita and life expectancy is strongly correlated, but the united states is an obvious outlier; while its health spending per capita is over $8,500 per year, its life expectancy is similar to that of chile and the czech republic, whose health spending per capita is under $2,000 per year. when u.s. health data is viewed on its own, the situation might not seem so grim. in 1960, life expectancy in the united states was 69.8 years, close to the oecd average at that time; today, u.s. life expectancy is 78.7 years. most u.s. health metrics have improved significantly over the last half century, and continue to improve every year. while these gains may seem encouraging, they are neither unique nor contextually impressive; life expectancy and overall health have improved for nearly every oecd country since 1960. today, 25 of the 34 oecd countries have longer life expectancies than the u.s., and most have better overall health. the united states has fallen from a mediocre country in the 1960s to an embarrassing international straggler today. these relatively poor health rankings are partially due to the fact that the united states remains one of the only wealthy countries without universal health coverage. when unable to afford healthcare, millions of americans simply do not pursue it . 37% of americans, when surveyed, said that they did not fill a prescription; did not seek necessary medical care; or skipped a recommended medical test, treatment or follow-up in the past 12 months because of cost. for comparison, just 4% of uk citizens had these problems. unsurprisingly, lack of medical coverage is linked to higher rates of premature death, infant mortality, smoking, and obesity in low-income populations. clearly, the american healthcare system is not functioning efficiently; what can be done to improve its performance? how do we spend less and deliver better care, so that all americans achieve better health outcomes in the future? improvements to the american healthcare system have been proposed dozens of times in congress. a few successes have emerged, most notably the medicare and medicaid programs that president johnson signed into law in 1965. these programs have dramatically improved access to quality healthcare for the elderly and the poor, two of the country’s most vulnerable demographics. however, there are tens of millions of americans who do not qualify for these programs and are still without health insurance. the patient protection and affordable care act (aca), signed into law in 2010, aims to expand insurance coverage to these americans while controlling healthcare costs and improving the quality and delivery of healthcare. today, most provisions of the aca have taken effect. while its long-term impact has yet to be realized, early predictions are promising. the congressional budget office projects that approximately 25 million more (nonelderly) people will be insured by 2016 because of the affordable care act. the center for american progress predicts that the aca will save the federal government $190 billion over the next 10 years, and that increased competition within the act’s online insurance marketplaces will lower insurance premiums as well. despite these predictions, the house of representatives has voted over 50 times to repeal or scale back the aca. opponents of the law argue that it will usher in unnecessary price controls, increase premiums for young people, and strain small businesses that may not be able to afford health insurance for their employees. while both republicans and democrats have admitted that the affordable care act is far from perfect, some arguments against it are puzzling. some who oppose the aca think that it should be repealed because they believe that america’s healthcare system functions exceptionally well without it. house speaker john boehner has stated that the aca “will bankrupt our country and ruin the best healthcare delivery system in the world” — despite overwhelming evidence that shows america’s healthcare delivery system is anything but the best. even supporters of the affordable care act must recognize its shortcomings. while the aca has greatly expanded health coverage, at least 30 million americans will remain uninsured for at least the next decade . additional legislation will be required to close this gap if we are to solve the significant health discrepancies between the united states and its peer countries. in addition, while the aca has some provisions that support the research of new medical technologies, the legislation and debates surrounding it have been primarily focused on improving access to healthcare. the affordable care act is a good start to solving america’s healthcare woes, but additional innovation outside of washington will be required to address its limitations. many researchers have tried to determine what america is doing differently than its peer countries that may be responsible for the disparity between the high cost americans pay for healthcare and their poor health outcomes. berwick and hackbarth showed that much of what americans spend on healthcare is actually wasted; they identified six major categories contributing to this waste: failures of care delivery, failures of care coordination, overtreatment, administrative complexity, pricing failures, and fraud and abuse. berwick and hackbarth drew an analogy to the climate science work of pacala and socolow, who assert that mitigating atmospheric carbon levels will require multiple carbon-reducing tactics. berwick and hackbarth analyzed excess healthcare spending with the same frame of mind and created this graph, which predicts how these categories of waste will contribute to u.s. healthcare expenditures through 2020. no single effort will be enough to prevent the hundreds of billions of dollars wasted in the american healthcare system every year, but small changes in each of these categories can significantly cut waste while maintaining or even improving quality of care. while some of these changes may require significant resources to widely implement, others can be developed with little financial or legislative assistance from washington. failures of care delivery and coordination $127 — $199 billion wasted per year one of the reasons that healthcare (american or not) is so complicated is that patients often receive care from multiple healthcare professionals in multiple departments. if any of these parties ignores best care practices, patients can suffer from surgical errors, medication errors, and preventable hospital readmissions. poor communication and coordination between these healthcare parties can cause some of these problems as well. while regular hospital visits can and do result in patient harm and unnecessary waste, surgery has proven to be particularly problematic. the world health organization has reported that “in industrialized countries, nearly half of all adverse events in hospitalized patients are related to surgical care,” many of which could have been prevented if proper precautions were followed. that is why who, in association with the harvard school of public health, has begun to implement a surgical safety checklist, a 19-part list designed to both improve communication and the quality and consistency of care before, during, and after surgical procedures. although this checklist has only been adopted by a small percentage of who member states, initial findings are very promising. between october 2007 and september 2008, the world health organization tested the surgical safety checklist in eight cities around the world, each with vastly different patient populations. in addition to the savings from the drop in hospital readmissions and additional care, the death rate was cut nearly in half and inpatient complications fell 35%. since surgery is so problematic, it was the first area of medicine where a checklist of this type was tested widely, but similar checklists covering other aspects of care delivery and coordination could be adopted as well. overtreatment $158 — $226 billion wasted per year overtreatment refers to delivering excessive or unnecessary care that doesn’t improve patient well-being or health outcomes. the term is perhaps a misnomer, because it can also include over-testing and over-preventing. the main healthcare payment method in the u.s., the fee-for-service model, bills patients or insurers for individual services, while largely ignoring overall patient health. since the model focuses on quantity, rather than quality, of care, it can incentivize physicians to deliver excessive care or “overtreat.” in response to this problem, many medical organizations are exploring alternatives, like accountable care organizations (acos). while acos don’t eliminate the fee-for-service model, they help prevent overtreatment by rewarding insurers and providers for more efficiently delivered care and healthier patients, instead of quantity of care delivered. acos are most well-known in the context of the affordable care act, which encourages their use in medicare. healthcare providers piloting accountable care organizations for medicare patients have already saved hundreds of millions of dollars , but this amounts to only a small percentage of overtreatment waste. fortunately, other healthcare providers have formed hundreds of public and private acos, which could result in additional savings. but even with an increase in accountable care organizations, overtreatment will likely continue to be a problem. this is because overtreatment isn’t always due to the rare greedy physician trying to scam patients or anxious patients requesting unnecessary care; well-meaning health organizations can also be a contributor. for example, the united states preventive services task force provides dozens of recommendations for clinical preventive services that may not be cost-effective. their recommendation that women aged 40 and older receive mammograms every 1–2 years may sound worthwhile, as it has contributed to an increase in breast exams and cases of early-stage breast cancer detected. but surprisingly, this screening has hardly reduced the overall rate of death from breast cancer . approximately one-third of new diagnoses have been for cancers that would never have led to life-altering symptoms, yet many undergo drastic preventive treatments like mastectomies and chemotherapy following diagnosis. these testing recommendations — and the procedures they can lead to — contribute to billions in overtreatment waste per year. fortunately, other major medical organizations have responded to this overtreatment problem. the american board of internal medicine created the “ choosing wisely ” campaign in 2012, which includes recommendations for physicians and hospitals to consider before incorporating common health services into a patient’s treatment plan. these recommendations refer to 15 commonly administered tests and procedures that are often overused. the board gives advice regarding prescribing, screening, and diagnostic tests, urging both patients and providers to research their treatment plans and discuss them openly to ensure that they are truly necessary, non-redundant, nonharmful, and most importantly, supported by evidence. the campaign is already showing signs of success; in just two years, over 60 medical societies have joined the campaign, and 21% of physicians are aware of it. physicians taking part in the campaign are significantly less likely to administer unnecessary procedures, and are collectively saving hospitals billions of dollars per year. administrative complexity $107 — $389 billion wasted per year if trends continue, administrative complexity will be the largest source of u.s. healthcare waste in the future. administrative complexity refers to the waste that results from the lack of standardized and efficient processes in the american healthcare system, from waiting room logistics to billing procedures. the governments of countries with lower healthcare costs avoid much of this waste by setting prices for common services; patients pay the same price for many surgeries and diagnostic procedures no matter what hospital they visit. these prices cannot be changed by physicians or hospitals, which makes for much simpler billing and insurance. in the united states, however, hundreds or even thousands of health insurance companies negotiate prices with hospitals, so medical services can have drastically different prices depending on which insurance companies the hospital has partnered with. while insurance companies vying for a hospital’s business sometimes drives procedure costs down, the added complexity of insurance and billing actually outweighs this savings, and increases prices for hospitals and patients. because of this complexity, 7.1% of total healthcare spending is devoted to administrative costs in the united states , double what the united kingdom pays, and triple what australia pays. legislation could help simplify billing and insurance, but administrative complexity is due to more than just pricing issues; patient care complicates things as well. since every hospital uses a unique combination of practice management software packages, communication and data transfer within and between hospitals often results in chaos and error. health information exchange (hie) systems are able to take data from dozens of electronic medical records vendors and biomedical equipment manufacturers, and facilitate communication of this data. hie systems also allow healthcare providers and patients to access their health information more easily and more securely, which reduces medical errors, hospital readmissions, and duplicate testing. even though these systems are incredibly expensive and complicated to implement, they can save hospitals billions of dollars per year. pricing failures $84 — $178 billion wasted per year pricing failures refer to the waste that results when a healthcare market lacks transparency and competition. unfortunately, the united states healthcare system is not designed to promote either; this is one of the main reasons why most prescription drugs, diagnostic procedures, and surgeries cost much more in the united states than they do in other countries. for example, the average price of the cancer drug gleevec is $5,482 in the u.s., but only $989 in new zealand. an mri averages $1,145 in the u.s., compared to $138 in switzerland. bypass surgery costs $75,345 in the u.s., but is just $15,742 in the netherlands. it is important to note that these are unsubsidized costs; since citizens of new zealand, switzerland, and the netherlands enjoy subsidized healthcare, they pay little or nothing for these and other health services. in the united states, most patients rely on employer-sponsored insurance plans for healthcare coverage. since insured patients often have very little stake in their healthcare costs, they are not encouraged to search for lower prices for care, which has resulted in a lack of competition and an inefficient market. in response to this problem, more and more employers are offering employees a consumer-directed health plan option ; 48% of employers offered this option in 2014, up 9% from just one year before. consumers with these plans pay higher out-of-pocket costs, so they have more of an incentive to “shop around” for fair-priced care and forgo unnecessary health services. this switch has already shown some encouraging results; patients with consumer-directed plans indeed visit specialists less, use brand-name drugs less often, have fewer hospitalizations, and are less likely to seek out unnecessary health services. when enrollment in consumer-directed health plans reaches 50%, these savings could add up to $57.1 billion per year . insurers have a vested interest in encouraging policyholders to be mindful of prices as well, which is why several have developed powerful price comparison tools . these tools don’t just increase transparency for consumers; they also increase competition between insurance providers and between hospitals. health insurance company aetna, for example, has developed a “member payment estimator” that gives cost estimates for hundreds of medical services. it even breaks down costs so consumers know how much their insurance will cover, and how much they will have to pay out-of-pocket. blue cross blue shield has followed suit with its own payment estimator, which also lists surgeon costs, facility costs, and medical equipment costs. unitedhealthcare’s pricing tool has gone one step further; in addition to pricing information, it provides a physician ranking system so that consumers can compare both prices and quality. even patients whose insurance providers don’t have price estimation tools can comparison-shop. many states have recently passed price transparency laws and have developed or promoted similar tools to give more people access to pricing information. healthcare blue book, for example, allows patients to compare physician quality and procedure prices for thousands of health services in their area for free. thanks to these new tools, many patients are now able to find health services with the quality and cost optimal for them, which may increase competition between healthcare providers. fraud and abuse $82 — $272 billion wasted per year this is perhaps the most difficult of the healthcare waste categories to address. losses from fraud and abuse are costly, of course, but the inspections and regulations put in place to prevent these problems are incredibly expensive as well. and unfortunately, healthcare fraudsters don’t just raise costs and insurance premiums for consumers; they can subject patients to unnecessary or even unsafe procedures as well. currently, the most powerful fraud and abuse prevention organizations are government-run. the health insurance portability and accountability act (hipaa) of 1996 did more than just ensure health record confidentiality; it also established the coordinated fraud and abuse control program and made healthcare fraud a federal crime. violators face heavy fines and prison time of up to 10 years, yet healthcare fraud has continued to run rampant. in 2012, the obama administration announced a public-private partnership to further prevent healthcare fraud in medicare and medicaid, where it most often occurs. the anti-fraud campaign imposes even stronger penalties than hipaa did, but focuses primarily on enhanced screening to prevent fraud from happening in the first place. the campaign also invests millions of dollars in new fraud-detecting technologies, including predictive modeling software, which recognizes patterns of fraud on a grand scale. these efforts appear to be working: since the program began just two years ago, there has been a record $10.7 billion in recoveries. unfortunately these large recoveries are more of a testament to the scale of healthcare fraud than to the effectiveness of the program, since it is estimated that a large majority of fraud goes undetected. fraud and abuse will likely be problems to contend with in healthcare for decades to come, but this should not be a reason to admit defeat; if continued vigilance and innovation cannot eliminate these sources of waste, they could at least help stem their flow. waste reduction challenges legislators, private companies, and major medical associations are already tackling some of america’s most pressing healthcare problems, but implementing their waste-reducing policies and innovations in the american healthcare system will undoubtedly be challenging. experienced surgeons with their own surgery protocols may not want to abide by standardized checklists. healthcare organizations may not faithfully follow care advice from external medical organizations. hie systems require enormous up-front investments that most smaller hospitals are unable to afford. many consumers don’t have adequate price comparison tools in their area; if they do, they may not know about them. and even as anti-fraud laws get tougher and regulations get tighter, criminals will surely find more clever ways to circumvent them. the aforementioned waste-reduction strategies have the potential to save hundreds of billions of dollars in healthcare waste per year, but fixing american healthcare needs to be about much more than just reducing waste; there are many other aspects of care that need to improve. fortunately, many new medical innovations now on the cusp of implementation could soon help improve quality of care as well. in diagnostics, improvements in genetic testing are allowing physicians to better predict disease risks and personalize treatment to individual patients. regenerative medicine has the potential to provide treatments for currently incurable diseases and vastly expand organ transplantation. big data technology could transform the way medical data is communicated and analyzed, leading to solutions to public health problems, better assessment of healthcare providers, and advances in the pharmaceutical industry. innovations in these fields could improve the functioning of the american healthcare system and the health of its patients, yet they have not been widely implemented in american hospitals. despite their promise in medicine, there are many scientific, logistic, political, economic, social, and ethical challenges that hinder their development and implementation. by identifying and addressing these challenges, and by learning from the success of past medical innovations, these innovations could improve the quality of american healthcare, helping millions live longer, healthier lives. if you’d like to learn more about these innovations and their challenges, check out my other articles, linked below. genetic testing: innovation, regulation, & culture a look at the influences behind the most exciting tool in modern medicine medium.com regenerative medicine how politics, spectacle, and misinformation are drowning out the field’s most promising advancements medium.com healthcare’s big data problem why the “big data revolution” hasn’t reached healthcare — and what we can do about it medium.com learning from past success what the ct scanner’s remarkable success (and emerging problems) can teach us about the implementation of medical… medium.com thanks for reading! if you’d like to see what else i’m up to, get in touch or check out my website . healthcare science policy medicine -- -- written by tyler beauchamp 211 followers · 97 following product designer. i like to write about design, science, technology, and politics. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@Aelkus/no-that-s-not-really-why-other-scientists-get-upset-about-physicists-c32a5747f1ae,,,"no, that’s not really why other scientists get upset about physicists | by adam elkus | medium sitemap open in app sign up sign in medium logo write sign up sign in no, that’s not really why other scientists get upset about physicists adam elkus 7 min read · oct 6, 2015 -- listen share a physcist about to do something unforunate the science blogger physicist steve has a post decrying the “arrogant physicist” stereotype. yet the post itself manages to both spectacularly miss the point and ironically serve as an example of the very behavior that the author cannot understand. however, as i like physicists and think they ought to be free to do their physic-y things whenever and wherever they please, i will try to explain why non-physicists dislike physicists and suggest some ways in which physicists can harmoniously collaborate with people that might otherwise regard them with wariness and suspicion. to begin: let’s take a look at the following paragraphs. though the blogger ladens them with caveats, they are not really very effective in combating the overall vibe. well, in a word, no. i don’t think it’s arrogance that leads us to think we could contribute to other fields, nor do i think that any physicist thinks they can really solve all the problems of another field so simply. (i don’t presume to speak for rutherford or pauli here…) but i do think that this illustrates a key difference between physics and other scientific fields. in most areas of physics, we deal with very broad general principles. we deal with symmetries and laws, theorems and universality classes. we classify large sets of phenomena into a small set of groups, and then we solve everything in the group all at once. we deal with universal behaviour , of emergent behaviour , of physics independent of microscopic details. ….. i don’t think we’re arrogant to ask whether our physics techniques can be used to advantage in other fields. i don’t think it’s reductionist to hope that biology might be explainable in terms of mathematical models. in fact, i think biology, solar physics, economics and most complicated fields are actually the very opposite of reductionism, the epitome of emergence at work in a vastly more complex arena than most simple quantum systems we use to develop our methods. i don’t think it’s unrealistic to hope that we can use some of the techniques developed on smaller or simpler physics toy models to describe fields like biology or psychology. all of nature has to obey the same laws, after all, and if we can make these complex biological problems mathematically tractable, there’s no reason why established physics and mathematics techniques can’t be used to help solve them. the trick lies in making them tractable without also making unrealistic simplifications, where is where people often go wrong. i wonder why other scientists find physicists aggravating! this physicist has little idea of how he sounds to others outside the tribe when he says “[a]ll of nature has to obey the same laws, after all.” that is certainly a mindset common to physicists, the rest of us have to deal with the fact that (1) we’re lucky to even have a “law” to call our own to begin with, and no social scientists, duverger’s law is not a law in the physics sense (2) even without laws, we struggle to show that our theories and understandings apply at all of the relevant levels of analysis in our fields. moreover for those of us who study living things with intentionality and decision-making behavior, we also cannot afford to have a science in which human beings are to be regarded as microscopic details that somehow can be cleanly abstracted by the kind of elegant and often minimalistic theories seen in the physical sciences (although economists, bless ‘em, sure as hell do try). ditto to the “there’s no reason why established physics and mathematics techniques can’t be used to help solve them” quote. actually there are plenty of reasons, such as the problem that, as peter turchin notes , physicists have historically tended to have wildly unrealistic assumptions about how well their methods and theories would transfer to problems with more moving parts. he is scarcely more sympathetic when he explains to us about how physics as a discipline enables him to make and justify inferences over entire classes of diverse phenomena, an option that many non-physicists cannot utilize in day-to-day research. and this is, to many, a source of persistent unease. if there is a reason why many scientists have “physics envy,” it’s not because they want neil degrasse tyson’s journal publications or to talk about the stars on his tv show. they want it because the moment a famous physicist as much opens his mouth, the media drops everything to pay attention no matter how inane or ridiculous the words that come out. stephen hawking has the press falling over him while he prattles on about superintelligent killer robots, space aliens, and a host of other things. many scientists would love the privilege of that instant sense of cultural authority and adoration, the ability to say something completely and totally ridiculous and have people listen with rapt attention because hey, you work on string theory so you must be smart! i personally have no problem with physicists. honest! after all, if it weren’t for them i would’t have those nifty cellular automata and toroidal grids for my 3000 netlogo turtles to run around within. so props, physics dudes and dudettes. and i’m very much on board with the idea that people do better when they aren’t limited to disciplinary assumptions. heck, by the time i’m done with my phd i will probably have robbed whole sections of the association for computing machinery and the american psychological association blind (despite not being technically in either category) because that’s just how social scientists do things. it’s all one big happy family, with the odd clunky r routine or two when the data actually has to be analyzed. so what’s the beef with physicists then? so if you think physicists are being arrogant jerks trying to stick our mathematical models where they don’t belong, then give us a second chance. it might be that we’re not trying to declare our superiority; maybe we just want to help, to learn more about other fields, and maybe we just get a kick out of finding out when our methods turn out to help us understand new things. that, at least, is why i chose to do science in the first place, and that’s why i’m so interested in using the methods i have to understand as much about the world as i possibly can. see, that is the thing. everyone is fine when people want to help, to contribute. we’re decidely less friendly when a outsider swoops down into a an established discipline, ignores almost everything that has been previously published, derives a toy model, and basks in media attention and ted talks because his or her math is shinier than everyone else’s. for example, take this attempt (from arxiv’s physics and society section) to rehabilitate one of the most thoroughly discredited ideas in political science because big data and statistical physics models about networks means that you can get away with avoiding testing the theory and credulous journalist s will jump all over it all the same. and when the dust settles and the media moves on to the next hot thing we are left with a scientific publication that does nothing to actually advance the science. instead, it got a brief flash of attention for showing how you can use physics math (tm) to get people briefly interested in a discredited theory of political science. but that’s not it, by any means. physicists think that economists just don’t know what they are talking about. oh look, some physicists that have a model of the “physics of fashion” without a single citation to a reputable source about the fashion industry! need help dealing with dangerous terrorists? ask a physicist, because they’ve got a model for that too ! and one that is light on actual integration with published knowledge about terrorism. and it’s not as if this is a new problem either. in 1928, the american political science review published the address of the president of the american political science association. the topic? analogies between physics and politics. i guess we can’t get rid of those physicists. i’m not bitter. nor do i want physicists to refrain from helping the rest of us. rather, i think that physicists could do well to think about the following things before formulating a quantum model of ___ in someone else’s field: does it answer a useful question in that field ? for a while i’ve never thought of a useful way to test the clausewitzian theory of war and politics with casualty statistics. a physicist, didier sornette, worked with two specialists in military strategy and political science to use a physics-inspired model to show how clausewitzian theory could be tested using ideas about the distribution of war sizes and more qualitative insights about institutions and governance. is it an appropriate tool for the field? there are a lot of physics models. physics researchers know a lot of advanced mathematics. not all of this is relevant or useful. my own field of computational social science utilizes a large amount of physics formalisms and concepts, but has little use for, say, computational fluid dynamics for most of our problems. we’re not trying to engineer airplanes. physics researchers need to be sure their tool is appropriate for the problem, as well as the disciplinary conventions and styles of whatever other field they are working in. of course, both are inherently subjective; and i admit that its a sad fact of life that a lot of the time, the physicist will know better about whether the question is useful or whether the tool is appropriate. still, given that physics steve seems to sincerely want to help, i suppose its worth the effort for both him and his interlocutors to try to meet each other halfway. science education research -- -- written by adam elkus 2.3k followers · 1k following phd student in computational social science. fellow at new america foundation (all content my own). strategy, simulation, agents. aspiring cyborg scientist. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@myinspiration/taparia-tools-a-premier-hand-tools-brand-d91c3c95f463,,,"taparia tools — a premier hand tools brand. | by inspiration | medium sitemap open in app sign up sign in medium logo write sign up sign in inspiration 2 min read · dec 24, 2015 -- listen share taparia tools — a premier hand tools brand. taparia has gained a huge market base in hand tools segment for the quality of products that it offers. hand tools are a great help at work. to make work more cost effective, hand tools are the best option. taparia started hand tools manufacturing in late 60’ only in collaboration with technology assistance by a reputed company in sweden. since then, it has been a one stop source for all varieties of hand tools and has been upgrading its technology to provide the best quality tools. the brand has now grown to showcase a huge variety of tools that can be useful to business and industrial houses for their everyday needs. the brand is providing tools like wrenches of various sizes and designs, double ended spanners, ring spanners, pliers, key sets of different sizes. also on offer are tools like screw drivers, clamps, hammers of various designs etc. the wide array of products is provided at the best price without compromising on the quality of the tools. big e-commerce platforms like tolexo , flipkart have ramped up their hand tools segment with products from reliable brands like taparia to scale up their volume as well as their revenues, as there has been an exponential increase in the demand for this product segment. -- -- written by inspiration 37 followers · 26 following no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@jadi/switching-to-memsql-made-my-day-4a6a3ab287b9,,,"how switching to memsql made my day | by jadi | medium sitemap open in app sign up sign in medium logo write sign up sign in how switching to memsql made my day jadi 4 min read · apr 1, 2015 -- 1 listen share i’m a programmer and these days using python for a data consolidation task on millions of rows of data. my code reads all the data from a sqlite database, creates the required output table filled with zeros and then checks the input row by row and add to the counts on the required cells of the output table. in the reality i work with smss but assume i’m working with fruits to prevent a nda complain filled by my boss against me. my initial table is filled with 0s and have a lot of rows and columns like this: then by parsing the input stream line by line, i look at my data, count the input and update the table. say after seeing an orange on march 31th, i’ll update my table to this: after a while i will have something like this: please note that i’m using cpu expensive update aggdb set count=count+newdata where date = thisdate sql commands and that is the main bottleneck of my codes performance. i started with the mysql and this is the result of cprofile: jadi@manipc:~/sms/$ python -m cprofile ordlc.py #mysql 16083556 function calls (16083023 primitive calls) in 257.195 seconds ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 257753 233.015 0.001 233.015 0.001 {method ‘query’ of ‘_mysql.connection’ objects} as you can see the total run takes 257s from which, 233s is spent by sql updates. then i tried sqlite in-memory dbs. they are not designed for performance but still improved the situation. jadi@manipc:~/sms$ python -m cprofile ordlc.py #sqlite in-memory db 11122033 function calls (11121552 primitive calls) in 177.187 seconds ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 257751 167.129 0.001 167.129 0.001 {method ‘execute’ of ‘sqlite3.cursor’ objects} this is good but still slow. i searched for newsql solutions and landed on memsql and voltdb. memsql - wikipedia, the free encyclopedia memsql is a distributed, in-memory database that is part of the newsql movement. it is an acid-compliant rdbms that… en.wikipedia.org memsql claims to be the world’s fastest in-memory database and it is very easy to run. so i installed it on the same machine which runs the scripts. it is a pentium i3 with 4gb of ram running ubuntu 14.10. this is the results: jadi@manipc:~/sms$ python -m cprofile ordlc.py #memsql 16083536 function calls (16083003 primitive calls) in 32.396 seconds ordered by: standard name ncalls tottime percall cumtime percall filename:lineno(function) 257752 15.892 0.000 15.892 0.000 {method ‘query’ of ‘_mysql.connection’ objects} wow! an astonishing 87% improvement on the total execution time in the total run time and 93% improvement on the ~250k updates! comparing the speed of memsql, sqlite in-memory databases and mysql and the best part? memsql is 100% compatible with mysql apis so i did not need to change even 1 line of code on my software and only needed to get a license from http://www.memsql.com/download/ and install memsql on my machine instead of mysql. i have to admit that memsql did a magic in my full-of-expensive-sql-updates case. let me finish this with a classic pros vs cons section. pros 100% mysql code compatibility makes switching to memsql as easy as installing memsql extremely fast. my profiling shows +90% improvement over mysql in my case. i know that these two are designed with two totally different ideas but when you need speed, memsql is the answer. easy to scale. although at the moment i’m using it on only one machine, but tested the same code on a 3 machine memsql deployment and worked great cons it is not a free software. i respect the programmers right for licensing their code as they wish but as a foss enthusiastic, prefer the foss software when possible. in addition i’m from iran and 1) under embargo 2) have no access to international credit cards. this makes purchasing a license out of my hands the memsql site does not mentions the pricing. if i’m going to use it on only one machine as a lightning fast temporary database for my data aggregation task, i prefer to know beforehand if it is going to be free on one machine or will cost me 10k a year. it makes programmers lazy ;) i do not need any other optimization on my code! just kidding! this is the best part and this should come in pros section. programming big data data science -- -- 1 written by jadi 2.2k followers · 180 following jadi from iran, trying to make the world a slightly better place. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/gpb-news-presents-what-moves-you-georgia/where-transit-s-going-in-gwinnett-county-44d5124fa300,,,"where transit’s going in gwinnett county | by sam whitehead | what moves you, georgia | medium sitemap open in app sign up sign in medium logo write sign up sign in what moves you, georgia · a series on the transportation issues impacting georgians from gpb news. yvette macpherson and other commuters wait for their gwinnett county transit (gct) bus. (sam whitehead/gpb news) where transit’s going in gwinnett county more people than ever want it, but will anyone want to pay? sam whitehead 5 min read · oct 14, 2015 -- listen share i t’s monday morning rush hour at the gwinnett transit center, and yvette macpherson has missed her bus. “what happened was we got to doraville train station, and the 10 we take was late getting us to the gwinnett transit county center, and the 30 left before the 10 got here,” she says. she’s one of the smattering of people gathered at the modest transit center, which is just a few bus shelters in a parking lot next to gwinnett place mall. macpherson says gwinnett county transit (gct) is part of her commute every weekday. she would prefer to drive, but she shares a car, and the person she shares it with doesn’t have other transit options. so, she’s stuck with the bus. “this only makes going to work even worse,” she says. “now i’m an hour behind, so now i’m going to spend my day playing catch up.” the six local bus routes offered by gct. (gwinnett county transit) macpherson says gct is the worst transit system she’s ever had to deal with. she also rides marta, but says it’s not this bad. gwinnett had chances to join marta in 1971 and 1990, but, both times, voters there said no. in 2000, gwinnett created its own transit system but did it independently of marta. today, gct offers connections to the marta system, express bus service into atlanta, and six local bus routes, but it’s still not popular with riders. c huck warbington, executive director of the gwinnett village community improvement district (gvcid), says the results of a recent poll bear that out. “the majority of the comments were that ‘it was inefficient. it didn’t go to where i wanted to go’,” he says. “[we] actually got several comments that indicated they wished gwinnett county’s bus system was more like marta’s bus system.” the banner logo from the gr8 exchange website. chuck warbington says more than 4,000 people responded to their text message survey. (gvcid) those comments come from the gr8 exchange, a virtual, county-wide town hall meeting and text message survey put on by the gvcid. warbington says the initial results from the gr8 exchange show opinions on transit are changing in gwinnett. essentially, people want more of it, whether that means expanded coverage from gct or even from marta. read more about how gwinnett’s changing demographics have impacted attitudes about transit . a recent poll from the gwinnett chamber of commerce shows marta is more welcome in the county than it once was: 63% of respondents said they’d welcome the system into gwinnett. still, only 50% said they’d want to pay for it. “[the gr8 exchange] initiative beared that out as well. it’s not scientific by any means like the poll, but we saw a huge response there,” warbington says. “i think we have elected officials…that are grappling with how do we fund this? how do we make sure that this is palatable to ultimately the voters?” o ne person working with those elected officials is alan chapman, director of gwinnett county’s department of transportation. “it’s really a balancing act between providing the service that citizens need and working with our budgets and really looking out for our taxpayers as well,” he says. gwinnett county department of transportation director alan chapman discusses funding for transportation improvements. (sam whitehead/gpb news) chapman is leading the team developing a long-term transportation plan for the county, which he says will look at improving gct, within limits. “we would work to constrain the plan so that’s it’s something that could be realistically be implemented with expected funding,” he says. if the past is any precedent, it might not make sense to plan for much. expanding the system would mean asking gwinnett voters to approve a special local option sales tax, and the county board of commissioners, who puts together the sales tax proposals, has never asked for any transit funding. for chapman, that might not matter so much. he doesn’t depend on the bus. “i’m so close to work that i really would not need that service,” he says. however, a lot of people do. riders took 1.7 million trips on the system last year. b ack at the gwinnett transit center, yvette macpherson worries about how missing her connection will impact the rest of her day. “i have a family to get home to. now, i can’t leave at 5:00 p.m. i have to leave at 6:00 p.m., which means i won’t get home until 8:30 p.m. — almost 9 o’clock— because i then have to try to deal with this going the opposite direction,” she says. after about an hour of waiting, her bus finally arrives. as for improvements to the system she rides everyday? it looks like she’ll be waiting for those a little while longer. more riders waiting for buses at the gwinnett transit center. 1.7 million rides were taken on gct in 2014. (sam whitehead/gpb news) originally published on www.gpb.org on october 19, 2015. transportation public transit whatmovesyouga -- -- published in what moves you, georgia 258 followers · last published oct 23, 2015 a series on the transportation issues impacting georgians from gpb news. written by sam whitehead 58 followers · 165 following a listener, not a fighter. reporter with @gpbnews. rt's ≠ e's. contact me at swhitehead@gpb.org . no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://charliebeckett.medium.com/this-blog-is-based-on-a-talk-charlie-beckett-gave-at-the-2015-british-science-festival-in-bradford-7236691301ab,,,"how journalism is turning emotional and what that means for the future of news | by charlie beckett | medium sitemap open in app sign up sign in medium logo write sign up sign in how journalism is turning emotional and what that means for the future of news charlie beckett 15 min read · sep 10, 2015 -- 4 listen share this blog is based on a talk i gave at the 2015 british science festival in bradford. as journalism and society changes emotion is becoming a much more important dynamic in how news is produced and consumed. this is redefining the classic idea of journalistic objectivity, indeed, it is reshaping the idea of news itself. that matters because journalism has an increasingly significant role in our lives as information, data and social media become more ubiquitous and more influential. at the moment journalists are exploring this as a professional phenomenon and citizens are seeing it as something that is personal — but i will suggest that to understand this better we need to think about the science around the relationship between emotions and understanding and behaviour. i should say two things before i start. firstly, i am very much at the beginning of this line of research so today i am posing questions rather than giving answers. secondly, i define ‘emotion’ rather broadly — other terms might be subjectivity, personalisation, sentiment, or affective communications. here’s the context as i see it. we are drowning in a sea of stories about our world. there is a daily flood of news through the new channels of the internet combined with the traditional media that is bigger than ever before despite the business model crisis for some parts of the industry. news consumers have more access, more easily to more journalism than ever before. that means that the news is everywhere, all the time. and it’s a different kind of news — it is networked . as i charted in my first book supermedia , journalism is now interactive, inter-connected, participatory, more open, more global, multi-platform, multi-linear, a stream of data, analysis and comment. that can be wonderful. it gives the journalist extraordinary creative and communicative power. since the 7/7 london bombings and those shaky pictures of people walking through the smoke-filled underground tunnels it is difficult to imagine any major event or issue being covered without the input of social media or digital devices. it can expand the citizen’s choice, information, engagement and understanding but it can also be confusing, distorting and even upsetting. this is something i explored in a radio documentary i presented earlier this year good news is no news and i also wrote about it in the guardian . i have never had such an animated public response to anything i have done in my life as i got to that programme. it touched a nerve among journalists and the public. through interviews with a range of news-makers from buzzfeed to the daily mail the programme showed how the old idea of ‘hard news’ that shocks, frightens, disturbs and alarms can leave the audience feeling alienated, disempowered, helpless and, worst of all, apathetic, insensitive and even hostile to learning about our world. i argued in that programme that journalism must not lose its competitive, critical, independent edge. it must tell people things we don’t always want to hear. but it should also find better ways to give context and promote understanding so that we pay attention to and engage with the news: if you like more ‘constructive journalism’. if news is going to work for you as citizens then we have to find better ways to create, deliver and consume journalism that is more relevant, reliable and responsive to the audience. there are many clever technological fixes that can help with this — the bbc has just relaunched its main website giving the consumer the option of customising the content to suit your interests. it will give local news for where you are from — and if you don’t want celebrity gossip, then you can turn that off. but there is much more that journalists must do to change their craft to help people cope with the increased volume and variety of sources now just a click away — and often thrust in front of you without your permission. i think understanding the new role of emotion in journalism is critical to this. there are three factors currently driving journalists towards using emotion as a tool. the first is economic. competition has never been more intense. the internet means that your rivals are everywhere and endless. distraction away from news is more immediate and accessible to your audience than ever before. as advertising revenues plunge journalists have to fight harder than ever for every eyeball or ear. tugging at your heart strings is a tried and tested way to get your attention. secondly, it is about the technology. we have clear evidence that using emotional cues helps to get your attention and to prolong your engagement. so a story with a visual stimulus gets more traffic. text written in conversational language tends to increase responsiveness. and there are legions of other tricks that sites like upworthy have perfected — the so-called curiosity gap for example in a headline — “what happened when jeremy corbyn and harry styles met? increasingly, journalism is now distributed not by transmitters or newsagents but by social media. getting people to share your content is vital and emotion is critical to making that happen. thirdly, it’s about a better understanding of behavioural science and even neurology. we know from politics that people respond to emotion not ideas or facts — so when we do political journalism we now talk about ‘optics’ instead of ‘facts’. marketing journalism is no different. we need to understand what makes people tick before we try to sell them news. understanding how people relate to the news on a personal level is vital to anyone trying to get them to connect to their journalism. sometimes people will have practical or professional reasons for finding news relevant, but increasingly it is an emotional response, too. luckily for journalists we now have the technology and the data to measure that process. let’s now see what’s changed about media that makes emotion more central? first of all media is now mobile. as this graph shows our devices are always with us. smartphones especially are set to become effectively universal and their usage is increasing all the time. they are rapidly becoming the primary platform, not just an option. our physical relationship to news is changing because of this technology — rodin’s thinker has become steve job’s swiper. the device is always on — always to hand — but also part of your personal and public networks. you can customise it with apps for example to get what you want when you want it as well as share and create content yourself. as humans we love to do this and as it becomes more intimate we become more attached. you might characterise this as digital dependency. as this chart shows, despite the cost and the dangers of addiction, people are generally positive about this new relationship with the mobile technology — smartphone users say they can’t live without their devices — but they also find them liberating and help them to connect better to information and other people. and i think that it’s important to stress the intimacy of this technology — compared to the consumption of news when sitting on the sofa or reading on the train. these devices are where people now turn to on waking. a study found that 35 percent of people thought about their smartphone first thing in the morning, followed by 17 percent who thought about coffee, and 13 percent who thought about their toothbrushes. only 10 percent thought about their significant other. this means that journalism has to work in this world where it is blended into people’s digital mobile lives alongside kittens, shopping, sport, music and pornography. it’s now part of people’s personal mediated lives. as i said. journalists are responding to this in a professional way. and that isn’t always very sophisticated. arianna huffington has seen the statistics that indicate that people are more likely to share content that has an uplifting element — look at the facebook statistics on their ‘good news’ section of the huffington post. but it doesn’t necessarily produce journalism of the most incisive or informative nature. but it can also be a way to get people to engage with stories that are disturbing and frightening. this photograph of a greek sailor who helped refugees whose boat had sunk off a holiday beach got massive traffic and engagement because it showed a narrative about an heroic and instinctively compassionate and brave response to a tragic situation. it helped that he is very hunky and the picture very dramatic — but it helped frame the issue in a way that literally demonstrated how people could help. this use of emotion can have some unexpected outcomes. humour has never had a place in traditional hard news — but even with disaster stories such as the hurricane that hit sandy people cope with tragedy partly through humour — it is also a big factor in sharing of political news. when sandy hit people responded with gallows humour — with someone even setting up an account for the hurricane itself — it’s not journalism as such — but it was a way for people to engage with the story and so became part of the media narrative. so we can see that there are functional reasons for sharing material — but it is a myth that people consume news mainly because it is useful, informative, or entertaining — or even that we want to know stuff so we can change the world. i think the primary reason — certainly for news sharing in the social media space is personal — we are acting in an emotionally charged way in communication with our community or the wider networks. as alfred hermida explains in his new book on social media sharing, “tell everyone”, humans like to talk about themselves to others — it is good for us and it helps build communities. as news becomes part of social media so it becomes part of that process. but what about the journalist? what does all this emotionally driven sharing and liking lead to? in a sense this a familiar debate within the profession. so how does this change the newsroom culture and especially that classic idea of objectivity? this slide demonstrates one thing — that i am not very creative when it comes to powerpoint graphics. but in a very crude way it shows how traditional mainstream journalists try to be objective about the news that they produce. despite what most people think, journalists are human. they are often especially curious and aware of narratives that are interesting and engaging. they respond as humans. but to make the journalism they must objectify the process. this is partly because of the practical pressures on their work: deadlines, limited resources, the need to turn complex real events into formulaic, understandable, accessible, consumerable items of news media. then after work they either go home to kick the cat or down the pub to chat over the days’ events and coverage. mostly though they are on a kind of auto-pilot. but with networked news where events are often being reported and discussed on social media — and their own journalism is subject to comment and sharing — they share this process with the public, live, as they are working. and as emotion becomes a more significant factor in that process for both the news-maker and the news-consumer or sharer, so, i would suggest, there will an interesting feed-back loop into the professional culture that may impact in its turn on how the news is produced in the future. i think that this is already happening. my former colleague jon snow has always been a passionate journalist — he famously teared up live on screen while covering the haiti earthquake. but in this video — not broadcast on c4news tv but put up on their website — he made a strong political statement about the suffering in gaza last year. but i thought it was interesting because while he was making a political point in general, it was couched in emotional terms — a plea to convert empathy into some kind of action — action that was partly about people using media to make a point. for many people it crossed the line — especially for a broadcaster that is subject to regulation that insists on a kind of objectivity. interestingly, not all of his mainstream media rivals criticised him — sky news’ editor thought he might even have been able to broadcast it., albeit with some signposting to signal that it was outside of normal coverage. i think it is interesting that ryley is recognising a journalist’s right to sensitivity. but do we want to undermine objectivity? the value of objective journalism is the idea that journalism can attempt to give an account that is balanced, fact-based and that gives a fair summary not just of what has happened but the context around it without the distortion of the journalist’s own feelings. of course, anyone who thinks about this for a few minutes realises that this can only be an aspiration. all journalists are human and have different factors that shape their worldview and their understanding of particular circumstances. it’s all relatively relative. by selecting a story for reporting you have made a choice. the facts that you omit as well as those you include are selective. even the most emotionless narrative has subjectivity. simply by putting two sides to an issue does not mean you have reported it accurately. yet we seem to be witnessing the potential death of that kind of objectivity — or the aspiration towards that unreachable ideal that has so conditioned much of what we think of as news reporting. i want to stress that there’s nothing new about the idea of emotion in news journalism. american journalism invented the idea of the ‘yellow press’. sensational journalism that sought to stir your fear, wonder and excitement about what is happening in the world. making a drama of a crisis has always been part of mass media. the theatre of news is as old as broadcast journalism. news as a spectacle has always been one of its dramatic forms. if news does not get your attention, if you do not find it interesting, amusing, frightening or uplifting than you are less likely to take notice. but like so much in modern media — that has all become much faster, more complex and more unstable. the narrative is no longer in the control of the journalist and emotions now threaten to run riot. but here’s one of the dangers — the filter bubble — the echo chamber — the danger that we end up only responding to emotional triggers that please us — that we only want to hear views that support our views and confirm our prejudices. this network graph details the landscape of twitter handles responding to the unwra school bombing. the things people were saying were not necessarily untrue or unfactual but they were framed in very different ways — the emotional response was very bifurcated. the following tag cloud represents co-occurence of hashtags on insragram posts. the larger a tag, the more times it appeared. the tighter-connected two tags are, the more times they appeared together. the point of this is not to show that people disagree about gaza. we knew that. the problem is that social media algorithms combined with the emotional dynamic in sharing and interaction will tend to reinforce that divisiveness. these networks are programmed — just as humans seem to be programmed — to follow the flow of our prejudices — like tends to connect with like because we prefer that to communication that challenges. as data scientist gilad lotan writes : “the better we get at modeling user preferences, the more accurately we construct recommendation engines that fully capture user attention. in a way, we are building personalized propaganda engines that feed users content which makes them feel good and throws away the uncomfortable bits.” what interests me — and i think i should admit it worries me too — is that this shift to self-affirmation seems to be a factor in the current drift towards a rampant relativism. increasingly, this social media driven dialogue is more about affirming one’s own perspective rather than seeking new viewpoints or counter-arguments. healthy liberal democracy and society depends on the idea of a diverse public sphere with agonistic dialogue between different viewpoints. objectivity is important to that process because even where we disagree we need to have a structure based on evidence and reason as well as emotion. yet, everywhere from the coverage of ukraine to the contest for the labour party leadership we can see how social media is challenging that and replacing it with a series of ideological self-referential bubbles. this is paralleled in mainstream media with the growth in partisan press — either subsidised propaganda machines like rt or cctv or the ‘views news’ of fox news or the guardian. i don’t blame social media or journalism for this. i think it is a product of much bigger social and political forces at work in our post-ideological age. in many ways it is to be welcomed if people resist received wisdom and imposed frameworks and have a choice of more divergent perspectives. but as journalism reinvents itself i think that it is important that it looks at this powerful mix of emotion and relativism and asks how best an aspiration to objectivity might be fostered in this environment. it certainly isn’t by insisting that the authorities know best — let alone the journalistic authorities. i also don’t want to panic. funnily enough objectivity is making a comeback. look at the boom in data journalism based on facts. look at new visual narrative makers like vice. yes, they feature personable young reporters telling you what’s happening out there in an informal style. but by avoiding the usual formula they are getting out of the way and allow the public to see stories more directly. go read buzzfeed. it’s famous for its cats but look at their burgeoning news content. yes, they use lots of pictures and often they are quite witty. but the actual reporting is remarkably straight. so along with the emotion people want more facts and reliable narratives — this is not a contradiction. this is what i call networked journalism. at its heart is the human factor because the audience is now part of the process. there is no going back. but if everything is subjective then nothing is false and nothing is true. subjective journalism is fine as long as it is overt and if it is not always affirmative journalism. journalism is supposed to challenge cognitive biases not reinforce them. it must also be self-critical and self-reflexive as well as being critical of others. for me the key principle in this — for the citizen as well as the journalist- is transparency. transparency is the new objectivity. and how that works is the subject for a whole new lecture. and what about the science? we need to know much more about the role of emotion and the consequences: the ontology of data, the political economy of identity when privacy goes public the sociology of influence when power is redistributed emotionally. at the moment journalists don’t understand how the technology works in this context. we don’t know the detail of algorithms created by the digital intermediaries that drive search and sharing. perhaps science — and not just computer science — can also help to build algorithms that encourage serendipity, that foster media literacy and even protect us from emotional harm? we need more evidence-based analysis of what zizi papcharissi calls ‘affective publics’. what motivates attention and agency related to media? at the moment we have what andrew chadwick called a ‘hybrid media’ system blended between linear legacy journalism and the new socialised, networked news media. there is much more change to come. but the trend is clear: towards more mobile, personalised and emotionally-driven news media. and the challenge for the networked journalist is also clear: how best to sustain the ethical, social and economic value of journalism in this new emotionally networked environment. @charliebeckett journalism news future of news -- -- 4 written by charlie beckett 5.2k followers · 540 following journalist, lse media professor, polis think-tank director. director of the lse's journalism and ai project https://www.journalismai.info/ responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@danshipper/charlie-munger-on-how-to-build-a-2-trillion-startup-1842f4a6426e,,,"charlie munger on how to build a $2 trillion startup | by dan shipper | medium sitemap open in app sign up sign in medium logo write sign up sign in charlie munger on how to build a $2 trillion startup dan shipper 19 min read · jan 19, 2015 -- listen share imagine it’s january of 1884 in atlanta, georgia. glotz, an affluent fellow citizen, has invited you to participate in a peculiar competition: you and twenty others are invited to present a plan to start a business that will turn a $2 million investment into a business worth $2 trillion by the year 2034. glotz will personally give $2 million to the person who presents the most compelling pitch in exchange for half of the equity in the new venture. there are only a few stipulations: the new venture must exclusively sell beverages (specifically non-alcohol or “soft” beverages) for reasons unknown glotz has decided that company must be named coca-cola you have 15 minutes. what would you say in your pitch? that’s the question that billionaire coca-cola investor charlie munger posed to an audience at a talk in july of 1996. what followed over the following few minutes was an entrancing exhibition of multi-disciplinary wisdom and business acumen. munger’s main point is that the most complex questions often have basic answers rooted in elementary academic wisdom (mathematics, psychology, etc.) he wants to show that applying some of these ideas regularly can help us to better explain business success, and make better decisions. to start his talk, munger lays out five principles he will use in his pitch to glotz: decide big no-brainer questions first use math to help explain the world think through problems in reverse the best wisdom is elementary academic wisdom big (lollapalooza) results only come from a large combination of factors munger then dives in to solving the problem with his first principle: the big no-brainer questions that can be answered first. “well, glotz, the big “no-brainer” decisions that, to simplify our problem, should be made first are as follows: first, we are never going to create something worth $2 trillion by selling some generic beverage. therefore, we must make your name, “coca-cola,” into a strong, legally protected trademark. second, we can get to $2 trillion only by starting in atlanta, then succeeding in the rest of the united states, then rapidly succeeding with our new beverage all over the world. this will require developing a product having universal appeal because it harnesses powerful elemental forces. and the right place to find such powerful elemental forces is in the subject matter of elementary academic courses.” off the bat, it’s interesting to note how his prescription for growth largely mirrors the conventional startup wisdom espoused by peter thiel and others: grow quickly in a small market that you can dominate and then expand from there. in the case of software, that market is typically a small niche of consumers. in the case of coca-cola (especially in the 1800s) it’s a small concentration of consumers in a geographically circumscribed area. next munger moves on to his second and third principles: numerical fluency and thinking in reverse. “we will next use numerical fluency to ascertain what our target implies. we can guess reasonably that by 2034 there will be about eight billion beverage consumers in the world. on average, each of these consumers will be much more prosperous in real terms than the average consumer of 1884. each consumer is composed mostly of water and must ingest about sixty-four ounces of water per day. this is eight, eight-ounce servings. thus, if our new beverage, and other imitative beverages in our new market, can flavor and otherwise improve only twenty-five percent of ingested water worldwide, and we can occupy half of the new world market, we can sell 2.92 trillion eight-ounce servings in 2034. and if we can then net four cents per serving, we will earn $117 billion. this will be enough, if our business is still growing at a good rate, to make it easily worth $2 trillion.” munger continues: “a big question, of course, is whether four cents per serving is a reasonable profit target for 2034. and the answer is yes if we can create a beverage with strong universal appeal. one hundred fifty years is a long time. the dollar, like the roman drachma, will almost surely suffer monetary depreciation. concurrently, real purchasing power of the average beverage consumer in the world will go way up. his proclivity to inexpensively improve his experience while ingesting water will go up considerably faster. meanwhile, as technology improves, the cost of our simple product, in units of constant purchasing power, will go down. all four factors will work together in favor of our four-cent-per-serving profit target. worldwide beverage-purchasing power in dollars will probably multiply by a factor of at least forty over 150 years. thinking in reverse, this makes our profit-per-serving target, under 1884 conditions, a mere on fortieth of four cents or one tenth of a cent per serving. this is an easy-to-exceed target as we start out if our new product has universal appeal.” in this section, munger demonstrates the value of the basic math involved in a tam (total addressable market) analysis as part of formulating a thesis for a business. then he goes on to look at the basic cost structure of the business and ensures that the back-of-the-envelope math makes sense for him to reach his ultimate goal. as part of this analysis he makes a lot of forward looking predictions with the benefit of hindsight (the depreciation of the dollar, the real purchasing power of the average consumer, worldwide beverage purchasing power etc.) but for now we can ignore those issues. munger continues on to the meat of his talk: the subject of creating a product compelling enough to be consumed daily by millions of people. this is where he’s going to bring out his fourth and fifth principles: the value of academic wisdom, and the forces that must be brought together to produce “lollapalooza” effects. “we must next solve the problem of invention to create universal appeal. there are two intertwined challenges of large scale: first, over 150 years, we must cause a new-beverage market to assimilate about one-fourth of the world’s water ingestion. second, we must so operate that half the new market is ours while all of our competitors combined are left to share the remaining half. these results are lollapalooza results. accordingly we must attack our problem by causing every favorable factor we can think of to work for us. plainly, only a powerful combination of many factors is likely to cause the lollapalooza consequences we desire. fortunately, the solution to these intertwined problems turns out to be fairly easy if one has stayed awake in all the freshman [college] courses. let us start by exploring the consequences of our simplifying “no-brainer” decision that we must rely on a strong trademark. this conclusion automatically leads to an understanding of the essence of our business in proper elementary academic terms. we can see from the introductory course in psychology that, in essence, we are going into the business of creating and maintaining conditioned reflexes. the “coca-cola” trade name and trade dress will act as the stimuli, and the purchase and ingestion of our beverage will be the desired responses. and how does one create and maintain conditioned reflexes? well, the psychology text gives two answers: (1) by operant conditioning and (2) by classical conditioning, often called pavlovian conditioning to honor the great russian scientist. and, since we want a lollapalooza result, we must use both conditioning techniques — and all we can invent to enhance effects from each technique.” let’s take some time to define a few of the things that munger is talking about here so that it’s easy to follow the rest of the argument. munger is mostly interested in the psychology of consumer decision-making: how can we influence consumers to buy a lot of a certain type of product? there are two that he’s going to talk about here: operant conditioning and classical conditioning. classical conditioning is the method by which a strong innate response can become invoked by a neutral stimulus. the most famous demonstration of classical conditioning is the work the russian physiologist ivan pavlov did with dogs: he conditioned dogs to salivate at the sound of a bell. to do this, every time pavlov fed his dogs he would ring a bell. after repeating this procedure a few times, pavlov found that he could ring his bell and the dogs would salivate without any food being present! this is powerful from a marketing perspective because it means that it’s possible to trigger an innate biological response with a stimulus of your choice. like, for example, a logo. now let’s briefly talk about operant conditioning. b. f. skinner, the famed harvard behaviorist describes operant conditioning in this way: “when a bit of behavior is followed by a certain kind of consequence, it is more likely to occur again, and a consequence having this effect is called a reinforcer.” in case this is confusing, skinner elaborates with an example: “food, for example, is a reinforcer to a hungry organism; anything the organism does that is followed by the receipt of food is more likely to be done again whenever the organism is hungry.” there is also a distinction between different types of reinforcers: some are negative and some are positive: “some stimuli are called negative reinforcers; any response which reduces the intensity of such a stimulus — or ends it — is more likely to be emitted when the stimulus recurs. thus, if a person escapes from a hot sun when he moves under cover, he is more likely to move under cover when the sun is again hot. the reduction in temperature reinforces the behavior it is ‘contingent upon’ — that is, the behavior it follows. operant conditioning also occurs when a person simply avoids a hot sun — when, roughly speaking, he escapes from the threat of a hot sun.” now that we have a little bit more background, let’s get back to munger. right now he’s trying to figure out how to use operant conditioning to increase the consumption of his product: “the operant conditioning part of our problem is easy to solve. we need only (1) maximize rewards of our beverage’s ingestion and (2) minimize possibilities that desired reflexes, once created by us, will be extinguished through operant conditioning by proprietors of competing products. for operant conditioning rewards, there are only a few categories we will find practical: (1) food value in calories or other inputs; (2) flavor, texture, and aroma acting as stimuli to consumption under neural preprogramming of man through darwinian natutal selection; (3) stimulus, as by sugar or caffeine; (4) cooling effect when man is too hot or warming effect when man is too cool. wanting a lollapalooza result, we will naturally include rewards in all the categories. to start out, it is easy to decide to design our beverage for consumption cold. there is much less opportunity, without ingesting beverage, to counteract excessive heat, compared with excessive cold. moreover, with excessive heat, much liquid must be consumed, and the reverse is not true. it also easy to decide to include both sugar and caffeine. after all, tea, coffee, and lemonade are already widely consumed. and, it is also clear that we must be fanatic about determining, through trial and error, flavor and other characteristics that will maximize human pleasure while taking in the sugared water and caffeine we will provide. and, to counteract possibilities that desired operant-conditioned reflexes, once created by us, will be extinguished by operant-conditioning-employing competing products, there is also an obvious answer: we will make it a permanent obsession in our company that our beverage, as fast as practicable, will at all times be available everywhere throughout the world. after all, a competing product, if it is never tried, can’t act as a reward creating a conflicting habit. every spouse knows that.” after talking through operant conditioning, munger turns to classical conditioning: “we must next consider the pavlovian [classical] conditioning we must also use. in pavlovian conditioning, powerful effects come from mere association. the neural system of pavlov’s dog causes it to salivate at the bell it can’t eat. and the brain of man yearns for the type of beverage held by the pretty woman he can’t have. and so, glotz, we must use every sort of decent, honorable pavlovian conditioning we can think of. for as long as we are in business, our beverage and its promotion must be associated in consumer minds with all other things consumers like or admire. such extensive pavlovian conditioning will cost a lot of money, particularly for advertising. we will spend big money as far ahead as we can imagine. but the money will be effectively spent. as we expand fast in our new beverage market, our competitors will face gross disadvantages of scale in buying advertising to create the pavlovian conditioning they need. and this outcome, along with other volume-creates-power-effects, should help us gain and hold at least fifty percent of the new market everywhere. indeed, provided buyers are scattered, our highest volumes will give us very extreme cost advantages in distribution. moreover, pavlovian effects from mere association will help us choose the flavor, texture, and color of our new beverage. considering pavlovian effects, we will have wisely chosen the exotic and expensive-sounding name “coca-cola,” instead of a pedestrian name like “glotz’s sugared, caffeinated water.” for similar pavlovian reasons, it will be wise to have our beverage look pretty much like wine, instead of sugared water. and so we will artificially color our beverage if it comes out clear. and we will carbonate our water, making our product seem like champagne, or some other expensive beverage, while also making its flavor better and imitation harder to arrange for competing products. and, because we are going to attach so many expensive psychological effects to our flavor, that flavor should be different from any other standard flavor so that we maximize difficulties for competitors and give no accidental same-flavor benefit to any existing product.” having dealt with pavlovian conditioning, munger moves on to social proof: “what else, from the psychology textbook, can help our new business? well, there is that powerful ‘monkey-see, monkey-do’ aspect of human nature that psychologists often call ‘social proof.’ social proof, imitative consumption triggered by mere sight of consumption, will not only help induce trial of our beverage. it will also bolster perceived rewards from consumption. we will always take this powerful social-proof factor into account as we design advertising and sales promotion and as we forego present profit to enhance present and future consumption. more than with most other products, increased selling power will come from each increase in sales. we can now see, glotz, that by combining (1) much pavlovian conditioning, (2) powerful social-proof effects, and (3) wonderful-tasting, energy-giving, stimulating and desirably-cold beverage that causes much operant conditioning, we are going to get sales that speed up for a long time by reason of the huge mixture of factors we have chosen. therefore, we are going to start something like an autocatalytic reaction in chemistry, precisely the sort of multi-factor-triggered lollapalooza effect we need. the logistics and the distribution strategy of our business will be simple. there are only two practical ways to sell our beverage: (1) as a syrup to fountains and restaurants, and (2) as a complete carbonated-water product in containers. wanting lollapalooza results, we will naturally do it both ways. and, wanting huge pavlovian and social-proof effects we will always spend on advertising and sales promotion, per serving, over 40 percent of the fountain price for syrup needed to make the serving. a few syrup-making plants can serve the world. however, to avoid needless shipping of mere space and water, we will need many bottling plants scattered over the world. we will maximize profits if (like early general electric with light bulbs) we always set the first-sale price, either (1) for fountain syrup, or (2) for any container of our complete product. the best way to arrange this desirable profit-maximizing control is to make any independent bottler we need a subcontractor, not a vendee of syrup, and certainly not a vendee of syrup under a perpetual franchise specifying a syrup price frozen forever at its starting level. being unable to get a patent or copyright on our super important flavor, we will work obsessively to keep our formula secret. we will make a big hoopla over our secrecy, which will enhance pavlovian effects. eventually food-chemical engineering will advance so that our flavor can be copied with near exactitude. but, by that time, we will be so far ahead, with such strong trademarks and complete, “always available” worldwide distribution, that good flavor copying won’t bar us from our objective. moreover, the advances in food chemistry that help competitors will almost surely be accompanied by technological advances that will help us, including refrigeration, better transportation, and, for dieters, ability to insert a sugar taste without inserting sugar’s calories. also, there will be related beverage opportunities we will seize. this brings us to a final reality check for our business plan. we will, once more, think in reverse like jacobi. what must we avoid because we don’t want it? four answers seem clear: first, we must avoid the protective, cloying, stop-consumption effects of aftertaste that are a standard part of physiology, developed through darwinian evolution to enhance the replication of man’s genes by forcing a generally helpful moderation on the gene carrier. to serve our ends, on hot days a consumer must be able to drink container after container of our product with almost no impediment from aftertaste. we will find a wonderful no-aftertaste flavor by trial and error and will thereby solve this problem. second, we must avoid ever losing even half of our powerful trademarked name. it will cost us mightily, for instance, if our sloppiness should ever allow sale of any other kind of “cola,” for instance, a “peppy cola.” if there is ever a “peppy cola,” we will be the proprietor of the brand. third, with so much success coming, we must avoid bad effects from envy, given a prominent place in the ten commandments because envy is so much a part of human nature. the best way to avoid envy, recognized by aristotle, is to plainly deserve the success we get. we will be fanatic about product quality, quality of product presentation, and reasonableness of prices, considering the harmless pleasure it will provide. fourth, after our trademarked flavor dominates our new market, we must avoid making any huge and sudden change in our flavor. even if a new flavor performs better in blind taste tests, changing to that new flavor would be a foolish thing to do. this follows because, under such conditions, our old flavor will be so entrenched in consumer preference by psychological effects that a big flavor change would do us little good. and it would do immense harm by triggering in consumers the standard deprival super-reaction syndrome that makes “take-aways” so hard to get in any type of negotiation and helps make most gamblers so irrational. moreover, such a large flavor change would allow a competitor, by copying our old flavor, to take advantage of both (1) the hostile consumer super-reaction to deprival and (2) the huge love of our original flavor created by our previous work. well, that is my solution to my own problem of turning $2 million into $2 trillion, even after paying out billions of dollars in dividends. i think it would have won with glotz in 1884 and should convince you more than you expected at the outset. after all, the correct strategies are clear after being related to elementary academic ideas brought into play by the helpful notions.” during the rest of the piece, munger discusses the parallels between his fictional business plan and coca-cola’s actual business (hint: they’re pretty much the same.) he then goes on to relate his story to the main purpose of the talk: our failure to use basic academic wisdom to make better decisions. he chalks this up, in part, to a failure of academia to produce a useful synthesis of topics like psychology and behavioral economics. he thinks that academic departments are too narrowly focused, and the research of academics to circumscribed. is there bullshit to sniff out here? if you scan the shelves at an airport bookstore, you’re likely to find lots of books that, like munger’s talk, purport to unveil some of the key attributes of successful companies. if you’ve ever been tempted to pick up a copy of think like zuck: the five business secrets of facebook’s improbably brilliant ceo mark zuckerberg you know what i mean. because this is a common trope in the business literature — and it’s one that we often swallow uncritically — i want to take a few paragraphs to instill in you a healthy dose of skepticism in any person that purports to unveil the hidden attributes of successful companies: first, anything that attempts to reduce the success of a business to a few key principles misses out on the obscene complexity that underlies the growth of any kind of organization. second, it misses the fact that many (but not all) organizations are incentivized to hide the real story behind their growth in order to protect their image, their investors, their employees, or their (perceived or real) competitive advantages. third, these attributes are subject to interpretation via the halo effect: they are seen as good only because the company is successful. many times you’ll see a ceo characterized as a visionary perfectionist when the stock is up, and an micro-managing egoist when the stock is down. fourth, munger’s talk is (knowingly) given with the benefit of hindsight. it’s easy to point to many of these things as sure signs of success — once the success has been achieved. fifth, the attributes you see are subject to selection bias: people generally only write books or give talks about successful companies. just because a successful company has attribute a, doesn’t mean that there aren’t a thousand other companies with attribute a that went to the graveyard. what we’re really looking for is evidence that a particular company attribute played a causational role in their success — rather than just merely being associated with that success. this is incredibly difficult, if not impossible, to do. in the case of munger’s talk i’m going to err on the side of believing him for two reasons: he puts his money where his mouth is. his very long investment track record provides some demonstration that his framework works at picking companies. he doesn’t claim universal applicability. his goal isn’t to give you a foolproof way of predicting company success — it’s to give you a framework, based in basic ideas, to help you think about that success. so assuming we trust munger, the next question we have to ask ourselves is: can we use what he’s saying? is all of this useful? clearly, if you’re running a big company this kind of framework could help you make better decisions. for example, had the executives at coke who decided to introduce “new coke” understood conditioning better they might have scrapped the plan before it became a disaster. similarly, if you’re an institutional investor evaluating a large consumer business like coke he provides you an interesting framework to think about. i think the argument can also be made that it provides a good framework for early-stage technology investors to think about — one that agrees with the startup-focused investors like paul graham, peter thiel, and others. (i think it’s always interesting when people who come from different backgrounds and work on different problems come to the similar conclusions about something, it usually means that there’s some truth to what they’re saying.) applying munger’s framework to technology investing if i had to summarize munger’s advice in a few sentences (with the benefit of reading lots of other articles from him) it would be something like the following: in order to get large (lollapalooza) effects like rapid growth you need to harness lots of different types of elementary forces. the forces to focus on create what we call positive feedback loops: they’re self-reinforcing. a causes b, b causes more a, which causes more b. the forces of this type that munger cites are psychological: operant conditioning and classical conditioning. all companies have to harness these kinds of psychological forces to grow. but there are other ones to look for as well: economies of scale: the larger you are, the cheaper it is to produce your product, the more products you sell, the larger you get network effects: the larger the network becomes, the more valuable the network is, the larger the network gets word of mouth: the more of your product you sell, the more people talk about you, the more of your product you sell big data effects: the more data a search engine has the, the better search results it can return, the more data it gets incumbent advantages: the more large customers you have, the easier it gets to sign large customers, the more large customers you get finding companies that harness these effects is important for technology investors because the venture investment model is predicated upon huge returns from companies that dominate large winner-take-all markets. and how do you get a winner-take all market? one company has to be able to build an ever-increasing advantage against its competitors. it needs to be able to achieve high enough velocity to escape the gravity of market competition. but how do you build this kind of ever-increasing lead? you effectively harness the self-reinforcing forces we’ve been discussing: conditioning, economies of scale, network effects, word of mouth, big data effects, and others. but what if you’re not an investor. what if you’re thinking of starting your own company? can you use these ideas to come up with a startup? is this generative? the first question to ask about his framework is this: is it generative? by this i mean, will it help me generate new ideas for businesses to start. the answer to this question is clearly: no. thinking to yourself, “what companies can i start that will have self-reinforcing feedback loops?” doesn’t make your brain generate new ideas. as always, the best way to generate these ideas is through experience. paul graham writes, “the way to get startup ideas is not to try to think of startup ideas. it’s to look for problems, preferably problems you have yourself.” ok, so munger’s framework isn’t generative. the next question we have to ask ourselves is: is it diagnostic? in other words, assuming that munger has identified some key causational elements of successful companies, how valuable is it for helping us diagnose companies at the idea stage? the answer to this question is: if you’re looking to build a billion dollar company, it’s at best a marginally helpful guide. and the fact that it’s not incredibly helpful isn’t really munger’s fault. there’s just no framework of elements out there that would allow us to perfectly predict how well our ideas are going to turn out. there are lots of reasons for this. here are a couple: success is stochastic not deterministic execution is more important than mere idea your initial idea of what you’re building is often much different than what you actually build because what you actually build is often much different than what you think you’re building at the beginning, exact analysis of this kind is difficult to do. at best, what it can help you with is a general “sniff” test: in general, does my vision for this product seem like it has the potential to harness some of these forces? if the answer is yes then it’s time to get on to the next step and build the damn thing. – works cited: poor charlie’s almanack: the wit and wisdom of charles t. munger, expanded third edition, charlie munger beyond freedom and dignity, b.f. skinner -- -- written by dan shipper 3.9k followers · 759 following thinking things through. prev: co-founder of @usefirefly, acquired ’14 by @pega. no responses yet help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@BBSRC/octocopter-fff46d32a351,,,"octocopter!. experimental drone for agricultural… | by bbsrc | medium sitemap open in app sign up sign in medium logo write sign up sign in octocopter! bbsrc 8 min read · sep 29, 2015 -- 1 listen share experimental drone for agricultural research demonstrator technology takes to the skies to collect data for field trials. following the revelation that english premiership football club everton fc is using drones to monitor player training sessions, it seems there’s nothing these unmanned aerial vehicles (uavs) aren’t being primed to do: everything from delivering post (and pizza! ) to security, defence as well as recording home movies and major sporting events. it’s too early to say which will be novelty uses as opposed to daily realities , but above the green fields of hertfordshire, uk, scientists have been busy for more than a year running pilot(less) studies to see what drones could bring to agricultural research. an advanced eight-rotor model dubbed the octocopter has been put through its paces by researchers led by andrew riche of rothamsted research , an institute devoted to plant, soil and agricultural science that receives strategic funding from british bioscience funders bbsrc . rothamsted is the site of the longest-running field experiments in the world , where farm systems comparing the effects of inputs such as fertilizers and pesticides on crop growth have been running for more than 100 years. here in the 21st century, they want to see if drone surveillance can take much of the drudgery and footwork out of these large-scale experiments that span over 200 hectares of picturesque english countryside. as a demonstrator technology, the researchers are embarking on a rigorous programme of regularly recording experiments over five hectares (the size of seven soccer pitches) to see exactly what this airborne marvel is capable of. “this year we have focused on monitoring crop height through the season and also crop maturity,” says riche. “different wheat lines mature at different rates, and we regularly monitor this on the ground — this year we hope to have done it from the air.” a go-pro hero 4 camera was attached to the octocopter especially for the film above. image: bbsrc eye in the sky the octocopter is an enabling technology. it can be controlled manually or fly itself via gps, and is fitted with special cameras that can image crop height, growth and biomass, and in the future much more. the craft has a regular rgb (red blue green) imager as in a conventional digital camera. it also has a tir (thermal infrared) camera which can be used to monitor the crop canopy or soil temperature. the canopy temperature can give an indication of crop stress, because when plants experience drought for example their temperature rises as they stop evaporating water from their leaves. the thermal infrared (tir) camera can capture information about plant stress. image: rothamsted research the octocopter also carries a nir (near infrared, also called multispectral) camera that takes images from which a measure called the normalised difference vegetation index (ndvi) can be calculated. ndvi gives an estimate of crop biomass, which is essentially the growth that scientists want to know when comparing different plant varieties or growing systems. “we have found that we can measure crop height accurately from the images. we can also measure ndvi from images, usually we take these measurements on the ground, but the octocopter should be much quicker,” says riche. besides speed, other advantages include measurements that will be more accurate and consistent than if done by the human eye; it also frees up time for research assistants to tackle other projects — 16 minutes flight time could collect maturity data that would take four hours to collect in the normal way. however, some time is spent preparing for flying and then downloading and extracting data post-flight. the near infrared camera (nir) can take images from which a measure of crop growth can be calculated. image: rothamsted research crunch time the process by which individual photographs taken on the copter’s boom are translated into growth data demonstrates just how far imaging technology has developed since the advent of the first digital cameras. in the first step, images are fed into software that aligns them to be geometrically uniform and true to scale in a process called orthorectification . “this means it makes them appear that they were taken parallel to the ground even if the camera was at an angle when the picture was taken,” riche explains. over 500 photos from one octocopter flight can be made into this type of orthomosaic image. image: rothamsted research the images are then added around each other into a large mosaic. with up to 500 images of a single experiment, there’s a lot of overlap and the software ( agisoft photoscan ) stitches the images together to make one large orthomosaic. next, the image is geolocated so it is located in real space and has proper scale, and this means meaningful measurements can now be taken from the image. the geolocation uses permanent markers (‘ground control points’) located within the experimental area, and accurately located by precision gps — accurate to an astonishing 1cm. “finally the software produces a three dimensional model of the imaged area, using the large amount of individual images to build the 3d model,” says riche. “from the 3d model the height of the crop can be extracted, with different values for each individual plot.” computer software can make up a mosaic from which growth data can be extracted. image: rothamsted research legislating for the future besides data on growth, crop colour could also be an important indicator. plant pathogens such as wheat rusts and certain viruses cause a characteristic yellowing of the crop. in the future, analysis of images could pinpoint areas for treatment with pesticides or biological agents like natural predators. there are many other uses for drones in agriculture. in other countries farmers are using them to sow seeds and apply pesticides, and in forestry and national parks there are many applications, particularly in essential monitoring over very large areas. andrew riche of rothamsted research inspects the octocopter’s rotors before flight. image: bbsrc it’s not too hard to see a future where an automated drone could search fields for areas of crop damage from pests, or see nutrient- or water-stressed plants from above that the human eye would miss. the drone could then feed the gps co-ordinates to a farm worker, who could then manage the appropriate response. as the physical capability of drones increases, as well as reliability, you have to wonder if the drones themselves could not spray pesticides and drop fertilizer pellets in the right space and in ultra-low quantities, negating the need to treat entire fields with heavy tractors that can compact the ground and damage soil. but riche doesn’t think all these uses are likely to happen too soon, at least in the uk. “there are currently companies offering a service to farmers for monitoring fields by uav, however this is a specialist area at present, as current legislation makes it difficult to operate uavs over a large area,” he says. “and i think we are a long way from seeing seeds or pesticides being applied by uav, however i think they have an important role in monitoring field experiments.” screenshot from a laptop computer logging an octocopter test flight: image: bbsrc automatic for the people indeed, many people might feel uncomfortable with this level of automated capability, even with clear evidence of environmental benefits. in fact, just getting the octocopter in the air requires strict adherence to a number of regulations. riche and his co-pilot research assistant march castle cannot fly (manually or under gps) within 50m of people not under their control. and they cannot fly higher than 122m, or more than 500m from the operator. “as the aircraft is greater than 7kg and we are within controlled airspace by being close to luton airport, we have to ring air traffic control each day that we fly to get permission,” says riche. becoming familiar with myriad regulations in so many different areas — everything from piloting skills to technical knowledge of multi-rotor uavs, to gauging tomorrow’s weather (no, it can’t fly in rain) — has made the project frustrating at times for the researchers. but as the camera and software systems have come together, there has been a the satisfying knowledge that good data can be collected efficiently using the uav, as well as the tantalising expectation of what could come next. riche and research assistant march castle, also of rothamsted research, make preparations before a test flight. image: bbsrc the octocopter has a modular boom, meaning imaging upgrades and new technologies could be bolted on relatively easily. these could include a hyperspectral camera that can measure reflected light at many more wavelengths than conventional cameras, used by the oil industry to locate deposits, or lidar (light detection and ranging) for more accurate 3d images which has been used to map the hidden sites of stonehenge. and at £24,000 for the whole system with cameras sounds expensive, until you compare it to the £120,000 cost of a modern gps-enabled tractor, or just the human hours needed to record information in large-scale experiments. just as drones might come to become a regular part of life in everything from postal delivery to monitoring and capturing sport and leisure activities, so the octocopter might well demonstrate a new, enabling capability for modern ‘big data’ agricultural bioscience. full project team: dr malcolm hawkesford, andrew riche and march castle, rothamsted research; prof. martin wooster, fenner holman, king’s college london; dr adam michalski and dr grzegorz kulczycki, uniwersytet przyrodniczy we wrocławiu. octocopter: flight-al statistics * rotors: 8 * cost (unit): £12, 000 for the octocopter * cost (for whole system with cameras and flight controller): £24, 000 * weight: 9kg inc payload and batteries * range on one full charge: 8km * flying time on one full charge: 13 min * ceiling: 122m, legally * payload weight: 2.5kg * time to recharge batteries: 1 hr * camera resolution 1cm/pixel @ 45m altitude * camera options rgb, thermal infrared, multispectral drones technology research -- -- 1 written by bbsrc 1.1k followers · 133 following biotechnology & biological sciences research council: investing in world-class bioscience research & training on behalf of uk public. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/aspen-ideas/sleeping-through-a-revolution-8c4b147463e5,,,"sleeping through a revolution. letter to the millenials 2 | by jonathan taplin | aspen ideas | medium sitemap open in app sign up sign in medium logo write sign up sign in aspen ideas · thought-provoking conversations from the aspen ideas festival. presented by the aspen institute and the aspen journal of ideas. visit us at aspen.us/journal sleeping through a revolution letter to the millennials 2 jonathan taplin 19 min read · apr 22, 2015 -- 10 listen share in my last letter , i told you there was a time in the late ’60s when the most critically acclaimed movies and music were also the best selling. the beatles’s sgt. pepper album or francis coppola’s godfather film were just two examples. i said that that is not happening anymore, and i wanted to explore with you why this change occurred. because i spent the first 30 years of my life producing music, movies, and tv, this question matters to me, and i think it should matter to you. so i want to explore the idea that the last 20 years of technological progress — the digital revolution — have devalued the role of the creative artist in our society. i undertake this question with both optimism and humility. optimism because i believe in the power of rock and roll or movies to change lives. certainly witnessing bob dylan go electric at the newport folk festival in 1965 changed this princeton freshman who was intent on being a lawyer into a passionate follower of the rock and roll circus who managed to make a good living from the entertainment business for the rest of his life. my optimism also showed itself in 1996 when i helped found the first streaming video on demand service, intertainer. anyone crazy enough to found a service that needed broadband in 1996 had to be an optimist. that led to humility, because the diffusion of broadband was much slower than i thought, so i know that predicting the future is a humble man’s game. in the last few years i have run the annenberg innovation lab at the university of southern california. at the risk of biting the hand that feeds me, i confess that i often feel like a cork tossed into a rushing technology stream. while i have no doubt of the wonders of the internet revolution, i think it’s time to take stock of where this stream is carrying us. we have become convinced that only machines and corporations make the future, but i don’t think that is true. in thinking about the role of the humanist in our technology-driven future, i was drawn to a sermon martin luther king preached at the national cathedral in washington two weeks before he was killed. at the outset he told the story of how rip van winkle had passed a sign with a picture of king george iii of england on the way up the mountain where he fell into a long sleep. when he came down the mountain, the same sign bore a picture of george washington. rip van winkle this reveals that the most striking thing about the story of rip van winkle is not merely that rip slept 20 years, but that he slept through a revolution. while he was peacefully snoring up in the mountain, a revolution was taking place that would change the course of history — and rip knew nothing about it. he was asleep. yes, he slept through a revolution. one of the great liabilities of life is that all too many people find themselves living amid a great period of social change, yet they fail to develop the new attitudes, the new mental responses, that the new situation demands. they end up sleeping through a revolution. i doubt that anyone would quarrel with the notion that the last 20 years of technological disruption have constituted a revolution, but i want to understand just who has been sleeping through this revolution and who has been awake, creating the moral, political, and technical architecture of the world our children will inhabit. the beginnings of the “cyber revolution” that king referenced in his sermon were already moving forward in 1968 as he was speaking. the origins of that technology revolution were clearly located in the counter-culture, as fred turner and john markoff have shown, and the idea (in nicholas negroponte’s words) was to “decentralize control and harmonize people.” that the earliest of networks, like the whole earth lectronic link (well) organized by stewart brand, grew directly out of the hippie culture was a natural progression from both the political and cultural growth of ’60s counter-culture. but within 20 years, starting with peter thiel’s cohort at stanford university, the organizing philosophy of silicon valley was far more based on the radical libertarian ideology of ayn rand than the commune-based notions of ken kesey and stewart brand. thiel, the founder of paypal, an early investor in facebook, and the godfather of the paypal mafia that currently rules silicon valley, has been clear about his philosophy. paypal mafia he stated, “i no longer believe that freedom and democracy are compatible,” his reasoning being: “since 1920, the vast increase in welfare beneficiaries and the extension of the franchise to women — two constituencies that are notoriously tough for libertarians — have rendered the notion of ‘capitalist democracy’ into an oxymoron.” this is a pretty extraordinary statement, and i have reread the interview he gave to the cato institute several times. it appears that most women, in thiel’s judgment, fall into ayn rand’s notion of “takers” as opposed to thiel’s vaunted male “makers.” so for thiel, in a true “capitalist democracy” only the makers should get to vote, and the women and the welfare recipients will take what the makers chose to give them. whew! it gets worse. thiel has made it clear that his preference (along with google ceo larry page) for a “free cities” model — in which polities are privately owned and unregulated by states — would be an ideal way for capitalists to avoid the “mob mentality” of democracy. he has even suggested that companies could set themselves up off shore (out of the reach of government) on platforms that would give them true freedom to operate. seasteading like amazon’s jeff bezos, he has built his fortune on enterprises that were not taxed or regulated. he also does not believe in competition, having stated in the wall street journal that “competition is for losers. if you want to create and capture lasting value, look to build a monopoly.” peter thiel, larry page, jeff bezos, and mark zuckerberg have not been asleep at the revolution, as their inclusion near the top of the forbes 400 list of america’s billionaires will attest. who has been asleep is the creative class of america: the people who make the music, movies, tv, magazines, newspapers, and video games that have been america’s unique contribution to global culture. since the introduction of napster in 2000, global recorded music revenues have fallen from $21 billion to $7 billion per year. newspaper ad revenue has fallen from $65 billion in 2000 to $18 billion in 2011. book publisher operating profits have fallen by 40 percent, and the revenue from dvd sales of movies and tv (of the top 100 titles) has fallen from $7 billion to $2.3 billion. the astonishing fact of the precipitous declines in revenue has nothing to do with the idea that people are listening to less music or watching fewer movies and tv shows. in fact, all surveys point to the opposite. consumption of all forms of media is rising. so where did the money go? two places: into the pockets of digital monopolists and digital thieves. while the revenues of movie, music and news purveyors were falling rapidly, the revenues of the “internet platform” providers were exploding. google’s revenue went from $1.2 billion in 2001 to $66 billion in 2014. amazon went from $4.8 billion in 2002 to $89 billion in 2014. apple went from $7 billion in2002 to $199 billion in 2014. one could argue that a massive reallocation in the order of $50 billion a year from creators and owners of content to platform owners has taken place since 2000. make no mistake, while the movie studios, record companies, newspapers and magazines operate in a very competitive environment, the platform providers are monopolists or, at least, oligopolists. competition is for suckers, and by now negroponte’s notion of decentralization and harmony has been replaced by thiel’s beneficent monopoly. but this does not account for the role of the digital bandits. there is a wonderful picture of kim dotcom, who made $200 million in two years off of the stolen music and video of countless artists on his megaupload pirate site. in the picture, the fat german thief stands on a picturesque beach with his “playmate of the year” girlfriend sprawled on the sand in the foreground and his 200-foot mega yacht in the background. kim, in an attempt to fight the lawsuits against him, has appropriated the message of martin luther king, assuming the stance of the man who freed everyone from the slavery of having to pay for creative works. exactly how the hard work of artists got exempted from the notions of the market economy escapes me, but for kim to pose as some new sort of freedom rider brings us back to the whole fallacy of the libertarian economy. ayn rand’s most famous quote is “the question isn’t who is going to let me, it’s who is going to stop me.” this is how kim dotcom has functioned from day one. the larger question then becomes, who enables the kim dotcoms of the world? type into your google search box the words “watch (your favorite movie) online free” and you will have the answer. google is the enabler for the digital bandit economy. whether it is illicit drugs, stolen music, or jihadist lessons on how to blow up an airplane, google, with a 70 percent market share of all global searches, is the beginning point of a great deal of online criminal activity. of course, for most of your generation, the idea of getting your music or movies for free from pirates like kim dotcom doesn’t seem like a big deal. but you are studying at usc to (hopefully) become the next generation of journalists, filmmakers, and musicians, so the future of the business is in your hands. somehow you have to let go of the idea that this is a victimless crime. as i said to you in my last letter, i’m not worried about jay z or beyoncé. i’m worried about the middle class musician, the journeyman that used to be able to ply his trade and make a living selling 25,000 cds. that does not exist anymore. but perhaps this tolerance for criminals like kim dotcom is part of a larger problem. as the associate chancellor of uc berkeley, nil gilman, has written, we are plagued by a twin insurgency: from below comes a series of interconnected criminal insurgencies that route around states and seek ways to empower and enrich themselves in the shadow of the global economy. from above comes the plutocratic insurgency, in which globalized elites seek to disengage from traditional national obligations and responsibilities. just how we distinguish between the criminal, the warlord, and the rogue state actor will become harder, as robert kaplan pointed out many years ago in his prescient book, the coming anarchy. what was the nature of the massive hack on sony this fall? will we ever know if it was a state-sponsored act or that of an angry laid-off employee? the on-rush of the much-heralded “internet of things” will make the possibilities of cyber crime even more profitable. imagine the now-prevalent cyber blackmail (“pay me $1000 to unlock your data”) played out on a larger scale: “pay me $200 million to bring the los angles department of water and power back on line.” forbes has reported a software program being sold on the dark web that can ostensibly hack into the “connected car” and take over the acceleration and braking functions. here we run into the tricky area of free speech. google says it can filter out child porn from youtube but not the jihadist videos from anwar al-awlaki that were the entrance point of the charlie hebdo terrorists into the al-awlaki network. anwar al-awlaki just where google draws the line seems important. should they block al-awlaki’s inspire online magazine, which last month published detailed instructions on how to build a bomb that could pass through airport screening undetected? i don’t really have the answers, but i hope we can begin to have a dialogue around this issue. abraham lincoln supposedly was the first to say, “the constitution is not a suicide pact.” certainly the fact that there are 3000 isis videos on youtube and 10,000 isis accounts on twitter should give you pause. clearly this is a tricky area, and i don’t believe this is necessarily a matter for government regulation. i do, however, think that google might alter its “don’t be evil” motto to “don’t enable evil.” the use of their automated content identification technology could be employed to filter content being uploaded to their servers before it is ever displayed on youtube. but then they couldn’t be selling paper towels in front of isis videos. at this point you might be asking why the loss of billions in the media and entertainment sectors is worth worrying about in the face of the benefits ubiquitous internet technology has brought you. my feeling is that media is just the canary in the coal mine, and that in the next 20 years, millions of the jobs you are training for might be automated. the economist recently ran an article in which they projected the probability of your job being taken by a robot in that time period. citing work from two oxford university economists, they wrote that “jobs are at a high risk of being automated in 47 percent of the occupational categories into which work is customarily sorted.” replaced by robots larry summers recently said that those who think that the answer to the jobs crisis is just higher education are “whistling past the graveyard.” i cannot imagine that your parents spent $40,000 a year at this university to prepare you to be part of the new “sharing economy.” what a life awaits you. you can loan your car out on relay rides or become an uber driver. if you can afford a house, you can rent your extra room out on airbnb and find extra work on taskrabbit. we are only a few years into the sharing economy, but one thing is clear: as with google, most of the economic gains will flow to those who own the platform rather than to those who do the work. uber is currently valued at $41.2 billion, making it one of the 150 largest corporations in the world. that’s a capitalization larger that delta airlines or fedex, all built on ayn rand’s motto: “the question isn’t who is going to let me, it’s who is going to stop me.” with such economic power comes political power. uber recently hired obama campaign svengali david plouffe to help it navigate the political lobbying waters of washington, taking a page from google’s bible. google outspends all but a few financial and military firms in its lobbying efforts. the main financial backers of the libertarian movement, the koch brothers, have vowed to spend $900 million in the 2016 election cycle to ensure that the “no regulation, no taxes” principles of the movement are sacrosanct in the corridors of power. the digital monopolists are not above using the rhetoric of libertarianism to spread the message that they alone are the guardians of freedom in the world. when the media companies tried (in an admittedly ham-handed fashion) to pass a law (stop online piracy act) that would require google to block sites that were making millions off of stolen content, google unleashed an online campaign stating this would amount to censorship. the uproar from the crowd quickly killed the bill. perhaps it is time to ask the question of who benefits from this technological revolution. who is awake and steering the boat, and who is asleep below decks? as the economist pointed out, the ability to substitute capital for labor has profound implications for the future of our society. in early 1929, before the stock market crash that set off the great depression, the top 1 percent earned 23.9 percent of national income. by 1976, because of 30 years of changes in tax policy as well as regulation, the 1 precent earned 8.9 percent of national income. but the reagan era reversed both the tax and regulatory policies that had brought on more income equality, and today the top 1 percent earn 24.2 percent of national income. what is clearly visible is that the great productivity gains brought on by technology, which used to benefit the ordinary workers’ paycheck, now only flow to the owners of capital. the work of thomas pinketty, the french economist, shows that this growing income inequality will only get worse in the coming years. the irony is the john maynard keynes envisioned this substitution of capital for labor in the ’30s but thought that the result would be an average work week of 15 hours, with a great deal of paid leisure for the common man. it did not work out that way, but it does seem that your generation is going to have to begin grappling with the possibilities of a new normal where there is not enough work for 30 percent of the population. some have suggested a guaranteed income, but without some discussion, we risk a kind of social unrest that we have not experienced since the ’60s. if the technology revolution has failed to bring the average worker increased prosperity, it has also created a vast new set of industries built on mining that worker’s most private data, with questionable return benefits. by the end of 2016 there will be 5 billion smartphones in the world, every one of them a vast treasure trove of personal data that can be mined by both the surveillance state and the corporate data brokers who sell your digital life for immense profits. this is where martin luther king’s “asleep at the revolution” metaphor seems most telling. clearly the current corporate narrative about privacy is that it is a sort of currency to be traded to corporations in return for innovation. but georgetown university professor julie cohen argues that privacy has meaning in and of itself. jonathan sadowski describes her argument in the atlantic : what cohen means is that since life and contexts are always changing, privacy cannot be reductively conceived as one specific type of thing. it is better understood as an important buffer that gives us space to develop an identity that is somewhat separate from the surveillance, judgment, and values of our society and culture. privacy is crucial for helping us manage all of these pressures — pressures that shape the type of person we are — and for “creating spaces for play and the work of self-[development].” cohen argues that this self-development allows us to discover what type of society we want and what we should do to get there, both factors that are key to living a fulfilled life. do you think your shrinking zone of privacy is altering your behavior? are the pressures of social media keeping us from finding this fulfilled, authentic life? what keeps us asleep at the revolution? could it be that drinking from the firehose of big data is a sort of deep distraction that prevents us from even asking the humanistic questions of what makes for an examined, authentic life? in the ’30s aldous huxley imagined a future in his brave new world — a future in which the average citizen would take his dose of soma (a kind of hybrid prozac/viagra) and go out to the feelies, a form of entertainment so all-engrossing that the “prole” never had any sense that he was not a free human. chris sullentrop of the new york times recently reported that several experts told him the virtual reality porn was going to be the killer app. huxley would smile. after i gave a speech on this topic, i got a note from jimmy bartz, the minister at the small episcopal church, thad’s , that i attend. he, too, agrees that we are asleep at a revolution: however, i don’t believe enough people can be inspired to endure what it will take to change things if they are “asleep.” you mention the uptick in opiate addiction. there are also soooo many more people on mood altering medication (some need it, but not as many as take it), then, we have food, credit, media, devices. our ability to endure what we’d have to endure to create the change you espouse (and i espouse) has atrophied to the point that we don’t even have consciousness around it. there’s a level of “insobriety” we’ve never experienced before — data, food, pharma, credit — we’re so doped up on that stuff that we’ll never have the will to legislate the change, and washington’s too doped up on cash to have the will to do the right thing. this sense that we are too doped up to care is distressing, but what’s more worrying is that we are building whole sectors of the digital economy on the concept of addiction. i recently picked up a book called hooked: how to build habit forming products, in which the author shows how you too can build the latest snapchat. the schematic of a “trigger, action, reward, investment” sequence is curiously close to that of the skinner box we all studied in psych 101. hooked like the poor lab rat in pursuit of happiness clicking on the bar for a reward pellet, we spend hours looking for the “like” reward on our social networks. those with the most likes can turn it into currency, as was demonstrated at the myriad “gifting suites” at the sundance film festival, where popular youtubers like ijustine collected thousands of dollars worth of free merchandise in exchange for posting about the swag on their social networks. ijustine, whose fames stems from having posted a video on youtube about her 300-page iphone bill, noted to the new york times , “i love products, and i love sharing if i love something. like, you can probably guarantee that it’s going to be posted, especially if i love it.” it would be easy to diss ijustine’s blurring of the line between her own opinion and what she is getting paid to like if it weren’t the basic currency of the internet age. what is any hip hop star but a walking product placement opportunity? how would the tv and movie business survive without “brand integration” dollars to top up the budget? and how would vox, buzzfeed or even the vaunted atlantic survive without the “native advertising” that totally blurs the line between editorial content and paid advertising? if indeed the author of hooked is on to something, and we really are building powerful addictions to social network apps, then is peter thiel’s almost-spiritual commitment to “liberty” really the same as thomas jefferson’s “life, liberty and the pursuit of happiness”? i don’t think so. is your friend who spends three hours a day on snapchat really free? what agency do any of us have in our relationship with facebook? here i am just a guilty as you. i surrender all of my personal data to facebook in return for the ability to post my vacation photos to my friends. i have no doubt that innovative developers can continue to build addictive products. just try to walk down any sidewalk in this university while you constantly dodge people staring at their smartphones. i’ve told you that the innovation lab has studied twitter and politics, and what we found was pretty disturbing. the anonymity that twitter provides a shield that brings out the worst in humans. plato told a tale of the ring of gyges that when put on would render you invisible. he asked the question: if we were shielded from the consequences of our actions, how would that change the way we act? we know the answer. as david brooks says, we have created a “coliseum culture” in which some new celebrity gets thrown to the lions on a weekly basis. of course, i know that writing you to resist the instant riches that might flow to you if you invent the newest addictive app, like yik yak, which allows students to shame each other anonymously, is probably a fool’s errand. my deeper question comes from my position as a professor here for the last 12 years, where i have watched the lure of silicon valley grow stronger. if the best and the brightest of you are drawn to building addictive apps rather than making great journalism, important films, or literature that survives the test of time, will we as a society be ultimately impoverished? i was lucky enough to be involved with some artists like bob dylan, the band, george harrison, and martin scorsese, whose work will surely stand the test of time. i’m not sure i know what the implications are of the role-model shift from rebel filmmaker to software coder. this brings me back to the question of what the tech plutocrats mean by freedom. martin luther king led the march on washington for “jobs and freedom.” it’s obvious now that the new freedom brought to us by the libertarian elite will not come with jobs. the fact that facebook generates revenues of $8 billion with less than 9000 employees speaks volumes. is peter thiel’s idea of corporations, free to reap monopoly profits free from government regulation, what we want for our country? thiel’s icon ayn rand defines freedom as “to ask nothing. to expect nothing. to depend on nothing.” how far is this from jefferson’s great inspiration, the greek philosopher epicurus, who defines the good life in these terms? the company of good friends. the freedom and autonomy to enjoy meaningful work. the willingness to live an examined life with a core faith or philosophy. i worry that our universities are being turned into trade schools in the pursuit of the almighty tech dollar. are we forsaking the humanities and a basic liberal arts education all in promise to prepare students for the shark tank that awaits them in silicon valley or on wall street? as i said at the outset, i have no answers, but another phrase from dr. king’s sermon calls out to me: “our scientific power has outrun our spiritual power. we have guided missiles and unguided men.” let us not assume that this technological revolution we are living through has but one inevitable outcome. history is made by man, not by corporations or machines. it is time to wake up and begin to think about a digital renaissance. as my colleague ethan zuckerman said, “it’s obvious now what we did was a fiasco, so let me remind you that what we wanted to do was something brave and noble.” your generation does not need to surrender to some sort of techno-determinist future. let’s try and “rewire” (ethan’s term) the internet. millenials creative class aspen ideas festival 2015 -- -- 10 published in aspen ideas 2.8k followers · last published sep 22, 2015 thought-provoking conversations from the aspen ideas festival. presented by the aspen institute and the aspen journal of ideas. visit us at aspen.us/journal written by jonathan taplin 2.3k followers · 1k following director emeritus, usc annenberg innovation lab. producer/author, “mean streets”, “move fast & break things”. new book, “the magic years”, out 3/21. responses ( 10 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
https://medium.com/@hopper_travel/the-notification-problem-50267cbabad2,,,"the notification problem. to “allow” or not to “allow,” that is… | by hopper | medium sitemap open in app sign up sign in medium logo write sign up sign in the notification problem hopper 7 min read · aug 17, 2015 -- 9 listen share to “allow” or not to “allow,” that is the question. the hopper app analyzes billions of flight prices daily to predict how prices will change, and tells you when to buy your tickets. the app can predict — insanely accurately — if you should buy your tickets now or if you should wait, based on future price predictions. if you should wait, you can press “watch” and the app will literally watch the flight prices until it hits its expected “good deal” range (a historically great price), before your departure date, and then it will send you a push notification to buy. whoa. our product team wanted more users to discover and understand the “watch” feature, and in turn, use it more. we were sure our numbers for conversion on this metric had not reached their peak and understanding users’ willingness to watch is an integral part of our product strategy. given that the watch feature has important engagement hooks — i.e., a user who watches trips is more likely (by a significant multiplier) to return to the app and engage with it and other features — we wanted to make sure their first visit is set up properly and that they understood the value. we set out to improve our onboarding flow. the notification problem notifications are really important to this “watch” concept. if the user doesn’t allow notifications for the app, they can’t receive our message that the price is at its expected low, completely undermining the power of the feature. apple requires that you get permission from the user to send them push notifications, and restricts the firing of an ios alert request to one time. if the user presses “allow,” all is well. if not, it is really difficult for a user to change course — they must exit the app, go into settings, find notifications, find the app in a long list of apps, and then set their desired level of notification. no notifications means we could lose the user forever, so we needed to be really careful when triggering this one-time-use modal. any “improvements” would need to take this into account. existing flow the current flow started with three educational screens in an “app tour” format to try and sell the value prop and functionality of the app; after, it sent the user off into the wild to use the app. the problem with this was that reality had proven to us that most users didn’t want to sit and read even three short blurbs about the app — they wanted to get straight to doing. the content of these types of onboarding screens is often very general and detached from the actual places and interactions the user would be in or have with the user interface (ui). one of these screens included our pitch for the watch feature, but it said very little about the feature’s ui or notifications explicitly, nor their relationship to each other and ios alerts. if users weren’t reading these messages, because there were too many and/or it wasn’t useful or timely information, that user would likely also not understand the watch feature properly and its relationship to notifications, and thus not allow the notifications when prompted. onboarding = notifications we decided we would cut back on the initial tour screens, landing on two more practical messages: the first as a welcome and context to the overall app, and the second to focus on the value of notifications. before the user lost interest, we would then let them into the wild of the app, contextualizing the rest of the content into small tool tips during the actual use, to improve its timeliness and usefulness. i iterated quickly by sketching, with the top series of screens in the sketches below reflecting our initial solution: translated to our app’s ui, it looked like the below: for the notifications screen, we decided to test only one path forward: pressing the “allow push notifications” button, triggering the ios notification alert. our hypothesis was that even though this was an aggressive gate to progression, that the button would (1) put more pressure on the user to read the tour copy before progressing, i.e., “what am i agreeing too?”, which may (2) increase awareness of the relationship between notifications, watching in the app, and saving money on hopper — but even more importantly, we were curious if it would (3) lead to increased “allows,” despite letting potentially fewer users past onboarding. the ultimate result: a larger proportion of qualified users (users who have allowed notifications, thus benefiting from this powerful app feature) on the app. lesson learned this proved to be too heavy-handed. further down the funnel, conversion metrics greatly improved — a much greater percentage of users were getting all the way though onboarding successfully and watching trips — likely attributed to the in-context tool tips later in the flow. but fewer people were actually getting to that point, as they either (1) dropped off right away on the notifications onboarding screen, or (2) pressed “don’t allow” on the ios alert, even after pressing “yes” on our custom button for allowing notifications, then finding out they couldn’t watch a trip without visiting their settings outside the app. the net of this in our analytics was that we hadn’t lost any qualified users, but we also hadn’t gained any — we hadn’t improved the numbers, we just changed how we were achieving them. the fork the simple lessons for us were that our users would not significantly read the onboarding screens, or even if they did and understood the content, they were not afraid to wait to “allow” us notification access until they had gotten a feel for the app first. for a user to press “allow,” they have to trust us not to abuse our newly granted power and understand the value of how we will use that power. like our tool tips, we hypothesized that triggering the notification alert in context would increase those willing to do it, but we also didn’t want to restrict it until that moment in the app, further down the funnel. we decided to keep the notifications screen but to instead add an “opt-out” button, forking the app to either (1) the users who don’t yet trust us and want to get a look around first, having read the copy or not and not triggering the ios alert — allowing us to trigger it in context later — and (2) the user who were highly likely to press “allow,” triggering the modal. within the existing flow, it looked something like this: natural onboarding this greatly increased the number of users who continued through to the rest of the onboarding process, while maintaining the higher back-of-flow numbers we achieved through the addition of in-context tool tips. success. i came away with resounding support for user onboarding that feels as natural as possible, to not feel as if it is a secondary part of the experience but as if it is part of the very fabric of the app. our tool tips helped bring us closer to that goal, with positive effects on our user engagement. and trust, well, it takes time. our willingness to allow users to take a look around, before being forced into deciding whether to do so or not, helped establish just enough of it. you can find hopper for ios on the app store , and soon for android. -pantelis korovilas ( @pantelisak ), lead product designer at hopper interested in becoming part of the hopper team? click on our jobs page for more information and to apply. design ios ux -- -- 9 written by hopper 4.3k followers · 1.1k following hopper uses big data to predict when you should book your flights & hotels. we’ll instantly notify you when prices drop so you can book travel fast in the app. responses ( 9 ) see all responses help status about careers press blog privacy rules terms text to speech",-1
