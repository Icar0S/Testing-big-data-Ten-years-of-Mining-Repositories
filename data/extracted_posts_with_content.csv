Titulo,Conteudo
Testing data quality with SQL assertions - DEV Community,"Testing data quality with SQL assertions - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Lewis Hemens
for
Dataform
Posted on
Jun 26, 2019
• Edited on
Jul 26, 2019
• Originally published at
dataform.co
Testing data quality with SQL assertions
#
sql
Ensuring that data consumers can use their data to reliably answer questions is of paramount importance to any data analytics team. Having a mechanism to enforce high data quality across datasets is therefore a key requirement for these teams.
Often,
input data sources are missing rows, contain duplicates, or include just plain invalid data
. Over time, changes to business definitions or the underlying software which produces input data can cause drift in the meaning of columns - or even the overall structure of tables. Addressing these issues is critical to creating a successful data team and generating valuable, correct insights.
In this article we explain the concept of a SQL data assertion, look at some common data quality problems, how to detect them, and - most importantly -
how to fix them in a way that persists for all data consumers
.
The SQL snippets in this post apply to Google BigQuery but can be ported easily enough to Redshift, Postgres or Snowflake data warehouses.
What is a data assertion?
A data assertion is a query that looks for problems in a dataset
. If the query returns any rows then the assertion fails.
Data assertions are defined this way because it’s much easier to look for problems rather than the absence of them. It also means that assertion queries can themselves be used to quickly inspect the data causing the assertion to fail - making it easy to diagnose and fix the problem.
Checking field values
Let’s take a look at a simple example.
Assume that there is a
database.customers
table containing information about customers in the database.
Some checks that we might want to verify on the table’s contents include:
The field
email_address
is always set
The field
customer_type
is one of
“business”
or
“individual”
The following simple query will return any rows violating these rules:
SELECT
customer_id
FROM
database
.
customers
WHERE
email_address
IS
NULL
OR
NOT
customer_type
IN
(
“
business
”
,
“
individual
”
)
Enter fullscreen mode
Exit fullscreen mode
Checking for unique fields
We may also want to run checks across more than one row. For example, we might want to verify that the
customer_id
field is unique. A query like the following will return any duplicate
customer_id
values:
SELECT
customer_id
,
SUM
(
1
)
AS
count
FROM
database
.
customers
GROUP
BY
1
HAVING
count
>
1
Enter fullscreen mode
Exit fullscreen mode
Combinining multiple assertions into a single query
We can combine all of the above into a single query to quickly find any
customer_id
value violating one of our rules using
UNION ALL
:
SELECT
customer_id
,
“
missing_email
”
AS
reason
FROM
database
.
customers
WHERE
email_address
IS
NULL
UNION
ALL
SELECT
customer_id
,
“
invalid_customer_type
”
AS
reason
WHERE
not
customer_type
in
(
“
business
”
,
“
individual
”
)
FROM
database
.
customers
UNION
ALL
SELECT
customer_id
,
“
duplicate_id
”
AS
reason
FROM
(
SELECT
customer_id
,
SUM
(
1
)
AS
count
FROM
database
.
customers
GROUP
BY
1
)
WHERE
count
>
1
Enter fullscreen mode
Exit fullscreen mode
We now have one query we can run to detect any problems in our table, and we can easily add another unioned
SELECT
statement if we want to add new conditions in the future.
Creating clean datasets
Now that we’ve detected the issues in our data, we need to clean them up. Ultimately choosing how to handle data quality issues depends on your business use case.
In this example we will:
Remove any rows that are missing the
email_address
field
Set a default customer type if it’s invalid
Remove rows with duplicate
customer_id
fields, retaining one row per
customer_id
value (we don’t care which one)
Rather than editing the dataset directly, we can create a new clean copy of the dataset
- this gives us freedom to change or add rules in the future and avoids deleting any data.
The following SQL query defines a view of our
database.customers
table in which invalid rows are removed, default customer types are set, and duplicate rows for the same
customer_id
are removed:
SELECT
customer_id
,
ANY_VALUE
(
email_address
)
AS
email_address
,
ANY_VALUE
(
CASE
customer_type
WHEN
“
individual
”
THEN
“
individual
”
WHEN
“
business
”
THEN
“
business
”
ELSE
“
unknown
”
END
)
AS
customer_type
FROM
database
.
customers
WHERE
NOT
email_address
IS
NULL
GROUP
BY
1
Enter fullscreen mode
Exit fullscreen mode
This query can be used to create either a view or a table in our cloud data warehouse, perhaps called
database_clean.customers
, which can be consumed in dashboards or by analysts who want to query the data.
Now we've fixed the problem, we can check that the above query has correctly fixed the problems by re-running the original assertion on the new dataset.
Continuous data quality testing
Assertions should be run as part of any data pipelines to make sure breaking changes are picked up the moment they happen.
If an assertion returns any rows, future steps in a pipeline should either fail, or a notification delivered to the data owner.
Dataform
has built in support for data assertions, and provides a way to run them as part of a larger SQL pipeline.
These can be run at any frequency, and if an assertion fails an email will be sent to notify you of the problem. Dataform also provides a way to easily create new datasets in your warehouse, making managing the process of cleaning and testing your data extremely straightforward.
For more information on how to start writing data assertions with Dataform, check out the
assertions documentation
guide for Dataform’s
open-source framework
, or
create an account for free
and start using Dataform's fully managed Web platform.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Technical Interview Questions - Part 4 - Git + SQL vs noSQL
Giuliana Olmos -
Nov 14
On-Demand Refresh | Materialized Views (Mviews) in ORACLE SQL
Pranav Bakare -
Nov 13
TIL: joining with LIKE
Augusts Bautra -
Nov 1
SQL Leetcode Questions
Nozibul Islam -
Nov 11
Dataform
Follow
Dataform makes it easy to develop and deploy SQL-based operations in your cloud data warehouse. Publish tables, write data tests and automate complex SQL workflows in a few minutes.
Sign up to Dataform
More from
Dataform
Building an end to end Machine Learning Pipeline in Bigquery
#
machinelearning
#
sql
How to write unit tests for your SQL queries
#
sql
#
testing
Consider SQL when writing your next processing pipeline
#
sql
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
dbt for Crowdsourced Data Quality Testing & Alerting at FINN - DEV Community,"dbt for Crowdsourced Data Quality Testing & Alerting at FINN - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Jorrit
for
FINN
Posted on
Mar 21, 2022
• Edited on
Apr 20, 2022
dbt for Crowdsourced Data Quality Testing & Alerting at FINN
#
dbt
#
datawarehouse
#
engineering
#
testing
This article is a sneak peek at how we ensure data quality at
FINN
. It explains why we monitor and alert on data quality, which data quality is essential to us, and how we implemented it via dbt. Furthermore, the article illustrates how it can leverage the distributed knowledge regarding business processes by allowing all teams to contribute to data quality.
The impact of this on FINN
More than 70% of business logic data testing comes from people who are not part of our core data engineering team.
100% of all operational departments at FINN contribute data tests.
So far, hundreds of data tests have been implemented in a decentralized (""crowd sourced"") manner within one quarter.
Background - What is dbt?
""dbt is a transformation workflow that lets teams quickly and collaboratively deploy analytics code following software engineering best practices like modularity, portability, CI/CD, and documentation. Now anyone who knows SQL can build production-grade data pipelines.""
source
Why
FINN
monitors and alerts on data quality.
FINN's
growth in 2021 caused challenges for the entire technical infrastructure.
FINN
has grown 10x in the number of signed subscriptions, and the number of employees doubled in 6 months.
During this growth, data-generating business processes change rapidly to evolve
FINN's
products. This rapid development challenges data warehousing since any change in business logic needs to be considered in the data pipelines to keep analytics valid. Further, moving physical assets (cars) is challenging since many things can go wrong in the non-digital world.
To generate bulletproof analyses of the entire fleet/business state at any point in time, the data engineering team has to build a scalable/successful data platform on which analytics, data science, and machine learning teams can build.
A successful data platform ensures data quality.
One success metric of
FINN's
data platform is the quality of the data we provide to other teams. Hence, we developed a technical solution to test data quality, alert data owners, track data quality over time, and enable all teams to contribute tests.
More specifically, we need to test things like:
Basic data quality: uniqueness, nullness, types, accepted values
Business logic: ""Signed subscriptions need to have fields xyz filled"", ""Cars in state x need to have fields xyz filled with""
The contribution to data quality should be decentralized.
The data engineering team shouldn't be the bottleneck for data quality tests. The experts in business logic are part of mission-based teams. Therefore, each team should implement tests for data generated by their processes.
The requirements we set for the target system.
Req-1: Different teams implement tests in a non-centralized way.
Req-2: Data quality tests should be easy to implement via SQL.
Req-3: We send out notifications to corresponding teams via Slack for each test.
Req-4: We track a history regarding test performance.
The target architecture and execution.
The picture shows the following components (left to right):
model.yml
: yml files that are part of a git repository next to data model definitions. These files are typically used in a dbt project to
document data models and run tests at column or table level
.
FINN DWH dbt GIT Repo
: The git repository contains all of FINNs data models and corresponding documentation.
dbt Cloud Internals
: dbt cloud executes scheduled data transformations to create the data models (tables) (defined in the git repository). Those scheduled data transformations create
job Artefacts
that we fetch via
dbt Cloud API
.
DWH dbt_tests_output
: Tables in our data warehouse that contain details of failing rows.
FINN Data Quality Alerting Service
: An
Integromat
make
scenario that implements some logic to get job artefacts from dbt cloud, get the corresponding failing rows from the data warehouse, and forward those test results to corresponding slack channels.
The execution looks like this:
A scheduled (hourly) dbt cloud job runs
dbt test –store-failures
(running on dbt version v1.0.0).
According to the
model configurations
(.yml files), the tests (SQL/
dbt macros
) are executed on column and table levels. Job artifacts are saved.
The Data Quality Alerting Service (an
Integromat
make
scenario) fetches the job artifacts via
dbt Cloud API
.
The service queries corresponding results (failed rows) from the data warehouse according to the artifacts.
The service sends failing rows to the corresponding team (Slack channel), including a resolution description.
Let's break down the requirements step by step.
✅ Req-1: The central pillar of the system outlined above is dbt. It enables a decentralized data testing workflow (and much more).
dbt
is particularly good at performing data transformations. The tooling around dbt lets our data people work in a development workflow that feels like software engineering. Other teams, which don't focus on data engineering, are also empowered to contribute changes to data transformations. Pull requests, code reviews, data tests, and column profiling (via
Datafold
) ensure changes don't break downstream data models. These features allow us to scale data (test) engineering decentralized across teams.
(Our data stack:
Fivetran
,
BigQuery
,
dbt
, dbt Cloud,
Looker
,
Datafold
)
✅ Req-2: Straightforward SQL to implement the tests.
The following image shows an exemplary data quality test. The configuration specifies the Slack channel to be alerted, while the description should help the corresponding team to resolve failing tests.
The SQL statement describes how to query for wrong rows. If the query returns more than 0 rows, these incorrect rows are reported in the Slack channel and resolved immediately.
{{
config
(
enabled
=
True
,
tags
=
[
'fleet_alert'
,
'fleet_intelligence_data_quality_alerts'
],
description
=
'Test to verify VIN aligning between invoice and fleet car'
)
}}
WITH
cars
AS
(
SELECT
cars
.
vin
AS
fleet_car_vin
,
invoices
.
invoice_vin
AS
invoice_vin
,
cars
.
lookup_financing_partner
FROM
cars
LEFT
JOIN
invoices
ON
cars
.
finn_car_unit_id
=
invoices
.
finn_car_unit_id
)
SELECT
*
FROM
cars
WHERE
invoice_vin
!=
fleet_car_vin
Enter fullscreen mode
Exit fullscreen mode
✅ Req-3: The Data Quality Alerting Service sends out alerts
The service (an
Integromat
make
scenario) frequently looks for new test results via the dbt Cloud API, fetches the corresponding faulty (if any) rows from our data warehouse, and sends them to the appropriate Slack channel. The failing rows are manually checked in the data sources and cleaned up from here on.
✅ Req-4: Daily snapshots of test outputs enable tracking over time
We take daily snapshots of test results, aggregate them, and send
Looker
dashboards to the appropriate teams.
Thanks for reading! 💜
Feel free to reach out on
LinkedIn
or comment here if you're interested in further details.
Contributors & Social Links
Felix - Data Products Manager:
LinkedIn
Shahbal - Data Engineer:
LinkedIn
Philipp - Senior Data Products Manager:
LinkedIn
Alex - VP Data Engineering:
LinkedIn
Jorrit - Tech Lead Data Engineering:
Twitter
,
LinkedIn
Next Steps:
Open-source the implementations
Blogpost: The data stack at
FINN
More articles by
FINN
engineers:
Running a startup with No Code - until when?
How did we tackle Airtable's API limits at FINN?
By the way,
we're hiring
! (also remote/US)
FINN on LinkedIn
.
Among the top 10 LinkedIn startups
in Germany.
Top reviews on Glassdoor
.
Tons to learn + incredible people. 🚀
Thanks to
Johannes Klumpe
and
Gregor Albrecht
for reading drafts of this!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Test Reporting Tools - Exploring the Best
Anudeep -
Oct 7
Testing Laravel with PHPUnit and PCOV
Sergio Peris -
Oct 7
iOS App Testing: A Comprehensive Step-by-Step Guide
Ronika Kashyap -
Oct 7
Cypress and Angular: A Step-by-Step Guide to Efficient E2E Testing
Soumaya Erradi -
Oct 6
FINN
Follow
We make mobility fun and sustainable
Join us on our mission to build the
most popular car subscription platform
and revolutionise conventional car ownership.
Find a job at FINN
Trending on
DEV Community
Hot
What was your win this week?
#
weeklyretro
#
discuss
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Meme Monday
#
discuss
#
jokes
#
watercooler
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
"Data Quality at Scale with Great Expectations, Spark, and Airflow on EMR - DEV Community","Data Quality at Scale with Great Expectations, Spark, and Airflow on EMR - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Cícero Joasyo Mateus de Moura
for
AWS Community Builders
Posted on
Apr 24, 2023
Data Quality at Scale with Great Expectations, Spark, and Airflow on EMR
#
aws
#
programming
#
greatexpectations
#
tutorial
Data quality is one of the biggest challenges that companies face nowadays, as it's necessary to ensure that the data is accurate, reliable, and relevant so that the decisions made based on this data are successful.
In this regard, we have seen several trends emerge, such as the Modern Data Stack that brings data quality as one of the main practices.
The
Modern Data Stack (MDS)
is a set of tools and technologies that help companies store, manage, and learn from their data quickly and efficiently. Concepts such as Data Quality and Data Observability are highlights of the MDS.
This article aims to explore Great Expectations, a data validation tool contained within the MDS, which can be used in conjunction with Spark to ensure data quality at scale.
The code used in this article can be found in
this repository
.
The link to the data docs generated by Great Expectations can also be
accessed here
.
Great Expectations
Great Expectations
(GE) is an
open-source
data validation tool that helps ensure data quality.
With Great Expectations, it's possible to define expectations about your data and check whether they meet them or not.
Some of the existing functionalities include the ability to validate data schema, ensure referential integrity, check consistency, and detect anomalies.
GE is very flexible and scalable, allowing integration into our data pipelines, whether to validate, generate reports, or even prevent the pipeline from advancing by recording inconsistent data in the most ""curated"" layers of the Data Lake.
Some points that we can highlight:
it's possible to create tests for your data directly from Spark or Pandas dataframe;
it's possible to create data documentation in HTML including expectation suites and validation reports;
it's possible to save a set of tests (suite) to be used later (
checkpoints
);
we can use a large number of ready-made expectations or easily create
custom expectations
that meet our test cases;
-it has a
CLI
that simplifies the creation of test cases, or we can generate tests by coding in Python;
it's possible to connect directly to
data source origins
, consequently validating data more quickly.
Practical case with Great Expectations
In this article, we present a scenario that is closest to what we find in our daily lives, so we will work with the following case:
We have data stored in a Data Lake located on AWS S3 and need to verify data quality before the business makes critical decisions based on it.
The dataset used is about product sales from an e-commerce website (Amazon) and tells us a lot about the behavior of that store's customers.
The dataset used in this article is open and can be found on Kaggle at this
this link
.
Using Great Expectations with Spark on EMR
This article will use Great Expectations with Spark to execute test cases.
The Spark environment will be on EMR and Airflow will be the means of orchestrating the jobs that will run.
To facilitate the understanding of the process, we will analyze the architecture design below:
We can highlight the following points:
the Spark code containing all the logic to execute GE with Spark will be stored in S3;
the data is also stored in S3 in CSV format;
the generated data docs will also be stored in an S3 bucket configured for a
static site
;
Airflow will orchestrate the EMR and control the lifecycle of the jobs.
1. Creation of Spark script with Great Expectations
To create the Spark script that contains the test cases, we will divide it into some steps, as follows:
Configuration of the context
The GE context indicates the main configurations to be considered to run the tests.
The following code configures the context through a YAML created by a Python object itself.
datasource_yaml
=
f
""""""
name: my_spark_datasource
    class_name: Datasource
    module_name: great_expectations.datasource
    execution_engine:
        module_name: great_expectations.execution_engine
        class_name: SparkDFExecutionEngine
    data_connectors:
        my_runtime_data_connector:
            class_name: RuntimeDataConnector
            batch_identifiers:
                - some_key_maybe_pipeline_stage
                - some_other_key_maybe_airflow_run_id
""""""
def
create_context_ge
(
output_path
):
context
=
ge
.
get_context
()
context
.
add_expectation_suite
(
expectation_suite_name
=
suite_name
)
context
.
add_datasource
(
**
yaml
.
load
(
datasource_yaml
))
config_data_docs_site
(
context
,
output_path
)
return
context
Enter fullscreen mode
Exit fullscreen mode
The configuration of this context is basically informing that Spark is used to perform the tests, as it could be another scenario, such as the use of Pandas.
Configuring Data Docs
An important point is setting where our
data docs
will be saved. By default, the HTML documentation is generated on the local disk, but for this article, the data docs will be
stored and hosted by S3
.
The destination bucket (
output_path
) is a parameter in the following code, so the script becomes more dynamic and customizable.
def
config_data_docs_site
(
context
,
output_path
):
data_context_config
=
DataContextConfig
()
data_context_config
[
""
data_docs_sites
""
]
=
{
""
s3_site
""
:
{
""
class_name
""
:
""
SiteBuilder
""
,
""
store_backend
""
:
{
""
class_name
""
:
""
TupleS3StoreBackend
""
,
""
bucket
""
:
output_path
.
replace
(
""
s3://
""
,
""""
)
},
""
site_index_builder
""
:
{
""
class_name
""
:
""
DefaultSiteIndexBuilder
""
}
}
}
context
.
_project_config
[
""
data_docs_sites
""
]
=
data_context_config
[
""
data_docs_sites
""
]
Enter fullscreen mode
Exit fullscreen mode
Creation of a Validator
Before adding the test cases, we must configure a
Validator
to indicate the tests as a
Batch Request
.
The Validator already incorporates data validation functions in a built-in way, as we will see later, which makes the creation of test cases much easier and more intuitive.
The code below configures and creates the Validator using the context of our tests and the dataframe containing the data for validation.
def
create_validator
(
context
,
suite
,
df
):
runtime_batch_request
=
RuntimeBatchRequest
(
datasource_name
=
""
my_spark_datasource
""
,
data_connector_name
=
""
my_runtime_data_connector
""
,
data_asset_name
=
""
insert_your_data_asset_name_here
""
,
runtime_parameters
=
{
""
batch_data
""
:
df
},
batch_identifiers
=
{
""
some_key_maybe_pipeline_stage
""
:
""
ingestion step 1
""
,
""
some_other_key_maybe_airflow_run_id
""
:
""
run 18
""
,
},
)
df_validator
:
Validator
=
context
.
get_validator
(
batch_request
=
runtime_batch_request
,
expectation_suite
=
suite
)
return
df_validator
Enter fullscreen mode
Exit fullscreen mode
Creating Test Cases
The most awaited moment has arrived, creating the test cases.
At this stage, the objective is to work with two scenarios of test cases: the first is to run a data profile and the other is to add custom test cases as the business needs.
Data Profile
Data Profile is the process  of examining, analyzing, reviewing, and summarizing datasets to obtain information about the quality of the data.
The GE allows you to create a data profile automatically and very simply.
In this
profile
, information will be generated for all data columns, including tests to check for null values, data types, and the most frequent pattern in each column.
To create a data profile and add it to the test context, you just need to have the following code:
def
add_profile_suite
(
context
,
df_ge
):
profiler
=
BasicDatasetProfiler
()
expectation_suite
,
validation_result
=
profiler
.
profile
(
df_ge
)
context
.
save_expectation_suite
(
expectation_suite
,
suite_profile_name
)
Enter fullscreen mode
Exit fullscreen mode
An important point is that the profile is executed through a Spark object created by GE (
df_ge
), which will be seen later, it differs from the other test cases that will be added next, as they are based on the Validator object (created in the previous step).
Another point to highlight is that a name was used for the test suite of the profile and another for the validator tests, so they will be separated in the data docs, which helps with documentation organization.
Test cases
Now just add the test cases as needed for data validation.
The following code adds the following tests:
Validate if
all desired columns
are in the dataset;
Validate if the
product_id
field has unique and non-null values;
Validate if the
discount_percentage
field contains only values between 0 and 100;
Validate if the
rating
field contains only values between 0 and 5;
Validate if the
product_link
field contains only data with a valid link format, using a regex to validate the pattern.
After adding all desired test cases, save the test suite's expectations:
def
add_tests_suite
(
df_validator
):
columns_list
=
[
""
product_id
""
,
""
product_name
""
,
""
category
""
,
""
discounted_price
""
,
""
actual_price
""
,
""
discount_percentage
""
,
""
rating
""
,
""
rating_count
""
,
""
about_product
""
,
""
user_id
""
,
""
user_name
""
,
""
review_id
""
,
""
review_title
""
,
""
review_content
""
,
""
img_link
""
,
""
product_link
""
]
df_validator
.
expect_table_columns_to_match_ordered_list
(
columns_list
)
df_validator
.
expect_column_values_to_be_unique
(
""
product_id
""
)
df_validator
.
expect_column_values_to_not_be_null
(
""
product_id
""
)
df_validator
.
expect_column_values_to_be_between
(
column
=
'
discount_percentage
'
,
min_value
=
0
,
max_value
=
100
)
df_validator
.
expect_column_values_to_be_between
(
column
=
'
rating
'
,
min_value
=
0
,
max_value
=
5
)
df_validator
.
expect_column_values_to_match_regex
(
column
=
""
product_link
""
,
regex
=
r
'
^https:\/\/www\.[a-zA-Z0-9\-\.]+\.[a-zA-Z]{2,}$
'
,
mostly
=
0.9
)
df_validator
.
save_expectation_suite
(
discard_failed_expectations
=
False
)
return
df_validator
Enter fullscreen mode
Exit fullscreen mode
Running the tests
Now it's time to connect all the dots.
The code below is the main function that will be called by Spark. It reads the data we want and invokes the other functions we discussed earlier to set up and execute the test suites:
def
process_suite_ge
(
spark
,
input_path
,
output_path
):
path_data
=
join
(
input_path
,
'
sales
'
,
'
amazon.csv
'
)
df
=
spark
.
read
.
format
(
""
csv
""
).
option
(
""
header
""
,
""
true
""
).
load
(
path_data
)
df_ge
=
SparkDFDataset
(
df
)
context
=
create_context_ge
(
output_path
)
suite
:
ExpectationSuite
=
context
.
get_expectation_suite
(
expectation_suite_name
=
suite_name
)
add_profile_suite
(
context
,
df_ge
)
df_validator
=
create_validator
(
context
,
suite
,
df
)
df_validator
=
add_tests_suite
(
df_validator
)
results
=
df_validator
.
validate
(
expectation_suite
=
suite
)
context
.
build_data_docs
(
site_names
=
[
""
s3_site
""
])
if
results
[
'
success
'
]:
print
(
""
The test suite run successfully:
""
+
str
(
results
[
'
success
'
]))
print
(
""
Validation action if necessary
""
)
Enter fullscreen mode
Exit fullscreen mode
2. Creating the DAG in Airflow
In this step, it's time to create a DAG in Airflow to run
the tests with GE inside the EMR with Spark
.
We will have the following tasks in our DAG:
create_emr: task responsible for creating the EMR for job execution. Remember to configure the connection with AWS (aws_default) or IAM if you're running Airflow on AWS.
EMR configurations can be found in the project repository
.
add_step: responsible for adding a job to the EMR (step). We will see the configuration of this job (
spark-submit
) later on.
watch_step: an Airflow
sensor
responsible for monitoring the status of the previous job until it is completed, either successfully or with failure.
terminate_emr: after the job is finished, this task terminates the EMR instance allocated for running the tests.
Below is the code for the DAG:
create_emr
=
EmrCreateJobFlowOperator
(
task_id
=
'
create_emr
'
,
aws_conn_id
=
'
aws_default
'
,
job_flow_overrides
=
JOB_FLOW_OVERRIDES
,
dag
=
dag
)
add_step
=
EmrAddStepsOperator
(
task_id
=
'
add_step
'
,
job_flow_id
=
""
{{ task_instance.xcom_pull(task_ids=
'
create_emr
'
, key=
'
return_value
'
) }}
""
,
steps
=
STEPS_EMR
,
dag
=
dag
)
watch_step
=
EmrStepSensor
(
task_id
=
'
watch_step
'
,
job_flow_id
=
""
{{ task_instance.xcom_pull(task_ids=
'
create_emr
'
, key=
'
return_value
'
) }}
""
,
step_id
=
""
{{ task_instance.xcom_pull(
'
add_step
'
, key=
'
return_value
'
)[0] }}
""
,
aws_conn_id
=
'
aws_default
'
,
dag
=
dag
,
)
terminate_emr
=
EmrTerminateJobFlowOperator
(
task_id
=
'
terminate_emr
'
,
job_flow_id
=
""
{{ task_instance.xcom_pull(
'
create_emr
'
, key=
'
return_value
'
) }}
""
,
aws_conn_id
=
'
aws_default
'
,
trigger_rule
=
TriggerRule
.
ALL_DONE
,
dag
=
dag
,
)
create_emr
>>
add_step
>>
watch_step
>>
terminate_emr
Enter fullscreen mode
Exit fullscreen mode
Now I'll detailthe configuration of the job that will be added to EMR to process the tests, which is basically a
spark-submit
.
We can check all the settings in the code below, including the script parameters.
args
=
str
({
'
job_name
'
:
'
process_suite_ge
'
,
'
input_path
'
:
'
s3://cjmm-datalake-raw
'
,
'
output_path
'
:
'
s3://datadocs-greatexpectations.cjmm
'
})
STEPS_EMR
=
[{
'
Name
'
:
'
Run Data Quality with Great Expectations
'
,
'
ActionOnFailure
'
:
'
CONTINUE
'
,
'
HadoopJarStep
'
:
{
'
Jar
'
:
'
command-runner.jar
'
,
'
Args
'
:
[
'
/usr/bin/spark-submit
'
,
'
--deploy-mode
'
,
'
client
'
,
'
--master
'
,
'
yarn
'
,
'
--num-executors
'
,
'
2
'
,
'
--executor-cores
'
,
'
2
'
,
'
--py-files
'
,
'
s3://cjmm-code-spark/data_quality/modules.zip
'
,
'
s3://cjmm-code-spark/data_quality/main.py
'
,
args
]
}
}]
Enter fullscreen mode
Exit fullscreen mode
It's important to highlight that the code that will be executed in Spark is stored in S3, both the
main.py
file that calls the other functions and the
modules.zip
file that contains all the logic for the tests to run.
This coding model was adopted to be scalable and easier to maintain, besides allowing us to easily run Spark in client or cluster mode.
3. Executing the Script on EMR
With the script developed, and the Airflow DAG created, we can now run the tests.
Below is an example of the Airflow DAG that ran successfully:
The following images shows more details about the job successfully executed on EMR:
4. Results
Now it's time to analyze the two results of the executed tests.
The first one is the data docs files saved in the S3 bucket, as shown in the following image:
The second result is accessing the data docs, as shown below:
Remember that the data docs created in this article can be accessed at
this link
.
When accessing the suite with the data
profile
, we have the following result:
And when accessing the suite with the created
test cases
, we have the result below:
Conclusion
Great Expectation is the fastest-growing open-source data quality tool with a highly active community, constantly updated, and several large companies worldwide using it.
With GE, we can easily create test cases for various scenarios that accommodate different datasets and customize tests for our use cases.
In addition to bringing statistical results of tests that we can save and use as desired, it also brings ready-to-use data docs in HTML with a lot of helpful information about data quality.
Great Expectation is an excellent tool with easy integration and management. It uses concepts we already know in the world of Big Data, so it is worth testing and using it daily to mature your Data Governance and Data Quality Monitoring.
Remember:
More than having data available for analysis, it is essential to ensure its quality.
Top comments
(2)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Jay Sen
Jay Sen
Jay Sen
Follow
Location
San Jose, CA
Joined
May 19, 2021
•
Oct 3 '23
Dropdown menu
Copy link
Hide
very detailed blog on using these tools together.
but i think when u talk about performing data monitoring/quality checks via great expectation, we can not expect it be done at scale :) as all such tools/platform are SQL based and would not scale unless heavily provided with compute capacity, neverthless it will do job for basic cases though.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Cícero Joasyo Mateus de Moura
Cícero Joasyo Mateus de Moura
Cícero Joasyo Mateus de Moura
Follow
Data Engineer Tech Lead,
Architecture, development and mentoring in Big Data, national speaker on topics such as Big Data and Data Science.
AWS Community Builder and AWS User Group Leader
Location
Brazil
Education
Big Data and Machine Learning Specialist
Work
Data Engineer Tech Lead
Joined
Apr 19, 2023
•
Oct 24 '23
Dropdown menu
Copy link
Hide
Thanks for the feedback, Jay Sen!
However, scalability depends on execution in distributed computing, which in this case is Spark, with it we will be able to scale our environment for data quality
Like comment:
Like comment:
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
JSDoc: La Guía Definitiva para Documentar tu Código JavaScript
Joaquín Gutiérrez -
Dec 6
anyone wanna join me building my first app?(I'm a beginner bare with me)
Andrew Mclean -
Dec 6
What is new feature of React version19
Anil -
Dec 10
Ceres Search
Robert Mion -
Dec 6
AWS Community Builders
Follow
Build On!
Would you like to become an AWS Community Builder? Learn more about the program and apply to join when applications are open next.
Learn more
More from
AWS Community Builders
Top re:Invent 2024 Videos
#
aws
#
techtalks
#
youtube
#
python
AWS EKS Auto Mode: Automating Kubernetes Cluster Management
#
aws
#
eks
Protect Sensitive Data on AWS: A Beginner’s Guide to Amazon Macie
#
aws
#
cloud
#
security
#
privacy
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Improving Data Quality in ClickHouse Databases with Soda - DEV Community,"Improving Data Quality in ClickHouse Databases with Soda - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Shahab Ranjbary
Posted on
Jan 19
• Edited on
May 22
Improving Data Quality in ClickHouse Databases with Soda
#
clickhouse
#
soda
#
dataquality
#
database
Introduction
Data quality is the bedrock of any data-driven project, ensuring the integrity and reliability of information.
Soda
, a versatile tool catering to Data Engineers, Data Scientists, and Data Analysts, empowers users to conduct data quality tests precisely when and where they need to. This guide merges theoretical insights with practical examples from the
soda-clickhouse
project, showcasing how Soda can significantly enhance data quality in
ClickHouse
databases.
Soda Use Case Guides
1. Testing Data in a Pipeline
Integrate Soda seamlessly into your data pipeline to perform continuous data quality checks. Define checks using SodaCL, include scans at key pipeline stages, and monitor results for early issue detection, ensuring data quality at every step.
2. Testing Data Before Migration
Prioritize data integrity before migration by configuring Soda for both source and target databases. Leverage Soda's reconciliation checks to verify data consistency and resolve any issues identified in pre-migration scans, ensuring a smooth transition.
3. Testing Data in CI/CD
Incorporate Soda into your CI/CD pipeline for automated data quality checks. Configure alerts for failures, seamlessly integrate with GitHub Actions, and maintain reliable data throughout the development lifecycle, fostering a culture of quality in your data-driven processes.
4. Self-Serve Soda
Empower your teams with self-serve capabilities using Soda. Facilitate easy access, encourage collaborative definition of quality checks, enable browser interface usage, and integrate with data catalogs for a holistic overview of dataset health, allowing teams to take ownership of their data quality.
Soda Cloud: Advanced Analytics and Collaboration
When it comes to reviewing scan results and investigating issues, Soda Cloud takes your data quality management to the next level. Here's what you can do:
Review Scan Results:
Access visualized scan results not only in the command-line interface but also through the intuitive dashboard on Soda Cloud.
Set Alert Notifications:
Configure custom alert notifications to stay informed about any deviations or anomalies detected during scans. Proactively address potential data quality issues.
Track Trends Over Time:
Gain insights into the trends of your data quality metrics over time. Track improvements or deviations and make informed decisions about your data pipeline.
Integration with External Tools:
Seamlessly integrate Soda Cloud with your existing messaging, ticketing, and data cataloging tools. Collaborate more effectively by connecting with platforms like Slack, Jira, and Atlan.
Soda Cloud not only enhances the visibility of your data quality but also facilitates collaboration and advanced analytics, making it a powerful companion for managing and improving your data quality standards.
Project Samples:
soda-clickhouse
Explore the
soda-clickhouse
project on GitHub, featuring two insightful samples:
Sample_1: Data Quality in
dim_customer
Table
Dive into practical examples of performing quality checks on the
dim_customer
table within ClickHouse using Soda.
Sample_2: Data Migration Validation
Ensure data integrity during migration from a
MySQL
to ClickHouse data source using reconciliation checks in Soda.
Conclusion
Implementing Soda for data quality assurance provides a comprehensive and effective approach. Whether you are testing data in a pipeline, before migration, in CI/CD, or enabling self-serve capabilities, Soda enhances the reliability and trustworthiness of your datasets. Explore the
soda-clickhouse
project, experiment with the provided samples, and leverage Soda's capabilities to elevate your data quality standards and foster a data-driven culture. Elevate your data quality standards with Soda - making data reliability an integral part of your data journey.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Avoiding Connection Pinning in Lambda and RDS Proxy with NestJS and Proxy Splitting
Atsushi Suzuki -
Nov 2
How to use migrations with Golang
Albert Colom -
Nov 6
Deploying a MongoDB Collection Generator on Kubernetes
Dmitry Romanoff -
Nov 1
Linking PostgreSQL Data Through Time: When Tables Share No Keys
JetThoughts Dev -
Dec 4
Shahab Ranjbary
Follow
Once upon a time, in the rapidly evolving world of technology, a curious and compassionate data engineer embarked on a journey to transform the way we understand and interact with data.
Work
Senior Data Engineer at Snapp!
Joined
Sep 7, 2023
More from
Shahab Ranjbary
Tracking User Account Changes in Real-Time: A Debezium and ClickHouse Implementation
#
debezium
#
clickhouse
#
kafka
#
cdc
Storing and Handling Confidential Data in ClickHouse
#
clickhouse
#
encryption
#
kafka
#
decryption
Universal Data Migration: Using Slingdata to Transfer Data Between Databases
#
slingdata
#
clickhouse
#
postgres
#
datamigration
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How Do You Automate Big Data Testing? Everything To Know - DEV Community,"How Do You Automate Big Data Testing? Everything To Know - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
umaprasad0052
Posted on
Dec 21, 2021
How Do You Automate Big Data Testing? Everything To Know
Big Data has passed the tipping point. It’s no longer a fringe interest being practiced by data scientists. The Big Data industry is currently on track to be worth $77 billion by 2023.
There’s a saying in programming and data analysis – “Garbage In, Garbage Out.” That is to say, your data analytics are only as good as the data that fuels them. This is why Big Data testing is so important.
Testing your Big Data by hand rather defeats the purpose of data-driven strategies in the first place. It might not even be possible to assess all of your data depending on how there is. That’s where Big Data testing tools come into play.
Guide To Big Data Testing
Back in 2018,
92% of businesses
reported wanting to incorporate Big Data automation testing tools by 2020. Clearly, this is something that’s been on tech-savvy business owners’ minds for some time. Luckily, with today’s Big Data testing tools, this is more feasible than ever for businesses of all sizes.
Data testing is fairly simple and straightforward for routine data applications. Repetitive business practices like forms are highly predictable. A simple program would likely be enough to catch any potential errors in structured data.
Much business data is unstructured or semi-structured. It is estimated that around 80% of data collected by businesses is either unstructured or semi-structured like JSON.
Here are some steps you can take to incorporate an automated Cloud Big Data testing tool in your data pipeline.
Incorporate an ETL Testing Tool
At the beginning of your data pipeline, it’s highly recommended you incorporate an extract, transform, and load (ETL) testing tool. An
ETL testing tool
can be configured to monitor an incoming data stream for data relevant to your business.
Once this data is gathered, an ETL testing tool will transform the data into a format suitable for your Big Data cloud platform. Once it’s clean, it’s loaded into your data analytics environment.
Implement A Big Data Testing Strategy
You’ll also want to put a solution in place to make sure your Big Data testing tools are functioning properly. This presents certain challenges when dealing with the monumental amount of data that Big Data involves.
A Big Data testing strategy usually involves putting conditions in place to make sure you’re getting the data you need. Some examples of common data constraints could include:
Conformity
Accuracy
Duplication
Consistency
Completeness
Trying to assess every byte of data could slow your Big Data analytics down to a crawl, however. You’ll also want to decide on the scope for your testing as representative of the entire body. You might test every 10th entry, for instance, and have a subroutine in place of errors rising above a certain rate.
Structure Each Source
To get the most accurate
Big Data testing
, you should configure each data entry point to make sure the data is configured correctly. Say you wanted to collect data from your blog for analysis. Examples of data you might collect from blog posts might include:
Publication data
Wordcount
Time published
SEO metadata
Social shares
You should spend some time figuring out where you want to collect data from when you’re compiling your data testing strategy. Once you’ve got a list of where your data is coming from, you should then think about what data you want to harvest from that particular source.
Taking the time to answer these questions will help you set up your ETL tool properly. When all of these steps have been handled correctly, your Big Data pipeline can truly deliver automated insights!
Consolidate Data
Some of your data streams are likely to contain repeat data or monitor the same assets. Leaving all that data unstructured is going to bog down your data analytics platform significantly. You might want to implement an additional abstraction layer for additional processing.
Say you’re analyzing temperature data from a list of cities. This data might be entered as a pair, as is often the case, with the name of the city acting as the key and the temperature as the value.
Depending on where this data is coming from, these values could be returned at a specified rate. Or if it’s coming from a scientific sensor it might be a string of continuous data. You’ll want to determine the scope of the data you want returned to your testing platform, for starters.
Setting up an additional layer for each city makes this problem relatively simple to solve. All of the data for that particular city would be returned to that city’s specific layer. You can put additional constraints in place to make sure the data is in a usable and useful form.
Say you put a filter in place to only return the highest and lowest temperature from a particular city. Now it doesn’t matter if it’s a continuous stream from a sensor or collected periodically. It ensures that all of your data will work nicely together.
It also makes it so that your Big Data testing platform can receive data from however many sources you like. This is essential for making your Big Data testing solution scalable and adaptable to any solution you apply it to!
These are just a few things to keep in mind to illustrate how the right data testing tools and the proper foresight sets you and your data-driven business up for success. It ensures your data is formatted properly no matter how much there is or where it’s coming from.
Are You Looking For Big Data Testing Tools?
Big Data
is quickly making science fiction become science fact. Disciplines like machine learning and artificial intelligence were still in the realm of sci-fi even 10 years ago. Now they’re available for anybody to benefit from!
If you’re ready to find out how data-driven tools like Big Data testing can empower you and your business,
Sign Up for a Demo today
!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Calendly and Accessibility Problems
Chris  -
Dec 12
Target had to pay $6,000,000 due to web accessibility failures
Chris  -
Dec 12
Comparing Debouncing and Cancellation Tokens: When to Use Each in Web Applications
Victor Olorunfemi -
Dec 12
Recording My Coding Journey
Abhay -
Dec 12
umaprasad0052
Follow
I am staffing author in Technologies like BiG DaTa...i love to share the my knowledge to people and learning from others too..
Location
Virginia, United States
Education
Harvard University
Work
https://www.datagaps.com/
Joined
Dec 21, 2021
Trending on
DEV Community
Hot
Atomic Note-Taking Guide
#
productivity
#
vim
#
neovim
#
obsidian
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
"Apache Spark, Hive, and Spring Boot — Testing Guide - DEV Community","Apache Spark, Hive, and Spring Boot — Testing Guide - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Semyon Kirekov
Posted on
Apr 22, 2022
Apache Spark, Hive, and Spring Boot — Testing Guide
#
bigdata
#
testing
#
java
#
docker
Big Data is trending. The companies have to operate with a huge amount of data to compete with others. For example, this information is used to show you the relevant advertisements and recommend you the services that you may find interesting. The problem with Big Data software systems is their complexity. Testing becomes tough. How could you verify the app behaviour locally when it's tuned to connect to the HDFS cluster?
In this article, I'm showing you how to create a Spring Boot app that loads data from
Apache Hive
via
Apache Spark
to the
Aerospike Database
. More than that, I'm giving you a recipe for writing integration tests for such scenarios that can be run either locally or during the CI pipeline execution. The code examples are taken from
this repository
.
Firstly, let's get over some basic concepts of the Big Data stack we're using. Don't worry, it won't take long. But it's necessary to understand the core idea.
Basics of HDFS
HDFS (Hadoop Distributed File System)
is a distributed file system designed to run on many physical servers. So, a file in HDFS is an abstraction that hides the complexity of storing and replicating the data between multiple nodes. Why do we need HDFS? There are some reasons.
Hardware Failures
Hard disk drives crash. That's the reality we have to deal with. If a file is split between multiple nodes, individual failures won't affect the whole data. Besides, data is replicated in HDFS. So, even after a disk crash, the information can be restored from the other sources.
Really Large Files
HDFS allows building of a network of not so powerful machines into a huge system. For example, if you have 100 nodes with 1TB disk storage on each one, then you possess 100TB of HDFS space. If the replication factor equals 3, it's possible to store a single file with a size of 33TB.
Not to mention that lots of local file systems do not support so large files, even if you have the available disk space.
The Speed of Reading
If you read the file sequentially, it will take you
N
. But if the file is split into 10 chunks between 10 nodes, you can get its content in
N/10
time! Because each node can read the chunk in parallel. So, HDFS is not only about safety. It's about swiftness.
We have omitted the time spend on network communications. But if files are huge, this part is just a fraction.
Basics of Apache Hive
Apache Hive
is the database facility running over HDFS. It allows querying data with HQL (SQL-like language).
Regular databases (e.g. PostgreSQL, Oracle) act as an abstraction layer over the local file system. While Apache Hive acts as an abstraction over HDFS. That's it.
Basics of Apache Spark
Apache Spark
is a platform for operating and transforming huge amounts of data. The key idea is that Apache Spark workers run on multiple nodes and store the intermediate results in RAM. It's written in Scala but it also supports Java and Python. Take a look at the schema below. It's the common representation of the Apache Spark batch job.
Apache Spark loads data from
Data Producer
, proceeds some operations on it, and puts the result to
Data Consumer
(in our case, Apache Hive is data producer and Aerospike is data consumer). Apache Spark application is a regular
.jar
file that contains the transformation logic. Take a look at the example below.
JavaRDD
<
String
>
textFile
=
sc
.
textFile
(
""hdfs://raw_data.txt""
);
JavaPairRDD
<
String
,
Integer
>
counts
=
textFile
.
flatMap
(
s
->
Arrays
.
asList
(
s
.
split
(
"" ""
)).
iterator
())
.
mapToPair
(
word
->
new
Tuple2
<>(
word
,
1
))
.
reduceByKey
((
a
,
b
)
->
a
+
b
);
counts
.
saveAsTextFile
(
""hdfs://words_count.txt""
);
Enter fullscreen mode
Exit fullscreen mode
It's a simple word-count application. Firstly, we load the content of the
raw_data.txt
HDFS file. Then we split each line by
"" ""
, assign
1
for every word, and reduce the result by words to summarize the whole numbers. Then the obtained pairs are saved to
word_count.txt
.
The flow is similar to
Java Stream API
. The difference is that every lambda expression is executed on the workers. So, Spark transfers the code to the remote machines, performs the calculation, and returns the obtained results. If we owe a sufficient number of workers, we can proceed with the amount of data that is measured by terabytes or even zettabytes.
The Apache Spark approach of delivering code to data has some drawbacks. We'll discuss it when we get to the development.
Another important aspect is
laziness
. Just like Stream API, Apache Spark does not start any calculations until terminal operation invocation. In this case,
reduceByKey
is the one. The rest operations build the pipeline rules but do not trigger anything.
Build Configuration
Let's start the development process. Firstly, we need to choose the Java version. At the moment of writing the latest stable Apache Spark release is
3.2.1
. It supports Java 11. So, we gonna use it.
Currently Apache Spark does not support Java 17. Make sure you don't use it for running integration tests. Otherwise, you'll get bizarre error messages.
The project is bootstrapped with
Spring Initializr
. Nothing special here. But the dependencies list should be clarified.
Dependencies Resolution
ext
{
set
(
'testcontainersVersion'
,
'1.16.2'
)
set
(
'sparkVersion'
,
'3.2.1'
)
set
(
'slf4jVersion'
,
'1.7.36'
)
set
(
'aerospikeVersion'
,
'5.1.11'
)
}
dependencies
{
annotationProcessor
'org.springframework.boot:spring-boot-configuration-processor'
implementation
(
'org.springframework.boot:spring-boot-starter-validation'
)
{
exclude
group:
'org.slf4j'
}
implementation
(
""com.aerospike:aerospike-client:${aerospikeVersion}""
)
{
exclude
group:
'org.slf4j'
}
compileOnly
""org.apache.spark:spark-core_2.13:${sparkVersion}""
compileOnly
""org.apache.spark:spark-hive_2.13:${sparkVersion}""
compileOnly
""org.apache.spark:spark-sql_2.13:${sparkVersion}""
compileOnly
""org.slf4j:slf4j-api:${slf4jVersion}""
testImplementation
'org.apache.derby:derby'
testImplementation
""org.apache.spark:spark-core_2.13:${sparkVersion}""
testImplementation
""org.apache.spark:spark-hive_2.13:${sparkVersion}""
testImplementation
""org.apache.spark:spark-sql_2.13:${sparkVersion}""
testImplementation
'org.springframework.boot:spring-boot-starter-test'
testImplementation
""org.slf4j:slf4j-api:${slf4jVersion}""
testImplementation
'org.codehaus.janino:janino:3.0.8'
testImplementation
'org.testcontainers:junit-jupiter'
testImplementation
'org.awaitility:awaitility:4.2.0'
testImplementation
'org.hamcrest:hamcrest-all:1.3'
}
Enter fullscreen mode
Exit fullscreen mode
Core Dependencies
First comes Apache Spark dependencies. The
spark-core
artefact is the root. The
spark-hive
enables data retrieving from Apache Hive. And the
spark-sql
dependency gives us the ability to query data from Apache Hive with SQL usage.
Note that all the artefacts have to share the same version (in our case, it is
3.2.1
). As a matter of fact, the Apache Spark dependencies' version should match the one that runs the production cluster in your company.
All Spark dependencies have to be marked as
compileOnly
. It means that they won't be included in the assembled
.jar
file. Apache Spark will provide the required dependencies in runtime. If you include them as
implementation
scope, that may lead to hard-tracking bugs during execution.
Then we have
aerospike-client
dependency. You have probably noticed that the
org.slf4j
group is excluded everywhere and included as a
compileOnly
dependency as well. We'll talk about this later when we get to the Apache Spark logging facility.
Test Dependencies
And finally, here comes test scoped artefacts. Apache Spark ones are included as
testImplementation
. Because integration tests will start the local Spark node. So, they are required during the runtime. The
slf4j-api
is also the runtime dependency.
Testcontainers
will be used to run the Aerospike instance. The
janino
is required by Apache Spark during the job execution. And we need
Apache Derby
to tune Apache Hive for local running. We'll get to this point soon.
Logging Configuration
Apache Spark applies
log4j
with the
slf4j
wrapper. But the default Spring Boot logger is
logback
. This setup leads to exceptions during Spring context initializing due to multiple logging facilities present in the classpath. The easiest way to solve it is to exclude all auto-configured Spring Boot logging features. That's not a big deal. Anyway, Apache Spark provides its own
slf4j
implementation during the runtime. So, we just need to include this dependency as
compileOnly
. That is sufficient.
Excluding
logback
from the Spring Boot project is easy with Gradle. Take a look at the example below.
configurations
{
compileOnly
{
extendsFrom
annotationProcessor
}
all
{
exclude
group:
'org.springframework.boot'
,
module:
'spring-boot-starter-logging'
exclude
group:
'org.springframework.boot'
,
module:
'snakeyaml'
}
}
Enter fullscreen mode
Exit fullscreen mode
Possible
application.yml
issues
The
snakeyml
exclusion requires special attention. Spring Boot uses the library to parse properties from
.yml
files (i.e.
application.yml
). Some Apache Spark versions use the same library for internal operations. The thing is that the versions required by Spring Boot and Apache Spark differ. If you exclude it from Spring Boot dependency and rely on the one provided by Apache Spark, you will face the
NoSuchMethodError
(Spring Boot invokes the method that is absent in the version provided by Apache Spark). So, I would recommend sticking with the
.properties
format and removing Spring Boot YAML auto-configuration. That will help you to avoid unnecessary difficulties. Take a look at the code example below.
@SpringBootApplication
(
exclude
=
{
GsonAutoConfiguration
.
class
})
public
class
SparkBatchJobApplication
{
public
static
void
main
(
String
[]
args
)
{
SpringApplication
.
run
(
SparkBatchJobApplication
.
class
,
args
);
}
}
Enter fullscreen mode
Exit fullscreen mode
Fat Jar
The result
.jar
is going to submitted to Apache Spark cluster (e.g.
spark-submit
command
). So, it should contain all runtime artefacts. Unfortunately, the standard Spring Boot packaging does not put the dependencies in the way Apache Spark expects it. So, we'll use
shadow-jar Gradle plugin
. Take a look at the example below.
plugins
{
id
'org.springframework.boot'
version
'2.6.3'
id
'io.spring.dependency-management'
version
'1.0.11.RELEASE'
id
'java'
id
'com.github.johnrengelman.shadow'
version
'2.0.4'
}
...
shadowJar
{
zip64
true
mergeServiceFiles
()
append
'META-INF/spring.handlers'
append
'META-INF/spring.schemas'
append
'META-INF/spring.tooling'
transform
(
PropertiesFileTransformer
)
{
paths
=
[
'META-INF/spring.factories'
]
mergeStrategy
=
""append""
}
}
Enter fullscreen mode
Exit fullscreen mode
Now we can run all tests and build the artefact with the
./gradlew test shadowJar
command.
Starting Development
Now we can get to the development process.
Apache Spark Configuration
We need to declare
JavaSparkContext
and
SparkSession
. The first one is the core Apache Spark for all operations. Whilst
SparkSession
is the part of
spark-sql
projects. It allows us to query data with
SQL
(which is quite handy for Apache Hive). Take a look at the Spring configuration below.
@Configuration
public
class
SparkConfig
{
@Value
(
""${spring.application.name}""
)
private
String
appName
;
@Bean
@Profile
(
LOCAL
)
public
SparkConf
localSparkConf
()
throws
IOException
{
final
var
localHivePath
=
Files
.
createTempDirectory
(
""hiveDataWarehouse""
);
FileSystemUtils
.
deleteRecursively
(
localHivePath
);
return
new
SparkConf
()
.
setAppName
(
appName
)
.
setMaster
(
""local""
)
.
set
(
""javax.jdo.option.ConnectionURL""
,
""jdbc:derby:memory:local;create=true""
)
.
set
(
""javax.jdo.option.ConnectionDriverName""
,
""org.apache.derby.jdbc.EmbeddedDriver""
)
.
set
(
""hive.stats.jdbc.timeout""
,
""80""
)
.
set
(
""spark.ui.enabled""
,
""false""
)
.
set
(
""spark.sql.session.timeZone""
,
""UTC""
)
.
set
(
""spark.sql.catalogImplementation""
,
""hive""
)
.
set
(
""spark.sql.warehouse.dir""
,
localHivePath
.
toAbsolutePath
().
toString
());
}
@Bean
@Profile
(
PROD
)
public
SparkConf
prodSparkConf
()
{
return
new
SparkConf
()
.
setAppName
(
appName
);
}
@Bean
public
JavaSparkContext
javaSparkContext
(
SparkConf
sparkConf
)
{
return
new
JavaSparkContext
(
sparkConf
);
}
@Bean
public
SparkSession
sparkSession
(
JavaSparkContext
sparkContext
)
{
return
SparkSession
.
builder
()
.
sparkContext
(
sparkContext
.
sc
())
.
config
(
sparkContext
.
getConf
())
.
enableHiveSupport
()
.
getOrCreate
();
}
}
Enter fullscreen mode
Exit fullscreen mode
SparkConf
defines configuration keys for the Apache Spark job. As you have noticed, there are two beans for different
Spring profiles
.
LOCAL
is used for integration testing and
PROD
is applied in the production environment. The
PROD
configuration does not declare any properties because usually they are passed as command-line arguments in the
spark-submit
shell script.
On the contrary, the
LOCAL
profile defines a set of default properties required for proper running. Here are the most important ones.
setMaster(""local"")
tells Apache Spark to start a single local node.
javax.jdo.option.ConnectionURL
and
javax.jdo.option.ConnectionDriverName
declare the JDBC connection for Apache Hive meta-storage. That's why we added Apache Derby as the project dependency
spark.sql.catalogImplementation
means that local files shall be stored in the Apache Hive compatible format
spark.sql.warehouse.dir
is the directory for storing Apache Hive data. Here we're using temporary directory.
JavaSparkContext
accepts the defined
SparkConf
as the constructor arguments. Meanwhile
SparkSession
wraps the existing
JavaSparkContext
. Note that Apache Hive support should be enabled manually (
enableHiveSupport
).
Creating Apache Hive Tables
When we submit an application to the production Apache Spark cluster, we probably won't need to create any Apache Hive tables. Most likely the tables have already been created by someone else. And our goal is to select rows and transfer the data to another storage. But when we run integration tests locally (or in the CI environment), there are no tables by default. So, we need to create them somehow.
In this project, we're working with one table -
media.subscriber_info
. It consists of two columns. MSISDN (phone number) and some subscriber ID.
Before each test run, we have to delete previous data and add new rows to ensure verifying rules' consistency. The easiest way to achieve it is to declare scripts for table creation and dropping. We'll keep them in the
resources
directory. Take a look at the structure below.
V1_media.hql
Creates
media
database if it's absent.
create
database
if
not
exists
media
Enter fullscreen mode
Exit fullscreen mode
V2__media.subscriber_info.hql
Creates
subscriber_info
table if it's absent.
create
table
if
not
exists
media
.
subscriber_info
(
subscriber_id
string
,
msisdn
string
)
row
format
delimited
fields
terminated
by
','
lines
terminated
by
'
\n
'
stored
as
textfile
Enter fullscreen mode
Exit fullscreen mode
DROP V1__mediatv_dds.subscriber_info.hql
Drops the
subscriber_info
table.
drop
table
if
exists
media
.
subscriber_info
Enter fullscreen mode
Exit fullscreen mode
V[N]
prefixes are not obligatory. I put them to ensure that each new table script will be executed as the last one. It is helpful to make tests work deterministically.
OK, now we need a handler to process those HQL queries. Take a look at the example below.
@Component
@Profile
(
LOCAL
)
public
class
InitHive
{
private
final
SparkSession
session
;
private
final
ApplicationContext
applicationContext
;
public
void
createTables
()
{
executeSQLScripts
(
getResources
(
applicationContext
,
""classpath:hive/ddl/create/*.hql""
));
}
public
void
dropTables
()
{
executeSQLScripts
(
getResources
(
applicationContext
,
""classpath:hive/ddl/drop/*.hql""
));
}
private
void
executeSQLScripts
(
Resource
[]
resources
)
{
for
(
Resource
resource
:
resources
)
{
session
.
sql
(
readContent
(
resource
));
}
}
}
Enter fullscreen mode
Exit fullscreen mode
The first thing to notice is
@Profile(LOCAL)
usage. Because we don't need to create or drop tables in the production environment.
The
createTables
and
dropTables
methods provide the list of resources containing the required queries.
getResources
is the utility function that reads files from the classpath. You can discover the implementation
here
.
So, now we're ready to write the business code!
Business Code
Facade
The core interface is
EnricherService
public
interface
EnricherService
{
void
proceedEnrichment
();
}
Enter fullscreen mode
Exit fullscreen mode
We're expecting that it might have many implementations. Each one represent a step in whole batch process.
Then we have
EnricherServiceFacade
that encapsulates all implementations of
EnricherService
and run them one by one.
@Service
public
class
EnricherServiceFacade
{
private
final
List
<
EnricherService
>
enricherServices
;
public
void
proceedEnrichment
()
{
List
<
EnrichmentFailedException
>
errors
=
new
ArrayList
<>();
for
(
EnricherService
service
:
enricherServices
)
try
{
service
.
proceedEnrichment
();
}
catch
(
Exception
e
)
{
errors
.
add
(
new
EnrichmentFailedException
(
""Unexpected error during enrichment processing""
,
e
));
}
if
(!
errors
.
isEmpty
())
{
throw
new
EnrichmentFailedException
(
errors
);
}
}
}
Enter fullscreen mode
Exit fullscreen mode
We're trying to run every provided enrichment step. If any of them fails, we throw the exception that combines all errors into a solid piece.
Finally, we need to tell Spring to execute
EnricherServiceFacade.proceedEnrichment
on application startup. We could add it directly to the
main
method, but it's not the
Spring way
. Therefore, it makes testing harder. The better option is
@EventListener
.
@Component
@Profile
(
PROD
)
public
class
MainListener
{
private
final
EnricherServiceFacade
enricherServiceFacade
;
@EventListener
public
void
proceedEnrichment
(
ContextRefreshedEvent
event
)
{
final
long
startNano
=
System
.
nanoTime
();
LOG
.
info
(
""Starting enrichment process""
);
try
{
enricherServiceFacade
.
proceedEnrichment
();
LOG
.
info
(
""Enrichment has finished successfully. It took ""
+
Duration
.
ofNanos
(
System
.
nanoTime
()
-
startNano
));
}
catch
(
Exception
e
)
{
String
err
=
""Enrichment has finished with error. It took ""
+
Duration
.
ofNanos
(
System
.
nanoTime
()
-
startNano
);
LOG
.
error
(
err
,
e
);
throw
new
EnrichmentFailedException
(
err
,
e
);
}
}
}
Enter fullscreen mode
Exit fullscreen mode
The
proceedEnrichment
method is being invoked, when the Spring context is started. By the way, only the active
PROD
profile will trigger the job.
EnricherService Implementation
We're going to deal with a single
EnricherService
implementation. It simply selects all rows from the
media.subcriber_info
table and puts the result in the Aerospike database. Take a look at the code snippet below.
@Service
public
class
SubscriberIdEnricherService
implements
EnricherService
,
Serializable
{
private
static
final
long
serialVersionUID
=
10L
;
private
final
SparkSession
session
;
private
final
AerospikeProperties
aerospikeProperties
;
@Override
public
void
proceedEnrichment
()
{
Dataset
<
Row
>
dataset
=
session
.
sql
(
""SELECT subscriber_id, msisdn FROM media.subscriber_info ""
+
""WHERE msisdn IS NOT NULL AND subscriber_id IS NOT NULL""
);
dataset
.
foreachPartition
(
iterator
->
{
final
var
aerospikeClient
=
newAerospikeClient
(
aerospikeProperties
);
iterator
.
forEachRemaining
(
row
->
{
String
subscriberId
=
row
.
getAs
(
""subscriber_id""
);
String
msisdn
=
row
.
getAs
(
""msisdn""
);
Key
key
=
new
Key
(
""my-namespace""
,
""huawei""
,
subscriberId
);
Bin
bin
=
new
Bin
(
""msisdn""
,
msisdn
);
try
{
aerospikeClient
.
put
(
null
,
key
,
bin
);
LOG
.
info
(
""Record has been successfully added {}""
,
key
);
}
catch
(
Exception
e
)
{
LOG
.
error
(
""Fail during inserting record to Aerospike""
,
e
);
}
});
}
);
}
}
Enter fullscreen mode
Exit fullscreen mode
There are multiple points that has to be clarified.
Serialization
Apache Spark applies a standard Java serialization mechanism. So, any dependencies used inside lambdas (
map
,
filter
,
groupBy
,
forEach
, etc.) have to implement the
Serializable
interface. Otherwise, you'll get the
NotSerializableException
during the runtime.
We have a reference to
AerospikeProperties
inside the
foreachPartition
callback. Therefore, this class and the
SubscriberIdEnricherService
itself should be allowed for serializing (because the latter one keeps
AerospikeProperties
as a field). If a dependency is not used within any Apache Spark lambda, you can mark it as
transient
.
And finally, the
serialVersionUID
manual assignment is crucial. The reason is that Apache Spark might serialize and deserialize the passed objects multiple times. And there is no guarantee that each time auto-generated
serialVersionUID
will be the same. It can be a reason for hard-tracking floating bugs. To prevent this you should declare
serialVersionUID
by yourself.
The even better approach is to force the compiler to validate the
serialVersionUID
field presence on any
Serializable
classes. In this case, you need to mark
-Xlint:serial
warning as an error. Take a look at the Gradle example.
tasks
.
withType
(
JavaCompile
)
{
options
.
compilerArgs
<<
""-Xlint:serial""
<<
""-Werror""
}
Enter fullscreen mode
Exit fullscreen mode
Aerospike Client Instantiation
Unfortunately, the Java Aerospike client does not implement the
Serializable
interface. So, we have to instantiate it inside the lambda expression. In that case, the object will be created on a worker node directly. It makes serialization redundant.
I should admit that Aerospike provides
Aerospike Connect Framework
that allows transferring data via Apache Spark in a declarative way without creating any Java clients. Anyway, if you want to use it, you have to install the packed library to the Apache Spark cluster directly. There is no guarantee that you'll have such an opportunity in your situation. So, I'm omitting this scenario.
Partitioning
The
Dataset
class has the
foreach
method that simply executes the given lambda for each present row. However, if you initialize some heavy resource inside that callback (e.g. database connection), the new one will be created for every row (in some cases there might
billions
of rows). Not very efficient, isn't it?
The
foreachPartition
method works a bit differently. Apache Spark executes it once per the
Dataset
partition. It also accepts
Iterator<Row>
as an argument. So, inside the lambda, we can initialize ""heavy"" resources (e.g.
AerospikeClient
) and apply them for calculations of every
Row
inside the iterator.
The partition size is calculated automatically based on the input source and Apache Spark cluster configuration. Though you can set it manually by calling the
repartition
method. Anyway, it is out of the scope of the article.
Testing
Aerospike Setup
OK, we've written some business code. How do we test it? Firstly, let's declare Aerospike setup for
Testcontainers
. Take a look at the code snippet below.
@ContextConfiguration
(
initializers
=
IntegrationSuite
.
Initializer
.
class
)
public
class
IntegrationSuite
{
private
static
final
String
AEROSPIKE_IMAGE
=
""aerospike/aerospike-server:5.6.0.4""
;
static
class
Initializer
implements
ApplicationContextInitializer
<
ConfigurableApplicationContext
>
{
static
final
GenericContainer
<?>
aerospike
=
new
GenericContainer
<>(
DockerImageName
.
parse
(
AEROSPIKE_IMAGE
))
.
withExposedPorts
(
3000
,
3001
,
3002
)
.
withEnv
(
""NAMESPACE""
,
""my-namespace""
)
.
withEnv
(
""SERVICE_PORT""
,
""3000""
)
.
waitingFor
(
Wait
.
forLogMessage
(
"".*migrations: complete.*""
,
1
));
@Override
public
void
initialize
(
ConfigurableApplicationContext
applicationContext
)
{
startContainers
();
aerospike
.
followOutput
(
new
Slf4jLogConsumer
(
LoggerFactory
.
getLogger
(
""Aerospike""
))
);
ConfigurableEnvironment
environment
=
applicationContext
.
getEnvironment
();
MapPropertySource
testcontainers
=
new
MapPropertySource
(
""testcontainers""
,
createConnectionConfiguration
()
);
environment
.
getPropertySources
().
addFirst
(
testcontainers
);
}
private
static
void
startContainers
()
{
Startables
.
deepStart
(
Stream
.
of
(
aerospike
)).
join
();
}
private
static
Map
<
String
,
Object
>
createConnectionConfiguration
()
{
return
Map
.
of
(
""aerospike.hosts""
,
Stream
.
of
(
3000
,
3001
,
3002
)
.
map
(
port
->
aerospike
.
getHost
()
+
"":""
+
aerospike
.
getMappedPort
(
port
))
.
collect
(
Collectors
.
joining
(
"",""
))
);
}
}
}
Enter fullscreen mode
Exit fullscreen mode
The
IntegrationSuite
class is used as the parent for all integration tests. The
IntegrationSuite.Initializer
inner class is used as the Spring context initializer. The framework calls it when all properties and bean definitions are already loaded but no beans have been created yet. It allows us to override some properties during the runtime.
We declare the Aerospike container as
GenericContainer
because the library does not provide out-of-box support for the database. Then inside the
initialize
method we retrieve the container's host and port and assign them to the
aerospike.hosts
property.
Apache Hive Utilities
Before each test method we are suppose to delete all data from Apache Hive and add new rows required for the current scenario. So, tests won't affect each other. Let's declare a custom test facade for Apache Hive. Take a look at the code snippet below.
@TestComponent
public
class
TestHiveUtils
{
@Autowired
private
SparkSession
sparkSession
;
@Autowired
private
InitHive
initHive
;
public
void
cleanHive
()
{
initHive
.
dropTables
();
initHive
.
createTables
();
}
public
<
T
,
E
extends
HiveTable
<
T
>>
E
insertInto
(
Function
<
SparkSession
,
E
>
tableFunction
)
{
return
tableFunction
.
apply
(
sparkSession
);
}
}
Enter fullscreen mode
Exit fullscreen mode
There are just two methods. The
cleanHive
drops all existing and creates them again. Therefore, all previous data is erased. The
insertInto
is tricky. It serves the purpose of inserting new rows to Apache Hive in a statically typed way. How is that done? First of all, let's inspect the
HiveTable<T>
interface.
public
interface
HiveTable
<
T
>
{
void
values
(
T
...
t
);
}
Enter fullscreen mode
Exit fullscreen mode
As you see, it's a regular Java functional interface. Though the implementations are not so obvious.
public
class
SubscriberInfo
implements
HiveTable
<
SubscriberInfo
.
Values
>
{
private
final
SparkSession
session
;
public
static
Function
<
SparkSession
,
SubscriberInfo
>
subscriberInfo
()
{
return
SubscriberInfo:
:
new
;
}
@Override
public
void
values
(
Values
...
values
)
{
for
(
Values
value
:
values
)
{
session
.
sql
(
format
(
""insert into %s values('%s', '%s')""
,
""media.subscriber_info""
,
value
.
subscriberId
,
value
.
msisdn
)
);
}
}
public
static
class
Values
{
private
String
subscriberId
=
""4121521""
;
private
String
msisdn
=
""88005553535""
;
public
Values
setSubscriberId
(
String
subscriberId
)
{
this
.
subscriberId
=
subscriberId
;
return
this
;
}
public
Values
setMsisdn
(
String
msisdn
)
{
this
.
msisdn
=
msisdn
;
return
this
;
}
}
}
Enter fullscreen mode
Exit fullscreen mode
The class accepts
SparkSession
as a constructor dependency. The
SubscriberInfo.Values
are the generic argument. The class represents the data structure containing values to insert. And finally, the
values
implementation performs the actual new row creation.
The key is the
subscriberInfo
static method. What's the reason to return
Function<SparkSession, SubscriberInfo>
? Its combination with
TestHiveUtils.insertInto
provides us with statically typed
INSERT INTO
statement. Take a look at the code example below.
hive
.
insertInto
(
subscriberInfo
())
.
values
(
new
SubscriberInfo
.
Values
()
.
setMsisdn
(
""msisdn1""
)
.
setSubscriberId
(
""subscriberId1""
),
new
SubscriberInfo
.
Values
()
.
setMsisdn
(
""msisdn2""
)
.
setSubscriberId
(
""subscriberId2""
)
);
Enter fullscreen mode
Exit fullscreen mode
An elegant solution, don't you think?
Spark Integration Test Slice
Spring integration tests require a specific configuration. It's wise to declare it once and reuse it. Take a look at the code snippet below.
@SpringBootTest
(
classes
=
{
SparkConfig
.
class
,
SparkContextDestroyer
.
class
,
AerospikeConfig
.
class
,
PropertiesConfig
.
class
,
InitHive
.
class
,
TestHiveUtils
.
class
,
TestAerospikeFacade
.
class
,
EnricherServiceTestConfiguration
.
class
}
)
@ActiveProfiles
(
LOCAL
)
public
class
SparkIntegrationSuite
extends
IntegrationSuite
{
}
Enter fullscreen mode
Exit fullscreen mode
Inside the
SpringBootTest
we have listed all the beans that are used during tests running.
TestAerospikeFacade
is just a thin wrapper around the Java Aerospike client for test purposes. Its implementation is rather straightforward but you can check out the source code by
this link
.
The
EnricherServiceTestConfiguration
is the Spring configuration declaring all implementations for the
EnricherService
interface. Take a look at the example below.
@TestConfiguration
public
class
EnricherServiceTestConfiguration
{
@Bean
public
EnricherService
subscriberEnricherService
(
SparkSession
session
,
AerospikeProperties
aerospikeProperties
)
{
return
new
SubscriberIdEnricherService
(
session
,
aerospikeProperties
);
}
}
Enter fullscreen mode
Exit fullscreen mode
I want to point out that
all
EnricherService
implementations should be listed inside the class. If we apply different configurations for each test suite, the Spring context will be reloaded. Mostly that's not a problem. But Apache Spark usage brings obstacles. You see, when
JavaSparkContext
is created, it starts the local Apache Spark node. But when we instantiate it twice during the application lifecycle, it will result in an exception. The easiest way to overcome the issue is to make sure that
JavaSparkContext
will be created only once.
Now we can get to the testing process.
Integration Test Example
Here is a simple integration test that inserts two rows to Apache Spark and checks that the corresponding two records are created in Aerospike within 10 seconds. Take look at the code snippet below.
class
SubscriberIdEnricherServiceIntegrationTest
extends
SparkIntegrationSuite
{
@Autowired
private
TestHiveUtils
hive
;
@Autowired
private
TestAerospikeFacade
aerospike
;
@Autowired
private
EnricherService
subscriberEnricherService
;
@BeforeEach
void
beforeEach
()
{
aerospike
.
deleteAll
(
""my-namespace""
);
hive
.
cleanHive
();
}
@Test
void
shouldSaveRecords
()
{
hive
.
insertInto
(
subscriberInfo
())
.
values
(
new
SubscriberInfo
.
Values
()
.
setMsisdn
(
""msisdn1""
)
.
setSubscriberId
(
""subscriberId1""
),
new
SubscriberInfo
.
Values
()
.
setMsisdn
(
""msisdn2""
)
.
setSubscriberId
(
""subscriberId2""
)
);
subscriberEnricherService
.
proceedEnrichment
();
List
<
KeyRecord
>
keyRecords
=
await
()
.
atMost
(
TEN_SECONDS
)
.
until
(()
->
aerospike
.
scanAll
(
""my-namespace""
),
hasSize
(
2
));
assertThat
(
keyRecords
,
allOf
(
hasRecord
(
""subscriberId1""
,
""msisdn1""
),
hasRecord
(
""subscriberId2""
,
""msisdn2""
)
));
}
}
Enter fullscreen mode
Exit fullscreen mode
If you tune everything correctly, the test will pass.
The whole test source is available by
this link
.
Conclusion
That's basically all I wanted to tell you about testing Apache Hive, Apache Spark, and Aerospike integration with Spring Boot usage. As you can see, the Big Data world is not so complicated after all. All code examples are taken from
this repository
. You can clone it and play around with tests by yourself.
If you have any questions or suggestions, please leave your comments down below. Thanks for reading!
Resources
Repository with examples
HDFS (Hadoop Distributed File System)
Apache Hive
Apache Spark
Apache Derby
Aerospike Database
Aerospike Connect Framework
Java Stream API
Spring Initializr
Spring profiles
Testcontainers
Gradle plugin shadow-jar
Top comments
(4)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
CitronBrick
CitronBrick
CitronBrick
Follow
Developper
Work
Junior Front End Engineer
Joined
Jan 11, 2021
•
Aug 8 '22
Dropdown menu
Copy link
Hide
RDD
s are oudated since Spark 2.
Dataset
should be used instead.
Note that, before Spark 2.0, the main programming interface of Spark was the Resilient Distributed Dataset (RDD). After Spark 2.0, RDDs are replaced by Dataset, which is strongly-typed like an RDD, but with richer optimizations under the hood.
Spark docs
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
worldofsolitaires
worldofsolitaires
worldofsolitaires
Follow
Joined
May 10, 2021
•
Apr 28 '22
Dropdown menu
Copy link
Hide
Thanks a lot for this detailed and specific guide
super mario bros
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Akarsh-
Akarsh-
Akarsh-
Follow
Joined
May 16, 2023
•
May 16 '23
Dropdown menu
Copy link
Hide
Hi, thanks for explanation. Is there a way we can use spring version 3.0.5+ and spark. I am getting servlet error class servletContainer is not a javax.servlet.Servlet
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Semyon Kirekov
Semyon Kirekov
Semyon Kirekov
Follow
Java team lead. Passionate about giving talks at conferences, writing meaningful articles, and lecturing for students.
Location
Russia, Moscow
Education
Polzunov Altai State Technical University
Work
Java team lead, a conference speaker, and a lecturer
Joined
Sep 20, 2021
•
May 16 '23
• Edited on
May 16
• Edited
Dropdown menu
Copy link
Hide
I think you just need to remove dependency spring-web-starter
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Run devcontainers as a non-root user
Theodor Heiselberg -
Nov 11
AI Test Report - Summarize Failed Tests
SpecialBroccoli -
Nov 5
NextJs, Docker and IP Geolocation
Jorge -
Nov 9
Vedro Hooks
Mickey Streicher -
Nov 28
Semyon Kirekov
Follow
Java team lead. Passionate about giving talks at conferences, writing meaningful articles, and lecturing for students.
Location
Russia, Moscow
Education
Polzunov Altai State Technical University
Work
Java team lead, a conference speaker, and a lecturer
Joined
Sep 20, 2021
More from
Semyon Kirekov
Rich Domain Model with Spring Boot and Hibernate
#
java
#
hibernate
#
jpa
#
springboot
Integration Tests for N + 1 problem in Java
#
java
#
testing
#
docker
#
performance
JUnit 5: link tests with task tracker issues
#
java
#
documentation
#
testing
#
githubactions
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How Fast is SciChart’s iOS Chart? - DEV Community,"How Fast is SciChart’s iOS Chart? - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Andrew Bt
Posted on
May 2
• Originally published at
scichart.com
How Fast is SciChart’s iOS Chart?
#
webdev
#
javascript
iOS Chart Performance Comparison
Updated: March 2019 with v2.5 / Metal Results
Updated results for our iOS Chart Performance Comparison can be seen at the page
iOS Chart Performance: Metal vs. OpenGL
.
In this post, we compare the performance of iOS Charting Libraries including open source iOS chart libraries as well as paid charts. Our goal is to demonstrate the performance comparison between well known iOS Chart libraries, and if you are looking for a feature comparison vs. open source charts, then
please look no further than this article.
Which iOS Chart Libraries do we compare?
In this article we compare the performance and big data capabilities of four of the main iOS Chart libraries. These are:
Core plot
:
a popular open source iOS Chart Library
Charts
:
formerly 'iOS Charts', a Swift port of MPAndroidChart
Shinobi Charts
:
a commercial iOS Chart Component
SciChart iOS:
our own real-time iOS Chart Component.
What do we define as Big-Data?
Read a case study where a developer used iOS Charts to display 100,000 points from heart rate sensors.
Big-Data means tens of thousands to millions of points in a chart or graph. People are starting to create
quite innovative apps which read data from heart-rate sensors
, or plot signals from
many sensors or bluetooth devices simultaneously
, or even just plot lots of recordings from GPS data (running, cycling apps). As they push the envelope of what the iPhone / iPad devices can do, charts or graphs fast become a bottleneck.
What do we mean by Realtime iOS Charts?
Read a case study where a customer created iOS & Android trading apps with streaming updating charts
Real-Time iOS charts are charts which are dynamically updating. Another term for this is 'Streaming iOS charts'.
Realtime iOS Charts are required when data is being downloaded continuously from the web, or continuously pushed from bluetooth or wire-connected sensors.
In modern iOS apps it is becoming increasingly common to see realtime charts, apps such as
heart rate monitors
,
health & fitness apps
,
trading apps
and server or process monitoring apps. We compare the realtime performance of several iOS Chart libraries to show you which perform the best with streaming updates.
How to we measure iOS Chart Performance?
To carry out performance tests we've built a iOS Chart comparison application in Objective-C. This application performs a number of identical tests on the four chart providers:
Core Plot
,
iOS Charts
, Shinobi and SciChart and outputs the results to a CSV file.
Results include refresh rate (FPS - Frames per Second) as well as CPU Usage and Startup Time for each iOS Chart test case. The tests are designed to really stress the chart control and find it’s limits.
What Tests are Run
There are four different test cases run in our iOS Charts Performance Comparison app, under different conditions. They are designed to really really stress the chart, by having huge numbers of series, or points, or many updates per second, with varying thickness of pens / number of pixels filled.
A certain test is run, and FPS, or refresh rate, is measured via the
CADisplayLink
, which fires once after each drawing pass completes.
Test 1: NxM Series Test
In this test, N series of M points are appended to series, then the chart redraws (the same data) as fast as possible by just resizing the Y-Axis for 10 seconds per test.
FPS is measured using the
CADisplayLink
, which fires once after each drawing pass completes.
Areas Stressed
: Big-Data, many (hundreds of) series, many points per series.
Test 2: Scatter Series Test
In this test, N Scatter points (ellipses) are appended to a series, then the chart redraws. Immediately after, the points are updated in a Brownian motion fashion and the chart is drawn again.
FPS is measured using the
CADisplayLink
, which fires once after each drawing pass completes.
Areas Stressed:
Big-Data, updating scatter charts
Test 3: FIFO (Streaming) Series Test
In this test N points are appended to a series, but scrolled from right to left (as new points arrive, old are discarded). SciChart for iOS & Android has a built-in mode to handle this called Fifo data series which is implemented as a circular buffer. Other chart components you simply remove a point for each point added.
FPS is measured using the
CADisplayLink
, which fires once after each drawing pass completes.
Areas Stressed:
Streaming, scrolling charts. Big-data.
Test 4: Append Data Test
In this test N points are appended to a series, then M points are appended between each draw of the chart. The data is random-walk and the chart automatically zooms to fit when data is updated. This gives the appearance of a live streaming realtime chart.
FPS is measured using the
CADisplayLink
, which fires once after each drawing pass completes.
Areas stressed:
Streaming, Realtime Charts. Big-Data.
Test Hardware
We’ve used SciChart iOS v2.0 for this purpose and compared it with the latest version of other iOS Charts on a number of devices. Test results are presented with the iPad 2017 (A9 processor, iOS 10).
The Test Results
You can find the test results of our iOS Chart Performance Comparison below. All test result
numbers are in FPS
(Frames per second –
Higher is Better
), meaning, the average number of redraws per second during the test, as measured by
CADisplayLink
.
Because many of the charts we tested became unstable as dataset size increased, we skip running further tests once a chart has dropped below 5FPS. That's why you see lots of zeros below. It means that this test has been skipped to avoid crashing or failing to run.
The final column (SciChart Speedup) is the speed increase of SciChart iOS vs. the other iOS Charting Packages, e.g. 11.52 means SciChart was 11.52x faster than the closest competitor for this test run.
iOS Chart Performance Test Results (FPS, Higher is Better). In these tests SciChart is put head to head vs. CorePlot, iOS-Charts and Shinobi. In all tests, SciChart outperformed all competitors. In one test, SciChart was able to perform up to 18.54x faster than the closest competitor.
Test Results in Chart Form
What iOS charting performance comparison would be complete without the results in chart form?
You can find the test results plotted below:
NxM Series Test Results
In this test SciChart is green and the results are FPS (Higher is better).
At lower series counts (10 series, 10 points) core plot is already starting to struggle, only able to refresh below 20 FPS. The others are doing well.
At 33 series x 33 points now both Core plot and iOS Charts are struggling but Shinobi is still in the game.
By 100 series x 100 points now only SciChart is rendering at a smooth rate (30 FPS or more)
Above 200 series x 200 points and 500 series x 500 points only SciChart can complete the tests.
Scatter Series Test Results
SciChart iOS is Green, results are FPS (Refresh rate) so higher is better.
For 100 scatter points, all four charts (Core plot, iOS Charts, Shinobi, SciChart) are responsive (above 30 FPS)
For 300 scatter points, all four charts are still refreshing quickly (over approx 40 FPS)
At 1,000 scatter points, Core plot and iOS Charts are suffering, managing just over 10 FPS. Shinobi is third with ~25 FPS
At 10000 scatter points, SciChart is still able to render near 60 FPS but core plot and iOS Charts are unable to complete the test. Shinobi has dropped below 5 FPS and is disqualified form further tests.
For 25,000 scatter points, 50,000 scatter points and 100,000 scatter points, SciChart is the only chart provider able to complete the tests.
Append Points Test Results
SciChart iOS is Green, results are FPS (Refresh rate) so higher is better.
In this test we start off with 10,000 points on a chart. We then add 1k points each timer tick (10ms).
For the first test Core plot, iOS Charts and Shinobi are all struggling, below 5 FPS. They are disqualified from further tests.
SciChart renders above 35 FPS for the first six tests (up to 100k points + 1k new points per timer tick)
SciChart is able to draw 100k points and 10k new points per timer tick at 15 FPS
The last test 1 Million points + 10k new points per timer tick, SciChart is able to draw just under 10 FPS. Slow, but it handles it.
Fifo Scrolling Test Results
SciChart iOS is green, results are FPS (Refresh rate) so higher is better.
In this test we start off with 1000 points and we attempt to scroll the chart (add one point, remove one point)
For test #1 Coreplot is off to a rough start, struggling at under 10 FPS. iOS Charts is copying very well at 60 FPS along with SciChart, and Shinobi is posting just over 20 FPS.
For the second set of tests 5k points scrolling point by point, iOS Charts performance has dropped significantly to 14 FPS. Shinobi is under 10. SciChart remains near 50 FPS.
For the third set of tests at 10k points SciChart is the only contender left.
Two further tests at 100k points and 1Million points are performed. SciChart is the only chart able to draw this amount of data.
SciChart suffers a bit with wider strokes. We noticed that from 1 stroke thickness to 10 stroke thickness there's an approximate 50% performance drop. It's still capable of drawing huge datasets though in excess of other charts.
Performance Comparison Conclusions
According to the Performance Comparison Results, Most iOS Chart libraries are unsuitable for real-time updates on the chart. Most perform poorly once the amount of data reaches a few thousand points.
4th: Core plot performance
Core plot was unable to compete most performance tests. For the line series tests, only showing 10 series with 10 points per series caused Core-plot to stutter, averaging under 20 FPS. For the scatter chart tests Core plot will display a few hundred data-points comfortably but not more. Core plot was not able to handle 1,000 data-points streaming without significant slowdown.
3rd: iOS Charts Performance
Charts came third in our tests, performing well for streaming (scrolling) 1,000 points on a line chart, but poorly in almost every other test. We observed that Charts would display a few thousand points in a static chart but as soon as you change the axis range, zoom or update data, it would stall and did not perform well. In some cases the chart would flicker or go completely blank while updating.
2nd: Shinobi Charts
Shinobi iOS Charts performed better in performance tests, however still struggled under the following conditions:
More than 1,000 scatter points (FPS dropped below 30)
More than 100 series with 100 points per series (FPS: 15)
More than 1,000 points streaming (scrolling) chart (FPS 21)
Appending 1,000 points to an existing series (FPS: 2)
1st: SciChart iOS Charts
SciChart is the clear winner in the performance tests. Test results showed SciChart outperforming competitors with refresh rates up to 10-20x faster(1), and able to show up to 200x more data(2).
For realtime updates, SciChart outperformed the nearest competitor by up to 20x (FPS)
For the FIFO streaming test, SciChart was able to scroll / stream 1 Million points whereas the nearest competitor topped out at 5,000 (200x more data)
Conclusion: SciChart is the Fastest chart available for iOS Apps which require big-data, or streaming, realtime updates.
A version of the test-application is available on request, if you would like to
contact us
. If you are one of the authors of chart components mentioned in this article and wish to verify the results or submit a modification to optimize your code, please feel free. We would also welcome submissions from other chart vendors and authors.
Finally let us know what you think in the comments! If there’s anything else you want us to test, please ask!
Best regards,
Yaroslav,
SciChart Team
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
📹 How YouTube Scaled MySQL to Support 2.49 Billion Users 🚀
Hamza Khan -
Nov 26
OKX DEX API Essentials: Single and Cross-Chain Swaps on Avalanche C-Chain
Julian Martinez -
Nov 26
How to Toggle Auto-Completion in Cursor Editor 🎉
Takehiro_Yamazaki -
Nov 26
Why Code Reuse is Important in the Age of AI
John Brooks -
Dec 9
Andrew Bt
Follow
Former electronic engineer who works in software. With experience in languages from C/C++ to C#, JavaScript and TypeScript, and now specialises in performance optimisation and data visualisation
Location
United Kingdom
Joined
Dec 14, 2023
More from
Andrew Bt
SciChart.js Javascript 3D Charts with WebGL & WebAssembly
#
webdev
#
javascript
#
charts
#
webassembly
SciChart.js Performance Demo: 1 Million Datapoints in under 15ms
#
webdev
#
javascript
#
datapoints
Algorithmic Trading with SciChart
#
webdev
#
javascript
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
🤔 Is It Possible to Achieve 100% Test Automation? - DEV Community,"🤔 Is It Possible to Achieve 100% Test Automation? - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Carlos Gonzaga
Posted on
Jul 18
🤔 Is It Possible to Achieve 100% Test Automation?
#
softwarequality
#
softwaredevelopment
#
automatedtesting
#
bigdata
The pursuit of total test automation is a common goal in many development teams. As an automated test analyst, I believe that achieving 100% automation is possible. Despite numerous challenges such as test scenario complexity and the constant need for script maintenance, overcoming these barriers is feasible with the right approaches and tools.
Test complexity is a reality, especially those involving human interactions and exploratory testing. However, with the continuous evolution of automation tools and the integration of artificial intelligence and machine learning, we are increasingly closer to effectively replicating these interactions. Advanced tools can learn from past behaviors and adapt to new patterns, reducing the reliance on manual testing.
The maintenance of test scripts, often seen as a burdensome task, can be efficiently managed with good coding practices and the use of robust frameworks. Moreover, test automation should be viewed as a long-term investment. While the initial cost may be high, the benefits in terms of speed, accuracy, and test coverage outweigh the initial financial challenges.
A crucial aspect to ensure the integrity of automated tests and achieve 100% automation is the structure of big data. Well-updated and structured big data is essential for test effectiveness. It allows tests to be conducted under conditions that reflect the reality of the production environment, ensuring accurate and reliable results.
Another consideration is the continuous updating of automation tools, which are becoming more comprehensive and integrated. With end-to-end automation, from unit tests to integration and acceptance tests, it is possible to cover all aspects of the software, ensuring quality and reliability.
Achieving 100% test automation is not just an ambitious goal but a real possibility with the right approach. The key lies in combining advanced technologies, good development practices, a robust and updated big data framework, and a continuous commitment to quality. Therefore, while many may view total automation as an unattainable ideal, I firmly believe that we are on the right path to turning this vision into reality.
https://www.linkedin.com/in/carlos-gonzaga-1b84137a
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to *really* use AI Tools when building software.
Samyak Jain -
Sep 24
Making YouTube Videos Downloader Using Python(mp3 and mp4)
Trix Cyrus -
Sep 23
Load Balancing and Traffic Management for .NET Core Microservices
Soham Galande -
Sep 23
Understanding State and Props in React 💻🌱
Anik Dash Akash -
Sep 23
Carlos Gonzaga
Follow
Joined
May 21, 2018
Trending on
DEV Community
Hot
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Mastering Essential Software Architecture Patterns: A Comprehensive Guide🛠️
#
softwaredevelopment
#
softwareengineering
#
architecture
#
microservices
15 System design tradeoffs for Software Developer Interviews
#
programming
#
development
#
systemdesign
#
softwaredevelopment
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
10 Reasons for Flaky Tests - DEV Community,"10 Reasons for Flaky Tests - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Oyedele Temitope
Posted on
May 29
10 Reasons for Flaky Tests
#
testing
#
test
#
webdev
#
devops
Testing is one of the important aspects of software development. It helps to ensure that your codes are high quality and reliable. However, we sometimes run into issues, one of which is flaky tests. They can be very frustrating, which is why knowing why they can occur is a big step to solving them. In this article, we'll look at the ten reasons for flaky tests and provide some solutions to get your testing back on track.
What are Flaky Tests
Flaky tests
is a test that both passes and fails at the same time periodically without any code change and by code change, we mean no change in the code that run the tests nor change in the code of the application in the same environment.
Flaky tests can be problematic because they reduce the reliability and effectiveness of automated testing. Let's say, for instance, that you're testing out a feature to be included in software that calculates the total price of items for a shopping cart. You write an automated test that adds the items to the cart,  checks them, and calculates the total price. However, the test fails. Sometimes, this test runs, and the total price is calculated correctly, but other times, it fails and gives an incorrect total price.
This behavior is what we call flaky. It causes an uncertainty about the result. The fact that it passes sometimes and fails the next time doesn't bring about this uncertainty as we are not sure if we are breaking something or just some flaky behavior.
This brings us to why a flaky test can occur during testing.
Reasons for Flaky Tests
In this section, we'll run through some reasons why flaky tests occur. Note that they are not limited to these but are some of the common ones we encounter
Poor Tests Data
When the data used to test is poor, it can lead to a flaky test. When we talk about data being poor, we mean it can be old, incomplete, incorrect, or even outdated. And using it can lead to tests periodically failing or passing even when we test under the same conditions.
To reduce the chances of encountering tests due to poor data, we can employ the following strategies:
Use Realistic Test Data
ValidateTest Data
Refresh Test Data Regularly
Monitor test data quality
Inconsistent Test Environment
A test environment that varies in hardware, software, and configuration can lead to inconsistent test results.
For instance, a test can pass on a developer's machine since it has all the configurations needed but fail in a
CI/CD
environment due to the environment possessing a different version of the configurations or settings from the local machine. This incompatibility often leads to flakiness.
To minimize the chances of encountering a flaky test due to an inconsistent test environment, You can employ the following strategies:
Virtualization
Containerization
Cloud-based testing
Automated environment validation
Poor Test Designs
When it comes to the reliability and consistency of tests, Test designs play a crucial role. A poorly designed test combined with the absence of a sufficient setup, teardown procedure, and inadequate error handling can lead to a flaky test.
To address issues due to poor test designs, we must have the following:
Proper setup and teardown
Robust error handling
Specific assertions
Resource Constraints
Resource constraints refer to the limitations in resources that are available for testing. These limitations can be related to hardware, software, or infrastructure and can impact the execution of tests, leading to flakiness.
For instance, a test that has a specific memory or  CPU limit is bound to be flaky if the available memory fails to meet the required memory for execution.
To prevent issues due to environmental constraints, consider the following:
Upgrading Hardware
Optimizing Test Scripts
Distributing Tests Across Multiple Machines
Using Cloud Resources
Implementing Resource Monitoring
Timing Issues
Timing issues can be encountered when tests depend on certain timing conditions, such as network latency or UI elements. These conditions are sensitive and can cause tests to pass or fail.
To address timing issues, we can implement
synchronization techniques
like explicit wait and timeouts. These techniques ensure that the test waits for the necessary conditions before proceeding. Doing this will reduce the likelihood of flakiness due to timing issues.
External Factors
When we talk about external actors, we refer to issues ranging from network connectivity to outages in third-party services. When a test depends on an external service, they are prone to failure, and if such dependencies become unavailable for one reason or another.
To minimize interference from external factors and improve the reliability of tests, consider the following solutions:
Use Isolated Test Environments
Implement Retry Mechanisms
Monitor External Factors Before Test Execution
Mock External Services
Test Dependencies
Test dependencies refer to a situation where one test relies on another test's outcome. This usually occurs when tests share resources, and the order in which the tests are executed influences their outcomes.
For instance, if a test modifies a shared resource and another test relies on the resource's original state, the dependent test may fail if it runs before the modifying test has finished.
To reduce the impact of test dependencies, we can:
Ensure test isolation
Clear test dependency
Mock external dependency
Using Hard-Coded Test Data
Hard-coded test data refers to embedding specific values into test values instead of generating dynamic or dummy data. Even an automation engineer will tell you this is a bad practice that can lead to flaky tests. Using hardcoded test data can lead to issues like difficulty in debugging, data duplication, obsolete data, etc.
A better way is to use dynamic test data, mock external dependencies, or parameterize tests.
Poorly Written Tests
Poorly written tests are those that are not well structured. They do not define what they are testing, and sometimes, they are overly complex, which makes them difficult to understand.
One of the numerous ways to cause flaky tests is having a test written poorly. To address the issue of poorly written tests, here are some solutions:
Refactor and Simplify Tests
Ensure Proper Cleanup
Use descriptive test names
Avoid redundant tests
Use mocking and stubbing
Lack of Proper Framework
A proper framework establishes a structured environment for writing, performing, and maintaining tests. This ensures that tests are reliable, efficient, and easy to understand. Not having a proper framework can make a test fail. This is because they lack the necessary tools, libraries, and configurations to run consistently. Having a proper framework includes the process of running the tests, what is needed, and also how to do it.
To address improper framework issues, we can:
Use Robust and Reliable Test Automation Frameworks and Tools
Properly Manage Test Environments and Configurations
Regularly Maintain and Update Tests
Conclusion
In this article, we've identified ten key reasons contributing to flaky tests, each with its own implications. Understanding these reasons is crucial for addressing flaky tests effectively and ensuring the reliability and accuracy of tests.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Cybersecurity Course for Beginners: Your Ultimate Guide In 2025
Daksh Kataria -
Nov 23
🇩🇪 PHP 8.4: Was ist neu?
Pascal Kleindienst -
Nov 23
Zoraxy vs Nginx Proxy Manager
Toby Chui -
Nov 23
Pathfinding: Solving the N-Queens Problem Using Backtracking Introduction:
TAMIL SELVAN M IT -
Nov 22
Oyedele Temitope
Follow
I’m a versatile software engineer, technical writer, and DevOps enthusiast, I possess a strong background in both  front-end  and back-end development.
Location
The internet
Pronouns
Mr
Work
Software developer and technical writer
Joined
May 21, 2021
More from
Oyedele Temitope
How to Set Up Authorization in a Bookstore Management System with Go, HTMX, and Permit.io
#
authorization
#
htmx
#
go
#
webdev
Understanding the React Cache function
#
nextjs
#
react
#
webdev
#
javascript
Understanding the React Cache function
#
nextjs
#
react
#
webdev
#
javascript
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Why data quality is key to successful ML Ops - DEV Community,"Why data quality is key to successful ML Ops - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Kyle Eaton
Posted on
Sep 28, 2020
Why data quality is key to successful ML Ops
#
machinelearning
#
datascience
#
devops
In this first post in our 2-part ML Ops series, we are going to look at ML Ops and highlight how and why data quality is key to ML Ops workflows.
Machine learning has been, and will continue to be, one of the biggest topics in data for the foreseeable future. And while we in the data community are all still riding the high of discovering and tuning predictive algorithms that can tell us whether a picture shows
a dog or a blueberry muffin
, we’re also beginning to realize that
ML isn’t just a magic wand
you can wave at a pile of data to quickly get insightful, reliable results.
Instead, we are starting to treat ML like other software engineering disciplines that require processes and tooling to ensure seamless workflows and reliable outputs.
Data quality
, in particular, has been a consistent focus, as it often leads to issues that can go unnoticed for a long time, bring entire pipelines to a halt, and erode the trust of stakeholders in the reliability of their analytical insights:
”Poor data quality is Enemy #1 to the widespread, profitable use of machine learning, and for this reason, the growth of machine learning increases the importance of data cleansing and preparation. The quality demands of machine learning are steep, and bad data can backfire twice -- first when training predictive models and second in the new data used by that model to inform future decisions.” (
tdwi blog
)
In this post, we are going to look at ML Ops, a recent development in ML that bridges the gap between ML and traditional software engineering, and highlight how data quality is key to ML Ops workflows in order to accelerate data teams and maintain trust in your data.
What is ML Ops?
Let’s take a step back and first look at what we actually mean by “ML Ops”. The term
ML Ops
evolved from the better-known concept of “DevOps”, which generally refers to the set of tools and practices that combines software development and IT operations. The goal of DevOps is to
accelerate software development
and deployment throughout the entire development lifecycle while
ensuring the quality
of software by streamlining and automating a lot of the steps required. Some examples of DevOps most of us are familiar with are version control of code using tools such as git, code reviews, continuous integration (CI), i.e. the process of frequently merging code into a shared mainline, automated testing, and continuous deployment (CD), i.e. frequent automated merges of code into production.
When applied to a machine learning context, the goals of ML Ops are very similar:
to accelerate the development and production deployment of machine learning models while ensuring the quality of model outputs
. However, unlike with software development, ML deals with both code and data:
Machine learning starts with data that’s being ingested from various sources, cleaned, transformed, and stored using code.
That data is then made available to data scientists who write code to engineer features, develop, train and test machine learning models, which, in turn, are eventually deployed to a production environment.
In production, ML models exist as code that takes input data which, again, may be ingested from various sources, and create output data that’s used to feed into products and business processes.
And while our description of this process is obviously simplified, it’s clear to see that
code and data
are tightly coupled in a machine learning environment, and ML Ops need to take care of both.
Concretely, this means that ML Ops incorporates tasks such as:
Version control of any code used for data transformations and model definitions
Automated testing of the ingested data and model code before going into production
Deployment of the model in production in a stable and scalable environment
Monitoring of the model performance and output
How does data testing and documentation fit into ML Ops?
Let’s go back to the original goal of ML Ops: to accelerate the development and production deployment of machine learning models while ensuring the quality of model outputs. Of course, as data quality folks, we at Great Expectations believe that data testing and documentation are absolutely essential to accomplishing those key goals of acceleration and quality at various stages in the ML workflow:
On the stakeholder side, poor data quality affects the trust stakeholders have in a system, which negatively impacts the ability to make decisions based on it. Or even worse, data quality issues that go unnoticed might lead to incorrect conclusions and wasted time rectifying those problems.
On the engineering side, scrambling to fix data quality problems that were noticed by downstream consumers is one of the number one issues that cost teams time and slowly erodes team productivity and morale.
Moreover, data
documentation
is essential for all stakeholders to communicate about the data and establish data contracts:
“Here is what we know to be true about the data, and we want to ensure that continues to be the case.”
In the following paragraphs, we’ll look at the individual stages in an ML pipeline at a very abstract level, and discuss how data testing and documentation fits into each stage.
At the data ingestion stage
Even at the earliest stages of working with a data set, establishing quality checks around your data and documenting those can immensely speed up operations in the long run. Solid data testing gives engineers confidence that they can safely make changes to ingestion pipelines without causing unwanted problems. At the same time, when ingesting data from internal and external upstream sources, data validation at the ingestion stage is absolutely critical to ensure that there are no unexpected changes to the data that go unnoticed.
Twitter thread by
Pete Skomoroch
and
Vincent D. Warmerdam
We’ve been trying really hard to avoid this cliché in this post, but here we go:
Garbage in, garbage out
. Thoroughly testing your input data is absolutely fundamental to ensuring your model output isn’t completely useless.
When developing a model
For the purpose of this article, we’ll consider feature engineering, model training, and model testing to all be part of the core model development process. During this often-iterative process, guardrails around the data transformation code and model output support data scientists so they can make changes in one place without potentially breaking things in others.
In classic DevOps tradition, continuous testing via CI/CD workflows quickly elicits any issues introduced by modifications to code. And to go even further, most software engineering teams require developers to not just test their code using existing tests, but also
add new tests when creating new features
. In the same way, we believe that running tests
as well as writing new tests
should be part of the ML model development process.
When running a model in production
As with all things ML Ops, a model running in production depends on both the code and the data it is fed in order to produce reliable results. Similar to the data ingestion stage, we need to secure the data
input
in order to avoid any unwanted issues stemming from either code changes or changes in the actual data. At the same time, we should also have some testing around the model output to ensure that it continues to meet our expectations. We occasionally hear from data teams that a faulty value in their model
output
had gone undetected for several weeks before anyone noticed (and in the worst case, they were alerted by their stakeholders before they detected the issue themselves).
Especially in an environment with black box ML models, establishing and maintaining standards for quality is crucial in order to trust the model output. In the same way,
documenting
the expected output of a model in a shared place can help data teams and stakeholders define and communicate “data contracts” in order to increase transparency and trust in ML pipelines.
What’s next?
By this point, it’s probably clear how data validation and documentation fit into ML Ops: namely by allowing you to implement tests against both your data and your code, at any stage in the ML Ops pipeline that we listed out above.
We believe that data testing and documentation are going to become one of the key focus areas of ML Ops in the near future, with teams moving away from “homegrown” data testing solutions to off-the-shelf packages and platforms that provide sufficient expressivity and connectivity to meet their specific needs and environments.
Great Expectations
is one such data validation and documentation framework that lets users specify what they expect from their data in simple, declarative statements.
In the second blog post in this two-part series, we will go into more detail on how Great Expectations fits into ML Ops
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
🤖 RAG vs. Agents: A Comparison and When to Use Each
Mhammed Talhaouy -
Dec 10
New AI Breakthrough Makes Self-Driving Cars 15x Faster and Safer with Truncated Diffusion Model
Mike Young -
Dec 1
Automating Flask & PostgreSQL Deployment on KVM with Terraform & Ansible
Daniel Pepuho -
Dec 10
Building a Secure and Scalable CI/CD Pipeline for EKS Using Jenkins and GitHub Actions
akhil mittal -
Nov 30
Kyle Eaton
Follow
Location
Ann Arbor, MI
Work
UX/Growth at Superconductive Health / Great Expectations
Joined
Feb 2, 2020
More from
Kyle Eaton
How DAGs grow: Deeper, wider, and thicker
#
datascience
#
database
#
dataquality
#
devops
Your data tests failed! Now what?
#
datascience
#
devops
#
database
#
dataops
Continuous Integration for your data with GitHub Actions and Great Expectations
#
datascience
#
testing
#
devops
#
github
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Five Data Quality Tools You Should Know - DEV Community,"Five Data Quality Tools You Should Know - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Michael Bogan
Posted on
Dec 1, 2021
Five Data Quality Tools You Should Know
#
database
#
webdev
Data quality ensures that an organization’s data is accurate, consistent, complete, and reliable. The quality of the data dictates how useful it is to the enterprise. Ensuring data quality—especially with the sheer amount of data available to today’s enterprises—is a tremendously complex beast. To do this effectively, the modern enterprise relies on data quality tools.
In this post, we will consider five data quality tools and see how they can help you in your data journey:
Great Expectations
Spectacles
Datafold
dbt (Data Build Tool)
Evidently
Before we dive in with our tools, let’s first set the stage for
why
ensuring data quality is a business essential.
Why Bother with Data Quality?
Today’s businesses are dependent on data more than ever before.
According to a recent Gartner data quality market survey
, poor data quality costs enterprises an average of $15 million annually. Erroneous data can result in lost business opportunities, bad market reputation, low customer confidence, and significant financial losses.
How Do We Get Bad Data?
Broadly, both external and internal factors can cause data quality issues. External causes refer to things that are out of an enterprise’s control such as third-party data. Internal causes stem from problems within the organization’s ecosystem.
External factors can manifest when organizations use outside data sources. For example, different companies’ IT systems may be integrated after a merger or acquisition. Some organizations may ingest data from third-party sources like Meta, Google Analytics, or AWS Data Exchange. A failure to audit data quality sourced from third parties, or poor input validation in applications, can also cause data quality issues.
An example of an internal factor could be enterprise data silos. Data silos are little-known data sources, used by only certain teams or departments within an organization. Another factor is the lack of proper data ownership. Yet another cause can be using wrong data types and data models. On top of this, software engineers at any layer of the application may introduce code updates that change fields and break the data. Any of these factors can cause poor data quality and inconsistencies.
Measuring Data Quality
How data quality plays out in an organization largely depends on how that organization defines it and prioritizes it. However, there are seven useful indicators of quality.
Relevancy
: how relevant the data is for the business.
Accuracy
: how accurate the data is.
Completeness
: whether the data is complete and in a stable state.
Consistency
: how consistent the data is across the organization. If the same piece of data from multiple sources is transformed using the same application, the output should always be the same.
Conformity
: whether the data adheres to the standards and format expected by business rules.
Uniqueness
: whether multiple copies of the same data are available in the enterprise or if it’s the single source of truth.
Timeliness
: how timely the data is for current business needs.
Ensuring Data Quality
Enterprises typically use automated tools for checking data quality. These tools can be either custom developed or vendor supplied. Both choices have their pros and cons.
In-house solutions are feasible if there are IT resources available and data quality requirements are well defined. Roll-your-own solutions can also help cut down capital expenditures. On the other hand, building a custom solution can be time-consuming, and that lost time sometimes outweighs the benefits.
Buying a solution is best if a company needs a reliable solution quickly and doesn’t want to maintain it themselves.
Now that we’ve laid the groundwork, let’s look at our five data quality tools. Of course, this is not an exhaustive list, and there are many other tools in the market. The tools you choose to mix and match for your data quality needs will depend on your use case and budget.
Great Expectations
Great Expectations
is an open-source library used for validating, documenting, and profiling data. Users define
assertions
in the form of
expectations
. An expectation is exactly what the name suggests—it’s the quality you are expecting from the data. Assertions are written in a declarative language. For example, here is a sample assertion where the column
passenger_count
value has to be between 1 and 6.
Another feature is automated data profiling. Great Expectations can auto-generate expectations from the data based on its statistics. This can save time, as data quality engineers don’t have to write assertions from scratch.
Once expectations are ready, they can be incorporated into the data pipeline. For example, in Apache Airflow, the data validation step can be defined as a
checkpoint script
using the
BashOperator
. This will trigger the quality check as data flows through the pipeline. Great Expectations is compatible with most data sources such as CSV files, SQL databases, Spark DataFrames, and Pandas.
Spectacles
Spectacles
is a continuous integration (CI) tool designed to validate your project’s LookML.
LookML
is Looker’s data modeling language. If you aren’t familiar with
Looker
, it’s a BI platform that allows people who don’t “speak SQL” to analyze and visualize data.
Spectacles validates LookML by running the SQL query behind it and checking for errors. It integrates with GitHub, GitLab, and Azure DevOps. The tool fits nearly any deployment pattern—whether called manually, triggered from a pull request, or run as part of an ETL job. Having Spectacles as part of a CI/CD workflow means LookML queries are automatically validated before the code is deployed into production.
Datafold
Datafold
is a proactive data quality platform that has three main components: Data Diff, Data Catalog with column-level lineage, and Data Monitoring.
Data Diff allows you to compare two datasets (for example, dev and prod) before merging them into production. This helps users to adopt a more proactive development strategy. Data Diff can also be integrated into a team’s CI/CD pipeline, such that diffs can show up alongside code changes in GitHub or GitLab.
As an example, let’s look at the
taxi_trips
dataset that comes with Datafold’s sandbox environment. As you can see in the image below, we have run a Data Diff operation between
datadiff-demo.public.taxi_trips
and
datadiff-demo.dev.taxi_trips
datasets.
On the detailed panel on the right, you can select different tabs to get different perspectives on the results.
The
Overview
tab will contain a rundown of the successful and failed tests.
The
Schema
section shows whether the columns from both datasets (data type, order of appearance) match.
The Primary Keys
section shows what percentage of primary keys are unique, not NULL, and matching between both datasets.
Although the
Overview
tab is a great source of information, other tabs provide more useful details. For example, the
Schemas
tab can look like this:
Here, the highlighted column is where the two datasets differ. This saves time because data engineers can concentrate on the content of those two fields.
The
Data Catalog
lists all the data sources registered with Datafold. It also allows users to find and profile any particular dataset using filters. This can be a huge timesaver for organizations with hundreds—or even thousands—of datasets. It is an especially useful way to uncover anomalies. Particularly, the data lineage feature can help answer questions like the following:
Where is this value coming from?
How is this value affecting other tables?
How are these tables related?
The Data Catalog presents a dashboard like the one shown below. As you scroll down, you will see a list of each column.
The columns provide important details at a glance:
Completeness
refers to what percentage of the values are not NULL.
Distribution
shows which values appear the most and the least. It will also show if the values are skewed towards a certain range.
Clicking the icon under
Lineage
will show something like this:
The graphical lineage diagram helps data engineers quickly find where a column’s values come from. Lineage can be checked for tables, for all or specific columns, and for upstream or downstream relationships.
Datafold’s monitoring feature allows data engineers to write SQL commands to find anomalies and create automated alerts. Alerting is backed by machine learning, which studies the trends and seasonalities in the data to accurately spot anomalies. The images below show such a query:
The query in this example is auto-generated by Datafold. It keeps track of the daily total fare and the total number of trips in the taxi dataset.
Datafold also allows users to examine the trend of anomalies over time as shown in the image below:
Here, the yellow dots represent anomalies with respect to the minimum and maximum values.
dbt
dbt
is a data transformation workflow tool. It runs the data transformation code against the target database
before
deployment, showing how the code will affect the data and highlighting any potential problems. dbt does this by running SELECT statements to build the end state of the data based on the transformation logic.
dbt is easy to integrate into the modern BI stack and can be a valuable part of a CI/CD pipeline. It can be run automatically upon a pull request or on a schedule. It has an automatic rollback feature that stops potentially code-breaking changes from getting deployed.
Datafold and dbt can be used together for automating data quality testing. Just like dbt, Datafold can be integrated into the CI/CD pipeline. When used together, they show how your code affects your data.
Evidently
Evidently
is an open-source Python library that analyzes and monitors machine learning models. It generates interactive reports based on Panda DataFrames and CSV files for troubleshooting models and checking data integrity. These reports show model health, data drift, target drift, data integrity, feature analysis, and performance by segment.
To see how Evidently can help, you can open up a new notebook on Google Colab and copy the following code snippet:
!pip install evidently

import pandas as pd
import math
from sklearn import datasets
from evidently.dashboard import Dashboard
from evidently.tabs import DataDriftTab

wine = datasets.load_wine()
wine_frame = pd.DataFrame(wine.data, columns = wine.feature_names)

number_of_rows = len(wine_frame)

wine_data_drift_report = Dashboard(tabs=[DataDriftTab])
wine_data_drift_report.calculate(wine_frame[:math.floor(number_of_rows/2)], wine_frame[math.floor(number_of_rows/2):], column_mapping = None)

wine_data_drift_report.save(""report_1.html"")
Enter fullscreen mode
Exit fullscreen mode
This code will generate and load a report in the browser.
The report’s dashboard overview will show the distribution of references and current values based on every feature.
More close-up inspection is possible from here. For example, the graph below shows the exact values where current and reference datasets differ.
Evidently has nifty features like numerical target drift, categorical target drift, regression model performance, classification model performance, and probabilistic classification model performance.
Final Words
Data quality standards and business requirements are constantly evolving, making the task of ensuring data quality a continuous journey. The introduction of bad data into your data-driven decision-making will have many consequences—all of which are detrimental to the business. That’s why it’s necessary to maintain standards of data quality at every step in the data journey.
The five tools discussed here can be used during different phases of data processing and usage. In your own organization’s data journey, it’s worth testing out some of these tools individually—or in different combinations—to see how they can benefit your use cases.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Why Monorepo Projects Sucks: Performance Considerations with Nx
Jeferson F Silva -
Dec 11
How to Clear Browser Cache on Android
TahneetKanwal -
Dec 11
Flexible Angular Builds: A Guide to Angular 19 Build-Time Variables with Docker
Daniel Sogl -
Dec 10
Microsoft Copilot: Redefining Productivity with AI
Layan Yasoda -
Dec 6
Michael Bogan
Follow
I write tech articles about things I love. I also run DevSpotlight - we create tech content for tech companies. If you need tech content, or want to create tech content, reach out!
Location
Indianapolis
Work
Technical Architect and Writer at DevSpotlight
Joined
Jun 12, 2019
More from
Michael Bogan
Working with Heroku Logplex for Comprehensive Application Logging
#
webdev
#
logging
#
heroku
#
programming
Caching RESTful API requests with Heroku’s Redis Add-on
#
redis
#
heroku
#
webdev
#
restapi
Exploring the Cadence Access Model: Fine-Grained permissions for flow contracts
#
web3
#
cryptocurrency
#
blockchain
#
webdev
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Test Data Management: A Comprehensive Guide - DEV Community,"Test Data Management: A Comprehensive Guide - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
keploy
Posted on
Nov 21
Test Data Management: A Comprehensive Guide
#
webdev
#
javascript
#
programming
#
tutorial
Why Test Data Management is Crucial for Modern Development
In today’s fast-paced software development landscape,
test data management
(TDM) plays a pivotal role in ensuring quality, speed, and efficiency in testing processes. As businesses strive for faster releases and better software quality, managing test data effectively has become more important than ever.
What is Test Data Management?
Test data management refers to the process of creating, managing, and maintaining data sets used for testing software applications. These data sets mimic real-world scenarios to ensure that software is rigorously tested before deployment. The goal of TDM is to provide accurate, consistent, and secure data to support testing processes.
The Importance of Test Data Management in the Software Development Lifecycle (SDLC)
Effective TDM is critical to achieving accurate test results, faster releases, and robust application performance in the SDLC. It ensures that test environments mirror production scenarios as closely as possible, leading to more reliable outcomes. Without proper TDM, software teams may encounter issues like incomplete test coverage, delays, and compromised quality.
Key Components of Test Data Management
A successful TDM strategy consists of several crucial components:
Data Discovery
: Understanding and identifying data requirements is the first step in TDM. This involves analyzing the application and defining the type and volume of data needed.
Data Provisioning
: Preparing and allocating test data sets to various testing environments ensures that teams have the resources they need at every stage.
Data Masking
: To protect sensitive information, data masking techniques anonymize data while maintaining its usability for testing purposes.
Data Integrity
: Maintaining consistency and accuracy of test data is essential for trustworthy testing outcomes.
Challenges in Test Data Management
Despite its importance, managing test data effectively comes with its own set of challenges:
Handling large data volumes
: Managing big data sets for testing can be resource-intensive.
Ensuring data compliance and security
: With stringent regulations like GDPR, safeguarding sensitive data is a top priority.
Providing realistic test scenarios
: Ensuring that test data reflects real-world conditions is crucial but challenging.
Managing costs
: The expenses associated with storage, provisioning, and management can add up quickly.
Benefits of Efficient Test Data Management
Organizations that implement robust TDM practices experience multiple benefits, including improved testing accuracy and reduced time-to-market.
Enhanced test coverage enables teams to identify and fix bugs early.
Accelerated testing cycles reduce delays in the development process.
Better compliance with data privacy regulations protects businesses from legal risks.
Cost efficiency in testing processes saves valuable resources.
Best Practices for Test Data Management
Implementing the following best practices can help streamline TDM and maximize its effectiveness:
Define clear data requirements
: Collaborate with stakeholders to determine the exact test data needs.
Use automated tools
: Automating data generation, masking, and provisioning saves time and reduces errors.
Maintain separate environments
: Isolating test environments from production prevents data contamination.
Regularly update test data
: Keep test data sets relevant by refreshing them periodically.
Tools for Test Data Management
Various TDM tools simplify the complexities of handling test data, enabling teams to focus on quality assurance.
Popular tools like
Informatica
,
IBM Optim
,
Delphix
, and
TDM Pro
offer features such as automated data provisioning, masking, and compliance support.
When selecting a tool, look for capabilities such as scalability, ease of integration, and advanced reporting.
Test Data Management for Compliance and Security
Ensuring compliance with data privacy regulations like GDPR, HIPAA, and CCPA is a crucial aspect of TDM.
Data masking and encryption techniques
help safeguard sensitive information.
Regular audits and monitoring ensure compliance with industry standards.
Following regulatory guidelines reduces risks and builds customer trust.
Real-World Applications of Test Data Management
TDM finds applications across industries, from banking to healthcare, where precise and secure testing environments are critical.
Case study
: A leading banking firm implemented automated TDM, reducing testing time by 40% while ensuring compliance with stringent data privacy regulations.
Use cases in healthcare involve anonymizing patient records for software testing, while retail companies use TDM to test their e-commerce platforms.
Future Trends in Test Data Management
Emerging technologies like AI and machine learning are shaping the future of TDM, enabling smarter and more efficient testing.
AI-powered data generation
creates realistic test scenarios with minimal manual effort.
Cloud-based TDM solutions offer scalability and cost efficiency.
The focus on data compliance continues to grow, ensuring that TDM evolves with regulatory requirements.
Conclusion: Embracing Test Data Management for Better Software Quality
Test data management is not just a support process but a strategic enabler for delivering high-quality software applications efficiently and securely. By investing in robust TDM strategies and tools, organizations can accelerate development cycles, enhance software quality, and stay ahead in the competitive technology landscape.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
2054. Two Best Non-Overlapping Events
MD ARIFUL HAQUE -
Dec 8
AI Breakthrough: Evolution-Based System Creates More Efficient Neural Networks
Mike Young -
Dec 4
Rust 🦀 version 1.83.0 came out a few days ago. It is upgrade time!
Gabor Szabo -
Dec 4
TypeScript's progressive adoption strategy for front-end projects
Tianya School -
Dec 4
keploy
Follow
Keploy is an AI-powered testing tool that specializes in creating test cases and generating stubs/mocks for end-to-end testing. It can achieve an impressive 90% test coverage in just a min.
Joined
Sep 23, 2023
More from
keploy
React Testing: A Comprehensive Guide
#
webdev
#
javascript
#
beginners
#
tutorial
Webhooks vs APIs: Understanding the Differences
#
webdev
#
javascript
#
beginners
#
tutorial
GraphQL vs REST: A Comprehensive Comparison
#
javascript
#
programming
#
beginners
#
tutorial
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Self-grading quizzes with Airtable - DEV Community,"Self-grading quizzes with Airtable - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Taq Karim
Posted on
Mar 16, 2020
Self-grading quizzes with Airtable
#
learning
#
tutorial
#
serverless
#
showdev
This semester, I am teaching a course at Baruch University on Big Data Technologies.
I resolved to administer a small quiz at the start of class to get a feel for how the class is doing overall in terms of comprehension of key materials.
However, grading quizzes - especially at such a recurring cadence - will definitely be time consuming, even for a small class of say 15 people (my class has 47 students).
Requirements
I wanted a system that would:
Make it easy to create multi-question,
multiple choice
quizzes
Make grading automatic
Share individual grades automatically to students via email or some other manner
Require minimal infrastructure and coding (ie: maybe a lambda function somewhere? ideally no code at all)
TL;DR:
Using Airtable, there is a way to create multi-choice, self grading quizzes. I've created a set up that you can
find and replicate
here for your class's usecase:
AIRTABLE BASE
QUIZ FORM
Airtable based solution
Leveraging
Airtable
, I have come up with an approach that generally has worked for me thus far.
The key to this solution is to create a table that represents a quiz. Each row will include student name, email, created time, ""questions"", ""checks"" and a ""grade"".
The last two types of rows: ""checks"" and ""grade"" are both formula types.
Here's a screenshot:
The columns ""1""...""5"" represent questions and each are formatted to be ""single select"" types with 4 predefined selections.
Formula Columns
To assert correctness, each ""question"" column has an accompanying ""solution"" column that asserts for correctness.
As seen here, ""Solution 1"" is a formula
{1} = ""C""
which is ensuring that the selection for column ""1"", if ""C"" is correct. This can be repeated with as many questions as necessary.
To compute grade, we simply find the average:
The ""code"" for this formula:
({Solution 1} + {Solution 2} + {Solution 3} + {Solution 4} + {Solution 5})/5
Enter fullscreen mode
Exit fullscreen mode
Where the label inside each
{}
corresponds to the column name
Solution 1
...
Solution 5
.
That's it! That's the internals of the quiz. Now, per student who completes this quiz, a corresponding row will be created with a ""creation time"" (for future filtering if needed) and ""checks"" for each question submitted and a ""grade"" that computes average.
If needed that ""grade"" column can also be updated to weight individual questions differently as well. 👍
Creating the quiz
For the ""frontend"", we need a form and the actual quiz questions! Airtable supports a concept of a ""view"" where it is possible to create a ""form"" that will populate the rows in the table.
Here's an example:
Essentially, each column from the table can be added (or removed!) as an input field. Also, it is possible to make certain fields required and to add arbitrary text (in my quiz, I even had a few coding questions).
Additionally, this form is by default not shared:
but can be toggled to shareable via a private link at any time (so it is easy to ""publish"" and ""unpublish"" a quiz)
The Verdict
For a non-custom coded system, this works ""well enough""; looking back to the requirements:
✅Make it easy to create multi-question,
multiple choice
quizzes
✅Make grading automatic
❌Share individual grades automatically to students via email or some other manner
✅Require minimal infrastructure and coding (ie: maybe a lambda function somewhere? ideally no code at all)
Some happy additional features:
✅Tables and views are easy to duplicate, meaning multiple quizzes can be easily supported
✅Publishing/unpublishing forms are actually super helpful
✅At the bottom of each column, airtable calculates some metrics such as class average, etc which can be read in real time as students complete quiz
There are some definite drawbacks here as well:
❌lack of fine tuned controls in terms of formatting the questions
❌adding more than 4-5 questions becomes somewhat unreasonable due to the additional column that much be added
❌
RATE LIMITS
! If more that ~50 people open the form all at once (or refresh a lot), I've seen cases where the form is ""disabled momentarily"" (a few hours)
❌no automatic support for emailing grades to students
For that last item - I have come up with a (similarly codeless) solution! If there is sufficient interest (please let me know in the comments!) I will follow up this post with a tutorial for how I achieved near-real time email notification to students based on quiz submission 👍
Top comments
(4)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
mayla-g
mayla-g
mayla-g
Follow
hellooo
Joined
Apr 28, 2021
•
May 6 '21
Dropdown menu
Copy link
Hide
This is very helpful. Thanks for sharinng!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
bri
bri
bri
Follow
Joined
Oct 9, 2020
•
Oct 9 '20
Dropdown menu
Copy link
Hide
Hi Taq. I am interested in the tutorial for a near-real time email notification to students based on quiz submission.
@taqkarim
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Taq Karim
Taq Karim
Taq Karim
Follow
Serverless all day 💪
Location
New York
Education
The Cooper Union
Work
Principal Software Engineer at Oracle
Joined
Dec 13, 2019
•
Dec 13 '20
Dropdown menu
Copy link
Hide
Hi bri - sorry for the late reply. While somewhat tricky, have you considered something like zapier or
integromat
? Those are some ""no code"" type solutions that could solve your problem.
I prefer to give students a randomized ID at the start of class and post all grades at once. Having the ID means no student can be identified (for the most part) but I can make class trends etc public.
If you
really
wanted near real time email notifications I'd recommend looking into AWS Simple Email Service paired with SQS or something like that.
Cheers!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
xiaobai5883
xiaobai5883
xiaobai5883
Follow
Joined
Mar 11, 2021
•
Mar 13 '21
Dropdown menu
Copy link
Hide
having alternative (codeless or with minimal coding) is always welcoming~! please do update and let us know.. thank you so much~!
Like comment:
Like comment:
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Unlock the Power of Custom Formatting in Go: A Deep Dive into the Formatter Interface
Archit Agarwal -
Nov 27
Every Student Needs to Join a Tech Community
Iqra Firdose  -
Dec 10
Creating and Paying a Freight Invoice with the Rapyd API and FX
Drew Harris -
Dec 10
How to Use Swagger UI Locally: A Step-by-Step Guide
Wanda -
Dec 10
Taq Karim
Follow
Serverless all day 💪
Location
New York
Education
The Cooper Union
Work
Principal Software Engineer at Oracle
Joined
Dec 13, 2019
More from
Taq Karim
Custom Args in Makefile
#
testing
#
go
#
tutorial
Generate Lambda Layer Artifacts w/Docker
#
docker
#
serverless
#
python
Extending SimpleNamespace for Nested Dictionaries
#
python
#
tutorial
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Data Quality: The Hidden Driver of AI Success - DEV Community,"Data Quality: The Hidden Driver of AI Success - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Jimmy Guerrero
for
Voxel51
Posted on
Nov 12
• Originally published at
voxel51.com
Data Quality: The Hidden Driver of AI Success
#
computervision
#
machinelearning
#
ai
Author:
Markus Woodson
(Machine Learning Engineer at Voxel51)
Data quality is the unsung hero of successful machine learning models. While researchers push the boundaries with ever-evolving architectures, in practice, performance gains come from better data, not just bigger models. Poor-quality inputs—like blurry, underexposed, or overly compressed images; noisy lidar or depth signals; misaligned point clouds; incomplete or inconsistent 3D object reconstructions; and inaccurate sensor fusion between modalities—set models up for failure, much as they hinder human perception. In addition, balancing your data across all potential issues and edge cases enables the model to generalize across conditions. Even the best algorithms can’t compensate for weak data foundations.
The secret to building robust AI systems lies in exercising the data fundamentals: ensuring completeness, consistency, and relevance, supported by high-quality labeling and thoughtful data curation. In the end, it’s not just about models—it’s how we handle the data that drives meaningful innovation.
In this post, we’ll explore why data quality is essential to building reliable, high-performing machine learning models, especially in computer vision. We’ll see model failure cases due to data and examine just how pervasive these issues are, even in “cleaned” academic datasets. We’ll also discuss what
FiftyOne
has done and will continue to do to help users improve their data quality.
If you’re not yet familiar, hundreds of thousands of AI builders use FiftyOne to build visual AI applications by refining data and models in one place.
Data Quality In the Wild
The quality of your dataset can be critically important, especially in safety-focused applications like autonomous driving. Studies[
1
,
2
,
3
] of real-world incidents have shown that crashes often occur in scenarios where models encounter underrepresented or complex visual conditions, such as extreme glare, low visibility, fog, low-contrast environments, or scenarios where actors in the scene behave in novel and unique ways. These conditions are challenging not only for AI systems but also for human drivers. Finding these issues makes it easier to remedy them. Once found you can increase your data diversity by simulating similar scenarios similar to these. All of your sensors could have issues as well, not just the camera. There are potential hidden issues in your lidar, depth, or other sensor data. Without thorough data curation or generation that includes a variety of lighting, weather, and environmental settings, models are less prepared for these unusual yet impactful edge cases. Incorporating diverse, high-quality data with some of these issues mentioned can help create models that perform reliably across a wider range of scenarios, potentially reducing risks and improving overall safety in real-world applications.
Here are just two examples of failure modes of self-driving systems that led to fatal or serious accidents. In both, you’ll notice the images seen by the vehicle are very atypical when compared to normal driving data. There is fog, harsh lighting, darkness, and a lack of contrast.
You might think these edge cases are hard to come by or cover, but looking at longtime standard datasets like
KITTI
gives a different story. In the below pictures, you can see samples from the KITTI dataset loaded into FiftyOne that I filtered using a simple brightness parameter. You can see that these types of images with issues related to brightness have been commonplace for a while now.
Example images from the KITTI dataset. The samples were filtered by their brightness level to look at only the extreme bright cases using a new panel coming soon to FiftyOne Teams. We can quickly find these types of problematic images and use this information to guide model improvements for these cases.
Data quality is equally crucial in the world of generative AI, where massive datasets like
LAION
play a foundational role in training models such as
Stable Diffusion
. Because the LAION dataset is open, we can see firsthand the types of images that go into shaping these models through websites like
haveibeentrained
. While it includes a wide variety of visual content, it also brings to light common quality issues: near duplicates, exact duplicates, images lacking meaningful content, and all types of issues in between. These types of issues can lead to memorization, regurgitation, or generation of content which does not match the prompt. Additionally, datasets of this scale sourced from the entire internet can inadvertently include problematic material, like
graphic
or
offensive content
, which can influence the outputs of generative models. Such issues highlight the importance of rigorous data curation to ensure models not only generate diverse and creative outputs but also maintain quality and relevance.
Searching LAION for basic queries such as “man running” shows just how prevalent issues are in these datasets. Near and exact duplicates galore! Not even to mention the unrelated or potentially non-useful images present as well.
What Prioritizing Data Looks Like
Some scientists and engineers such as
these
and
these
recognize the importance of data quality, dedicating entire pages of their technical report to how they curate and annotate their large datasets.
From our own experiences building high-performing visual AI systems, we know well that AI/ML specialists struggle with the
challenges of curating high-quality datasets
. That's why we’ve invested in tools and plugins such as the
data quality plugin
for FiftyOne, which helps you find problematic images in your dataset such as blurry images, too bright or too dark images, and potentially noisy images. And
this deduplication plugin
for FiftyOne helps you find near and exact duplicates in your dataset.
But what else can we do? Going forward these are some of the ways we at Voxel51 ML are thinking about data quality:
Labels have quality issues as well!
Just because you have labeled your data does not mean those labels are perfect. Human-labeled data can be prone to errors. We want to make finding and preventing these errors easier, improving annotation workflows.
Issues are problem-specific.
You may consider the same data sample an issue while another might not, it all depends on the use case. As an extreme case, if you are training a model that should work in blurry and clear conditions, then blurry samples should be allowed. But if you expect all your test samples to be clear, then training on blurry samples might be a waste of valuable compute and model capacity. Customizing the detection of issues to your use case is critical.
Data curation is key.
Recently, we’ve seen a surge of research in data-centric AI being applied to problems on a sometimes very large scale.
Research
has shown that curating a better dataset can give a better model and reduce training time, saving you both time and money. Going forward, we think better data curation for all types of problems in computer vision should be a core part of the model development process.
In the near future, you can expect new experiences in FiftyOne that will allow for automated discovery and handling of common quality issues. Building off of our robust
plugin ecosystem
, you can expect features that not only automatically detect issues but also give you, the ML practitioners, full control over defining what issues mean in your use case and quick ways to solve them. Concretely, we will deliver along the following fronts:
Automatically identifying potential issues based on smart presets
The ability to modify said presets to adapt to your dataset and use-case needs
Novel dataset explorations experience through the axes of these issues. Think, looking for bright or dark images, all through the touch of a slider.
All of this and more will be delivered through our newest feature,
Python Panels
!
P.S. There is a sneak peek of this new panel in this post!
Conclusion & Next Steps
We explored how essential data quality is to building high-performing visual AI systems and the practical challenges that arise in the process. By equipping AI/ML specialists with tools like FiftyOne, we’re aiming to make data curation, analysis, and model building not only more efficient but also fundamentally better.
For those ready to take the next step, FiftyOne offers a variety of resources and community support to get you started:
FiftyOne Community Slack:
Join thousands of fellow AI builders in our
Slack community
, where you can exchange ideas, ask questions, and get insights directly from experienced developers and scientists working on real-world AI challenges.
Getting Started Workshops:
Attend one of our
workshops
, which covers everything you need to get up and running with FiftyOne for streamlined dataset and model workflows.
GitHub Repository:
Access the
FiftyOne GitHub repo
to dive into our open-source code, tutorials, and sample projects designed to help you incorporate FiftyOne into your own AI/ML workflows.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Using AI for Real-Time Customer Sentiment Tracking
Daniel -
Nov 25
A beginner's guide to the Flux-1.1-Pro model by Black-Forest-Labs on Replicate
Mike Young -
Nov 25
Part 4: Building Your Own AI - Diving Deeper into Supervised Learning
Trix Cyrus -
Dec 8
Speech to Text using Assembly AI
Sunder Kumar -
Nov 24
Voxel51
Follow
More from
Voxel51
Dec 12 - Virtual AI, Machine Learning and Computer Vision Meetup with Meta AI
#
ai
#
machinelearning
#
computervision
#
datascience
Data-Centric Visual AI Linkedin Learning course!
#
computervision
#
ai
#
machinelearning
#
datascience
Five Must Read Data-Centric AI Papers from NeurIPS 2024
#
computervision
#
machinelearning
#
ai
#
datascience
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Test-Driven Development For Analytics Engineering - DEV Community,"Test-Driven Development For Analytics Engineering - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Aram Panasenco
Posted on
Oct 18
• Edited on
Oct 21
Test-Driven Development For Analytics Engineering
#
data
#
analytics
#
tdd
#
datascience
As long as end users trust their queries against raw data more than they trust the analytics engineering team and their data models, nothing the analytics engineering team does matters. While so-called 'best practices' are almost never applicable for every kind of organization and every situation, I do believe that every analytics engineering team can benefit from adopting test-driven development. The most important thing about test-driven development is not just that it enhances data quality and the perception of data quality (though it does do those things), but that it enables analytics engineers to have trust and confidence in themselves.
What is test-driven development?
Test-driven development
(TDD), as the name implies, is about making tests drive the development process. The steps of test-driven development are:
Gather concrete requirements.
Turn a requirement into a test.
Run the test - it should fail.
Implement the change.
Run all tests - they should all pass now.
Repeat from step 1.
Following this simple process will have huge effects on the quality of your data models and your relationships with data customers.
The meaning of trust
In his 2016 book
The Speed of Trust
, Stephen M. R. Covey defines trust as consisting of four components:
Integrity
Intent
Capabilities
Results
Covey also writes that being trusted by others has to start by trusting yourself. Do you as an analytics engineer have confidence in your own integrity, intent, capabilities, and results? That confidence is the prerequisite to being trusted by your data customers.
How TDD enhances your confidence in your own integrity
In my experience, analytics engineers are quick to promise ""I'll fix it so it doesn't happen again,"" but are hesitant to promise ""I'll catch it first if it happens again."" Subconsciously they betray their own confidence in their own integrity. After all, how can you be sure you've fixed an issue if you don't know whether it's happening?
TDD allows you to give a factual statement like ""I've written a test that reproduces the issue you're experiencing"" instead of giving promises about things potentially outside of your control. Depending on the maturity of your automated testing and alerting framework, you may be able to say even more. For example: ""Once deployed, this test will run daily and will alert us if this issue reoccurs.""
Data issues tend to spontaneously ""un-fix"" themselves all the time, and you don't necessarily have control over that. But you do have control over your development process. Writing tests first will enable you to communicate what you're doing instead of burying yourself deeper and deeper in promises. This will grow confidence in your integrity, from yourself as well as from others.
How TDD enhances your confidence in your own intent, capability, and results
Put yourself in the shoes of a data customer. You've carefully prepared examples of the kind of output you need and sent them to the analytics engineer. The engineer comes back with a finished data model. While validating the results, you find that one of the very examples you've given them isn't even correct in the output! Would you believe that analytics engineer really cares about helping you? That they have solid abilities? That they can drive results? And what effect would all this have on that engineer's opinion of themselves?
Most of the time, this kind of experience is caused not by a lack of care or ability, but by a
regression
. Regressions happen even to the most caring and capable engineers. Here's how: The analytics engineer works on the examples one at a time. However, the logic changes they make to satisfy the second example can inadvertently break the first example. The problem compounds as new edge cases are introduced. Working on the tenth example can break any one of the previous nine. Without automated testing, these regressions can be almost impossible to catch.
Over the course of the last major analytics engineering project I've worked on, the tests I wrote caught three regressions I'd accidentally introduced. If I hadn't had the tests, there's a chance that I could be thought of (and think of myself) as a sub-par engineer who doesn't even get the example rows right three times in a single project. Instead, I have complete confidence that all the examples are satisfied, and that I can take on any additional complexity without introducing regressions. This is a matter of discipline, not of intelligence or ability.
How to do test-driven development
Start with the data the customer actually needs
It all starts by talking to the data consumer and understanding their needs. If something broke, what's an example? Turn that example into your test. If something new is needed, what does it look like? Ask them to mock up a few examples cases in a spreadsheet-like format. The rows of that spreadsheet become your tests.
Don't worry about source data, facts, or dimensions in your tests. Focus on the highest level of what the customer needs. Ask them and they'll be able to represent their needs in a spreadsheet-like format every time.
Think about the columns of that spreadsheet. One or more of those columns will be able to be used as an identifier of that particular example row. There should only be one row with that identifier. Values in some of the rest of the columns will represent the business logic of the example. Therefore, we need to test two things: Does that example row exist, and do the business logic values match?
A
dbt data test
returns failing records. In other words, the test has succeeded when no rows are returned. Here's an example implementation:
with
row_count
as
(
select
count
(
*
)
as
n
from
{{
ref
(
""denormalized_view""
)
}}
where
id1
=
'some_id'
and
id2
=
'other_id'
)
select
'Not exactly 1 row'
as
error_msg
from
row_count
where
n
<>
1
union
all
select
'Row test failed'
as
error_msg
from
{{
ref
(
""denormalized_view""
)
}}
where
id1
=
'some_id'
and
id2
=
'other_id'
and
not
(
value1
=
'some example value'
and
value2
is
not
null
and
value2
=
'some other value'
and
abs
(
value3
)
<=
0
.
01
)
Enter fullscreen mode
Exit fullscreen mode
Let's dissect what's happening in this query. There are two
select
statements joined together with a
union all
. The first will return a failing record if the row identified by the identifier(s) doesn't exist in the data. This is important so we don't inadvertently pass a test when the data is not there in the first place. The second identifies that same row, and then looks for any discrepancies in the business logic values. That's easiest to achieve by wrapping the expected values in a
not()
.
Do watch out for null values. Due to
three-valued logic in SQL
, the filter
not(column = 'value')
will not return rows where the column is null.  I recommend testing for nulls separately using dbt's generic
not_null test
so that you don't have to remember each time.
This kind of test is very easy to copy and paste and adapt quickly. It's also easy to read and maintain. This will be all you need 90% of the time.
It's easy to accidentally write a SQL query that produces no rows. That's why it's also easy to write a dbt data test that accidentally passes. The test should be written and run first, before any development work is done. The test should fail. Then the change should be implemented, and the test should succeed.
Documentation-driven development
In addition to tests, I also encourage you to write the documentation as you're gathering requirements, not after the data model is written. dbt allows
model properties
(including documentation) to be defined before any SQL is written.
For example, if you're developing a transactions model with your accounting team, you can create the file
models/denormalized/accounting/_accounting__models.yml
:
models
:
-
name
:
accounting_transactions
description
:
Transactions table for accounting
columns
:
-
name
:
transaction_key
description
:
Synthetic key for the transaction
tests
:
-
not_null
-
unique
-
name
:
action_date
description
:
Date of the transaction
tests
:
-
not_null
Enter fullscreen mode
Exit fullscreen mode
You should be taking notes when gathering data customer requirements anyway. Instead of writing the notes down in something like a Google Doc or an email, take notes in this YAML format instead. That'll get you kick-started on your documentation and testing.
Edge cases that don't exist in production data
You will often have to write logic that encompasses things that don't exist in production data, but potentially could. On one hand, it's good to be defensive and handle potential mistakes before they happen. On the other hand, it's very hard to write tests for things that aren't there.
There are a couple of potential solutions here:
Just write the logic, don't write any additional tests.
If there's a non-production environment of the source system, the data model could be pointed to that non-production environment for development and pull requests. Then all kinds of edge cases could be created in the non-production system and tests written as normal.
If there is no non-production environment of the source system,
dbt unit tests
can be used. Using the unit test syntax, you can define your edge case inputs in your model's YAML.
The most realistic approach is to just write the logic without writing additional tests. Analytics engineers work on tight enough deadlines that writing tests for things that aren't there is just not worth it in most situations. In the minority of cases where the logic's correctness is critical enough to justify the additional time investment, approach 2 or 3 above can be used.
Enforcing test-driven development
It's a good idea to work towards enforcing test-driven development in your analytics engineering team. Rather than surprising folks with a new policy, I recommend setting a deadline by which test-driven development will be mandated, and ensuring the team gets familiar with it before the deadline.
Here's an example workflow that incorporates test-driven development:
All dbt models are stored in a Git repo with a write-protected production branch. All changes to production have to come through pull requests with at least one peer approval.
Analytics engineers create feature branches off the production branch and open pull requests when the features are ready.
Every peer reviewer is expected to only approve the pull request if they see tests corresponding to every feature. If they don't see corresponding tests, that means TDD wasn't followed and the pull request shouldn't be approved.
What about TDD for data engineering?
It's tempting and somewhat justified to make data engineers follow TDD as well. However, the value proposition of TDD for data engineering is not as clear as for analytics engineering.
Since the predominant data warehousing paradigm shifted from ETL (extract-transform-load) to ELT (extract-load-transform), the role of data engineers also changed. Data engineers are now focused on querying APIs and then loading the responses into the data warehouse in the rawest possible form.
Analytics engineers work inside the data warehouse, which is a deterministic environment. The same inputs always produce the same outputs, and the logic used to create the outputs is complex. That's a perfect environment for TDD to be impactful.
Data engineers work in an almost opposite environment. Since they just extract and load, there's no logic at all. At the same time, they have to interface with external systems, which can have a whole host of unpredictable issues.
It's definitely possible to do test-driven development as a data engineer, but it's difficult and produces questionable benefits. Suppose you're loading data from the Facebook Ads API. How do you do that in a test-driven way? You could use
requests-mock
to simulate possible inputs with corresponding outputs and errors. However, the only thing you do with the output is load it into the data warehouse as directly as possible, so there's not much to test there. Additionally, you may not know what the possible errors are, and even if you do, there's nothing you can do about them from your end except retry.
For these reasons, I don't attempt to follow test-driven development when writing extract-and-load processes, and instead focus on
architectural simplicity
and plenty of retry mechanisms written with
backoff
or
tenacity
.
Conclusion
If you're an analytics engineer, I hope this post has convinced you to give test-driven development a try. If you're an analytics engineering team leader, I hope you consider making test-driven development a requirement for your team.
Analytics engineering is uniquely well-suited to test-driven development. The cost of effort of creating tests from end user requirements is low, and the cost of regressions from complex and untested business logic in your data models is high. In addition, test-driven development boosts trust in data throughout your organization, and overall makes the experience of working with data more pleasant and fun for everyone.
Cover image generated using
FLUX.1-schnell
on
HuggingFace
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
The Importance of Data for GenAI
purvika -
Oct 16
Claude 3.5 AI Assistant Achieves 87% Success Rate in Computer Interface Navigation Study
Mike Young -
Nov 19
ViT Enhancements for Abstract Visual Reasoning: 2D Positions and Objects
Mike Young -
Oct 16
AI Creates Digital Twins of 1,000 People with 85% Accuracy in Behavioral Predictions
Mike Young -
Nov 19
Aram Panasenco
Follow
More insightful than an AI... for now. 😉
Joined
May 12, 2024
More from
Aram Panasenco
The Simplest Data Architecture
#
data
#
architecture
#
dataengineering
#
analytics
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How to check for quality? Evaluate data with AWS Glue Data Quality - DEV Community,"How to check for quality? Evaluate data with AWS Glue Data Quality - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Wendy Wong
for
AWS Heroes
Posted on
Jul 7, 2023
• Edited on
Jan 21
How to check for quality? Evaluate data with AWS Glue Data Quality
#
aws
#
etl
#
analytics
#
tutorial
Getting Started with AWS Glue (3 Part Series)
1
How to ETL with AWS Glue and AWS Glue Studio to transform your data - Part 1
2
How to ETL with AWS Glue and Amazon Glue Studio to transform your data - Part 2
3
How to check for quality? Evaluate data with AWS Glue Data Quality
Data is the new oil
The Women in Data Science (WiDS) Conference 2017
trailer
from Stanford University aimed to inspire the audience in the field of data science.
In the trailer, the panelist mentioned the power of data.
Who gets to control data, controls the society that's the reality. -- Bell Wei (Carolyn Guilding Chair in Engineering and Innovative Learning, San Jose State University)
Having access to the right data and understanding your data will empower you to make data-driven decisions for your organization.
Data understanding
is Step 2 of the Data Analytics and Data Science workflow called Cross-industry standard process for data mining (i.e. CRISP-DM).
You may read more about CRISP-DM
here
. The second step:
Data understanding – What data do we need or is available? Is it clean?
This is the data science workflow using CRISP-DM explaining all the stages and the image is from
Wikipedia
:
Learning Objectives
In this lesson you will learn the following:
How to create rules with DQDL
What is AWS Glue?
What is AWS Glue Data Quality?
Solution Architecture
How to check for data quality results?
Who owns data quality?
Is data quality the responsibility of the data engineer building your ETL data pipelines? Is it the responsibility of the data analyst building the data modelling? Or the data scientist building hypotheses and machine learning models responsible for data quality?
Take a look at this Twitter post on data quality
here
.
In fact we can all play part to ensure that with good quality data, we can improve our data analysis and machine learning model performance to make business decisions.
What is AWS Glue?
AWS Glue is an AWS service that allows data engineers, business analysts, data analysts, developers and data scientists to integrate data from multiple sources and also perform ETL.
It is serverless
data integration service
to allow you to easily scale your workloads in preparing data and moving transformed data into a target location.
You do not need to provision any servers or clusters before using AWS Glue with a managed serverless experience.
You may bring in your own code or notebook to create an ETL job on demand, batch or streaming. You may create an AWS Glue data catalog to make data available for others to use.
You may also use AWS Glue Studio a visual editor to create your ETL pipeline.
What is AWS Glue Data Quality?
AWS Glue Data Quality is a new product feature of AWS Glue that is now
generally available
since 6 June 2023.
AWS Glue Data Quality allows you to measure and monitor 'good or bad data' in your ETL pipelines before it enters your data lake or data warehouse to ensure high quality data is available to make data-driven decisions.
What are the benefits?
In the
AWS Glue Developer Guide
for AWS Glue Data Quality, the benefits include the following:
Serverless – there is no installation, patching or maintenance.
Get started quickly – AWS Glue Data Quality quickly analyzes your data and creates data quality rules for you.
Improvise your rules to check the integrity and accuracy of the data
Evaluate data quality and make confident business decisions
Zero in on bad data with errors
Pay as you go - There are no fixed costs and you pay for your usage when you use AWS Glue Data Quality
Data quality checks - you can implement checks in the AWS Glue data catalogue
You may create rules to check the profile of your dataset.
You may get started quickly
Solution Architecture
Below is my own solution overview of the new product feature of AWS Glue Data Quality.
Dataset
Let's examine the Amazon Data Science Books Dataset from
Kaggle.com
Pre-requisite
You may read this
blog
to learn how to get started with AWS Glue Data Quality.
You may read this blog
How to ETL with AWS Glue - Part 1
.
You may dive deeper with a practical example in
Part 2
.
You have an AWS Account, if you do not have an account you may learn how to create one
here
.
Tutorial 1: Add the Evaluate Data Quality transform to the visual job in AWS Glue Data Studio
In this
tutorial
you may refer to the instructions from the AWS Glue User Guide.
Step 1: Log into your AWS account as an
IAM Admin User
.
Step 2: Navigate to Amazon S3 and
create your bucket
in an AWS region of your preference and click
Create bucket
.
Step 3: Upload the
Amazon data science books dataset
into your Amazon S3 bucket and click
Upload
.
Step 4: Navigate to AWS Glue dashboard.
Click
Create a job to evaluate data quality
Step 5: Click on
Visual
and select
Evaluate Data Quality
.
Step 6: Add the data quality node.
On the AWS Glue Studio console, choose
Visual with a source and target
from the Create job section.
Choose Create
.
Step 7: Choose a node on which to apply the data quality transformation. Currently there is no node selected.
You may select the transform node (i.e. Transform - ApplyMapping).
Step 8: On the left-hand side click the blue plus sign and select from the drop-down menu
Evaluate Data Quality
towards the bottom section. You may name this job and be sure to save it e.g. GlueDataQuality_Tutorial.
Step 9: The selected
Evaluate Data Quality transform
node will be displayed in the visual editor.
On the right-hand side you may inspect if you would like to retain the current parent node or change it from the drop-down menu. (Note: The parent node is connected to the Evaluate Data Quality node).
Step 10: You may validate data quality rules across multiple datasets. The rules that support multiple datasets
include
:
Referential Integrity
DatasetMatch
SchemaMatch
RowCountMatch
AggregateMatch
When you add multiple inputs to the Evaluate Data Quality Transform, you need to select your
primary
input.
Select your primary input which is the
Amazon data science books dataset
to validate data quality for.
All other nodes or inputs are considered as references.
Use
Evaluate Data Quality transform
to identify specific records that failed data quality checks.
(Note: New columns flagged as bad records are added to the primary dataset).
Step 11:  In the
Data Source - S3 bucket
select the S3 bucket where the dataset or primary input is saved.
Step 12: Click on
Output schema tab
to modify any data types e.g. change price from string to integer.
Click
Apply
to change the data types.
Step 13: Select the
Data target - S3 bucket
and save the transformed dataset in this location.
Tutorial 2: Create rules using DQDL builder
Step 1: Preview the
Amazon data science books dataset
and let's create a rule using the DQDL rule builder to check for the completeness of data.
You may browse the available data quality rules from the
Rules type
tab which include the following:
ColumnCount
ColumnLength
ColumnExists
ColumnDataType
ColumnValues
ColumnNameMatchesPattern
Completeness
CustomSql
DataFreshness
DatasetMatch
DistinctValuesCount
Entrophy
IsComplete
IsPrimaryKey
Sum
Uniqueness
ReferentialIntegrity
Mean
RowCount
RowCountMatch
StandardDeviation
UniqueValueRatio
SchemaMatch
I selected the data quality rule for
Completeness
, because I would like to check the percentage of missing data greater than 80% for the following variables:
Price
Price (that includes used books)
Number of book reviews
Average rating
Firstly preview the
primary data source
to understand the data.
The following rules were created in the DQDL Builder in the * Schema* tab.
Rules= [
    Completeness ""avg_reviews""> 0.8, Completeness ""n_reviews""> 0.8, Completeness ""price"" > 0.8, Completeness ""price (including used books)"" > 0.8
]
Enter fullscreen mode
Exit fullscreen mode
The Completeness rule will check for the specified columns if there is greater than 80% of non-null values present in the primary data source.
Tutorial 3: Configure Data Quality Outputs
Step 1: After the data quality rules are created, you can select additional options to be included in the data quality results output.
I have selected two additional options:
Action: publish actions to Cloudwatch
Data Quality Results: to flag fail or pass results
Below is an image of the rule outcomes node.
Step 2: Under
Data Quality Transformation Output
, I also checked the box for
Original data
as this will also append additional columns to the primary dataset to indicate bad errors.
Tutorial 4: Configure Data Quality actions
After a data quality rule is created, you may select actions for CloudWatch to publish metrics or stop jobs based on a
criteria
.
Actions in CloudWatch are also published to Amazon Eventbridge and can be used to create alert notifications.
On ruleset failure
Fail job after loading data to target
Fail job without loading to target data
Tutorial 5: View data quality results
Click
Save
and initiate the AWS Glue job by selecting
Run
.
You may click
Run details
to inspect the progress of the Glue job.
After the job has completed, select the
Data Quality New
tab to inspect the results.
You will be able to see that the data quality rules have passed successfully and you may click
Download results
as a csv file
[
    {
        ""ResultId"": ""dqresult-xxxxxxxxxxxxxxxxxxxxxxxxxx"",
        ""Score"": 1,
        ""RulesetName"": ""EvaluateDataQuality_nodexxxxxxxxxx"",
        ""EvaluationContext"": ""EvaluateDataQuality_nodexxxxxxxxx"",
        ""StartedOn"": ""2023-07-07T08:57:48.117Z"",
        ""CompletedOn"": ""2023-07-07T08:58:08.203Z"",
        ""JobName"": ""GlueDataQuality_tutorial"",
        ""JobRunId"": ""xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"",
        ""RuleResults"": [
            {
                ""Name"": ""Rule_1"",
                ""Description"": ""Completeness \""avg_reviews\"" > 0.8"",
                ""EvaluatedMetrics"": {
                    ""Column.avg_reviews.Completeness"": 1
                },
                ""Result"": ""PASS""
            },
            {
                ""Name"": ""Rule_2"",
                ""Description"": ""Completeness \""n_reviews\"" > 0.8"",
                ""EvaluatedMetrics"": {
                    ""Column.n_reviews.Completeness"": 1
                },
                ""Result"": ""PASS""
            },
            {
                ""Name"": ""Rule_3"",
                ""Description"": ""Completeness \""price\"" > 0.8"",
                ""EvaluatedMetrics"": {
                    ""Column.price.Completeness"": 1
                },
                ""Result"": ""PASS""
            },
            {
                ""Name"": ""Rule_4"",
                ""Description"": ""Completeness \""price (including used books)\"" > 0.8"",
                ""EvaluatedMetrics"": {
                    ""Column.price (including used books).Completeness"": 1
                },
                ""Result"": ""PASS""
            }
        ]
    }
]
Enter fullscreen mode
Exit fullscreen mode
If you are running multiple data quality jobs, you may filter the data quality results by date and time.
If you navigate to the tab
Script
you will be able to see that AWS Glue Studio automatically created Python code for the  transformation steps that you could easily download for reusability.
Conclusion
You have learnt how to set up data quality rules in AWS Glue Data Quality using the visual editor of AWS Glue Studio. You have explored how to create an ETL job and examine the data quality results for a pass or fail from rules that were created.
Until the next lesson happy learning! 😀
AWS Glue Data Quality Quick Start videos on Youtube
If you would like to learn more you may watch the following videos
AWS Glue Data Quality Overview
Measure and Monitor Data Quality of your Datasets in AWS Glue Data Catalog
Introducing AWS Glue Data Quality for ETL Pipelines
Reference
AWS Glue Data Quality
Getting started with AWS Glue Data Quality for ETL pipelines
Data Quality Definition Language reference
AWS Glue User Guide
Top Announcements of AWS re:invent 2022
Join the Preview AWS Glue Data Quality
New announcements from AWS Glue
AWS Glue for Ray
AWS Glue Studio
Next Week: AWS Builders Online Series - 13 July 2023
You may
register
to join AWS Builders Online Series and learn from AWS experts on architectural best practices.
Getting Started with AWS Glue (3 Part Series)
1
How to ETL with AWS Glue and AWS Glue Studio to transform your data - Part 1
2
How to ETL with AWS Glue and Amazon Glue Studio to transform your data - Part 2
3
How to check for quality? Evaluate data with AWS Glue Data Quality
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Deploy and Manage GitLab Runners on Amazon EC2
Fernando Muller Junior -
Dec 9
Day 11: Docker Compose
Jonas Scholz -
Dec 11
Types: char and boolean
Ricardo Caselati -
Dec 9
Google Axion: A New Leader in ARM Server Performance
Dimitrios Kechagias -
Dec 8
AWS Heroes
Follow
A vibrant, worldwide group of AWS experts.
More from
AWS Heroes
Joins, Scale, and Denormalization
#
aws
#
postgres
#
aurora
#
dsql
DynamoDB-style Limits for Predictable SQL Performance?
#
aws
#
postgres
#
distributed
#
database
Aurora DSQL - Simple Inserts Workload from an AWS CloudShell
#
aws
#
aurora
#
dsql
#
yugabytedb
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Transform Your Data Like a Pro With dbt (Data Build Tool) - DEV Community,"Transform Your Data Like a Pro With dbt (Data Build Tool) - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Mwenda Harun Mbaabu
Posted on
Jun 8, 2023
• Edited on
Jul 8
Transform Your Data Like a Pro With dbt (Data Build Tool)
#
data
#
datascience
#
python
#
sql
In today's data-driven world, efficient data management and processing are crucial for businesses to gain valuable insights and make informed decisions. This is where data build tools come into play.
In this guide, we will explore the concept of data build tools, their importance, and how they can streamline the data engineering process. We'll focus on one of the popular tools in this domain: dbt (Data Build Tool).
What is dbt?
dbt, which stands for Data Build Tool, is an open-source command-line tool that facilitates the development and management of data transformation pipelines. It empowers data analysts and engineers to transform raw data into structured formats in a modular and maintainable manner.
With dbt, you can leverage the power of SQL to define transformations, validate data quality, and deploy changes seamlessly.
From the
official documentation
dbt™ is a transformation workflow that lets teams quickly and collaboratively deploy analytics code following software engineering best practices like modularity, portability, CI/CD, and documentation. Now anyone who knows SQL can build production-grade data pipelines.
Key Features and Benefits:
Modular Transformations:
dbt introduces the concept of models, which represent individual units of transformation in your data pipeline. Models are SQL queries that define how raw data is transformed into structured output. By breaking down your transformations into modular components, you can enhance reusability, maintainability, and collaboration within your data team.
Testing Data Quality:
Quality assurance is essential when working with data. dbt provides a built-in testing framework that allows you to define data tests using SQL queries. You can verify data types, check for missing values, validate uniqueness, and perform various data quality checks. These tests ensure the integrity and reliability of your transformed data.
Source Control and Documentation:
With dbt, you can version control your data transformation logic and collaborate effectively with your team. It integrates seamlessly with popular version control systems like Git. Additionally, dbt allows you to document your models and transformations using Markdown. This feature helps in maintaining a central knowledge repository, enhancing understanding, and facilitating collaboration.
Seamless Deployment:
dbt simplifies the process of deploying data transformations to your data warehouse or data lake. It provides deployment commands that ensure your changes are applied consistently and reliably. This feature enables you to iterate quickly, experiment with different transformations, and promote changes to production with confidence.
Getting Started with dbt:
To begin using dbt, follow these steps:
Install dbt on your local machine or development environment. Refer to the official dbt documentation for installation instructions specific to your operating system.
Set up a connection to your data warehouse or data lake. dbt supports various database systems, including popular ones like Snowflake, BigQuery, and Redshift.
Define your models using SQL files. Organize your transformations into separate models based on their logical purpose and dependencies.
Configure sources to define the connection details and schemas for your raw data sources. This step enables dbt to understand the structure of the source data.
Write tests to validate the quality and integrity of your transformed data. Utilize the testing framework provided by dbt to create SQL queries that cover your data quality requirements.
Run dbt commands to execute your transformations, perform data tests, and generate documentation. Use the dbt command-line interface to run commands such as dbt run, dbt test, and dbt docs generate.
Main commands:
Here are he main commands used dbt (Data Build Tool) commands along with their explanations:
1).
dbt init:
Initializes a new dbt project in the current directory. This command creates the necessary project structure and configuration files.
dbt
init
Enter fullscreen mode
Exit fullscreen mode
2).
dbt run:
Runs the dbt models in the project. This command executes the SQL queries defined in the dbt models and creates the corresponding tables or views in the target database.
dbt
run
Enter fullscreen mode
Exit fullscreen mode
3).
dbt test:
Runs tests defined in the dbt project. This command validates the data in the created tables or views based on the defined tests and raises errors if any discrepancies are found.
dbt
test
Enter fullscreen mode
Exit fullscreen mode
4).
dbt compile:
Compiles the dbt models without executing them. This command generates the compiled SQL files that can be reviewed or used for other purposes.
dbt
compile
Enter fullscreen mode
Exit fullscreen mode
5).
dbt snapshot:
Creates a snapshot of the data in the target database. This command takes a snapshot of the tables or views defined in the dbt project for comparison or auditing purposes.
dbt snapshot
Enter fullscreen mode
Exit fullscreen mode
6).
dbt seed:
Loads seed data into the target database. This command populates the specified tables with initial data defined in seed files.
dbt
seed
Enter fullscreen mode
Exit fullscreen mode
7).
dbt clean:
Removes the artifacts generated by dbt, including tables, views, and compiled SQL files. This command is useful for cleaning up the target database.
dbt
clean
Enter fullscreen mode
Exit fullscreen mode
8).
dbt docs generate:
Generates the documentation for the dbt project. This command creates HTML documentation that describes the models, tests, and sources defined in the project.
dbt
docs
generate
Enter fullscreen mode
Exit fullscreen mode
9).
dbt docs serve:
Serves the generated documentation locally. This command starts a local web server that allows you to view the generated documentation in a web browser.
dbt
docs
serve
Enter fullscreen mode
Exit fullscreen mode
Conclusion:
Data build tools like dbt are essential for modern data engineering workflows. They enable efficient data transformation, testing, documentation, and deployment, resulting in improved collaboration, maintainability, and data quality.
By following this guide, you can get started with dbt and harness its power to build robust and scalable data pipelines. Explore the official dbt documentation for detailed instructions and examples, and unleash the true potential of your data engineering endeavors.
This tutorial is part of Data Build Tool Ultimate Guide that am creating on GitHub and i am inviting anyone with want to collaborate on building a one stop guide to Data Build Tool.
Use this link,
https://github.com/HarunMbaabu/Data-Build-Tool-Ultimate-Guide
to access the repository.
Important Link:
1).
dbt Ultimate Guide
.
2).
Official Documentation
.
3).
Data Build Tool Repository
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
PostgreSQL Connection Pooling vs No Pooling: Benchmark Analysis
Naresh Nishad -
Nov 18
Remaking a rule-engine DSL
Choon-Siang Lai -
Nov 18
Batch, Mini-Batch & Stochastic Gradient Descent
Super Kai (Kazuya Ito) -
Nov 17
Beginner's Guide to Python: A Quick Tutorial - 2
Ajmal Hasan -
Dec 1
Mwenda Harun Mbaabu
Follow
A Data Engineer and technical with over 5 years of experience. Let discuss your next project: mbaabuharun8@gmail.com
Location
Kenya
Education
Bsc( Mathematics and Computer Science)
Pronouns
he/him
Joined
Jan 5, 2020
More from
Mwenda Harun Mbaabu
LuxDevHQ (Lux Academy and Data Science East Africa) Training Program in AI, Data Science, Analytics, and Data Engineering
#
luxdevhq
#
python
#
sql
#
ai
5-Week Data Career Boot Camp: Orientation and Foundations.
#
python
#
sql
#
data
#
datascience
One AI Tool For All Your Needs❗️
#
ai
#
virtualmachine
#
delle
#
datascience
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Run environment-aware database migrations with yuniql - DEV Community,"Run environment-aware database migrations with yuniql - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Rodel E. Dagumampan
Posted on
Jun 1, 2020
Run environment-aware database migrations with yuniql
#
database
#
devops
#
dotnet
#
dotnetcore
Background
Let me start this again by saying, I am writing this both to create awareness on
yuniql
; an open source database devops tool I am maintaining and to share some experiences that I hope would help our community make software better, faster, safer.
The solutions here works with SqlServer, PostgreSql and MySql. :) Not let's get to meat of the story...
One of the challenges when working with databases are variations of scripts and schema definitions driven by compliance to existing governance and practicalities.
Cross-database queries 🍵
Some organization requires database names to represent the environment where it's created. Let’s say we have HR and PAYROLL databases where each have DEV, TEST and PROD environments. As a result, in DEVELOPMENT environment we have
HRDB-DEV
and
PAYROLLDB-DEV
. This script certainly cannot be applied to TEST and PROD databases as the database names are suffixed with an environment code DEV.
CREATE
VIEW
[
dbo
].[
vw_employee_timesheets
]
AS
SELECT
E
.
employee_id
,
E
.
first_name
,
E
.
last_name
,
E
.
position
,
T
.
checkin_time_utc
,
T
.
checkout_time_utc
FROM
[
HRDB
-
DEV
].[
dbo
].[
employees
]
E
INNER
JOIN
[
PAYROLLDB
-
DEV
].[
dbo
].[
timesheets
]
T
ON
E
.
employee_id
=
t
.
employee_id
GO
Enter fullscreen mode
Exit fullscreen mode
Environment-specific scripts 🍈
In another case, we may want to create partitioned tables best-fit for big data in TEST and PROD environment. We can observe the relatively complex configuration of the same table when created in PROD.
CREATE
PARTITION
FUNCTION
RangePartFunction
(
datetime
)
AS
RANGE
RIGHT
FOR
VALUES
(
'20200101'
,
'20200201'
);
CREATE
PARTITION
SCHEME
RangePartScheme
AS
PARTITION
RangePartFunction
TO
([
PartBefore2020
],
[
Part202001
],
[
Part202002
]);
CREATE
TABLE
[
dbo
].[
timesheets
](
[
timesheet_id
]
[
int
]
IDENTITY
(
1
,
1
)
NOT
NULL
,
[
employee_id
]
[
int
]
NOT
NULL
,
[
checkin_time_utc
]
[
datetime
]
NOT
NULL
,
[
checkout_timeutc
]
[
datetime
]
NULL
,
CONSTRAINT
[
PK_timesheets
]
PRIMARY
KEY
CLUSTERED
([
timesheet_id
]
ASC
)
)
ON
RangePartScheme
(
checkin_time_utc
);
CREATE
CLUSTERED
INDEX
IDX_Part
On
[
dbo
].[
timesheets
](
datetime
)
ON
RangePartScheme
(
checkin_time_utc
);
Enter fullscreen mode
Exit fullscreen mode
But wait, this may not be necessary for local development and testing where a minimal set of samples are being loaded. A simpler model like this may just work.
CREATE
TABLE
[
dbo
].[
timesheets
](
[
timesheet_id
]
[
int
]
IDENTITY
(
1
,
1
)
NOT
NULL
,
[
employee_id
]
[
int
]
NOT
NULL
,
[
checkin_time_utc
]
[
datetime
]
NOT
NULL
,
[
checkout_timeutc
]
[
datetime
]
NULL
,
CONSTRAINT
[
PK_timesheets
]
PRIMARY
KEY
CLUSTERED
([
timesheet_id
]
ASC
)
)
ON
[
PRIMARY
];
Enter fullscreen mode
Exit fullscreen mode
Solutions
yuniql addresses this in two ways:
via token replacement
via environment-reserved directories/folders.
To begin with, lets create a new repository repository. A yuniql repository is nothing but a git-ready repository to hold all your scripts files. For guide on how to get started with yuniql, please visit
https://yuniql.io/docs/
.
# install yuniql cli
dotnet tool
install
-g
yuniql.cli
# prepare your repository
cd
c:
\t
emp
md yuniql-environment-aware
cd
c:
\t
emp
\y
uniql-environment-aware
# initialize your repository
yuniql init

c:
\t
emp
\y
uniql-environment-aware>yuniql init
INF   2020-05-31T...   Created script directory c:
\t
emp
\y
uniql-environment-aware
\_
init
INF   2020-05-31T...   Created script directory c:
\t
emp
\y
uniql-environment-aware
\_
pre
INF   2020-05-31T...   Created script directory c:
\t
emp
\y
uniql-environment-aware
\v
0.00
INF   2020-05-31T...   Created script directory c:
\t
emp
\y
uniql-environment-aware
\_
draft
INF   2020-05-31T...   Created script directory c:
\t
emp
\y
uniql-environment-aware
\_
post
INF   2020-05-31T...   Created script directory c:
\t
emp
\y
uniql-environment-aware
\_
erase
INF   2020-05-31T...   Created file c:
\t
emp
\y
uniql-environment-aware
\R
EADME.md
INF   2020-05-31T...   Created file c:
\t
emp
\y
uniql-environment-aware
\D
ockerfile
INF   2020-05-31T...   Created file c:
\t
emp
\y
uniql-environment-aware
\.
gitignore
INF   2020-05-31T...   Initialized c:
\t
emp
\y
uniql-environment-aware.
Enter fullscreen mode
Exit fullscreen mode
Token replacement with yuniql 🍒
When using yuniql, we can specify tokens to be replaced during migration run. For the given case, we use
ENVIRONMENT
as token key.
CREATE
VIEW
[
dbo
].[
vw_employee_timesheets
]
AS
SELECT
'hello yuniql!'
message
;
--SELECT E.employee_id, E.first_name, E.last_name, E.position, T.checkin_time_utc, T.checkout_time_utc
--FROM [HRDB-${ENVIRONMENT}].[dbo].[employees] E
--INNER JOIN [PAYROLLDB-${ENVIRONMENT].[dbo].[timesheets] T
--ON E.employee_id = t.employee_id
GO
Enter fullscreen mode
Exit fullscreen mode
Then during migration run, we pass the token key/value pair. In the process, yuniql inspects all tokens in script files and replaces them.
yuniql run
-a
-k
""ENVIRONMENT=DEV""
yuniql run
-a
-k
""ENVIRONMENT=TEST""
yuniql run
-a
-k
""ENVIRONMENT=PROD
Enter fullscreen mode
Exit fullscreen mode
Alternatively, we can pass multiple tokens in a single call.
yuniql run
-a
-k
""ENVIRONMENT=DEV,USERNAME=rdagumampan,SOURCE=AzDevOpsTask""
Enter fullscreen mode
Exit fullscreen mode
Environment-reserved directories with yuniql 🍋
Sometimes it would be simpler to group all scripts in a single environment-reserved directory. While it forces us to make duplicate script files, it can also help us stay organized. In this sample, let’s create
_development
,
_test
and
_production
directories.
When we call
yuniql run
, we can pass
--environment
to demand an environment-aware migrations. yuniql discover all directories, sort and preapare for execution. When environment-reserved directory is present, it only picks the right directory.
yuniql run
-a
--environment
development
yuniql run
-a
--environment
test
yuniql run
-a
--environment
production
Enter fullscreen mode
Exit fullscreen mode
Parting words ㊗️
Whoah! You have reached this far! Thanks! 🍻
Environment-aware migrations using token replacements and environment -reserved directories addressed the variations of scripts demanded by internal enterprise policies and guidelines. You may also find other use cases for token replacements such as annotating the scripts and baseline data.
P.S. Please support yuniql by clicking
GitHub Star!
For a young project like this, a star can help capture more user experiences and improve the tool in its early stage.
https://github.com/rdagumampan/yuniql
Cheers!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Some comments may only be visible to logged-in visitors.
Sign in
to view all comments.
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Day 10: Docker Networking
Jonas Scholz -
Dec 10
🚀 Introducing Amazon Aurora DSQL: The Next Evolution in Databases 📊
Sarvar Nadaf -
Dec 10
Flexible Angular Builds: A Guide to Angular 19 Build-Time Variables with Docker
Daniel Sogl -
Dec 10
ACID Properties in Databases: Your Shield Against Transactional Chaos
Meqdad Darwish -
Dec 10
Rodel E. Dagumampan
Follow
Software Architect at Ørsted A/S Denmark, Technical Writer, Community Contributor, DevSecOps Enthusiast
Location
Copenhagen, Denmark
Work
Software Architect at Ørsted A/S, Denmark. Where we build and operate the largest offshore wind farms in the world :)
Joined
Jan 3, 2020
More from
Rodel E. Dagumampan
Released yuniql v1.2.25. Multi-tenant support, Oracle and largest set of bug fixes
#
yuniql
#
database
#
devops
#
dataengineering
Seed your SqlServer, PostgreSql and MySql databases with CSV files using yuniql migrations
#
devops
#
sql
#
database
#
dotnet
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Ensuring Quality in Data & AI: A Comprehensive Approach to Quality in the Age of Data & AI [Testμ 2023] - DEV Community,"Ensuring Quality in Data & AI: A Comprehensive Approach to Quality in the Age of Data & AI [Testμ 2023] - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
LambdaTest Team
for
LambdaTest
Posted on
Nov 2, 2023
• Edited on
Jul 22
• Originally published at
lambdatest.com
Ensuring Quality in Data & AI: A Comprehensive Approach to Quality in the Age of Data & AI [Testμ 2023]
#
datascience
#
database
#
ai
#
testing
As AI and ML continuously reshape our technological landscape, ensuring the quality of AI systems has become a critical aspect of software development. The introduction of AI brings a different set of challenges that the traditional QA methods fail to address.
In this session, our speaker,
Bharath Hemachandran
, presented a comprehensive framework for testing and validation in Data & AI projects.
The framework covered five key pillars: data quality, model quality, infrastructure quality, compliance and ethics, and data governance. While discussing the significance of each pillar, he has provided real-life examples and practical strategies for implementing effective Quality measures in AI projects.
About the Speaker
Bharath Hemachandran currently works with Thoughtworks as a Quality Analyst and Principal Consultant. He has 16 years of experience in the software industry in various roles, from developer to IT Head of a real estate company. Coming to his interests, he loves to look at technology with a business mindset and solve real-world problems using technology. His passions include researching the use of Generative AI in Software Development and blogging.
If you couldn’t catch all the sessions live, don’t worry! You can access the recordings at your convenience by visiting the
LambdaTest YouTube Channel
.
What is Data Quality?
Getting started with the session, Bharath talked about what quality looks like. To explain this, he introduced a five-pillar framework approach. Further, he highlighted the challenge of determining what constitutes a high-quality system in data and AI, where traditional criteria like performance and security might not be sufficient.
Bharath suggested that simply meeting functional and cross-functional requirements is inadequate for assessing quality in these systems. The goal is to develop a way to differentiate between well-defined and poorly defined systems from the perspective of AI and data. To support his views, he shared five use cases where the systems were aligned cross-functionally but were considered failures:
1.Google Flu Trends:
Data quality issue: Bad/incorrect data
has caused the system to fail and led to incorrect predictions.
2.Apple Card:
Algorithmic flaws
have caused Gender Bias, even if gender was not a parameter to define the credit score.
3.Equifax Data Breach: A small, unpatched vulnerability has caused a huge breach. It shows that
infrastructure quality
supporting the system is just as important as the data quality.
4.Cambridge Analytica Scam: Highlights the importance of
compliance and ethics
in data and AI systems: Misuse of the data has caused the system failure.
5.External factors like regulation imposed by Govt. and regulatory bodies on different industries like GDPR, HIPAA, etc., highlight the importance of
data governance
for a system.
So, in addition to functional and cross-functional requirements, you need to have:
Good data quality
Guarding against data shifts or user behavior changes
Explainable and functional data models
Secure and well-governed infrastructure
Compliance with ethics standards
Conformance to data governance and industry regulations
5 Pillar Model for the Quality of Data and AI System
After gaining relevant experience in software testing and quality assurance, Bharath came up with his five-pillar model to define quality for AI systems. He explained each pillar in depth as follows:
Pillar 1: Data Quality
Data quality is super important for any system to work well. This means the data should be fresh, complete, accurate, and consistent. Bharath walked us through various aspects of data quality:
Data Profiling:
Profiling data involves analyzing its characteristics, which assists in measuring data quality. Tools like DQ and Great Expectations are recommended.
Data Cleansing:
Anonymizing data, avoiding exposing PII (Personally Identifiable Information), and ensuring data validity are crucial for maintaining quality.
Tracking Data Lineage:
Understanding data sources, transformations, and usage is vital. Metadata management tools are highlighted for maintaining data lineage.
Tool Frameworks:
Apache’s tools and other data quality tools are available for addressing these aspects. Some great tools can be used, like Deequ.
Pillar 2: Algorithm/Model Quality
Bharath explained that algorithms play a great role in determining the accuracy of these models, which further affects the predictions they generate.
Model Validation:
Ensuring the quality of AI algorithms or models involves collaboration with data scientists, ML engineers, and others.
Bias Testing:
Detecting and mitigating bias in AI models is important. Tools like IBM Fairness 360 aid in bias testing to ensure fairness.
Interpretability and Explainability:
Understanding the model’s working, changes over time, and implications on results is crucial for success.
Data Quality Impact:
The quality of training and testing data directly influences model performance. Poor data quality can lead to unexpected results.
Pillar 3: Infrastructure Quality
The infrastructure quality can have a direct impact on the performance and availability of the system. Bharat further broke it down into different parts:
Infrastructure Testing:
Maintaining infrastructure quality involves thorough testing to ensure optimal performance of systems and servers.
Configuration Testing:
Ensuring configuration consistency to avoid dependency issues and performance unpredictability.
Continuous Monitoring:
Regularly tracking the quality of infrastructure over time and adapting to changes is essential.
Pillar 4: Ethical Compliances
While many people wonder ethical compliances won’t play a major role in determining quality, Bharath explained its importance in maintaining the trust of customers and stakeholders.
Ethical Considerations:
Addressing ethical concerns is crucial, considering the impact of systems on users, values, and potential biases.
Guidelines and Guardrails:
Setting ethical guidelines and guardrails, such as respecting privacy, client data usage, and defining acceptable uses of AI models.
Risk Management and Audits:
Employing third-party audits to assess compliance and risk management, ensuring adherence to ethical guidelines.
Regulatory Compliance:
Ensuring data security, privacy, and regulatory compliance to avoid exposing sensitive data or violating regulations.
Pillar 5: Data Governance
Last but not least, Bharath highlighted the significance of Data Governance when considering the quality of AI systems. He further explained the aspects as follows:
Data Governance Importance:
Data governance combines technical and functional aspects to ensure proper data management and usage.
Service Level Objectives:
Defining service level objectives and policies related to data privacy, security, and authorization.
Stewardship:
Appointing stewards for data sources to ensure accountability and proper usage guidance.
Audits and Checklists:
Employing audits and checklists to validate compliance with established policies.
Data Cataloging and Access Management:
Proper cataloging and access management tools enforce policy adherence and control data usage.
Impact of Quality on Roles
As Bharath explained data quality, he discussed a significant change in the roles within the tech landscape, particularly for developers, QAs, and BAs. He emphasized that the roles are no longer limited to traditional developers, QAs, and project managers. With the rise of data and AI, new roles have emerged, including data professionals like data scientists, data engineers, and data product managers.
Impact on Developer Roles
Bharath discussed that developers today go beyond traditional coding and work with data specialists to understand cutting-edge storage and processing techniques. They execute cross-functional testing, provide rationales behind data decisions, and guarantee data validity to build systems that align with the five quality pillars.
Impact on QA Roles
He discussed how QA roles have extended beyond functional tests, including cross-functional and data platform evaluations. Today, engineers detect bias, verify ethical usage, and validate data quality, ensuring compliance. Bridging technical and non-technical teams, they advocate a holistic quality approach.
Impact on Business Analyst Roles
While discussing the impact of quality on BA roles, Bharath mentioned that BAs are pivotal in grasping ethical data usage and meeting regulatory demands. They communicate user needs, collaborate with QAs for compliance, and shape systems adhering to the five pillars of quality.
As the session approached the end, Bharath concluded by discussing the future of AI and data. He suggests everyone concentrate on their areas of expertise, remain open to skill-set shifts, and foster a comprehensive understanding, including compliance, ethics, security, and privacy considerations to sustain in the digital landscape. He recommended everyone adapt to the new changes and remain well-equipped in this new era of technology.
Time for Some Q&A
Q. How are we able to have the required traceability for related testing to requirements?
Bharath provided a concise approach to ensuring traceability of testing to requirements. He outlined five key considerations: regulatory compliance, ethical implications, infrastructure suitability, explainability of models, and comprehensive definitions of ‘done’. By doing these steps, the speaker showed how to ensure testing matches what’s needed.
Q. How do you see the interplay between model quality and overall system quality and what strategies can be employed to ensure both are maintained?
Bharath answered this question by discussing model quality’s effect on the system. He mentioned three aspects of model quality, i.e., algorithms used, the quality of data for training, and the system’s explainability. To ensure model and system quality, Bharath recommended using Oracles to test and track model versions over time. This helps maintain good models and understand any changes.
Q. What are some of the most common challenges that arise when applying the traditional software quality methodologies to the systems?
Bharath mentioned about the different challenges when it comes to
applying traditional software quality methodologies to data and AI systems. Shifting from a tester-centric approach to involving all stakeholders in quality ownership is crucial. Addressing governance and ethical concerns upfront is essential, as these can’t be changed later. Choosing the right approach and not overcomplicating solutions is important. Data quality must be a priority, even for seemingly simple problems. Effective communication with new roles like data scientists and analysts is key.
Got more questions? Drop them on the
LambdaTest Community
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Create Your First AI-Powered C# App with Semantic Kernel: A Step-by-Step Guide
CodeStreet -
Dec 10
AI Search Capabilities Hit Roadblock: Transformers Struggle with Large-Scale Path Finding
Mike Young -
Dec 10
Azure OpenAI in a single page: Zero to Hero – A Complete Integration Guide
Pratik Pathak -
Dec 10
🔧 Embed SQL Editor ✍️ in Your Internal Web Portal 💻
Yiran Jing -
Dec 10
LambdaTest
Follow
LambdaTest is a cloud based Cross Browser Testing Platform.
Visit Us
More from
LambdaTest
It’s Time For A TRIM(S): Richard Bradshaw [Testμ 2022]
#
webdev
#
testing
#
browser
#
automation
36 DevOps Testing Tools [2024]
#
devops
#
testing
#
tools
#
automation
Use Cases of XPath Python In Selenium With Examples
#
webdev
#
beginners
#
testing
#
programming
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Using Datafold to Enhance DBT for Data Observability - DEV Community,"Using Datafold to Enhance DBT for Data Observability - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Michael Bogan
Posted on
Dec 7, 2021
Using Datafold to Enhance DBT for Data Observability
Modern businesses depend on data to make strategic decisions, but today’s enterprises are seeing an exponential increase in the amount of data available to them. Churning through all this data to get meaningful business insights means there’s very little room for data discrepancies. How do we put in place a robust data quality assurance process?
Fortunately for today’s DataOps engineers, tools like
Datafold
and
dbt
(Data Build Tool) are simplifying the challenge of ensuring data quality. In this post, we’ll look at how these two tools work in tandem to bring observability and repeatability into your data quality process.
But first, let’s set the context for our data problem.
Manual Data Quality Is No Longer Manageable
Typically, an organization’s data comes from a multitude of sources and in a variety of forms. This data needs to be cleaned, processed, and then transformed into a usable format. These activities are usually handled by ETL (extract, transform, load) or ELT (extract, load, transform) pipelines built around the data. For small datasets, data analysts would manually check the data for discrepancies. With increasingly larger volumes of data, however, data quality checks require automation.
In the past, data engineering was left out of the standardization and process innovation enjoyed by application development. There was no version control for data models or data pipelines, no CI/CD or documentation, and no automated regression testing strategies. In other words, ensuring quality without effective tools was next to impossible.
DataOps sorely needed automated, repeatable, and reliable processes that would bring some predictability into data operations. The ideal tools would automatically profile source data and show its dependencies. DataOps engineers needed to see how a change in the data itself—its structure or the transformation code—would affect the data downstream.
With the recent emergence of stronger and better DataOps tools, we got dbt and Datafold.
Introducing dbt
With the increased power of modern data warehouse engines, ELT processes are being preferred over ETL. ELT depends more on the transformation capabilities of the data warehouse
after
data has been ingested by it, rather than on independent transformation logic pre-ingestion.
dbt brings sanity to these transformation jobs. It helps represent the transformation logic in its proprietary language, which is very similar to SQL. dbt then compiles that logic to the SQL format of the target data warehouse and runs it there. dbt supports all modern data warehouses, including Google BigQuery, Snowflake, and AWS Redshift.
dbt can help data engineers create reusable SQL code modules that can be accessed from other SQL scripts. This helps code modularization and improves code maintainability. dbt accomplishes this by pairing SQL with a powerful templating framework called Jinja.
Perhaps the strongest feature of dbt is its ability to
show the changes that would happen to the data by the transformation logic
. dbt does this by representing the transformation jobs as SELECT statements, taking care of other boilerplate activities like generating DDL commands and ensuring version control. It also provides a mechanism to add documentation to the data models.
Another feature of dbt is its ability to
facilitate automated testing using custom metrics and scripts to ensure the integrity of the SQ
L. It can handle testing requirements ranging from simple non-null value assertions to business logic-based assertions.
dbt also helps engineers to better understand raw data sources. It can inform about the freshness of the data, add documentation and metadata to data sources, and keep track of models that depend on various data sources. It
maintains snapshots of data
at various points, helping data engineers to recreate historical checkpoints of values.
dbt can be accessed via a command-line utility that can be installed on the development machine or through the dbt cloud, which comes with a web-based IDE.
Datafold
Datafold is a data observability platform that proactively helps reduce data quality incidents and data outages. To do this, it combines a few core functionalities.
Datafold’s flagship feature is Data Diff:
automated data regression testing
integrated seamlessly into the development process. The one-click regression functionality executes a predefined sequence of tests on specific datasets and adds the test report to the code review process. Data Diff compares the new version of the data resulting from a change in code to its original version. It can track schema changes as well as value changes.
The table and column-level lineage tracking feature
shows where a particular piece of data comes from—and how it relates to an upstream source or a downstream destination
. Data lineage is incredibly helpful for debugging data issues, as it can quickly surface why a piece of data is not what it’s supposed to be. For example, a report may be showing unexpected data for a particular field. Using the column-level lineage for the field, DataOps can check the field’s source and original value, and how that field was transformed through various upstream steps in the data pipeline.
Datafold’s Data Catalog provides a one-stop shop for engineers to
explore all the registered organizational datasets
. It goes a long way in helping analysts find the right data asset and understand it. Data Catalog profiles data and creates usable metadata about it. This kind of profiling brings immediate visibility to how complete or fresh a dataset might be. In addition, Data Catalog can pull in metadata tags from other sources (like dbt, or even a data warehouse like Snowflake).
Datafold can help monitor data discrepancies by creating custom alerts based on SQL queries. This feature helps data engineers stay proactive so they can preempt and prevent quality incidents or outages. These alerts are powered by ML models to adapt to seasonality, and they can be configured to ensure that only the correct people receive the correct alerts when needed. With alerts in place, data consumers can know
early
when they might have unavailable or inaccurate data.
Observing data and ensuring its quality is just one part of the challenge. Executing the actual transformation jobs reliably to build the insights is the other part. This is where dbt can help.
How Do Datafold and dbt Work Together?
Now that we’ve covered the basics of both dbt and Datafold, let’s see how they work together.
dbt and Datafold address two different problems in the ELT space. dbt helps formalize the transformation jobs and enables standard software engineering practices like documentation, version control, and continuous integration. Datafold, on the other hand, is focused on reducing data quality incidents by automated regression testing, visualizing data lineage, and providing data alerts.
One area where both dbt and Datafold overlap is in data cataloging. Both dbt and Datafold can capture metadata about the data sources. Datafold goes one step further and can even synchronize metadata from dbt. How dbt and Datafold can work together to solve data quality issues depends on the specific data lifecycle scenario. Let’s consider a typical workflow with dbt.
A Typical dbt Workflow
dbt provides a CLI and a cloud-based IDE. The broad sequence of steps to use dbt is very similar in both cases. The process starts with creating a project in dbt and then connecting to the target data warehouse. Once connected, the project can be initialized using a built-in template. The SQL code to run transformations is stored as
.sql
files in a directory called
models
. A YAML file contains the configurations for running the transformations. Data engineers can create more models in the dbt project by adding SELECT statements and saving them as
.sql
files. dbt takes care of generating DDL statements for the models.
Once the models are developed, they can be committed to a Git repository from the web IDE. In line with the modern software development workflow, once a commit is made to a branch, a pull request is created to initiate deploying the model to production. This pull request can be reviewed by peer developers and then merged into the production pipeline. To define tests and documentation, developers can add tests and documentation to the YAML configuration file. dbt then generates the documentation and lineage by scanning all the entries in the dbt project.
Naturally, the next question is this: How does Datafold fit into this process? Datafold integrates with the continuous integration (CI) configuration of dbt projects in order to help prevent data quality incidents. When integrated with dbt, Datafold synchronizes the metadata information from the
.yml
files and the
.sql
files. It builds a complete profile of the data sources based on this information and by connecting directly to the data warehouses. Datafold can then be used in three different ways.
Data Diff
When integrated into the CI pipeline, Datafold can be configured to run Data Diff and attach a report to the pull request. Data Diff runs both the previous model version and the current model version of the code on the test data, and then it calculates diff metrics to help the pull request reviewer make an approval decision.
Having a report like this attached to a pull request can help the code reviewer be 100% sure about the decision they make to approve (or reject) the code change.
Data Lineage
While dbt provides some table-level data lineage information, Datafold provides comprehensive table and column-level lineage in an interactive user interface. This makes it easy to see which data will be impacted by changes to a column’s calculation or even a field’s name or definition.
Data lineage functionality is an extension of the Data Catalog feature provided by Datafold. Beyond lineage information, Data Catalog provides everything required by the developers to find the right data asset.
Data Monitoring and Alerts
Datafold keeps track of all the pipeline runs by dbt and can run diagnostics on each execution to generate alerts. Simple SQL statements can be specified to enable Datafold to generate these alerts in case unexpected patterns are found in the output. This can help in the early detection of quality incidents. Setting up alerts is as easy as selecting a data source and configuring an SQL statement.
Even if the developer has not specified any alert conditions, Datafold’s machine learning models are intelligent enough to detect anomalies in the output data and then generate alerts. The models can adapt to the seasonality and trend variations in the data and will construct dynamic thresholds, thereby eliminating unwanted alerts in case of natural variations.
Conclusion
Data in the modern enterprise is evolving constantly, especially as more and more data sources are brought into the mix. It’s not enough for DataOps teams simply to manage data assets; their KPIs increasingly include data quality and reliability for those assets.
This stresses the team’s need for visibility and effective troubleshooting, to ensure data quality amidst any changes made to the existing data. Even the smallest of errors in the source data could harm downstream analytics modules and machine learning models. Because this directly affects an organization’s strategic decision-making process, it’s imperative for data engineering processes to be reliable, repeatable, and understandable—and that’s exactly what today’s DataOps teams get when they combine Datafold with dbt.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Join us for the Bright Data Web Scraping Challenge: $3,000 in Prizes!
dev.to staff -
Dec 11
Just Launched: Shopify’s BORING Edition!
DEV Launches -
Dec 11
Should You Use an Open-source SaaS Boilerplate Starter or a $300+ Paid One?
Matija Sosic -
Dec 11
This API Client is More Secure and Better Than Postman
Anmol Baranwal -
Dec 12
Michael Bogan
Follow
I write tech articles about things I love. I also run DevSpotlight - we create tech content for tech companies. If you need tech content, or want to create tech content, reach out!
Location
Indianapolis
Work
Technical Architect and Writer at DevSpotlight
Joined
Jun 12, 2019
More from
Michael Bogan
8 Ways AI Can Maximize the Value of Logs
#
devops
#
ai
#
devsecops
#
logging
5 Simple Steps to Get Your Test Suite Running in Heroku CI
#
cicd
#
pipeline
#
heroku
#
testing
How To Build a Simple GitHub Action To Deploy a Django Application to the Cloud
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Efficient Iteration of Big Data in Django - DEV Community,"Efficient Iteration of Big Data in Django - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Kyle Johnson
Posted on
May 6, 2021
Efficient Iteration of Big Data in Django
#
django
#
bigdata
#
python
Running out of memory is not fun.
Unfortunately, when working with larger datasets its bound to happen at some point.
For example, I tried to run a
Django
management command that updated a value on a model with a large amount of rows in the database table:
python manage.py my_update_command
Killed
Enter fullscreen mode
Exit fullscreen mode
That was inside of Kubernetes which killed the process when it exceeded its memory limit. In a more traditional environment, you can completely freeze up the server if it runs out of memory.
Context
You're probably wondering why I'm trying to run a management command like this in the first place. When working with large datasets, its best to avoid anything that is
O(n)
or worse. In this case, I had a JSONField with a bunch of data. I also had an IntegerField on the model that stored a calculation based on some of the data in the JSONField.
Of course, requirements change and the calculation I had been using needed to use different values from the JSONField. This also needed to happen for all the existing data in the database (the large amount of rows). Luckily I had everything stored in the JSONField and making this change was as simple as running the management command and patiently waiting.
from django.core.management import BaseCommand

from ...models import SomeModel
from ...utils import some_calculation


class Command(BaseCommand):
    help = ""Updates SomeModel.field based on some_calculation""

    def handle(self, *args, **options):
        self.stdout.write(""Starting"")

        try:
            queryset = SomeModel.objects.all()

            for obj in queryset:
                obj.field = some_calculation(obj)
                obj.save(update_fields=[""field""])
        except KeyboardInterrupt:
            self.stdout.write(""KeyboardInterrupt"")

        self.stdout.write(""Done"")
Enter fullscreen mode
Exit fullscreen mode
Killed, now what?
Naturally, it wasn't that simple. Method 1 was using enough memory to have
Kubernetes
stop it. I tried a few different things here including moving the code into a data migration and running multiple asynchronous tasks. I had difficulties getting these approaches working and struggled to monitor progress.
Really, I just wanted a simple, memory-efficient management command to iterate through the data and update it.
QuerySet.iterator
Django's built-in solution to iterating though a larger QuerySet is the
QuerySet.iterator method
. This helps immensely and is probably good enough in most cases.
However, method 2 was still getting killed in my case.
# simplified command using QuerySet.iterator
class Command(BaseCommand):
    def handle(self, *args, **options):
        queryset = SomeModel.objects.all().iterator(chunk_size=1000)

        for obj in queryset:
            obj.field = some_calculation(obj)
            obj.save(update_fields=[""field""])
Enter fullscreen mode
Exit fullscreen mode
Behold the Paginator
I needed to iterate through the QuerySet by using smaller chunks in a more memory-efficient manner then the iterator method. I started to roll out my own solution before I realized that this sounded very familiar.
Django has pagination support built-in
which is exactly what I was about to implement. I ended up using the
Django Paginator
to iterate through the QuerySet in chunks.
Method 4 works great.
# simplified command using Paginator
class Command(BaseCommand):
    def handle(self, *args, **options):
        queryset = SomeModel.objects.all()

        paginator = Paginator(queryset, 1000)

        for page_number in paginator.page_range:
            page = paginator.page(page_number)

            for obj in page.object_list:
                obj.field = some_calculation(obj)
                obj.save(update_fields=[""field""])
Enter fullscreen mode
Exit fullscreen mode
Memory Usage
At this point there are the three methods described above, plus another two I added that use
QuerySet.bulk_update
:
Regular QuerySet
QuerySet.iterator
QuerySet.iterator and QuerySet.bulk_update
Paginator
Paginator and QuerySet.bulk_update
I ran comparisons on these approaches with 50,000 items in the database. Here is memory usage for all five methods with three runs each:
The plot clearly shows that method 1 is not a good choice since the entire QuerySet is loaded into memory before it can be used. Method 3 is also showing a steady increase in memory (note: it appears there is a memory leak here that I was unable to resolve). Zooming in on methods 2, 4 and 5 it becomes more clear that methods 4 and 5 are the winners:
I don't claim that the Paginator solution is always the best or even the best for my problem. More importantly it solved my problem and gave me the opportunity to dive into the memory differences between these approaches described above. If you have a similar problem, I recommend diving in and seeing how the comparison pans out for the specific problem.
A shortened version of the method 5 management command is:
# simplified command using Paginator and QuerySet.bulk_update
class Command(BaseCommand):
    def handle(self, *args, **options):
        queryset = SomeModel.objects.all()

        paginator = Paginator(queryset, 1000)

        for page_number in paginator.page_range:
            page = paginator.page(page_number)
            updates = []

            for obj in page.object_list:
                obj.field = some_calculation(obj)
                updates.append(obj)

            SomeModel.objects.bulk_update(updates, [""field""])
Enter fullscreen mode
Exit fullscreen mode
This article was originally posted on our
blog
.
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Dmitriy Reztsov
Dmitriy Reztsov
Dmitriy Reztsov
Follow
Joined
Apr 3, 2024
•
Apr 3
Dropdown menu
Copy link
Hide
nice, just need to add .order_by(""id"") in queryset before to pass it into Paginator
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Deploy Hugging Face Models to AWS Lambda in 3 steps
Christian Nuss -
Nov 26
QtWidgets and QtCore
KateMLady -
Nov 26
Any Country's capital finder in Python
Aj -
Nov 26
Introducing the DNA-KEY System: Taking the Password Generator to the Next Level! 🔐
Yassine Sallami -
Oct 23
Kyle Johnson
Follow
Location
Montana
Work
Software Engineer at NextLink Labs
Joined
May 6, 2021
More from
Kyle Johnson
How to Name Django Migrations (and Why It's Important)
#
django
#
python
#
database
A Django Upgrade Guide for Major and Minor Releases
#
django
#
python
#
upgrade
Improving Django View Performance with Async Support
#
django
#
python
#
api
#
performance
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
DBT and Software Engineering - DEV Community,"DBT and Software Engineering - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Pradip Sodha
Posted on
Aug 24
• Edited on
Oct 3
DBT and Software Engineering
#
dbt
#
softwareengineering
#
dataengineering
#
learning
In recent years, the competition for data solution tools has heated up. While AWS, Azure,
GCP
and many more companies investing heavily into Data Engineering such as
AWS Glue Studio
,
Azure DataFlow
,
GCP cloud data fusion
. While most companies focusing on low-code and drang-n-drop path. However,
DBT
(Data Build Tool) takes a different approach by embracing
software engineering principles
.
Instead of opting for the easy path, DBT proposed the right way of doing things, grounded in sound engineering practices. we will explore why DBT is different from big giants but today let's dive into part of it.
Table of Contents
Introduction
Audience
Software Engineering
Limitations of Today's Data Pipelines
DBT's Adherence to Software Engineering Practices
Conclusion
Introduction
In this Post, we'll explore
Software Engineering
methods used in DBT (Data Build Tool). While a basic understanding of DBT's features from its
documentation
might suffice for contributing to a project.
So, why read this? Well, we'll explain the Software Engineering methods used by DBT and why they matter, In short, we'll uncover the reasons behind DBT's features.
That's make difference cause knowing reason and potential of feature is much more important than just mastering any feature, if you violate the reason or core of the feature, than that's feature is killed and it's just another workaround or patch.
Audience
Data Engineers
Data Analytics
Software Engineers
Software Engineering
The realm of software engineering holds a vast history, having witnessed the contributions of numerous
scientists
and professionals.
Their collective efforts have propelled software methodologies to new heights, constantly striving to surpass previous achievements while upholding an audacious spirit.
Software engineering stands as the bedrock of modern technological advancements, weaving a rich tapestry of methodologies and
practices that shape the way we design, develop, and maintain software systems.
Its roots stretch back to the
mid-20th century
, evolving from simple programming to a comprehensive discipline encompassing various principles, tools, and frameworks.
Over the decades, software engineering has propelled innovations, enhancing reliability, scalability, and maintainability of systems across diverse industries.
Limitations of Today's Data Pipelines
In the realm of
big data
, the sophistication of data pipelines has surged, enabling the handling of massive datasets.
However, conventional data pipelines often exhibit limitations. They are prone to complexities, becoming intricate webs of disparate scripts, SQL queries, and manual interventions.
These pipelines lack standardization, making them difficult to maintain and comprehend. As the data grows, managing these pipelines becomes a daunting challenge, hindering scalability and agility.
DBT: A Solution Rooted in Software Engineering
Enter DBT (Data Build Tool), a paradigm shift in the world of data engineering that embodies the core principles of software engineering.
DBT redefines the way data pipelines are built and managed, aligning itself with established software engineering practices to tackle the challenges prevalent in traditional data pipelines.
DBT, stands as a revolutionary force in the domain of data transformation.
It reimagines the handling of data by infusing principles of agility and discipline akin to those found in the software engineering realm.
By treating data transformation as a form of software development,
DBT enables the scalability and seamless management of significant data components, facilitating collaboration among large teams with
unparalleled ease.
DBT's Adherence to Software Engineering Practices
Separation of Concerns
DBT distinguishes between data transformation logic and
data modeling
, allowing for modularization and easier management. For
instance, SQL queries in DBT focus on transforming raw data, while models define the final structured datasets.
DBT has divided the usual data transformation into four parts: 1. Business logic (DQL), 2.
Materialization
(DDL & DML), 3.
Testing
,
and 4.
Documentation
. These four areas now scale and maintain independently. Also, they are easier to read.
Analytics engineers
can focus on one thing separately. For example, they can solely concentrate on business logic (just select statements) while writing
models. How to store or test or document have different section.
Benefits
Enhanced Maintainability
Improved Reusability
Better Collaboration
Scalability and Flexibility
Security and
Risk Mitigation
(Individual can models can have access control and owner)
Future-proofing
Reduction of Complexity
Reusability
Just as software modules can be reused, DBT promotes reusable code blocks (
macros
) and models. This allows data
engineers to build upon existing components, fostering efficiency and consistency. Also DBT has good amount of packages that can
import and use directly in projects, allowing share standard and tested expression at glob.
Benefits
Efficiency
Consistency and Standardization
Ease of Maintenance
Cost-Effectiveness
Facilitates Collaboration
Future-Proofing
Unit Testing
Similar to software unit
tests
, DBT enables data engineers to create tests to validate the accuracy of transformations,
ensuring data quality throughout the pipeline. You can test each of your single transformation (a model) before subsequent step run.
Benefits
Error Identification in Isolation
: It allows the testing of individual components (units) of code in isolation, pinpointing errors or bugs specific to that unit. This facilitates easier debugging and troubleshooting.
Enhanced Code Quality
: Unit tests enforce better coding practices by promoting modular and understandable code. Writing tests inherently requires breaking down functionalities into smaller, manageable units, leading to more maintainable and robust code.
Regression Prevention
: Unit tests serve as a safety net. When modifications or updates are made, running unit tests ensures that existing functionalities are not negatively impacted, preventing unintended consequences through regression testing.
Facilitates Refactoring
: Developers can confidently refactor or restructure code knowing that unit tests will quickly identify any potential issues. This flexibility encourages code improvements without the fear of breaking existing functionalities.
Improved Design and Documentation
: Writing unit tests often necessitates clearer interfaces and more detailed documentation. This leads to better-designed APIs and clearer understanding of how code should be used.
Accelerates Development
: Despite the initial time investment in writing tests, unit testing can speed up development by reducing time spent on debugging and rework. It aids in catching bugs early in the development cycle, saving time in the long run.
Supports Agile Development
: Unit tests align well with agile methodologies by promoting frequent iterations and continuous integration. They facilitate a faster feedback loop, allowing developers to quickly verify changes.
Encourages Modular Development
: Unit tests require breaking down functionalities into smaller units, promoting a modular approach to development. This modularity fosters reusability and simplifies integration.
Boosts Confidence in Code Changes
: Unit tests provide confidence when making changes or additions to the codebase. Passing
tests indicate that the modified code behaves as expected, reducing the risk of introducing new bugs.
Abstraction
The abstraction principle involves concealing intricate underlying details while presenting a simplified and accessible
interface or representation. In DBT, for instance, model files encapsulate solely business logic, abstracting materialization and test
cases. This seemingly simple feature proves immensely helpful. It's akin to skimming a newspaper headline—if more details are needed,
delve deeper; if not, move swiftly to the next topic.
Benefits
Simplification of Complexity
Enhanced Readability and Understandability
Focus on Higher-Level Concepts
Reduced Cognitive Load
Coupling
The coupling principle refers to the degree of interconnectedness or dependency between different components or modules within a
system. Lower coupling indicates a lesser degree of dependency, while higher coupling suggests a stronger interconnection between
components.
In DBT, managing coupling involves reducing dependencies between different parts of the data transformation process. Lower
coupling is desirable for several reasons.
Documentation
DBT facilitates comprehensive
documentation
for data models and transformations, akin to software documentation.
This documentation aids in understanding the data flow, enhancing collaboration and knowledge sharing.
Environment Separation
In the software world, it's common to use different environments like Development (Dev), User Acceptance Testing (UAT), and Production (Prod) to manage changes effectively and ensure stability. This practice, known as Environment Separation, helps isolate changes, allowing teams to test and validate new features or fixes in a controlled setting before exposing them to real users.
It mitigates risks, ensures consistency, and facilitates compliance and security. Similarly, dbt (data build tool) seamlessly supports environment separation, allowing teams to define and manage different environments such as Dev, UAT, and Prod. This practice promotes better DataOps by ensuring that data transformations are thoroughly tested and validated before they impact production, improving reliability and reducing the risk of errors.
Backward Compatibility
Clients often provide new requirements, or we may discover more optimal ways to perform tasks. When this happens, we tend to modify our existing models or queries. However, in a large project, a single query might be relied upon by many clients, making it challenging to notify all teams of changes.
Additionally, new changes can sometimes introduce faults, which can disrupt data pipelines and violate one of the core principles of big data: availability.
To address this, the software industry already employs strategies to manage such issues effectively. dbt (data build tool) supports different
model versions
, allowing teams to maintain multiple versions, such as a pre-release version for testing and a stable version for production use.
This versioning approach makes dbt highly adaptive, enabling teams to migrate to new versions at their own pace. Furthermore, dbt allows setting a deprecation period, specifying how long an old API version will be supported before it is phased out, aligning with the concept of a Deprecation Policy.
Benefits
User Experience Stability
Reduced Migration Costs
Minimized Downtime
Flexibility in Adopting Updates
Flexibility in Adopting Updates
Encourages Innovation
Risk Mitigation
Conclusion
DBT's fusion of software engineering principles with the domain of big data revolutionizes how data pipelines are conceived, constructed, and maintained. By embracing the tenets of software engineering, DBT addresses the shortcomings of traditional data pipelines, ushering in a new era of efficiency, reliability, and agility in data engineering. As software engineering continues to evolve, its synergy with big data technologies like DBT paves the way for more robust, scalable, and manageable data ecosystems.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
What is JavaScript ?
Muhammad Bilal -
Nov 4
Writing polyfills — Javascript
Manoj -
Nov 3
How would you consider your API to be Restful?
CodeWithVed -
Nov 7
Example Training Images Dataset, Trained Models, Grids and Full Training Configs, json files and more
Furkan Gözükara -
Nov 2
Pradip Sodha
Follow
Explorer
Joined
Aug 24, 2024
More from
Pradip Sodha
Top 5 Things You Should Know About Spark
#
spark
#
dataengineering
#
development
#
coding
Avoid These Top 10 Mistakes When Using Apache Spark
#
dataengineering
#
development
#
coding
#
dbt
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How to ensure the quality of smart contracts in decentralized applications - DEV Community,"How to ensure the quality of smart contracts in decentralized applications - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
a1qa testing lab
Posted on
Jul 26, 2019
• Originally published at
a1qa.com
How to ensure the quality of smart contracts in decentralized applications
#
testing
#
qa
#
softwaretesting
#
qualityassurance
The world of blockchain and cryptocurrency is associated with decentralization.
Decentralization eliminates the need of one authority and ensures the privacy of information and security as nobody fully owns the system. What makes it work is cryptography – a method of using digital signatures in a form so that only those network members, who should get access to some piece of data, will get it.
The possibilities of blockchain application are endless – and this is where decentralized apps come into play.
Before we dive into testing specifics, let’s quickly go over some important terminology.
A smart contract is a small piece of code that codifies some business logic and is executed on blockchain in a decentralized manner. Their main functions are to store rules, facilitate rules, and self-execute rules.
Smart contracts help gain autonomy and independence from third-party structures and have other advantages like reliability, security, accuracy, and profitability.
A decentralized application (DApp) is the application that is run on a peer-to-peer network of computers and doesn’t depend on a central computer to send or get information. Decentralized apps use the power of blockchain and smart contracts to enforce the agreement between the parties.
Today, the most popular way to run a dApp is to install it on a blockchain platform (for example, on Ethereum).
And the world of business eagerly embraces smart contracts and dApps. We see now and will see in the nearest future numerous applications in IoT, financial sector, etc. The prime examples include games, DEX (Decentralized Exchange), NFT marketplaces, gambling, blockchain-powered social networks, DeFi (Decentralized Finance).
Now let’s focus on how to test smart contracts and how to ensure high quality of decentralized applications.
WHY IS IT IMPORTANT TO TEST SMART CONTRACTS?
Due to the immutable nature of blockchain, if any errors are found, a new contract should be created because you cannot change the code of the blockchain-based contract.
Even a minor logic error in the application can lead to serious damages and costs.
For example, a couple of years ago, Parity Ethereum MultiSig wallets were blocked due to smart contracts vulnerabilities. Could the blockage have been avoided? Yes. By means of thorough testing.
WHAT TO TEST IN A SMART CONTRACT?
Testing is the best way for using the contract in different situations without any risk to make sure it’s working properly.
Here are the aspects of the smart contract that a professional QA team should take into consideration:
Digital signature process
Contract code and its changes fixation
Subject of the contract
Tools that are necessary for the contract execution
The terms of execution in the contract code
Events
Errors and sending messages
Changes made in the status of contracts and their balance
HOW TO TEST A SMART CONTRACT?
It is necessary to simulate, consider, and predict all possible conditions in the logic of each particular contract:
what events should be initiated
what techniques should be fulfilled
how the status and balance of the contract are changing
to whom and how to transfer funds
what error to cause
check the author of the message and the correct use of the current time.
For example, when executing a smart contract in Ethereum, it is important to take into account the limitation and spending of the Gas value – the cost of a smart contract that Ethereum has set for its launch.
All this is closely related to the security of the agreements. The situation when means are “stuck” in the contract differs just a little from the situation when they are stolen.
Otherwise, the general principle is similar to testing any other code: a set of reference method calls is created in a predefined environment, for the results of which the expected results are written.
HOW TO MAKE SOFTWARE TESTING PROCESS MORE EFFECTIVE?
In the case of smart contracts, the most accurate solution would be to conduct
automated testing
, especially considering a large number of validations during contract processing, rapid growth, and network transformation, the presence of multiple nodes and their various combinations.
Often for defect-free work, BDD (Behavior-Driven Development) practices are used, which allows creating documentation and examples along with tests. We have talked more about BDD
here
.
THE ELEMENTS OF DAPP
Let’s move on to testing of decentralized applications.
Any application consists of the back-end and front-end. The back-end of a decentralized application has its own internal code that is run on the blockchain.
DApp also consists of an intermediate – a browser (for example, Toshi) or extensions for it (like MetaMask). Front-end can be written in any programming language.
TESTING A DAPP
Although blockchain applications are still an innovative industry, many methods applied for testing other products are also suitable for dApp checks.
Apparently, there is no difference between a dApp and any other application. Many users do not even know that they have an application where the back-end is a blockchain.
In our previous
article
, we have discussed the characteristics of functional, security, and load testing that help improve the quality of blockchain-based products.
In addition, other testing types should be carried out:
Smart contracts testing
Transaction testing
This type of testing includes checking the number of operations performed, service fields, hash validity, time of the transaction, data about the author and recipient, and many more.
End-to-end testing
This type of testing is performed to ensure the quality of all the functionality of the application and verify that all dApp parts work as intended. As a rule, end-to-end checks try to predict the behavior of a real user and how he/she will interact with the system.
Integration testing
QA engineers will check the interaction of modules and systems as a whole and the integration of data that reaches the back-end from the front-end.
Read more about the tools to test blockchain-based apps in our previous
article
.
ON A FINAL NOTE
Getting rid of centralization has become a dominant trend and has greatly changed workflows in many industries. First-gen P2P networks, DAO, and other dApps, smart contracts, EOS (decentralized OS) are just beginning to gain momentum and determine what the Internet will be like in the future.
Early software testing can help business successfully take advantage of seamless blockchain technology, solve longstanding problems, and take their place in the competitive market.
No doubt, blockchain is a powerful technology. But not being tested properly, it can cause troubles you may not think about.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to run Selenium tests on Chrome using ChromeDriver?
Steve Wortham -
Oct 7
Join the Open-Source Revolution with AxioDB: Calling All Developers!
Ankan Saha -
Oct 6
We made a horror movie inspired by spooky prod bugs 🎃
Ivanha Paz -
Oct 31
3 useful VS Code extensions for testing Ruby code
Jokūbas Pučinskas -
Nov 8
a1qa testing lab
Follow
A strong unit of QA experts in DevOps in testing aspects, delivering test automation solutions, creating continuous testing strategies, improving the continuous delivery process, and more.
Location
Lakewood, CO
Work
QA Consultant at a1qa
Joined
Jun 24, 2019
More from
a1qa testing lab
Get more value with DevOps: the evolving role in test automation
#
testautomation
#
softwaretesting
#
devops
#
testing
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Top Software Testing Trends to Watch Out For in 2020 - DEV Community,"Top Software Testing Trends to Watch Out For in 2020 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
TestingNews
for
Katalon Platform
Posted on
Dec 19, 2019
• Originally published at
katalon.com
on
Dec 5, 2019
Top Software Testing Trends to Watch Out For in 2020
#
softwaretestingtrends
#
testautomation
#
katalon
The software testing landscape continues to evolve. We have seen the continuation of developing trends and the emergence of new trends in 2019. This year, our team of automation experts has cast a few predictions on the latest trends in the software testing industry. Check them out!
To see the recap on 2019 software testing trends, read our article
here
.
1. Artificial Intelligence and Machine Learning in Testing
Intelligent automation will continue to be on the software testing radar in 2020, according to a variety of reports.
Applications of artificial intelligence and machine learning (AI/ML) have been leveraged in software test automation before. AI makes testing smarter. Teams can leverage AI/ML to optimize their automation strategies, adapt faster, and operate more effectively.
In 2019, quality assurance (QA) teams have applied AI/ML in predicting test quality, prioritizing test cases, classifying defects, detecting test objects, interacting with applications under tests (AUT), and so on.
It is expected that AI will be omnipresent in every sphere of innovative technology. Investments in this area are expected to fall around $6-to-7 billion in North America alone. By 2025, it is forecast to reach nearly $200 billion. We will expect to see applications of AI in more testing areas — most of which will be relevant to reports and analytics:
Log analytics:
Identify unique test cases that need manual and automated testing
Test suite optimization:
Detect and eliminate redundant, unnecessary test cases
Ensure test requirements coverage:
Extracting keywords from the Requirements Traceability Matrix (RTM)
Predictive analytics:
Forecast key parameters and specifics of end-users’ behaviors and identify application areas to focus on
Defect analytics:
Identify application area and defects that ties to business risks
The other pillar on which intelligent automation rests is machine learning. ML is expected to reach another level of maturity in 2020. According to the Capgemini World Quality report, 38% of organizations have planned to implement ML projects in 2019. Industry experts predict that this number will rise in the next year.
What does this mean for organizations?
Even though there is a rising demand in prospects of AI/ML application in software testing, experts still regard AI/ML in testing is still in its infancy stage. However, we are very much likely able to see maturity coming.
As AI is making new demands in testing and QA teams, Agile teams must start adopting AI-related skillsets—which include onboarding data science, statistics, mathematics. These new skillsets will not replace, but a complement to the core domain skills in automated testing and software development engineering testing (S-DET).
Also, business acumen is another essential skill to adopt. Successful testers need to have a combination of pure AI skills and non-traditional skills. Indeed, last year, new roles have been introduced such as AI QA analyst and test data scientist.
As for automation tool developers, they should focus on building tools that are practical. Companies are running PoCs and reassessing options to make the best use of AI and considering budgets. A good AI-assisted tool has to fulfill both the business cost-efficiency and the technical aspects such as reading production logs, generating test scenarios, or responding to production activities.
2. Test Automation in Agile teams
Test automation is undoubtedly no longer a foreign idea in quality assurance. Indeed,
44% of IT organizations expect to automate 50% or more of all testing in 2019
. We predict that more adoption of automated testing will continue to be on the rise next year.
As more businesses adopt the latest Agile and DevOps processes to fulfill the demand for Quality at Speed, test automation has become an indispensable component. Test automation continues to lead by helping teams perform repetitive tasks, detect bugs faster and more precisely, provide continuous feedback loops, ensure test coverage. Therefore, organizations that implement automated testing in their QA processes can save a significant amount of costs, time, and human resources.
Test automation in 2020 is expected to be championed especially by millennial entrepreneurs, leveraging the combination of open-source and commercial tools.
What does this mean for QA practitioners?
Test automation, however, will not eliminate manual testing. In fact, robust QA teams must appropriately combine manual and automated testing to achieve the most in ensuring software quality. The role of automated testing is undeniable—but some testing types such as exploratory or usability testing still need to be manually carried out.
QA practitioners, in addition, have to develop a smart, common, and end-to-end environment. There has been an increasing need to automated from build through deployment. Test automation is no longer regarded as a functional but as a full-cycle requirement.
This process is easier said than done. That’s why many organizations have not been able to squeeze the most out of automated testing and received the desired return on investment. The Capgemini World Quality Report suggests that instead of looking at automation as a capability, QA teams should think of it as a broad, smart, and connected platform.
What does this mean for test automation solution providers?
Test automation tools developers must continuously update and upgrade tools to fulfill QA teams’ demands. Future test automation solutions must follow some basic criteria, for example:
Easy to adopt and use
for end-users at any testing level
Provide smart frameworks, meaning letting issues resolve themselves
See Autohealing Smart XPath
and
Katalon Smart Wait
_ _
Ensure full test coverage and quality bugs detection
Cross-platform testing for web, API, mobile, and desktop automation
Integrate with CI/CD tools and allow Continuous Testing
Integrate with intelligent dashboards and analytics for quality insights
See Katalon TestOps
3. Big Data Testing
Big data has served an essential role in a variety of business sectors including technology, healthcare, banking, retail, telecom, media, and so on. There has been more focus placed on using data to segment and optimize decision-making processes.
Big data testing allows industries to deal with huge data volumes and diverse data types. It also helps make better decisions with precise data validations, as well as enhancing market strategizing. Big data testing is no longer a new phenomenon. However, it is expected to grow exponentially as many industries are shifting toward a data-oriented world.
The trend of testing big data has been widely adopted, mainly because of the robust processes that most of the enterprises are following make the most of their marketing strategies. Big data testing is not an uncommon practice and it is expected to become popular in the next year. Therefore, we forecast that the need for testing big data applications will see a new rise in 2020.
4. QAOps: Quality Assurance Sees Changes in DevOps Transformation
If you haven’t heard of the term ‘QAOps’ yet, now’s the time.
You might have been familiar with ‘DevOps’—a set of software development practices that combines development (Dev) and information technology operations (Ops). The goal of DevOps is to shorten the systems development life cycle (SDLC), while teams can focus on building features, fixing bugs, and pushes frequent updates that are in alignment with business objectives. DevOps abridges the collaboration between developers and business operationalists.
In the same spirit, QAOps helps increase the direct communication flow between testing engineers and developers by integrating software testing into the CI/CD pipeline, rather than having the QA team operate in isolation. In short, QAOps is defined in two key principles:
QA activities should be incorporated into the CI/CD pipeline
QA engineers should work in alignment with developers and be involved throughout the CI/CD process.
Facebook is one of the best examples of QAOps adoption. In 2014, the Facebook team decided to migrate to Facebook Graph API version 2.0 and enforce Login Review across all apps. To ensure a smooth migration process, the team wanted to test out this new version on the 5,000 largest apps. In-house testing did not allow this to be possible, so they chose to apply QAOps through outsourcing. Eventually, the team was able to test across 5,000 apps in one month and managed to address critical problems—which could have been impossible had the process been carried out by the internal team alone.
QAOps can be applied not only in giant tech companies but also in medium and small teams. This practice can be flexibly scaled down or up to fit any business size.
Because more teams are gearing toward DevOps, we will expect to see QAOps as a growing trend in 2020.
5. IoT Testing
The rise of testing Internet of Things (IoT) devices was already prominent
in 2019
. The number of IoT devices all around the world will reach 20.5 billion by 2020, according to Gartner.
IoT testing means testing the IoT devices for security assurance, ease of use, trustworthiness, compatibility of device versions and protocols, versatility of programming items, monitoring connection delay, scalability, data integrity evaluation, device authenticity, so on and so forth.
IoT testing engineers often face an overwhelming amount of work in this area, especially with monitoring communication protocols and operating systems and multiple combinations of different elements of an IoT system. Therefore, QA teams should expand their knowledge and enhance their skills in usability, security, and performance IoT testing.
Another challenge that IoT testers will face in the upcoming years lies in strategies. Although IoT devices and applications have been growing exponentially, 34% of respondents said their products have IoT functionality, but their team still does not have a proper testing strategy, according to the World Quality Report.
6. Demands for Cybersecurity and Risk Compliance
The digital revolution brings about increasing security threats. CIOs and CTOs from almost every enterprise across all sectors continue to acknowledge the importance of security testing of their software, applications, network, systems. Software developing teams even work with their partners to make their products more resilient to threats, taking the cybersecurity shield to the next level.
Testing for security helps secure not only transactions (be it money or data), but also protection of their end-users. Because cyber threats can take place in any form, at any moment, security testing will continue to be a popular topic in the following year.
Conclusion
These are our compiled list of predictions on the most popular software testing trends in 2020. No matter how the digital transformation is going to turn out in the following year, it is certain that testing engineers, as well as software products enterprises, will continue to witness changes and adjustments. As a result, quality assurance teams, leaders, and practitioners must constantly evolve in order to stay agile in this ever-changing industry.
The post
Top Software Testing Trends to Watch Out For in 2020
appeared first on
Katalon Solution
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
🚀 DebugSwift Hits 500 Stars on GitHub! 🎉
Matheus Gois -
Dec 11
Turn Any Webpage to Markdown with this Ridiculously Simple Bookmarklet
AIRabbit -
Dec 11
From Sisterly Wisdom to Debian Dreams: My Outreachy Journey
Divine Attah-Ohiemi -
Dec 11
How to Avoid Chaos in Branches: Best Practices for Git Workflows
Crypto.Andy (DEV) -
Dec 11
Katalon Platform
Follow
A modern, comprehensive quality management platform
Download Katalon for free at katalon.com
More from
Katalon Platform
Announcing Katalon TestCloud | Cloud-based Cross-Browser Testing in The Katalon Platform
#
testcloud
#
katalon
#
newrelease
Top 7 Test Reporting Tools | Latest Update
#
testreporting
#
testingtools
#
testops
#
testautomation
Smart Test Reporting in Software Testing | Katalon TestOps
#
reporting
#
testautomation
#
testops
#
devops
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
SciChart is the fastest JS Chart library available - DEV Community,"SciChart is the fastest JS Chart library available - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Andrew Bt
Posted on
Mar 13
• Originally published at
scichart.com
SciChart is the fastest JS Chart library available
#
javascript
#
chart
#
library
#
webdev
It’s been just over a year since we released SciChart.js: a High Performance Realtime
JavaScript Chart Library
. From the start, our goal has been to create the fastest possible JavaScript Charts, but also produce visually appealing, productive, highly configurable and flexible / powerful TypeScript / JavaScript charting controls, capable of being used in demanding business apps.
Our first version of SciChart.js was able to
draw a million data points
in a browser, and we have only worked to improve our performance since then.
Version 2.1 of SciChart.js (in BETA now)
can draw over 10 million data points
and has even been tested up to
100 million data points
. That's more than enough to show the entire history of Bitcoin in a 1-minute chart, or visualise data in the most demanding applications.
Why would you need this speed in a Js Chart?
Currently, most JavaScript applications use charting in the sense that they may have a few hundred to a few thousand data-points on an infographic, chart, or graph.
Most existing
Fast
JavaScript Chart Libraries
can only render a few thousand points before slowdown. However, trends in Js application development are changing.
Most JavaScript Chart Libraries can render a few thousand points before slowing down. Big-Data is a trend across industries and the amount of data is only getting bigger. If the speed of your JS Chart Library is not an issue now, it will be soon.
There are a lot of legacy native-code applications in
Science, Engineering
and
Finance
which need to render millions of data-points from big-data stores, sensors (telemetry, IoT). Even hardware devices like electronic test equipment, process monitoring equipment can have integrated displays which include real-time charts to visualise data. Many legacy applications are moving toward npm / Typescript / React as a UI tech stack.
Big-Data
is a trend across industries and the amount of data to visualise is only getting bigger. If the performance of your JS Chart Library is not an issue now, it will be soon. With new tools like
SciChart
allowing you to visualise bigger datasets, it will become a competitive advantage to be able to visualise and gain valuable insights from the rich data your organisation has collected.
Before SciChart, there was no viable solution for big-data, dynamic or real-time charts in JavaScript applications, however we've built an award-winning, cutting-edge solution based on gaming technology, with WebGL and WebAssembly to provide high performance JavaScript Charts.
SciChart solves tomorrow's problems today: visualising large datasets, or dynamic datasets in data-intensive applications.
Charting Performance Test Cases
We've built a demanding test-suite which compares the performance across a number of chart components. Included in the test are the following JavaScript Chart libraries:
Plotly.js
- an SVG and WebGL powered JS chart
HighCharts
- SVG powered JS charts
HighCharts
Boost
- a higher performance plugin for HighCharts
Chart.js
, an open source library based on HTML5 Canvas
SciChart.js
, our WebGL & WebAssembly powered chart library
Note: Other chart libraries were tested internally but not included in the results below, as we are on focussing on the most popular libraries.
We put them head to head in a demanding performance test that stress your CPU & GPU to the max to find out which is the fastest.
These are the test cases:
Thousands of Line Series test
Realtime Scatter Chart test
XY Scatter Line test
Column Charts test
Candlestick Charts Test
Mountain (Area) Charts Test
Realtime Scrolling Lines Test
Realtime Updating Lines Test
Demanding, Varied test cases stress different types of drawing or rendering. In our
JavaScript Chart Library
Performance Comparison we test:
Plotting JS Line charts
with hundreds, or thousands of line series
Plotting JS Scatter charts
with real-time updates
XY Lines (scatter line)
. A challenging test case as it can't be downsampled easily.
Column Charts
. Columns require extra geometry and are difficult to optimise
Mountain or Area Charts
. Also require extra geometry
Candlestick Charts
. An important test case for stock chart applications which demand good rendering performance.
Realtime JS Chart Cases
. One with scrolling line series (for realtime monitoring applications) and another with new data added to the chart at high speed.
The Performance Comparison Test Results
You can find the test results of our JavaScript Chart Performance Comparison below.
Test Results Table
Below, you can find the performance test results in table form. The test case & parameters are on the left, and the results are in FPS (Frames per second). Drawing speed is measured in
FPS
(Frames per second –
Higher is Better
), meaning, the average number of redraws per second during the test. An FPS result above 30 is smooth to the eye.
Heatmap colours highlight the winners & losers. On the right, find the Speed Increase of SciChart.js compared to the second-fastest JS Chart Library as a percentage.
Performance Comparison between SciChart.js, HighCharts (with and without Boost), Plotly.js and Chart.js. Tests carried out with SciChart.js v2.1 BETA
We'll draw a conclusion below, but some immediate takeaways from the table above:
SciChart.js is incredibly fast!
Of the test results which passed, the average speed increase was 1,975% (~20x faster). The maximum speed increase was 11,126% (111x faster than the second-fastest competing JavaScript Chart). In some cases, SciChart.js was able to draw hundreds, or even thousands of times more data.
More on the performance test results below...
Test Results in Chart Form
Let's see the test results in chart form. Click on a chart to view full size!
JavaScript Chart Performance Test: Thousands of line series
JavaScript Chart Performance Test: Dynamic Scatter series
JavaScript Chart Performance Test: XY Line series
JavaScript Chart Performance Test: Column series
JavaScript Chart Performance Test: Candlestick
JavaScript Chart Performance Test: Mountain (Area)
JavaScript Chart Performance Test: Realtime Lines
JavaScript Chart Performance: Realtime Lines (2)
The bar charts above make it easier to see how SciChart.js (green) is performing vs. the competition. Even with 1 to 10 million data-points, SciChart.js is drawing charts at high refresh rates, meaning that SciChart is able to plot a chart in record time.
Some competing JS charts drop off very quickly. Take a look at the Candlestick chart test for example. All competitors are struggling with only 1,000 candles. At 10,000, they can barely redraw. Only SciChart.js can handle millions of candlesticks - enough to plot the entire history of Bitcoin in a 1-minute timeframe.
Performance Comparison Conclusions
So, some pretty incredible performance results above for a variety of popular JavaScript chart libraries! What conclusions can we draw from these?
According to the Performance Comparison Results,
most JavaScript Chart libraries are unsuitable for datasets over a few thousand points, or dynamic updates on the chart
.
Most perform poorly once the amount of data reaches only a few thousand points.
All struggled to draw more than a thousand candlesticks
in stock chart configuration.
Many failed / crashed when larger datasets were given to the chart
. For example, HighCharts.js simply crashed (even with boost module) when attempting to draw 500 line series with 500 points.
How does SciChart.js Compare?
In demanding and varied test cases,
SciChart.js beat all competing JavaScript Chart libraries
in performance tests carried out.
For several test cases, competing charts failed to draw (crashed or hanged the webpage).
For the above test cases,
SciChart.js was on average 1,975% faster (~20x faster) than the 2nd fastest competitor, and a maximum of 11,126% faster (111x faster!)
For Candlestick Charts,
SciChart.js can draw 1,000x more candlesticks than competing libraries
, allowing you to show large histories such as years of 1-minute stock, forex or cryptocurrency data in JavaScript.
For Column Charts,
SciChart.js was also able to show 100x more data than competing charts
SciChart.js was able to draw thousands of line series
, opening up advanced statistical analysis or comparisons which are not possible with other JS Chart libraries.
If you are building an application with demanding data-visualisation requirements, or porting from Windows (native code) to JavaScript, then SciChart is the only viable solution today for big-data applications, complex and data-intensive apps. If chart flexibility, rendering performance and reliability are mission critical to the success of the app, and your business, then we hope we've demonstrated why we remain the expert's choice.
Find out More
at the link below.
https://scichart.com/javascript-chart-features
JavaScript Charts by SciChart
JavaScript Charts by SciChart are extremely fast and allow robust, high performance data-visualisation across a number of chart types & scenarios. Learn more about the features and start a trial today by clicking the button below.
Start Trial
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Part 5: Functions and Methods in C#
Adrián Bailador -
Dec 1
Animated Movie Card w/ HTML and CSS
Tilak Jain -
Dec 1
Web Components? Make it so!
Danny Engelman -
Dec 5
Gamedev.js Survey 2024 is open!
Andrzej Mazur -
Dec 6
Andrew Bt
Follow
Former electronic engineer who works in software. With experience in languages from C/C++ to C#, JavaScript and TypeScript, and now specialises in performance optimisation and data visualisation
Location
United Kingdom
Joined
Dec 14, 2023
More from
Andrew Bt
How Fast is SciChart’s WPF Chart? DirectX vs. Software Comparison
#
chart
Android Chart Performance Comparison
#
chart
#
android
How Fast is SciChart’s iOS Chart?
#
webdev
#
javascript
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Data Quality: Technical Debt From Hell - DEV Community,"Data Quality: Technical Debt From Hell - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Miguel Barba
Posted on
Sep 24, 2017
Data Quality: Technical Debt From Hell
#
dataquality
#
datamigration
#
technicaldebt
This post was originally published
here
.
Technical debt can take many forms as most of you probably know it. Data quality is just one of those forms.
Data quality is a major issue in large organisations, where multiple systems, applications and services interact between themselves, exchanging and altering data. Incoherences will always occur. Either because someone made a mistake, either because there’s an unidentified bug somewhere, either because the system’s architecture isn’t as robust as it should or simply because people prefer to ignore it (this last one happens way more than it should, trust me!). This will contribute for a consistent and sometimes quiet but steady increase of your technical debt.
Don’t let your selves be fooled. It’s easy to start getting reckless regarding data quality, furthermore when you’re working on a data migration project for several months, for example, and you simply start to reach a point when, even though you don’t want to, you start getting tired and making mistakes. Hell, sometimes you’re just having a bad day… It can happen to any of us!
There are several ways to address this issue. Here are some ways:
Let’s start by stating the obvious. Make sure your services are the least error-prone possible. This is where all technical debt should start being addressed: before it even has a chance to exist. I know I’m being a dreamer, unrealistic or whatever but in a perfect world, this is how it should work.
It’s nice when you have a team dedicated to performing data analysis and comparisons between different systems and enforcing data repairs in order to correct the data; this action should be performed as often as possible on a regular basis.
Understanding what’s causing the incoherence; it may be very difficult to achieve it but when you manage to do it, the probability of eradicating it will be extremely high.
This all looks great and relatively simple to achieve but then enters another common problem in any organisation: people. We are our worst enemies most of the cases when we end up going in opposite directions when trying to solve some problem we have in common. Fortunately, this doesn’t happen always and as time goes by the tendency it’s becoming quite the opposite, although there’s still a large margin for improvement (there always is!).
The idea of writing this post was a direct result of identifying a data incoherence carried throughout 4 system upgrades and migrations and that finally haunted someone (me in this particular case) more than 16 years later the flawed data was first created. Nightmarish, don’t you think? What’s yours?
Photo credit: T a k (
https://www.flickr.com/photos/takashi/18862634/
) via
VisualHunt
/
CC BY-NC
Top comments
(4)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Tryggvi Björgvinsson
Tryggvi Björgvinsson
Tryggvi Björgvinsson
Follow
I'm the head of IT at Statistics Iceland. I describe myself as a hacker turned manager so that other hackers don't have to (but I sure do miss hacking all the time).
Email
me@trickvi.is
Location
Iceland
Education
Ph.D in software engineering
Work
Head of IT at Statistics Iceland
Joined
Feb 26, 2017
•
Sep 25 '17
Dropdown menu
Copy link
Hide
Nice post. I'm the author of an upcoming book (in early access program now) called
The Art of Data Usability
which is at its core about data quality. I've never thought of data quality as technical debt. That's a really nice way to frame it. I really like it :)
One thing I'd recommend is setting up monitoring of your quality attributes (like the incoherency you talk about). You monitor the attributes to make sure the quality continues to stay at the level you want (that you don't start collecting technical debt again) but you do it from the start (when you start the working on lowering the debt) to know when you've reached that level of quality. As you said, we start making mistakes, have a bad day or something. Monitoring quality helps us stay focused.
You can think of it as data quality tests. You monitor afterwards for regression testing and you develop the metrics and monitoring before you start as some sort of a TDD approach.
Again, a really good post and a fresh perspective on data quality.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Miguel Barba
Miguel Barba
Miguel Barba
Follow
Miguel Barba is licensed in Computer Engineering by the Instituto Superior Técnico, in Lisbon. He joined Accenture in November 2007 as a Junior Programmer. Since then he has been involved in Telco ...
Location
Lisbon, Portugal
Joined
Mar 9, 2017
•
Sep 25 '17
Dropdown menu
Copy link
Hide
Thanks for the feedback.
And congrats on your book, by the way!
""One thing I'd recommend is setting up monitoring of your quality attributes""
- Yes, that would be the ideal scenario and it used to happen here but unfortunately the team responsible for doing it is from another department and our priorities and approaches to problem solving aren't always as aligned as they should be, so this ends up having a negative impact when it comes to detect and correct data issues on a regular basis.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Alan Barr
Alan Barr
Alan Barr
Follow
QA Engineer
Email
alan@alanmbarr.com
Location
Columbia, MO
Education
San Jose State University
Work
QA Engineer at Veterans United Home Loans
Joined
Jan 13, 2018
•
Jul 8 '18
Dropdown menu
Copy link
Hide
Painful stuff. One area I have seen recently is not owning or having a solid handle on the full domain model of one's data. Or even just being at the whim of a third parties representation of it. Huge effort behind this if it's not considered from the beginning or early on.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Miguel Barba
Miguel Barba
Miguel Barba
Follow
Miguel Barba is licensed in Computer Engineering by the Instituto Superior Técnico, in Lisbon. He joined Accenture in November 2007 as a Junior Programmer. Since then he has been involved in Telco ...
Location
Lisbon, Portugal
Joined
Mar 9, 2017
•
Oct 20 '17
Dropdown menu
Copy link
Hide
I was reading the latest post by John Allspaw when I realized that it sums up perfectly the concept I was referring to when I wrote this post:
""My main argument isn’t that technical debt’s definition has morphed over time; many people have already made that observation. Instead, I believe that engineers have used the term to represent a different (and perhaps even more unsettling) phenomenon: a type of debt that can’t be recognized at the time of the code’s creation. They’ve used the term “technical debt” simply be- cause it’s the closest descriptive label they’ve had, not because it’s the same as what Cunningham meant. This phenomenon has no countermeasure like refactoring that can be applied in anticipation, because it’s invisible until an anomaly reveals its presence.""
Feel free to read the complete post
here
, because it's quite worth it!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Salesforce vs. HubSpot: Choosing the Best CRM for Scaling Businesses in 2025
Chetan Mistry -
Dec 11
Microsoft Dynamics Testing: Ensuring Security, Scalability, and Reliability
DynaTech Systems -
Dec 11
Taiwan Website CMS Rankings: Adobe Experience Manager (AEM) Stays on Top
leads-technologies -
Dec 11
Common Errors While Developing Websites with Next.js
Brandstory Mumbai -
Dec 11
Miguel Barba
Follow
Miguel Barba is licensed in Computer Engineering by the Instituto Superior Técnico, in Lisbon. He joined Accenture in November 2007 as a Junior Programmer. Since then he has been involved in Telco ...
Location
Lisbon, Portugal
Joined
Mar 9, 2017
More from
Miguel Barba
Data Migration
#
datamigration
#
architecture
#
development
#
discuss
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Optimizing Salesforce Data Integration: Tools and Best Practices - DEV Community,"Optimizing Salesforce Data Integration: Tools and Best Practices - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Dorian Sabitov
Posted on
Nov 12
• Originally published at
sfapps.info
on
Nov 12
Optimizing Salesforce Data Integration: Tools and Best Practices
#
blog
#
howto
#
sponsored
Introduction: Why Do You Need a Data Integration?
For businesses that rely on Salesforce, data integration plays a key role in ensuring that information is accurate, accessible, and actionable. Many organizations use Salesforce to manage customer data, an ERP system for financials, and other platforms for inventory or marketing. When these systems aren’t connected, data can become scattered, making it challenging to access the complete picture and slowing down workflows. Effective Salesforce data integration solves this by unifying data, so teams have a consistent, real-time view across all systems.
A robust ETL (Extract, Transform, Load) tool simplifies this integration. With ETL, data can be extracted from multiple sources, transformed to fit Salesforce’s data structure, and loaded where it’s needed — all while maintaining high data quality. For instance, imagine a retail business that uses Salesforce to track customer orders and a separate system to manage inventory. Without data integration in Salesforce, sales reps may have to check multiple systems to confirm product availability, which can delay responses and create potential errors. By using an ETL tool to bring inventory data into Salesforce, sales reps get real-time availability updates in one place, speeding up service and reducing mistakes.
In this article, we’ll cover Salesforce data integration tools, best practices, and popular methods for data integration in Salesforce, while providing links to helpful
Salesforce migration tools
that can keep your data organized and consistent. Whether integrating Salesforce with a data warehouse or simply connecting it to another app, these strategies and tools can make your data integration smoother and more reliable.
Understanding Data Integration in Salesforce
What Is Data Integration?
Types of Data Integration in Salesforce
Benefits of Data Integration in Salesforce
Key Data Integration Tools for Salesforce
1. Salesforce Data Loader
2. DataLoader.io
3. Data Import Wizard
4. Sliced Bread ETL
Setting Up Effective Data Integration Strategies in Salesforce
Why Data Integration Strategies Matter
Key Data Integration Strategies for Salesforce
Best Practices for Successful Salesforce Data Integration
1. Start with Data Preparation
2. Automate Recurring Data Integration Tasks
3. Monitor and Validate Data Quality Regularly
4. Test Data Integration in a Sandbox Environment
5. Establish Clear Data Governance Policies
6. Regularly Review and Update Integration Strategies
7. Prioritize Security Throughout the Integration Process
8. Document Your Integration Processes
How Sliced Bread ETL Solves Common Data Challenges for You
Wrapping up: Optimize Data Integration in Salesforce with the Right Tools
Understanding Data Integration in Salesforce
Data is everywhere — in every system, database, and app we use. But when each platform keeps its own isolated data, it’s hard to see the complete picture. Data integration in Salesforce fixes this by combining information from different sources, breaking down data silos, and making all data accessible in one place. This unified approach helps teams make better decisions, work more efficiently, and stay up-to-date with real-time data.
What Is Data Integration?
Data integration is the process of connecting data from multiple systems into one view. For example, a company might have customer information in Salesforce, sales data in a separate database, and inventory details in a third system. Without integration, it’s difficult to get a full view of the business. Salesforce data integration solves this by unifying data so it’s easier to analyze, providing clearer insights, and enabling quicker, more confident decision-making.
Insight:
Do you know what exactly ETL tools do? ETL tools simplify the complex process of managing data by focusing on three key steps:
Extract
: ETL tools gather data from multiple sources, such as spreadsheets, databases, cloud applications, or even third-party software. Think of it as gathering all the different pieces of information you need, no matter where they currently live.
Transform
: Once the data is collected, ETL tools clean, restructure, and format it to make sure it matches Salesforce’s requirements. This step ensures that all inconsistencies are resolved and the data is ready for smooth use. Imagine transforming messy, unorganized data into something precise and consistent—like turning a pile of ingredients into a ready-to-serve meal.
Load
: Finally, the polished data is moved into Salesforce, making it accessible and ready for your team to use effectively. With this step, everything you need is neatly in place, within Salesforce, at your fingertips.
Without ETL tools, these steps would have to be done manually—a process that would be not only time-consuming but also prone to errors. By automating these steps, ETL tools save time and minimize mistakes, giving you clean and usable data without the headache.
Types of Data Integration in Salesforce
Different approaches to data integration in Salesforce allow businesses to choose the method that best meets their needs. Here are some common types:
Data Warehousing Integration
– Data from multiple sources is cleaned, organized, and stored in one central place — a data warehouse. This makes it easy to run comprehensive reports, as all data is available in a single view. By using data warehouse integration with Salesforce, businesses can analyze trends and make informed decisions based on all their data.
Middleware Data Integration
– Middleware acts as a connector between systems, moving data between Salesforce and other sources in a way that keeps everything in sync. It’s especially useful for companies that have multiple systems exchanging data regularly, like syncing customer information between Salesforce and an ERP.
Data Consolidation
– Data consolidation gathers information from different systems and combines it into one place. ETL (Extract, Transform, Load) tools make this process easy by pulling data from each source, reformatting it to fit Salesforce, and loading it in. This approach creates a complete view of company data and is especially helpful for mass updates and record management in Salesforce.
Application-Based Integration
– Application-based integration uses software to pull data from various systems, often in real time. This is ideal for companies needing up-to-the-minute data, such as sales teams who need instant customer insights. This type of integration makes sure information across platforms stays current.
Data Virtualization
– Data virtualization allows teams to access data from its original source without moving or copying it. It creates a combined view of the data without storing it in a central repository, offering flexibility for businesses with diverse data storage needs.
Benefits of Data Integration in Salesforce
Data integration offers a range of benefits for any organization, helping them work smarter and operate more effectively. Here are a few major advantages of using data integration tools in Salesforce:
Better Decision-Making
: Integrated data gives a full view, enabling smarter choices. When all data is accessible in Salesforce, teams can make decisions based on complete and accurate information, whether it’s for customer insights, trend analysis, or internal planning.
Increased Efficiency
: Automating data integration saves time and reduces mistakes from manual data entry. This streamlined process frees up teams to focus on high-impact tasks instead of managing data across systems.
Improved Data Quality
: During integration, data from different sources is cleaned and standardized, leading to high-quality, reliable information. This is essential for accurate reporting and data analysis.
Salesforce ETL tools
help ensure data accuracy, especially during complex integrations.
Stronger Collaboration
: Integrated data creates a single source of truth that everyone in the organization can access. This shared information allows different teams to collaborate easily, knowing they’re all working with the same data.
Cost Savings
: With integrated data, companies can reduce storage and maintenance costs by managing data in a single, organized system. Automation also lowers expenses by cutting down on manual tasks, which saves time and resources.
By understanding these types of data integration in Salesforce and the benefits they bring, businesses can choose the approach that best fits their needs. This ensures they’re getting the most out of their data for better insights, smoother operations, and faster decision-making.
Key Data Integration Tools for Salesforce
Moving data into Salesforce can be challenging, especially when dealing with large datasets or needing specific data transformations. ETL (Extract, Transform, Load) tools streamline this process, helping to move, clean, and organize data for easy access and use in Salesforce. Here’s an overview of some leading Salesforce data integration tools and their unique features:
1. Salesforce Data Loader
Salesforce Data Loader
is Salesforce’s native tool for bulk data operations, perfect for high-volume imports and exports. Users can upload or delete thousands of records at once, but it lacks advanced data transformation capabilities. Salesforce Data Loader is suitable for straightforward data tasks where complex field mapping or automation isn’t required, making it ideal for handling large but simple data imports.
2. DataLoader.io
DataLoader.io
is a browser-based tool, removing the need for software installation. It’s flexible and easy to use, with options for importing, exporting, updating, and deleting data directly through the cloud. The free plan allows up to 10,000 records per task, while premium plans unlock the capacity to handle up to 5 million records per upload. DataLoader.io also offers scheduling features, which let users set recurring tasks—useful for regular data updates without manual input.
3. Data Import Wizard
The
Data Import Wizard
is another built-in Salesforce tool that guides users through a step-by-step process for importing smaller datasets, such as contacts or leads. With a limit of 50,000 records per import, it’s best suited for smaller projects or simple updates. While it’s easy to use, it lacks features for advanced transformations, validation, and automation, making it suitable for straightforward data uploads but less ideal for complex tasks.
4. Sliced Bread ETL
Sliced Bread ETL
is a powerful ETL tool tailored specifically for Salesforce, offering a range of advanced features that enhance data integration. Known for its comprehensive data transformation, automation, and error-handling capabilities, Sliced Bread ETL goes beyond the functionality of standard tools to meet the demands of businesses with large or complex data requirements.
Here are some standout features of Sliced Bread ETL:
Comprehensive Data Transformation Capabilities
: Sliced Bread ETL excels at data transformation, making it easy to map and convert data into Salesforce-compatible formats. This tool allows users to clean, format, and organize data before loading, helping ensure data integrity. With flexible field mapping and support for various data types—like text, numbers, and dates—Sliced Bread ETL enables smooth and precise data integration, keeping Salesforce data consistent.
Automation and Scheduling
: Automation is a core feature of Sliced Bread ETL. Users can set up automated schedules—daily, weekly, or even hourly—to run their data extraction, transformation, and loading tasks, minimizing manual work. Additionally, it supports trigger-based automation, where tasks initiate based on specific events or conditions, making it ideal for businesses needing regular, reliable updates in Salesforce.
Error Handling and Data Validation
: To maintain high data accuracy, Sliced Bread ETL offers advanced error handling. It can detect and correct common issues, like duplicates or missing values, as data is processed, reducing manual data cleaning. The tool also generates validation logs for each ETL task, providing a summary of any errors or issues encountered so users can review and resolve them promptly.
Scalability for Growing Businesses
: As a business grows, so does its data, and Sliced Bread ETL is built to handle increasing volumes and complexity. With optimization for large datasets, it completes tasks quickly, even with millions of records. Additionally, it offers flexible resource allocation, allowing users to scale resources as needed to accommodate their data demands without unnecessary costs.
Seamless Salesforce Integration
: Sliced Bread ETL is specifically designed for Salesforce, integrating smoothly with its data structure and handling objects, fields, and relationships with precision. This native compatibility minimizes errors and keeps data consistent across systems, enhancing the Salesforce experience.
Customization Options
: Every organization’s data needs are different, and Sliced Bread ETL allows users to create custom data workflows, transformation rules, and validation processes tailored to their unique requirements. Users can define custom fields and formulas within the ETL process, giving them control over how data is managed and presented in Salesforce.
Sliced Bread ETL
stands out as a comprehensive solution for Salesforce data integration. For example, an e-commerce company needing to sync customer and order data from multiple platforms could automate these data flows with Sliced Bread ETL. This reduces manual work, prevents errors, and ensures that Salesforce always has the latest data on hand.
Insight:
Did you know that Sliced Bread ETL can help you mass upload and import records into Salesforce from not just CSV or JSON files, but also from Excel, XML, and NDJSON? This versatility allows you to work with different file formats, making data integration more convenient and efficient.
Each of these tools provides unique capabilities to meet different business needs. While Salesforce Data Loader and Data Import Wizard are solid for basic tasks, Sliced Bread ETL offers the advanced transformation, automation, and scalability options that larger or data-intensive organizations often require.
Setting Up Effective Data Integration Strategies in Salesforce
Establishing effective data integration strategies is key to keeping your Salesforce data consistent, accurate, and accessible. With clear strategies, you can avoid common data issues like duplicates, mismatched formats, or incomplete records that often result from pulling data from different sources. Implementing these strategies helps streamline your data integration and reduces the time spent on manual corrections.
Why Data Integration Strategies Matter
When data flows into Salesforce from various systems—such as ERP, marketing, or finance platforms—each may have a different structure or format. Defining integration strategies ensures that information arrives in Salesforce accurately and seamlessly, supporting day-to-day operations and decision-making. By automating these processes, you not only save time but also keep your Salesforce data clean and reliable.
Key Data Integration Strategies for Salesforce
Data Standardization
– Standardizing data means setting common formats and conventions that all data must follow. For example, for
Salesforce mass update records
, ensuring all dates appear in the same format (e.g., YYYY-MM-DD) or using consistent casing for text fields, like names. This strategy makes data easier to analyze and search. Tools like Sliced Bread ETL can automatically apply these standardizations, reducing manual reformatting.
Data Deduplication
– Duplicate records clutter Salesforce, making it hard for teams to find accurate information. Using deduplication strategies helps prevent this by checking for uniqueness. For example, when importing customer records, you might set a strategy to check for duplicate email addresses or phone numbers. Sliced Bread ETL automates deduplication by detecting and managing duplicate records, helping maintain a clean dataset in Salesforce.
Data Validation
– Validation ensures that data is complete and meets required conditions before it’s integrated. For instance, you might require every customer record to include an email and phone number. Sliced Bread ETL allows you to set these validation checks as part of your integration, ensuring that incomplete or incorrect data doesn’t enter Salesforce. Validation logs also help identify and resolve any issues during integration.
Field Mapping and Conversion
– Field mapping aligns data fields from source systems with those in Salesforce. For example, if an ERP system uses “Customer ID” but Salesforce uses “Account ID,” mapping connects these fields correctly. Field mapping and conversion strategies are essential when dealing with multiple data sources that might use different formats or names. Sliced Bread ETL simplifies this process with flexible mapping and conversion tools, aligning fields accurately and transforming data types as needed.
Automated Data Cleaning
– Data cleaning removes irrelevant or outdated information from the dataset before it’s integrated into Salesforce. This strategy helps eliminate empty fields, unnecessary characters, and invalid entries, ensuring that Salesforce only contains useful, high-quality data. With automated cleaning tools like those in Sliced Bread ETL, you can ensure consistent data quality without manual adjustments.
Error Handling and Resolution
– Handling errors during integration is crucial, especially for large or complex datasets. Error-handling strategies specify how to manage issues when they arise—whether to halt the integration or continue processing while logging errors. Sliced Bread ETL offers advanced error-handling options that automatically detect and correct issues, while also providing logs for further review. This helps prevent errors from disrupting the integration process.
After defining your strategies, the next step is to implement them in Salesforce and configure your chosen ETL tool. For instance, Sliced Bread ETL allows you to build custom workflows to automate these strategies, with scheduled tasks that ensure consistency. Testing in a sandbox environment can help identify and resolve any potential issues before going live.
Setting up effective Salesforce data integration strategies improves data quality and keeps your Salesforce environment organized. With an advanced ETL tool like Sliced Bread ETL, you can automate and enforce these strategies, creating a more efficient and dependable data integration process.
Best Practices for Successful Salesforce Data Integration
Implementing Salesforce data integration best practices is essential for maintaining data quality, reducing errors, and ensuring the integration process is efficient. Following these practices, along with using
Salesforce data quality tools
, helps organizations maximize their data integration in Salesforce, leveraging accurate and organized data across systems. Below are some strategies to help you optimize Salesforce data integration and enhance the use of Salesforce data integration tools.
1. Start with Data Preparation
Thorough data preparation is the foundation of successful data integration in Salesforce. This involves cleaning data, removing duplicates, and standardizing formats before integrating it into Salesforce. By ensuring only high-quality data enters Salesforce, businesses can avoid common issues and ensure data flows smoothly according to established Salesforce data integration rules.
2. Automate Recurring Data Integration Tasks
Automating recurring tasks in Salesforce data integration minimizes manual effort and reduces the risk of errors. Automation is especially helpful for regular data syncs and updates, ensuring data accuracy without ongoing intervention. Many Salesforce data integration tools offer automation features, allowing for reliable and consistent data flows.
3. Monitor and Validate Data Quality Regularly
Ongoing validation and monitoring are crucial for maintaining high data quality in Salesforce. Regularly reviewing data helps catch discrepancies or issues that may arise during data integration in Salesforce. Validation is a key component of Salesforce data integration rules and supports better reporting and decision-making across the organization.
4. Test Data Integration in a Sandbox Environment
Testing in a sandbox environment before applying Salesforce data integration options in a live setting ensures that integrations run smoothly. This practice allows teams to address potential issues without affecting production data. By testing data integration rules in Salesforce beforehand, organizations can prevent errors in their live environment.
5. Establish Clear Data Governance Policies
Data governance policies provide a framework for managing and securing data during Salesforce data integration. These policies define who can access and edit data, ensuring consistency and security. Effective governance aligns departments around data integration rules Salesforce and promotes a reliable data integration Salesforce that everyone can depend on.
6. Regularly Review and Update Integration Strategies
As business needs change, so should your Salesforce data integration strategies. Regularly revisiting and refining your Salesforce data integration options ensures they continue to meet organizational goals. For example, increasing data sync frequency or updating validation criteria can help keep data integration strategies effective as your business and data volumes grow.
7. Prioritize Security Throughout the Integration Process
Data security is critical when handling sensitive information in Salesforce. Ensuring that data integration tools in Salesforce follow security protocols helps protect valuable information. By using secure practices and regularly reviewing Salesforce data integration rules, businesses can maintain data integrity across systems and guard against data breaches.
8. Document Your Integration Processes
Clear documentation of Salesforce data integration processes supports consistency, troubleshooting, and team onboarding. Comprehensive documentation should include data sources, field mappings, transformation rules, and error-handling methods. This clarity strengthens the overall integration process, making it easier to adapt as your organization’s data integration needs evolve.
By following these best practices for Salesforce data integration, organizations can enhance data quality and streamline workflows. In conclusion, we’ll discuss how advanced Salesforce data integration tools support these practices, making it easier to create reliable, secure, and efficient data integration in Salesforce.
Insight:
How Sliced Bread ETL Solves Common Data Challenges for You
Sliced Bread ETL makes handling data much simpler by addressing common issues in the ETL process:
Automated Processes
: Sliced Bread ETL automates the entire ETL workflow, making it faster and easier to manage large amounts of data. Instead of manually extracting, transforming, and loading data, everything runs automatically, saving time and effort.
Accurate Data Transformation
: The tool ensures data from various sources is cleaned up and properly formatted for Salesforce, which reduces inconsistencies and ensures everything is in the correct structure. This way, your data is always reliable and ready for use.
Error Handling
: Sliced Bread ETL spots and fixes errors during the data processing stages, which means fewer mistakes and less manual intervention. This reduces the risk of bad data causing problems later.
Handles Large Data Volumes
: Built to handle growing data needs, Sliced Bread ETL is ideal for expanding businesses that have increasingly large datasets to work with. It scales as your business does.
Enhanced Security
: The tool includes security features that protect your data throughout the ETL process, ensuring sensitive information is kept safe and secure.
Using a tool like Sliced Bread ETL makes managing Salesforce data so much easier. It takes the stress out of moving, cleaning, and loading data, allowing you to focus on using that data to grow your business instead of getting bogged down by manual processes.
How Sliced Bread ETL Solves Common Data Challenges for You
Sliced Bread ETL makes handling data much simpler by addressing common issues in the ETL process:
Automated Processes
: Sliced Bread ETL automates the entire ETL workflow, making it faster and easier to manage large amounts of data. Instead of manually extracting, transforming, and loading data, everything runs automatically, saving time and effort.
Accurate Data Transformation
: The tool ensures data from various sources is cleaned up and properly formatted for Salesforce, which reduces inconsistencies and ensures everything is in the correct structure. This way, your data is always reliable and ready for use.
Error Handling
: Sliced Bread ETL spots and fixes errors during the data processing stages, which means fewer mistakes and less manual intervention. This reduces the risk of bad data causing problems later.
Handles Large Data Volumes
: Built to handle growing data needs, Sliced Bread ETL is ideal for expanding businesses that have increasingly large datasets to work with. It scales as your business does.
Enhanced Security
: The tool includes security features that protect your data throughout the ETL process, ensuring sensitive information is kept safe and secure.
Using a tool like Sliced Bread ETL makes managing Salesforce data so much easier. It takes the stress out of moving, cleaning, and loading data, allowing you to focus on using that data to grow your business instead of getting bogged down by manual processes.
Wrapping up: Optimize Data Integration in Salesforce with the Right Tools
Integrating data into Salesforce can bring big benefits, helping businesses get better insights, simplify processes, and make smarter decisions. By following best practices for Salesforce data integration—like preparing data, checking for accuracy, using automation, and testing in a sandbox—organizations can keep their Salesforce data clean, accurate, and reliable.
Choosing the right Salesforce data integration tools is key to making this process easier and more effective. Each tool has its strengths: Salesforce Data Loader is useful for bulk data uploads, DataLoader.io works well directly in your browser, and the Data Import Wizard is great for smaller, simpler tasks.
For advanced needs, however, Sliced Bread ETL stands out with its powerful features built specifically for Salesforce. This tool is designed to handle complex data processes efficiently. It offers extensive data transformation options, allowing you to clean and organize data before it reaches Salesforce. Its automation capabilities let you schedule tasks to run at set times, reducing manual work and keeping your data up-to-date. Plus, Sliced Bread ETL’s error-handling and validation features catch and correct common issues, like duplicates or missing fields, ensuring data quality. Built for scalability, this tool is equally effective for managing small datasets or millions of records, adapting as your business grows and your data needs evolve.
Whether you’re syncing Salesforce with other systems, managing large data imports, or using a Salesforce data warehouse integration for deeper analysis, the right tools and strategies can simplify these tasks. By combining the right approach with the best tools, businesses can keep their data organized, break down silos, and make Salesforce a single, trusted source of information.
In short, Salesforce data integration is about more than connecting systems—it’s about building a smooth, secure, and efficient setup that helps your business grow.
The post
Optimizing Salesforce Data Integration: Tools and Best Practices
first appeared on
Salesforce Apps
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Understanding Big O Notation: A Beginner's Guide
mohamed Tayel -
Dec 10
How to create a presigned URL for an S3 bucket on AWS.
Afeez Adeyemo -
Dec 10
DevOps - Não só mais um artigo
Pedro Christo -
Dec 10
15 System design tradeoffs for Software Developer Interviews
Soma -
Dec 10
Dorian Sabitov
Follow
I am a skilled Salesforce Administrator and Developer with four certifications. I'm quite interested in Salesforce and all of its uses in the modern corporate world.
Location
Ukraine
Joined
Oct 8, 2023
More from
Dorian Sabitov
Reasons to Hire a Remote Salesforce Administrator
#
blog
#
talentmarket
How to Implement Salesforce GDPR Compliance
#
blog
#
howto
#
salesforce
Mailchimp Salesforce Integration Guide: Easy Setup and Tools
#
blog
#
integrations
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Don't believe the hype. - DEV Community,"Don't believe the hype. - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Chris Bertrand
Posted on
Jul 8, 2020
• Originally published at
blog.designpuddle.com
Don't believe the hype.
#
beginners
#
opinion
#
career
#
productivity
I've been in this industry for over a decade now, and the one thing I've learnt is that you should never believe the hype.
Technologies come and go, things get superceded, things improve. You just need to look at the technologies or frameworks that are no longer with us, such as Flash and those that are now barely used including the myriad of JS solutions we've used. Sure, sometimes you might want to ride the wave, it'll be fun, but you might not be left with much afterwards.
Using what is cemented into the ecosystem and battle tested will mean you don't come across any horrible surprises. You have a myriad of documentation and stack overflow questions answered on the most superfluous of issues. Getting things done is easy, if not always elegant.
Selling you the dream.
Twitter, blog posts and conferences will sell you a reality that in most cases doesn't exist in the corporate world. That's the purpose of those mediums, they are trying to build and audience, create a market, get you interested in what is new.
It's basic marketing.
Mircroservices, Macrofrontends, Unit tests, Static code analysis, Best practices, Containers, K8s, Serverless, Cloud, ai, big data, react, go, nosql, sass, PWAs, design systems, automation, agile.
All these things are useful and important in the right circumstances, however if you think your company should be following all of these methodologies then you're being naive. Businesses have objectives and stakeholders to appease. They're not here for us to create the mecca of the technology world. Sure, some big businesses can afford to live the dream, but those are far and few between.
K.I.S.S.
There is nothing wrong with a
well organised and structured monolith
. In fact for most of us, that's still the best way to go.
All these things have been
heralded
as the
holy grail
and yet most businesses are using at most a couple of these idioms.
Our new devs are lambasted with complicated tech stacks, overpriced hosting solutions and preachy beliefs that make all but the most simple hello world application a difficult and unsatisfying proposition.
The Fundamental Fallacy
Rather than looking at problems,
we memorize
frameworks, syntax and convoluted ways to complete simple tasks.
I'm sick of telling people that you can achieve things in a few lines of JavaScript. There is
no need
for a complex SPA application written in React/Angular/Vue installing 37 dependencies to make an http request, or fill a simple form.
I'm tired of reading countless blog posts on how to use Gatsby and React to create the most perfect blog site that you can host on Netlify or GitHub pages.
If you want to blog, create some content,
don't spend 3 months creating a damn blog framework, there are hundreds out there already! Just use one!
The internet and technology has been around for a long time, and people are always pining for what's new. It's understandable, it's new, it's shiny, it's cool. But it's not always necessary. Actually it's very rarely necessary.
Sometimes a few lines of JQuery can do everything you need. Sometimes you don't need 100% code coverage across your codebase. Sometimes you just use WordPress to deploy your blog, and focus on the content you are trying to share.
Sometimes you shouldn't believe the hype.
Top comments
(35)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
David Sanwald
David Sanwald
David Sanwald
Follow
My other car is cdr!
Location
Berlin
Work
Developer
Joined
Jul 3, 2019
•
Jul 8 '20
Dropdown menu
Copy link
Hide
I don't agree. Sure, at the job it's often not about the newest or shiniest tech and unfortunately, the code quality is often problematic.
But if every blog was about good enough solutions using only the most proven, commonly used, and boring tech, what would be my value as a reader? It would be just like work, only without pay.
I'm here to get inspired and learn about things beyond my day to day work.
Like comment:
Like comment:
13
likes
Like
Comment button
Reply
Collapse
Expand
Maks Yadvinskyy
Maks Yadvinskyy
Maks Yadvinskyy
Follow
RxJS Plumber | Photographer | Car Enthusiast.
Location
Madison, WI
Work
Staff Software Engineer
Joined
May 28, 2019
•
Jul 8 '20
Dropdown menu
Copy link
Hide
So true! Sure you can use a lot of other’s blog frameworks but will that make you a better dev? I don’t think so. For those who are joining the field need all the experience they can get.
Like comment:
Like comment:
7
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
It's not about being a better dev, it's about creating a solution that solves your problem. Do you think your end user cares if you use jQuery, Less or your code doesn't have tests?
Like comment:
Like comment:
2
likes
Like
Thread
Thread
Dhruv garg
Dhruv garg
Dhruv garg
Follow
A generalist having imprecise knowledge of many things and trying to be specialist in Mobile app development, and backend development.
Location
New Delhi, India
Education
B. Tech in computer science
Work
Full Stack Developer Intern
Joined
Oct 1, 2019
•
Jul 9 '20
Dropdown menu
Copy link
Hide
I agree that tool and framework don't matter if you are building a good application in whatever framework you want, but I think still tests are important. Otherwise, the developer will have to spend time manually checking for everything after changes.
Like comment:
Like comment:
5
likes
Like
Thread
Thread
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
Isn't that what testers are for? Or should I say, your end users? 😛
Like comment:
Like comment:
1
like
Like
Thread
Thread
Dhruv garg
Dhruv garg
Dhruv garg
Follow
A generalist having imprecise knowledge of many things and trying to be specialist in Mobile app development, and backend development.
Location
New Delhi, India
Education
B. Tech in computer science
Work
Full Stack Developer Intern
Joined
Oct 1, 2019
•
Jul 9 '20
Dropdown menu
Copy link
Hide
In small teams(startup) you are everything ☹️
My app crashing in hands of end-user sounds scary. Even though there will always be bugs and crashes they can be limited with proper testing. But, I do confess that I haven't written tests as an Intern.
Like comment:
Like comment:
5
likes
Like
Thread
Thread
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
Haha, sorry. I think you misinterpreted my jovialness. Of course testing is a net positive. Covering every single line written is probably overkill. Focus on the important bits, and as Kent says. Write tests, not too many, focus on integration! 🤓
Like comment:
Like comment:
6
likes
Like
Thread
Thread
Dhruv garg
Dhruv garg
Dhruv garg
Follow
A generalist having imprecise knowledge of many things and trying to be specialist in Mobile app development, and backend development.
Location
New Delhi, India
Education
B. Tech in computer science
Work
Full Stack Developer Intern
Joined
Oct 1, 2019
•
Jul 10 '20
Dropdown menu
Copy link
Hide
Yeah, I think so 😅
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
• Edited on
Jul 9
• Edited
Dropdown menu
Copy link
Hide
Hey David, I'm not against learning new things and getting inspired, life would be a boring place if nothing changed. I'm saying that just because something is new and popular doesn't mean it's right for you and you need to use it right away. Being an early adopter is usually a painful experience, Take AngularJS, there are still countless companies using this now outdated tech because they jumped in too soon. See my post on why you should make your own decisions here...
Cargo Cult Programming!
Chris Bertrand ・ Jan 15 '19 ・ 6 min read
#webdev
#beginners
#learning
#career
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Kristjan Siimson
Kristjan Siimson
Kristjan Siimson
Follow
I have worked on web sites and web applications since 2009. I started with LAMP stack, now also JAMstack / others.
Location
Norway
Work
Software Developer at Sportradar AS
Joined
Jan 2, 2020
•
Jul 9 '20
• Edited on
Jul 9
• Edited
Dropdown menu
Copy link
Hide
There is no need for a complex SPA application written in React/Angular/Vue installing 37 dependencies to make an http request, or fill a simple form.
... few paragraphs later...
Sometimes a few lines of JQuery can do everything you need
TBH, jQuery is even worse than React. jQuery 2.1.3 minified is 84 kB. Preact is a lightweight alternative to React at only 3 kB. Sure, you create simple interactions with vanilla JS (or complex if feeling masochistic), but what do you need jQuery for in 2020? It's just a whole lot of bloat we needed because it gave some degree of compatibility with IE 6.
At end of the day, just use what works for you.
Like comment:
Like comment:
8
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
jQuery might be 84kb but how big is the bundle with all the dependencies in the complete React app? Sure Preact is a good alternative, but so is Vanilla JavaScript. I'm not advising people to use jQuery, far from it. At the end of the day, it's all the same code. Use whatever you want, dont pick one because it's cool. It sounds like you're aware of this already. Not everyone is. Thanks for the healthy debate!
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Kristjan Siimson
Kristjan Siimson
Kristjan Siimson
Follow
I have worked on web sites and web applications since 2009. I started with LAMP stack, now also JAMstack / others.
Location
Norway
Work
Software Developer at Sportradar AS
Joined
Jan 2, 2020
•
Jul 9 '20
Dropdown menu
Copy link
Hide
The dependency hell is not something forced upon you when use React, it's something that many React users have inflicted on themselves by not being careful what dependencies they introduce to their application. That comes from another hype that everything should be reusable, and the belief that if you copy paste two lines you will lose your limbs or something. 😁
Like comment:
Like comment:
8
likes
Like
Comment button
Reply
Collapse
Expand
Randal Vance Cunanan
Randal Vance Cunanan
Randal Vance Cunanan
Follow
Joined
Mar 20, 2019
•
Jul 11 '20
• Edited on
Jul 11
• Edited
Dropdown menu
Copy link
Hide
jQuery was also a hype back in 2000's. React is almost a decade old now, and I will no longer call it a hype.
I do understand the use of jQuery for legacy projects, if it aint broke, don't replace it.
But, when I see someone use jQuery to build a new project in 2020, I just see it as a refusal to learn a superior modern technology. There's just no excuse for it. It's the same reason new houses and buildings are built using new practices, while old houses are just maintained.
Like comment:
Like comment:
7
likes
Like
Thread
Thread
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 11 '20
Dropdown menu
Copy link
Hide
Everything is new and shiny at one point! 💍🏚️🚀
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Adrian Perea
Adrian Perea
Adrian Perea
Follow
Software Engineer. AI Enthusiast. Writer. Teacher. I talk about JavaScript and Artificial Intelligence. Check me out at: https://adrianperea.dev
Location
Kanagawa, Japan
Work
Software Engineer
Joined
May 17, 2020
•
Jul 9 '20
Dropdown menu
Copy link
Hide
I don't completely agree with you. On the one hand, you're right in saying that things that are marketed as shiny don't always shine so bright. But on the other hand, we need people to test new things and innovate. Otherwise, tech wouldn't move forward, and we won't have any new breakthroughs.
I don't know about you, but I don't want to believe in a world where there is no longer anything new and all the magic that can be found have already been exhausted.
Like comment:
Like comment:
8
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
That's completely fine Adrian, that's the joy of discussion! Tech has come on leaps and bounds, and I most certainly advocate for the use of new tech in development projects. I don't think that you constantly need to change your application to use them though. The problem you are trying to solve is more important than how you solve it. But yes, things would be a little dull if we were still programming in COBOL. 😉
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Adrian Perea
Adrian Perea
Adrian Perea
Follow
Software Engineer. AI Enthusiast. Writer. Teacher. I talk about JavaScript and Artificial Intelligence. Check me out at: https://adrianperea.dev
Location
Kanagawa, Japan
Work
Software Engineer
Joined
May 17, 2020
•
Jul 11 '20
Dropdown menu
Copy link
Hide
I wholeheartedly agree! There's a tendency to be hell-bent on using new technology since it's ""the next big thing"". A thorough understanding of what's already being offered is necessary. More often than not, the new technology is overkill of what you are trying to accomplish.
Cheers, Chris!
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Anik Khan
Anik Khan
Anik Khan
Follow
Convincing computers to do human tasks
Location
Nomad
Education
Computer Science
Work
Software Engineer  at Five Jack(itemku)
Joined
Jul 2, 2020
•
Jul 9 '20
Dropdown menu
Copy link
Hide
I agree with you up to a point, but it needs to be clarified whether you talking about individuals or companies. My hunch is that you are talking about companies. If its the case, I think you made a good point.
On the other hand, at the individual level developers should always try out the new things (my very personal opinion). But sometimes we just caught up in the hype & that's okay, life isn't about being perfect.
Like comment:
Like comment:
6
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
None of us are perfect. That's why pencils have erasers!
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Anik Khan
Anik Khan
Anik Khan
Follow
Convincing computers to do human tasks
Location
Nomad
Education
Computer Science
Work
Software Engineer  at Five Jack(itemku)
Joined
Jul 2, 2020
•
Jul 9 '20
Dropdown menu
Copy link
Hide
Agreed
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Vladimir Kapustin
Vladimir Kapustin
Vladimir Kapustin
Follow
A long time ago, in a galaxy far far away...
Location
Kharkov, Ukraine
Education
Higher
Work
Junior Full-Stack ASP.NET Core Developer at Self-Employed
Joined
Jul 9, 2020
•
Jul 9 '20
Dropdown menu
Copy link
Hide
Totally agree with author. New technologies that make more effective the result are wonderful things - that is the progress! But thousands of unnecessary solutions which only repeating existing ones, sometimes with minor changes, this is real trash but not an innovation. If you have an instrument that absolutely solves your tasks - use it, or at least you can make it better (But be sure that there are already people who responsible for it :)) People often make product to prove their value, but not to make ""something new"". It is just wasting of employer`s money or their own time. If things are like that it means that people cannot propose something new but only imitating. Sorry for seditious comment.
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
That's right Vladimir, progress is a wonderous thing, and people can and should test out these tools and learn from them. Just don't think they'll solve all your problems!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Troy Fitzwater
Troy Fitzwater
Troy Fitzwater
Follow
Work
Software Engineer
Joined
Sep 10, 2019
•
Jul 9 '20
Dropdown menu
Copy link
Hide
""Rather than looking at problems, we memorize frameworks, syntax and convoluted ways to complete simple tasks.""
This is so true for a lot of people, and I've seen people around me with this mindset.
Often enough, I think people overthinking these problems have just not done a needs assessment to figure out what they need to solve.
Also, a strong agree on content. It's such an important rule: Content. Is. King.
Awesome post!!
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
Thanks Troy, that mindset is a hard one to get out of, especially as we're fed all these new shiny things all the time! But yes, Content. Is. King! 🙌
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
MxL Devs
MxL Devs
MxL Devs
Follow
I write code. I scrape data. I build bots.
Location
Toronto
Joined
Jul 4, 2020
•
Jul 9 '20
Dropdown menu
Copy link
Hide
There's always going to be something new, and most of it will fade into day old history.
But sometimes there's going to be that one thing just blows everything out of the water. It just so happens that most of the big things tend to be dominated by the big players who integrate it into their platforms for all their users.
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
Yes, and those big players have reason and justfication for creating and using them. I'm a big fan of what Blazor brings to the table, would I consider using it for a full fledged system right now. Debatable. Will it grow into something incredible. No doubt. Just take TypeScript as an example. Released in 2012, how easy to use do you think it was back then? How much tooling was built around it? How much has it progressed into what is a incredible product right now.
Like comment:
Like comment:
4
likes
Like
Comment button
Reply
Collapse
Expand
Rob Waller
Rob Waller
Rob Waller
Follow
I am a developer with a passion for testing. I've been coding for 14 years and I want to share my experience and learnings with other developers to help them write better software.
Location
Aylesbury, UK
Education
History Degree
Work
Solution Architect
Joined
May 14, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
""Sure, some big businesses can afford to live the dream, but those are far and few between.""
This is not a well understood point in our industry. I often listen to / read devs who work for one of the wealthy big players and think we live in entirely different universes. 😂
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 9 '20
Dropdown menu
Copy link
Hide
100% The notion that you can spend 6 months working on a POC with the latest tech and find out it isn't for you, scenario isn't too prevalent for most! 🤳
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Randal Vance Cunanan
Randal Vance Cunanan
Randal Vance Cunanan
Follow
Joined
Mar 20, 2019
•
Jul 11 '20
• Edited on
Jul 11
• Edited
Dropdown menu
Copy link
Hide
Being an early adaptor of a technology is synonymous to buying stocks. If the technology or tool becomes popular, you are one of the very first experts of it. If it doesn't, then some of your time is wasted (although not totally as you can still apply some concepts you have learned in other tools).
For me, I don't use shiny tools for my personal and work projects, but wait for them to be stable first, probably even when the second major version is released.
I still try out the shiny tools just to play around with it, but not to create a real project. I keep my eye on it and stay updated until I can finally decide it's ready to be used in real projects.
Like comment:
Like comment:
5
likes
Like
Comment button
Reply
Collapse
Expand
Chris Bertrand
Chris Bertrand
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
•
Jul 11 '20
Dropdown menu
Copy link
Hide
Totally. Try it, have a play, see what is offered and make your own decision. That's what I'm advocating. It's the buzz around certain technologies that can blindside people. You know... Hype. 🙈
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Randal Vance Cunanan
Randal Vance Cunanan
Randal Vance Cunanan
Follow
Joined
Mar 20, 2019
•
Jul 11 '20
Dropdown menu
Copy link
Hide
Using vanilla JS is starting to become the new hype these days. You have something to brag about if you used no framework to build your blog. It's actually the reverse now, in which I would advice using a library instead of using vanilla JS just to show off.
Like comment:
Like comment:
4
likes
Like
Comment button
Reply
View full discussion (35 comments)
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Code Smarter, Not Harder: Tips You Wish You Knew Yesterday
0x3d Site -
Dec 8
.𝗡𝗘𝗧 𝟵 𝗟𝗜𝗡𝗤 𝗠𝗮𝗴𝗶𝗰 : 𝗚𝗮𝗺𝗲-𝗖𝗵𝗮𝗻𝗴𝗶𝗻𝗴 𝗠𝗲𝘁𝗵𝗼𝗱𝘀 𝗬𝗼𝘂 𝗡𝗲𝗲𝗱 𝗧𝗼 𝗞𝗻𝗼𝘄!
Apurv Upadhyay -
Dec 9
Top 8 Free Database Design Tools for 2025
Roxana Maria Haidiner -
Dec 9
Supabase Queues
Yuri -
Dec 5
Chris Bertrand
Follow
Full Stack .Net | Angular | Salesforce Developer in Reading, UK
Location
Reading
Education
Gotta degree! :D
Work
Senior Dev
Joined
Aug 22, 2017
More from
Chris Bertrand
The Best Way to Improve & Automate your Code Quality!
#
productivity
#
learning
#
webdev
#
beginners
3 Reasons to use a Clipboard Manager
#
productivity
#
webdev
#
software
#
beginners
Are you using Void correctly?
#
webdev
#
rant
#
beginners
#
productivity
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Learn SQL in 7 days - (SDET) - DEV Community,"Learn SQL in 7 days - (SDET) - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Suresh Ayyanna
Posted on
Jun 11
Learn SQL in 7 days - (SDET)
#
sdet
#
sql
#
database
#
important
Here's a condensed 7-day plan that covers all the essential SQL topics for QA, along with top 25 interview questions for an SDET (Software Development Engineer in Test) lead role.
7-Day Learning Plan for SQL for QA
Day 1. Introduction to SQL, Basic SQL Queries, Aggregate Functions
Day 2. SQL Joins, Advanced SQL Functions, Subqueries and Nested Queries
Day 3. Data Manipulation (INSERT, UPDATE, DELETE), Data Definition Language (DDL)
Day 4. Indexes and Performance Tuning, SQL Views and Stored Procedures
Day 5. Transactions and Concurrency Control, Error Handling and Exception Handling in SQL
Day 6. SQL Security and Permissions, Practical Case Study 1, Practical Case Study 2
Day 7. Review and Practice, Mock Interviews, Advanced Topics (NoSQL vs SQL, JSON, XML), Data Warehousing, ETL, Big Data
Detailed Explanation
Day 1: Introduction to SQL, Basic SQL Queries, Aggregate Functions
Topics:
Basic concepts of databases and SQL syntax
Installation of SQL tools (e.g., MySQL, PostgreSQL)
SELECT, FROM, WHERE
Aggregate functions: COUNT, SUM, AVG, MIN, MAX
Activities: Install SQL software, write basic queries, and practice aggregation.
Day 2: SQL Joins, Advanced SQL Functions, Subqueries and Nested Queries
Topics:
INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL JOIN
GROUP BY, HAVING, ORDER BY
Subqueries and nested queries
Activities: Write complex queries combining data from multiple tables, practice grouping, sorting, and filtering data.
Day 3: Data Manipulation (INSERT, UPDATE, DELETE), Data Definition Language (DDL)
Topics:
INSERT, UPDATE, DELETE statements
DDL commands: CREATE, ALTER, DROP
Activities: Modify database records, create and alter database structures.
Day 4: Indexes and Performance Tuning, SQL Views and Stored Procedures
Topics:
Creating indexes, performance tuning techniques
Creating and using views, writing stored procedures
Activities: Optimize queries, develop reusable SQL code with views and stored procedures.
Day 5: Transactions and Concurrency Control, Error Handling and Exception Handling in SQL
Topics:
ACID properties, handling transactions, managing concurrency
Error handling mechanisms, TRY...CATCH blocks
Activities: Implement transactions, handle concurrency issues, and error handling in SQL scripts.
Day 6: SQL Security and Permissions, Practical Case Study 1, Practical Case Study 2
Topics:
Managing user permissions, ensuring database security
Applying learned concepts to real-world projects
Activities: Grant and revoke permissions, and develop projects involving database design and query development.
Day 7: Review and Practice, Mock Interviews, Advanced Topics (NoSQL vs SQL, JSON, XML), Data Warehousing, ETL, Big Data
Topics: Review key concepts, mock interviews
NoSQL vs SQL, handling JSON and XML in SQL
Data warehousing concepts, ETL processes, SQL for big data
Activities: Comprehensive revision, practice interview questions, explore advanced topics, and big data handling.
Top 25 Interview Questions for SDET Lead Role
Explain the role of an SDET in an Agile team.
How do you ensure the quality of test automation scripts?
Describe your experience with continuous integration and continuous deployment (CI/CD) pipelines.
What strategies do you use to handle flaky tests?
Explain how you prioritize test cases in a limited time frame.
Describe a challenging bug you found and how you resolved it.
What is the importance of code reviews in test automation?
How do you measure the success of your test automation?
Explain the concept of test-driven development (TDD) and how you've applied it.
What tools and frameworks have you used for test automation?
How do you approach performance testing?
Describe your experience with API testing and tools used.
What is the difference between functional and non-functional testing?
How do you manage and maintain test data?
What is your experience with version control systems like Git?
How do you ensure security in your test automation scripts?
Describe a time when you had to mentor a junior SDET.
How do you handle changes in requirements during the testing phase?
Explain your approach to testing microservices.
What is your experience with mobile testing?
How do you integrate testing with DevOps practices?
Describe your approach to testing in a cloud environment.
What are the key components of a good test plan?
How do you stay updated with the latest testing tools and technologies?
Explain how you handle test environment setup and management.
This 7-day plan and the interview questions should help you gain a comprehensive understanding of SQL for QA and prepare for a lead role in SDET.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Some comments may only be visible to logged-in visitors.
Sign in
to view all comments.
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Optimizing Pagination in PostgreSQL: OFFSET/LIMIT vs. Keyset
Nishant -
Oct 20
Benchmarking Crunchy Data for latency
T Sudhish Nair -
Oct 15
When and Why to Use JSON Columns in SQL Databases
Sospeter Mong'are -
Nov 18
Simplifying SQL View Management in Laravel Migrations
Nasrul Hazim Bin Mohamad -
Oct 16
Suresh Ayyanna
Follow
Suresh Ayyanna | 6+ Years Exp. | SDET | ISTQB certified |Manual Testing | Automation Testing |Java | POM | Cucumber |  Python | robotFrameork | HTML | Bengaluru || Life coach
Location
Bangalore , India
Work
Senior Test Automation Engineer at HID Global
Joined
Dec 31, 2021
More from
Suresh Ayyanna
SQL Basics (Zero to Hero)- Part 01
#
sql
#
testing
#
database
#
programming
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
re:Invent 2020 Part II - Data Sessions Reviewed - DEV Community,"re:Invent 2020 Part II - Data Sessions Reviewed - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Tom Milner
Posted on
Jan 15, 2021
re:Invent 2020 Part II - Data Sessions Reviewed
#
reinvent2020
#
aws
#
dataanalytics
#
redshift
Just in case you thought it was all over, re:Invent 2020 continued the second part of the show this week, albeit in 2021. It's been that kind of year for most of us.
re:Invent Part II was 3 days of feature packed talks, finishing up on Friday 15th. I am a data engineering manager with a keen interest in serverless. This article summarizes the talks that I watched with regards to data services on AWS. My team and I are heavy users of Redshift so we were very happy with the amount of Redshift talks and references in re:Invent.
Design best practices for architecting a data-to-insights solution on AWS
https://virtual.awsevents.com/media/0_ptoikufw
Adnan Hasan does a very good job in this level 200 talk in showcasing the updated AWS data ecosystem and how different architectures can be put together to create scalable systems. If you think you've seen it all before, you haven't. This is an updated view of their products with new features leveraged to augment previous patterns.
Adnan introduces and expands on the principal of ""in-place analytics with minimal data movement"" presenting 4 designs that could be applied to meet this principal. Interesting for me was to see Redshift being used as a data virtualization tool, allowing users to read data from both S3 and operational databases.
He then continues with best practices for an analytics and BI pipeline and how personas can be used to break up your system into distinct parts within corresponding security contexts.
Adnan closes with a few pointers on cost optimization. In summary it was a very interesting talk and a good place to start before progressing into the lower level talks below.
The lake house approach to data warehousing with Amazon Redshift
https://virtual.awsevents.com/media/0_8pa51n7s
Vinay Shukla dives deeper in this level 300 talk on where Redshift fits in the lake house paradigm that has become fashionable lately.
He expands on the subject of personas used in Adnan's talk, suggesting that you should use them to drive your choice of analytics engine. This is a topic close to my heart and one that the cloud pay-as-you-go model makes a reality. Vinay explores 4 common data use cases and suggests patterns that could be applied to each. He also covers security and shows how different access levels can be applied in practice to separate personas using AWS Lake Formation.
Vinay closes with a slide each on how recently released Redshift features can be used to support a Lakehouse architecture. These are Redshift Spectrum and Lake Formation integration, data lake export, bloom filters, cost controls for concurrency scaling and Redshift Spectrum, support for reading Apache Hudi and Delta Lake tables, federated query, materialized views on external tables and Redshift ML.
Getting the most out of Amazon Redshift automation
https://virtual.awsevents.com/media/0_0ps5qt6d
Paul Lappas gives a level 200 talk on the DW automation features that AWS has released for Redshift. The talk focuses on background automation features that can improve the performance of your cluster while reducing the operational overhead on your IT team. Paul shows how numerous features can help automate performance tuning.
The talk builds around the new Automatic Table Optimization (ATO) features of Automatic distribution and sort keys. Redshift now has the ability to detect the most suitable columns to use as partition and sort keys for your table. By enabling this feature, you are handing this decision over to AWS and out of the hands and minds of your database developers. Redshift profiles the queries that are run against the tables to choose the best columns. All decisions are logged within a system table so that an audit trail can be monitored. Combining these two features with the existing features of automatic vacuum and table sort provides an extremely powerful data-driven solution to what can be a time-consuming decision for data developers.
Paul also spends time on how Redshift Advisor can be used to improve data ingestion, query tuning, table design and cost. A common reference architecture is shared and he shows how automation can make operational analytics and BI reporting easier and faster within the context of this architecture.
Paul finishes with a hands on demo of the new ATO features and how they can improve query performance automatically for your end customers. An important point was made that this feature will primarily benefit customers who run adhoc queries on the cluster. ETL workloads are more well defined whereas customer queries can consist of multiple flexible access patterns. By allowing Redshift to automatically decide the distribution and sort keys of your tables, your most common patterns can be better served with this data-driven approach.
As a long time Redshift user, I can vouch for the improvements that AWS has released over the years to reduce the operational overhead on data teams. For example, we had built our own jobs to analyse and vacuum tables in the past. With the automation of these features from AWS, we can now deprecate these jobs and hand this operation over fully to AWS. This reduces the amount of code and support we need to carry in the team.
Deep dive on best practices for Amazon Redshift
https://virtual.awsevents.com/media/0_2njtyt5g
Harshida Patel gives a deep dive level 400 talk on best practices for Redshift. She sets the foundation for her talk by reviewing the fundamental architecture of Redshift, discussing how to connect to leader node and how the leader node coordinates activities across the compute nodes. Harshida then shows how Spectrum works and is aligned to the compute nodes of each cluster. From there we see how the architecture of the RA3 nodes differs from the traditional node types and then how AQUA will accelerate read performance of RA3 nodes even further.
With the baseline set, Harshida gives an overview of the new features including data-sharing, Redshift advisor and Automatic table optimisation.
One thing I noticed here is the upgrade in the amount of storage now available per RA3.4xl and RA3.16xl nodes is now 128TB. This used to be 64TB until very recently. This is an interesting upgrade.
Harshida's recommendations are then grouped into the 4 fundamental areas of Table design, Data ingestion, Workload management and Cluster resize. She dives into the concepts and terminology relevant to each area with the best practices then summarized onto a single slide for each.
This is a great talk, bringing a consolidated focus on how to boost the performance of your cluster. Harshida gives clear, actionable recommendations that you can make to your cluster. I would definitely recommend this talk if you are operating a Redshift cluster in production.
How to build a test framework for your data lake
https://virtual.awsevents.com/media/0_5hopvbgs
Marie Yap and Hemant Borole team up for this level 300 talk on the why and how of building a framework to test your data lake. Marie goes through the different steps in data testing, namely schema validation and data quality and gives approaches for both. She then continues looking at when is the best point in your data pipeline to execute functional and performance tests versus load and verification tests.
Marie finished her talk with a look at rightsizing your Glue jobs for executing tests and how this will affect the performance and cost of running such tests.
Hemant then walks through a very detailed implementation of test pipelines for EMR, Redshift and Glue. The actual tests are stored in DynamoDB and the framework utilizes a number of AWS Step Functions state machines to coordinate the tests. Tests are executed using the open source Deequ framework by AWS Glue.
Hemant then provides a link to a
GitHub repo
where the Step Functions ASL code is available for adaption and utilization in your own systems. It looks like a great resource to get started.
I loved this talk as it's something my team is currently investigating. The level of detail that Marie and Hermant go to is great if you are working with this problem. If you are interested in finding out more about utilising Deequ in AWS for data testing, check out this article on the AWS blog
https://aws.amazon.com/blogs/big-data/building-a-serverless-data-quality-and-analysis-framework-with-deequ-and-aws-glue/
.
Understanding state and application workflows with AWS Step Functions
https://virtual.awsevents.com/media/0_v56t8ffd
Not strictly data related but if you're interested in Steps Functions for orchestrating your data processes, this level 400 talk could be of interest. Rob Sutter does a great job summarizing improvements to the Amazon States Language (ASL) that is used to generate individual Step Functions state machines. He also shows what happens in each state change of a single step, how to handle errors with retries and catchers and closes with how to run steps in parallel up to a set concurrency.
re:Invent Part I
If you're looking for a summary of re:Invent Part I, Helen Anderson does a great job of it here
https://dev.to/aws-heroes/the-aws-re-invent-sessions-i-m-looking-forward-to-387m
.
And if you want to learn more on Redshift, these two talks from December are great place to start exploring other new Redshift features such as the snowflake melting data-sharing, multi-az (finally) and others.
New use cases for Amazon Redshift
https://virtual.awsevents.com/media/1_2gnlgxcz
What’s new with Amazon Redshift
https://virtual.awsevents.com/media/1_rgx2ul5q
Conclusion
There is a wealth of resources available on the
re:Invent
site that will be immensely useful to anyone developing on AWS. I find the 30 minute format just right for me and having easy access to the slides is also great. If you're looking for more granular control of playback speed, check out
Jeremy Daly's chrome extension
. It's very useful when you need something between 1x and 2x.
The data sessions linked in this article provide a great starting place with Redshift and also an update on recently released features. Personally, I am very excited to see the level of investment in the service. The upgrade to RA3 has made possible some fundamental changes to the legacy Redshift architecture and I hope we see more as the 2021 progresses.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
A Step by Step Guide on How to Migrate from AWS to Azure
Vikas Singh -
Nov 18
Move objects from one folder to other in the same S3 Bucket using C# in AWS
DotNet Full Stack Dev -
Nov 17
C4 Model en la Nube: Implementación Práctica con AWS y Terraform
Antonio Jesús Castillo Cotán -
Nov 17
Amazon SQS: The Backbone of Asynchronous Communication
Lindiwe -
Dec 9
Tom Milner
Follow
Data Engineering Leader with >20 years experience engineering data solutions. AWS Community Builder with 6 current AWS certifications. Snowflake Data Superhero
Location
Dublin
Work
Director of Engineering at Tenable
Joined
Jan 8, 2020
More from
Tom Milner
Passing the AWS Certified DevOps Engineer - Professional exam
#
aws
#
devops
#
certification
Which AWS Certification exam should I sit?
#
aws
#
certification
Amazon Neptune Serverless - The Graph DB for Greek Gods
#
aws
#
database
#
graphdb
#
neptune
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Death of the Coding Test: Interview Methods that Better Evaluate Candidate Competency - DEV Community,"Death of the Coding Test: Interview Methods that Better Evaluate Candidate Competency - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Bradston Henry
for
Developers @ Asurion
Posted on
Sep 21, 2022
Death of the Coding Test: Interview Methods that Better Evaluate Candidate Competency
#
career
#
programming
#
discuss
#
interview
Interviewing in Tech (2 Part Series)
1
Why I Stopped Interviewing with Companies That Require a Coding Test
2
Death of the Coding Test: Interview Methods that Better Evaluate Candidate Competency
A little over 7 months ago, I wrote a blog that made a much larger impact in the developer community than I thought ever possible...
Why I Stopped Interviewing with Companies That Require a Coding Test
Bradston Henry ・ Feb 2 '22
#programming
#career
#discuss
#interview
The feedback to my blog was
way bigger
than I could have ever imagined and the coolest thing about it all was that it elicited a varying degree of responses:
Strong Agreement
Strong Disagreement
Hilarious Memes
Vigorous Debate in the comments
Insults directed at my writing ability 😅
And amongst all of the different responses and comments, one particular thought and question really stuck with me...
If we don't do Coding Tests, What interview approach should replace them in the developer interview process?
In this blog, I hope to answer that question. I'll share some interview approaches that I believe are better (or at least as good) at evaluating the competency of developers, engineers, and programmers for roles at your company or on your teams.
Note: I would love to hear other's thoughts on this as well, so please share your thoughts in the comments below!
Let's Set the Record Straight
Before I share my list of best interviewing approaches, I would like to make sure I convey my opinion on coding tests as succinctly as possible.
I believe that coding tests, as they are currently implemented in the tech industry, are not the best method for evaluating candidate competency or future ""on-the-job"" performance.
Coding tests take on several forms but essentially challenge the test taker to answer a question or a set of questions that are used to evaluate the takers understanding, knowledge, or competency on core Computer Science or programming fundamentals. Tests can consist of algorithm based challenges, tricky coding problems, and sometimes even questions to assess a test takers knowledge on Computer Science principles and/or programming design patterns.
Coding tests are somewhat standardized across the tech industry but I do not believe that they are a great gauge for evaluating the diverse sets of skills that candidates posses and do not consider the wide range of backgrounds that candidates bring to the interview process.
There are some uses for coding tests in the industry and I don't believe they should be
completely
thrown away but I do believe they are only truly useful for a smaller portion of the development population and should not be considered the industry standard.
What Do We Do Without Coding Tests?
Something Better!
One nice thing about me being in the tech industry for as long as I have been, is that I've been in
A LOT
of interviews. Interviews to find new jobs, interviews to join new projects and interviews needed for promotions.
That has given me a broad amount of experience as interviewee, as well as interviewer. And in my current role as Developer Relations Manager @
Asurion
, I have the opportunity to talk with developers and engineers on a very regular basis and have learned a lot about how to best understand and evaluate the varied and broad skills of any particular programmer.
So here is my list of better alternatives to coding tests:
1. Take Home Task or Project Assignment
This approach gives the candidate a programming or coding task that could be considered a ""take-home assignment"". The interviewee would be tasked with solving a programming problem or project challenge that resembles something similar to what they would do in their day-to day in their respective role. In most cases, the candidate is given a timeframe in which to finish the assignment and is expected to return their solution to the interviewee once completed.
I have personally been evaluated in the interview process using this method and feel that it was a great way to measure my skills. In one case, I passed the project assignment with flying colors resulting in a job offer and in another case, I performed decently but not as well as other candidates in the process and was not extended an offer.
In both cases, I felt great about the results. Because the interviewer/company did a great job setting up the parameters and task to reflect what would be needed for the job, I felt it was a great reflection of my actual skill at accomplishing the given task.
I think this a great method for roles that might need to evaluate a candidate on a host of different skillsets; Coding skill, problem solving, critical thinking and time management.
This may not be the best approach to use if you are on a restrictive timeline for finding a candidate for a role. The reason being is that since you are asking people to complete a take-home assignment, you will need to give them the ability to find time in their schedule to do the assignment. For some, they may be able to complete the assignment quickly but for others, who may have busy schedules or a lot of home responsibilities, it may take a bit more time to fit completing the assignment into their schedule.
So here are a few more pros and cons for using this approach:
Pros
Reduces Candidate Performance Anxiety
: Candidates are given the ability to select the best timing and environment to accomplish the task. For those similar to myself, this can help them to perform closer to their actual competency level while reducing anxiety that would heavily affect their interview performance.
Customizable Evaluation Approach
: Interviewers can create a more bespoke experience for the assignment depending on the candidates skill level, position requirements or time constraints. Interviewers can add or remove objectives as needed to accommodate interviewee.
Two-Stage Evaluation
: This works great for evaluating a candidate on coding and programming competency and evaluating their communication and actual understanding of their code. If after receiving a candidate's returned assignment, if you believe they performed well enough to continue in the interview process, you can now use the take-home assignment in a future step of the interviewing process. You can now ask them questions directly about their code and how and why they made their decisions to better understand their critical thinking and communication style and skills.
Cons
At Home Tech Requirement
: One thing to be aware of is that not every candidate has the same tools to work with at home. Some may have the skills to perform on the job but not the hardware to perform the take-home assignment reasonably. This needs to be a consideration and should not remove a candidate from the interview process if they lack the personal tools or hardware to accomplish the task at home.
Candidate Time Constraints
: As I mentioned a bit earlier, not all candidates have equal time capacity in the interview process. Be aware that this method may not be great for candidates who only have a limited amount of time to participate in the interview process. Consider using a different approach if a candidate expresses that they may struggle finishing or setting aside the time for the take-home assignment.
2. Pair Programming Session
Another great way to evaluate a candidates ability and to see how they may perform in the working environment is to conduct a pair programming session in the interview process.
The pair programming session approach tasks the interviewee at solving a problem or tackling some task with the guidance and assistance of an interviewee. This approach can be done in-person or virtually and can easily be adapted to best match the candidates needs.
I have experienced pair programming-esque techniques being used all throughout different interview processes but I want to make sure I clarify something about how this should be implemented.
This is actual pair-programming NOT a proctored coding test.
I have participated in interviews where it was explained to me that there would be a pair programming session only to find that I was given a task and then was simply watched and questioned about my coding decisions as I accomplished the task.
That is not how pair programming works and not how pair programming should be implemented into the interview process. Pair programming at it's heart is a collaborative effort where two developers try to
work together
to complete some task.
In the interview process, this is exactly the same. The interviewer and interviewee work together to solve a task while learning from each other. In the interview process, this gives the interviewer a chance to really see how a candidate would work in pair programming environment.
A candidate should not feel as if they are trying to get the problem given to them ""right"" or ""wrong"" but should be encouraged to operate as realistically as possible. This can be difficult in the interview process but can be extremely effective on evaluating a candidate.
A huge key to making this approach effective is to make sure to pair program on a problem or task that reflects the actual job. It can be a wasted opportunity if not working on a task that reflects real on the job responsibilities.
So here are a few more pros and cons for using this pair programming approach:
Pros
Code in Realtime
: This gives the candidate an opportunity to code directly in front of the interviewer to get their skills evaluated. It can be sometimes hard to get an actual gauge of a candidates' coding proficiency without actually seeing them code.
Low Pressure but Some Pressure
: This a good opportunity to see how a candidate performs under some form of pressure in the interviewing process. As natural as you attempt to make the pair programming process, it will still feel like some what of a test to the interviewee. This can give you an understanding of how a candidate performs under pressure. Though I am an individual who can suffer from performance anxiety, I do think it is appropriate to push candidates especially when they are likely to encounter pressure situations on the job.
Cons
Finding the Right Interviewer
: The most difficult  aspect of this approach is finding the right person to conduct the pair programming. The interviewer should be familiar with the pair programming process but, more importantly, should have the temperament to conduct this portion of the interview. They should have the ability to not only evaluate the candidate's skill but also the ability to work with and support the candidate. To get the best results, the candidate should feel as they are allies with the interviewer and tackling the task alongside them.
3. The ""How Would You Solve..."" Prompt
In this approach, the interviewer asks the candidate to try and solve a real or hypothetical problem that they would likely encounter on the team they would be joining. The interviewer gives the interviewee a prompt with the basic details of a problem that the team is trying to solve and asks the interviewee to walk them through how they would solve it.
It's extremely important to express at the very beginning of this process that the candidate can ask any questions to get more details on the problem. This makes a big difference because it lets the candidate know it's okay to communicate with the interviewer to get clarity.
One great thing you get by using this process is the ability to evaluate how the interviewee thinks though problems and how they come to conclusions. If the interviewee asks very few clarifying questions or if they ask a lot of clarifying questions, you can get perspective of their experience with dealing with technical, development, and general problems on the job.
Depending on the role that the individual is interviewing for, you can get an idea of if they have the critical thinking and experience needed for the role (and even if they may have more experience and knowledge than the role requires).
Similar to other interviewing approaches, I do not believe interviewers should be looking for the correct solution to the problem to be presented but how the interviewee got to the solution. We can sometimes detract from noticing the benefits of how a person came to their conclusion if we are too focused on them getting the ""correct"" solution. And as in most things in life, there is rarely a right solution to any problem but solutions with different advantages and disadvantages.
Personally, this is one of my favorite approaches in evaluating a candidate. Sometimes there are individuals who can perform really well in live coding and coding test scenarios but lack insight on how to get to a solution organically. They are very technical and studied individuals who perform well under pressure but can lack critical thinking skills that really matter on the job or in their role.
So let's look at a few more pros and cons for the prompt approach:
Pros
See Their Thinking
: As I mentioned before, this method is a great way to get into the mind of the interviewee. Instead of looking at the results of a test or even some static code where you might need to interpret intention or how they might have came to some conclusion, you get to see how a particular candidate thinks. In the long run, you want a great critical thinker on your team, not just a good coder.
Gauge Future Team Impact
: One benefit of getting a chance to hear how someone solves a problem is that you can sometimes identify how someone will impact your team. Something they may say or share may indicate that they are able to bring something new or needed to your team that would have never been identifiable through code.
Cons
Be Wary of ""Good"" Talkers
: There are particular individuals who excel at verbal communication and can even at times deceive the most adept of interviewers with clever words. It's important to follow-up and ask deeper questions to determine if someone knows what they are actually talking about or just knows how to speak in a way that seems to indicate understanding.
Best When Used with Other Approaches
: This could likely be said for any interview approach, but this rings even more true for this method. In an interview process it is always a good idea to get multiple performance indicators to help you better understand a candidate's possible future performance. This approach can help identify strong critical thinking skills but is not always the best way to evaluate more technicals skills like coding ability and understanding.
4. Personal Project Code Review
Using this approach, an interviewer asks to review a candidate's personal projects, open-source code or any other technical work that they have done outside of being employed in a traditional sense.
This allows candidates to self-select the technical measure on which they will be evaluated and gives an interviewer an idea of what the candidate believes is representative of their skillset. The assumption is that whatever work or code a candidate shares with the interviewer is likely work or code they are particularly proud of and something they would stand by.
When used, this approach should be implemented as a supplemental tool or an optional approach in the interview process. The truth is that not every great developer or programmer codes in their free time or has interest in working on personal projects outside of their normal daily work on the job.
Just because a candidate does not have a personal project or code to share does not mean they are any less of a programmer than an individual who does.
Another thing to note is that it is crucial to not only evaluate their code without them present and then make a determination about their skills. It is important to review their code and then discuss with them the how and why behind their code.
There may be instances where an individual makes, what seems to you, a terrible coding decision but there may be a very specific reason that actually is very clever and not intuitive at first glance. Ask questions about the code, the thinking around the code, and find ways to evaluate their skills in their entirety.
Here are a few more pros and cons for the personal code review approach:
Pros
Can be used as Substitute to Other Technical Evaluations
: If a candidate has a substantial amount of personal project code that is available, there are some circumstances where you could use this approach in lieu of another (E.G. Personal Project Evaluation as substitute for Take-Home Assignment). I would recommend making this approach optional in early stages of the interview process as it may allow you to ""skip"" other parts of the interview process, thus shortening the overall interview experience.
See a Different Coding Style
: From my experience, people tend to code differently in their personal projects than they do on their job. This gives an interviewer some insight into the candidate's personal coding style and competency when the only person who will truly evaluate their code is the candidate themselves. It also gives you a glimpse into their general documentation style as they will likely only document what they deem to be important to document.
Cons
Might not be their Code
: One thing to be aware of is that when viewing a candidate's personal project is that not all the code may be their code. In our development culture with the existence of StackOverflows and Dev.to-like sites, there is a chance that someone copy-pasted their project to ""look good"". It is extremely important to ask the interviewee what made them make their decisions and to hear their perspective and thinking around the highest quality and lowest quality sections of their code.
Project Code can be Dated
: If code in a project was created long enough in the past, it can present two problems; the code reflects their skill when they were at a lower level OR the code reflects their skill at a level they are not currently at anymore. It is important to have context on when the code was written to understand if the code represents their current skill level or a skill level benchmark of the past.
5. The ""Take a Ticket"" Approach
In a similar fashion of the ""Take-Home Assignment"" approach, this method involves giving the candidate a problem to solve or a feature to implement. How this differs is that this would happen during the actual interview, may it be virtual or in-person, and the candidate is given a set amount of time to work on the problem.
To make this approach the most effective, it is a good idea to have the interviewee to solve a problem or implement a feature that is ""real"" or directly reflects what would be on the job. It's also important to give the candidate access to the same or similar resources they would have on the job and not to ""test"" them on knowledge that wouldn't matter in a real on-the-job scenario.
To make the ""ticket"" they would have to solve as real as possible, I would encourage either giving them access to real code (that is not proprietary, running live or would not effect your business advantage in any way) or to make a ""dummy"" repository with older or slightly modified real code that they could work with. Give them a specific ""ticket"" that would likely need to be resolved in the real-world and let them code away.
If the interview was happening in person, you could even place them at a desk in your office to help simulate their future work environment as closely as possible.
In this approach, I would recommend going for a more proctored approach where you let the candidate code but do not directly interact with them unless they ask a question relevant to the ticket. You could also, let them work on their own without an interviewer present but you might miss the opportunity to answer any crucial questions they may have or view how the candidate codes and works.
All and all, the goal is to mimic what they would
actually
be doing on-the-job as closely as possible and to give the opportunity to operate in the method that best suits them.
Let's now look at the pros and cons for the ""Take a Ticket"" approach:
Pros
Mimics Real Work Experience
: This is a great way to gauge how a candidate might operate in a real-world work environment. If set up correctly and if the parameters are communicated effectively, you might see a side of the interviewee that you may have never seen in a ""traditional"" interview process.
Helps Candidate Evaluate if Work Environment is a Good Fit
: One thing I have not mentioned much in this blog is that it's also important for the candidate to evaluate if the job or role they are interviewing for is a good fit for them. This can be a great opportunity to help the candidate feel out what the job might be like and if it's a good match for them.
Cons
More Effort to Implement
: Though this method can pay dividends in the long-term, it does take some work to implement correctly. You have to set up the work environment, the code base/tickets, need a solid interviewer to proctor and need to set aside the time in the interview process to make it happen. Given the upfront effort it may not be the best fit for everyone.
Be Wary of Performance Anxiety
: Just be aware that some candidates may perform poorly in the proctored format of this approach. It may be useful to think through an alternative implementation if the interviewer notices the candidate is suffering from an episode of performance anxiety. Try your best to take that into consideration.
Conclusion
If there is one thing I'd like you to walk away with in this blog is that there are lots of solid alternatives to the traditional Coding Test that has become standard in our industry.
Just like with any interview method, no method on its own is perfect for every scenario and for every position/role that you may be interviewing candidates for. In my personal opinion, to get a more full understanding of the competency of a candidate, it's best to implement two or more of these methods in the interview process.
I'm not a fan of very long multi-stage interview processes BUT I do believe it is very difficult to evaluate a candidates technical ability or potential through one individual technical screening. And in some cases, using just one approach can create false positive and negative indicators that may not allow you to select the best candidate for the job.
As I mentioned in my
previous blog
, I think using a blanket approach for evaluating candidates, no matter the approach, may be more efficient but shows a lack of care for candidates and increases the likelihood of hiring the wrong person for roles you are attempting to fill.
I truly believe the effort you put into creating an interview process and the consideration you put into approaches you use to evaluate candidates, will determine not only the quality of candidates you hire but will have direct impact on their sentiment of your company and teams as they go through the interview experience.
Coding tests may be the standard but there are better alternatives that we can consider for interviewing and I encourage anyone who is in the position to impact the process to take some time to consider some of the alternatives I have shared above.
Are there any other alternative interviewing methods to coding tests that you have found to be effective?
Do you think any of the approaches I've shared are not as effective as they seem?
I Would love to hear your thoughts!
Thanks for Reading!
Bradston Henry
Photo Credits (as they appear):
Cover Photo by
Pixabay
Photo by
Tim Gouw
Photo by
Any Lane
Photo by
Christina Morillo
Photo by
ThisIsEngineering
Photo by
Pavel Danilyuk
Photo by
ThisIsEngineering
Photo by
Tima Miroshnichenko
Follow me on my Socials:
https://linktr.ee/bradstondev
Interviewing in Tech (2 Part Series)
1
Why I Stopped Interviewing with Companies That Require a Coding Test
2
Death of the Coding Test: Interview Methods that Better Evaluate Candidate Competency
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Robert Packer
Robert Packer
Robert Packer
Follow
Joined
May 29, 2024
•
May 29
Dropdown menu
Copy link
Hide
Having just come out of a ""share your screen and do these logic problems"" interview, (and now apparently looking for some confirmation bias), I'm glad there's another dev out there that feels the same way I do. The take home project is my favorite of mine. Even those timed logic problem tests that you do on your own schedule are better. But, now with AI coding tools they're a thing of the past (which I also got complaints about, so had to use a text editor.)
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
15 System design tradeoffs for Software Developer Interviews
Soma -
Dec 10
3243. Shortest Distance After Road Addition Queries I
MD ARIFUL HAQUE -
Nov 27
Why Span<T> and Ref Structs Can't Directly Participate in Asynchronous Operations
Hootan Hemmati -
Nov 26
New AI Breakthrough Makes Self-Driving Cars 15x Faster and Safer with Truncated Diffusion Model
Mike Young -
Dec 1
Developers @ Asurion
Follow
Where Talent and Inspiration Collide
More from
Developers @ Asurion
Don't Dead End Your Dev Career: Using Communication to Amplify Your Possibilities
#
developer
#
career
#
careerdevelopment
#
communication
Why You Should Learn Kubernetes
#
kubernetes
#
devops
#
webdev
#
programming
Two Lines of Code Eluded me for Several Months
#
scala
#
async
#
threading
#
programming
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
23 Software Testing Trends To Look Out For In 2023 - DEV Community,"23 Software Testing Trends To Look Out For In 2023 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
YashBansal651
Posted on
Apr 27, 2023
• Originally published at
lambdatest.com
23 Software Testing Trends To Look Out For In 2023
#
automationtesting
#
cloudtesting
#
trends
#
softwaretesting
As software continues to play a critical role in businesses and daily life, testing will keep evolving to meet the demands of the modern digital landscape. With that, organizations strive to deliver their software faster and more frequently, and the testing process needs to become more integrated into the development process.
In 2023, we can see various
software testing
trends emerging, each with the potential to significantly impact how organizations approach testing and ensure software quality.
This article discusses the top 23 software testing trends for 2023 and their impact and scope on organizations. Without ado, let’s dive deep into it.
1. Accelerating Time-to-Market with Shift Left Testing
As software becomes more complex and the need for speed increases, software testing trends like
shift left testing
, a software testing approach that prioritizes testing earlier in the software development process. Instead of waiting until the end of development to begin testing, shift left testing emphasizes starting testing as soon as the first code is written.
It’s not just about changing the timing of testing activities but also about how testing is done. To accelerate time-to-market, cloud testing platforms like LambdaTest help perform shift left testing that enables developers to test their web applications on a range of real devices and browsers, identify and resolve bugs early in the development cycle, and deliver high-quality software at a faster pace.
If you’re looking to adopt Agile or DevOps practices, our video on shift-left testing is a must-watch. It will help you understand the benefits of shifting testing left and how to do it effectively.
You can also Subscribe to the
LambdaTest YouTube Channel
and stay updated with the latest tutorials around
Selenium testing
,
Playwright
, and more.
Need a great solution for testing
Safari browser for Windows
? Forget about emulators or simulators — use real online browsers. Try LambdaTest for free.
2. Revolutionizing Software Testing with AI and ML
As testing becomes more automated, software testing trends incorporating AI & ML technologies enhance testing processes, such as utilizing AI algorithms to generate test cases and ML techniques to identify patterns and anomalies in test data.
By leveraging AI and ML algorithms, organizations can significantly increase the speed and accuracy of their testing activities and deliver software with higher quality and reliability. As organizations look to increase the speed, accuracy, and efficiency of their testing activities, the application of
AI and ML in automation testing
will expand and continue to keep evolving, bringing software testing new and better algorithms.
3. Empowering Teams to Deliver Secure Software Faster with DevSecOps
Software testing trends are shifting towards a more secure approach with DevSecOps, a modern software development methodology that integrates security into the development process, making it a core component of the development lifecycle.
DevSecOps provides a way for organizations to reduce the risk of security vulnerabilities in software by incorporating security into every stage of the development process, from design to deployment. By doing so, organizations can detect and address security issues earlier in the development process when they are less expensive and time-consuming. The scope of DevSecOps will continue to grow as organizations seek to increase the safety of their software and meet the evolving threat landscape.
Need a great solution for cross browser testing on Safari? Forget about emulators or simulators — use real online browsers. Try LambdaTest to test on
safari browser online
.
4. Enabling Faster, Smarter Software Development with Continuous Testing
With the rise of
Agile development
, faster and more reliable
continuous testing
has become an essential aspect of this new paradigm as it involves testing the entire development process, from development to deployment. The main objective is to provide early feedback and meet quality expectations before releasing software to end users. As organizations seek to increase speed and quality in their software delivery processes, the scope of continuous testing will continue to grow.
5. Exploring the Possibilities of IoT Testing for Smart Devices and Systems
With the rise of the Internet of Things, software testing trends are shifting towards IoT testing. The Internet of Things (IoT) is a network of interconnected physical objects like cars, appliances, and other household items with connectivity, electronics, and software that allow them to collect and exchange data.
To ensure these devices work effectively and efficiently, security and
functional testing
are becoming increasingly important as the number of IoT devices increases, leading to high growth in IoT testing. It aims to confirm that IoT systems and devices perform as anticipated and adhere to end users’ expectations.
6. Unleashing the Potential of RPA for Faster, Smarter Testing
As the complexity of RPA solutions continues to grow, software testing trends are moving towards Robotic Process Automation (RPA) testing, a technology that automates routine and repetitive tasks, freeing human workers to focus on higher-value tasks. The goal of RPA automated testing is to ensure that RPA systems function as expected and meet the requirements of the end-users. RPA aims to improve efficiency, reduce errors, and lower costs.
More than ever, there is a need for dependable and effective RPA systems, given the growing use of RPA in crucial systems like finance, healthcare, and customer service. As RPA’s usage and system complexity continue to rise, so will the scope of automated testing for RPA applications. Organizations can ensure the excellence and dependability of their RPA systems by automating the testing process.
7. Securing Your Digital Footprints with Security Testing
To protect users’ data from growing digital threats, software testing trends increasingly focus on incorporating security testing into the
software testing life cycle
. Because cyberattacks are so sophisticated, security lapses can have catastrophic results, resulting in monetary losses, reputational harm, and the loss of sensitive data.
As a result, organizations now place more emphasis on testing the security of their systems, making security testing a crucial component of software development. As the demand for secure software rises and the complexity of software systems rises, the scope of security testing will expand, allowing businesses to guarantee the security of critical systems like finance and healthcare.
Run your
Selenium
Automation Testing scripts on the LambdaTest cloud grid. Test on 3000+ desktop & mobile environments. Try it for free.
8. Impact of Scriptless Automation on the Testing Landscape
Scriptless
automation testing
is becoming an increasingly popular choice for software testing trends as it eliminates the need for scripting and coding in software testing. It aims to simplify and streamline the testing process, making it easier for organizations to perform automated testing without requiring specialized technical skills.
It is accomplished using tools that allow testers to create test cases and test scenarios using graphical user interfaces instead of writing code, thus, reducing the time to create and maintain test scripts. The use of
codeless automation testing
will expand as businesses look for ways to restructure, streamline, and open up the testing process to a broader audience of users.
9. Streamlining Cloud Deployments with Cloud-Native Testing
As software testing trends continue to focus on improving the quality of cloud-based applications, cloud-native testing refers to testing cloud-based applications that are designed, developed, and deployed using cloud-native principles and technologies.
Cloud-native testing guarantees the performance, dependability, and security of cloud-based systems and applications and confirms their suitability for use in contemporary, highly scalable, and dynamic environments. The demand for cloud-based systems and applications is increasing; therefore, cloud-native testing will grow as organizations increasingly adopt cloud-based solutions and technologies.
10. Revolutionizing AI Testing with Artificial Neural Network Testing
Software testing trends are changing to incorporate new testing techniques and strategies for testing AI applications as artificial neural network technology progresses. Testing artificial neural networks (ANN) determines how well a neural network model performs on a particular task or issue. The use of ANNs in numerous applications, including image recognition, medical diagnosis, and autonomous driving, has significantly increased in recent years, where the model’s accuracy and dependability are crucial. The scope of ANN testing is anticipated to grow in the future as deep learning and artificial intelligence progress.
11. Power of Multi-Device Testing for Smarter Test Automation
Software testing trends are putting more emphasis on multi-device testing as the number of connected devices keeps increasing to ensure that applications function properly on all connected devices, including smartphones, tablets, laptops, and smart home devices. Regardless of size, shape, or operating system, multi-device testing ensures that all devices, irrespective of shape and size.
As more people use multiple devices, the scope of multi-device testing will continue to grow in the coming years. LambdaTest allows multi-device testing as
parallel testing
to test multiple devices and browsers simultaneously, reducing the overall time and effort required for testing. Additionally, LambdaTest’s platform integrates with popular testing frameworks like
Selenium
and
Appium
, making it easy for teams to integrate their existing testing processes.
In this
Appium automation
tutorial, learn about Appium and its benefits for mobile automation testing. Take a look at how Appium works and see how to perform Appium testing of your mobile applications.
12. Navigating the Cybersecurity Testing Landscape
As cybersecurity risks continue to increase, software testing trends are shifting toward more frequent and rigorous testing practices. An organization’s overall security posture is evaluated by conducting cybersecurity testing on its computer systems, networks, and applications to spot potential flaws.
Sensitive data and systems are safeguarded from unauthorized access, theft, and damage thanks to cybersecurity testing, which aims to identify and reduce potential security risks. Cybersecurity testing is anticipated to grow in the upcoming years due to the increased use of mobile devices, cloud computing, and the Internet of Things (IoT).
13. The Future of Digital Experience Testing
With the rise and
importance of digital transformation
, testing trends are shifting towards more comprehensive testing of the entire digital experience.
Digital Experience Testing
ensures that organizations’ digital products and services are seamless and offer a good user experience.
In digital experience testing, we can
test browser compatibility
across various platforms and devices in addition to functionality, usability, and performance. Conducting digital experience testing allows businesses to find and fix problems that have a negative impact on the user experience, which ultimately increases client satisfaction, loyalty, and revenue. Additionally, digital experience testing is a great way for businesses to ensure that their digital goods and services meet user expectations and industry standards.
14. Power of User Experience Testing in Driving Business Success
Modern software testing trends recognize that user experience is crucial to create software that not only works well but also delights users. User experience testing is the process of analyzing how end users interact with a given product or service. It aims to recognize and address problems like poor usability, perplexing navigation, or inconsistent design that may have a negative effect on the user experience.
As the value of the customer experience continues to rise, so will the scope of UX testing. In the upcoming years, it is anticipated that mobile devices, e-commerce, and the growing significance of customer satisfaction will all contribute to the growth of UX and
UI testing
.
15. Blockchain Testing for Enhanced Security and Performance
As blockchain technology continues to gain momentum,
blockchain testing
has become a crucial testing trend in software development, and more and more companies are adopting it. This is a crucial step in developing blockchain-based applications that enable businesses to ensure their products are safe and dependable.
Blockchain testing aims to locate and address problems, such as security holes or compatibility issues, that could adversely affect how the blockchain functions. As interest in blockchain technology increases in the upcoming years, so will the scope of blockchain testing.
Through this
usability testing
tutorial, you will learn how usability testing is a great way to discover unexpected bugs, find what is unnecessary or unused before going any further, and have unbiased opinions from an outsider.
16. Impact of Cloud Performance Engineering on Business Continuity
Examining and improving the performance of cloud-based applications and services is the key objective of cloud performance engineering, a crucial component of cloud computing among various software testing trends. Cloud provides unparalleled scalability, flexibility, and cost savings, but without proper performance engineering, organizations risk experiencing poor application performance, a diminished user experience, and even financial losses.
Businesses that depend on cloud-based services and applications to ensure their clients are happy and loyal must invest in cloud performance engineering. The demand for cloud performance engineering will only increase due to the ever-increasing complexity and need for cloud-based services. As part of their overall cloud strategy, businesses must therefore give cloud performance engineering priority.
17. Testing of Big Data and Data Analytics
Software testing trends in Big Data and Data Analytics is moving towards automation and using specialized tools to handle the complexity and variety of data. This refers to verifying and validating data pipelines and processing applications that deal with large volumes of data in different formats. The scope of testing is vast and includes verifying data ingestion, storage, processing, analysis, visualization, and modeling.
Testing also ensures data quality, accuracy, and consistency while validating real-time data processing. The scope of testing is expanded due to the complexity of big data systems and requires specialized knowledge and tools to validate the correctness of the processed data.
18. Unlocking the Potential of Automated Mobile Testing
The mobile testing landscape is constantly evolving, with new devices, operating systems, and testing methodologies emerging as software testing trends evolve.
Mobile testing
is the practice of using
automated testing
tools to evaluate the performance and functionality of mobile applications. It aids in finding any problems and flaws in the application before it is made available to the general public, guaranteeing that it is of high quality and offers a seamless user experience.
Mobile applications are now crucial for modern businesses to make sure their applications function correctly and perform at their best due to the growing use of mobile devices. Given the continued growth in mobile usage, the scope of
automated mobile testing
is expanding significantly due to the rising demand for high-quality mobile applications.
LambdaTest offers a cloud-based platform to perform
mobile app testing
for Android and iOS devices without worrying about the device or location. It supports a wide range of
test automation frameworks
to test on more than 3000+ browsers, OS, and device combinations.
In this
System testing
tutorial, learn why System testing is important and all the intricacies of the System testing process.
19. Transforming Automation Testing with Chatbots
In recent software testing trends; chatbots have developed into sophisticated tools that can interact with customers, handle requests, and provide information thanks to artificial intelligence and natural language processing developments. In the context of automation, chatbots have been used in the testing sector to improve and streamline the testing process.
As chatbots provide several advantages to businesses, their use in automation is anticipated to expand significantly in the coming years. Chatbots can speed up the testing process by automating routine tasks so testers can concentrate on more difficult ones. They can also give immediate feedback on test results and automate repetitive tasks.
20. Maximizing Efficiency with Real-Device Automation Testing
Automation on real devices refers to automating
tests on real devices
, such as smartphones, tablets, and laptops. It accurately represents an application’s performance on real devices, considering device performance, network conditions, and hardware constraints. As the use of mobile devices continues to rise, it is anticipated that the scope of automation on real devices will expand significantly in the coming years of software testing trends.
It enables finding and fixing any problems or flaws in the application before release, lowering the possibility of a poor user experience. Additionally, since manual testing can be time- and resource-intensive, organizations are shifting from
manual to automation
.
21. Impact of Technology on Virtual and Augmented Reality Testing
In recent times, for software testing trends, Augmented reality (AR) and Virtual reality (VR) technologies have become increasingly popular. Many businesses use these technologies in the entertainment, gaming, educational, and healthcare sectors.
The range of VR and AR testing encompasses confirming the precision and functionality of user interactions, performance, and
compatibility testing
across various hardware and software configurations, 3D modeling and rendering, audio and
visual regression testing
, and environmental testing to make sure the virtual or augmented environment is in line with the real world.
22. Microservices Testing in the Digital Age
To keep up with the software testing trends, organizations must shift from traditional monolithic application testing approaches to a more modular and flexible testing strategy, called
microservices testing
. Microservices is an approach to software development that breaks applications into small, loosely coupled services. The microservice architecture style offers many benefits, including improved scalability, faster development and deployment, and reduced risk of system downtime.
However, it also brings new
challenges related to microservice architecture
in testing. Despite these challenges, organizations are expected to increase their investment in microservices testing as they adopt this architecture style. However,
DevOps
can help organizations continuously test and deploy microservices reducing the time to market for new features and improvements.
23. Making the Most of Image and Video Testing
As per recent software testing trends, users now demand high-quality visual experiences from their applications, thanks to the popularity of mobile devices and fast internet access. As a result, businesses are stressing the importance of testing images and videos in their applications.
As visual content becomes more complex, the scope of image testing will keep growing. To meet this demand, organizations use automated testing methods like compression testing to ensure that images are high-performing and of high quality. When
manual testing
is labor-intensive and prone to mistakes, automated testing also aids organizations in lowering the risk of human error.
Wrapping Up!
The software testing industry is constantly evolving, and 2023 is bringing new software testing trends and technologies to help businesses stay ahead of the competition. Organizations that embrace these trends and invest in robust testing strategies will be well-positioned to deliver high-quality software and meet changing customer needs.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How Custom Software Solutions Can Transform Businesses
EurosHub -
Dec 12
Generative AI Reloaded Ensuring security in your Website: A Journey of Security and Vulnerability
uknowWho -
Dec 12
Simplifying Payments with Pay gem: A Guide to Using Stripe in Rails
JetThoughts Dev -
Dec 12
Top paid courses to learn CSS
Kushagra Gour -
Dec 12
YashBansal651
Follow
Yash Bansal works as a Product Marketing Manager at LambdaTest. Previously worked as Content Marketing Manager with over 5+ years of experience in the e-learning industry; loves to share his thoughts
Joined
Apr 26, 2023
More from
YashBansal651
Webinar: End-to-End Test Automation with Provar [Experience (XP) Series]
#
automation
#
end2end
#
softwaretesting
Webinar: Client Feedback and Quality Assurance in Web Design for Agencies [Experience (XP) Series]
#
qualityassurance
#
webdesign
#
automationtesting
Webinar: Democratize Automation to Build Autonomy and Go-To-Market Faster [Experience (XP) Series]
#
webinar
#
automation
#
softwaretesting
#
automationtesting
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
The Rise of Bioinformatics Clouds: A New Era for Big Data in Life Sciences - DEV Community,"The Rise of Bioinformatics Clouds: A New Era for Big Data in Life Sciences - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
LILIAN KIGUNDA
Posted on
Nov 14
The Rise of Bioinformatics Clouds: A New Era for Big Data in Life Sciences
#
opensource
#
datascience
#
cloud
Introduction
As biology enters an era of high-throughput sequencing and big data, traditional in-house computing systems struggle to keep up with the flood of data generated by advanced biological experiments. Recognizing this challenge, the bioinformatics community is turning to cloud computing as a transformative solution. In their insightful review,
Bioinformatics Clouds for Big Data Manipulation
, Lin Dai and colleagues discuss how cloud technologies offer scalable, flexible, and cost-effective options for storing and analyzing massive datasets. This shift has the potential to reshape bioinformatics, making complex analyses more accessible and fostering a more collaborative scientific landscape.
1. Cloud Computing as a New Utility in Bioinformatics
Cloud computing allows bioinformatics to transition from costly, resource-intensive local infrastructures to a more versatile model where computation, storage, and data access are available as online services. Dai et al. note that this utility-based model offers a structure similar to essential services like water and electricity: scalable, on-demand resources paid for as needed. By using cloud services, labs of any size can harness vast computational power, without the financial burden of physical servers and IT maintenance.
2. Classifying Bioinformatics Cloud Services
Dai et al. break down bioinformatics cloud services into four main categories, each with its unique purpose:
Data as a Service (DaaS)
: Centralized repositories, such as Amazon Web Services (AWS), provide a wealth of public datasets. This on-demand access allows researchers to retrieve valuable data, such as GenBank or the 1000 Genomes Project, without the need for localized storage.
Software as a Service (SaaS)
: Cloud-based software, accessed through the internet, eliminates the need for local installation, facilitating remote data analysis. This approach broadens access to bioinformatics tools, enabling laboratories to use advanced software without requiring extensive IT support.
Platform as a Service (PaaS)
: PaaS platforms allow researchers to develop, test, and deploy applications directly in the cloud. By automating resource scaling to meet demand, PaaS minimizes the technical burden on users, making it easier to conduct and adapt analyses for big data.
Infrastructure as a Service (IaaS)
: IaaS provides customizable virtual infrastructures, enabling users to tailor environments specific to their analysis needs. Examples include virtual machines that deliver the power of a dedicated server without the expense of physical hardware.
This model gives researchers more control over how they access, store, and process data, creating a robust ecosystem for bioinformatics exploration.
3. Overcoming the Challenges of Data Transfer
Despite its promise, bioinformatics on the cloud is not without challenges. One major bottleneck is the sheer difficulty of transferring massive biological datasets into the cloud. According to Dai et al., the transfer process can often require physically shipping storage devices. Innovations like Aspera’s fasp™ technology, which enables high-speed data transfers, have emerged to address this challenge. Such technologies dramatically improve transfer times, making cloud storage and analysis a feasible option even for the largest genomics projects.
4. Lowering Barriers with Lightweight Programming Environments
In an ideal bioinformatics cloud setup, users would be able to run complex analyses without deep programming knowledge. Dai et al. emphasize the need for accessible, lightweight environments that support a drag-and-drop interface, enabling users to set up bioinformatics pipelines with minimal coding. A lightweight cloud-based programming environment would democratize access to cloud computing, empowering researchers to focus on biological insights rather than technical setup.
5. The Vision of Open and Collaborative Bioinformatics Clouds
A significant part of Dai et al.’s vision for the future involves making bioinformatics clouds open and accessible to the entire scientific community. Open access to data and tools supports reproducibility, transparency, and collaboration—cornerstones of scientific progress. In a cloud-based model, researchers worldwide could access the same datasets and tools, encouraging interdisciplinary collaboration and speeding up discoveries. Dai et al. advocate for open bioinformatics clouds, where data sharing and collective intelligence become essential drivers of innovation.
Conclusion: A New Horizon for Bioinformatics
The insights from Dai et al. reinforce that cloud computing holds the potential to transform bioinformatics from a field struggling to manage big data into one empowered to leverage it. Cloud computing offers bioinformatics not just a storage solution but a pathway toward a more dynamic, accessible, and collaborative scientific future. As the field continues to grow, cloud technologies will likely become as indispensable to bioinformatics as laboratory equipment, setting a new standard for how we understand and analyze biological data in the years to come.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How To Setup Password Hash Synchronization In Microsoft Azure
Samuel Arogbonlo -
Nov 30
Launching EC2 Instances with AWS CLI and Advanced Features
Sushant Gaurav -
Nov 29
Announcements from Matt Garman Keynote at re:Invent 2024
Eyal Estrin -
Dec 3
My First Publish to crates.io (and cross compilation)
TheoForger -
Nov 28
LILIAN KIGUNDA
Follow
Joined
Oct 17, 2024
Trending on
DEV Community
Hot
Encore Launch Week Day 2: Public Buckets & CDN
#
opensource
#
go
#
typescript
#
webdev
8 Type of Load Balancing
#
discuss
#
webdev
#
cloudcomputing
#
programming
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Data Privacy Laws: Navigating Compliance in the Age of Big Data - DEV Community,"Data Privacy Laws: Navigating Compliance in the Age of Big Data - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Suraj Vishwakarma
for
Documatic
Posted on
Oct 24, 2023
Data Privacy Laws: Navigating Compliance in the Age of Big Data
#
beginners
#
programming
#
webdev
#
database
Data Privacy Laws: Navigating Compliance in the Age of Big Data
Introduction
In modern applications, the need for data from the user is rapidly increasing. This can be in the form of a registration form, personal details, or any data related to the user of the application. With the rise in AI models, the a need to pass data to the model to extract a better user experience or results to any query. Getting and storing data from users led us to the handling of data in terms of privacy.
Data has become some much in this world as it can change from basic purchasing decisions to threats like voting decision manipulation. Building an AI model requires tons of data to make it more accurate. Many a time source of this is not revealed. This kind of misuse of data can cause unnoticeable but significant damage to society. That’s why,  the government around the world has implemented regulations regarding the privacy of user data. They have restricted any kind of misuse through data leaking or selling of data. It consists of a variety of laws.
So, today we are going to look into the laws that deal with the data. In this way, you can make sure that your product is not violating any law.
Let’s get started.
The information provided in this article is intended as a general guide and should not be construed as legal advice. It is crucial to consult with qualified legal professionals or experts to obtain advice tailored to your specific situation and to ensure compliance with applicable data privacy laws and regulations.
Laws around the world
There are many laws that have been implemented around the world by major governments and organizations. Let’s look into some of those regulations Acts:
General Data Protection Regulation (GDPR) in Europe:
This regulation is applied to businesses that are operating within the European Union. Also, those are targeting the citizens of the EU.
California Consumer Privacy Act (CCPA) in the USA:
As the name says it is applicable to the privacy rights of Californian residents. This law is also applicable to businesses around the USA if they meet certain criteria.
Personal Data Protection Bill in India:
This is a law that aims to regulate the processing of personal data in India.
Personal Information Protection Law (PIPL) in China:
China has a law for the data privacy of individuals.
Data Protection Act in the UK:
After Brexit, the UK implemented its Data Protection Act for businesses.
These are some of the laws that are implemented across some of the major regions around the world. We are going to look into some of the command laws that are implemented across the Act. These regulations are the common ones across the Act.
Data Minimization and Purpose
This states that you should collect the data that is required. Do not collect unnecessary data that will not be helpful for the application. This law ensures that that data will not be beneficial to the user should not be collected.
Also, while collecting data, you should also provide the purpose of collecting that data. This will ensure that the user knows how this data will be used or required.
Consent Management
This states that you should obtain consent from the user before collecting and processing any data. Consent is necessary and the user should know what processes will be run on their data. Also, the user should have the option to withdraw from the consent at any time.
Data Security Measures
The collected data should be protected from unauthorized access. Data collected from users can be very sensitive in some cases. So, it became necessary to implement guidance to protect the data breaches. This can include encrypting the data(transit or rest), having backups, restricting unauthorized access, etc.
You should also perform regular checks to find any security vulnerabilities. According to any vulnerabilities apply and implement necessary protocols to secure data. Leaking or hacking of data should be minimized.
Data Subject Rights
The data that is collected from users should respect the individual’s rights. The right to access, rectify, erase, or port their data.
You should build your application to address this kind of functionality if requested by the user. These rights are designed to empower individuals and enhance their privacy and data protection in the digital age
Data Protection Impact Assessments(DPIAs)
This is also known as Privacy Impact Assessment in some regions. It is a process to help organizations identify, assess, and mitigate any risk associated with their data processing. This assessment ensures that data protection is considered throughout the application during the development phase.
While conducting DPIA an organization assesses the necessity and proportionality of the data processing activities. It checks whether it poses any potential risk to the individual's rights. By evaluating these, we can implement measures to minimize or eliminate the risk of any data privacy violation. It helps in ensuring compliance with the regulations.
Incident Response Plan
There should be a plan for the adversity that can happen with data such as data leaks or breaches. It can also with access, loss, disclosure, alteration, or destruction of sensitive data. The IRP focuses on swift incident identification, classification based on severity, containment to prevent further damage, eradication of the root cause, and systematic recovery, including data restoration and stakeholder communication.
This helps in minimizing the impact of security incidents and maintain customer trust. Regular testing, refinement, and adaptation of the IRP ensure its effectiveness against the evolving landscape of cyber threats, making it an indispensable tool for modern businesses and institutions.
Conclusion
These are some of the most common regulations that are applied throughout the world. Implementing this in your organization can result in better data handling and not violating any data protection laws. We do look at some of the major government and their laws about data protection, you can look into that for further understanding of the laws as per your working region.
I hope this article has helped you know more about the data protection laws. Thanks for reading the article.
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Kote Isaev - #StandWithUkraine
Kote Isaev - #StandWithUkraine
Kote Isaev - #StandWithUkraine
Follow
Joined
Mar 22, 2023
•
Oct 27 '23
Dropdown menu
Copy link
Hide
""In modern applications, the need for data from the user is rapidly increasing. "" This is one of biggest lie of 21 century. Applications need a very limited subset of data from user:
1) account identity data (username, and sadly it become email for most apps for pure laziness and at expense of extra seconds during sign up)
2) authentication data (password OR third-party auth data like social media id)
3) password/account recovery data, usually email is enough
That is ALL data an app itself really NEED to know about the person who use software.
All other data is optional, and for most apps it is completely unnecessary and just part of harvesting people by data-hungry marketing department in hope to boost sales for another 0.00001% or 0.0001$ per account/month.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Angular vs Next.js: A Detailed Comparison
Hitesh Chauhan -
Dec 5
Brain debugging. Interview with Anders Schau Knatten, author of ""C++ Brain Teasers: Exercise Your Mind""
Anastasiia Vorobeva -
Dec 5
AI System Learns How Objects Interact in Images to Generate New Scenes with Same Relationships
Mike Young -
Dec 5
Dockerize CodeIgniter 3: A Step-by-Step Guide
Ehtesham Ali -
Dec 5
Documatic
Follow
More from
Documatic
Transitioning to a Microservices Architecture: Overcoming Obstacles
#
javascript
#
beginners
#
programming
#
webdev
The Future of Cloud Computing: Predictions and Trends
#
webdev
#
javascript
#
beginners
#
tutorial
Guide to Implementing Function-as-a-Service: A Deep Dive into Serverless Computing
#
webdev
#
javascript
#
programming
#
beginners
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How I Crushed My AWS Certification Renewals Back-to-Back (and Why It Was a Bad Idea) - DEV Community,"How I Crushed My AWS Certification Renewals Back-to-Back (and Why It Was a Bad Idea) - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Justin Wheeler
for
AWS Community Builders
Posted on
May 9
How I Crushed My AWS Certification Renewals Back-to-Back (and Why It Was a Bad Idea)
#
aws
#
certification
I renewed all 12 of my active AWS certifications in 22 days! Most of the exams were back-to-back (based on testing center availability), and I even took two exams in a single day. The entire experience was insanely stressful, yet incredible.
Why?
In January, I was compiling a list of my goals this year, determined to be more accomplished than I was last year. I set some extremely ambitious goals for myself, to say the least. As if they are too ambitious, I guess we will wait and see. Since I had some AWS certifications expiring this year and some next year, I wanted to get them all renewed so I could focus on other priorities next year (Kubernetes, here I come)! The obsessive tendencies in me thought it would be nice if I could align the dates in some logical way rather than the random mess they were previously in. Although I had originally planned on finishing my AWS certifications by the end of Q1, plans changed. Aside from everyday life, there were other complications.
Schedule
4/08 Data Analytics Specialty
4/22 DevOps Engineer Professional
4/23 Solutions Architect Professional
4/24 Advanced Networking Specialty
4/25 Machine Learning Specialty
4/27 Security Specialty
4/29 Database Specialty
4/29 SAP on AWS Specialty
If you didn't already know that professional-level exams automatically renew some lower-level certifications, now you do.
DevOps Engineer Professional renews
Developer Associate
SysOps Administrator Associate
Cloud Practitioner
Solutions Architect Professional renews
Solutions Architect Associate
Cloud Practitioner
Retiring Exams
AWS had announced they were retiring three of their exams, which I already wrote about in
Navigating AWS Certifications in 2024
.
Data Analytics Specialty (retiring April 8th)
Database Specialty (retiring April 29th)
SAP on AWS Specialty (retiring April 29th)
Obviously, I contemplated skipping these exams; after all, they are retiring. Why didn't I? What's the point? Well, like I already said in
this LinkedIn post
in regards to the Data Analytics Specialty exam, ""I don't agree with Amazon Web Services decision to retire this exam"".
Big Data
is more important now than ever before. Perhaps the Data Engineer Associate is meant to replace this exam. Although I think we may need a Data Engineer Professional if we expect the same level of depth.
I feel that the Database Specialty is equally valuable. With all of the
purpose-built databases
that AWS has to offer, it is invaluable to build a strong understanding to ensure the best database for the job can be chosen.
My feelings for the SAP on AWS Specialty are not the same; I'm not terribly sad to see it go, yet I still wanted to renew it. Why?
I had previously planned on taking this exam before I learned about the retirement. If I had skipped it, part of me would have always wondered if I could've actually passed it again.
I wanted to test my SAP knowledge, as I am particularly weak in this domain. Even simple SAP configurations on AWS can easily cost
thousands of dollars per month
, which makes it impossible to experiment on my own.
How?
I've talked about my study methods
before
and this time was pretty similar, to be honest. Except this time, I only had to focus on the new stuff since I've already passed these tests before.
These are the websites that I used the most:
A Cloud Guru (Paid)
Cloud Academy (Paid)
ML Exam (Free)
Pluralsight (Paid)
Practice Tests
I started with practice tests to gauge my knowledge before hitting the books. This helped me focus on the newer services and concepts I wasn't familiar with.
Did you know that API Gateway can directly call some other AWS services without Lambda? It's called
Direct integrations
, and it's definitely something I learned through this experience.
The scores varied wildly between A Cloud Guru and CloudAcademy, which was interesting. How can I get a 50% on CloudAcademy and a 90% on A Cloud Guru for the same exam? 🤔
Flash Cards
I used
Quizlet
to create online flashcards. The AI functionality that has been added to Quizlet recently is really great. I'll highlight two examples.
Magic Notes
uses AI to generate flash cards automatically from a large source of text. I used this a couple of times to paste AWS FAQs or documentation.
Q-Chat
uses AI to interact with your own flash cards using natural language. With this, I don't have to worry about exactly how my flash cards are typed anymore.
In the middle of this AI storm, it seems like Quizlet is doing AI the right way!
Dedicated Study Time
For a while now, I have always dedicated time every day to studying something. This year, I just focused the allocated time strictly on AWS. Nearing my exams, I would often trade time I would've spent on entertainment like movies or shows for studying instead to ensure I was as prepared as I could be. Do you know how hard it is to study for eight exams at the same time? In my mind, it was similar to ""finals week"" in college.
Dedicated Testing Time
Typically, when I take a single exam, I will take a half day and be back to work. Although, this was much more taxing, I used a week of PTO so I could focus on my exams exclusively. I prefer testing in person, which resulted in commuting to testing centers that were about 35 miles away. Still, not all of my tests were in person. I did three exams using the online option called
OnVUE
.
Expenses
The costs can be high for these types of exams. I had access to a bunch of benefits through my company and my engagement in AWS communities.
First,
AWS Community Builders
and
AWS User Group Leaders
both provided me with a 100% voucher.
Next, I was able to use multiple 50% vouchers that I had stockpiled from earning all of the certifications previously.
AWS provides
benefits
to certified individuals.
Finally,
Bravo LT
reimbursed me for every exam I passed.
Words of Caution 🚧
I will never do this again. If you are thinking of attempting something similar, I hope I can change your mind. Overall, I felt like everything turned out okay, but it could've been far worse.
My mental health suffered from this endeavor in a couple ways. I experienced anxiety, stress, and
Imposter Syndrome
. I lost enjoyment in my life for a time because I was sacrificing all of my spare time to studying.
My physical health was impacted as well. I found myself skipping meals to finish practice tests or additional lessons. Furthermore, my quality of sleep was affected.
When planning, I want to ask that you don't overwhelm yourself. Give yourself plenty of time to accomplish your goals without rushing.
As always, if you liked this content, maybe you would like to
Buy Me a Coffee
or connect with me on
LinkedIn
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Aurora Limitless - Sharded Table
Franck Pachot -
Nov 25
Smoke Testing in Software Testing: Core Basics Guide
keploy -
Nov 25
Crafting a Zero Downtime Multi-Region Architecture on AWS
Farrukh Khalid -
Nov 24
Managed Secure SFTP using Terraform
Darryl Ruggles -
Nov 24
AWS Community Builders
Follow
Build On!
Would you like to become an AWS Community Builder? Learn more about the program and apply to join when applications are open next.
Learn more
More from
AWS Community Builders
🚀 ""Building an Enterprise-Grade 💼 Generative AI Chatbot 🤖 Using Amazon Q"" 🔍
#
aws
#
ai
#
genai
#
chatgpt
Demystifying an interesting relation between ECR and S3
#
aws
#
ecr
#
s3
#
vpc
Secure AWS VPC using Public and Private Subnets
#
aws
#
cloud
#
vpc
#
security
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Hoai Nam NGUYEN - DEV Community,"Hoai Nam NGUYEN - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Follow
User actions
Hoai Nam NGUYEN
I am a programmer living in the Paris area. I am currently interested in (big) data projects, functional programming, clean code, software testing, algorithms, devops and automations
Location
France
Joined
Joined on
Oct 20, 2018
github website
Work
Data Engineer at freelance
Six Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least six years.
Got it
Close
Writing Debut
Awarded for writing and sharing your first DEV post! Continue sharing your work to earn the 4 Week Writing Streak Badge.
Got it
Close
Five Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least five years.
Got it
Close
Four Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least four years.
Got it
Close
Three Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least three years.
Got it
Close
Two Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least two years.
Got it
Close
One Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least one year.
Got it
Close
More info about @namnguyen
GitHub Repositories
comparator
Scala
•
1 star
clustering
Scala
zipper
Scala
Skills/Languages
Scala, big data, python, distributed computing, algorithms, and AI
Currently learning
haskell, concurrency programming, distributed systems, advanced functional programming
Post
5 posts published
Comment
1 comment written
Tag
7 tags followed
Integration tests and microservices applications
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Follow
Nov 8 '19
Integration tests and microservices applications
#
testing
#
data
6
reactions
Comments
Add Comment
4 min read
Want to connect with Hoai Nam NGUYEN?
Create an account to connect with Hoai Nam NGUYEN. You can also sign in below to proceed if you already have an account.
Create Account
Already have an account?
Sign in
Create a Validation DSL from scratch with scala
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Follow
Mar 23 '19
Create a Validation DSL from scratch with scala
#
scala
#
functional
8
reactions
Comments
Add Comment
4 min read
Designing a comparator library for regression test
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Follow
Oct 22 '18
Designing a comparator library for regression test
#
scala
#
test
9
reactions
Comments
Add Comment
8 min read
Designing a json migration tool
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Follow
Oct 22 '18
Designing a json migration tool
#
scala
#
json
9
reactions
Comments
Add Comment
6 min read
A json migration tool
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Hoai Nam NGUYEN
Follow
Oct 22 '18
A json migration tool
#
json
#
scala
7
reactions
Comments
Add Comment
4 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How I tested a new tool for recruiting letters - DEV Community,"How I tested a new tool for recruiting letters - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
kevin_kevin
Posted on
Oct 4, 2018
How I tested a new tool for recruiting letters
#
discuss
#
job
#
career
A few days ago I wrote an article about the bot I made for sorting job proposals. I received a lot of comments even though it was my first post here. As I got it, the topic is pretty hot. And seems I am becoming like ""anti-recruiter blogger"". Ha-ha, no! I like them all, just a bit bored...
So I've decided to test the tool I was talking about lately (found it on Product Hunt Upcoming). It's called
Reply.id
.
What's that?
AI Gmail filter + Bot that talks to recruiters
Now all the tools are AI MACHINE LEARNING BIG DATA (you know). But let's see, what do they mean.
Onboarding staff
You choose your skill set and level. You sign in with Gmail account.
Set up the questions a bot will ask recruiters.
Here is the bot I've made on Reply.id just in a minute or so
. And remember
my bot
on Messenger?
How it actually works
I've chosen to allow integration between Reply.id and Gmail account. When I received a letter from the recruiter, some new folder called ""Job proposals"" appeared.
So, the algorithm sees recruiters letters (so-called ""AI Gmail filter"") and sends them to a different folder. Alright. Afterwards, the tool automatically responds to all these letters with the link to bot and explanation (with the template I've chosen already during onboarding, so there are few of them).
That's how you see in ""Sent"":
If recruiter talks to bot, you'd see just the answers in a kind of spreadsheet. You can also open it up and see all the answers to the questions. Questions can also be added manually if needed.
So, it's a good tool for
those who really struggle from recruiters;
those who somehow are ready to talk about new career opportunities (then just put the link to a bot on social media and Github);
those who want inbox empty and structured.
Nice.
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Darius
Darius
Darius
Follow
Location
Michigan
Joined
Sep 18, 2018
•
Oct 7 '18
Dropdown menu
Copy link
Hide
Pretty neat. Sometimes, recruiters emails sound like they come from a bot too, and not a very smart one at that.
The 21st century's version of ""talk to my secretary"", will be: ""talk to my bot""...
or even...
""ask your bot to talk to my bot, and we'll take it from there""
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Thursday Quiz
Scofield Idehen -
Oct 31
🌟 The Ultimate Guide: Top Open-Source Tools and Repositories Every Developer Must Know 🌟
Hanzla Baig -
Dec 4
Who's hiring (December 2024)
flo merian -
Dec 2
How does Cognitive Science influence knowledge Representation in AI?
Vikas76 -
Oct 29
kevin_kevin
Follow
front end
Location
berlin
Joined
Sep 28, 2018
More from
kevin_kevin
Recruiters mess up all the technologies. Should we show them their mistakes? Or... laugh?
#
discuss
#
career
#
job
#
productivity
8 things I hate in recruitment letters [add yours, it's kind of research]
#
discuss
#
career
#
job
#
productivity
Have you ever been hunted by recruiters on Tinder?
#
discuss
#
career
#
job
#
productivity
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
"What is Big Data? Characteristics, types, and technologies - DEV Community","What is Big Data? Characteristics, types, and technologies - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Hunter Johnson
for
Educative
Posted on
Sep 7, 2022
• Originally published at
educative.io
What is Big Data? Characteristics, types, and technologies
#
datascience
#
database
#
bigdata
#
tutorial
Big Data is a modern analytics trend that allows companies to make more data-driven decisions than ever before. When analyzed, the insights provided by these large amounts of data lead to real commercial opportunities, be it in marketing, product development, or pricing.
Companies of all sizes and sectors are joining the movement with data scientists and Big Data solution architects. With the Big Data market expected to nearly double by 2025 and user data generation rising, now is the best time to become a Big Data specialist.
Today, we'll get you started on your Big Data journey and cover the fundamental concepts, uses, and tools essential for any aspiring data scientist.
Here’s what we’ll go through today:
What is Big Data
What is Big Data used for?
How does Big Data work?
Big Data Terminology
Big Data Technologies
What to learn next
What is Big Data?
Big data refers to
large collections of data
that are so complex and expansive that they cannot be interpreted by humans or by traditional data management systems. When properly analyzed using modern tools, these huge volumes of data give businesses the information they need to make informed decisions.
New software developments have recently made it possible to use and track big data sets. Much of this user information would seem meaningless and unconnected to the human eye. However, big data analytic tools can track the relationships between hundreds of types and sources of data to produce useful business intelligence.
All big data sets have three defining properties, known as the
3 V's
:
Volume
: Big data sets must include millions of unstructured, low-density data points. Companies that use big data can keep anything from dozens of terabytes to hundreds of petabytes of user data. The advent of cloud computing means companies now have access to zettabytes of data! All data is saved regardless of apparent importance. Big data specialists argue that sometimes the answers to business questions can lie in unexpected data.
Velocity
: Velocity refers to the fast generation and application of big data. Big data is received, analyzed, and interpreted in quick succession to provide the most up-to-date findings. Many big data platforms even record and interpret data in real time.
Variety
: Big data sets contain different types of data within the same unstructured database. Traditional data management systems use structured relational databases that contain specific data types with set relationships to other data types. Big data analytics programs use many different types of unstructured data to find all correlations between all types of data. Big data approaches often lead to a more complete picture of how each factor is related.
Correlation vs. Causation
Big data analysis only finds correlations between factors, not causation. In other words, it can find if two things are related, but it cannot determine if one causes the other.
It's up to data analysts to decide which data relationships are actionable and which are just coincidental correlations.
Big Data History
The concept of Big Data has been around since the 1960s and 70s, but at the time, they didn't have the means to gather and store that much data.
Practical big data only took off around 2005, as developers at organizations like YouTube and Facebook realized the amount of data they generated in their day-to-day operations.
Around the same time, new advanced frameworks and storage systems like Hadoop and NoSQL databases allowed data scientists to store and analyze bigger datasets than ever before. Open-source frameworks like Apache Hadoop and Apache Spark provided the perfect platform for big data to grow.
Big data has continued to advance, and more companies recognize the advantages of predictive analytics. Modern big data approaches leverage the Internet of Things (IoT) and
cloud computing
strategies to record more data from across the world and
machine learning
to build more accurate models.
While it's hard to predict what the next advancement in big data will be, it's clear that big data will continue to become more scaled and effective.
What is Big Data used for?
Big data applications are helpful across the business world, not just in tech. Here are some use cases of Big Data:
Product Decision Making
: Big data is used by companies like Netflix and Amazon to develop products based on upcoming product trends. They can use combined data from past product performance to anticipate what products consumers will want before they want them. They can also use pricing data to determine the optimal price to sell the most to their target customers.
Testing
: Big data can analyze millions of bug reports, hardware specifications, sensor readings, and past changes to recognize fail-points in a system before they occur. This helps maintenance teams prevent the problem and costly system downtime.
Marketing
: Marketers compile big data from previous marketing campaigns to optimize future advertising campaigns. Combining data from retailers and online advertising, big data can help finetune strategies by finding subtle preferences for ads with certain image types, colors, or word choices.
Healthcare
: Medical professionals use big data to find drug side effects and catch early indications of illness. For example, imagine there is a new condition that affects people quickly and without warning. However, many of the patients reported a headache during their last annual checkup. This would be flagged as a clear correlation using big data analysis but may be missed by the human eye due to differences in time and location.
Customer Experience
: Big data is used by product teams after a launch to assess the customer experience and product reception. Big data systems can analyze large data sets from social media mentions, online reviews, and feedback on product videos to get a better indication of what problems customers are having and how well the product is received.
Machine learning
: Big data has become an important part of machine learning and artificial intelligence technologies, as it offers a huge reservoir of data to draw from.
ML engineers
use big data sets as varied training data to build more accurate and resilient predictive systems.
How does Big Data work?
Big data alone won't provide the business intelligence that many companies are searching for. You'll need to process the data before it can provide actionable insights.
This process involves 3 major stages:
1. Data flow intake
The first stage has data flowing into the system in huge quantities. This data is of many types and will not be organized into any usable schema. Data at this stage is called a
data lake
because all the data is lumped together and impossible to differentiate.
Your company's system must have the data processing power and storage capacity to handle this much data. On-premises storage is the most secure but can become overworked depending on the volume.
Cloud computing and
distributed storage
are often the secrets to effective flow intake. They allow you to divide storage among multiple databases on the system.
2. Data analysis
Next, you'll need a system that automatically cleans and organizes data. Data at this scale and frequency is too large to organize by hand.
Popular strategies include setting criteria that throw out any faulty data or building in-memory analytics that continually adds new data to ongoing analysis. Essentially, this stage is like taking a pile of documents and ordering it until it’s filed in a structured way.
At this stage, you'll have the raw findings but not what to do with the findings. For example, a ride-share service may find that over 50% of users will cancel a ride if the incoming driver is stopped for more than 1 minute.
3. Data-driven decision-making
At the final stage, you'll interpret the raw findings to form a concrete plan. Your job as a data scientist will be to look at all the findings and create an evidence-supported proposal for how to improve the business.
In the ride-share example, you might decide that the service should send drivers on routes that keep them moving, even if it takes slightly longer to reduce customer frustration. On the other hand, you could decide to include an incentive for the user to wait until the driver arrives.
Either of these options is valid because your big data analysis cannot determine which aspect of this interaction needs to change to increase customer satisfaction.
Big Data terminology
Structured Data
:
This data has some pre-defined organizational property that makes it easy to search and analyze. The data is backed by a model that dictates the size of each field: its type, length, and restrictions on what values it can take. An example of structured data is ""unit's produced per day"", as each entry has a defined
product type
and
number produced
fields.
Unstructured Data
:
This is the opposite of structured data. It doesn't have any pre-defined organizational property or conceptual definition. Unstructured data makes up the majority of big data. Some examples of unstructured data are social media posts, phone call transcripts, or videos.
Database:
An organized collection of stored data that can contain either structured or unstructured data.
Databases
are designed to maximize the efficiency of data retrieval. Databases have two types: relational and non-relational.
Database management system
:
Usually, when referring to databases such as MySQL and PostgreSQL, we are talking about a system, called the database management system. A DBMS is software for creating, maintaining, and deleting multiple individual databases. It provides peripheral services and interfaces for the end-user to interact with the databases.
Relational Database (SQL)
:
Relational databases consist of structured data stored as rows in tables. The columns of a table follow a defined
schema
that describes the type and size of the data that a table column can hold. Think of a schema as a blueprint of each record or row in the table. Relational databases must have structured data and the data must have some logical relationship to each other.
For example, a Reddit-like forum would use a relational database as the data's logical structure is that users have a list of following forums, forums have a list of posts, and posts have a list of posted comments. Popular implementations include Oracle, DB2, Microsoft SQL Server,
PostgreSQL
, and
MySQL
.
Non-relational Database
:
Non-relational databases have no rigid schema and contain unstructured data. Data within has no logical relationship to other data in the database and is organized differently based on the needs of the company. Some common types include key-value stores (Redis, Amazon Dynamo DB), column stores (HBase, Cassandra), document stores (
Mongo DB
, Couchbase), graph databases (Neo4J), and search engines (Solr, ElasticSearch, Splunk). The majority of big data is stored on non-relational databases as they can contain multiple types of data.
Data Lake
:
A repository of data stored in raw form. Like water, all the data is intermixed, and no collection data can be used before it can be separated from the lake. Data in the data lake doesn't need to have a defined purpose yet. It is stored in case a use is discovered later.
Data Warehouse
:
A repository for filtered and structured data with a predefined purpose. Essentially, this is the structured equivalent of a data lake.
Big Data technologies
Finally, we'll explore the top tools used by modern data scientists as they create Big Data solutions.
Hadoop
Hadoop is a reliable, distributed, and scalable distributed data processing platform for storing and analyzing vast amounts of data. Hadoop allows you to connect many computers into a network used to easily store and compute huge datasets.
The lure of Hadoop is its ability to run on cheap commodity hardware, while its competitors may need expensive hardware to do the same job. It's also open source. Hadoop makes Big Data solutions affordable for everyday businesses and has made Big Data approachable to those outside of the tech industry.
Hadoop is sometimes used as a blanket term referring to all tools in the Apache data science ecosystem.
MapReduce
MapReduce
is a programming model used across a cluster of computers to process and generate Big Data sets with a parallel, distributed algorithm. It can be implemented on Hadoop and other similar platforms.
A MapReduce program contains a
map
procedure that filters and sorts data into a usable form. Once the data is mapped, it's passed to a
reduce
procedure that summarizes the trends of the data. Multiple computers in a system can perform this process at the same time to quickly process data from the raw data lake to usable findings.
MapReduce programming model has the following characteristics:
Distributed
: The MapReduce is a distributed framework consisting of clusters of commodity hardware that run
map
or
reduce
tasks.
Parallel
: The map and reduce tasks always work in parallel.
Fault-tolerant
: If any task fails, it is rescheduled on a different node.
Scalable
: It can scale arbitrarily. As the problem becomes bigger, more machines can be added to solve the problem in a reasonable amount of time; the framework can scale horizontally rather than vertically.
Mapper Class in Java
Let's see how we can implement MapReduce in Java.
First, we'll use the Mapper class added by the Hadoop package (
org.apache.hadoop.mapreduce
) to create the
map
operation. This class maps input key/value pairs to a set of intermediate key/value pairs. Conceptually, a mapper performs parsing, projection (selecting fields of interest from the input), and filtering (removing non-interesting or malformed records).
For example, we'll create a mapper that takes a list of cars and returns the brand of the car and an iterator; a list of a Honda Pilot and a Honda Civic would return
(Honda 1)
,
(Honda 1)
.
Mapper class in Hadoop:
public class CarReducer extends Reducer<Text, IntWritable, Text, LongWritable> {

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        long sum = 0;

        for (IntWritable occurrence : values) {
            sum += occurrence.get();
        }

        context.write(key, new LongWritable(sum));
    }
}
Enter fullscreen mode
Exit fullscreen mode
The most important part of this code is on
line 9
. Here, we output key/value pairs that get sorted and aggregated by reducers later on.
Don't confuse the key and value we write with the key and values being passed into the
map(...) method
. The key is the name of the car brand. Since each occurrence of the key denotes one physical count of that brand of car, we output 1 as the value. We want to output a key type that is both serializable and comparable, but the value type should only be serializable.
Reducer Class in Java
Next, we'll implement the
reduce
operation using the
Reducer
class added by Hadoop. The
Reducer
automatically takes the output of
Mapper
and returns the total number of cars of each brand.
The reduce task is split among one or more reducer nodes for faster processing. All tasks of the same key (brand) are completed by the same node.
Reducer class in Hadoop:
public class CarReducer extends Reducer<Text, IntWritable, Text, LongWritable> {

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        long sum = 0;

        for (IntWritable occurrence : values) {
            sum += occurrence.get();
        }

        context.write(key, new LongWritable(sum));
    }
}
Enter fullscreen mode
Exit fullscreen mode
Lines 8-10
iterate through each map of the same key and sum the total count using the
sum
variable.
Mapper
and
Reducer
are the backbone of many Hadoop solutions. You can expand these basic forms to handle huge sums of data or reduce them to highly specific summaries.
What to learn next
With this introduction to Big Data, you're prepared to start practicing with common data science tools and advanced analytical concepts.
Some next steps to look at are:
Explore the Hadoop Distributed File System (HDFS)
Build a model using Apache Spark
Generated findings using MapReduce
Familiarize yourself with different input/output formats
To help you master these skills and continue your Big Data journey, Educative has created the course
Introduction to Big Data and Hadoop
. This course will give you hands-on practice with Hadoop, Spark, and MapReduce, tools used by data scientists every day.
By the end, you'll have used your learning to complete a Big Data project from beginning to end that you can use on your resume.
Happy learning!
Continue reading about Big Data and data science on Educative
Applied Data Science: serverless functions, pipelines and PySpark
The top 10 ML algorithms for data science in 5 minutes
Data Science Simplified: What is language modeling for NLP?
Start a discussion
What do you think are the most interesting use cases for Big Data? Was this article helpful? Let us know in the comments below!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Brain-Inspired Method Cuts Neural Networks by 90% Without Losing Accuracy
Mike Young -
Nov 22
Master CSS Selectors: The Complete Beginner-to-Expert Guide
chintanonweb -
Nov 26
Learn how to create a bottom navigation with Tailwind CSS
Michael Andreuzza -
Nov 26
Supabase | My Way of Designing & Managing DB
01kg -
Dec 5
Educative
Follow
Learn anything from CSS to System Design, interactively.
Level up on in-demand tech skills - at your own speed.
Text-based courses with embedded coding environments help you learn without the fluff.
Try a preview
More from
Educative
Basics of Fuzzy Logic
#
datascience
#
machinelearning
#
programming
#
tutorial
What is Clustering: An Introduction
#
machinelearning
#
programming
#
tutorial
#
productivity
Hands-on AWK
#
programming
#
tutorial
#
productivity
#
beginners
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Why is Big Data Important? 🤔 - DEV Community,"Why is Big Data Important? 🤔 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Subham
Posted on
Jun 1, 2023
Why is Big Data Important? 🤔
Big data is a term that describes
extremely large data sets
that can be analyzed using computers to reveal
patterns, trends, and associations
. Big data can come from various sources and have different formats, such as text, audio, video, or numbers. Big data can help us understand
human behavior and interactions
, as well as other aspects of the world.
But why is big data important for businesses and organizations? Here are some reasons:
1. Big Data Can Save Costs 💰
Big data can help businesses and organizations find more efficient ways of doing things, such as storing data, managing resources, or delivering services. For example, some tools of big data, like
Hadoop
and
Cloud-based Analytics
, can reduce the cost of storing and processing large amounts of data.
2. Big Data Can Reduce Time ⏱️
Big data can help businesses and organizations make faster decisions based on real-time or near-real-time data. For example, some tools of big data, like
Hadoop
and
in-memory analytics
, can process data streams quickly and provide insights on the spot.
3. Big Data Can Understand the Market Conditions 📈
Big data can help businesses and organizations analyze customer behavior and preferences, as well as competitor actions and strategies. For example, by analyzing customers' purchasing behaviors, a company can find out the products that are sold the most and produce products according to this trend². By this, it can get ahead of its competitors.
4. Big Data Can Control Online Reputation 😎
Big data can help businesses and organizations monitor and improve their online presence and image. For example, big data tools can do
sentiment analysis
, which means analyzing the emotions and opinions of people who mention a company or a brand online. By this, a company can get feedback and improve its customer satisfaction.
5. Big Data Can Boost Customer Acquisition and Retention 💯
The customer is the most important asset any business depends on. Big data can help businesses and organizations attract new customers and keep existing ones loyal. For example, big data can help analyze customer needs, preferences, and feedback, and provide personalized offers, recommendations, and services².
6. Big Data Can Offer Marketing Insights 🚀
Big data can help businesses and organizations design and execute effective marketing campaigns. For example, big data can help segment customers into different groups based on their characteristics and behaviors, and target them with tailored messages and ads².
7. Big Data Can Drive Innovations and Product Development 🚀
Big data can help businesses and organizations create new products and services that meet customer needs and expectations. For example, big data can help identify gaps in the market, test new ideas, and evaluate customer feedback.
As you can see, big data is important for many reasons. It can help businesses and organizations improve their performance, efficiency, competitiveness, and profitability. It can also help them discover new opportunities and challenges.
But how do we handle big data? How do we store it, process it, analyze it, and use it? That's where
big data analytics
comes in.
Big data analytics is the branch of computing that deals with developing systems and solutions for managing and analyzing big data.
Big data analytics involves using software that can handle large, complex data sets. There are many software-as-a-service (SaaS) companies that specialize in this field. Some examples are
Oracle
,
IBM
,
Microsoft
,
Google
,
Amazon
,
Facebook
,
Twitter
, etc.
These companies use big data analytics to provide various services to their customers, such as:
How big data is important in the following roles:
Data storage:
Big data can be used to store large amounts of data in a cost-effective and efficient way. This can be helpful for businesses that need to store data for compliance purposes, or for research organizations that need to store large datasets for analysis.
Data processing:
Big data can be used to process large amounts of data quickly and efficiently. This can be helpful for businesses that need to process data in real time, or for research organizations that need to process large datasets for analysis.
Data visualization:
Big data can be used to visualize data in a way that is easy to understand and interpret. This can be helpful for businesses that need to make sense of large amounts of data, or for research organizations that need to communicate their findings to others.
Data mining:
Big data can be used to discover patterns and trends in data that would not be visible with smaller datasets. This can be helpful for businesses that need to identify new opportunities, or for research organizations that need to make new discoveries.
Data modeling:
Big data can be used to build models that can predict future behavior. This can be helpful for businesses that need to make predictions about customer behavior, or for research organizations that need to make predictions about the future.
Data science:
Big data is a key component of data science, which is the field of study that deals with the collection, analysis, and interpretation of large datasets. Data scientists use big data to solve a wide variety of problems, such as fraud detection, risk management, and customer segmentation.
Machine learning:
Big data is a key component of machine learning, which is a type of artificial intelligence (AI) that allows computers to learn without being explicitly programmed. Machine learning algorithms can be used to analyze large datasets to identify patterns and make predictions.
Artificial intelligence:
Big data is a key component of artificial intelligence (AI), which is a branch of computer science that deals with the creation of intelligent agents, which are systems that can reason, learn, and act autonomously. AI can be used to solve a wide variety of problems, such as fraud detection, risk management, and customer segmentation.
Natural language processing:
Big data is a key component of natural language processing (NLP), which is a field of computer science that deals with the interaction between computers and human (natural) languages. NLP can be used to analyze large amounts of text data to extract information, such as sentiment analysis, topic modeling, and named entity recognition.
Sentiment analysis:
Big data can be used to analyze sentiment in text data, such as social media posts, product reviews, and customer feedback. This can be helpful for businesses to understand how customers feel about their products and services, and to identify areas where they can improve.
Image recognition:
Big data can be used to train image recognition algorithms, which can be used to identify objects in images. This can be helpful for businesses to automate tasks such as product tagging and quality control.
Speech recognition:
Big data can be used to train speech recognition algorithms, which can be used to transcribe audio recordings. This can be helpful for businesses to automate tasks such as customer service and transcription.
Recommendation systems:
Big data can be used to build recommendation systems, which can suggest products or services to users based on their past behavior. This can be helpful for businesses to increase sales and improve customer satisfaction.
Fraud detection:
Big data can be used to detect fraud, such as credit card fraud and insurance fraud. This can help businesses to protect themselves from financial losses.
Risk management:
Big data can be used to assess risk, such as the risk of a customer defaulting on a loan or the risk of a natural disaster. This can help businesses to make informed decisions about how to manage their risks.
Security:
Big data can be used to improve security by identifying and preventing cyberattacks. This can help businesses to protect their data and systems from unauthorized access.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Five software recommendations for GIS tiling technology
Austin -
Dec 11
Astro & Strapi Website Tutorial: Part 3 - Project Build
Theodore Kelechukwu Onyejiaku -
Dec 11
Day7 Task:
AmalaReegan -
Dec 11
Zanurz się w ciszy: Sztuka wyciszania ścian w Twoim domu
Drejer Tonnesen -
Dec 11
Subham
Follow
I love Coffee ᵕ̈☕︎
Pronouns
Subham
Joined
Nov 23, 2022
More from
Subham
OOPS in JAVA - Ultimate
#
javascript
#
java
#
webdev
#
beginners
Is Redux Dead? Why I Kicked Redux Out of Our SaaS App
#
react
#
nextjs
#
webdev
#
javascript
OOPS in JS - Ultimate
#
javascript
#
webdev
#
beginners
#
tutorial
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Challenges and Solutions in Implementing AI for Software Testing - DEV Community,"Challenges and Solutions in Implementing AI for Software Testing - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Anil @ C Simplify IT
Posted on
Nov 4
Challenges and Solutions in Implementing AI for Software Testing
#
testing
The integration of artificial intelligence (AI) into software testing is transforming the software development landscape, promising efficiency, accuracy, and speed. However, incorporating AI-driven testing methods isn't without obstacles. Organizations encounter various challenges, from data quality issues to skill gaps, that complicate AI adoption. In this article, we’ll explore these common challenges and outline strategies to overcome them, encouraging a proactive approach to harnessing the full potential of AI in software testing.
Introduction: Common Obstacles in AI Adoption for Testing
AI has proven to be a powerful tool for identifying bugs, predicting defects, and automating test case generation. Yet, deploying AI in software testing requires careful planning and preparation, as it impacts multiple areas, including data handling, system integration, and team skills. For many organizations, these hurdles can slow or stall AI implementation, despite its evident benefits. Understanding these obstacles and knowing how to mitigate them is key to a successful AI adoption strategy.
Challenge 1: Data Quality and Availability
Data is the fuel that powers AI. However, achieving high-quality, representative data for AI testing is one of the primary challenges. Test data needs to be comprehensive and accurate to train AI models effectively. Low-quality data can lead to poor AI performance, resulting in inaccurate test results and untrustworthy predictions. Common data-related issues include:
Inadequate Data Quantity: AI models require large datasets to learn from diverse scenarios. In software testing, gathering such extensive data can be challenging, especially when testing new software or applications with limited usage history.
Unlabeled Data: AI relies on labeled data to classify and predict outcomes. In testing, obtaining labeled data often requires manual tagging, which can be time-consuming and resource-intensive.
Data Privacy and Security Concerns: Some applications, especially those handling sensitive user information, face regulatory constraints in using real-world data. Synthetic data can be a solution, but creating synthetic data that accurately mimics real-world conditions poses its own set of challenges.
Challenge 2: Integration with Existing Systems
Integrating AI-driven testing tools with legacy systems can be complex and costly. Existing test management systems and tools may not be compatible with new AI-powered solutions, and this can lead to several problems:
Compatibility Issues: Many traditional testing systems were not designed with AI in mind, which can make seamless integration difficult. These systems may lack the APIs or flexible architectures necessary for interoperability.
Infrastructure Limitations: Legacy infrastructure may struggle to support the computational needs of AI algorithms, leading to performance bottlenecks. AI testing tools often require substantial processing power, which may not be available in all environments.
Change Management: Integrating AI into existing workflows can disrupt established testing processes. Organizations may face resistance from teams accustomed to traditional methods, slowing the adoption of AI and impacting productivity.
Challenge 3: Skill Gaps in Teams
The demand for AI expertise is growing rapidly, yet there remains a significant skill gap in the workforce. Most software testing teams have limited experience with machine learning or data science, leading to several challenges:
Limited AI Knowledge: Effective AI testing requires understanding how machine learning models work, which data to use, and how to interpret AI-generated insights. Traditional testers may lack the required skills and may need extensive training.
Tool Familiarity: AI-based testing tools require specific expertise. For instance, using tools that incorporate natural language processing or predictive modeling may require advanced knowledge that testing teams might not possess.
Ongoing Learning Requirements: AI is evolving quickly, and staying up-to-date requires continuous learning. This can be difficult for teams balancing daily testing workloads with skill development.
Solutions: Strategies to Overcome These Challenges
Despite these challenges, adopting AI for software testing is achievable with a structured approach. Here are some effective strategies to tackle these obstacles:
Improving Data Quality and Availability
Utilize Synthetic Data Generation: To address data scarcity and privacy concerns, synthetic data generation can help create datasets that mimic real-world data without exposing sensitive information. Tools that automatically generate diverse test cases can improve data quality and ensure that AI models are exposed to a range of scenarios.
Invest in Data Labeling Solutions: Automated data labeling tools can speed up the data preparation process and reduce the need for manual tagging. Leveraging crowdsourced data labeling platforms or dedicated AI data-labeling services can also be a viable option.
Establish Data Quality Standards: Establish guidelines to regularly review and clean data to ensure it remains relevant and accurate. Incorporating quality control checks, such as using algorithms to detect anomalies in data, helps maintain a high-quality data pipeline.
Streamlining Integration with Existing Systems
Use AI-Enabled Testing Platforms with Built-In Compatibility: Several modern AI-powered testing tools are designed with compatibility in mind, offering APIs and integration options that support legacy systems. Opt for tools with a modular approach, allowing integration with minimal disruption to existing workflows.
Upgrade Infrastructure Gradually: Organizations can consider cloud-based testing environments to handle the computational requirements of AI. By scaling up infrastructure incrementally, companies can avoid upfront investments and pay for resources based on actual usage.
Promote Incremental Adoption: Implement AI in stages to allow teams to adjust gradually. Start with non-critical systems or applications to familiarize teams with AI-based testing before moving to core systems. This staged approach minimizes risks associated with large-scale disruption and helps gain team buy-in.
Bridging Skill Gaps in Teams
Invest in Training Programs: Offering training programs focused on AI fundamentals, machine learning, and AI-based testing tools can help close the skills gap. Online courses and certifications can empower testing teams to learn at their own pace.
Partner with AI Experts: Hiring AI specialists or collaborating with third-party experts can provide the necessary support while teams upskill. Consultants or AI engineers can oversee initial implementation phases, while testing teams gradually develop their AI expertise.
Foster a Culture of Continuous Learning: Encourage a culture of ongoing education and upskilling. Designate time for training sessions or knowledge-sharing workshops, and provide resources for continuous learning, such as online tutorials and AI-focused seminars.
Conclusion: Encouraging Proactive Adoption
While implementing AI for software testing presents several challenges, these obstacles are not insurmountable. By addressing data quality issues, prioritizing integration compatibility, and closing skill gaps within teams, organizations can unlock the full potential of AI in testing. Adopting AI in stages, training employees, and utilizing advanced tools designed for interoperability are all proactive steps that can smooth the transition to AI-driven testing.
Organizations that approach AI adoption strategically will be better positioned to capitalize on the numerous benefits it offers: increased test accuracy, faster release cycles, and ultimately, higher-quality software products. Embracing AI in software testing not only enhances the development process but also strengthens the organization’s ability to adapt to the rapid pace of technological advancement in the software industry.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Top 10 WebSocket Testing Tools for Real-Time Apps (Updated 2024)
Emmanuel Mumba -
Nov 18
From Cypress to Playwright - Saleor’s Voyage
Michalina Graczyk -
Nov 29
Top 5 Alternatives to Playwright for Cross Browser Testing
satyaprakash behera -
Nov 29
Why Shift Testing Left Part 2: QA Does More After Devs Run Tests
Signadot -
Oct 25
Anil @ C Simplify IT
Follow
Joined
Nov 1, 2024
Trending on
DEV Community
Hot
Meme Monday
#
discuss
#
jokes
#
watercooler
What was your win this week?
#
weeklyretro
#
discuss
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Passed AWS Solutions Architect Associate - DEV Community,"Passed AWS Solutions Architect Associate - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Rishab Kumar
Posted on
Apr 22, 2020
• Edited on
Jul 19, 2020
Passed AWS Solutions Architect Associate
#
aws
#
beginners
#
career
AWS Certified Challange (3 Part Series)
1
Passed AWS Cloud Practitioner, preparing for AWS SAA ♥
2
Passed AWS Solutions Architect Associate
3
Passed the AWS Certified Developer Associate
Tick tock. Tick tock.🕐
As the clock ticks on, with a deafening silence we have never known before, like many of us, I didn't want to sit and count those seconds scrolling by.
Time ain't really money (I mean we should have been rich by that logic long time back) but it sure is an opportunity.
So, I decided to put this period of lockdown to unlock more of my potential, by diving into the clouds!
The World of Big Data and Cloud Services
By the end of this sentence, millions and billions bytes of data would have been sent and received. And every time that happens, there are curious minds waiting to listen to what the data is saying.
This ever growing avenue led to the concept of Big Data and Cloud Services, which is becoming not just a capability, but a necessity in all fields today.
Cloud is the Captain America version of Steve Rogers, basically on-premise services on steroids.
Like a nineties kid, I watched the big bang of bits and bytes and revolution of internet.
The next wave is gonna be of big data and cloud services, and this time I want to be right at the centre of it rather than being a kid staring at it with jaw dropping to the floor.
Becoming an AWS Ninja!
When I decided to jump in the world of cloud services, the first question was to go with which tech on offer?
Simple. I chose AWS.
Why? 🤔
Well, it's like asking why Tesla is the talk of the town and not GM anymore.
Gartner's famous Magic Quadrant released in July 2019 paints a clear picture that AWS is the lead runner in race of cloud technology.
Azure and Google are definitely catching up, but so is GM and Ford for Tesla (not yet there!)
Even if you are working towards a promotion, or churn some more dollars, AWS is the skill to have right now.
My journey of AWS Certifications 🚴‍♀️:
Step 1:
My baby steps on the journey to becoming a certified developer on AWS started with the AWS Cloud Practitioner certification back in November 2019.
So, if you are still working towards that, this article might not be for you. You can check this article out to ace the Practitioner certification.
Step 2:
From thereon, the next thing you need to do is practice. Try to experience AWS first hand. Explore around, learn new stuff and implement what all you need.
For me, working in an environment where the work revolves around cloud services was a blessing as I spent good part of the last year honing my skills on the platform before feeling ready to step up the game.
You can also use the
Free Tier
, Amazon provides on AWS to further brush your skills before you jump into the advance stuff.
Step 3
Resources I used 📘:
As much as one would love to have a single source for all there is to learn, more often than not we need to club multiple sources to get the most out of them.
For AWS,
ACloud Guru's Solutions Architect Associate
course and another one by
Andrew Brown on freecodecamp's youtube channel
worked the best for me.
Step 4
One of the biggest add on for me, was to go through the FAQ's provided by AWS for service's like
S3
,
EC2
,
Load Balancers
,
VPC
and
Route53
.
This will help to clear all your doubts and get you better prepared for the test.
Step 5: Practice Tests
Nothing sets you up nicely for the real test than the practice tests.
It might be boring, but believe me they will go long way as a reality check for you.
Whizlabs
provides the best tests out there at a minimal cost at $17(may have changed). The 7 practice tests are the seven steps to clearing your test.
In case you have passed the previous AWS certificate for Cloud Practitioner, you are provided with a free practice test and 50% off for the exam fee as well.
Step 6: Support
I would recommend to join the
#AWSCertified
challenge and join the
Discord server
.
I am available for any doubt clarification or any other stuff you might need.
Step 7: Notes
If you just want to refresh your concepts or want to just go through concise notes before the tests, I have posted them
here
for your reference.
Step 8: The T-Day
Well, if you are here, it means you are ready for the test!
Just fill up for the test and in no time you will be a certified AWS Solution Architect.
I took around 15 days to brush up my skills, go through practice tests before I went ahead to ace my test.
It might take a little longer/shorter for you, but following these steps will certainly set you up for the test.
Good Luck 👍
AWS Certified Challange (3 Part Series)
1
Passed AWS Cloud Practitioner, preparing for AWS SAA ♥
2
Passed AWS Solutions Architect Associate
3
Passed the AWS Certified Developer Associate
Top comments
(13)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Chris Ford
Chris Ford
Chris Ford
Follow
Devops ronin.
Location
Keller TX
Work
Junior Cloud Engineer
Joined
Apr 8, 2020
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Congrats Rishab! I passed my solutions architect last month, and I followed a path similar to yours. I primarily used Linux Academy, and the practice test they provide. Shooting for SysOps next.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Rishab Kumar
Rishab Kumar
Rishab Kumar
Follow
Staff Developer Evangelist at Twilio
Location
Ontario, Canada
Education
St. Lawrence College
Work
Staff Developer Evangelist at Twilio
Joined
Nov 26, 2019
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Congratulations Chris! And good luck for the SysOps, let me know how it goes, I am planning to go with Developer Associate.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Tapan
Tapan
Tapan
Follow
Joined
May 30, 2019
•
May 28 '20
Dropdown menu
Copy link
Hide
Hi Rishab, what are you referring for Developer Associate preparation?
Like comment:
Like comment:
1
like
Like
Thread
Thread
Rishab Kumar
Rishab Kumar
Rishab Kumar
Follow
Staff Developer Evangelist at Twilio
Location
Ontario, Canada
Education
St. Lawrence College
Work
Staff Developer Evangelist at Twilio
Joined
Nov 26, 2019
•
May 28 '20
Dropdown menu
Copy link
Hide
Hey Tapan,
I went through these courses :
Andrew's course on Youtube
Stephane's course on Udemy for Developer Associate
Like comment:
Like comment:
4
likes
Like
Thread
Thread
Tapan
Tapan
Tapan
Follow
Joined
May 30, 2019
•
May 28 '20
Dropdown menu
Copy link
Hide
Thanks
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Andrew Brown 🇨🇦
Andrew Brown 🇨🇦
Andrew Brown 🇨🇦
Follow
I make free cloud certification courses
Email
andrew@exampro.co
Location
Schreiber
Education
Starfleet Academy
Work
CEO at ExamPro
Joined
Oct 19, 2018
•
Apr 22 '20
Dropdown menu
Copy link
Hide
🔥🔥 Congratulations Rishab! 🔥🔥
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Rishab Kumar
Rishab Kumar
Rishab Kumar
Follow
Staff Developer Evangelist at Twilio
Location
Ontario, Canada
Education
St. Lawrence College
Work
Staff Developer Evangelist at Twilio
Joined
Nov 26, 2019
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Thanks Andrew!
And thanks for the course!❤️
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
noahwilliam43546
noahwilliam43546
noahwilliam43546
Follow
Education
higher
Work
Memeber
Joined
Oct 14, 2021
•
Oct 21 '21
Dropdown menu
Copy link
Hide
This was very interesting but I say you, AWS Certified Solutions Architect – Associate certification exam question help pass the exam on the first attempt. The exam preparation material is available in PDF questions, desktop & web-based App. You can also take a free demo of
AWS Certified Solutions Architect – Associate certification exam question
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
pavlo kozub
pavlo kozub
pavlo kozub
Follow
I am a software engineer and I very like AWS :)
Location
Dnipro
Work
Software Engineer
Joined
Apr 22, 2020
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Rishab, thank you for the new kick my ass. I learned half of cloud.guru course at the last fall. And stopped. Big mistake from my side. Thank you for the topic - will do another attempt :)
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Rishab Kumar
Rishab Kumar
Rishab Kumar
Follow
Staff Developer Evangelist at Twilio
Location
Ontario, Canada
Education
St. Lawrence College
Work
Staff Developer Evangelist at Twilio
Joined
Nov 26, 2019
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Good luck pavlo! Let me know how it goes! 👍
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
KB
KB
KB
Follow
Location
Hamburg, Germany
Work
Senior Frontend Developer at Freelance
Joined
Mar 27, 2020
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Congrats and free time we'll spend 👍👍🍺
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Sammy J
Sammy J
Sammy J
Follow
🌼Stay-at-home mom turned Cloud Developer
Location
Thunder Bay
Work
Cloud Developer Advocate at ExamPro
Joined
Apr 22, 2020
•
Apr 22 '20
Dropdown menu
Copy link
Hide
Congrats!!🎉 🎉
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Rishab Kumar
Rishab Kumar
Rishab Kumar
Follow
Staff Developer Evangelist at Twilio
Location
Ontario, Canada
Education
St. Lawrence College
Work
Staff Developer Evangelist at Twilio
Joined
Nov 26, 2019
•
Apr 23 '20
Dropdown menu
Copy link
Hide
Thanks!
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
View full discussion (13 comments)
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Axios Or Fetch in NextJs
taki089.dang -
Dec 11
Create Your First AI-Powered C# App with Semantic Kernel: A Step-by-Step Guide
CodeStreet -
Dec 10
Every Student Needs to Join a Tech Community
Iqra Firdose  -
Dec 10
Day 2: Exploring GSAP ScrollTrigger and Advanced Animations 🚀
Ashish prajapati -
Dec 10
Rishab Kumar
Follow
Staff Developer Evangelist at Twilio
Location
Ontario, Canada
Education
St. Lawrence College
Work
Staff Developer Evangelist at Twilio
Joined
Nov 26, 2019
More from
Rishab Kumar
The Cloud Resume Challenge - Beginner Cloud Project
#
cloud
#
aws
#
azure
#
gcp
How I passed the AWS DevOps Engineer Professional Exam
#
aws
#
cloud
#
certification
Deploying a static website to AWS with Pulumi
#
aws
#
devops
#
cloud
#
iac
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Starting your Journey with Big Data Analytics - DEV Community,"Starting your Journey with Big Data Analytics - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Adit Modi
for
Cloud Tech
Posted on
Mar 10, 2021
• Edited on
Apr 23, 2021
Starting your Journey with Big Data Analytics
#
aws
#
bigdata
#
career
#
beginners
I started my Journey with Big Data in June 2019. I started learning about Big Data through Blogs and Articles . I was really Interested in Learning Hadoop Framework which is needed in order to understand Big Data. I was also Interested in Spark and hadoop .
My Background: Cloud and Big Data Enthusiast | 4x AWS Certified | 3x Oracle Cloud Certifed | 3x Azure Certified | Big Data Certified . I have recently completed 8+ Hadoop & Spark Courses from IBM & achieved badges . I am working from the past one year on AWS Services that make use of Big Data Technologies.
What is Big Data ?
Big Data is also data but with a huge size. Big Data is a term used to describe a collection of data that is huge in volume and yet growing exponentially with time. In short such data is so large and complex that none of the traditional data management tools are able to store it or process it efficiently.
What is big data used for?
Big data has been used in the industry to provide customer insights for transparent and simpler products, by analyzing and predicting customer behavior through data derived from social media, GPS-enabled devices, and CCTV footage. The big data also allows for better customer retention from insurance companies.
what is hadoop in big data ?
Hadoop is an open-source software framework used for storing and processing Big Data in a distributed manner on large clusters of commodity hardware. … Hadoop was developed, based on the paper written by Google on the MapReduce system and it applies concepts of functional programming.
Some Helpful Skill Sets for Learning Hadoop for Beginners
this skills are mandatory but not compulsory for learning.
· Linux Operating System
· Programming Skills
· SQL Knowledge
How to start working with Hadoop ?
· Watch Webinars and Documentation available on Internet.
· Get yourself acquainted with the underlying architecture of Hadoop. To do that try to understand how the components like HDFS, MapReduce and Yarn works in the architecture.
· There are a lot of good books available in the market that can help you at all stages. Books like Hadoop — the Definitive Guide works like the bible of Hadoop for the beginners.
· Join a course
· Follow a certification path
what is Spark in Big Data ?
Like Hadoop, Spark is open-source and under the wing of the Apache Software Foundation. Spark is seen by techies in the industry as a more advanced product than Hadoop — it is newer, and designed to work by processing data in chunks “in memory”. Additionally, Spark has proven itself to be highly suited to Machine Learning applications. Unlike Hadoop, Spark does not come with its own file system — instead it can be integrated with many file systems including Hadoop’s HDFS, MongoDB and Amazon’s S3 system.
How to start working with Spark ?
key features of Spark include:
Currently provides APIs in Scala, Java, and Python, with support for other languages (such as R) on the way
Integrates well with the Hadoop ecosystem and data sources (HDFS, Amazon S3, Hive, HBase, Cassandra, etc.)
Can run on clusters managed by Hadoop YARN or Apache Mesos, and can also run standalone.
Get yourself acquainted with the features of Spark .
Spark Core
is the base engine for large-scale parallel and distributed data processing.
SparkSQL
is a Spark component that supports querying data either via SQL or via the Hive Query Language. It originated as the Apache Hive port to run on top of Spark (in place of MapReduce) and is now integrated with the Spark stack.
Spark Streaming
supports real time processing of streaming data, such as production web server log files (e.g. Apache Flume and HDFS/S3), social media like Twitter, and various messaging queues like Kafka.
MLlib
is a machine learning library that provides various algorithms designed to scale out on a cluster for classification, regression, clustering, collaborative filtering and so on.
GraphX
is a library for manipulating graphs and performing graph-parallel operations.
you can learn more on the basics of Apache Spark from ( Radek Ostrowski’s Blog ).
Once the Concepts are Clear , to Boost your Concepts Enroll in a Course . Follow the learning path to complete the course and getting practice and hands-on regarding the concepts the course has to offer.
My Experience of learning hadoop framework and Spark Technology
has been great . From being a beginner to being a Big Data Geek , the Journey has been amazing . People feel that learning a new technology must be difficult but when you are interested in a Technology like Big Data , you never stop and keep learning and keep improving your skills while developing projects to test your knowledge. I have completed several courses of big data including some offered by Coursera and UCSD and very popular Udemy Course by Frank Kane and practiced big data on AWS for a very long time . I have also achieved IBM badges for passing Hadoop Courses offered by cognitiveclass .
My Suggestion for people starting their journey into Big Data
and mainly Hadoop is Start from the basics , know what are the basic terminologies involved in hadoop , Study the Core Concepts preferably with help of a course and once familiar with the concepts and having through Knowledge , practice the Concepts by making small projects to master the basic concepts.
Hope all of you Understood the Big Data Analytics Using Hadoop and Spark and In case of any queries or doubts
feel free to reach out to me on
LinkedIn.
You can view my badges
here.
I also am working on various AWS Services and Developing various AWS and Big Data Projects.
Do Follow me on
Github
to get Knowledge of Some Big Data Projects Using AWS and Hadoop Framework.
If you liked this content then do clap and share it . Thank You .
To conclude, learning any technology is a journey, not a destination. Hence, you should have persistence and motivation to walk in this challenging world of technology.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Financial Post Mortem For The Commit Your Code Conference
Danny Thompson -
Dec 9
🚀 Introducing Amazon Aurora DSQL: The Next Evolution in Databases 📊
Sarvar Nadaf -
Dec 10
JavaScript Object Destructuring
Tyn -
Dec 10
Architecting AWS with Terraform Series.
Erick Okal -
Dec 10
Cloud Tech
Follow
Everything About Cloud &amp; Tech
We share latest news and articles related to Different Cloud Providers like AWS , Azure , GCP etc and much more.
Visit CloudTech
More from
Cloud Tech
Best practices for ML lifecycle stages
#
aws
#
machinelearning
#
datascience
#
beginners
Machine Learning Lifecycle Process
#
machinelearning
#
datascience
#
bigdata
#
beginners
Welcome to DEV, CLOUD TECH!
#
beginners
#
career
#
codenewbie
#
computerscience
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Data Analytics at Potloc I: Making data integrity your priority with Elementary & Meltano - DEV Community,"Data Analytics at Potloc I: Making data integrity your priority with Elementary & Meltano - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Stéphane Burwash
for
Potloc
Posted on
Jan 6, 2023
• Edited on
Jan 9, 2023
Data Analytics at Potloc I: Making data integrity your priority with Elementary & Meltano
#
meltano
#
dbt
#
dataquality
#
sql
Foreword
This is the first of a series of small blog posts where we describe plugins that our data engineering team at
Potloc
developed in order to solve business issues that we were facing.
These features were developed to enhance our current stack, which consists of:
Meltano
as our DataOps platform + Extract / Load tool
dbt
as our data transformation tool
Airflow
Bigquery
as our data warehouse
AWS Fargate
as our hosting infrastructure, managed through
terraform
In this article, it is assumed that the reader has basic knowledge of:
Meltano
dbt
We hope that you enjoy the article! If you have any questions, feel free to reach out.
This article was not sponsored by Elementary in any way, we're just big fans 😉.
Data Integrity - It's more than a buzzword
Data integrity is an essential component of a data pipeline. Without integrity and trust, your pipeline is essentially worthless, and those hundreds of hours you spent integrating new sources, optimizing load times, and modeling raw data into usable insights go down the drain. More than once, our data team has created ""production ready"" dashboards, only to realise that the integrity / buisness logic behind the dashboard was completely flawed. What was supposed to be a 2 day project became a 3 week debacle.
To circumvent falling into the data quality trap, you can write tests using a number of powerful open-source data integrity solutions such as
Great Expectations
,
Soda Core
or even natively using
dbt tests
to validate that your data is doing what it's supposed to. But as you write more and more tests, you can start running into some issues, mainly:
Tracking over time
: How do you keep track of test results over time? How are you progressing in terms of tackling these issues?
This was the starting point for our quest to find a long-term data integrity solution at Potloc. We were attempting to map integrity issues in user-inputted data. We also wanted to give our team an accurate report of their progress as they resolved these issues one-by-one in the source data.
Readability
: As you go from 5 to 50 to 500 to 5000+ tests, reading results becomes exponentially more complicated and time-intensive.
Reproducibility
: Once you have detected an integrity issue, how do you quickly reproduce the test to be able to investigate?
Unknown unknowns
: While it is possible to test for every known possible issue, it can be harder / impossible to test for unknown unknowns such as dataset shifts, anomalies, large spikes in row count, etc.
These issues can be circumvented by integrating
Elementary
into your workflow.
Features
Elementary
is an open-source tool for data observability and validation that wraps around an existing dbt project. It allows users to graduate from simply
having
integrity tests to
using
them in order to improve confidence in your data product.
Here are only some of the reasons why I personally love Elementary:
1. The UI, the glorious UI:
Elementary and its associated CLI (command line interface)
edr
natively allow you to
generate a static HTML file containing test results
. This file can either be viewed locally, sent through slack or even hosted in a cloud service to be viewed as a webpage.
From this UI, you can view your most recent test results, historical test results, check model run times and even view lineage graphs.
If you have any failures in tests run, you can view samples of offending entries or copy the SQL query that generated these errors to quickly investigate.
You can play around with Elementary's
demo project
to get a feel for it.
2. Stored results and historical views
Elementary integrates with your dbt project in order to store all test runs and uses
on-run-end
hooks to upload results. This all happens on the dbt package, without need to connect to the data warehouse.
This allows us to view test results over time, view progress from run to run, and
use test results for internal reporting
.
It can sometimes be hard to share integrity metrics with the rest of your non-data-literate team. Having easy access to results & metrics such as row count directly in your warehouse allows you to create integrity dashboards curated for business use cases, giving your team the opportunity to start tackling issues and take stock of their progress.
3. Anomaly detection tests
As mentioned above, it is hard to deal with unknown unknowns and issues that arise over longer periods of time (days, weeks, or even months). Even if your data is clean when your model first goes into production, it does not mean that mistakes/issues cannot slip in as time goes on. A supported table requires constant monitoring, especially if business logic has been hard-coded.
This task can be greatly alleviated by making use of Elementary's native
anomaly detection tests
, which monitor for shifts at the table and column level for metrics such as:
Row count
Null count
Percentage of missing
Freshness
Etc.
A full list of all anomaly metrics Elementary tests for can be found
here
.
By basing itself on
past results
rather than hard-coded baselines (ex: an increase of 10% in row count or half-day delay in freshness), Elementary can easily be integrated out-of-the-box without needing to fine-tune from pipeline to pipeline.
At Potloc, we mainly use this feature to identify
freshness issues
. Elementary allows you to easily setup freshness checks
without having to specify hard deadlines
(ex: 12h since last sync, 24h since last updated, etc.). This means that we can change our upstream Extract/Load job schedules without having to change our downstream tests; the tool will automatically flag the issue, and then adapt to the new schedule as it becomes norm. This also makes the intial setup is quick and painless.
Integrating Elementary into your existing Meltano Project
We developed an
Elementary plugin
for Meltano using the Meltano EDK that can easily be integrated into your project.
To add it, simply run:
meltano add utiliy elementary
Enter fullscreen mode
Exit fullscreen mode
This should add the following code snippet to your
meltano.yml
file:
- name: elementary
    variant: elementary
    pip_url: elementary-data==<EDR VERSION> git+https://github.com/potloc/elementary-ext.git
Enter fullscreen mode
Exit fullscreen mode
You will also need to add the following snippet to your
packages.yml
file:
packages:
  - package: elementary-data/elementary
    version: <DBT PACKAGE VERSION>
    ## Docs: <https://docs.elementary-data.com>
Enter fullscreen mode
Exit fullscreen mode
As you can see, we have 2 elements we now need to complete:
EDR version
dbt package version
Both of the can be found in the elementary
quickstart documentation
or in their respective package indexes (
pypi
and
dbt packages
).
It is important that
both of these versions are aligned in accordance with creator releases
. If CLI and dbt package versions are misaligned, errors can ensue.
At the time of writing this article, we would be using
EDR VERSION = 0.63
&
DBT PACKAGE VERSION = 0.66
Note:
We will be working on making this process easier so that you do not need to specify package versions.*
Next, you need to set all of your environment variables for Elementary so that they use the same as your existing dbt project. A typical setup could look like this:
- name: elementary
        namespace: elementary
        pip_url: elementary-data[platform]==0.6.3 git+https://github.com/potloc/elementary-ext.git
        executable: elementary_invoker
        settings:
        - name: project_dir
          kind: string
          value: ${MELTANO_PROJECT_ROOT}/transform/
        - name: profiles_dir
          kind: string
          value: ${MELTANO_PROJECT_ROOT}/transform/profiles/platform/
        - name: file_path
          kind: string
          value: ${MELTANO_PROJECT_ROOT}/path/to/report.html
        - name: skip_pre_invoke
          env: ELEMENTARY_EXT_SKIP_PRE_INVOKE
          kind: boolean
          value: true
          description: Whether to skip pre-invoke hooks which automatically run dbt clean and deps
        - name: slack-token
          kind: password
        - name: slack-channel-name
          kind: string
          value: elementary-notifs
        config:
          profiles-dir: ${MELTANO_PROJECT_ROOT}/transform/profiles/platform/
          file-path: ${MELTANO_PROJECT_ROOT}/path/to/report.html
          slack-channel-name: your_channel_name
          skip_pre_invoke: true
        commands:

          initialize:
            args: initialize
            executable: elementary_extension
          describe:
            args: describe
            executable: elementary_extension
          monitor-report:
            args: monitor-report
            executable: elementary_extension
          monitor-send-report:
            args: monitor-send-report
            executable: elementary_extension
Enter fullscreen mode
Exit fullscreen mode
Make sure to
specify the platform
, which should be specified in your profile (we use bigquery).
After this, simply follow the instructions in the
Elementary Quickstart Guide
to get the plugin up and running.
Generating your first report
Once you have got elementary up and running, it's time to generate your first report. Simply run the command
meltano invoke elementary:monitor-report
Enter fullscreen mode
Exit fullscreen mode
and a brand new report should be generated at the specified
file-path
(
${MELTANO_PROJECT_ROOT}/path/to/report.html
in our case).
Next steps
Once you've generated your first report, the sky is the limit in terms of integrating elementary to your workflow.
The Elementary team has made it incredibly easy to send a report as a slack message. At Potloc, we receive reports twice a day to monitor the state of our pipeline.
You can also set up hosting for your report on s3 or send slack alerts when an error occurs. Experiment and find what best works for you!
A quick closing statement
While incredibly powerful, Elementary
is not a replacement for best practices
.
When writing tests, ensure that they are
pertinent
and
targeted
.
Tests should be written to identify data integrity issues that can compromise business insights, not simply to identify null values.
If you write too many tests without thinking of the meaning behind them, you run the risk of falling into the
""too many errors = no errors""
paradigm where you have so many warnings that it's impossible to differentiate between actual issues and unfixable noise.
I speak from experience; at one point, we had multiple tests in our pipeline that returned a warning of over
5 000 erroneous values
, with one going up to
180 000
. These errors were unactionable, and yet the tests remained. Even with Elementary, this made it hard for us to differentiate between useless warnings and
actual integrity issues
that needed to be resolved.
Make sure to reach out to the Elementary team if you have any questions about their product!
Interested in what we do at
Potloc
? Come join us!
We are hiring
🚀
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Day 2 of SQL Series || Basic Commands
Akshat Sharma -
Nov 23
Data Architecture Best Practices
DQOps -
Nov 23
True Alphanumeric / natural sorting in MySQL - why is the answer always recursion?
GrahamTheDev -
Nov 17
Populating a Vertica Database with Random Data
Dmitry Romanoff -
Oct 19
Potloc
Follow
Trending on
DEV Community
Hot
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
8 Type of Load Balancing
#
discuss
#
webdev
#
cloudcomputing
#
programming
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
"Spark: unit, integration and end-to-end tests. - DEV Community","Spark: unit, integration and end-to-end tests. - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Gustavo Martin Morcuende
for
Adevinta Spain
Posted on
Oct 15, 2020
Spark: unit, integration and end-to-end tests.
#
scala
#
spark
#
testing
Introduction
At Adevinta Spain we are building a Data Platform where we use multiple applications and frameworks some of them based on Spark.
In order to increase the quality of our Spark applications we wanted to run tests in the same way as we did with other frameworks. What means, we wanted to be able to run unit, integration and end-to-end tests.
This article explains the way Spark tests are run at Adevinta Spain. Hopefully, it will be useful for other big data developers searching ways to improve the quality of their code and at the same time their CI pipelines.
Unit, integration and end-to-end tests.
When working with Spark, developers usually will be facing the need of implementing these kinds of tests. Other tests like smoke tests, acceptance tests, etc, etc are outside the scope of this article so I will not be mentioning them.
Unit Tests:
at this level we will be dealing with code that does not require a Spark Session in order to work. Also, this kind of code does not talk with the outside world.
Integration Tests:
at some point we will need to use a Spark Session. At this level we will be testing Spark transformations and in many cases we will have to deal with external systems such as databases, Kafka clusters, etc, etc.
End-to-end Tests:
our application probably will be composed of several Spark transformations working together in order to implement some feature required by some user. Here, we will be testing the whole application.
Spark project layout
This is a typical scala project layout. I think this layout should work under any use case but if it does not work for you, at least I hope, it will bring some inspiration or ideas to your testing implementation.
src/
├── main
│   └── scala
│       └── example
│           ├── app
│           │   └── AwesomeApp.scala
│           ├── job
│           │   └── AwesomeJob.scala
│           └── service
│               └── AwesomeService.scala
└── test
    ├── resources
    │   ├── awesomejob
    │   │   └── sourcepath
    │   │       └── awesome.json
    │   └── log4j.properties
    └── scala
        └── example
            ├── app
            │   └── AwesomeAppEndToEndTest.scala
            ├── job
            │   └── AwesomeJobIntegrationTest.scala
            ├── service
            │   └── AwesomeServiceTest.scala
            └── SharedSparkSessionHelper.scala
Enter fullscreen mode
Exit fullscreen mode
Application layout
app package
Under this package we will find the classes in charge of running our Spark applications. Typically we will have only one Spark application.
job package
A Spark application should implement some kind of transformations. Modules under this package run Spark jobs that require a Spark Session.
service package
Sometimes business logic does not require a Spark Session in order to work. In such cases, we can implement the logic in a different module.
Shared Spark Session
One of the biggest problems to be solved when running Spark tests is the isolation of these tests. Running a test should not affect the results of another. In order to achieve this goal we are going to need a Spark Session for each set of tests, in this way, the results of these tests will not affect others that will also require a Spark Session.
So, we need to implement a system that will enable us to run, clear and stop a Spark Session whenever we need it (before and after a set of related Spark tests)
The details of the implementation are explained down below:
beforeAll:
beforeAll
is a scala test function that runs before any other test in our class under test. We will be using this function for starting our Spark Session.
sparkConf:
sparkConf
function enables us to load different Spark Sessions with different Spark configurations.
embedded hive:
spark-warehouse
and
metastore_db
are folders used by Spark when enabling the Hive support. Different Spark Sessions in the same process can not use the same folders. Because of that, we need to create random folders in every Spark Session.
beforeEach
:
scala test function that creates a temporary path which is useful when our Spark tests end up writing results in some location.
afterEach
:
clears and resets the Spark Session at the end of every test. Also, it removes the temporary path.
afterAll
:
stops the current Spark Session after the set of tests are run. In this way we will be able to run a new Spark Session if it is needed (if there is another set of tests requiring the use of Spark)
How it works
The basic idea behind
SharedSparkSessionHelper
lies in the fact that there is one Spark Session per Java process and it is stored in an
InheritableThreadLocal
. When calling
getOrCreate
method from
SparkSession.Builder
we end up either creating a new Spark Session (and storing it in the InheritableThreadLocal) or using an existing one.
So, for example, when running an end-to-end test, because
SharedSparkSessionHelper
is loaded before anything else (by means of the
beforeAll
method), the application under test will be using the Spark Session launched by
SharedSparkSessionHelper
.
Once the test class is finished, the
afterAll
method stops the Spark Session and removes it from the
InheritableThreadLocal
leaving our test environment ready for a new Spark Session. In this way, tests using Spark can run in an isolated way.
Awesome project
This article would be nothing without a real example. Just following this
link
you will find a project with
sbt
,
scalatest
,
scalastyle
,
sbt-coverage
and
scalafmt
where I use the SharedSparkSessionHelper trait.
This application can be run in any of the available clusters that currently exist such as
Kubernetes
,
Apache Hadoop Yarn
, Spak running in
cluster mode
or any other of your choice.
Conclusion
Testing Spark applications can seem more complicated than with other frameworks not only because of the need of preparing a data set but also because of the lack of tools that allow us to automate such tests. By means of the SharedSparkSessionHelper trait we can automate our tests in an easy way.
I hope this article was useful. If you enjoy messing around with Big Data, Microservices, reverse engineering or any other computer stuff and want to share your experiences with me, just follow
me
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Join the Open-Source Revolution with AxioDB: Calling All Developers!
Ankan Saha -
Oct 6
We made a horror movie inspired by spooky prod bugs 🎃
Ivanha Paz -
Oct 31
3 useful VS Code extensions for testing Ruby code
Jokūbas Pučinskas -
Nov 8
Top 10 API Testing Tools for Testers in 2024
Wanda -
Nov 8
Adevinta Spain
Follow
OpenSource Projects of Adevinta Spain
Contribute
More from
Adevinta Spain
Testeando una Spring Boot App dockerizada
#
showdev
#
springboot
#
docker
#
testing
Usando AWS S3 en local con LocalStack
#
backend
#
aws
#
microservices
#
testing
Integrando TestContainers en el contexto de Spring en nuestros tests
#
testing
#
microservices
#
springboot
#
docker
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Airbnb-quality data for all - DEV Community,"Airbnb-quality data for all - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Jeremy Stanley
Posted on
Jan 8, 2021
Airbnb-quality data for all
#
database
#
datascience
#
testing
How to build and maintain high quality data without raising billions
Airbnb has always been a data driven company.
Back in 2015, they were laying the foundation to ensure that
data science was democratized at Airbnb
. Meanwhile, they have grown to more than 6,000 people and have raised more than $6b of venture funding.
To stay data driven through this massive change has required making big investments in data quality, as outlined by their recent Data Quality at Airbnb series:
Part 1 — Rebuilding at Scale
and
Part 2 — A New Gold Standard
.
Companies aspiring to be as data driven and successful as Airbnb will also need to prioritize data quality.
It does not matter how much data is collected, how fast it can be queried, how insightful analyses are or how intelligent a machine learning model is. If the underlying data is unreliable and of poor quality, then everything that depends upon it will also be suspect.
Fortunately, companies no longer need to reinvent the wheel or make massive investments to improve and maintain high quality data. New startups, such as ours at
Anomalo
, are building the technology needed to monitor, triage and root cause data quality issues efficiently at scale.
In their first post,
Part 1 — Rebuilding at Scale
, Airbnb set the following goals for themselves.
All of these investments are critical, and projects like
dbt
are making it easier to build high quality data pipelines, and there are
many open source data discovery tools under development
.
But in this article I want to focus on two of their goals in particular: ensuring important data meets SLAs and is trustworthy and routinely validated.
In their latest post,
Part 2 — A New Gold Standard
, Airbnb outlines the following automated data quality checks they run to validate important datasets:
To summarize, Airbnb has the following key requirements:
Row count time series analysis
Has the table row count dropped below a predicted range? Have the row counts plummeted for any key segments? Did fresh data arrive in the table on time?
Time series anomaly detection for key business metrics
Did a metric suddenly move outside of a predicted range? Is the movement unusual given seasonality, trend and holidays?
Standard run-time data quality checks
Are basic data constraints satisfied: no unexpected NULL or blank values, no violations of unique constraints, strings match expected patterns, timestamps follow a defined order, etc.?
Perfect metric consistency checks
Do columns and metrics satisfy critical relationships that can be expressed as simple equations?
For the rest of this post, we will use
open source data in BigQuery
to illustrate how each of these requirements is supported in Anomalo.
Row count time series analysis
Has the table row count dropped below a predicted range? Have the row counts plummeted for any key segments? Did fresh data arrive in the table on time?
The first question to answer for any production dataset is “has it been updated?” In particular, are there any records from the most recent date and is the row count within an expected range?
If not, then something probably broke upstream in the collection, loading or processing of the table. This must be fixed before anyone can trust the latest data.
In Anomalo, every table is automatically configured to monitor if the most recent period of data is complete.
For example, for the
NY 311 service request data
, here is the time series of row counts by day, and the predicted interval used to determine if the row counts are complete:
For November 23rd, we expected at least 5,223 rows, and in fact there were 7,056.
You can also pick any column and check that the row counts haven’t plummeted for any important segments in that column. For example, we can evaluate the row counts by borough (neighborhood) in the NY 311 data:
Anomalo customers use this feature to ensure their data is complete for a wide variety of segments. Ranging from geography (country, state) to event or API call type to device platform (iOS v Android).
Finally, we can also tell if the data was significantly delayed. This can indicate that an upstream processing stage is taking longer than normal, and may eventually cause data to be incomplete when users query for it.
Here is how long it took for each of the last 8 days of data to arrive for the New York 311 dataset:
On average, it takes around 25 hours for the New York 311 data to arrive in BigQuery, and you can easily set a threshold of when you would like to be notified for delayed data:
For more on data completeness, why it happens so often and how tools like Anomalo can be used to monitor it check out our post on
When Data Disappears
.
Time series anomaly detection for key business metrics
Did a metric suddenly move outside of a predicted range? Is the movement unusual given seasonality, trend and holidays?
Once we know that data is complete in a timely manner, the next step is to ensure that any metrics we compute from the data are within expected ranges.
In Anomalo, you can easily configure new metrics, for example, we can easily monitor the mean score for posts on the
Hacker News dataset in BigQuery
:
If the score ever moves sharply outside of the predicted interval, a notification is sent in Slack. This saves a lot of time spent watching metrics for suspicious changes.
Underneath the hood, Anomalo is building a sophisticated time series model, which decomposes the metric into an overall trend (blue), holiday spikes (orange), season of year (red) and day of week (teal) components:
It looks like hacker news scores have been trending up over time, are sensitive to holidays (Thanksgiving and Christmas look good, others bad), and are much higher on weekends. Hopefully someone posts this article on
hacker news
on a Sunday near Christmas 🎄.
In Anomalo, metrics can be defined using a variety of pre-defined aggregates:
Or you can define a custom metric using any SQL aggregate statement available in your warehouse. For example, in the
SF Fire Department service calls dataset in BigQuery
, we can measure how many minutes on average it takes for the fire department to reach the scene of a call, and be alerted whenever it takes longer than 15 minutes:
It looks like the average time for the SF Fire Department to respond to calls increased dramatically on October 19th:
To learn more about testing your data with time series and unsupervised learning models, check out our post on
dynamic data testing
.
Standard run-time data quality checks
Are basic data constraints satisfied: no unexpected NULL or blank values, no violations of unique constraints, strings match expected patterns, timestamps follow a defined order, etc.?
Ensuring that metrics are within expected ranges is important, but ultimately only tests the
surface
of your data. Even if all of your metrics are within expected tolerances, there could be cracks appearing in your data foundation.
That is where rule based data quality checks come in. These checks typically test that a condition never or always is satisfied.
For example, in Anomalo we can easily set up a number of foundational data quality checks on the
Hacker News dataset in BigQuery
. For example, to test that
id
is always unique:
We can then see at a glance which rules have passed, and which have failed:
For failing checks, such as “
timestamp
is never NULL”, we can see a summary of the issue:
A sample of bad and good records we can scan through:
And a statistical analysis identifying the segments of data where the issue is most prominent:
Depending upon the nature of the check, the summary visualization changes to provide the most useful context. In this case, it appears there are many stories with duplicate titles:
Knowing not just that the data is broken, but exactly how, where and when the issue occurs is critical to quickly triaging, root causing and fixing it.
Perfect metric consistency checks
Do columns and metrics satisfy critical relationships that can be expressed as simple equations?
Real world data is complicated. While key metric and fixed rule based checks may capture 95% of the common data quality use cases, the last 5% often require complex logic specific to an organization and dataset.
For example, in the
New York Police Department dataset of motor vehicle collisions on BigQuery
, there are multiple fields tracking how many people were injured or killed in an accident.
We can validate that the fatality counts are consistent with the following:
When run, we find that there are 0.003% of records (45 of 1,734,004 as of 2020-11-27) that have inconsistent fatality counts.
When such a validation rule fails, we can also see sample bad and good records:
In this case, it appears that there are records where motorists were killed, and yet they are not appearing in the total. Again, we also show any segments of the data that indicate where the issue is occurring most often (limited in this case to just records with some non-zero fatality rows).
In this case, when at least one of the fatality columns is non-zero this issue is most likely to happen when the
contributing_factor_vehicle
number
3
column is unspecified. This intelligence could be a meaningful hint towards identifying where and how this data quality issue arose.
Today, it is easier than ever to become a data driven organization.
The volume and diversity of data keeps growing. Warehouses like
Snowflake
and
BigQuery
make storing and querying it all easy. From BI and visualization through to machine learning, there are a
plethora of tools
that we can leverage to generate insights and create value.
But most companies have only just begun the journey to ensure the data they depend upon can be trusted. As demonstrated by Airbnb, investing in data quality is critical to
staying
data driven as data organizations mature.
At
Anomalo
, we have built a platform that will allow any company to achieve and sustain the same vision of high quality data. If you are interested in starting a trial, head to our
site to learn more
or
request a demo
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to test Edge on macOS? (with Top Methods)
Sarah Thomas -
Nov 28
Rest Assured Basics: A Beginner's Guide to Automated API Testing in Java
JigNect Technologies -
Oct 25
The Apache SeaTunnel Community Welcomes A New Committer From India!
Apache SeaTunnel -
Oct 25
How to Choose the Right Database for Your Full Stack Project
Kishan Empiric -
Nov 26
Jeremy Stanley
Follow
CTO and founder at Anomalo, previously VP Data Science at Instacart
Location
Oakland, CA
Work
CTO & Founder at Anomalo
Joined
Jan 8, 2021
Trending on
DEV Community
Hot
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
Meme Monday
#
discuss
#
jokes
#
watercooler
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Understanding DBT (Data Build Tool): An Introduction - DEV Community,"Understanding DBT (Data Build Tool): An Introduction - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Mage
Posted on
Aug 29, 2023
Understanding DBT (Data Build Tool): An Introduction
Guest blog by Shashank Mishra, Data Engineer @ Expedia
TLDR
DBT (Data Build Tool) is an open-source software tool that enables data analysts and engineers to transform and model data in the data warehouse. It simplifies the ETL process by focusing on the ‘T’ — transformation — and integrates seamlessly with modern cloud-based data platforms.
Outline
Overview of DBT
Core principles of DBT
DBT architecture
Challenges with DBT
Conclusion
Overview of DBT
DBT (Data Build Tool) is an open-source tool that has revolutionized the way data analysts and engineers view and handle data transformation and modeling in the modern data stack. Here’s an overview of DBT:
Philosophy:
Focuses on the ELT (Extract, Load, Transform) approach, leveraging modern cloud data warehouses.
Core Components:
Models: SQL queries that define data transformations.
Tests: Ensure data quality by validating models.
Snapshots: Track historical changes in data.
Documentation: Auto-generates documentation for clarity on data processes.
Development Workflow:
Developer-centric with version control (typically Git), branching, and pull requests.
Execution:
Compiles models into SQL and runs them directly on data warehouses like Snowflake, BigQuery, and Redshift.
Adapters:
Makes DBT versatile by connecting to various databases and data platforms.
(Source:
Giphy
)
Core principles of DBT
DBT (Data Build Tool) operates on a set of core principles that guide its philosophy and approach to data transformation and modeling:
Data Warehouse-Centric:
Raw data is ingested into the data warehouse, using its computational capabilities for in-database transformations. This principle capitalizes on modern warehouses like Snowflake, BigQuery, or Redshift for heavy computations.
ELT Workflow:
Instead of pre-transforming data (ETL), DBT supports ELT, where raw data is loaded into the data warehouse (Extract, Load) and then transformed using SQL-based models (Transform).
SQL as the DSL:
DBT uses SQL as its domain-specific language. This eliminates the need for proprietary transformation languages or GUI-based ETL tools, providing direct and transparent transformation logic.
Git-based Version Control:
DBT projects are typically version-controlled using Git, allowing for branch-based development, commit histories, and collaboration through pull requests.
Model Dependencies:
Models, written in SQL, can reference other models (ref() function). This creates a DAG (Directed Acyclic Graph) of dependencies, which DBT uses to run models in the correct order.
Data Testing:
DBT’s schema tests (e.g., unique, not_null, accepted_values) validate the integrity of the transformed data. Custom data tests can also be written in SQL to enforce specific business rules or constraints.
Jinja Templating:
DBT uses the Jinja2 templating engine. This allows for dynamic SQL code generation, loops, conditional logic, and macro creation for reusable SQL snippets.
CLI and API Integration:
DBT’s command-line interface (CLI) supports operations like run, test, and docs generate. It can also be integrated with CI/CD tools and other platforms through APIs.
Configurations & Hooks:
Technical configurations can be set at the project, model, or global level (dbt_project.yml). Pre- and post-hooks allow for operations (like data quality checks or audit trails) to be executed before or after a model runs.
Extensibility with Adapters:
DBT’s architecture allows for custom adapters. While it comes with adapters for popular data platforms, the community or organizations can develop adapters for other platforms, ensuring wide compatibility.
By emphasizing these technical principles and functionalities, DBT provides a powerful and flexible framework for data engineers and analysts to manage data transformations with precision and efficiency.
(Source:
Giphy
)
DBT architecture
DBT (Data Build Tool) employs a unique architecture that sets it apart from traditional ETL tools and frameworks. At its core, DBT is a command-line tool that uses SQL and Jinja2 templating to transform and model data. Let’s break down its architecture:
Command-Line Interface (CLI):
Central Control: DBT is primarily operated through its command-line interface, allowing users to run commands for transformations (dbt run), testing (dbt test), and documentation generation (dbt docs generate).
SQL + Jinja2 Templating:
Dynamic SQL Generation: By combining SQL with the Jinja2 templating engine, DBT allows for dynamic SQL code generation. This lets users incorporate loops, conditional logic, and macros into their transformation logic.
Projects and Configuration:
DBT Project: The foundational unit in DBT. It contains models, tests, snapshots, macros, and the essential dbt_project.yml configuration file.
Configuration Files: These YAML files (dbt_project.yml, profiles.yml, etc.) define project details, model configurations, and database connections.
Models & Directed Acyclic Graph (DAG):
Models: SQL files that represent the transformation logic.
DAG: DBT builds a DAG of model dependencies using the ref() function in models. The DAG determines the execution order when running transformations.
Adapters:
Database Compatibility: DBT uses adapters to connect and interface with different data platforms, like Snowflake, BigQuery, and Redshift. Adapters translate DBT’s generic SQL into database-specific SQL.
Testing Framework:
Built-in & Custom Tests: DBT supports both built-in tests (like unique or not_null) and custom tests defined in SQL, ensuring data quality and conformity to business rules.
Version Control Integration:
Git Integration: DBT projects are typically stored in Git repositories, enabling collaboration, versioning, and branching.
Documentation:
Auto-generation: DBT automatically generates a web-based documentation portal that visualizes model metadata, lineage, and descriptions.
Plugins and Extensibility:
Community Plugins: DBT’s architecture allows for extensions, and the community has contributed various plugins, adding functionality and compatibility with other tools.
Runtime Environment:
In-database Computation: Unlike ETL tools that may have their own computation engines, DBT compiles and runs SQL directly in the target data warehouse, leveraging its computational power for transformations.
(Source:
Giphy
)
Challenges with DBT
While DBT (Data Build Tool) has gained substantial popularity due to its approach to data transformation, it is not without its technical challenges, especially when viewed in the context of the broader data pipeline design:
Initial Data Ingestion:
DBT focuses mainly on the transformation (T) part of the ELT process. The extraction (E) and load (L) phases are out of its scope, requiring other tools or manual setups to ingest data into the data warehouse.
Complex Dependency Management:
As DBT projects grow, managing model dependencies (DAG) can become complex. Ensuring models run in the right order without causing circular dependencies is crucial and can be challenging in large projects.
Performance Considerations:
Relying on the computational power of the data warehouse for transformations can lead to increased costs, especially if not optimized.
Some transformations might be less efficient in SQL compared to other data processing languages or tools.
Concurrency and Parallelism:
Handling concurrent DBT runs or ensuring that parallel transformations don’t interfere with each other can be challenging. There’s a need to fine-tune data warehouse configurations and manage resource contention.
Incremental Processing:
While DBT supports incremental models, designing them effectively requires careful consideration to ensure data integrity and avoid data duplication.
Real-time Data Processing:
DBT is batch-oriented by design. Real-time or near-real-time data processing pipelines might need additional tools or configurations outside of DBT’s standard capabilities.
Integration with External Tools:
DBT’s ecosystem is primarily SQL-focused. Integrating with non-SQL tools or platforms might require additional effort or custom plugins.
Operational Monitoring and Alerting:
Out-of-the-box, DBT does not provide comprehensive monitoring or alerting mechanisms for transformations. Integration with monitoring tools or building custom alert systems might be necessary.
Error Handling:
Granular error handling, especially for non-fatal issues, can be complex. DBT will fail a run if a model encounters an error, requiring manual intervention or a robust orchestration tool to manage failures.
Security and Compliance:
Ensuring that DBT processes adhere to data governance, security, and compliance requirements might necessitate additional configurations, especially when working with sensitive data.
Scalability:
As data volume grows, some DBT models might need refactoring or optimization to maintain performance. This requires ongoing maintenance and tuning.
(Source:
Giphy
)
Conclusion
In the ever-evolving landscape of data processing and analytics, DBT emerges as a powerful tool that merges software engineering best practices with data operations. Its ELT-centric approach, modular design, and emphasis on code and collaboration make it an attractive solution for modern data teams.
Yet, like any tool, it is not without its challenges. Factors like dependency management, real-time processing, and scalability require thoughtful consideration in the broader context of data pipeline design.
With proper planning and awareness of its intricacies, DBT can be a pivotal element in a data team’s toolkit, driving efficiency, transparency, and reliability in data transformations. As with all tools, a balance of its strengths against its challenges is essential in leveraging its full potential effectively.
Link to the original blog:
https://www.mage.ai/blog/understanding-dbt-data-build-tool-an-introduction
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Setup a Micro-Frontend architecture in 15min with Vite!
Meidi Airouche -
Dec 10
First-Time Game Developing
Ricardo Araújo -
Dec 10
Encore Launch Week Day 2: Public Buckets & CDN
Marcus Kohlberg -
Dec 10
9 must-know open-source tools to land your dream job in 2025
Sunil Kumar Dash -
Dec 10
Mage
Follow
🧙 The modern replacement for Airflow. Mage is an open-source data pipeline tool for transforming and integrating data.
Location
Santa Clara, CA
Joined
Aug 22, 2021
More from
Mage
Data Integration: Google BigQuery with Mage
Google BigQuery: Serverless data warehousing made simple
#
dataanalytics
#
googlebigquery
#
datascience
#
cloudcomputing
Snowflake: Revolutionizing data warehousing
#
datawarehouse
#
snowflake
#
cloudcomputing
#
bigdata
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
"What is Big Data? Characteristics, types, and technologies - DEV Community","What is Big Data? Characteristics, types, and technologies - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Hunter Johnson
for
Educative
Posted on
Sep 7, 2022
• Originally published at
educative.io
What is Big Data? Characteristics, types, and technologies
#
datascience
#
database
#
bigdata
#
tutorial
Big Data is a modern analytics trend that allows companies to make more data-driven decisions than ever before. When analyzed, the insights provided by these large amounts of data lead to real commercial opportunities, be it in marketing, product development, or pricing.
Companies of all sizes and sectors are joining the movement with data scientists and Big Data solution architects. With the Big Data market expected to nearly double by 2025 and user data generation rising, now is the best time to become a Big Data specialist.
Today, we'll get you started on your Big Data journey and cover the fundamental concepts, uses, and tools essential for any aspiring data scientist.
Here’s what we’ll go through today:
What is Big Data
What is Big Data used for?
How does Big Data work?
Big Data Terminology
Big Data Technologies
What to learn next
What is Big Data?
Big data refers to
large collections of data
that are so complex and expansive that they cannot be interpreted by humans or by traditional data management systems. When properly analyzed using modern tools, these huge volumes of data give businesses the information they need to make informed decisions.
New software developments have recently made it possible to use and track big data sets. Much of this user information would seem meaningless and unconnected to the human eye. However, big data analytic tools can track the relationships between hundreds of types and sources of data to produce useful business intelligence.
All big data sets have three defining properties, known as the
3 V's
:
Volume
: Big data sets must include millions of unstructured, low-density data points. Companies that use big data can keep anything from dozens of terabytes to hundreds of petabytes of user data. The advent of cloud computing means companies now have access to zettabytes of data! All data is saved regardless of apparent importance. Big data specialists argue that sometimes the answers to business questions can lie in unexpected data.
Velocity
: Velocity refers to the fast generation and application of big data. Big data is received, analyzed, and interpreted in quick succession to provide the most up-to-date findings. Many big data platforms even record and interpret data in real time.
Variety
: Big data sets contain different types of data within the same unstructured database. Traditional data management systems use structured relational databases that contain specific data types with set relationships to other data types. Big data analytics programs use many different types of unstructured data to find all correlations between all types of data. Big data approaches often lead to a more complete picture of how each factor is related.
Correlation vs. Causation
Big data analysis only finds correlations between factors, not causation. In other words, it can find if two things are related, but it cannot determine if one causes the other.
It's up to data analysts to decide which data relationships are actionable and which are just coincidental correlations.
Big Data History
The concept of Big Data has been around since the 1960s and 70s, but at the time, they didn't have the means to gather and store that much data.
Practical big data only took off around 2005, as developers at organizations like YouTube and Facebook realized the amount of data they generated in their day-to-day operations.
Around the same time, new advanced frameworks and storage systems like Hadoop and NoSQL databases allowed data scientists to store and analyze bigger datasets than ever before. Open-source frameworks like Apache Hadoop and Apache Spark provided the perfect platform for big data to grow.
Big data has continued to advance, and more companies recognize the advantages of predictive analytics. Modern big data approaches leverage the Internet of Things (IoT) and
cloud computing
strategies to record more data from across the world and
machine learning
to build more accurate models.
While it's hard to predict what the next advancement in big data will be, it's clear that big data will continue to become more scaled and effective.
What is Big Data used for?
Big data applications are helpful across the business world, not just in tech. Here are some use cases of Big Data:
Product Decision Making
: Big data is used by companies like Netflix and Amazon to develop products based on upcoming product trends. They can use combined data from past product performance to anticipate what products consumers will want before they want them. They can also use pricing data to determine the optimal price to sell the most to their target customers.
Testing
: Big data can analyze millions of bug reports, hardware specifications, sensor readings, and past changes to recognize fail-points in a system before they occur. This helps maintenance teams prevent the problem and costly system downtime.
Marketing
: Marketers compile big data from previous marketing campaigns to optimize future advertising campaigns. Combining data from retailers and online advertising, big data can help finetune strategies by finding subtle preferences for ads with certain image types, colors, or word choices.
Healthcare
: Medical professionals use big data to find drug side effects and catch early indications of illness. For example, imagine there is a new condition that affects people quickly and without warning. However, many of the patients reported a headache during their last annual checkup. This would be flagged as a clear correlation using big data analysis but may be missed by the human eye due to differences in time and location.
Customer Experience
: Big data is used by product teams after a launch to assess the customer experience and product reception. Big data systems can analyze large data sets from social media mentions, online reviews, and feedback on product videos to get a better indication of what problems customers are having and how well the product is received.
Machine learning
: Big data has become an important part of machine learning and artificial intelligence technologies, as it offers a huge reservoir of data to draw from.
ML engineers
use big data sets as varied training data to build more accurate and resilient predictive systems.
How does Big Data work?
Big data alone won't provide the business intelligence that many companies are searching for. You'll need to process the data before it can provide actionable insights.
This process involves 3 major stages:
1. Data flow intake
The first stage has data flowing into the system in huge quantities. This data is of many types and will not be organized into any usable schema. Data at this stage is called a
data lake
because all the data is lumped together and impossible to differentiate.
Your company's system must have the data processing power and storage capacity to handle this much data. On-premises storage is the most secure but can become overworked depending on the volume.
Cloud computing and
distributed storage
are often the secrets to effective flow intake. They allow you to divide storage among multiple databases on the system.
2. Data analysis
Next, you'll need a system that automatically cleans and organizes data. Data at this scale and frequency is too large to organize by hand.
Popular strategies include setting criteria that throw out any faulty data or building in-memory analytics that continually adds new data to ongoing analysis. Essentially, this stage is like taking a pile of documents and ordering it until it’s filed in a structured way.
At this stage, you'll have the raw findings but not what to do with the findings. For example, a ride-share service may find that over 50% of users will cancel a ride if the incoming driver is stopped for more than 1 minute.
3. Data-driven decision-making
At the final stage, you'll interpret the raw findings to form a concrete plan. Your job as a data scientist will be to look at all the findings and create an evidence-supported proposal for how to improve the business.
In the ride-share example, you might decide that the service should send drivers on routes that keep them moving, even if it takes slightly longer to reduce customer frustration. On the other hand, you could decide to include an incentive for the user to wait until the driver arrives.
Either of these options is valid because your big data analysis cannot determine which aspect of this interaction needs to change to increase customer satisfaction.
Big Data terminology
Structured Data
:
This data has some pre-defined organizational property that makes it easy to search and analyze. The data is backed by a model that dictates the size of each field: its type, length, and restrictions on what values it can take. An example of structured data is ""unit's produced per day"", as each entry has a defined
product type
and
number produced
fields.
Unstructured Data
:
This is the opposite of structured data. It doesn't have any pre-defined organizational property or conceptual definition. Unstructured data makes up the majority of big data. Some examples of unstructured data are social media posts, phone call transcripts, or videos.
Database:
An organized collection of stored data that can contain either structured or unstructured data.
Databases
are designed to maximize the efficiency of data retrieval. Databases have two types: relational and non-relational.
Database management system
:
Usually, when referring to databases such as MySQL and PostgreSQL, we are talking about a system, called the database management system. A DBMS is software for creating, maintaining, and deleting multiple individual databases. It provides peripheral services and interfaces for the end-user to interact with the databases.
Relational Database (SQL)
:
Relational databases consist of structured data stored as rows in tables. The columns of a table follow a defined
schema
that describes the type and size of the data that a table column can hold. Think of a schema as a blueprint of each record or row in the table. Relational databases must have structured data and the data must have some logical relationship to each other.
For example, a Reddit-like forum would use a relational database as the data's logical structure is that users have a list of following forums, forums have a list of posts, and posts have a list of posted comments. Popular implementations include Oracle, DB2, Microsoft SQL Server,
PostgreSQL
, and
MySQL
.
Non-relational Database
:
Non-relational databases have no rigid schema and contain unstructured data. Data within has no logical relationship to other data in the database and is organized differently based on the needs of the company. Some common types include key-value stores (Redis, Amazon Dynamo DB), column stores (HBase, Cassandra), document stores (
Mongo DB
, Couchbase), graph databases (Neo4J), and search engines (Solr, ElasticSearch, Splunk). The majority of big data is stored on non-relational databases as they can contain multiple types of data.
Data Lake
:
A repository of data stored in raw form. Like water, all the data is intermixed, and no collection data can be used before it can be separated from the lake. Data in the data lake doesn't need to have a defined purpose yet. It is stored in case a use is discovered later.
Data Warehouse
:
A repository for filtered and structured data with a predefined purpose. Essentially, this is the structured equivalent of a data lake.
Big Data technologies
Finally, we'll explore the top tools used by modern data scientists as they create Big Data solutions.
Hadoop
Hadoop is a reliable, distributed, and scalable distributed data processing platform for storing and analyzing vast amounts of data. Hadoop allows you to connect many computers into a network used to easily store and compute huge datasets.
The lure of Hadoop is its ability to run on cheap commodity hardware, while its competitors may need expensive hardware to do the same job. It's also open source. Hadoop makes Big Data solutions affordable for everyday businesses and has made Big Data approachable to those outside of the tech industry.
Hadoop is sometimes used as a blanket term referring to all tools in the Apache data science ecosystem.
MapReduce
MapReduce
is a programming model used across a cluster of computers to process and generate Big Data sets with a parallel, distributed algorithm. It can be implemented on Hadoop and other similar platforms.
A MapReduce program contains a
map
procedure that filters and sorts data into a usable form. Once the data is mapped, it's passed to a
reduce
procedure that summarizes the trends of the data. Multiple computers in a system can perform this process at the same time to quickly process data from the raw data lake to usable findings.
MapReduce programming model has the following characteristics:
Distributed
: The MapReduce is a distributed framework consisting of clusters of commodity hardware that run
map
or
reduce
tasks.
Parallel
: The map and reduce tasks always work in parallel.
Fault-tolerant
: If any task fails, it is rescheduled on a different node.
Scalable
: It can scale arbitrarily. As the problem becomes bigger, more machines can be added to solve the problem in a reasonable amount of time; the framework can scale horizontally rather than vertically.
Mapper Class in Java
Let's see how we can implement MapReduce in Java.
First, we'll use the Mapper class added by the Hadoop package (
org.apache.hadoop.mapreduce
) to create the
map
operation. This class maps input key/value pairs to a set of intermediate key/value pairs. Conceptually, a mapper performs parsing, projection (selecting fields of interest from the input), and filtering (removing non-interesting or malformed records).
For example, we'll create a mapper that takes a list of cars and returns the brand of the car and an iterator; a list of a Honda Pilot and a Honda Civic would return
(Honda 1)
,
(Honda 1)
.
Mapper class in Hadoop:
public class CarReducer extends Reducer<Text, IntWritable, Text, LongWritable> {

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        long sum = 0;

        for (IntWritable occurrence : values) {
            sum += occurrence.get();
        }

        context.write(key, new LongWritable(sum));
    }
}
Enter fullscreen mode
Exit fullscreen mode
The most important part of this code is on
line 9
. Here, we output key/value pairs that get sorted and aggregated by reducers later on.
Don't confuse the key and value we write with the key and values being passed into the
map(...) method
. The key is the name of the car brand. Since each occurrence of the key denotes one physical count of that brand of car, we output 1 as the value. We want to output a key type that is both serializable and comparable, but the value type should only be serializable.
Reducer Class in Java
Next, we'll implement the
reduce
operation using the
Reducer
class added by Hadoop. The
Reducer
automatically takes the output of
Mapper
and returns the total number of cars of each brand.
The reduce task is split among one or more reducer nodes for faster processing. All tasks of the same key (brand) are completed by the same node.
Reducer class in Hadoop:
public class CarReducer extends Reducer<Text, IntWritable, Text, LongWritable> {

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

        long sum = 0;

        for (IntWritable occurrence : values) {
            sum += occurrence.get();
        }

        context.write(key, new LongWritable(sum));
    }
}
Enter fullscreen mode
Exit fullscreen mode
Lines 8-10
iterate through each map of the same key and sum the total count using the
sum
variable.
Mapper
and
Reducer
are the backbone of many Hadoop solutions. You can expand these basic forms to handle huge sums of data or reduce them to highly specific summaries.
What to learn next
With this introduction to Big Data, you're prepared to start practicing with common data science tools and advanced analytical concepts.
Some next steps to look at are:
Explore the Hadoop Distributed File System (HDFS)
Build a model using Apache Spark
Generated findings using MapReduce
Familiarize yourself with different input/output formats
To help you master these skills and continue your Big Data journey, Educative has created the course
Introduction to Big Data and Hadoop
. This course will give you hands-on practice with Hadoop, Spark, and MapReduce, tools used by data scientists every day.
By the end, you'll have used your learning to complete a Big Data project from beginning to end that you can use on your resume.
Happy learning!
Continue reading about Big Data and data science on Educative
Applied Data Science: serverless functions, pipelines and PySpark
The top 10 ML algorithms for data science in 5 minutes
Data Science Simplified: What is language modeling for NLP?
Start a discussion
What do you think are the most interesting use cases for Big Data? Was this article helpful? Let us know in the comments below!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
The Evolution of Data Analytics Roadmaps: Preparing for Industry Demands in 2025
Vikas76 -
Dec 4
FLUX Tools Complete Tutorial with SwarmUI : Outpainting, Inpainting, Redux Re-Imagine + Combine Images, Depth & Canny
Furkan Gözükara -
Dec 3
How Artificial Intelligence and Data Science Work Together to Solve Complex Problems in 2025
Vikas76 -
Nov 20
The future of software architecture: focus on event-driven architecture
ATIXAG -
Nov 20
Educative
Follow
Learn anything from CSS to System Design, interactively.
Level up on in-demand tech skills - at your own speed.
Text-based courses with embedded coding environments help you learn without the fluff.
Try a preview
More from
Educative
Basics of Fuzzy Logic
#
datascience
#
machinelearning
#
programming
#
tutorial
What is Clustering: An Introduction
#
machinelearning
#
programming
#
tutorial
#
productivity
Hands-on AWK
#
programming
#
tutorial
#
productivity
#
beginners
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
The Amazing Skill of Predicting The Interview - DEV Community,"The Amazing Skill of Predicting The Interview - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Nada
Posted on
Mar 11, 2020
• Edited on
Aug 16, 2022
The Amazing Skill of Predicting The Interview
#
career
Hello, my friends,
Today I am gonna teach you an amazing magic trick. I am gonna teach you how to predict the interview.
How do you like that?
Okay, I want you to apply for a position you like.
Next, I want you to sit comfortably in your chair and relax and let me show you the magic.
Now, say after me
Interviewto Predictum
.
Okay, all joking aside. There’s something I’ve been doing unintentionally for the past year that helped me a great deal in my interviews. I thought I write about it since it may be helpful for somebody out there.
In order to make sure things I mention here do work, I applied things I mention here in an actual interview and I kept notes.
So here come my amazing discoveries.
But First Comes The Warning ⚠️
WARNING:
We don’t take responsibility for the outcomes that result from applying what’s mentioned in this post. Please proceed with caution. Also, note that we don’t encourage nor approve underage wizardry, kindly have some wizard of age with you the whole time.
My Small Big Data
Some interviews are easier to predict than others. Yet, if we did our best we are most likely to end up with a good enough prediction.
The first thing I did is collect as much data as I possibly can about the company, their interviews, their selection process, and the interviewers. Some data are easily handed to you; like the interview duration, the process, and in most cases the interviewer’s name (in my test interview I didn’t know the interviewers’ names until they introduced themselves at the beginning of the interview).
To find my small big data, I searched:
the company’s website,
the company’s LinkedIn page,
the company’s reviews and interviews on Glassdoor,
for my own connections that work/ worked at the company,
and the wide wide web.
I know… I Assume… I Ask…
You know how in problem-solving, we sometimes make assumptions. These assumptions aren’t facts and they may be wrong. But making these assumptions makes it easier to tackle an unknown problem. That’s exactly what I did. I made some assumptions based on the things I knew for sure. And for what it’s worth, my assumptions weren’t wrong.
So, let’s start from the beginning. A week after I applied I got invited to
a 30-minute
call with a
tech recruiter
whose name is
Jane Doe
.
What do I know for sure?
​ ✅ The duration of the interview is going to be 30 minutes or less.
​ ✅ The interviewer’s name and position.
​ ✅ The interviewer’s background (because I did collect my small big data).
​ ✅ The data about the company and its products.
What did I assume?
There will not be any technical questions (since she is a recruiter).
I will be asked about my experience.
I will be asked about the company.
I will be asked why I am applying to an overseas company.
I did get asked all of these 👆🏻 questions.
The part that comes after making assumptions and preparing for them is preparing questions for things I couldn’t find data about and/ or I couldn’t assume.
What questions did I prepare?
What are the next steps? (I didn’t ask this one, because she explained the process to me before I needed to ask)
A question about how the product works.
What architecture do you use?
What language do you mainly use? Swift or Objective-C?
Questions about the iOS team.
I added the answers to these questions to my pile of data, and on to the next step.
Accumulate and Repeat
Next, I got invited to another
30-minute
interview with an
iOS Engineer
.
What do I know for sure?
​ ✅ The duration of the interview is going to be 30 minutes or less.
​ ✅ Only the interviewer’s position.
​ ✅ The information I learned in the first interview (architecture and language used…).
What did I assume?
There’s a good chance I won’t be asked any problem-solving questions (due to the short interview duration).
I will be asked about my experience.
I will be tested on my iOS knowledge.
There’s a good chance I might be asked architectural questions.
There was no problem-solving and I got asked about my experience, my iOS knowledge, and architectural design patterns.
What questions did I prepare?
How much of the code base is still in Objective-C?
More questions about the iOS team.
I accumulated the answers to these questions and his questions to me to my pile of small big data.
I did the same with the last interview as well, taking into account the things the second interviewer asked about and the questions of the technical test I had to pass before I get to the second interview. Because though the next interviewer won’t ask the same questions, they will probably want you to demonstrate your knowledge on topics that matter the most to them (topics you’ve been asked about before).
Work With What You’ve Got
Sometimes the information you can find about the company on the internet is very limited and sometimes the data you get from your first screening call is not of much use. You just have to work with what you’ve got and piece things together as you go.
Try putting yourself in the interviewer’s shoes and ask yourself what are you looking for in a candidate and drive the questions from there.
Another approach is to focus on the product and its features and ask yourself what are the concepts behind them and guess their questions.
It’s Not Failproof
It goes without saying that predictions are never failproof. They can very well be wrong, but preparing them is a good way to focus on the specific topics that are somewhat likely to be brought up in your interviews.
I don’t know about you but I feel that with a few specific topics I have a better chance of being well-prepared than the ocean of endless topics out there.
Plus, I believe that we learn a few valuable things when we make the wrong assumptions. We learn to better read and understand the interviewer’s position. We learn about the factors we didn’t take into consideration when we made our wrong assumptions. We learn to learn from our mistakes.
Interviewing is tough, my friend, but so are you 💪🏻!
Let me know how you prepare for interviews and if you found these ideas helpful.
Happy interviewing 👔!
Top comments
(2)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Ben Halpern
Ben Halpern
Ben Halpern
Follow
A Canadian software developer who thinks he’s funny.
Email
ben@forem.com
Location
NY
Education
Mount Allison University
Pronouns
He/him
Work
Co-founder at Forem
Joined
Dec 27, 2015
•
Mar 11 '20
Dropdown menu
Copy link
Hide
Nice post. I think you've really hit on an element of interviewing most don't take into account.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Nada
Nada
Nada
Follow
👩🏻‍💻iOS Developer and Blogger • 🤖App freak • 📚Ambitious reader
Location
Cairo, Egypt
Joined
Nov 23, 2019
•
Mar 11 '20
Dropdown menu
Copy link
Hide
Thank you, Ben!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to Build a Strong Tech Resume (Get Hired Faster)
KaKaComputer -
Nov 18
Move Zeroes to the End of an Array: A Practical Guide
Rathod Ketan -
Nov 17
Overthinking Software Projects
Pawel Kadluczka -
Nov 16
🦸‍♂️ Nobody Dreamed of Becoming a DevOps Engineer
Adrian Paul Nutiu -
Nov 8
Nada
Follow
👩🏻‍💻iOS Developer and Blogger • 🤖App freak • 📚Ambitious reader
Location
Cairo, Egypt
Joined
Nov 23, 2019
Trending on
DEV Community
Hot
Atomic Note-Taking Guide
#
productivity
#
vim
#
neovim
#
obsidian
8 Type of Load Balancing
#
discuss
#
webdev
#
cloudcomputing
#
programming
Road to becoming a GDE | The Google Developers Program
#
webdev
#
career
#
careerdevelopment
#
google
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How to Simplify Large Salesforce Data Migration - DEV Community,"How to Simplify Large Salesforce Data Migration - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Dorian Sabitov
Posted on
Jul 19
• Originally published at
sfapps.info
on
Jul 19
How to Simplify Large Salesforce Data Migration
#
blog
#
howto
#
sponsored
Is Data Migration Necessary for Salesforce?
We can’t imagine living without information, such as weather updates, traffic reports, and currency exchange rates. We rely on information from different sources daily to make decisions and stay informed. A CRM system, like Salesforce, also depends heavily on data to enhance its efficiency, accelerate workflows, and improve outcomes. Seamless data migration to Salesforce is critical for leveraging its full potential. Data migration involves transferring data from legacy systems, various databases, or even other CRMs into Salesforce, ensuring all information is consistent, clean, and readily accessible.
Data migration from legacy systems to Salesforce
can be a daunting task, especially when dealing with large volumes of information. However, with the right strategies, tools, and a thorough plan, it can be simplified and executed smoothly. This guide will explore key considerations and best practices for simplifying large Salesforce data migrations.
When transitioning from platforms like Shopify to Salesforce or integrating data from legacy systems, the process requires meticulous planning and the right set of tools. The complexity of data migration often leads businesses to seek specialized solutions to manage the transition effectively.
Understanding the Essentials of Data Migration
Data Extraction and Transformation Strategies
Data Extraction Methods
Data Transformation Techniques
Is It Hard to Implement an ETL Tool?
Overview of Sliced Bread ETL Implementation
Steps to Implement Sliced Bread ETL
Comparing Standard Salesforce Tools and Sliced Bread for Data Migration
Salesforce Data Loader
Salesforce Data Import Wizard
Sliced Bread App
Comparison Summary
Wrapping Up
What Should I Do Next? Understanding the Essentials of Data Migration
Step 1: Preparation and Planning
Conduct a comprehensive data quality assessment to identify and rectify inconsistencies, duplicates, and errors in your existing data.
Develop a detailed data migration plan that includes data extraction, transformation, mapping, validation, and migration steps​​.
Step 2: Choosing the Right Tools
Several tools can facilitate data migration to Salesforce. The Salesforce Data Loader and other ETL tools like
Sliced Bread
are popular choices that support various data sources and provide robust data mapping and transformation capabilities​.
Step 3: Data Cleansing and Deduplication
Prioritize data cleansing to remove unnecessary characters, correct spelling mistakes, and standardize data formats. Deduplication is crucial to ensure your Salesforce database performs efficiently and remains user-friendly​​.
Data Extraction and Transformation Strategies
When planning a large Salesforce data migration, it’s crucial to get the data extraction and transformation steps right. These processes ensure your data fits into Salesforce seamlessly, maintaining its accuracy and usability.
Data Extraction Methods
APIs and Direct Database Queries:
APIs
: Using APIs to pull data directly from your current systems is flexible and efficient. APIs handle various data formats, making them ideal for complex environments​​.
Direct Queries
: For large datasets, SQL queries can be very effective. They allow you to precisely extract the data you need from relational databases.
Bulk Exports:
For large volumes of data, bulk export methods work well. Tools like
Sliced Bread
can manage millions of records, making the process smooth and efficient​​.
Third-Party ETL Tools:
ETL Tools
: Tools like
Talend
,
Informatica
, and
Sliced Bread
are designed specifically for data migration. They offer advanced features for extracting, transforming, and loading data, giving you more control over the process​​.
Data Transformation Techniques
Data Cleaning
: Before importing data into Salesforce, you have to clean and format it. This includes correcting errors and standardizing formats. Data cleaning tools can automate this, reducing the risk of mistakes​.
Mapping Fields
: Ensure each data point from your source system has a corresponding field in Salesforce. This might require creating custom fields in Salesforce to accommodate unique data. A detailed mapping document reviewed by stakeholders helps avoid discrepancies later​​.
Removing Duplicates
: Cleaning out duplicate records before the migration helps maintain data quality. Tools and scripts can assist in identifying and merging duplicates, ensuring your data is clean and efficient​​.
Pilot Migration
: Conducting a test run helps identify issues in the extraction and transformation processes before the full migration. Validate the data to ensure it meets your expectations in terms of accuracy and completeness.
Is It Hard to Implement an ETL Tool?
Implementing an ETL (Extract, Transform, Load) tool can initially seem challenging, but with the right approach and tools, it can be straightforward and highly beneficial. Here’s an overview using the example of the
Sliced Bread ETL
app available on Salesforce AppExchange.
Overview of Sliced Bread ETL Implementation
Sliced Bread ETL simplifies the data migration process into Salesforce by offering a robust, user-friendly platform for extracting, transforming, and loading data. Here’s how it makes the process easier:
User-Friendly Interface:
The Sliced Bread ETL tool is designed with a user-friendly interface, making it accessible even for those with limited technical expertise. The intuitive design helps guide users through each step of the ETL process, reducing the learning curve and simplifying implementation.
Seamless Integration:
It supports integration with various data sources, including legacy systems, databases, and other CRMs. This versatility means you can consolidate data from multiple sources into Salesforce without needing extensive custom development.
Automated Processes:
Sliced Bread ETL automates many of the tasks associated with data migration. This includes data cleaning, deduplication, and validation, which reduces manual effort and minimizes the risk of errors during the migration process.
Advanced Data Mapping:
The tool provides advanced data mapping features that ensure data from your source systems is correctly aligned with Salesforce fields. This precise mapping is crucial for maintaining data integrity and usability post-migration.
Comprehensive Support:
Sliced Bread offers extensive documentation and support resources, including detailed guides, tutorials, and customer support. These resources can help users troubleshoot issues and optimize their use of the ETL tool.
Steps to Implement Sliced Bread ETL
Planning and Preparation:
Before starting, define your data migration goals and identify the data sources. Planning involves understanding the data structures in both the source systems and Salesforce.
Data Extraction:
Use the Sliced Bread ETL tool to extract data from your existing systems. The tool supports various extraction methods, ensuring compatibility with your data sources.
Data Transformation:
Transform the extracted data to fit Salesforce’s schema. This includes cleaning the data, standardizing formats, and removing duplicates. Sliced Bread’s automated features make these tasks easier and more reliable.
Data Loading:
Load the transformed data into Salesforce. The tool’s robust loading capabilities handle large volumes of data efficiently, ensuring a smooth transfer.
Validation and Testing:
Validate the data in Salesforce to ensure accuracy. Conduct tests to verify that all data has been correctly migrated and that relationships between data points are maintained.
Ongoing Maintenance:
Post-migration, use Sliced Bread’s maintenance features to keep your data clean and up-to-date. Regular audits and cleaning help maintain data quality over time.
Implementing an ETL tool like Sliced Bread can significantly streamline the data migration process, making it more manageable and less error-prone. By automating key tasks and providing a user-friendly interface, Sliced Bread helps businesses achieve successful data migrations into Salesforce.
Comparing Standard Salesforce Tools and Sliced Bread for Data Migration
When planning a data migration to Salesforce, choosing the right tool is essential. Here, we compare the standard Salesforce tools with the Sliced Bread app to help you decide which solution best fits your needs.
Salesforce Data Loader
Overview:
Salesforce Data Loader
is a powerful, native tool designed to handle large data volumes for import and export operations using CSV files.
Features:
Handles up to 5 million records efficiently.
Supports command-line operations for automated processes.
User-friendly interface for straightforward data operations.
Requires some technical knowledge for command-line automation and lacks advanced data cleaning capabilities.
Best Use Cases:
Suitable for large-scale migrations where the data is already well-structured and requires minimal transformation​​.
Salesforce Data Import Wizard
Overview:
Salesforce Data Import Wizard
is an in-built Salesforce tool intended for simpler data migrations.
Features:
Designed for ease of use with a straightforward setup process.
Can handle up to 50,000 records.
Allows custom field mapping for data alignment.
Limited to smaller data volumes and lacks advanced data validation and transformation features.
Best Use Cases:
Ideal for small businesses or initial data imports with straightforward data structures​.
Sliced Bread App
Overview:
The Sliced Bread app
, available on Salesforce AppExchange, offers advanced features tailored for more complex and comprehensive data migrations.
Features:
Intuitive Data Mapping:
Provides sophisticated data mapping capabilities, ensuring accurate alignment between source data and Salesforce.
Automated Data Cleaning:
Built-in tools for automatic data cleaning, including deduplication and formatting corrections.
Seamless Integration:
Supports integration with a wide range of data sources, making it versatile for various migration scenarios.
Comprehensive Validation:
Robust validation features to ensure data integrity at every stage of the migration process.
User-Friendly Interface:
Designed to be accessible even to users without extensive technical expertise.
Best Use Cases:
Best for businesses needing advanced data transformation, cleaning, and integration capabilities. It’s particularly useful for complex migrations like
Shopify Plus migration to Salesforce
involving multiple data sources and large datasets​.
Comparison Summary
Ease of Use:
While all tools are user-friendly, Salesforce Data Import Wizard is the simplest, but Sliced Bread offers a more intuitive interface for complex operations.
Data Volume Handling:
Salesforce Data Loader is suitable for handling millions of records, whereas Sliced Bread provides scalability and can manage various data volumes with enhanced processing capabilities.
Data Cleaning and Validation:
Sliced Bread excels with automated data cleaning and robust validation features, areas where standard Salesforce tools may require additional manual efforts or third-party solutions.
Integration and Flexibility:
Sliced Bread offers seamless integration with various data sources, providing more flexibility compared to the more rigid structures of standard Salesforce tools.
Sliced Bread ETL VS Salesforce Data Loader VS Salesforce Data Import Wizard
Wrapping Up
Migrating a large amount of data to Salesforce might seem challenging, but it’s manageable with the right strategies and tools. The key lies in preparation and planning, selecting the best data migration tools Salesforce for your needs, and ensuring your data is clean and organized. Even if your data isn’t perfectly clean, the Sliced Bread ETL app can help.
The Sliced Bread ETL app shines for its user-friendly interface and powerful features for data cleaning, mapping, and validation. It’s especially useful for complex data migration in Salesforce and integrating different data sources smoothly. However, Salesforce’s native tools like Data Loader and Data Import Wizard are also strong contenders, particularly for smaller or simpler data sets.
Choosing the right Salesforce data migration tool comes down to understanding your specific requirements. With careful planning, the right tools, and attention to data quality, you can ensure a smooth data migration to Salesforce. This way, you can fully leverage Salesforce’s capabilities to improve your workflows and outcomes.
That’s all folks! I hope this article helps you make the right choice when selecting a data migration tool Salesforce for your data migration into Salesforce. Whether you opt for data migration tools in Salesforce or a specialized app, ensuring a smooth transition is key to getting the most out of your CRM system.
The post
How to Simplify Large Salesforce Data Migration
first appeared on
Salesforce Apps
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Building Multi-Tenant Apps Using StackAuth's ""Teams"" and Next.js
ymc9 -
Dec 11
Top 10 Common Database Performance Mistakes in API Development
Aadarsh Nagrath -
Dec 11
Learn how to create a dismissible cookie banner with Tailwind CSS and Alpine JS
Michael Andreuzza -
Dec 11
Staging vs. Production: Key Differences You Need to Know
keploy -
Dec 11
Dorian Sabitov
Follow
I am a skilled Salesforce Administrator and Developer with four certifications. I'm quite interested in Salesforce and all of its uses in the modern corporate world.
Location
Ukraine
Joined
Oct 8, 2023
More from
Dorian Sabitov
Reasons to Hire a Remote Salesforce Administrator
#
blog
#
talentmarket
How to Implement Salesforce GDPR Compliance
#
blog
#
howto
#
salesforce
Mailchimp Salesforce Integration Guide: Easy Setup and Tools
#
blog
#
integrations
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Fetching Millions of Rows with Streams in Node.js - DEV Community,"Fetching Millions of Rows with Streams in Node.js - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Patrick God
Posted on
Feb 13, 2020
• Originally published at
patrickgod.com
Fetching Millions of Rows with Streams in Node.js
#
tutorial
#
node
#
webdev
#
beginners
Have you ever had to face the challenge of fetching several million rows of data with lots of columns from a database and display them on the web?
Well, I had to do this recently. Although I was not convinced that this would make any sense at all, I still had to do it.
Anyways, here's the solution.
But first, the technology stack: Node.js, Sequelize & MariaDB. The client doesn't matter, because at times the data was almost 4 GB big, so Chrome crashed anyway eventually.
Sequelize was the first big problem.
The solution to getting such big data results is
streaming
. Receiving that data with one big call led to Node crashing. So streaming is the answer, and Sequelize did not support streaming at that point.
Well, I was able to manually add streaming to Sequelize, but the service call takes ages in the end.
Here's a snippet of a usual Sequelize call:
await
sequelize
.
authenticate
();
const
result
=
await
sequelize
.
query
(
sql
,
{
type
:
sequelize
.
QueryTypes
.
SELECT
});
res
.
send
(
result
);
Enter fullscreen mode
Exit fullscreen mode
That's it. Of course, some parts are missing. Like the whole database configuration and the actual definition of the
get()
call (e.g. where does
res
come from?). But I think, you get the idea.
When you run this code, the result is simple. Node crashes. You could give node more memory with
--max-old-space-size=8000
, for instance, but that's not a real
solution
.
As already mentioned, you could kind of
force
Sequelize to stream the data. Now, how would that look like?
var
Readable
=
stream
.
Readable
;
var
i
=
1
;
var
s
=
new
Readable
({
async
read
(
size
)
{
const
result
=
await
sequelize
.
query
(
sql
+
` LIMIT 1000000 OFFSET
${(
i
-
1
)
*
1000000
}
`
,
{
type
:
sequelize
.
QueryTypes
.
SELECT
});
this
.
push
(
JSON
.
stringify
(
result
));
i
++
;
if
(
i
===
5
)
{
this
.
push
(
null
);
}
}
});
s
.
pipe
(
res
);
Enter fullscreen mode
Exit fullscreen mode
In this example, I knew the number of rows I would get back from the database, hence the line with
if (i === 5)
. It was just a test. You have to send
null
to end the stream. You could, of course, get the
count
of the whole result first and modify the code accordingly.
The whole idea behind this is to make
smaller
database calls and return the chunks with the help of the stream. This works, Node does not crash, but it still takes ages - almost 10 minutes for 3.5 GB.
What's the alternative?
The
MariaDB Node.js connector
.
That's how a usual query would look like:
const
mariadb
=
require
(
'
mariadb
'
);
const
pool
=
mariadb
.
createPool
({
host
:
""
HOST
""
,
user
:
""
USER
""
,
password
:
""
PASSWORD
""
,
port
:
3308
,
database
:
""
DATABASE
""
,
connectionLimit
:
5
});
let
conn
=
await
pool
.
getConnection
();
const
result
=
await
conn
.
query
(
sql
);
res
.
send
(
result
);
Enter fullscreen mode
Exit fullscreen mode
It's much faster. But let me jump right to the streaming code:
let
conn
=
await
pool
.
getConnection
();
const
queryStream
=
conn
.
queryStream
(
sql
);
const
ps
=
new
stream
.
PassThrough
();
const
transformStream
=
new
stream
.
Transform
({
objectMode
:
true
,
transform
:
function
transformer
(
chunk
,
encoding
,
callback
)
{
callback
(
null
,
JSON
.
stringify
(
chunk
));
}
});
stream
.
pipeline
(
queryStream
,
transformStream
,
ps
,
(
err
)
=>
{
if
(
err
)
{
console
.
log
(
err
)
return
res
.
sendStatus
(
400
);
}
})
ps
.
pipe
(
res
);
Enter fullscreen mode
Exit fullscreen mode
This may look a bit cryptic, but what happens here is, that you create a
pipeline
where you put stuff through. First, the
queryStream
which is the result of the database query. Then the
transformStream
to send the
stringified
chunks (only strings and buffers are allowed here, hence stringifying the object). And finally the
PassThrough
and a function for an error case.
With
ps.pipe(res)
you stream the result to the client.
And here's the result:
Under 4 minutes for the same data and you won't even notice that Node needs a bit RAM.
So, if you're challenged with a similar task, think about streaming the data.
Or you convince your client, that this kind of requirement is unrealistic for the web.
P.S. Pagination was not an option. We needed the whole data at once.
Image created by brgfx on
freepik.com
.
But wait, there’s more!
Let’s connect on
Twitter
,
YouTube
,
LinkedIn
or here on
dev.to
.
Get the
5 Software Developer’s Career Hacks
for free.
Enjoy more valuable articles for your developer life and career on
patrickgod.com
.
Top comments
(4)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Patrick God
Patrick God
Patrick God
Follow
Into code as long as I can remember. First games, then web, now both. Always eager to learn, create and teach something new.
Location
Frankfurt, Germany
Education
M.Sc. Computer Science
Work
Full Stack Developer at Insight Health
Joined
Sep 15, 2017
•
Mar 6 '20
Dropdown menu
Copy link
Hide
Hi Jordan,
Thanks for your question. Well, Chrome's default timeout did not get hit. And regarding Node, I changed the timeout with
req.setTimeout(2400000);
.
Hope this answers your question.
Take care,
Patrick
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Austin Felipe
Austin Felipe
Austin Felipe
Follow
Joined
Jul 30, 2018
•
Feb 13 '20
Dropdown menu
Copy link
Hide
Great topic! Have you tested on a production server? Did the response time considerably increase?
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Patrick God
Patrick God
Patrick God
Follow
Into code as long as I can remember. First games, then web, now both. Always eager to learn, create and teach something new.
Location
Frankfurt, Germany
Education
M.Sc. Computer Science
Work
Full Stack Developer at Insight Health
Joined
Sep 15, 2017
•
Feb 14 '20
Dropdown menu
Copy link
Hide
Thanks Austin.
Haven't tested it yet on production, I'm afraid. This was just a little study. Replacing Sequelize completely would be more complex right now. But maybe we'll switch to the MariaDB connector on some places.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
sjdonado
sjdonado
sjdonado
Follow
M.Sc CS - Software Engineer
Location
Berlin, Germany
Pronouns
he/him
Joined
Apr 4, 2020
•
Apr 6 '20
Dropdown menu
Copy link
Hide
Good approach! Works as use Datagrams in java
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Daily JavaScript Challenge #JS-35: Calculate the Product of Digits
DPC  -
Dec 2
Cómo crear un Wallpaper dinámico con la Hora y Fecha usando Python
Code Chappie -
Nov 30
Adding Interactive Charts and Graphs to Tailwind CSS Admin Templates: A Step-by-Step Guide
Hitesh Chauhan -
Nov 30
Mobile-Exclusive Web Experiences: How to Design for Mobile Users Only
Okoye Ndidiamaka -
Nov 30
Patrick God
Follow
Into code as long as I can remember. First games, then web, now both. Always eager to learn, create and teach something new.
Location
Frankfurt, Germany
Education
M.Sc. Computer Science
Work
Full Stack Developer at Insight Health
Joined
Sep 15, 2017
More from
Patrick God
Blazor WebAssembly Full Stack Bootcamp
#
beginners
#
webdev
#
dotnet
#
blazor
SQLite & Data Seeding with Entity Framework Core
#
beginners
#
tutorial
#
webdev
#
dotnet
Role-Based Authentication in .NET Core 3.1
#
beginners
#
tutorial
#
webdev
#
dotnet
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Darya Shirokova - DEV Community,"Darya Shirokova - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Follow
User actions
Darya Shirokova
I am a Software Engineer currently working at Google with background in big data processing and scalable systems, as well as dev tools (specifically developing debugger extension for Visual Studio).
Location
London
Joined
Joined on
May 9, 2023
Personal website
https://www.linkedin.com/in/darya-a-shirokova/
github website
Work
SWE
More info about @daryashirokova
Badges
One Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least one year.
Got it
Close
Writing Debut
Awarded for writing and sharing your first DEV post! Continue sharing your work to earn the 4 Week Writing Streak Badge.
Got it
Close
4 Week Writing Streak
You've posted at least one post per week for 4 consecutive weeks!
Got it
Close
Skills/Languages
Java, C#, Python, Testing, Big Data
Post
4 posts published
Comment
0 comments written
Tag
9 tags followed
Learning C# After Java: 5 Differences Explored
Darya Shirokova
Darya Shirokova
Darya Shirokova
Follow
Jun 13 '23
Learning C# After Java: 5 Differences Explored
#
csharp
#
java
#
beginners
22
reactions
Comments
Add Comment
4 min read
Understanding Asynchronous Code: A Comparison of Asynchronous Programming in C# and Java
Darya Shirokova
Darya Shirokova
Darya Shirokova
Follow
Jun 1 '23
Understanding Asynchronous Code: A Comparison of Asynchronous Programming in C# and Java
#
async
#
java
#
csharp
9
reactions
Comments
Add Comment
7 min read
Writing End-to-End test with pywinauto
Darya Shirokova
Darya Shirokova
Darya Shirokova
Follow
May 26 '23
Writing End-to-End test with pywinauto
#
pywinauto
#
testing
#
e2e
14
reactions
Comments
Add Comment
4 min read
Introduction into End-to-End Testing of GUI Applications
Darya Shirokova
Darya Shirokova
Darya Shirokova
Follow
May 23 '23
Introduction into End-to-End Testing of GUI Applications
#
testing
#
integration
#
e2e
11
reactions
Comments
Add Comment
7 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Your data tests failed! Now what? - DEV Community,"Your data tests failed! Now what? - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Kyle Eaton
Posted on
Nov 9, 2020
Your data tests failed! Now what?
#
datascience
#
devops
#
database
#
dataops
Congratulations
, you’ve successfully implemented data testing in your pipeline! Whether that’s using an off-the-shelf tool or home-cooked validation code, you know that securing your data through data testing is absolutely crucial to ensuring high-quality reliable data insights, and you’ve taken the necessary steps to get there. All your data problems are now solved and you can sleep soundly knowing that your data pipelines will be delivering beautiful, high-quality data to your stakeholders!
But wait… not so fast
. There’s just one detail you may have missed: What happens when your tests actually fail? Do you know how you’re going to be alerted? Is anyone monitoring the alerts? Who is in charge of responding to them? How would you be able to tell what went wrong? And… how do you fix any data issues that arise?
As excited as data teams might be about implementing data validation in their pipelines -
the real challenge (and art!) of data testing is not only how you detect data problems, but also how you respond to them
. In this article, we’ll talk through some of the key stages of responding to data tests, and outline some of the important things to consider when developing a data quality strategy for your team. The diagram below shows the steps we will cover:
System response to failure
Logging and alerting
Alert response
Root cause identification
Issue resolution
Stakeholder communication (across several stages)
System response
The first line of response to a failed data test, before any humans are notified, are automated responses of the system to the test failure that decide whether and how to continue any pipeline runs. This could take one of the following forms:
Do nothing. Continue to run the pipeline and simply log the failure or alert the team (more on that below).
Isolate the “bad” data, e.g. move the rows that fail the tests to a separate table or file, but continue to run the pipeline for the remainder.
Stop the pipeline.
The system response can also vary depending on the level of severity of the detected issue and the downstream use case: Maybe it’s okay to keep running the pipeline and only notify stakeholders for certain “warning” level problems, but it should absolutely not proceed for other, “critical”, errors.
Logging and alerting
While it is absolutely possible for data validation results to be simply written to some form of log, we assume that at least some of your tests will be critical enough to require alerting. Some things to consider here are:
Which errors need alerting, and which ones can be simply logged as a warning? Make sure to choose the correct level of severity for your alerts and only notify stakeholders when it’s absolutely necessary in order to avoid alert fatigue.
Which medium do you choose for the alerts? Are you sending messages to a busy Slack channel or someone’s email inbox where they might go unnoticed? Do critical alerts get mixed in with daily status reports that might be less relevant to look at? Using a tool such as PagerDuty allows you to fine-tune your alerts to match the level of severity and responsiveness required.
What is the timeliness of alerts? Do alerts get sent out at a certain time or do they just show up at some point during the day? This is an important factor to consider
when your alerting mechanism fails
- would anyone notice?
Alert response
Now that your alerting is nicely set up, you’re onto the next hurdle: Who will actually see and respond to those notifications? Some factors to take into account are:
Who gets notified and when? Upstream data producers, downstream data consumers, the team that owns the data pipelines, anyone else? Make sure you have a clear map of who touches your data and who needs to know if there are any issues.
Who is actually responsible for acknowledging and investigating the alert? This is probably one of the most crucial factors to consider when setting up data testing: Someone actually needs to own the response. This might not always be the same person or team for all types of tests, but you better have a clear plan in order to avoid issues going unnoticed or ignored, which in turn can cause frustration with stakeholders. I’m not saying you need an on-call rotation, but maybe… maybe, you need an on-call rotation. Having said that, please see the previous paragraph on fine-tuning the severity of your alerts: On-call does not necessarily mean getting a Pagerduty call in the middle of the night. It just means that someone knows they’re responsible for those alerts, and their team and stakeholders know who is responsible.
Are your notifications clear enough for your stakeholders to know what they imply? In particular, do your data consumers know how to interpret an alert and know what steps to take to get more information about the problem or a potential resolution? (Hint: Having a clear point of contact, such as an on-call engineer, often helps with this, too!)
Stakeholder communication
While it’s easy to jump right into responding to a test failure and figure out what’s going on, you should probably stop for a moment to think about who else needs to know. Most importantly, in most cases you’ll want to let your data consumers know that “something is up with the data” before they notice. Of course, this is not specific to data pipelines, but it’s often harder for downstream data consumers to see that data is “off” compared to, say, a web app being down or buggy. Stakeholders could either already be notified through automated alerting, or through a playbook that includes notifying the right people or teams depending on the level of severity of your alerts. You’ll also want to keep an open line of communication with your stakeholders to give them updates on the issue resolution process and be available to answer any questions, or if (and only if) absolutely necessary, make some quick fixes in case there are some urgent data needs.
Root cause identification
At a high level, we think of root causes for data test failures as belonging to one of the following categories:
The data is actually correct, but our tests need to be adjusted. This can happen, for example, when there are unusual, but correct, outliers.
The data is indeed “broken”, but it can be fixed. A straightforward example for this is incorrect formatting of dates or phone numbers.
The data is indeed corrupted, and it can’t be fixed, for example, when it is missing values.
One very common source of data issues that arise at the data loading or ingestion stage are changes that are mostly out of the control of the data team. In my time working with third party healthcare data, I’ve seen a variety of data problems that arose seemingly out of nowhere. Some common examples include data not being up-to-date due to delayed data deliveries, table properties such as column names and types changing unexpectedly, or values and ranges digressing from what’s expected due to changes in how the data is generated.
Another major cause of data ingestion issues are problems with the actual ingestion runs or orchestration, which often manifest themselves as “stale data”. This can happen when processes hang, crash, or get backed up due to long runtimes.
Now, how do you approach identifying the root cause of data ingestion issues? The key here is to be methodical about
Identifying the exact issue that’s actually happening and
Identifying what causes the issue.
Regarding the former, my recommendation is to not take problems and test failures at face value. For example, a test for NULL values in a column could fail because some rows have actual NULL values - or because that column no longer exists. Make sure you look at all failures and identify what exactly the problem is. Once the problem is clear, it’s time to put on your detective hat and start investigating what could have caused it. Of course we can’t list all potential causes here, but some common ones you might want to check include:
Recent changes to ingestion code (ask your team mates or go through your version control log)
Crashed processes or interrupted connections (log files are usually helpful)
Delays in data delivery (check if all your source data made it to where it’s ingested from in time)
Upstream data changes (check in the source data and confirm with the data producers whether this was intentional or not)
And finally, while data ingestion failures are often outside of our control, test failures on the transformed data are usually caused by changes to the transformation code. One way to counteract these kinds of unexpected side effects is to enable data pipeline testing as part of your development process and CI/CD processes. Enabling engineers and data scientists to automatically test their code, e.g. against a golden data set, will make it less likely for unwanted side effects to actually go into production.
Issue resolution
Now... how do I fix this? Of course, there is no single approach to fixing data issues, as the fix heavily depends on the actual cause of it - duh. Going back to our framework of the three types of root causes for test failures we laid out in the previous paragraph, we can consider the following three categories of “fixes” to make your tests go green again:
If you determine that the data is indeed correct but your tests failed, you need to adjust your tests in order to take into account this new knowledge.
If the data is fixable, some of the potential resolutions include re-running your pipelines, potentially with increased robustness towards disruptions such as connection timeouts or resource constraints, or fixing your pipeline code and ideally adding some mechanism to allow engineers to test their code to prevent the same issue from happening again.
If the data is broken beyond your control, you might have to connect with the data producers to re-issue the data, if that’s at all possible. However, there may also be situations in which you need to isolate the “broken” records, data sets, or partitions, until the issue is resolved, or perhaps for good. Especially when you’re dealing with third party data, it sometimes happens that data is deleted, modified, or no longer updated, to the point where it’s simply no longer suitable for your use case.
My data tests pass, I’m good!
Ha! You wish! Not to ruin your day here, but you might also want to consider that your data tests pass because you’re simply not testing for the right thing. And trust me, given that it’s almost impossible to write data tests for every single possible data problem before you encounter it the first time, you’ll likely be missing some cases, whether that’s small and very rare edge cases, or something glaringly obvious. I am happy to admit that I once managed a daily data ingestion pipeline that would alert if record counts
dropped
significantly from one day to the next, since that was usually our biggest concern. Little did I know that a bug in our pipeline would accidentally
double
the record counts in size, which besides some “hmm, those pipelines are running very slow today” comments aroused shockingly little suspicion - until a human actually looked at the resulting dashboards and noticed that our user count had skyrocketed that day.
So what do you do to make your tests more robusts against these “unknown unknowns”? Well, to be honest, this is a yet-to-be-solved problem for us, too, but here are some ideas:
Use an automated profiler to generate data tests in order to increase test coverage in areas that might not be totally obvious to you. For example, you might not even consider testing for the mean of a numeric column, but an automatically generated test could make your data more robust against unexpected shifts that are not caught by simply asserting the min and max of that column. One option to consider is putting these “secondary” tests into a separate test suite and reducing the alerting level, so you only get notified about actual meaningful changes.
Make sure to socialize your data tests within the team and do code reviews of the tests whenever they are added or modified, just like you would with the actual pipeline code. This will make it easier to surface all the assumptions the team working on the pipeline has about the data and highlight any shortcomings in the tests.
Do manual spot checks on your data, possibly also with the help of a profiler. Automated tests are great, but I would claim that familiarity with data is always an important factor in how quickly a team can spot when something is “off”, even when there is no test in place. One last step of your data quality strategy could be to implement a periodical “audit” of your data assets to ensure things still look the way they should and that tests are complete and accurate (and actually run).
Summary
We really hope this post has given you a good idea of the different steps to consider when you’re implementing data validation for your pipelines. Keep in mind that developing and running tests in production is only one aspect of a data quality strategy. You’ll also need to factor in things like alerting, ownership of response, communication with stakeholders, root cause analysis, and issue resolution, which can take a considerable amount of time and effort if you want to do it well.
If you want some more concrete examples, check out our case studies on how some users of Great Expectations, such as
Komodo Health
,
Calm
, and
Avanade
integrate Great Expectations into their data workflows.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to use migrations with Golang
Albert Colom -
Nov 6
Install Docker Desktop on Mac
Megha Sharma -
Nov 6
Deploying a MongoDB Collection Generator on Kubernetes
Dmitry Romanoff -
Nov 1
From prompts to programs: Language models' unbounded computational power
Mike Young -
Nov 6
Kyle Eaton
Follow
Location
Ann Arbor, MI
Work
UX/Growth at Superconductive Health / Great Expectations
Joined
Feb 2, 2020
More from
Kyle Eaton
How DAGs grow: Deeper, wider, and thicker
#
datascience
#
database
#
dataquality
#
devops
Continuous Integration for your data with GitHub Actions and Great Expectations
#
datascience
#
testing
#
devops
#
github
Why data quality is key to successful ML Ops
#
machinelearning
#
datascience
#
devops
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Renee Betina Esperas - DEV Community,"Renee Betina Esperas - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Follow
User actions
Renee Betina Esperas
Python, Automation, Big Data 
(ex-Java Dev)

*2022 Google Professional Certificate - IT Automation with Python
Location
Melbourne, Australia
Joined
Joined on
Jul 23, 2022
github website
Work
Automation Engineer
More info about @reneebetina
Badges
Two Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least two years.
Got it
Close
Writing Debut
Awarded for writing and sharing your first DEV post! Continue sharing your work to earn the 4 Week Writing Streak Badge.
Got it
Close
One Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least one year.
Got it
Close
Skills/Languages
Python and Automation (since 2017)
Big Data (since 2020)
ex-Java Dev for 4 years
Post
4 posts published
Comment
0 comments written
Tag
9 tags followed
Test Automation Tips and Recommendations
Renee Betina Esperas
Renee Betina Esperas
Renee Betina Esperas
Follow
Oct 24 '23
Test Automation Tips and Recommendations
#
testing
#
automation
#
testautomation
2
reactions
Comments
3
comments
5 min read
Top Skills you need as an AUTOMATION ENGINEER
Renee Betina Esperas
Renee Betina Esperas
Renee Betina Esperas
Follow
Jan 11 '23
Top Skills you need as an AUTOMATION ENGINEER
#
automation
Comments
Add Comment
3 min read
Top Skills You Need in Testing Big Data projects
Renee Betina Esperas
Renee Betina Esperas
Renee Betina Esperas
Follow
Aug 31 '22
Top Skills You Need in Testing Big Data projects
#
testing
#
bigdata
Comments
Add Comment
3 min read
Sending Emails using Python with Gmail SMTP SSL Setup 2022
Renee Betina Esperas
Renee Betina Esperas
Renee Betina Esperas
Follow
Aug 31 '22
Sending Emails using Python with Gmail SMTP SSL Setup 2022
#
python
#
smtp
#
automation
2
reactions
Comments
Add Comment
2 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
From Data Collection to Model Deployment: Key Deliverables in a Machine Learning Project - DEV Community,"From Data Collection to Model Deployment: Key Deliverables in a Machine Learning Project - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Bala Madhusoodhanan
Posted on
Apr 11, 2023
From Data Collection to Model Deployment: Key Deliverables in a Machine Learning Project
#
machinelearning
#
programming
#
documentation
AI Series (10 Part Series)
1
AI - Game Plan
2
A.I. Risk
...
6 more parts...
3
AI credit crunch- Tech Debt
4
Generative AI
5
Connecting the Dots: How Data Strategy Fuels AI Strategy
6
MLOPs - Overview
7
Maximizing the Value of Your Machine Learning Projects with the TOGAF Framework
8
From Data Collection to Model Deployment: Key Deliverables in a Machine Learning Project
9
From Data Gurus to Deployment Heroes: Exploring the Diverse Experts Behind the Scenes
10
Essential Consideration : Harnessing the Power of Generative AI
Intro
:
Documentation is often overlooked while developing software specially if the methodology is
agile
. Agile emphasizes working software over comprehensive documentation. But if the software development is related to a machine learning problem then documentation to support the development process is key.
Documentation Assets Inventory
:
Phase
Documentation
Inspiration / Problem   Identification
Business Objectives,   Business Success Criteria (Define experiment(s) to validate hypothesis)
Requirements, Assumptions, and Constraints
Risks and Contingencies   Terminology
Costs and Benefits
Data Mining Goals and Data   Mining Success Criteria
Project Plan, Initial Assessment of Tools and Technique
EDA   and Data Engineering
Data Exploratory Report and Data Description Report
Data Quality Assessment   Report
Data strategy (Rationale for Inclusion / Exclusion) for the model
Data engineering   design(cleansing, transformation rules)
Feature Engineering & ML   Models
Algorithm cheat sheet: show   algorithms for different use cases (Document selection and reason)
Link farm to research papers and relevant external resources
Test Design and evaluation   criteria
Model evaluation and approval for validating hypothesis
Assessment of Data Mining   Results w.r.t. Business Success
Actual Code
Operationalise
Continous Monitoring and Maintenance Plans
Troubleshooting guide for   performance and testing techniques
MLOps design
Operations Guide and   Configuration scripts for API
The Building Blocks of a Successful Machine Learning Project: Deliverables and Documentation
Machine learning models are built using data, and it's crucial that the data and methods used to build the model can be replicated by others. Documentation helps to ensure that the work done in the project is
repeatable
and reproducible.
Facilitate better
collaboration
between the team working on the machine learning problem. It also ensures that everyone is on the same page regarding the scope, goals, and methods used in the project.
By documenting the data sources, preprocessing steps, modeling techniques, and evaluation metrics, it's easier to make changes and improvements to the model as needed. Documentation helps to make it easier to
maintain
the machine learning model over time.
Documentation makes it easier for stakeholders and users to understand how the model works, what data it's based on, and how accurate it is. Documentation helps to ensure that the machine learning model is
transparent
and understandable.
Documentation helps to ensure that the machine learning project is
compliant
with relevant regulations and standards
Overall, documentation is an essential part of any machine learning project. It helps to ensure that the project is well-planned, well-executed, and well-documented, and that it can be easily maintained and scaled in the future.
AI Series (10 Part Series)
1
AI - Game Plan
2
A.I. Risk
...
6 more parts...
3
AI credit crunch- Tech Debt
4
Generative AI
5
Connecting the Dots: How Data Strategy Fuels AI Strategy
6
MLOPs - Overview
7
Maximizing the Value of Your Machine Learning Projects with the TOGAF Framework
8
From Data Collection to Model Deployment: Key Deliverables in a Machine Learning Project
9
From Data Gurus to Deployment Heroes: Exploring the Diverse Experts Behind the Scenes
10
Essential Consideration : Harnessing the Power of Generative AI
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Fetching Liked Posts Using the Bluesky API
Noah Matsell -
Dec 8
A beginner's guide to the Flux-1.1-Pro model by Black-Forest-Labs on Replicate
Mike Young -
Nov 25
Top 10 Developer Tools to Simplify and Enhance Your Workflow
Appvin tech -
Nov 25
773. Sliding Puzzle
MD ARIFUL HAQUE -
Nov 25
Bala Madhusoodhanan
Follow
- Curious Learner for Supply chain Domain, Sustainability, Machine Learning, RPA 
- Fostering innovation and exploring opportunities to drive efficiencies
Location
Aylesbury, United Kingdom
Joined
Oct 11, 2022
More from
Bala Madhusoodhanan
Value Before Volume : Code That Matters
#
programming
#
ai
#
githubcopilot
#
development
Pumpkin Patch Sweeper: A Gourd-geous HTML
#
html
#
gamedev
#
programming
#
tutorial
The Evolution of Language Models: From T9 to GPT-3+
#
machinelearning
#
nlp
#
education
#
llm
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Mocking AWS with Jest (and TypeScript) - DEV Community,"Mocking AWS with Jest (and TypeScript) - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Matt Morgan
Posted on
Aug 31, 2020
• Edited on
Mar 3, 2021
Mocking AWS with Jest (and TypeScript)
#
aws
#
typescript
#
jest
#
testing
This seems like a very punny subject, mocking the world's leading cloud provider. I tried to think of a few chuckles, but then I got to thinking about writing blogposts for free to promote the projects of companies that are worth hundreds of billions of dollars, and maybe the joke is just on me?
Anyway, no need to think on that one too much. I like working with these tools a lot and I like sharing the ways I've found to use them together. Some readers may think
Jest
is a lot like some of the other JavaScript/NodeJS testing frameworks, but it has a few features that I really think set it apart from anything else I've used. One of them is that it ships with a very powerful mocking capability. That is the subject of this article. The other features I like about Jest are snapshots and test tables, but they will need to be covered another time.
Table of Contents
The Road to Mocking
Mocking AWS
DynamoDB Mock
Import Paths
Returning Data
Mocking Errors
Test Data Persistence
TypeScript
Next Steps
The Road to Mocking
I've written a lot of tests in my time. I believe strongly in automated testing as a practice to keep systems stable and to enable innovation, refactoring and even to automate maintenance updates. The most important thing I've learned about testing is that it is a skill that requires some work as well as a smart approach.
Several years ago, I worked on a team that wrote ""unit tests"" in Java that mostly ran against a development copy of a relational database (Oracle, to make matters worse). If the tests couldn't negotiate a connection to that database, they would fail. Whenever the database schema changed, some tests would probably need to be updated, or they'd fail. Sometimes tests would fail because a row was deleted or added or because time had passed (how dare it!).
These brittle tests were more trouble than they were worth! In an effort to get some value out of unit testing, I struggled with some of the mocking libraries that were available at the time, such as the ironically-named EasyMock and the diminutive (???) Mockito. I did manage to write a few decent tests, but they took a long time to write and the return on investment just wasn't there to mock everything. As far as I know, that system still exists and still works but I'd be surprised if the unit tests are providing much value.
I'm happy to say I never again wrote a ""unit test"" against a live database after that. I did spend a bunch of time figuring out how to use Docker to build out a development environment and run tests against a pre-seeded database. Putting a full database (including tables and data) into a Docker image is actually a fantastic way to go if you are using RDBMS. I know that some say ""mock the database and test the business layer"", but that won't catch my app spitting out invalid or nonsensical SQL. Anyway, that has worked well for me and putting that Dockerized database into a CI pipeline has also produced good results for me.
As I started to get more into AWS, I reached for something else in that familiar pattern - Docker images that would give me a ""good enough"" implementation so I could develop offline. So I used
localstack
. If you just need a couple of AWS services, like an S3 bucket, localstack can work out very well. But if you're really going cloud native? It's miserable. There's basically no documentation. Apparently I'm to assume everything works just like the real AWS, except of course that isn't true and the gaps are for me to figure out. The biggest frustration I had with localstack wasn't just with running an AWS service but it was the effort of putting that service into some state where I could reasonably run a unit test against it. For example, if you want a test to run that involves an S3 bucket, you might think you can just create the bucket as part of a
docker-compose up
, but there isn't really a hook that localstack gives you to do things like that and so you wind up with race conditions and ""flaky"" tests. I actually did get this to work, but much like my Mockito tests, it was too much effort.
That's the key thing. It's not enough to have the perfect mock. It must be easy to set up and use. If it isn't, the ROI on its use will be too low. Eventually tests break, get skipped or become permanent TODOs. Testing should be a natural part of the development flow, not a big pile of extra work to do at the end of a feature implementation.
A good test setup is an amazing thing. I know that not everyone shares this view, but I like to see 100% unit test coverage on the projects I work on. I want to see unit tests on the simple bits, because I know they won't always be simple. Something is difficult to test? That's likely a code smell or maybe we haven't figured out a good way to manage the dependency in test. If we resolve those issues, getting the test coverage in isn't so hard and shouldn't take so long.
So that brings me to Jest and AWS. The latest project I've worked on has been all Lambda, API Gateway, DynamoDB and other services. As I was ramping up for this project, I spent a lot of time thinking about developer workflows and testing. I spent a lot of seeing how far I could go with
SAM
and even looked at focusing on the
Docker angle of SAM
.
In the end, we decided on a more cloud-native approach. We are using
CDK
without any local execution environment at all. We use unit tests and Jest mocks to write the code, then deploy to a developer-namespaced stack in our development account for integration testing. When we consider a feature complete, we open a pull request, get a review and eventually merge into the main branch, at which point continuous integration builds the main branch in our dev account, fires off some additional tests, then ships to a higher environment.
This is actually working really well for us because of the approach we use to mock AWS. The mocks we're using are extremely light and require very little maintenance or work to set up. This lets us focus on our custom code.
Mocking AWS
If you never mocked anything with Jest because you find their documentation too confusing, you likely aren't alone. I suppose it's challenging to be a maintainer of Jest attempting to be the premier JavaScript testing tool while supporting both pre-and-post ES6 code, ESM, TypeScript, JSX, Babel, NodeJS, etc. All that stuff doesn't make it easy to find your way as a consumer of said documentation. Additionally there are some of what I consider to be traps! Jest mentions a
DynamoDB mocking library right in their docs
. So that's the thing, right?
Well, now we know why they call it Jest I guess because you HAVE GOT TO BE KIDDING ME! The aws-sdk dependency makes sense, but it runs on Java? No way, that's not good to add Java as a dependency just to run a unit test. And even if this did work well for me, what happens when I want to include SQS or S3 or any other AWS service and all I have is a very specific DynamoDB mock? No, I need a way to do the whole thing.
The good stuff in the Jest documentation is the part on
Manual Mocks
. My advice is ignore all the stuff on ES6 Class Mocks as that will only draw you away from the right way to do this and that way is to put the modules you want to mock under
__mocks__
in the root directory (adjacent to node_modules) of your project. If you read the docs, you are basically going to provide your own mock version of whatever the module is, which sounds like a lot of work until you consider
jest.fn()
.
DynamoDB Mock
Okay, enough talk. Here's how I mock AWS using Jest. Let's start with some code that updates an item in DynamoDB. All code samples are
available
.
import
{
DynamoDB
}
from
'
aws-sdk
'
;
const
db
=
new
DynamoDB
.
DocumentClient
();
interface
Pet
{
legCount
:
number
;
likesIceCream
:
boolean
;
name
:
string
;
}
export
const
savePet
=
async
(
tableName
:
string
,
pet
:
Pet
):
Promise
<
void
>
=>
{
await
db
.
put
({
TableName
:
tableName
,
Item
:
{
PK
:
pet
.
name
,
...
pet
,
},
})
.
promise
();
};
Enter fullscreen mode
Exit fullscreen mode
The way ""Manual Mocks"" work in jest is that imports will look for modules in a
__mocks__
directory before they go to the regular
node_modules
source, so effectively I can intercept aws-sdk with a copy of my own. This works by comparing the import path so when I import from
aws-sdk
if I have
__mocks__/aws-sdk.ts
, that will intercept my import and replace the module with my mock.
Now you may be thinking that my plan to rewrite all of AWS SDK as a mock doesn't sound so lightweight after all, but that's where Jest really shines. I'm going to be able to provide only the bits I need while ignoring all the internals. Here's a basic mock that can be used with the code above.
export
const
awsSdkPromiseResponse
=
jest
.
fn
().
mockReturnValue
(
Promise
.
resolve
(
true
));
const
putFn
=
jest
.
fn
().
mockImplementation
(()
=>
({
promise
:
awsSdkPromiseResponse
}));
class
DocumentClient
{
put
=
putFn
;
}
export
const
DynamoDB
=
{
DocumentClient
,
};
Enter fullscreen mode
Exit fullscreen mode
I'm using
DocumentClient
in my code, so that's what the mock sdk will need to expose. Even though DynamoDB itself is a class in the sdk, here I'm just pulling a stack class from it, so this will work. I'm only calling one method on DocumentClient, so that's the only mock I need to provide for now.
What about the function implementation? If you look at my code, I'm calling the
put
method and then
promise()
on the object returned by it, so that's just what my mock does. It returns an object with a
promise
method on it (just as the real sdk does) and my code calls that method, which is another mock that just resolves the promise and returns the boolean
true
.
Putting all that together, I can now write a unit test that looks like this.
import
{
DynamoDB
}
from
'
../__mocks__/aws-sdk
'
;
import
{
savePet
}
from
'
./savePet
'
;
const
db
=
new
DynamoDB
.
DocumentClient
();
describe
(
'
savePet method
'
,
()
=>
{
test
(
'
Save Fluffy
'
,
async
()
=>
{
const
fluffy
=
{
legCount
:
4
,
likesIceCream
:
true
,
name
:
'
Fluffy
'
,
PK
:
'
Fluffy
'
};
await
savePet
(
'
Pets
'
,
fluffy
);
expect
(
db
.
put
).
toHaveBeenCalledWith
({
TableName
:
'
Pets
'
,
Item
:
fluffy
});
});
});
Enter fullscreen mode
Exit fullscreen mode
Note that it is not necessary to explicitly mock the sdk or import my mock. The only reason I did that is to be able to use
toHaveBeenCalledWith
in my test.
Import Paths
Some developers have the practice of not importing the entire sdk but just individual clients. This can lead to smaller Lambda sizes if you use any kind of bundling and tree-shaking such as webpack or parcel. I'm aware you can avoid bundling aws-sdk entirely by setting it as an external, but some benchmarks have shown that to be a worse practice performance-wise. In any case, suit yourself, but I like importing just the clients as it makes my code feel cleaner and makes the individual mocks smaller.
So here's the same code refactored to import only individual clients.
The implementation:
import
{
DocumentClient
}
from
'
aws-sdk/clients/dynamodb
'
;
const
db
=
new
DocumentClient
();
interface
Pet
{
legCount
:
number
;
likesIceCream
:
boolean
;
name
:
string
;
}
export
const
savePet
=
async
(
tableName
:
string
,
pet
:
Pet
):
Promise
<
void
>
=>
{
await
db
.
put
({
TableName
:
tableName
,
Item
:
{
PK
:
pet
.
name
,
...
pet
,
},
})
.
promise
();
};
Enter fullscreen mode
Exit fullscreen mode
The mock (now in
__mocks__/aws-sdk/clients/dynamodb.ts
):
export
const
awsSdkPromiseResponse
=
jest
.
fn
().
mockReturnValue
(
Promise
.
resolve
(
true
));
const
putFn
=
jest
.
fn
().
mockImplementation
(()
=>
({
promise
:
awsSdkPromiseResponse
}));
export
class
DocumentClient
{
put
=
putFn
;
}
Enter fullscreen mode
Exit fullscreen mode
And finally the test:
import
{
DocumentClient
}
from
'
../__mocks__/aws-sdk/clients/dynamodb
'
;
import
{
savePet
}
from
'
./savePet
'
;
const
db
=
new
DocumentClient
();
describe
(
'
savePet method
'
,
()
=>
{
test
(
'
Save Fluffy
'
,
async
()
=>
{
const
fluffy
=
{
legCount
:
4
,
likesIceCream
:
true
,
name
:
'
Fluffy
'
,
PK
:
'
Fluffy
'
};
await
savePet
(
'
Pets
'
,
fluffy
);
expect
(
db
.
put
).
toHaveBeenCalledWith
({
TableName
:
'
Pets
'
,
Item
:
fluffy
});
});
});
Enter fullscreen mode
Exit fullscreen mode
As you can see, nothing much has changed, so it's easy to choose the approach that works best for your project.
Returning Data
So far we have a pretty good way to just ignore the fact that DynamoDB
does stuff
that isn't too relevant to our code, but how can we reuse the same mock when we want to test a
get
request or otherwise inspect the return value from a call to an AWS service? That's where our friend
awsSdkPromiseResponse
comes into play. Because that is a Jest mock which is exported, we can alter the return value on the fly.
Let's take a
get
operation:
import
{
DocumentClient
}
from
'
aws-sdk/clients/dynamodb
'
;
const
db
=
new
DocumentClient
();
interface
Pet
{
legCount
:
number
;
likesIceCream
:
boolean
;
name
:
string
;
}
export
const
getPet
=
async
(
tableName
:
string
,
petName
:
string
):
Promise
<
Pet
>
=>
{
const
response
=
await
db
.
get
({
TableName
:
tableName
,
Key
:
{
PK
:
petName
}
}).
promise
();
if
(
response
.
Item
)
{
return
<
Pet
>
response
.
Item
;
}
else
{
throw
new
Error
(
`Couldn't find
${
petName
}
!`
);
}
};
Enter fullscreen mode
Exit fullscreen mode
(Note: please don't copy paste your interfaces! This is just to make examples more clear.)
Okay, so the table design is quite simple here with a PK that is the pet's name. If we pass along the right name, we can access that Pet item. If not, we get an error. Let's build out the mock a bit more to accommodate the new functionality.
export
const
awsSdkPromiseResponse
=
jest
.
fn
().
mockReturnValue
(
Promise
.
resolve
(
true
));
const
getFn
=
jest
.
fn
().
mockImplementation
(()
=>
({
promise
:
awsSdkPromiseResponse
}));
const
putFn
=
jest
.
fn
().
mockImplementation
(()
=>
({
promise
:
awsSdkPromiseResponse
}));
export
class
DocumentClient
{
get
=
getFn
;
put
=
putFn
;
}
Enter fullscreen mode
Exit fullscreen mode
I could even use exactly the same mock for both
getFn
and
putFn
, but doing that would make it a bit harder to test a workflow in which I was trying to count the number of gets vs. puts in a test. Again, this is a pretty basic design decision that you could pivot on without much trouble.
So based on the above, I could write another test like this:
import
{
DocumentClient
}
from
'
../__mocks__/aws-sdk/clients/dynamodb
'
;
import
{
getPet
}
from
'
./getPet
'
;
const
db
=
new
DocumentClient
();
describe
(
'
getPet method
'
,
()
=>
{
test
(
'
Save Fluffy
'
,
async
()
=>
{
await
getPet
(
'
Pets
'
,
'
Fluffy
'
);
expect
(
db
.
get
).
toHaveBeenCalledWith
({
TableName
:
'
Pets
'
,
Key
:
{
PK
:
'
Fluffy
'
}
});
});
});
Enter fullscreen mode
Exit fullscreen mode
Of course there are two big problems with this test.
I might actually have some code downstream that cares about this response and wants to do something with it - and I'm not getting that here.
I'll hit my error condition every time because the mock isn't returning the expected type.
The way to fix that is to alter the value returned by our mock sdk response.
import
{
DocumentClient
,
awsSdkPromiseResponse
}
from
'
../__mocks__/aws-sdk/clients/dynamodb
'
;
import
{
getPet
}
from
'
./getPet
'
;
const
db
=
new
DocumentClient
();
describe
(
'
getPet method
'
,
()
=>
{
test
(
'
Save Fluffy
'
,
async
()
=>
{
const
fluffy
=
{
legCount
:
4
,
likesIceCream
:
true
,
name
:
'
Fluffy
'
,
PK
:
'
Fluffy
'
};
awsSdkPromiseResponse
.
mockReturnValueOnce
(
Promise
.
resolve
({
Item
:
fluffy
}));
const
pet
=
await
getPet
(
'
Pets
'
,
'
Fluffy
'
);
expect
(
db
.
get
).
toHaveBeenCalledWith
({
TableName
:
'
Pets
'
,
Key
:
{
PK
:
'
Fluffy
'
}
});
expect
(
pet
).
toEqual
(
fluffy
);
});
});
Enter fullscreen mode
Exit fullscreen mode
Using
mockReturnValueOnce
here gives me the response I'm expecting from the sdk at which point I can continue processing. Our tests are passing now! But our coverage has slipped because we aren't hitting the error condition.
Mocking Errors
This is so easy, it's basically cheating since we already had an error above. We only need to put it into a test. We can use
try
and
catch
to surround a call that throws an error and then test the error response. It's a best practice to tell Jest how many assertions to expect when putting assertions into blocks. Otherwise the code could NOT throw an error and the test might still pass.
test
(
`Can't find Rover`
,
async
()
=>
{
expect
.
assertions
(
1
);
try
{
await
getPet
(
'
Pets
'
,
'
Rover
'
);
}
catch
(
e
)
{
expect
(
e
.
message
).
toBe
(
`Couldn't find Rover!`
);
}
});
Enter fullscreen mode
Exit fullscreen mode
Let's try something a little harder. What if AWS is broken and we want to see what happens with our function? (spoiler: it doesn't work). Instead of having awsSdkPromiseResponse return a value our code treats as an error, we can just have it throw an error.
test
(
`DynamoDB doesn't work`
,
async
()
=>
{
awsSdkPromiseResponse
.
mockReturnValueOnce
(
Promise
.
reject
(
new
Error
(
'
some error
'
)));
expect
.
assertions
(
1
);
try
{
await
getPet
(
'
Pets
'
,
'
Rover
'
);
}
catch
(
e
)
{
expect
(
e
.
message
).
toBe
(
`some error`
);
}
});
Enter fullscreen mode
Exit fullscreen mode
(It's left as an exercise to the reader to decide what kind of errors to handle here).
Test Data Persistence
In short, we don't do that. Some other mocks and frameworks attempt to create a persistent data store and mimic a real database. To me, this is antithetical to a good unit test. In short order we'll have tests that rely on other tests to put data in a certain state and that is not a good place to end up. A good unit test is completely independent as well as deterministic. We can achieve that by not mocking a DynamoDB database but by mocking the API we use to communicate with it.
TypeScript
If you aren't a fan of TypeScript, all of this could theoretically be done in JavaScript, but I'm not sure it is a solid of an idea. One of the reasons this works well is because the DocumentClient actually has a pretty type-opinionated API. If I pass an invalid payload to my
db.put
call, it'll fail in linting and my IDE will warn me I'm writing invalid code. With tools like VSCode, you can get some of that benefit even without TypeScript, but I wouldn't want to try this without any type hints at all. It's too likely to put you in a world where all your code seems to work and your tests pass but nothing works when you deploy it.
Next Steps
There's a lot I left off here because I just wanted to focus on the Jest mocks. After trying a few different things, my team is still bundling Lambda with webpack. Webpack has a learning curve, but it works well and is fast. As noted above, we're not really using SAM very much anymore and the team I'm working with is mostly relying on unit tests and deploying their own stacks to a development environment. In fact, we have constructed our application in such a way that the Lambda and CDK tests run together and it works beautifully.
So that's Jest, a great productivity tool if you like seeing a whole bunch of tests go green in a short amount of time. A couple of other great features we've made solid use of are snapshots and test tables, but since I've gone on and on already, I'll save that for another post.
Top comments
(7)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Brett Ryan
Brett Ryan
Brett Ryan
Follow
Beer drinking big data solution developer who loves to ride a mountain bike and play guitar.
Location
Melbourne, Australia
Work
Technical Project Manager at Entity-Consultel
Joined
Mar 3, 2021
•
Mar 3 '21
• Edited on
Mar 3
• Edited
Dropdown menu
Copy link
Hide
This is an excellent article.  In the last example for
Returning Data
, should the mock return value not be a promise?
awsSdkPromiseResponse
.
mockReturnValueOnce
({
Item
:
fluffy
});
Enter fullscreen mode
Exit fullscreen mode
might be the following?
awsSdkPromiseResponse
.
mockReturnValueOnce
(
Promise
.
resolve
({
Item
:
fluffy
});
Enter fullscreen mode
Exit fullscreen mode
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Matt Morgan
Matt Morgan
Matt Morgan
Follow
AWS Community Builder and co-author The TypeScript Workshop.
Education
Atari 2600
Work
Director of Engineering at CommentSold
Joined
Oct 13, 2019
•
Mar 3 '21
Dropdown menu
Copy link
Hide
Thanks for reading, Brett and good catch. It works the way I had it because you can
await 'hello'
but it's more accurate to have the Promise there.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Benjamin Houdu
Benjamin Houdu
Benjamin Houdu
Follow
Location
Paris
Work
Tech Lead at Euler Hermes
Joined
May 7, 2019
•
Sep 8 '20
Dropdown menu
Copy link
Hide
Good article! Are you not afraid you'll generate too many mocks if your codebase use many dynamo queries? I personally prefer using localstack that mocks ddb, with an init phase and a cleanup after. Less reliant on mock code, and well tested, also it should behave more closely to the final behaviour (arguable).
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Matt Morgan
Matt Morgan
Matt Morgan
Follow
AWS Community Builder and co-author The TypeScript Workshop.
Education
Atari 2600
Work
Director of Engineering at CommentSold
Joined
Oct 13, 2019
•
Sep 8 '20
Dropdown menu
Copy link
Hide
I'm not mocking the query, just the API, so I have to mock each method (get, put, etc) of the API, but don't need to write individual mocks for each query.
As I wrote in the post, I have experience with localstack. Needing to get each service into the correct state (setup, teardown) for tests to run in a continuous integration pipeline (which now needs Docker or localstack installed, of course) was considerably more work for me on the projects I employed that technique than writing a few jest mocks was. For my money, localstack is nearly as much overhead as just using a non-production AWS account and the latter is
considerably
easier to debug.
But ultimately my approach is a different approach. I am deliberately avoiding testing the AWS service. I'm only testing my own code - how does it form the correct payload for a call to an AWS service and how does it handle the expected response? That seems appropriate for a unit test. Thanks for reading and your thoughts!
Like comment:
Like comment:
4
likes
Like
Comment button
Reply
Collapse
Expand
James Starkie
James Starkie
James Starkie
Follow
Full stack developer with a particular passion for UI/interaction design and development.
Location
Liverpool, UK
Work
Senior Technical Lead at Doc Abode
Joined
Oct 8, 2019
•
Nov 5 '20
Dropdown menu
Copy link
Hide
Hi Matt, thanks for this article, absolute lifesaver! I'm used to mocking the AWS SDK in JavaScript, but moving to TypeScript was giving me some real headaches, so using your approach is a big help.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Matt Morgan
Matt Morgan
Matt Morgan
Follow
AWS Community Builder and co-author The TypeScript Workshop.
Education
Atari 2600
Work
Director of Engineering at CommentSold
Joined
Oct 13, 2019
•
Nov 5 '20
Dropdown menu
Copy link
Hide
Great to hear, James!
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Ricardo Sueiras
Ricardo Sueiras
Ricardo Sueiras
Follow
Technologist/Maker/Builder. Developer Advocate at AWS, specialising in open source. Find me on Mastadon at https://hachyderm.io/@094459
Location
London
Work
Developer Advocate at Amazon Web Services
Joined
Sep 2, 2019
•
Sep 4 '20
Dropdown menu
Copy link
Hide
Great post Matt!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Implementing Rate Limiting in NestJS with Custom Redis Storage
Indira Mythili -
Dec 9
Aurora DSQL - Simple Inserts Workload from an AWS CloudShell
Franck Pachot -
Dec 9
AWS re:Invent: Apple announces reliance on Amazon custom AI chips
Brian -
Dec 4
TypeScript's progressive adoption strategy for front-end projects
Tianya School -
Dec 4
Matt Morgan
Follow
AWS Community Builder and co-author The TypeScript Workshop.
Education
Atari 2600
Work
Director of Engineering at CommentSold
Joined
Oct 13, 2019
More from
Matt Morgan
Presenting AWS Speakers Directory, an AI Hackathon Project
#
aws
#
machinelearning
#
hackathon
#
serverless
AWS Users, Roles, and Identity Center Demystified
#
aws
#
iam
#
security
#
cloud
Avoiding the Serverless Workflow Antipattern
#
aws
#
serverless
#
stepfunctions
#
sqs
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Spatial Big Data Systems - a retrospective - DEV Community,"Spatial Big Data Systems - a retrospective - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Dan Voyce
Posted on
Dec 5, 2019
Spatial Big Data Systems - a retrospective
#
bigdata
#
analytics
#
presto
#
mpp
This retrospective was presented as a talk at Cakefest 2019 in Tokyo Japan
Background:
Our specialty is human movement data, we collect anonymized telemetry over millions of devices each day. This consists of some device information (OS, Brand, Model, Wireless capabilities etc). Also consists of Location data (Latitude, Longitude, Altitude etc).
By understanding human movement and intersecting this data with other datasets we can help businesses understand their customers – we can provide insights to allow a business to accurately predict customer intent / requirements.
We maintain a database of Millions of US POI’s, both shape data and ‘point’ data. By spatially joining the human movement data with this POI data we can get context as to the locations people visit and use this to determine intent.
Big Data:
Due to the size of these data sets and the complexity in spatial processing, a normal RDBMS simply won't cut it or allow us to process this data within the 24 hour window we require - let alone allow us to create an interface that allows a human being to design their own audiences on-the-fly and receive instant estimates of the audience size.
The previous article I wrote discusses some of the challenges we faced early on in our development of this platform (
https://www.linkedin.com/pulse/what-functional-big-data-dan-voyce
), This article describes our methodology of testing various solutions before and after that - to bring us to the final solution we are currently using to process over 200B rows each day in under an hour.
The problem:
""To create a system where a customer can design their own audience by choosing and combining customer intents""
I won't go heavily into the analysis of this problem in this article but we eventually landed upon a system that would generate a ""query"" (not necessarily SQL) that we can run against our data warehouse to produce the audience.
The tricky part of this - was
""How do we get an estimate displayed to the customer of the audience size""?
As the end user can design an audience from any combination of 8 filters (each containing 100's - 1000's of options) pre-caching the counts on each processing wasn't really feasible - especially since we were also providing the ability to filter between specific dates.
The solution:
There are a few well known methods for producing fast estimation of large datasets which I won't go into in this article but in both cases they came with a set of problems we needed to overcome, in the end it all came down to the choice of the underlying data warehouse / query engine we decided to use. This is what this article will focus on.
Why count(distinct()) is king - but definitely not equitable.
As developers most of us understand what count(distinct()) does, what many people don't understand (or think about) is HOW it does what it does.
Counting distinct entities in a huge dataset is actually a hard problem, it is slightly easier if the data is sorted but re-sorting data on each insert becomes expensive depending on the underlying platform used.
Generally a count distinct performs a distinct sort and then counts the items in each ""bucket"" of grouped values.
If you need a 100% accurate count then this is unfortunately pretty much the only way you can get it on a randomised dataset, there are tricks you can do in how things are structured in the platform to make things more efficient (partitioning, clustering / bucketing for example) but it essentially still has to perform the same operations.
Thankfully we are not in the position where we immediately require a 100% accurate answer - an estimate with a reasonable error margin is just fine for what we need. This opens up a few more options.
Rejected methods / platforms
These were methods we started to test out early on, we had a number of requirements that some were not able to provide (for example Geospatial processing is a pre-requisite we had as many of the counts we would have to provide would be based on Geospatial predicates).
Final method / platform choices
Which ever method we chose it would still need to be underpinned by a solid and future proof data warehouse, in the end we decided upon 2 to power different parts of the platform (to eventually be consolidated into a single data warehouse later in the process).
HyperLogLog++
For many years this was the gold standard in cardinality estimation, for static datasets it is still the recommended method as it is easily stored, very low maintenance and can merge counts from multiple ""columns"" to get an overall count. Unfortunately for us our queries were not static, we would have had to create a HLL sketch for every single combination of columns which wasn't feasible.
VerdictDB / Keebo
Early on during the process we started working with the guys at VerdictDB. VerdictDB is a fantastic product that uses probability / statistical theory to create estimates of cardinality on large datasets. With the support of the team at Verdict we were able to create a bridge to incorporate this into each of our platforms.
Presto
Presto is a query engine designed to provide an ""SQL on Anything"" type of interface for data. As all of our data was stored in Google Cloud Storage it was a natural choice. It is blazingly fast even with a small number of nodes and completely customisable. You can see why companies such as Teradata and Starburst data have adopted it to power their platforms.
There was a fair bit of optimisation required around the ORC format we chose to use with Presto but once this was completed we could power our Fast counts with a minimal 3 node cluster running over the scrambles and a 10 node cluster to perform the pre-processing required.
As long as good housekeeping was maintained on the data, the fast counts would usually return within around 5-7 seconds.
BigQuery
BigQuery is a Google product based on Dremel, it uses a massively parallel architecture to plan queries across a huge number of nodes all packaged as a fully managed and on-demand service to the user.
BigQuery BI is an in-memory caching service provided which cuts down on the 3s overhead to queries issued to BigQuery. By caching the scrambles and linking our infrastructure directly into this, we were able to ensure that the query would return in around 2 seconds on average.
Conclusion
With the use of VerdictDB both Presto and BigQuery provided the speed required to allow a human interface to our Data Warehouse, BigQuery out performed Presto in a number of areas when BigQuery BI was thrown into the equation, and although this is still in beta offering only 10GB (should be enough to cache a 1% scramble of 1TB of data), it has huge potential in offering a cost-effective and fast interface to Big Data.
If you want to avoid vendor lock-in then Presto is a fantastic choice, throwing more nodes at the situation would easily get the speed down to the BigQuery equivalent. There are also several other options that exist that could be used as the query interface once the scrambles are built on Presto!
Daniel Voyce - CTO
Daniel leads the technology strategy, architecture and development at LOCALLY.
He calls Melbourne, Australia home and is an accomplished technical leader with extensive experience in defining and developing digital roadmaps, highly technical systems and frameworks, and software platforms.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Angular Techniques for Performance and Maintainability
Karthika Murugesan -
Dec 12
How Clinical Trial Optimization Solutions Can Revolutionize Drug Development
Jesvira Dsouza -
Dec 12
🆚 Bun vs Deno: When to Use Each in Your Projects?
Raka Widhi Antoro -
Dec 12
Adonisjs
Mohanraj Rajaram -
Dec 12
Dan Voyce
Follow
Technical Solutions Director at DemystData, Formally CTO at LOCALLY: Big Data, PHP Frameworks, Open Source and Location / Geospatial data
Location
Melbourne, AU
Work
Director, Technology Solutions (APAC) at DemystData
Joined
Jul 11, 2019
More from
Dan Voyce
Converting CSV to ORC/Parquet fast without a cluster!
#
bigdata
#
orc
#
parquet
#
cudf
PySpark and Parquet - Analysis
#
pyspark
#
parquet
#
bigdata
#
analysis
Creating a proof of concept for Spatial Joins
#
qgis
#
spatial
#
geospatial
#
bigdata
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
What We Learned From Analyzing 20.2 Million CI Jobs In Trunk Flaky Tests - Part 2 - DEV Community,"What We Learned From Analyzing 20.2 Million CI Jobs In Trunk Flaky Tests - Part 2 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Vincent Ge
Posted on
Nov 27
What We Learned From Analyzing 20.2 Million CI Jobs In Trunk Flaky Tests - Part 2
#
testing
#
tooling
#
productivity
#
devops
This is part two of a blog series about building Trunk Flaky Tests.
Part one
covered what we’ve learned about the problem space surrounding flaky tests, like how they impact teams, why they’re prevalent, and why they’re so hard to fix. This part focuses on the technical challenges of building a product to detect, quarantine, and eliminate flaky tests.
If you’re curious about what we’ve built, you can find more in the
Trunk Flaky Tests Public Beta Announcement blog
.
Building a Solution
So far, we’ve only discussed the pain points we’ve discovered while working with our beta customers. To recap, flaky tests are prevalent if you write end-to-end tests, but teams underestimate how many of their tests flake and how much time is wasted dealing with Flaky Tests. Fixing flaky tests can cost more engineering resources than ignoring them, but letting flaky tests accumulate will become debilitating. Finding practical ways to deal with flaky tests is the challenge at hand. Let’s talk about some of the technical challenges we’ve faced in building a solution to this problem.
The Scale of Flaky Tests
In
Uber’s Go monorepo
, approximately 1,000 out of 600,000 tests were consistently detected as flaky.
Dropbox reported
running over 35,000 builds and millions of automated tests daily, with flaky tests causing significant operational overhead.
At Google
, out of 4.2 million tests run on their CI system, about 63,000 (1.5%) had a flaky run over a week. More strikingly, approximately 16% of all their tests exhibit flakiness. Over our private beta, we’ve also identified
2.4K
tests that we labeled flaky.
The Challenges That Come With Scale
Production test suites can be huge. They’re run hundreds of times daily on hundreds of PRs, making it challenging to process all that test data to detect flaky tests. Over our private beta, we
processed
20.2 million uploads. This means we received 20.2 million uploads of test results from different CI workflows. Each upload can contain the results of one or many test suites worth of results.
Even with a limited number of private beta customers, Flaky Tests processes 40 times more data than all of Trunk’s other product offerings
combined
. We see thousands of times more data for specific types of tables in our database. The volume of data is a challenge in its own right and difficult to query. Creating summary metrics for each test and overall summary metrics across thousands of runs of
tens of thousands of tests
is very time-consuming.
Single uploads could contain thousands of tests from a CI job. This data needs to be processed, transformed, and labeled to be displayed on various parts of the Flaky Test dashboard. For example, individual test runs need to have their failure reason summarized by an LLM to be displayed in the test details tab. They also need to be labeled with the correct status, such as if they’re healthy, broken, flaky, and if they’ve been rerun on the same PR.
The practical implication is that we can’t naively query and process the data at the time of request because it would take minutes, which is unacceptable. The metrics need to be precomputed in a timely manner. No one wants to wait more than a few minutes for a
summary of a PR
.
It’s also not trivial to precompute and cache metrics. Different metrics take a different amount of time to compute. If one metric updates faster than another, you’ll see
conflicting numbers on different pieces of UI
, which is unacceptable. It turns out that extracting useful metrics from a bunch of test runs and providing them quickly enough to be useful is already a challenge.
Displaying Tests Results
Another interesting challenge is displaying the thousands of tests, their results, their health status, whether they have an attached ticket/issue, and whether they’ve been quarantined. A lot of information needs to be displayed differently if you’re a team lead keeping track of overall test health or a contributor debugging flaky tests on a PR. After revisions with our beta partners, we landed on three key views for different workflows.
The first screen is the overview dashboard, which shows the overall health of your tests across many CI jobs. You’ll see how many tests are flaky and broken, and you’ll see time-series data on the trends in your repo. This view focuses on helping you understand the overall impact of flaky tests on your repo and finding the highest-impact tests to tackle next.
When you click on a single flaky test, it opens another screen that focuses on helping you debug an individual test. You’ll see a report of its results over the last seven days and the unique ways that the test has failed. You can see when the test started failing and on which branches. This helps you understand what might have caused the test to become flaky and how it fails when it’s flaky, and ultimately helps you debug the test.
On this same screen, you can click through the stack trace of different test runs with a similar failure reason to help debug flaky tests.
The third screen is a
PR Test Summary
, which shows an aggregated report for all test results from a single PR. This screen slices the data differently so engineers working on a single PR can quickly scan through failures and verify the results. This screen shows if a test failure is a known flaky test, a known broken test, if it’s quarantined, or if this PR introduces the failure. This helps engineers quickly identify the failures they need to address.
These screens don’t come from the requirements or feedback of a single customer, everyone has a different set of information that they intuitively believe to be important. Through the beta program, by pooling common feedback from different customers, we think these three different screens manage to condense the massive amount of information needed to understand flaky tests into a digestible format. It fits a variety of use cases and scales well even if you’ve got tens of thousands of tests.
Data Quality
There isn’t a
single shared standard output format
for test results. While most test runners produce JUnit XML, that is still a loose standard. You can't produce high-quality metrics if you don’t get high-quality data uploads.
When we ingested data from our customers, we saw random test names, automatic reruns reported as duplicates or overriding the original results, no test file paths, and all kinds of weird data forms. Handling and sanitizing all of this data is challenging. It felt like a game of constant whack-a-mole: We figure out a way to handle one type of data anomaly and out comes another.
Detecting the inconsistencies and problems in a customer’s uploaded data, prompting them with the proper follow-up steps, and sanitizing the rest is challenging and time-consuming. While we can justify the cost of handling new logic for a new language or test runner, we can’t imagine the pain of building a similar in-house solution, especially if you’ve got a diverse testing stack with legacy code.
Detection Rules
What is a flaky test? It’s a debatable and nuanced topic that begins to feel more like a philosophical question than an engineering one when scrutinized. A test can be non-deterministic in a variety of ways:
How frequently does it have to fail?
Is a timeout failure the same as a DB connection failed failure?
Should a failure on a PR treated the same as on the main branch?
Should the window for detecting a test’s health be N number of runs or a specific time window?
How do we adjust detection for repos that see 10 test runs vs 1000 test runs?
How do we determine if a test is healthy again after a fix?
What about edge cases of tests that fail more than they pass? What about tests that fail once every 1000 runs?
What if tests never run on
main
or another protected branch?
There are many signals to consider when detecting Flaky Tests; the answers here aren’t black and white. While detection can never be perfect and never needs to be perfect, it’s possible to hit a maximum for the vast majority of projects and fine-tune for the outliers. Even during a limited private beta, having access to many diverse technology stacks and millions of test runs gives Trunk Flaky Tests a unique advantage in refining our algorithm continuously.
The private beta customers have already helped us find a number of edge cases that have helped us detect flaky tests more accurately. Our partnership with partners during our private beta and our new public beta program will hopefully allow us to further improve this process.
Quarantining Tests
Quarantining lets you isolate failures for known flaky tests so they don't fail your CI jobs while continuing to run them. Quarantine looks for known flaky tests and determines which tests should be isolated at runtime so you can avoid code changes usually required to disable flaky tests. This is better than disabling tests because it doesn’t require commenting out tests, which will likely never be revisited. After all, disabled tests produce no noise. It also automatically allows for “un-quarantining” tests if we detect that it’s healthy after a fix. The bottom line is that you’re less likely to lose tests when quarantined.
Quarantining isn’t as challenging to implement as it is tedious. To implement quarantining, you first need a detection engine for flaky tests. Then, you need to serve that information through some API that a script can fetch in CI jobs to see if a test is known to be flaky. That script needs to run and collect the results of tests (recall the non-standard output formats), compare these results with the API response, and then override the exit code if all failures are quarantined. It must handle overrides for specific tests that should always be quarantined or never quarantined. And that script must work on whatever platform and environment you choose to run your CI jobs on. Finally, you must communicate what’s quarantined in each CI job and track quarantined test results. None of this is very sophisticated, but getting all the domino pieces to fall into place nicely is hard.
End-to-End UX
How do you manage flaky tests end-to-end? As discussed in Part 1 of this blog, we don’t see a consensus on a workflow online or among our beta partners. So, if a convention doesn’t exist, we must propose one ourselves. Here’s what we build with the help of our closed-beta customers.
What’s Next?
We don’t think flaky tests are a realistic or practical problem for any tooling team to tackle in-house. The problem is nuanced, and there are no conventions for dealing with it effectively. Our private beta customers allowed us to facilitate much-needed discourse around this problem. By having diverse teams and CI setups to test our solutions and pool feedback and ideas, we can help everyone reduce the cost of trial and error when dealing with flaky tests.
We’re opening Trunk Flaky Tests for public beta so you can help us build a more effective solution for all. The sooner we stop suffering in isolation, the sooner we’ll end flaky tests.
Join the Trunk Flaky Tests Public Beta
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Setup NVM in your organization/admin system's like a pro
Sudhanshu Chaubey -
Dec 7
Testing Different Navigation Options with Compose
Eevis -
Dec 4
How to Get Started with Elon Musk AI Grok for FREE
Vladislav Guzey -
Dec 11
Why Monorepo Projects Sucks: Performance Considerations with Nx
Jeferson F Silva -
Dec 11
Vincent Ge
Follow
Developer Relations Engineer at Trunk.io, prev. Appwrite.
Education
University of Waterloo
Work
Trunk.io
Joined
Jan 3, 2022
More from
Vincent Ge
What We Learned From Analyzing 20.2 Million CI Jobs In Trunk Flaky Tests - Part 1
#
testing
#
tooling
#
productivity
#
devops
Convincing non-technical leadership to invest in DevEx.
#
devex
#
tooling
#
testing
#
leadership
Regex, the good bits.
#
regex
#
tooling
#
cli
#
javascript
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Serverless Big Data Pipelines - Exploring NFL Football Data with Azure Databricks - DEV Community,"Serverless Big Data Pipelines - Exploring NFL Football Data with Azure Databricks - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
LaBrina Loving
Posted on
Sep 12, 2020
• Edited on
Sep 15, 2020
Serverless Big Data Pipelines - Exploring NFL Football Data with Azure Databricks
#
azure
#
databricks
#
serverless
#
azurebacktoschool
This article is part of
#AzureBacktoSchool
. You'll find other helpful articles and videos in this Azure content collection. New articles are published every day from community members and cloud advocates in the month of September. Thank you
Dwayne Natwick
for organizing this awesome idea!
So along with going back to school, American Football returned this week and I'm ready for some Football!!! I've always been very fascinated with how teams and sports analyst use historical data to predict player performance and predict who will win the game and other player statistics.  The concepts and approach here is no different than how you might use Big Data in Sales Forecasting, Predictive Repair for machinery, Inventory Demand scenarios.
This blog post will show how to start building a big data analytics platform using serverless technologies.  I will show how to move data from a source to
Azure Data Lakes
using
Azure Data Factory
and then do some exploration and analysis using
Azure Databricks
.  This will be the first in many posts I will do on Serverless Big Data Pipelines using football datasets.
Big Data Architectures
When you think about a big data analytics platform, there are typically these components:
Data sources
. All big data solutions start with one or more data sources. For my solution, I've identified several sources listed
here
.  For the purposes of this blog, I'm going to play around with play by play data found
here
.
Data storage
. Data for batch processing operations is typically stored in a distributed file store that can hold high volumes of large files in various formats. This kind of store is often called a
data lake
. A data lake provides a centralized store for data engineers and data scientists to explore data in it's raw state.  I will be using
Azure Data Lake Storage Gen 2
.  Azure Data Lake Storage provides fast, secure, scalable cloud storage along with hierarchical namespaces and a file access control using a combination of role based access (RBAC) and POSIX-compliant Access Control Lists (ACLs).
Orchestration
. Most big data solutions consist of repeated data processing operations, encapsulated in workflows, that transform source data, move data between multiple sources and sinks, load the processed data into an analytical data store, or push the results straight to a report or dashboard. In our solution, we will be moving files from various sources exploring, cleansing, experimenting, and aggregating.  For this, we will use
Azure Data Factory
.
Batch Processing
. Big data solutions often use long-running batch jobs to filter, aggregate, and otherwise prepare the data for analysis.  We'll use Azure Databricks is an Apache Spark-based analytics platform.
Analytical data store
. Many big data solutions prepare data for analysis and then serve the processed data in a structured format that can be queried using analytical tools.  I won't yet cover these in this blog post.
Analysis and reporting
. The goal of most big data solutions is to provide insights into the data through analysis and reporting. In this you might use tools like
Power BI
or Azure Synapse.  I also won't yet cover these in this blog post.
Getting Started
Ok, enough theory, let's actually build something!!!  To get started, let's provision the following services in our Azure Portal:
Azure Data Lake Storage Gen 2(ADLS)
Azure Data Factory
Azure Databricks Workspace
and
Cluster
Once you've got these services setup as shown below.
You'll want to go ahead and create a container in your ADLS.
Make sure that our Azure Data Factory can access our ADLS.  You'll do this by giving the Data Factory 'Storage Blob Data Contributor' rights to our ADLS using it's Managed Service Identity. Managed Service Identity is a way to give an identity to your Azure resources in Azure AD and handles cleaning up this identity automatically when the resource is deleted.  A managed identity is automatically created when you create an ADF resource through the Azure Portal.
To do this, go to the ADLS and select Access Control(IAM).  Click Add role assignment.  Select Storage Blob Data Contributor and select the name of the Azure Data Factory resource you created:
Copying data to ADLS
Now that the resources are setup in Azure, let's create a pipeline to copy over the files you need into ADLS.  Go into Azure Data Factory, click Author and Monitor.
You'll see an authoring canvas with a lot of quick start buttons to get you started.  Click on the Copy Data to launch the wizard.
Enter name for your copy pipeline and click 'Next'
Next up, you will setup our data source.  In order to setup a data source, you must create a Dataset and a Linked Service.  A dataset is a named view of data that simply points or references the data you want to use. Linked services are much like connection strings, which define the connection information needed for Data Factory to connect to external resources. So in setting up your data source, the dataset is the ""what"" and the linked service is the ""how"".  Azure Data Factory supports over 90 data connectors.
For this blog, I will start with the
2019 play-by-play
data located
here
.  Because we are using csv files located in Github, I will use the
HTTP connector
. To create your Http Connector Data Source, select File, and Add new Connection to create your HTTP Linked Service.
Give your Linked Service a name, and set the Base URL to '
https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/
'. I'm going to set this up with a Base URL because I will likely be copying several files over from this source and it will be easier to manage as my linked service.  Set Authentication type to 'Annonymous' and click Create.
You should see your Linked Service created, click next to setup dataset.
To setup our HTTP dataset, enter the relative url as 'play_by_play_data/regular_season/reg_pbp_2019.csv' and click Next.  You should see a preview of the file like this:
Click next.
To setup ADLS as our destination store, you will again need to create a Linked Service and Dataset.  Since you are saving this file to ADLS, select category Azure and click 'Create new connection'. Select Azure Data Lake Storage Gen2 and click Continue.
Give your linked service a name, change authentication to
Managed Identity
, select the ADLS storage you created.
Enter the output folder and destination file name.
Click through to summary to show our copy pipeline setup.
Finally deploy and run the pipeline.
To verify that our file has been properly copied over, you can open Storage Explorer, go to our Storage container and confirm file is there.
Data Exploration
So now that I have some files in my Data Lake, let's start exploring.  Typically you will want to explore the data, start building out data definitions, removing any unnecessary columns, and cleansing.  To start exploration, I will use Azure Databricks.
There are three ways of accessing Azure Data Lake Storage Gen2:
Mount an Azure Data Lake Storage Gen2 filesystem to DBFS using a service principal and OAuth 2.0.
Use a service principal directly.
Use the Azure Data Lake Storage Gen2 storage account access key directly.
For production systems, I would highly recommend going with option 1., but since we are just starting out, let's pick option 3.
To get started, launch the Azure Databricks resource you created above. Create a blank notebook.  In the first cell, you will connect to our ADLS context:
spark
.
conf
.
set
(
""fs.azure.account.key.<storage account name>.dfs.core.windows.net""
,
""<storage account key>""
)
Now that we've connected to our storage, we'll want to test out our connection by listing all the files in the directory.  We will use the
Filesystem utilities
in Databricks for this:
dbutils
.
fs
.
ls
(
""abfss://<container name>@<storage account name>.dfs.core.windows.net""
)
When you Run all, you will see the list command shows our .csv files copied over.
Now that the files are mounted, load them into a dataframe. A dataframe is a just a table similar to SQL table or even an Excel datasheet that you will perform operations like selecting, filtering, grouping, and joining.  The only difference is that it exists in memory only. To do this write the following code in our cell:
#set the data lake file location:
file_location
=
""abfss://<container name>@<storage name>.dfs.core.windows.net/*.csv""
#read in the data to dataframe df
df
=
spark
.
read
.
format
(
""csv""
).
option
(
""inferSchema""
,
""true""
).
option
(
""header""
,
""true""
).
option
(
""delimiter""
,
"",""
).
load
(
file_location
)
#display the dataframe
display
(
df
)
Notice I've done a couple of things.  I've set the file location to *.csv.  This will actually read all of our play-by-play csv files instead of just one.  I've also set ""inferSchema"" to true which and ""header"" to true.  Run all again, and you should see all of the rows in the file loaded into the dataframe:
Now at long last, let's start doing some data exploration with some queries.  To start, let's say that I want to see total passing yards by team and player.  In a new cell, I'll put the following code:
%
sql
SELECT
posteam
,
passer_player_name
,
sum
(
yards_gained
)
FROM
playbyplay_view
where
play_type
=
""pass""
and
yards_gained
>
0
group
by
posteam
,
passer_player_name
I used the magic %sql command so I can now write queries against my dataframe using normal SQL statements.  When I run this command, I see the following
Great!!! I'm now setup to do more exploration of this data and figure out key data that I want to further analyze and aggregate into permanent data store.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Optimizing Telemetry Tracking with Azure Application Insights: A Case Study
Akshay Ghadge -
Nov 18
Azure Function App (Flex Consumption) in private VNET via IaC
Roman Kiprin -
Nov 30
How to create an App Service Application and upload into it (WEB APP)
Adedapo -
Nov 3
Leveraging GenServer and Queueing Techniques: Handling API Rate Limits to AI Inference services
Ahsan Nabi Dar -
Nov 2
LaBrina Loving
Follow
Senior Engineer @Microsoft focused on Azure for ISV's and Startups,  Dev Community Organizer, Speaker, STEM Advocate.
Joined
Mar 1, 2018
More from
LaBrina Loving
Set Azure PlayFab Title Data from Google Sheets with No Code
#
gamedev
#
azure
#
playfab
Migrating SharePoint Files to Azure Data Lakes Storage
#
sharepoint
#
azure
#
storage
Power to the Enterprise – Building Bots with Power Virtual Agents
#
ama
#
powerfuldevs
#
azure
#
powerplatform
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Tech leadership playbook - DEV Community,"Tech leadership playbook - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Alexsandro Souza
Posted on
Nov 20, 2021
• Edited on
Feb 10
Tech leadership playbook
#
productivity
#
career
#
java
#
machinelearning
Follow me on Linkedin if you like provoking posts about software development.
Let's socialize!
It is often very challenging to think for yourself, and it is easier to maintain the status quo. I have a unique view of the software development world, and after researching and reflecting on different topics, I usually transform my thesis into blog posts.
Below are some of my reflections that will help you and your team achieve more when delivering value through software.
Team management
Team Principles
Forming, Storming, Norming and Performing
Great leaders create principles, not rules
Practical ways to improve team communication
Costs of loosing an employee
Is the 10x developer a myth?
You are bad at haring good engineers
Democratic voting can kill innovation and be an illusion of safety
Team effectiveness manifesto
What is DX and why you should invest in it?
Expert generalist
What important truth do very few people agree with you on?
The small fraction of your team will do most of the work
Project management
Agile estimation and planning
Software development life cycle
The cost of delays
The truth about technical debts
How to document software?
Epics, User Stories and Tasks
Project management guideline
It is all about trade-offs
Agile principles
Keep track of software decisions
Don't developers like meetings?
What is a bug?
Software design and architecture
How to structure code effectively
Rethinking Domain Driven Design: Beyond Tactical Patterns
Microservices vs monolithic
Are you starting with microservices? Forget the DRY principle and embrace redundancy
The modern way of managing APIs using Protobuf and OpenAPI
Protobuf API contract guideline
How to implement gRPC for production
QA
How to test software effectively
Questions/Answers about software testing
Architecture testing
Quality Assurance roles and responsibilities
Best practices
Code review best practices
Is it true that good programmers don't use the else clause?
Java best practices
Golang best practices
Meetings best practices
Logging best practices
DevOps
Service mesh - How to manage microservices communication with Istio
Big data
How to build real-time Big Data pipeline
Java
Java under the hood
JVM execution dilemma
Security
Security with PHP - Portuguese
Front-end
A new age for front-end developers - Portuguese
Business
Economy, investments, wealth, human nature and bitcoin
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Optimizing Docker Image Size for Java Applications: Multi-Stage Builds & JLink Explained
Subbu Tech Tutorials -
Dec 11
Understanding Strings and Arrays in Java
Ricardo Caselati -
Dec 11
Worst Interview Question Ever to Ask a Candidate ""Show us a project you are proud of""
mosbat -
Dec 11
How I Improved My Productivity as a Developer in 30 Days 🚀
Crypto.Andy (DEV) -
Dec 10
Alexsandro Souza
Follow
Team lead | Agile coach | Speaker | Active blogger | Opensource contributor | DevOps enthusiast | Computer vision practitioner
Location
Dublin
Joined
Mar 24, 2019
More from
Alexsandro Souza
Should we use Reactive architecture with Java?
#
java
#
reactiveprogramming
Recommended Software development life cycle
#
devops
#
cicd
#
testing
#
productivity
AI pair programming is a blessing or a curse?
#
productivity
#
programming
#
ai
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Securing Cloud-Native Databases and Big Data Solutions - DEV Community,"Securing Cloud-Native Databases and Big Data Solutions - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
iskender
Posted on
Nov 27
Securing Cloud-Native Databases and Big Data Solutions
Securing Cloud-Native Databases and Big Data Solutions
The shift towards cloud-native architectures and the exponential growth of data have propelled organizations to adopt cloud-native databases and big data solutions. These technologies offer scalability, flexibility, and cost-effectiveness, but they also introduce new security challenges. Securing these environments requires a comprehensive approach that addresses the unique characteristics of cloud-native deployments and the complexities of big data ecosystems.
I. Understanding the Security Landscape
Cloud-native databases and big data solutions differ significantly from traditional on-premises deployments. They leverage distributed architectures, microservices, containers, and serverless technologies, expanding the attack surface and introducing new vulnerabilities. Key security challenges include:
Distributed Attack Surface:
The distributed nature of cloud-native deployments increases the potential entry points for attackers, making it harder to maintain consistent security controls.
Ephemeral Infrastructure:
The dynamic and ephemeral nature of containers and serverless functions complicates traditional security approaches that rely on static configurations and perimeter-based defenses.
Data Volume and Velocity:
The sheer volume and velocity of big data make it challenging to implement real-time security monitoring and threat detection.
Complex Access Control:
Managing access to distributed data stores across various microservices and users requires granular and dynamic access control mechanisms.
Integration Complexity:
Cloud-native databases and big data solutions often integrate with numerous other cloud services, creating dependencies and potential security gaps.
Compliance and Governance:
Organizations must comply with industry regulations and data privacy laws, adding another layer of complexity to securing cloud-native data.
II. Core Security Principles for Cloud-Native Databases and Big Data
Securing cloud-native databases and big data solutions requires adhering to established security principles and adapting them to the cloud-native context. These principles include:
Zero Trust Security:
Assume no implicit trust and verify every access request, regardless of the source. This principle necessitates strong authentication, authorization, and continuous monitoring.
Least Privilege Access:
Grant users and services only the minimum permissions necessary to perform their tasks. This reduces the potential impact of compromised credentials.
Defense in Depth:
Implement multiple layers of security controls to protect against various threats. This includes network security, application security, data encryption, and access management.
Security Automation:
Automate security tasks as much as possible to improve efficiency, consistency, and responsiveness to threats. This encompasses automated vulnerability scanning, incident response, and security policy enforcement.
Continuous Monitoring and Threat Detection:
Continuously monitor the environment for suspicious activity and potential threats. Leverage security information and event management (SIEM) systems, intrusion detection systems (IDS), and machine learning-based analytics for threat detection.
Data Encryption:
Encrypt data at rest and in transit to protect it from unauthorized access. Utilize strong encryption algorithms and key management practices.
Regular Security Assessments:
Conduct regular vulnerability scans, penetration tests, and security audits to identify and address weaknesses in the system.
III. Specific Security Measures for Cloud-Native Databases
Cloud-native databases, such as NoSQL databases, NewSQL databases, and database-as-a-service (DBaaS) offerings, require specific security measures:
Network Segmentation:
Isolate database instances from other parts of the infrastructure using virtual private clouds (VPCs), security groups, and network access control lists (NACLs).
Database Authentication and Authorization:
Implement strong authentication mechanisms, such as multi-factor authentication (MFA), and granular authorization policies to control access to database resources.
Data Encryption:
Encrypt data at rest using database-native encryption features or cloud provider-managed encryption keys. Encrypt data in transit using TLS/SSL.
Database Auditing:
Enable database auditing to track user activity, data modifications, and security-related events.
Vulnerability Management:
Regularly scan database instances for vulnerabilities and apply security patches promptly. Utilize automated vulnerability management tools to streamline the process.
Secure Configuration Management:
Enforce secure configurations for database instances and ensure that they comply with industry best practices and organizational security policies.
Data Masking and Anonymization:
Implement data masking and anonymization techniques to protect sensitive data in non-production environments.
IV. Security Measures for Big Data Solutions
Securing big data solutions involves addressing the unique challenges of processing and storing massive datasets:
Data Lake Security:
Secure the data lake by implementing robust access controls, data encryption, and data governance policies. Control access to the data lake using role-based access control (RBAC) and attribute-based access control (ABAC).
Data Pipeline Security:
Secure data pipelines by encrypting data in transit and at rest, implementing authentication and authorization for pipeline components, and monitoring pipeline activity for anomalies.
Hadoop Security:
Implement Kerberos authentication, authorization using Apache Ranger or Sentry, and data encryption using Hadoop's encryption features for Hadoop-based big data deployments.
Spark Security:
Secure Spark applications by enabling authentication and authorization, encrypting data in transit, and implementing secure configuration practices.
Data Governance and Compliance:
Establish data governance policies and procedures to ensure data quality, privacy, and compliance with regulations. Implement data lineage tracking to understand the flow of data through the big data ecosystem.
Real-time Security Monitoring:
Implement real-time security monitoring and threat detection capabilities to identify and respond to attacks against big data infrastructure. Leverage machine learning-based analytics to detect anomalous behavior.
Secure Data Sharing:
Securely share data with authorized users and external partners using secure data sharing mechanisms, such as data clean rooms and privacy-enhancing technologies.
V. Leveraging Cloud Provider Security Services
Cloud providers offer a wide range of security services that can be leveraged to enhance the security of cloud-native databases and big data solutions:
Identity and Access Management (IAM):
Utilize IAM services to manage user identities, authentication, and authorization across the cloud environment.
Key Management Services (KMS):
Use KMS to securely manage encryption keys and secrets.
Security Information and Event Management (SIEM):
Integrate with cloud provider SIEM services to collect, analyze, and respond to security events.
Web Application Firewall (WAF):
Deploy WAFs to protect web applications that interact with databases and big data solutions from common web attacks.
Network Security Services:
Leverage network security services, such as virtual private clouds (VPCs), security groups, and firewalls, to isolate and protect cloud resources.
Data Loss Prevention (DLP):
Implement DLP solutions to prevent sensitive data from leaving the organization's control.
VI. DevSecOps for Cloud-Native Data Security
Integrating security into the development and operations lifecycle is crucial for securing cloud-native databases and big data solutions. DevSecOps practices enable organizations to automate security tasks, enforce security policies, and address security vulnerabilities early in the development process. Key DevSecOps practices include:
Infrastructure as Code (IaC):
Use IaC to define and manage infrastructure security configurations in a consistent and repeatable manner.
Automated Security Testing:
Integrate security testing tools, such as static code analysis, dynamic application security testing (DAST), and penetration testing, into the CI/CD pipeline.
Security Policy as Code:
Define and enforce security policies as code to ensure consistent security configurations across the environment.
Continuous Compliance Monitoring:
Automate compliance checks and audits to ensure that the environment meets regulatory requirements and organizational security policies.
VII. Incident Response and Recovery
Despite the best security measures, security incidents can still occur. Organizations need to have a well-defined incident response plan in place to detect, contain, and recover from security incidents:
Incident Detection and Analysis:
Implement security monitoring and threat detection capabilities to identify security incidents quickly.
Incident Containment and Eradication:
Take immediate action to contain the impact of a security incident and eradicate the root cause.
Data Recovery:
Develop data recovery procedures to restore data and systems after a security incident.
Post-Incident Analysis:
Conduct a thorough post-incident analysis to identify lessons learned and improve security controls.
VIII. Emerging Trends in Cloud-Native Data Security
The field of cloud-native data security is constantly evolving. Emerging trends that are shaping the future of cloud-native data security include:
Confidential Computing:
Confidential computing technologies enable data to be processed in encrypted memory, protecting it from unauthorized access even by privileged users or cloud providers.
Privacy-Enhancing Technologies (PETs):
PETs, such as differential privacy and homomorphic encryption, allow organizations to analyze data without revealing sensitive information.
Artificial Intelligence (AI) and Machine Learning (ML) for Security:
AI and ML are being used to enhance threat detection, automate security tasks, and improve security incident response.
Serverless Security:
Serverless computing introduces new security challenges, and new tools and techniques are emerging to address these challenges.
Data Mesh Security:
Data mesh architectures require a decentralized approach to data security, with security controls embedded within each data domain.
Conclusion
Securing cloud-native databases and big data solutions is a complex but essential undertaking. By adopting a comprehensive security strategy that encompasses zero trust principles, layered security controls, automation, continuous monitoring, and a strong incident response plan, organizations can mitigate the risks associated with cloud-native data and unlock the full
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Just Launched: Shopify’s BORING Edition!
DEV Launches -
Dec 11
Join us for the Bright Data Web Scraping Challenge: $3,000 in Prizes!
dev.to staff -
Dec 11
What is your favorite IDE?
Mercy -
Dec 11
Should You Use an Open-source SaaS Boilerplate Starter or a $300+ Paid One?
Matija Sosic -
Dec 11
iskender
Follow
Cyber Sec.
Location
Istanbul, Turkey
Joined
Nov 27, 2024
More from
iskender
Incident Response and Disaster Recovery
Multi-Factor Authentication (MFA)
Cloud-Based Vulnerability Scanners and Tools
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Big Data Storage Trends and Insights - DEV Community,"Big Data Storage Trends and Insights - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Mainul Hasan
Posted on
Oct 16
• Originally published at
webdevstory.com
Big Data Storage Trends and Insights
#
cloudcomputing
#
cloudstorage
#
bigdata
#
googlecloud
It’s just not an idea; data is growing at an unprecedented rate. However, it has become one of the most significant challenges for businesses to manage this enormous influx of data efficiently and securely.
By 2025, the world will create
463 exabytes
of data daily. That’s equivalent to over 200 million DVDs of new data produced every 24 hours. However, the question is whether our storage solutions are ready to hold this surge in data.
The challenge? Finding storage solutions that can keep up with the rapid growth of data while ensuring scalability, performance, and cost-effectiveness.
The good side is that our strategies to store and manage data are evolving. And with the advancement of storage solutions, we can confidently navigate the complexities of big data storage.
# Big Data: A Growing Challenge
Big data
has transformed the way organizations handle information. The five key characteristics of big data-velocity, Volume, Variety, Veracity, and Value (5Vs)-define and give direction to the challenge. Every second, a wide variety of sources generate new data, rendering traditional storage methods insufficient.
For example, IoT devices generate massive amounts of real-time data, and industries like healthcare and e-commerce rely on instant access to this real-time data. So, the storage infrastructure has had to keep pace. While this growth presents immense opportunities, it also comes with challenges, such as ensuring fast retrieval, reliability, and data security.
Due to their limitations, traditional storage methods, like RDBMS, struggled to keep up with growing data demands. This is still true, but today, storage systems like distributed file systems and cloud-native storage have become standard.
# Key Storage Challenges
Although the nature of big data remains consistent, how we store it has drastically changed.
Historically, challenges like inefficient storage utilization and power consumption were among the primary obstacles.
Today, those issues are worsening because of the need for real-time data access and
enhanced cybersecurity
.
Even with progress, there are still some problems to solve:
Storage systems often lead to wasted capacity. Today, dynamic provisioning and cloud-based systems have made it easier to allocate storage where and when needed. Many organizations and businesses still face over-provisioning issues.
Another critical priority for cloud providers is security because of the vast cyberattacks. While data centers and storage solutions have made great strides in security, challenges like ransomware have become even more complex, requiring encryption and real-time monitoring.
With the rise of data privacy regulations like
GDPR
in Europe and
CCPA
in California, companies must also ensure that their storage solutions comply with these strict data governance laws.
This adds another layer of complexity to the storage challenge as organizations secure data and provide global privacy standards to handle it.
Data governance is critical to any contemporary storage strategy, as noncompliance can lead to substantial penalties and harm to one’s reputation.
Developing Applications with Google Cloud Specialization: Learn how to build secure and scalable cloud-native applications with Google Cloud Training.
# Adapting to a Dynamic Environment
As we evolve beyond traditional storage systems, newer solutions are stepping in to address modern needs.
The rise of cloud-native storage and hybrid systems has enabled organizations to balance cost, flexibility, and control.
Companies can now use a combination of on-premises systems and cloud infrastructure, allowing them to scale their data storage dynamically based on real-time demands.
One of the most significant inventions has been
software-defined storage (SDS)
, which provides a more flexible and cost-effective way to store data.
SDS abstracts the storage hardware and enables centralized management, allowing for dynamic allocation. This means that organizations can allocate resources as needed without the inefficiencies of older methods.
For example, a mid-sized retail company struggles with slow data access during peak shopping seasons. Their traditional on-premises storage system cannot handle the influx of data traffic, leading to frequent delays.
By adopting a hybrid cloud model, the company improved response times by 40%, as it could scale storage resources during high-demand periods.
Besides, it reduced overall storage costs by 25% since it only paid for cloud resources when needed while keeping sensitive customer data securely on-premises.
Real-time data from wearable devices is gradually increasing. Healthcare organizations must turn to edge computing to process data closer to its source.
This reduces latency, improves decision-making, and ensures critical patient data is handled quickly and efficiently, even during emergencies.
Financial institutions often deal with fluctuating demand, especially during stock market volatility.
Hybrid cloud storage allows them to scale resources up or down as needed. It helps them to manage sudden increases in data traffic while keeping sensitive financial information secure on private servers.
So, by using hybrid storage and cloud-native infrastructure, any company can drastically improve performance while keeping costs low. This makes them perfect for businesses that have to deal with unpredictable data volumes.
# Sustainability — Growing Concern
With the increase in data, power consumption has become a growing concern. Storage systems use a lot of energy, making data centers very energy-intensive. That’s why energy efficiency is no longer an afterthought-it is a key priority today.
Innovations like serverless storage and carbon-neutral cloud services lead to the charge of reducing energy consumption.
Companies are now actively seeking greener options for their storage needs, not only to cut costs but also to meet sustainability goals.
The push for green computing has also sparked the development of data centers powered by renewable energy and energy-efficient technologies like liquid cooling systems for servers.
Companies can also optimize energy usage in real-time by adjusting workloads to minimize energy consumption during periods of lower demand with the help of AI-driven automation. These approaches ensure that storage solutions remain sustainable while also providing performance expectations.
As sustainability becomes a growing concern, new technologies like AI and
edge computing
are also helping to optimize storage efficiency.
Deploying Juniper Data Centers with EVPN VXLAN: A guide for IT professionals and network engineers, by Aninda Chatterjee
# The Future of Big Data Storage
AI, edge computing, and hybrid storage solutions will be the keys to the future of big data.
Many organizations already use hybrid storage, which stores data locally and in the cloud. The next frontier is AI, which can automatically find the best way to store data based on usage patterns.
We are also shifting towards edge computing, where data processing happens closer to the source of data generation (e.g., IoT devices), reducing latency and improving real-time decision-making.
This is especially critical for industries like autonomous vehicles, healthcare, and smart cities, where milliseconds matter.
# Common Mistakes to Avoid
There are some mistakes that you should not make when implementing cutting-edge storage solutions:
Over-provisioning:
Allocating too much cloud storage can lead to unnecessary expenses. It’s important to assess data needs and adjust provisioning accordingly regularly.
Underestimating Security Needs:
You cannot compromise security. Ensure that encryption, monitoring, and robust access control are part of your storage strategy from the beginning.
Neglecting Backup Strategies:
A lack of efficient backup systems can lead to substantial downtime costs. Ensure you’re implementing technologies like automated snapshots, which allow quick data restoration during a failure.
# Strategy for Implementing New Storage Solutions
When adopting new storage systems, you should consider the following things:
You should focus on whether your current storage setup meets your needs. If not, focus on the specific gaps (e.g., security, scalability, and cost).
If an organization is just starting with cloud storage, beginning small and integrating a hybrid solution can reduce risks while assessing performance.
Separate mission-critical data from less sensitive information to ensure you get the most out of your storage infrastructure without overpaying.
# Trends to Watch
While AI is growing, it can also provide valuable insights, such as optimizing data storage and access, predicting usage patterns, and reducing costs.
In the coming decades,
quantum storage
, which is still in its early stages, will revolutionize
data management
by offering storage systems with vastly increased capacity and speed.
More companies are moving toward multi-cloud approaches, using different cloud providers for their storage needs. This strategy offers greater flexibility and security, ensuring that companies aren’t reliant on a single provider.
Besides, these innovations, like quantum computing and
blockchain-based storage
, could completely redefine how we think about data management, offering nearly limitless scalability and unparalleled security.
The next era of storage will be as transformative as the one we live through today.
# Key Takeaways for Today’s Storage Solutions:
Adopt a hybrid storage model for flexibility and cost-efficiency.
Prioritize security, especially with the rise of cyber threats and stricter data privacy laws.
Regularly evaluate your storage needs to avoid over-provisioning or underutilizing resources.
Keep an eye on emerging technologies like AI and edge computing for more competent storage management.
🚀 Before You Go:
👏 Found these cloud storage tips helpful? Give it a like!
💬 Have your own tips? Share your insights!
🔄 Know someone who needs this? Share the post!
🌟 Your support keeps us going!
📩
Stay updated with the latest tech trends, tutorials, and tips
straight to your inbox!
👉
Subscribe
Note: Some links on this page might be affiliate links. If you make a purchase through these links, I may earn a small commission at no extra cost to you. Thanks for your support!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Developing cloud agnostic application
Rahul Kashyap -
Oct 10
Google’s Arcade Program - A Gateway to Creative Coding, Dev-ops, Sec Ops, and More.
Ogunkola Adeola  -
Oct 9
Best Practices for Monitoring Your Amazon VPC
Esteban -
Oct 8
How to use Cloud flare R2 storage with workers API & S3 compatibility
Sanya_Lazy -
Sep 17
Mainul Hasan
Follow
👨‍💻 Full Stack Developer 🛠️ Tech Enthusiast | 🎓 TA & MS Student in Informatics @ University of Oslo | 🌐 Sharing #webdev, #tech insights
Location
Oslo, Norway
Education
MSc in Informatics, University of Oslo
Work
TA & MS Student in Informatics @ University of Oslo
Joined
Jun 27, 2023
Trending on
DEV Community
Hot
8 Type of Load Balancing
#
discuss
#
webdev
#
cloudcomputing
#
programming
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Lambda Architecture: Revolutionizing Data Processing for Big Data - DEV Community,"Lambda Architecture: Revolutionizing Data Processing for Big Data - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Pragya Sapkota
Posted on
Sep 26, 2023
• Originally published at
pragyasapkota.Medium
Lambda Architecture: Revolutionizing Data Processing for Big Data
#
systemdesign
#
lambda
#
architecture
#
webdev
System Design Concepts (68 Part Series)
1
HTTPS: Is it better than HTTP?
2
Network Protocols
...
64 more parts...
3
Storage: The Underrated Topic
4
Latency and Throughput
5
Logging, Monitoring, and Alerting
6
Distributed system: The definition
7
Relational Database
8
Non-relational Database
9
Caching
10
Proxies
11
Scaling
12
Load Balancing: Why do we need them?
13
Hashing
14
Event-Driven Architecture
15
System Availability
16
Data Indexing, Replication, and Sharding: Basic Concepts
17
Messaging and Pub/Sub
18
Leader Election
19
Polling and Streaming
20
Endpoint Protection
21
Long Polling: A easily implemented concept
22
CAP Theorem: Consistency, Availability, & Partition Tolerance
23
PACELC Theorem: an ELC extension of CAP
24
CQRS
25
A brief introduction to Message Queue
26
Enterprise Service Bus (ESB)
27
SLA, SLO, and SLI: How important are they?
28
Heartbeat Messaging: What is it?
29
A brief introduction to Normalization
30
Denormalization
31
Event Sourcing
32
Circuit Breaker: A Basic Concept
33
IP Address: A concept we must know
34
IPv4 Vs. IPv6
35
OSI Model
36
Content Delivery Network (CDN)
37
Domain Name System (DNS)
38
Clustering: How much does it differ from Load Balancing?
39
Quadtrees
40
Geohashing: Encoding the location coordinates
41
Disaster Recovery
42
Monoliths
43
Microservices
44
Database Indexes
45
Transmission Control Protocol (TCP) and User Datagram Protocol (UDP)
46
Database Federation
47
Database Replication
48
Service Discovery
49
N-Tier Architecture
50
Message Brokers
51
Transactions
52
Distributed Transactions
53
Rate Limiting
54
Rate Limiting in Distributed Systems
55
Open Authorization (OAuth 2.0)
56
OpenID Connect
57
Single Sign On (SSO)
58
Security Assertion Markup Language (SAML) Vs. OAuth 2.0 and OpenID Connect (OIDC)
59
Redundancy
60
Redundancy and Reliability
61
Redundancy and Availability
62
Resilience
63
Virtual Machines and Containers
64
SSL, TLS, and mTLS
65
Consensus Algorithms: Paxos and Raft
66
Lambda Architecture: Revolutionizing Data Processing for Big Data
67
Canary Release: For Deployment Success
68
A Comprehensive Guide to Multi-Tenancy Architecture
We are living in a digital era where organizations deal with massive amounts of data generated from various sources. These data hold a huge potential for meaningful insights if processed correctly. It needs scalable and efficient data processing systems to harness this power and Lambda Architecture is one such approach with high prominence to handle large data volumes. It ensures fault tolerance and real-time processing capabilities that have benefited many organizations over the last few years.
What is Lambda Architecture?
Lambda Architecture is a data processing architecture that can handle large amounts of data in a fault-tolerant and scalable way. The architecture gets its name from the Greek letter lambda (λ), which signifies a function that transforms input data into output data.
Traditional databases and processing systems often struggle with the sheer volume, velocity, and variety of data generated in today’s digital landscape. They were originally designed for batch processing which makes them ill-equipped to provide real-time or near-real-time insights.
The concept of Lambda Architecture was first introduced in the book “Big Data: Principles and Best Practices of Scalable Realtime Data Systems” by Nathan Marz. It addressed the above challenges and provided a robust framework that can process large-scale data, making it suitable for applications ranging from social media analytics and e-commerce recommendation systems to fraud detection and IoT sensor data processing. It combines both batch processing and stream processing methods to make the data processing reliable and fast.
There are three main layers in Lambda Architecture:
Batch Layer
Serving Layer
Speed Layer
Let’s discuss these layers briefly:
1. Batch Layer
The first layer batch handles the process of storing and processing historical data in a batch-oriented fashion. With technologies like Hadoop MapReduce and Apache Stark, the batch layer performs large-scale batch processing jobs to generate batch views. Next, the batch views are precomputed and stored in a distributed file system or a NoSQL database.
Batch Layer can handle massive amounts of historical data efficiently without disturbing data consistency and fault tolerance. However, it is not suitable for real-time data processing.
2. Serving Layer
The serving layer is responsible for serving query results to users and applications in real-time. It uses the previously generated batch views from batch layers and uses databases like Apache HBase and Cassandra for quick data retrieval. The layer also provides low-latency access to query results.
3. Speed Layer
Lastly, we have the speed layer to address the need for real-time processing and incoming data. With technologies like Apache Kafka or Apache Flink, it ingests and processes streamlining data in near-real-time. The result is usually combined with those from the serving layer to provide up-to-date query results.
Why choose Lambda Architecture?
There are some reasons why you should choose lambda architecture for big data processing:
1. Scalability
Lambda Architecture is highly scalable and lets organizations handle growing volumes of data with additional computational resources.
2. Fault Tolerance
The architecture also brings redundancy and fault tolerance mechanisms that maintain data integrity and system reliability even when there are hardware failures or other issues.
3. Real-Time Processing
With the combination of batch and real-time processing, organizations can derive insights from historical and real-time data simultaneously.
4. Flexibility
Being a technology-agnostic architecture, it lets you choose the best-suited tools and technologies for specific use cases within each layer of the architecture.
5. Consistency
Batch views give a consistent and reliable source of truth for queries.
Challenges in Lambda Architecture
Though there are numerous benefits, lambda architecture also brings some challenges and considerations:
1. Complexity
Lambda Architecture can be complex to implement since it requires expertise in multiple tools and technologies.
2. Maintenance Overhead
With multiple layers, it might be hard to manage them and ensure synchronization. This can be resource-intensive which is a challenge for many.
3. Latency
Though the speed layer provides near-real-time processing, you can still experience some latency while delivering results compared to fully stream-based systems.
4. Data Consistency
It might be challenging to maintain data consistency between batch and real-time views.
Conclusion
Lambda Architecture is one of the powerful approaches to big data processing with the ability to handle large volumes of data while offering real-time processing capabilities and fault tolerance. If the architecture is implemented correctly, it has the potential to empower organizations to unlock valuable insights from their data and make data-driven decisions. However, it needs some careful consideration according to the needs of your organization. This helps later when you implement and maintain the architecture before adopting it for big data processing tasks.
I hope this article was helpful to you.
Please don’t forget to follow me!!!
Any kind of feedback or comment is welcome!!!
Thank you for your time and support!!!!
Keep Reading!! Keep Learning!!!
System Design Concepts (68 Part Series)
1
HTTPS: Is it better than HTTP?
2
Network Protocols
...
64 more parts...
3
Storage: The Underrated Topic
4
Latency and Throughput
5
Logging, Monitoring, and Alerting
6
Distributed system: The definition
7
Relational Database
8
Non-relational Database
9
Caching
10
Proxies
11
Scaling
12
Load Balancing: Why do we need them?
13
Hashing
14
Event-Driven Architecture
15
System Availability
16
Data Indexing, Replication, and Sharding: Basic Concepts
17
Messaging and Pub/Sub
18
Leader Election
19
Polling and Streaming
20
Endpoint Protection
21
Long Polling: A easily implemented concept
22
CAP Theorem: Consistency, Availability, & Partition Tolerance
23
PACELC Theorem: an ELC extension of CAP
24
CQRS
25
A brief introduction to Message Queue
26
Enterprise Service Bus (ESB)
27
SLA, SLO, and SLI: How important are they?
28
Heartbeat Messaging: What is it?
29
A brief introduction to Normalization
30
Denormalization
31
Event Sourcing
32
Circuit Breaker: A Basic Concept
33
IP Address: A concept we must know
34
IPv4 Vs. IPv6
35
OSI Model
36
Content Delivery Network (CDN)
37
Domain Name System (DNS)
38
Clustering: How much does it differ from Load Balancing?
39
Quadtrees
40
Geohashing: Encoding the location coordinates
41
Disaster Recovery
42
Monoliths
43
Microservices
44
Database Indexes
45
Transmission Control Protocol (TCP) and User Datagram Protocol (UDP)
46
Database Federation
47
Database Replication
48
Service Discovery
49
N-Tier Architecture
50
Message Brokers
51
Transactions
52
Distributed Transactions
53
Rate Limiting
54
Rate Limiting in Distributed Systems
55
Open Authorization (OAuth 2.0)
56
OpenID Connect
57
Single Sign On (SSO)
58
Security Assertion Markup Language (SAML) Vs. OAuth 2.0 and OpenID Connect (OIDC)
59
Redundancy
60
Redundancy and Reliability
61
Redundancy and Availability
62
Resilience
63
Virtual Machines and Containers
64
SSL, TLS, and mTLS
65
Consensus Algorithms: Paxos and Raft
66
Lambda Architecture: Revolutionizing Data Processing for Big Data
67
Canary Release: For Deployment Success
68
A Comprehensive Guide to Multi-Tenancy Architecture
Oldest comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Hackathon 101
Aditya Arpan Sahoo -
Nov 30
Build a Simple Real-Time SBOBET88-Style Website for Beginners with PHP, CSS, and JavaScript
Sbobet88 Dev -
Nov 30
Why You Should Avoid Using `try...catch` in SvelteKit Actions
Michael Amachree -
Nov 30
15 WordPress Search Plugins to Supercharge Your Website’s Search Functionality
Abhishek Deshpande -
Nov 30
Pragya Sapkota
Follow
Hope and Patience | Wisdom and Light
Location
Kathmandu, Nepal
Education
Amrit Science Campus
Work
I am a technical content creator.
Joined
Aug 25, 2021
More from
Pragya Sapkota
A Comprehensive Guide to Multi-Tenancy Architecture
#
multitenancy
#
system
#
designsystem
#
webdev
Chaos Engineering: Embracing Uncertainty
#
chaosengineering
#
webdev
Canary Release: For Deployment Success
#
systems
#
systemdesign
#
webdev
#
softwaredevelopment
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Fortify Your Website for Free: Testing the Power of SafeLine - DEV Community,"Fortify Your Website for Free: Testing the Power of SafeLine - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Lulu
Posted on
Sep 2
Fortify Your Website for Free: Testing the Power of SafeLine
#
cybersecurity
#
devops
#
docker
#
opensource
I've always had a strong liking for Chaitin Technology, and today, I’m testing out their WAF product called “
SafeLine
.” I’ve been aware of SafeLine for a long time, but back then, it was a paid service without a community edition. Now, I'm excited to give the community edition a shot.
What is SafeLine?
SafeLine is a network attack detection system based on big data and machine learning technologies. By continuously monitoring and analyzing threat intelligence, attack data, and vulnerability information from around the globe, SafeLine quickly identifies and recognizes unknown security threats. It accurately determines the type and origin of attacks and promptly issues alerts. Additionally, SafeLine features a self-developed intelligent defense engine and a user-friendly visual management interface, providing efficient attack prevention and comprehensive security monitoring. This makes it a robust and reliable cloud security solution for users.
Key features include:
Pre-configured protection:
SafeLine comes with effective out-of-the-box protection, requiring no manual rule maintenance.
Accurate detection:
The detection rules are stringent, minimizing false positives.
Detection of unknown threats:
It can detect attacks based on unknown behaviors.
Deep encoding attack detection:
SafeLine can detect deeply encoded attacks.
Bypass attack detection:
It is also effective against attacks that attempt to bypass traditional signatures.
Installation and Usage
Download Links
Official Documentation:
SafeLine Docs
GitHub Repository:
SafeLine on GitHub
Online Demo:
SafeLine Demo
Download & Install
Upload the package to your server.
Extract the files and grant the necessary permissions.
Use Docker to build SafeLine. Make sure your Docker version is >= 20.10.14, and Docker Compose is >= 2.0.0.
To check your current Docker version, use the following command:
yum list installed |
grep
docker
If necessary, uninstall older Docker packages:
yum
-y
remove docker
*
Install Docker and start it:
yum
install
-y
yum-utils
 yum-config-manager
--add-repo
https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
 yum
-y
install
docker-ce
 systemctl start docker
Access & Configure
After starting the container, you can access SafeLine through port 9443. Simply log in to get started.
Since my blog and WAF are on the same host, I configured it to use
127.0.0.1
.
My blog is accessible through port 8080 (you can also use port 80 if preferred).
Testing the WAF
I used AWVS (Acunetix Web Vulnerability Scanner) to test SafeLine's performance.
During the testing, the CPU load remained around 30%, which is quite impressive for small to medium-sized websites.
The WAF handled a significant amount of attack traffic effectively, and as per the official claims, it resulted in nearly zero false positives.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Flexible Angular Builds: A Guide to Angular 19 Build-Time Variables with Docker
Daniel Sogl -
Dec 10
350+ Tools to Make Game Development Easy
Abubaker Siddique -
Dec 10
From Zero to Observability: Your first steps sending OpenTelemetry data to an Observability backend
Jade -
Dec 10
How to use generics in pipe-and-combine
Wolfgang Rathgeb -
Dec 5
Lulu
Follow
📩 Feel free to reach out!
Joined
Jul 4, 2024
More from
Lulu
Do You Have a Website? Are You Worried About Hackers Attacking It?
#
cybersecurity
#
webdev
#
opensource
#
waf
Why You Need This Decade-Old Open-Source WAF for Ultimate Web Protection
#
opensource
#
webdev
#
docker
#
cybersecurity
Protect Your Site from Hackers with SafeLine: A Free and Open-Source WAF
#
opensource
#
cybersecurity
#
docker
#
devops
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Dataform - DEV Community,"Dataform - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Follow
Organization actions
Dataform
Dataform makes it easy to develop and deploy SQL-based operations in your cloud data warehouse. Publish tables, write data tests and automate complex SQL workflows in a few minutes.
Location
London, UK
Joined
Joined on
Jun 26, 2019
Twitter logo
GitHub logo
External link icon
Support email
team@dataform.co
Employees
10
Meet the team
Post
8 posts published
Member
3 members
Cut data warehouse costs with run caching
BenBirt
BenBirt
BenBirt
Follow
Sep 24 '20
Cut data warehouse costs with run caching
#
elt
#
dataengineering
#
pipeline
#
etl
5
reactions
Comments
Add Comment
3 min read
CI/CD for ETL/ELT pipelines
BenBirt
BenBirt
BenBirt
Follow
Jun 8 '20
CI/CD for ETL/ELT pipelines
#
cicd
#
etl
#
elt
#
dataengineering
19
reactions
Comments
Add Comment
3 min read
Building an end to end Machine Learning Pipeline in Bigquery
Ahmad Faiyaz
Ahmad Faiyaz
Ahmad Faiyaz
Follow
Mar 13 '20
Building an end to end Machine Learning Pipeline in Bigquery
#
machinelearning
#
sql
7
reactions
Comments
Add Comment
5 min read
How we store protobufs in MongoDB
BenBirt
BenBirt
BenBirt
Follow
Jan 9 '20
How we store protobufs in MongoDB
#
mongodb
#
protobuf
#
database
5
reactions
Comments
Add Comment
3 min read
How we use MobX at Dataform to solve our frontend application state problems
Ahmad Faiyaz
Ahmad Faiyaz
Ahmad Faiyaz
Follow
Oct 16 '19
How we use MobX at Dataform to solve our frontend application state problems
#
mobx
#
react
#
mobxreact
6
reactions
Comments
Add Comment
5 min read
How to write unit tests for your SQL queries
BenBirt
BenBirt
BenBirt
Follow
Jul 15 '19
How to write unit tests for your SQL queries
#
sql
#
testing
13
reactions
Comments
Add Comment
4 min read
Consider SQL when writing your next processing pipeline
BenBirt
BenBirt
BenBirt
Follow
Jun 27 '19
Consider SQL when writing your next processing pipeline
#
sql
43
reactions
Comments
10
comments
6 min read
Testing data quality with SQL assertions
Lewis Hemens
Lewis Hemens
Lewis Hemens
Follow
Jun 26 '19
Testing data quality with SQL assertions
#
sql
9
reactions
Comments
Add Comment
4 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
AI-Powered Software Testing: Unlocking Benefits for Large-Scale Projects - DEV Community,"AI-Powered Software Testing: Unlocking Benefits for Large-Scale Projects - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Aditya Pratap Bhuyan
Posted on
Oct 1
AI-Powered Software Testing: Unlocking Benefits for Large-Scale Projects
#
ai
#
testing
In the rapidly evolving landscape of software development, ensuring the quality and reliability of applications is paramount, especially in large-scale projects. Traditional software testing methods, while effective, often struggle to keep pace with the complexity and volume of modern software systems. Enter Artificial Intelligence (AI) – a transformative technology that is revolutionizing software testing by enhancing efficiency, accuracy, and scalability. This article explores the key benefits of using AI for software testing in large-scale projects, delving into how AI-driven approaches are reshaping the testing landscape and enabling organizations to deliver high-quality software faster and more cost-effectively.
Introduction
As businesses increasingly rely on complex software systems to drive operations, the demand for robust and efficient software testing processes has surged. Large-scale projects, characterized by their extensive codebases, numerous integrations, and high user expectations, pose significant challenges for traditional testing methodologies. Manual testing, while thorough, is time-consuming and prone to human error, making it difficult to maintain the speed and agility required in today's fast-paced development environments.
AI-powered software testing offers a solution to these challenges by automating and optimizing various aspects of the testing process. By leveraging machine learning, natural language processing, and data analytics, AI can enhance the effectiveness and efficiency of software testing, enabling teams to identify defects earlier, reduce testing time, and improve overall software quality. This article delves into the key benefits of integrating AI into software testing for large-scale projects, highlighting how AI-driven approaches can transform testing practices and deliver substantial value to organizations.
The Importance of Software Testing in Large-Scale Projects
Software testing is a critical component of the software development lifecycle, ensuring that applications meet specified requirements, function correctly, and provide a seamless user experience. In large-scale projects, the stakes are higher due to the complexity and scale of the software involved. These projects often involve multiple teams, extensive codebases, numerous integrations with third-party systems, and a wide range of user scenarios. Consequently, effective software testing is essential to identify and rectify defects, ensure compliance with standards, and deliver reliable and high-performing applications.
However, traditional testing approaches face significant limitations in large-scale projects. Manual testing is labor-intensive, time-consuming, and often fails to keep up with the rapid pace of development and deployment. Additionally, the sheer volume of test cases and data involved in large-scale projects can overwhelm testers, leading to incomplete coverage and undetected defects. These challenges necessitate the adoption of more advanced and scalable testing methodologies, with AI emerging as a powerful tool to address these issues.
Overview of AI in Software Testing
Artificial Intelligence (AI) encompasses a range of technologies that enable machines to perform tasks that typically require human intelligence. In the context of software testing, AI leverages machine learning algorithms, data analytics, and automation to enhance various testing activities. AI-driven testing tools can analyze vast amounts of data, identify patterns, predict potential defects, and automate repetitive tasks, thereby improving the efficiency and effectiveness of the testing process.
AI in software testing can be categorized into several key areas:
Test Automation
: Automating the execution of test cases, reducing the need for manual intervention, and speeding up the testing process.
Intelligent Test Case Generation
: Using AI to generate optimal test cases based on application usage patterns, code changes, and user behavior.
Defect Prediction and Analysis
: Identifying potential defects and vulnerabilities by analyzing code quality, historical data, and usage patterns.
Visual Testing
: Utilizing computer vision techniques to compare visual aspects of the application across different environments and detect inconsistencies.
Natural Language Processing (NLP)
: Analyzing requirements and test cases written in natural language to ensure alignment and completeness.
By integrating AI into software testing, organizations can overcome the limitations of traditional methods, enhance test coverage, and accelerate the delivery of high-quality software.
Key Benefits of Using AI for Software Testing in Large-Scale Projects
1. Enhanced Efficiency and Speed
One of the most significant benefits of AI in software testing is the dramatic increase in efficiency and speed. AI-powered tools can automate repetitive and time-consuming tasks such as test execution, result analysis, and reporting. This automation not only accelerates the testing process but also allows testers to focus on more complex and strategic activities.
Key Aspects:
Automated Test Execution
: AI can schedule and execute test cases automatically, reducing the time required for manual testing.
Parallel Testing
: AI-enabled tools can run multiple tests simultaneously across different environments, significantly reducing testing time.
Continuous Testing
: AI facilitates continuous integration and continuous deployment (CI/CD) by enabling real-time testing and feedback during the development process.
By streamlining the testing workflow, AI ensures that large-scale projects meet tight deadlines without compromising on quality.
2. Improved Accuracy and Precision
Human error is an inherent limitation of manual testing. Testers may overlook certain scenarios or make mistakes in test execution, leading to undetected defects. AI mitigates these issues by providing precise and consistent testing processes.
Key Aspects:
Consistent Test Execution
: AI ensures that tests are executed consistently every time, eliminating variability and increasing reliability.
Accurate Defect Detection
: Machine learning algorithms can analyze patterns in data to identify subtle defects that might be missed by human testers.
Reduced False Positives/Negatives
: AI enhances the accuracy of defect detection, minimizing the occurrence of false positives and negatives.
Enhanced accuracy ensures that large-scale projects maintain high standards of quality and reliability.
3. Cost Reduction and Resource Optimization
Implementing AI in software testing can lead to significant cost savings by reducing the need for extensive manual testing and minimizing the resources required for defect management.
Key Aspects:
Lower Testing Costs
: Automation reduces the labor costs associated with manual testing, allowing organizations to allocate resources more effectively.
Efficient Resource Utilization
: AI optimizes the allocation of testing resources, ensuring that critical areas receive the necessary attention while less important aspects are automated.
Early Defect Detection
: Identifying defects early in the development process reduces the cost of fixing issues, as defects are less expensive to address before deployment.
By optimizing resource allocation and reducing operational costs, AI contributes to the overall financial efficiency of large-scale projects.
4. Predictive Analytics and Risk Assessment
AI leverages predictive analytics to forecast potential defects and assess risks associated with software releases. By analyzing historical data, usage patterns, and code quality metrics, AI can provide valuable insights into areas that may require additional testing or attention.
Key Aspects:
Defect Prediction
: Machine learning models can predict the likelihood of defects in specific modules or components based on past data and trends.
Risk Assessment
: AI evaluates the potential impact and likelihood of risks, enabling proactive measures to mitigate them.
Prioritization of Testing Efforts
: AI helps prioritize testing activities based on predicted risks and defect probabilities, ensuring that critical areas are thoroughly tested.
Predictive analytics enhances the decision-making process, allowing organizations to focus their efforts on areas with the highest risk of defects.
5. Continuous Testing and Integration
In large-scale projects, the development process is often iterative, with frequent code changes and updates. AI facilitates continuous testing and integration by providing real-time feedback and ensuring that new changes do not introduce defects.
Key Aspects:
Real-Time Testing
: AI enables tests to be executed automatically with every code change, ensuring that issues are identified and addressed promptly.
Seamless Integration with CI/CD Pipelines
: AI-powered testing tools integrate seamlessly with CI/CD pipelines, supporting continuous delivery and deployment.
Automated Regression Testing
: AI automates regression testing, ensuring that new changes do not adversely affect existing functionalities.
Continuous testing ensures that large-scale projects maintain stability and quality throughout the development lifecycle.
6. Intelligent Test Case Generation
Creating comprehensive test cases is a time-consuming task, especially for large-scale projects with extensive functionalities. AI can intelligently generate test cases by analyzing application behavior, user interactions, and historical data.
Key Aspects:
Automated Test Design
: AI algorithms can design optimal test cases based on usage patterns and code changes, ensuring comprehensive coverage.
Adaptive Test Cases
: AI-generated test cases adapt to changes in the application, maintaining relevance and effectiveness as the software evolves.
Minimized Redundancy
: AI identifies and eliminates redundant test cases, optimizing the testing process and reducing unnecessary effort.
Intelligent test case generation enhances test coverage and ensures that all critical scenarios are adequately tested.
7. Better Test Coverage
Achieving comprehensive test coverage is essential for identifying and addressing defects across all aspects of a large-scale project. AI enhances test coverage by analyzing the entire application and identifying areas that require testing.
Key Aspects:
Comprehensive Analysis
: AI examines all components, integrations, and user scenarios to ensure that no critical areas are overlooked.
Dynamic Coverage
: AI continuously monitors the application and adjusts test coverage based on changes and new features.
Data-Driven Coverage
: AI utilizes data analytics to identify high-risk areas and allocate testing resources accordingly.
Better test coverage reduces the likelihood of undetected defects and enhances the overall quality of the software.
8. Enhanced Maintenance and Adaptability
Large-scale projects often undergo frequent updates and modifications. AI facilitates the maintenance of testing processes by adapting to changes and minimizing the effort required to update test cases.
Key Aspects:
Self-Healing Scripts
: AI-powered testing tools can automatically update test scripts in response to changes in the application, reducing maintenance efforts.
Adaptive Learning
: Machine learning algorithms enable testing tools to learn from past experiences and improve their adaptability to new scenarios.
Reduced Manual Intervention
: AI minimizes the need for manual updates to test cases, allowing testers to focus on more strategic tasks.
Enhanced maintenance and adaptability ensure that testing processes remain effective and efficient as the software evolves.
9. Reduced Human Error
Human error is a significant factor in manual testing, leading to overlooked defects and inconsistent test execution. AI mitigates these risks by providing automated and consistent testing processes.
Key Aspects:
Consistent Execution
: AI ensures that tests are executed in the same manner every time, eliminating variability and reducing errors.
Accurate Defect Identification
: AI algorithms can detect subtle defects that may be missed by human testers, enhancing defect identification accuracy.
Automated Reporting
: AI generates precise and comprehensive reports, minimizing the chances of errors in documentation and analysis.
By reducing human error, AI enhances the reliability and effectiveness of software testing in large-scale projects.
10. Scalability and Flexibility
Large-scale projects require testing solutions that can scale to handle increasing data volumes, user interactions, and application complexities. AI-powered testing tools offer the scalability and flexibility needed to accommodate these demands.
Key Aspects:
Scalable Infrastructure
: AI-enabled testing tools can scale horizontally and vertically to manage larger test suites and higher data volumes.
Flexible Integration
: AI integrates seamlessly with various development and testing tools, supporting diverse environments and workflows.
Customizable Testing Strategies
: AI allows organizations to tailor their testing strategies based on specific project requirements and objectives.
Scalability and flexibility ensure that testing processes can adapt to the growing and changing needs of large-scale projects.
AI-Powered Testing Tools
The effectiveness of AI in software testing is largely dependent on the tools and technologies used. Several AI-powered testing tools have emerged, offering a range of capabilities to enhance software testing processes.
1.
Testim
Testim leverages machine learning to create, execute, and maintain automated tests. It offers features such as self-healing locators, which automatically update test scripts in response to changes in the application, and intelligent test execution, which prioritizes tests based on risk and impact.
2.
Applitools
Applitools specializes in visual AI testing, utilizing computer vision algorithms to detect visual anomalies across different devices and browsers. It ensures that the application's user interface remains consistent and visually appealing, enhancing the overall user experience.
3.
Mabl
Mabl combines machine learning with automated testing to provide end-to-end testing solutions. It offers intelligent test creation, automated maintenance, and actionable insights, enabling teams to identify and address defects quickly and efficiently.
4.
Functionize
Functionize uses natural language processing and machine learning to create and execute tests based on plain language descriptions. Its AI-driven approach simplifies test creation and maintenance, making it accessible to both technical and non-technical users.
5.
Katalon Studio
Katalon Studio integrates AI-driven features such as smart element identification and automatic test generation. It supports a wide range of testing types, including functional, API, and mobile testing, providing a comprehensive solution for large-scale projects.
These tools exemplify how AI can enhance various aspects of software testing, from test creation and execution to defect detection and reporting.
Challenges and Considerations
While AI offers numerous benefits for software testing in large-scale projects, there are also challenges and considerations that organizations must address to ensure successful implementation.
1.
Data Quality and Availability
AI algorithms rely heavily on data to learn and make accurate predictions. Ensuring the availability of high-quality, relevant data is essential for the effectiveness of AI-powered testing tools. Poor data quality can lead to inaccurate results and reduced reliability.
Considerations:
Data Collection
: Implement robust data collection mechanisms to gather comprehensive and accurate testing data.
Data Preprocessing
: Clean and preprocess data to eliminate noise and inconsistencies.
Data Privacy
: Ensure that data handling practices comply with privacy regulations and protect sensitive information.
2.
Integration with Existing Systems
Integrating AI-powered testing tools with existing development and testing environments can be complex. Ensuring seamless integration is crucial for maximizing the benefits of AI and maintaining workflow continuity.
Considerations:
Compatibility
: Choose AI tools that are compatible with existing systems and technologies.
API Availability
: Utilize tools with robust APIs to facilitate integration and automation.
Scalability
: Ensure that AI tools can scale with the project's growth and evolving requirements.
3.
Skill Gap and Training
Implementing AI in software testing requires specialized skills and knowledge. Organizations may need to invest in training their teams or hiring new talent to effectively leverage AI technologies.
Considerations:
Training Programs
: Develop training programs to upskill existing team members in AI and machine learning.
Collaboration
: Foster collaboration between testers, developers, and data scientists to facilitate knowledge sharing.
Continuous Learning
: Encourage continuous learning to keep up with the latest advancements in AI and software testing.
4.
Cost of Implementation
Adopting AI-powered testing tools can involve significant upfront costs, including tool licenses, infrastructure investments, and training expenses. Organizations must carefully evaluate the return on investment (ROI) to justify these costs.
Considerations:
Cost-Benefit Analysis
: Conduct a thorough cost-benefit analysis to assess the financial implications and potential savings.
Phased Implementation
: Consider a phased approach to implementation, starting with pilot projects to demonstrate value before scaling.
Vendor Support
: Choose vendors that offer comprehensive support and flexible pricing models to manage costs effectively.
5.
Ethical and Security Concerns
AI systems can introduce ethical and security challenges, particularly regarding data privacy, algorithmic bias, and decision transparency. Addressing these concerns is essential to ensure responsible and secure use of AI in software testing.
Considerations:
Data Security
: Implement robust security measures to protect testing data from breaches and unauthorized access.
Bias Mitigation
: Ensure that AI algorithms are trained on diverse and unbiased data to prevent discriminatory outcomes.
Transparency
: Maintain transparency in AI decision-making processes to build trust and accountability.
By addressing these challenges, organizations can effectively harness the benefits of AI in software testing while minimizing potential risks.
Future of AI in Software Testing
The integration of AI into software testing is still in its early stages, and its future holds immense potential for further advancements and innovations. As AI technologies continue to evolve, their impact on software testing is expected to grow, offering even more sophisticated and intelligent testing solutions.
1.
Advanced Machine Learning Models
Future AI-powered testing tools will leverage more advanced machine learning models, capable of understanding complex patterns and relationships within data. These models will enhance defect prediction, test case generation, and risk assessment, providing deeper insights and more accurate results.
2.
Enhanced Natural Language Processing
Improved natural language processing (NLP) capabilities will enable AI tools to better understand and interpret requirements, user stories, and test cases written in natural language. This advancement will facilitate more effective communication between stakeholders and streamline the testing process.
3.
Integration with DevOps and Agile Practices
AI will play a pivotal role in integrating testing with DevOps and Agile practices, supporting continuous testing, continuous integration, and continuous deployment (CI/CD). AI-driven automation and analytics will enhance collaboration, accelerate development cycles, and ensure consistent quality across rapid iterations.
4.
Autonomous Testing Agents
The development of autonomous testing agents, capable of independently managing and executing testing tasks, will further enhance efficiency and reduce the need for human intervention. These agents will adapt to changing environments, learn from past experiences, and continuously improve their testing strategies.
5.
Personalized Testing Experiences
AI will enable personalized testing experiences by tailoring test cases and strategies based on individual user behaviors and preferences. This personalization will enhance the relevance and effectiveness of testing, ensuring that applications meet diverse user needs and expectations.
6.
Enhanced Collaboration and Communication
AI-driven tools will facilitate better collaboration and communication among development, testing, and operations teams. By providing real-time insights, automated reporting, and intelligent recommendations, AI will enhance transparency and foster a more collaborative testing environment.
7.
Greater Emphasis on Security Testing
With the increasing focus on cybersecurity, AI will play a crucial role in enhancing security testing. AI-powered tools will identify vulnerabilities, detect malicious activities, and ensure compliance with security standards, safeguarding applications against potential threats.
8.
Integration with IoT and Edge Computing
As the Internet of Things (IoT) and edge computing continue to expand, AI will integrate with these technologies to support testing of distributed and decentralized systems. AI-driven testing solutions will address the unique challenges posed by IoT and edge environments, ensuring reliability and performance across diverse platforms.
The future of AI in software testing is poised to bring transformative changes, driving innovation and excellence in large-scale projects.
Conclusion
AI is revolutionizing software testing, offering a myriad of benefits that are particularly valuable for large-scale projects. By enhancing efficiency, improving accuracy, reducing costs, and enabling predictive analytics, AI-powered testing tools empower organizations to deliver high-quality software at unprecedented speeds. The ability to automate repetitive tasks, intelligently generate test cases, and ensure comprehensive test coverage addresses the inherent challenges of large-scale projects, ensuring that applications are robust, reliable, and user-centric.
However, successful integration of AI into software testing requires careful consideration of data quality, system integration, skill development, and ethical concerns. By addressing these challenges and leveraging the strengths of AI, organizations can unlock significant value and maintain a competitive edge in the dynamic software development landscape.
As AI technologies continue to advance, their role in software testing will become increasingly integral, driving further innovations and elevating the standards of software quality. Embracing AI in software testing is not just a trend but a strategic imperative for organizations aiming to excel in the digital age.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Say Goodbye to tedious Code Reviews
Pratik Singh -
Nov 30
From Idea to Business Plan in Less Than an Hour with APEX
Justin L Beall -
Nov 2
Example Training Images Dataset, Trained Models, Grids and Full Training Configs, json files and more
Furkan Gözükara -
Nov 2
LLM APIs vs. Self-Hosted Models: Finding the Best Fit for Your Business Needs
Victor Isaac Oshimua -
Dec 6
Aditya Pratap Bhuyan
Follow
Aditya Pratap Bhuyan is an experienced IT professional with over 20 years in enterprise and cloud applications. With more than 40 industry certifications, he specializes in DevOps, cloud computing.
Location
Bangalore, India
Pronouns
He/Him
Work
Cloud Native Journey
Joined
Mar 24, 2024
More from
Aditya Pratap Bhuyan
Effective Strategies for Writing Unit Tests with External Dependencies like Databases and APIs
#
testing
#
database
#
unittest
Shift-Left Testing: The Key to Accelerating Quality in Agile Development
#
testing
#
agile
#
shiftlefttesting
The Essential Guide to Testing React Components
#
react
#
testing
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Test Automation Frameworks: Key to Effective and Efficient Software Testing - DEV Community,"Test Automation Frameworks: Key to Effective and Efficient Software Testing - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Bertha White
Posted on
Dec 19, 2023
Test Automation Frameworks: Key to Effective and Efficient Software Testing
#
testautomation
#
testautomationframeworks
#
softwaretesting
#
headspin
A
test automation framework
is a collection of rules or guidelines for generating and designing test cases. It includes a combination of practices and tools developed to assist the QA professionals in testing more effectively. Moreover, these guidelines may encompass coding standards, object repositories, test-data handling techniques, processes for securing test results, and data on accessing external resources.
These rules are mandatory, and testers must design or record tests without adhering to them. However, accessing an organized framework offers more benefits that professionals would miss out on otherwise. In this blog, we shed light on some of the prominent types of automated testing frameworks.
The Essence of Automated Testing Frameworks
Automated testing frameworks are designed to simplify the process of testing software applications. They provide a structured environment where test scripts can be executed and their outcomes evaluated without manual intervention. This approach reduces the effort involved in repetitive testing tasks, leading to increased efficiency and accuracy in the software development lifecycle.
Types of Test Automation Frameworks
Linear Scripting Framework
In a linear test automation framework, testers are not required to write code or generate functions. Moreover, the steps in it are written chronologically, and the process is also known as the record-and-playback framework. The tester records every step, like checkpoints, user input, navigation, etc., and then it is automatically played back to conduct the test. Considering there is no need for custom code, this framework can be executed without any expertise in test automation.
Advantages
Simplicity and Clarity: Linear scripting is straightforward, making it easier to understand and follow. It's particularly beneficial for beginners or when a clear, unambiguous progression is needed.
Predictability: Since the script follows a defined path, outcomes are predictable, which aids in planning and resource allocation. This predictability is essential in many programming and narrative contexts.
Ease of Debugging: A straightforward flow makes identifying and fixing issues simpler. There are fewer variables and branching paths to consider for effectively streamlining the debugging process.
Disadvantages
Lack of Flexibility: Linear scripting is often criticized for its rigidity. It doesn't easily accommodate changes or adapt to user inputs or scenarios.
Limited User Engagement: In games and interactive media, linear scripting can lead to a less engaging experience, as it offers limited options and interactivity for the user.
Predictability Can Be a Downside: While predictability is an advantage in some cases, it can also be a drawback. In narrative and game design, predictability can lead to boredom or a lack of challenge.
Modular Testing Framework
A modular testing framework requires testers to divide the application into separate testing categories like functions, units, sections, etc. Each of the classes is tested individually. Once the application is divided, testers create a test script for every part; they are then combined to create more extensive tests hierarchically. Additionally, the larger test sets work to represent different test applications. In the modular form of a testing framework, it is recommended to build an abstraction layer. This way, the sections won't influence the overarching module.
Advantages
Reusability: By dividing tests into independent, reusable modules, the framework facilitates reusability, which can significantly save time and resources.
Maintainability: Updates or fixes can be applied to individual modules without affecting the entire suite, enhancing maintainability.
Scalability: The framework can easily accommodate or expand new tests, making it highly scalable.
Disadvantages
Initial Setup Time: Setting up a modular testing framework can be time-consuming and requires careful planning.
Complexity in Integration: Integrating different modules can sometimes lead to complexities, especially if the modules are not designed with integration in mind.
Dependency Management: Managing dependencies between modules can be challenging and may require additional tools or strategies.
Library Architecture Testing Framework
The library architecture testing framework identifies similar tasks within the scripts and splits them into groups based on the function, rather than dividing the app under various tests. This allows the application to be grouped by the common objectives. Moreover, functions are stored in the library that they can access whenever needed.
Advantages:
Consistency and Standardization: Ensures all library components adhere to a defined architectural standard, leading to more consistent and predictable behavior.
Early Detection of Issues: Helps identify architectural issues early in the development process, reducing the time and cost of fixing them later.
Improved Code Quality: Encourages better coding practices and architectural decisions, leading to higher overall code quality.
Disadvantages:
Initial Setup Complexity: Setting up an architecture testing framework can be complex and time-consuming.
Possible Overhead: May introduce additional overhead in the development process, especially if it's too rigid or not well-integrated.
Learning Curve: There's a learning curve for developers to understand and effectively use the framework, making the process complex.
Data-Driven Framework
The data-driven framework breaks up script logic and test data and allows the testers to store data in the framework externally. At times, testers need to test a particular feature or function at different times with multiple data sets. In such situations, testers cannot rely just on the data coded into the script. This framework allows the testers to store and pass various parameters to test scripts from different data sources like text files, excel spreadsheets, ODBC repositories, SQL tables, etc.
Advantages:
Improved Decision-Making: Data-driven frameworks improve decision-making based on empirical evidence and analytics, which can lead to more accurate and objective outcomes.
Increased Efficiency: Automating data analysis help streamline processes, reducing time spent on manual data handling.
Enhanced Customer Insights: Data-driven approaches provide deep insights into customer behavior, aiding in better-targeted products and services.
Disadvantages:
Data Quality and Integrity Issues: A data-driven framework's effectiveness depends on how accurate and good the data is because poor data quality will bring misleading insights.
Privacy and Security Concerns: Handling large amounts of data, especially personal data, raises significant privacy and security concerns.
Complexity and Resource Intensity: Implementing a data-driven framework is complex and resource-intensive, requiring specialized skills and technology.
Hybrid Testing Framework
A hybrid test automation framework amalgamates different forms of testing automation frameworks to help leverage the potential of each while eliminating their weaknesses. Each application is different, and the process of testing these applications needs to be distinctive as well. And leveraging a hybrid framework allows testers to be more adaptable and agile to get better test results.
Advantages
Flexibility and Scalability: Hybrid frameworks are highly adaptable to changing requirements and can easily scale as the complexity of the software grows.
Reusability: They allow for the reuse of test scripts, reducing the effort required for test development.
Increased Test Coverage: Integrating different approaches ensures more comprehensive test coverage.
Disadvantages
Complexity: Integrating multiple frameworks can lead to increased complexity, making it more challenging to understand and manage.
Higher Initial Setup Cost and Time: Setting up a hybrid framework requires more time and resources than simpler frameworks.
Skilled Resources Required: Testers demand a higher level of expertise to use and maintain the framework efficiently.
Choosing the Right Framework: A Detailed Guide
Selecting the most suitable test automation framework is a pivotal decision in software development. This choice should be based on a thorough analysis of various factors, each of which plays a key role in understanding the strength of the testing process.
Project Requirements:
For project requirements, organizations can take into account the size, complexity, and type of application they are testing. For instance, a data-driven framework might be ideal for applications where input data varies significantly.
Team Expertise:
It’s vital that organizations assess the skill level of their testing team. Complex frameworks require a steep learning curve and more advanced programming skills. Opt for a framework that aligns with your team's abilities to ensure effective utilization.
Maintainability and Scalability:
Organizations can look for frameworks that are easy to maintain and scale as their application grows. Modular and hybrid frameworks are often preferred for large-scale projects due to their scalability and ease of maintenance.
Integration Capabilities:
Integration capabilities involve considering how well the framework integrates with other tools and technologies used in your project. This includes version control systems, continuous integration tools, and reporting tools.
Flexibility and Customization:
The ability to customize the framework to meet specific project needs can be a significant advantage. This includes custom reporting, integration with other tools, or adding specific functionalities.
The decision to choose a test automation framework should be made after careful consideration of these factors. It's about finding the right balance between the project's needs, team capabilities, and the overall goals of the software development process. This strategic approach ensures that the selected framework meets current requirements and is adaptable to future changes and advancements in the field.
Benefits of Automation Testing Frameworks
Automation testing frameworks offer many advantages, transforming the landscape of software testing. These benefits are pivotal in enhancing the efficiency of testing processes. Let's delve deeper into these benefits:
Increased Efficiency and Speed: Automation frameworks significantly expedite the testing process. By automating repetitive and time-consuming tasks, these frameworks allow tests to run faster and more frequently, which is especially beneficial for regression testing, where the same tests must be repeated for different software versions.
Enhanced Accuracy and Consistency: Human testers are prone to errors, especially when performing monotonous and repetitive tests. Automation eliminates this variability, ensuring that tests are performed consistently every time. This consistency is vital in detecting issues you can miss during manual testing.
Cost-Effectiveness in the Long Run: While the initial setup of automated testing frameworks may require investment, they are cost-effective over time. The ability to run tests repeatedly without additional costs leads to long-term savings, as it reduces the need for manual testing resources.
Scalability and Flexibility: Automation frameworks are highly scalable, allowing teams to easily add new test cases and adapt to software features or design changes. This scalability is crucial for accommodating the growth and evolution of software applications.
Improved Test Coverage: Automated testing can easily cover many test scenarios, including complex and tedious ones that might be challenging to execute manually. This comprehensive coverage ensures a thorough evaluation of the software's functionality and performance.
Adopting automation testing frameworks brings about transformative benefits, from enhancing efficiency and accuracy to ensuring high-quality software development. These frameworks are integral to modern software development strategies, enabling teams to meet their objectives more effectively and efficiently.
Challenges and Considerations in Implementing Automation Testing Frameworks
While automation testing frameworks offer many benefits, their implementation is not without challenges. Organizations must consider these potential pitfalls carefully when integrating these frameworks into their testing processes.
Selection of the Appropriate Framework: One of the primary challenges lies in choosing the right framework. The selection depends on various factors, such as the nature of the project, the testing team's skill set, and the software application's specific requirements. A mismatch in this selection can lead to inefficiencies in the testing process, increased costs, and delayed project timelines.
Initial Setup: Setting up an automation testing framework often requires a significant initial investment in time and resources. There's also a learning curve involved, especially for teams transitioning from manual to automated testing. Ensuring team members are adequately trained and equipped to handle the new system is crucial for its successful implementation.
Maintenance of Test Scripts: Over time, as the software evolves, maintaining the test scripts can become a challenging task. You need to regularly review and update test cases to align with the changes in the application. This ongoing maintenance requires dedicated effort and can be resource-intensive.
Integration with Other Tools and Systems: Automation testing frameworks must integrate various tools and systems, such as CI/CD pipelines, version control systems, and more. Ensuring seamless integration and compatibility can be a technical challenge.
While adopting an automation testing framework can enhance the testing process, organizations must navigate these challenges with strategic planning and continuous evaluation. This ensures that the benefits of automation are fully realized while minimizing potential drawbacks.
How HeadSpin Helps Adopt Test Automation
HeadSpin, a leader in mobile application testing, offers solutions that embody the principles of effective automated testing. Their platform supports various testing frameworks, ensuring high-quality application performance across devices.
HeadSpin helps organizations integrate with test automation frameworks like Appium, Appium inspector, Selenium, Xcode,Cucumber, Flutter, and more. With various APIs, issue tracking systems, CI/CD workflow integration capabilities, HeadSpin is fully equipped to help you with your test automation requirements.
Conclusion
Understanding and effectively utilizing automated testing frameworks is crucial in today's fast-paced software development environment. Organizations can deliver high-quality software products by choosing the right framework and leveraging platforms like HeadSpin.
Original Source:
https://www.headspin.io/blog/what-are-the-different-types-of-test-automation-frameworks
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
GoCodeo
GoCodeo
GoCodeo
Follow
Founder's Office at GoCodeo
Joined
Sep 26, 2024
•
Nov 5
Dropdown menu
Copy link
Hide
In today's fast-paced software development landscape, having a robust test automation strategy is crucial for maintaining code quality and accelerating release cycles.
Our latest blog delves into the importance of a structured approach to test automation, outlining key components from tool selection to risk prioritization.
By establishing a well-defined strategy, teams can minimize technical debt, enhance test reliability, and align testing efforts with business objectives.
Whether you're integrating automation into your CI/CD pipeline or looking to streamline your testing processes, this guide provides actionable insights to empower your engineering teams.
Discover how GoCodeo can enhance your automation capabilities and help your team deliver high-quality software with confidence and speed.
Read the full blog here: 👉
gocodeo.com/post/building-a-strong...
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
AI Safety Breakthrough: New System Cuts Harmful Content by 76% While Maintaining Performance
Mike Young -
Dec 12
New Research Shows How AI Programs Can Multi-Task Like Humans Through Improved Function Calling
Mike Young -
Dec 12
AI Video Generation Breakthrough: Point Tracking Makes Videos More Stable and Natural
Mike Young -
Dec 12
12 Ways Experts Break AI Language Models Revealed in New Study - A Deep Dive into Red Team Testing
Mike Young -
Dec 12
Bertha White
Follow
Automation Consultant | Selenium | DevOps | Agile | Automation | Digital Transformation | Big Data | Unit Testing | Integration testing
Work
Senior Automation Consultant
Joined
Jun 13, 2023
More from
Bertha White
Why A Stable Test Automation Environment is Vital
#
testautomation
#
automationtesting
#
softwaretesting
#
apptesting
Critical Aspects You Need to Know about Appium and Selenium
#
appium
#
selenium
#
testautomation
#
automationtesting
AI-driven Test Automation Approach for Large-scale Applications
#
testautomation
#
automationtesting
#
aiintestautomation
#
headspin
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
My Journey into the Cloud:Getting AWS Certified - DEV Community,"My Journey into the Cloud:Getting AWS Certified - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Adit Modi
for
AWS Community Builders
Posted on
Jan 17, 2021
• Edited on
Mar 13, 2021
My Journey into the Cloud:Getting AWS Certified
#
aws
#
cloudcertified
#
gettingstartedwithcloud
I started my journey into the cloud around june 2018. I was always interested in cloud technologies and found it fascinating. I was surfing a lot of random articles and blogs carrying information about cloud .
I had a lot of questions like
· what is cloud ?
· which cloud provider is the best from AWS , GCP, Azure etc ?
· what should be my learning path ? how should I proceed ?
My Background: I am Cloud and Big Data Enthusiast | 4x AWS Certified | 3x OCI Certified | 3x Azure Certified .
I started my Cloud Journey of Learning Amazon Web Services (AWS) from going through AWS training and certifications.
I found that In Order to learn AWS , I have to practice all the AWS Services and thus I quickly signed up on the AWS .
I used to spend almost 3 to 4 hours daily trying to practice the various AWS Services and it Helped me a lot in Understanding the AWS Architecture.
Now, Moving on to how to pass the AWS Certified Cloud Practitioner Exam ?
Always focus on learning AWS Services and never focus on passing the certified exams for the sake of it . I will provide the various learning materials which I used for passing the AWS Certified Cloud Practitioner Exam. For passing the Exam my tips are:
· Check study guide and sample questions.
· Go through all the AWS Training video lectures.
· Read the whitepapers and faqs .
· Go through this link.
· Also practice the AWS Services like EC2 , S3 and Understand the billing and cost policies related to AWS as they are being asked more for this exam.
The Exam itself is not super technical. You can earn this certification without knowing how to code. The official certification description recommends you have some basic IT knowledge and “six months of experience with the AWS Cloud in any role, including technical, managerial, sales, purchasing, or financial.”
· Lastly solve some practice papers from some sites mentioned below:
->digitalcloud.training
->whizlabs
The only intent for solving the practice papers should be to become more confident when it comes to the AWS Services and Your Understanding about the AWS Concepts can become more clear after practicing this questions . *
Do not go for Exam Dumps to pass AWS Exams .
*
Now After I cleared my CCP Exam , I moved onto the AWS Certified Solutions Architect Associate Exam. There are basically SAA-C01 (which is retiring ) and SAA-C02 (the updated exam) available on the aws site.
How to Prepare for clearing AWS Certified Solutions Architect Associate Exam (SAA-C01) ?
I spend almost all my time practising different AWS Services. The Most Important AWS Services are EC2 , VPC , S3 and ELB . This Services are asked a lot in this Exam but ensure that you practice all the services throughly.
Some Steps to follow for passing AWS Certified Solutions Architect Associate Exam are:
· Take a Course for learning the AWS Services. There are two popular courses offered:
-> Acloud-guru Course by Ryan Kroenberg.
-> Udemy Course by Stephane Maarek.
· I used to Spend 2–3 hours a day to practice various AWS Services giving at least 2 weeks of time on preparation for the Exam .
· Go through the Study Guide and Sample questions.
· Read the Whitepapers and Faqs.
· Solve practice papers from some sites mentioned below:
->
https://www.knowledgehut.com/practice-tests/aws-solutions-architect-associate
->
https://www.whizlabs.com/aws-solutions-architect-associate/
How to Prepare for clearing AWS Certified Developer Associate Exam ?
AWS Developer Exam is focused more on AWS Services like DynamoDB, AWS Lambda , Api Gateway ,Cloud Formation , Elastic Beanstalk , X-Ray , Step Functions , CI/CD Services like CodeCommit , CodeBuild , CodeDeploy and CodePipeline etc .
Important Concepts like global and local secondary indexes , provisioned throughput calculation , DAX , exponential backoffs , 4 deployment methods for elastic beanstalk , CodeDeploy — Inplace and blue/green , appspec.yaml file , hooks , CodeBuild — buildspec.yaml , X-Ray SDK, X-Ray Daemon etc .
Some Steps to follow for passing AWS Certified Developer Associate Exam are:
· Take a Course for learning the AWS Services. There are two popular courses offered:
-> Acloud-guru Course by Ryan Kroenberg.
-> Udemy Course by Stephane Maarek.
The Developer Exam is by far the most practical exam among all the associate exams.
· Go through the Study Guide and Sample questions.
· Read the Whitepapers and Faqs.
· Solve practice papers from some sites mentioned below:
->
https://www.knowledgehut.com/practice-tests/aws-certified-developer-associate
->
https://digitalcloud.training/aws-developer-associate-free-practice-exam-questions/
->
https://www.whizlabs.com/blog/aws-developer-associate-exam-questions/
How to Prepare for clearing AWS Certified SysOps Administrator Associate Exam ?
AWS SysOps Exam is focused more on AWS Services like CloudWatch, Cloud Networking, AWS OpsWorks, Systems Manager, CloudFormation etc.
SysOps Admin has “Monitoring and Reporting” which are worth 22% of the final exam score.
When you study for more than one AWS Associate certification, you will notice some overlap among the subjects covered on exams.
As a result, about half of this SysOps prep guide overlaps with other prep guides for other AWS associate certifications exams.
Some Steps to follow for passing AWS Certified SysOps Administrator Associate Exam are:
· Take a Course for learning the AWS Services. There are two popular courses offered:
-> Acloud-guru Course by Ryan Kroenberg.
-> Udemy Course by Stephane Maarek.
· This Exam is the Hardest among the AWS Associate Exams and should be taken only after through understanding of AWS Services which are asked in the exam.
· Go through the Study Guide and Sample questions.
· Read the Whitepapers and Faqs.
· Solve practice papers from some sites mentioned below:
->
tutorialsdojo
->
Udemy
->
whizlabs
My Experience with AWS has been a more than two years now and what I have Learned so far is to Practice AWS Services and Once you are Confident regarding your skills and feel that you understand the Concepts needed to Clear the Exams then and only then should you schedule the AWS Exams . AWS Exams require you to have Hands-On Experience with the AWS Services meaning you need to be familiar with AWS Services before scheduling your exams.
My Suggestions would definitely be try AWS Free Tier for about 6 months try various Services and Once you understand the Core Services and are Confident then you can easily pass the AWS Exams.
I hope that this guide helps you in building your career with AWS and getting AWS certified,
If you have any doubt or unable to understand any concept feel free to contact me on
LinkedIn
Instagram
Twitter
Github
You can view my badges on
I also am working on various AWS Services and Developing various AWS and Big Data Projects.
If you are interested in learning AWS Services then follow me on github.
If you liked this content then do clap and share it . Thank You .
Cloud computing is the third wave of the digital revolution. — Lowell McAdam, CEO of Verizon
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
girsaran
girsaran
girsaran
Follow
I work with Whizlabs, here to connect with the community.
Work
Vice president at Whizlabs
Joined
May 19, 2021
•
May 26 '21
Dropdown menu
Copy link
Hide
Congratulations, for the trial purpose you can explore Whizlabs cloud lab as well
play.whizlabs.com/
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Discover Amazon S3 Metadata: A New Way to Explore Your Storage at AWS re:Invent 2024
Ahmed Adel -
Dec 4
Error when retrieving token from sso Token has expired and refresh failed
Joao Marques -
Nov 13
Introducción a la Infraestructura como Código (IaC) con AWS CloudFormation
Brian -
Dec 3
Highlights from Peter Desantis keynote at AWS reinvent 2024
Eyal Estrin -
Dec 3
AWS Community Builders
Follow
Build On!
Would you like to become an AWS Community Builder? Learn more about the program and apply to join when applications are open next.
Learn more
More from
AWS Community Builders
Secure AWS VPC using Public and Private Subnets
#
aws
#
cloud
#
vpc
#
security
🚀 Introducing Amazon Aurora DSQL: The Next Evolution in Databases 📊
#
aws
#
database
#
rds
#
data
AWS re:Invent 2024 Day 4
#
aws
#
cloud
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
What are the alternatives to unit tests? - DEV Community,"What are the alternatives to unit tests? - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
K
Posted on
Jun 22, 2018
What are the alternatives to unit tests?
#
coding
#
discuss
When I was in university, I had a lecturer who didn't like unit tests. He was an eldery man who worked at IBM and gave lectures about mainframes. He always said, the hype about unit tests would simply double the amout of code written and do nothing for its safety.
When I did my first big project in 2009, a HTTP API, nobody in the company I worked for (the company was founded in 2001) had written any unit tests. They had huge C/C++ and PHP code-bases. They did integration tests, but the project I had been given was the first that used unit-tests.
I heard about it at university and wanted to make my first project look good right from the start. So I wrote a bunch of unit tests for every
class
, ending up with about 200 tests after the first version was released. Trying to hit that famous 100% coverage. Only a few months later the architecture of the API changed and somebody new at the project hat to rewrite more than 100 tests.
In the lifetime of the project, the unit tests didn't prevent any major bugs, just the stuff I had in mind while testing the code, but they slowed down the development progress tremendously. Also, they forced some style on the code that was mostly there to ease the writing of the tests and not the resulting application.
So what is your opinion about this? Did I do unit tests wrong? Is there an alternative? Are integration tests (black- or grey-box) enough when automated? Is TDD a placebo? Are type-systems the way to go?
Lets discuss! :)
Top comments
(47)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Chris James
Chris James
Chris James
Follow
Engineering Manager
Location
London
Joined
Mar 11, 2018
•
Jun 22 '18
• Edited on
Jun 22
• Edited
Dropdown menu
Copy link
Hide
So what is your opinion about this? Did I do unit tests wrong?
Probably yes (sorry!)
It's a classic issue of writing tests that are too coupled to implementation detail. People then get frustrated at tests because they can no longer refactor without changing everything
So I wrote a bunch of unit tests for every class, ending up with about 200 tests after the first version was released. Trying to hit that famous 100% coverage.
I'm going to speak in terms of TDD; and that does
not
prescribe writing tests for every class/function/whatever. It prescribes it for every
behaviour
. So you may write a thing that does X, internally it may have a few collaborators; don't make the mistake of writing tests for implementation detail. These
are still unit tests
.
Ask yourself. If I were to refactor this code, would I have to change lots of tests? The very definition of refactoring is changing the code without changing behaviour. So in theory you should be able to refactor without changing tests.
I would suggest looking into Kent Beck's book on test driven development. It's an easy read and quite short. Or if you like Go and dont want to pay any money
have a look at my book
. This video covers some of the main issues you talked about and probably explains what i've typed a lot better
infoq.com/presentations/tdd-original
Writing tests effectively takes a while to get proficient at, but the fastest way to get there is to study and retrospect the effect tests had on your codebase
Like comment:
Like comment:
44
likes
Like
Comment button
Reply
Collapse
Expand
Alessandro Ronchi
Alessandro Ronchi
Alessandro Ronchi
Follow
Experienced technical leader, devoted to software design, development, and training. 
Active Magento maintainer and contributor.
Location
Italy
Education
CS degree
Work
COO at Bitbull
Joined
Feb 24, 2017
•
Jun 23 '18
Dropdown menu
Copy link
Hide
My 2¢ about this discussion.
First of all, I think that this quote is fundamental to understand
why we test our code
:
“Testing shows the presence, not the absence of bugs”
~ E. W. Dijkstra
It means that our tests can't prove the correctness of our code, they can only prove that our code is safe against the bugs that we are looking for.
Having 100% code coverage doesn't guarantee that our code is 100% correct and bug-free.
It only means that our code is 100% safe against the bugs that we are looking for.
There may be bugs we aren't looking for even with a 100% code coverage passing tests.
Tests show the presence, not the absence of bugs.
Chris James says:
""the very definition of refactoring is changing the code without changing behavior.""
The behavior refactoring refers to is
external behavior
, that is, the expected
outcome
of a piece of code, not how the code behaves internally.
When we write a test, we can make assertions about internal behavior but it can change without modifying the expected output.
That's the very definition of refactoring.
When we make assertions about the internal behavior, we are coupling our test to an implementation: internal behavior changes will likely bring to change the test.
That's why I like what Michał T. says:
""code that is perfectly suited for unit tests are things that have predictable inputs and outputs, and don't have dependencies or global effects.""
The assertions about the behavior of our code will likely depend on the behavior of our dependencies.
Indeed, we mock external dependencies because we don't want our code being affected by their potentially bugged outcome.
Thus, we set up our environment to have a predictable output.
That's why even if external dependencies have bugs, our unit tests can pass. And that's why unit tests aren't enough to save us from having issues.
Reducing external dependencies will make our code easier to test and less prone to side effects coming from the outside.
My last thought, starting with this quote from connectionist:
""code changes happen all the time and unit tests have to change with them. It's unpleasant but necessary.""
Software, by definition, is
soft
to adapt to changes.
Otherwise, it would have been ""hard"" ware.
We have to deal with it. It should not be unpleasant but the opposite: it's its the
ability to change
that proves the real value of software.
The frustration that we feel when we have to change our software comes from the fact that as long as we add code we tend to reduce the flexibility of our software (we add
accidental complication
).
Thus, adapting to changes becomes frustrating.
But it's not software's fault.
It's not our customers' fault.
It's our fault.
It's only by making our code
better over time
that we can reduce that frustration.
And we can make it better by performing refactoring on a regular basis.
Everything that encourages refactoring should be welcome.
I warmly recommend watching this:
vimeo.com/78898380
Cheers
Like comment:
Like comment:
29
likes
Like
Comment button
Reply
Collapse
Expand
Nolan 🚀👉❤ :/
Nolan 🚀👉❤ :/
Nolan 🚀👉❤ :/
Follow
Joined
Jun 22, 2018
•
Jun 22 '18
Dropdown menu
Copy link
Hide
Adding to test the code's behavior, test that the code implements requirements: those things the end user, legal, marketing has to have. Then you get into tracing requirements to exact lines of code, and anything else can get deleted.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
K
K
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
Thanks, I'm going to read this book :D
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Frank Carr
Frank Carr
Frank Carr
Follow
Joined
Oct 13, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
One of the important things that unit tests will do is to get you focused on SOLID, most notably single responsibility. It reduces the temptation to write ""Swiss army knife"" functions or massive blocks of if..else or switch..case code. When you work in short blocks of testable code it makes debugging so much easier. Likewise, if you find tests becoming elaborate, maybe some refactoring is needed.
When you're working on a team, having the unit test gives other developers a guide as to how a particular function should work. If they come up with use cases you didn't anticipate, it provides an easy way for them to communicate it. When you're primarily working on the backend, it gives you something to demo in the sprint retrospective/demo.
When debugging issues unit tests make it easier to locate problem areas both in integration testing and in production. Without having this testing you can spin your wheels trying to find bugs.
Alternatives to unit tests? I've had to do these when working with legacy code where there were no tests originally written. Usually, these tests were in the form of one-off sandbox applications that would exercise a particular function or set of functions, trying to track down a bug. I've found this to be more inefficient than writing tests to begin with, particularly when trying to deal with critical production problems.
Like comment:
Like comment:
15
likes
Like
Comment button
Reply
Collapse
Expand
Kasey Speakman
Kasey Speakman
Kasey Speakman
Follow
collector of ideas. no one of consequence.
Location
Huntsville, AL
Joined
Apr 5, 2017
•
Jun 22 '18
• Edited on
Jun 22
• Edited
Dropdown menu
Copy link
Hide
Thanks for posting your experiences. ❤️ I have similar history with unit tests.
Nowadays, I no longer bother to test everything. I do not believe there is enough ROI in doing so for most of our apps. I mainly test business logic. And when I say that, I mean
only business logic
. I practice Dependency
Re
jection, so business logic code is purely logic and is very easy to test. I will highlight the difference with a couple of images.
This kind of design is what you normally see exemplified in unit test demos with interfaces being injected into business code. This makes ""business code"" not only responsible for business calculations but also handling IO. Despite those things being represented as interfaces, the code will likely need to know specifics like which exceptions are thrown or other effect details which are unique to type of integration. So it has the appearance of decoupling while potentially being still quite coupled.
This kind of design also creates a lot of work in unit tests, since you have to create behavioral mocks of the injected components. The fact that you need a framework to dull the pain is a hint that it is not an optimal strategy.
Instead, I do this.
Here, the business logic code (inner circle) has no knowledge of other components outside of its purview... not even their interfaces. It only takes data in and returns other data. If IO is necessary to fetch the data, the logic does not care about it and is not responsible for it. Only once it is fetched do you run the logic code. It is also fair to give the logic code an object representing no data (e.g. Null Object Pattern or a Maybe). This is ridiculously easy to test since all you only have to pass in some data and check the the output matches what you expect.
For example, I might have some logic code like this:
let
createItem
existingItem
data
=
match
existingItem
with
|
Some
i
->
Error
ItemAlreadyExists
|
None
->
validate
data
// validate: check required fields, invalid ranges, etc.
Then have a test like this:
[<
TestMethod
>]
member
__.
``Duplicate items cannot be created``
()
=
let
existingItem
=
Some
{
...
}
let
data
=
{
...
}
let
expected
=
Error
ItemAlreadyExists
let
actual
=
Logic
.
createItem
existingItem
data
eq
expected
actual
How do I handle IO? I have an outer piece of code (I call a use case handler, or just ""handler"") which is responsible for tying the logic to other integrations (database, API calls, etc.) needed for the use case. Sometimes logic steps are interleaved with IO, and so the logic has different function/methods for each step. The handler must check the logic response from the first step and perform appropriate IO before calling the next step.
This design draws a very fine line between which types of testing is appropriate for which parts. Unit testing (even property-based) is appropriate for business logic code. Integration testing is appropriate for the integration libraries used by the handler. End-to-end testing is appropriate for the handler code itself since it may deal with multiple integrations. But the main win, and the most important thing to the business is the business code -- that decisions are correct. And this is now the easiest piece to test. The other parts are no harder to test than they were before, but still not worth the ROI for us yet.
Like comment:
Like comment:
8
likes
Like
Comment button
Reply
Collapse
Expand
K
K
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
•
Jun 23 '18
Dropdown menu
Copy link
Hide
Ah, yeah I read about these things.
But all the examples were in FP languages I didn't know, so I didn't take much from it.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Blaine Osepchuk
Blaine Osepchuk
Blaine Osepchuk
Follow
I'm a small business programmer. I love solving tough problems with Python and PHP. If you like what you're seeing, you should probably follow me here on dev.to and then checkout my blog.
Joined
Jun 5, 2017
•
Nov 16 '18
Dropdown menu
Copy link
Hide
You might want to search for the ""humble object pattern"" if you want to learn more about Kasey's testing strategy.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
JeffD
JeffD
JeffD
Follow
Code-quality 🩺 Teamwork 🐝 & everything that can simplify the developper's life  🗂️.
Location
France
Joined
Oct 16, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
(I'm a unit-test-addict :) )
Did I do unit tests wrong?
In my opinion unit-tests are documentation, so if your product change, your unit-tests must be rewrited. If you had to rewrite to many tests for a little change so maybe you should make your tests more flexible, or use them only to test the ""freezed part of your code"" (utils functions and algorithms).
Is there an alternative?
In case of API (constantly evolving) some tools create test directly from spec (Swagger maybe).
Are integration tests enough?
It's difficult to test only a function with integration test, the scope is not the same. But testing ""GET .../user/1"" return the good object it could be ok. I higly recommand to use unit-test to deal with user-inputs (POST requests) because you can test the endpoint with a lot of bad entry (and check for security, malformed, bad type, ...)
Is TDD a placebo?
Personnaly it's a security I love to have :)
Like comment:
Like comment:
6
likes
Like
Comment button
Reply
Collapse
Expand
K
K
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
> maybe you should make your tests more flexible
How? :)
> or use them only to test the ""freezed part of your code""
Isn't this against the TDD philosophy?
> I higly recommand to use unit-test to deal with user-inputs
How does this eliminate the problem that I only test what I had in mind anyway when writing the functionality in the first place?
Like, when I test my software I find fewer bugs than when someone else tests it. etc.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
JeffD
JeffD
JeffD
Follow
Code-quality 🩺 Teamwork 🐝 & everything that can simplify the developper's life  🗂️.
Location
France
Joined
Oct 16, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
More flexibility:
Use Single  responsibility principle (SOLID) / functionnal programming pure function : it reduce the test scope.
Maybe using mock can help
It's recommanded to test only one case per test, I get the bad practice to put all my testing case into an array:
for (arg1, arg2, result) in [(1,2,3),(-1,-3,-4)]:
assert(my_sum_function(arg1, arg2) == 3)
It's bad but you can make a lot of case and change function name easily.
Maintains few tests is always better to have no test at all. To encourage your team adding tests it should be easy ;). So test the freezed functions is a good start.
I'ld love to write an article about ""unexpected testing cases"", I have this list of error cases:
Not found: unexisting file (or bad permission)
bad type: '01' instead of 01
bad format: negative integer, phone number with +XX prefix, (XSS injection for HTML field), too long (buffer overflow) ...
injection: SQL, XPath, LDAP, ...
textcase: get uppercase when we're waiting lowercase
None/undefined value
Exception: divide by zero, ...
timeout/network error/ database off
invalid endpoint/version/key/authentification (for API)
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
kashperanto
kashperanto
kashperanto
Follow
Joined
Jun 22, 2018
•
Jun 22 '18
Dropdown menu
Copy link
Hide
The main thing with TDD from my understanding is that tests
are
the requirements, so anything that falls outside of the tests is by definition irrelevant.  Most of the ""test everything"" recommendations come from the TDD mindset, so if you try to apply that outside of the TDD framework it can get messy.
This perspective helps limit the scope and coupling of your tests, since there is typically an astronomical number of tests that you
could
do, but a very finite number of testable requirements.  Refactoring should not generally break tests, but if refactoring occurs across/between several modules then you will probably have some rework, but I would argue that that is more of a ""redesign"" than a ""refactor"".
One good reason to test every module/class is to reduce the scope of any bugs you do come across.  If I have a suite of tests that demonstrate my module's behavior then I know where not to look for the bug.  With integration/system tests alone you will have some searching to do.
Like comment:
Like comment:
1
like
Like
Thread
Thread
K
K
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
I always have the feeling that is still a problem.
I get rather high leven requirements, but they are implemented by many parts of the code. So simply writing a ""Req1 passes"" would require to implement many many thigns till the requirement is met.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
ItsASine (Kayla)
ItsASine (Kayla)
ItsASine (Kayla)
Follow
Azure DevOps and git admin with a weird interest in résumés and portfolios 🎉
Lawful Neutral
Email
dev@itsasine.dev
Education
Master's in Applied Mathematics
Work
DevOps | CI/CD | Release Engineer
Joined
Feb 4, 2017
•
Jun 23 '18
• Edited on
Jun 23
• Edited
Dropdown menu
Copy link
Hide
I'm bookmarking this to read later (so many good comments!) but I'll chime in with a QA perspective:
If you're working at a place with a formal QA step, test your implementation, not your requirements
I've noticed in my devs' specs they'll have tests for things like ""this has been called once"", ""all parts of this
if
can be hit"", yada yada yada, and then there will be things like ""it returns all this info"", ""the info is sorted properly"", ""the info is in the right format"", etc.
Then if you look at my tests, they're ""it returns all this info"", ""the info is sorted properly"", ""the info is in the right format"", etc... things a user would see and that are in the story's acceptance criteria for the feature. Where I am, QA automation (end-to-end with a hint of integration testing) is a part of the formal definition of done, so a feature isn't considered done until both of us have written the same thing just at two different levels.
Like comment:
Like comment:
4
likes
Like
Comment button
Reply
Collapse
Expand
Camilla Santiago
Camilla Santiago
Camilla Santiago
Follow
I love refactoring.
Location
Philippines
Work
Indie Game Developer
Joined
Jan 21, 2018
•
Jun 22 '18
• Edited on
Jun 22
• Edited
Dropdown menu
Copy link
Hide
I haven't written any unit tests for Web APIs yet but here's my take on TDD:
In my part, I don't recommend writing unit tests for every class. Only for classes that changes behavior based on various arguments and conditions.
Writing unit tests helps me in various ways:
It validates my understanding of the requirement. There's a tendency for us developers to jump right into coding without fully grasping the requirement. Writing unit tests forces us to think and ask questions even before the actual coding. Which eventually saves us more time than rewriting code from previous assumptions.
It helps me make design decisions. That is, if a class is hard to test, it may still be broken down into smaller testable classes. Therefore, enforcing SRP (Single Responsibility Principle)
Acts as harness after refactoring and bug fixing. Tests should still be green after code changes. It's a quality layer that signals me that I didn't break anything.
Like @JeffD said, also a documentation. I've written and deleted a lot of unit tests. Requirements may or may not change in the future. You don't know when or if it will but for this time that it's true, its better to write unit tests than to write none in anticipation that it will just be deleted or changed in the future.
Hopefully, these insights helped you.
Like comment:
Like comment:
4
likes
Like
Comment button
Reply
Collapse
Expand
K
K
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
You're probably right.
I often read unit tests of libraries I used to understand them, but on the other hand I don't write libraries myself. They feel like they would lend themselves rather well to unit-testing, like APIs and such. UIs feel different somehow.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Idan Arye
Idan Arye
Idan Arye
Follow
Education
B.Sc. Computer Science and Mathematics
Joined
Dec 2, 2017
•
Jun 23 '18
Dropdown menu
Copy link
Hide
If you haven't already, you should read Joel Spolsky's excelent article
Five Worlds
. To sum it up - great programmers sometimes come up with tips and best practices that make perfect sense in the area they work in, but are not very useful and maybe even harmful in other areas.
I believe unit testing is one of these best practices. When it comes to library development, for example, unit testing are great. In other areas their RoI is too low to be useful, and other kind of tests should be preferred.
In my opinion, automated tests should be an automation of manual tests. It usually easy to decide how to test something manually. For example:
When you are developing a library function, you write a small
main
that prints it's output for some hard-coded inputs.
When you are developing a new feature, you run the entire program and use that feature to see that it works.
These workflows are intuitive:
You can't test the new feature with a custom
main
. I mean you can - but that would be a lot of work to re-create the environment needed to test that feature, and in the end it won't be very effective because that temporary environment may be different than what you use in the actual program.
You can't test that library function by running the entire program. I mean you can - but you won't be able to give it the inputs you want to test and you won't be able to directly see the output. Unless you use a debugger, that is. Also, having to run everything up to the point that function is used will make your cycles needlessly long.
Since the manual testing strategy is so clear, the automated testing strategy should mimic it. Use unit tests for the library function and integration tests for the feature. Some people will insist on unit tests for the feature, but that has the exact same drawbacks of manually testing it with a custom
main
.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
atsteffen
atsteffen
atsteffen
Follow
Solving big data problems in Genomics research. I'm known for oversimplifying the complex and overcomplicating the ""simple""
Location
Saint Louis
Work
Software engineer at Partek Inc.
Joined
Nov 5, 2018
•
Dec 10 '18
• Edited on
Dec 10
• Edited
Dropdown menu
Copy link
Hide
I second this!
When working with a mature framework, or using a good library, features should usually come in the form of extensions. The purest extensions are those that are almost entirely declarative.  i.e. you are just picking what functionality offered by the framework to compose into your new feature. When a piece of code simply composes, or declares constants, there is nothing to unit test.  There's no such thing (at a unit level) as declaring the wrong constant or composing the wrong functionality. The declarations should trivially match your requirements, and (though we may have our opinions) there are no wrong or right requirements.  If you write unit tests to re-assert declarative requirements, you will just have to change those tests as the requirements change without ever really protecting the ""correctness"" of anything.  Also, these extensions are usually the most sensitive thing to API changes, and can double your clean-up effort if you have a framework API update.
Of course there are usually logical utilities and functional bits added with feature extensions, but those can usually be tested in isolation of the declarative bits. Their functional bits can always be made into a local mini-library, which is again just composed into the final feature, locally testable, and ideally not sensitive to changes to the API that the feature is extending.
High level integration tests are what you need to guarantee that you've composed these features properly to produce the desired effect.
My guess from the OP stating that there were hundreds of tests to change on an API change is that he was either testing declarative bits, or didn't have declarative bits properly isolated.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
conectionist
conectionist
conectionist
Follow
Joined
Jan 11, 2018
•
Jun 22 '18
Dropdown menu
Copy link
Hide
Did I do unit tests wrong?
I can't say for sure, but what I can say if ""Trying to hit that famous 100% coverage"" is a nothing but a wild goose chase. To find out why, see this article:
dev.to/conectionist/why-code-cover...
Is there an alternative?
Code changes happen all the time and unit tests have to change with them.
It's unpleasant but necessary.
However, if a large part of your architecture has to change (and this happens quickly/frequently) then the problem is not with your unit tests.
It's with the architects and the faulty/rushed decisions they make when deciding upon an unstable/unreliable architecture.
Are integration tests (black- or grey-box) enough when automated?
NO!
Unit tests and integration tests serve different purposes. They are complementary. They are not meant to be a substitute for one another.
Unit tests are meant to test code. They are like a defense mechanism against yourself (or, more specifically, against accidental mistakes you might make).
The idea is the following:
you write a piece of code
you decide what your expectations you have after that code has run
you write some unit tests that make sure those expectations remain the same after you've made some changes in that area
Because it's possible that changes you make in some places, have undesired effects in other places. That's where unit tests come in. They tell you ""No, no! If you continue with these changes, you will break something that was working well. Back to the drawing board!""
Integration tests on the other hand test functionality. They check if everything works ok when it's all put together.
Is TDD a placebo?
Certainly not. But like all things, it works only if used properly.
As a side note, don't be discouraged if your unit tests didn't catch any major bugs. That's very good! That means your a good programmer who writes very good code.
If your unit tests failed every time you ran them, it would mean you're very careless (or in love and with your head somewhere else :)) )
Think of it this way:
If you hire a security guard and you have no break-ins, are you upset that you have no break-ins?
You're probably feel that you're paying the security guard for nothing.
But trust me, if he wasn't there, you'd have more break-ins that you'd like.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
K
K
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
Yes, I guess that's the problem.
After a few years of practice you write code that is pretty robust and the tests you write basically do nothing until the first changes to the software happen :)
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Michał T.
Michał T.
Michał T.
Follow
Joined
Jun 22, 2018
•
Jun 22 '18
• Edited on
Jun 22
• Edited
Dropdown menu
Copy link
Hide
From my experience unit tests are incredibly useful when developing code that is perfectly suited for unit tests, generally things that have predictable inputs and outputs, and don't have dependencies or global effects. On the other hand if you're testing boilerplate code with a lot of complex dependencies (i.e. an MVC controller) it's probably better to cover it with integration or acceptance tests.
You should move as much code as reasonably possible into unit-testable blocks, but going out of your way for 100% unit-test coverage leads to tests that aren't worth writing and updating.
Then there are tricks, like mocking outside services (so that you don't have to actually hit remote services when running acceptance tests) and comparision testing, i.e. not testing the contents of an XML document but just storing it and comparing output directly to it. When testing APIs I also automatically test inputs and outputs on endpoints against a specification, which is a pretty good way of testing both the endpoints and the specification.
I also think that unit tests are a great way to force yourself to write easily testable code, which is usually better structured than non-testable code :)
But in general code needs to be tested if you care about it working. Any endpoint you don't test will eventually be broken.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Jan van Brügge
Jan van Brügge
Jan van Brügge
Follow
Student of the technical university munich, chief software engineer for the MOVE-II CubeSat, likes functional programming and sports
Joined
Jan 20, 2017
•
Jun 22 '18
Dropdown menu
Copy link
Hide
My take on unit tests is to avoid them. Write your software ina way it
could
be tested easily, as this will keep your code decoupled, will force you to explicitly inject external stuff and more.
But if you have a decent type system
and
a bunch of integration/end-to-end tests, unit tests are not worth the hazzle.
After all you dont care about implementation details as long as your module/component/insert-similar-here does the correct thing
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
View full discussion (47 comments)
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
What's your favorite book on web development? 📖
Thomas Bnt -
Nov 13
12 Useful Developer Tools You Will Wish You Knew Sooner 🧑‍💻🧙
Madza -
Oct 31
CSS has a new logo (and new features)! 🎉
Best Codes -
Nov 23
Decoding Networking Protocols: A Guide to TCP/IP, HTTP, Socket, and WebSocket
Philip  -
Nov 13
K
Follow
A developer that became a full-time writer, here on dev.to!
Location
Stuttgart, Germany
Education
Computer Science and Media
Work
I wrote ""React From Zero"" at \newline
Joined
Jan 3, 2017
More from
K
Pattern-Match your JavaScript with Z
#
javascript
#
coding
React Hooks Demystified
#
react
#
coding
MDX - JSX in Markdown
#
coding
#
javascript
#
react
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Tools for effective DataOps implementation - DEV Community,"Tools for effective DataOps implementation - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Rootstack
Posted on
Sep 23
Tools for effective DataOps implementation
#
webdev
#
datascience
#
softwaredevelopment
#
devops
The increasing complexity of data in modern organizations has given rise to the need for more agile and efficient practices. DataOps emerges as a response to improve data management, optimize workflows, and foster greater collaboration between IT teams and data analysts.
In this blog, you will find key tools that help
implement DataOps
effectively, allowing companies to improve their agility and responsiveness.
What is DataOps?
DataOps is a methodology that combines DevOps principles with data management. Its main objective is to optimize the data lifecycle, from integration and storage to analysis and delivery, ensuring quality and reliability. Adopting the right tools is crucial to successfully implement DataOps and maximize the benefits of this methodology.
1. Prefect: Workflow Orchestration
Prefect is a powerful platform that enables the orchestration and automation of data workflows. This tool is key in a
DataOps
environment because it facilitates the management and execution of data pipelines, ensuring that all tasks are completed efficiently and without errors. Prefect also has an intuitive interface that allows teams to monitor and debug workflows in real time.
Advantages of Prefect:
Scalability in the execution of workflows.
Real-time monitoring.
Ease of error detection and correction.
2. Apache Airflow for DataOps
Apache Airflow is a widely used open source tool for creating, scheduling, and monitoring complex workflows. Airflow allows DataOps teams to design data pipelines in a flexible and modular way, integrating different data sources and destinations. It is ideal for automating tasks and facilitates collaboration between teams.
Advantages of Apache Airflow:
Flexibility to manage different types of data.
High level of customization.
Large support community.
3. dbt (Data Build Tool): Data Transformation in SQL
dbt is a tool that enables data analysts and scientists to efficiently transform and model data using SQL. This platform facilitates the development and maintenance of data transformation pipelines, ensuring data consistency and quality. In the context of DataOps, dbt is a key solution to ensure that transformed data is accurate and aligned with business objectives.
Advantages of dbt:
Direct integration with popular databases.
Automation of data quality testing.
Capability of versioning and documentation of data models.
4. Kubernetes for DataOps
Kubernetes is an open-source platform that enables the automation of deployment, scaling, and management of containerized applications. In a DataOps strategy, Kubernetes makes it easy to create scalable and resilient environments to handle large volumes of data and workflows. With Kubernetes, teams can efficiently orchestrate the necessary resources, ensuring that data applications operate smoothly.
Advantages of Kubernetes:
Automatic scalability.
Efficient resource management.
Integration with multiple data and analytics tools.
5. Talend: Data Integration
Talend is a comprehensive data integration platform that helps connect, transform, and manage data from diverse sources in real time. Its focus on automation and data quality makes it an essential piece in the implementation of
DataOps
. Talend facilitates data cleansing, enrichment, and migration, allowing companies to maintain high standards of quality and compliance.
Advantages of Talend:
Integration with multiple platforms and data sources.
Focus on data quality and governance.
Real-time processing.
6. Jenkins: Continuous Integration Automation
Jenkins is a widely known tool for its ability to automate continuous integration and deployment processes. In the context of DataOps, Jenkins is used to integrate new data, test pipelines, and ensure that data deployments are always up-to-date and error-free. Jenkins also facilitates the integration of new tools and technologies within the data lifecycle.
Advantages of Jenkins:
Extensive integration capabilities.
Automation of testing and deployments.
Flexibility and customization.
7. Git: Version Control and Collaboration for DataOps
Git is one of the most important tools for version control and collaboration between teams. In a DataOps strategy, Git allows data teams to version scripts, models, and configurations, ensuring that any changes are tracked and reversible. Collaboration between development and operations teams is facilitated by the use of Git, by allowing for an agile and coordinated workflow.
Advantages of Git:
Robust version control.
Collaboration between distributed teams.
Ease of reverting changes or resolving conflicts.
Effective
DataOps implementation
depends largely on the adoption of appropriate tools that enable automation, collaboration, and data quality control.
From workflow orchestration with Prefect and Apache Airflow, to data transformation with dbt, and deployment automation with Kubernetes, these tools help enterprises optimize their data infrastructure and improve data-driven decision making.
Adopting the right tools not only makes data management easier, but also boosts organizations’ agility and responsiveness.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to post a link with embed card on Bluesky with JavaScript
Piotr Kulpinski -
Dec 11
Multiple Stacks in AWS CDK (Typescript)
Matthew Evan Eichler -
Dec 11
🚀 The Rise of Bun.js: Why It’s More Than Just Another JavaScript Runtime 🥖
Hamza Khan -
Dec 11
Join us for the Bright Data Web Scraping Challenge: $3,000 in Prizes!
dev.to staff -
Dec 11
Rootstack
Follow
At Rootstack, we strive for excellence every step of the way. We provide high-quality technology and software outsourcing services and create outstanding digital experiences.
Location
Austin, Texas
Joined
Mar 4, 2024
More from
Rootstack
Continuous integration and continuous deployment: Best practices
#
softwaredevelopment
#
development
#
webdev
How to facilitate the integration of an IT staffing team with the internal team
#
softwaredevelopment
#
softwareengineering
#
developers
#
softwareteam
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Bertha White - DEV Community,"Bertha White - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Follow
User actions
Bertha White
Automation Consultant | Selenium | DevOps | Agile | Automation | Digital Transformation | Big Data | Unit Testing | Integration testing
Joined
Joined on
Jun 13, 2023
twitter website
Work
Senior Automation Consultant
More info about @berthaw82414312
Badges
One Year Club
This badge celebrates the longevity of those who have been a registered member of the DEV Community for at least one year.
Got it
Close
Writing Debut
Awarded for writing and sharing your first DEV post! Continue sharing your work to earn the 4 Week Writing Streak Badge.
Got it
Close
4 Week Writing Streak
You've posted at least one post per week for 4 consecutive weeks!
Got it
Close
Skills/Languages
Selenium, Appium, Software Testing, Test Automation
Post
74 posts published
Comment
0 comments written
Tag
0 tags followed
Why A Stable Test Automation Environment is Vital
Bertha White
Bertha White
Bertha White
Follow
Dec 12
Why A Stable Test Automation Environment is Vital
#
testautomation
#
automationtesting
#
softwaretesting
#
apptesting
Comments
Add Comment
4 min read
Transform Your OTT Experience: A Comprehensive Guide to Optimizing Video Streaming
Bertha White
Bertha White
Bertha White
Follow
Dec 9
Transform Your OTT Experience: A Comprehensive Guide to Optimizing Video Streaming
#
otttesting
#
videostreaming
#
ottexperience
#
videotesting
Comments
Add Comment
4 min read
The Ultimate Guide to Choosing Mobile Devices for Testing
Bertha White
Bertha White
Bertha White
Follow
Dec 4
The Ultimate Guide to Choosing Mobile Devices for Testing
#
mobiledevicetesting
#
mobileapptesting
#
apptesting
#
mobiletesting
Comments
Add Comment
4 min read
Critical Aspects You Need to Know about Appium and Selenium
Bertha White
Bertha White
Bertha White
Follow
Nov 28
Critical Aspects You Need to Know about Appium and Selenium
#
appium
#
selenium
#
testautomation
#
automationtesting
Comments
Add Comment
4 min read
The iOS App Testing Checklist: Your Path to Flawless Apps
Bertha White
Bertha White
Bertha White
Follow
Nov 25
The iOS App Testing Checklist: Your Path to Flawless Apps
#
iosapptesting
#
apptesting
#
iostesting
#
iosapps
Comments
Add Comment
5 min read
How to Use Playwright Locators: A Detailed Guide
Bertha White
Bertha White
Bertha White
Follow
Nov 22
How to Use Playwright Locators: A Detailed Guide
#
playwright
#
automatedtesting
#
playwrightlocators
#
crossbrowsertesting
Comments
Add Comment
9 min read
6 Best Practices for Cloud Performance Testing
Bertha White
Bertha White
Bertha White
Follow
Nov 18
6 Best Practices for Cloud Performance Testing
#
performancetesting
#
cloudperformancetesting
#
cloudtesting
Comments
Add Comment
7 min read
What is Continuous Integration? - A Comprehensive Guide
Bertha White
Bertha White
Bertha White
Follow
Nov 15
What is Continuous Integration? - A Comprehensive Guide
#
ci
#
continuousintegrationtools
#
cloudbasedtesting
#
continoustesting
Comments
Add Comment
5 min read
Transform Mobile Testing with Secured Cloud-Based Testing
Bertha White
Bertha White
Bertha White
Follow
Nov 15
Transform Mobile Testing with Secured Cloud-Based Testing
#
mobiletesting
#
cloudbasedtesting
#
mobileapptesting
#
cloudtesting
Comments
Add Comment
8 min read
Transform Mobile Testing with Secured Cloud-Based Testing
Bertha White
Bertha White
Bertha White
Follow
Nov 12
Transform Mobile Testing with Secured Cloud-Based Testing
#
mobiletesting
#
cloudbasedtesting
#
mobileapptesting
#
cloudtesting
Comments
Add Comment
8 min read
AI-driven Test Automation Approach for Large-scale Applications
Bertha White
Bertha White
Bertha White
Follow
Oct 21
AI-driven Test Automation Approach for Large-scale Applications
#
testautomation
#
automationtesting
#
aiintestautomation
#
headspin
Comments
Add Comment
5 min read
Choosing the Right Testing Strategy: Functional vs. Unit Testing
Bertha White
Bertha White
Bertha White
Follow
Oct 18
Choosing the Right Testing Strategy: Functional vs. Unit Testing
#
functionaltesting
#
unittesting
#
testingstrategy
#
testautomation
Comments
Add Comment
4 min read
Best SDLC Model for Mobile App Development
Bertha White
Bertha White
Bertha White
Follow
Oct 14
Best SDLC Model for Mobile App Development
#
mobileapptesting
#
apptesting
#
sdlc
Comments
Add Comment
7 min read
HeadSpin Recognized as Appium's First “Development Partner”
Bertha White
Bertha White
Bertha White
Follow
Oct 8
HeadSpin Recognized as Appium's First “Development Partner”
#
developmentpartner
#
headspin
#
appium
Comments
Add Comment
3 min read
What is Cypress Testing - A Complete Guide
Bertha White
Bertha White
Bertha White
Follow
Oct 1
What is Cypress Testing - A Complete Guide
#
cypresstesting
#
selenium
#
testautomation
#
softwaretesting
2
reactions
Comments
Add Comment
6 min read
Why Test Early in the Software Development Lifecycle
Bertha White
Bertha White
Bertha White
Follow
Sep 30
Why Test Early in the Software Development Lifecycle
#
softwaredevelopmentlifecycle
#
testing
#
shiftlefttesting
#
headspin
Comments
Add Comment
8 min read
Essential Steps for Effective Usability Testing of Banking Applications
Bertha White
Bertha White
Bertha White
Follow
Sep 27
Essential Steps for Effective Usability Testing of Banking Applications
#
usabilitytesting
#
bankingapplication
#
bankingapptesting
#
testautomation
Comments
Add Comment
6 min read
Inspect Element Android: What You Need to Know
Bertha White
Bertha White
Bertha White
Follow
Sep 25
Inspect Element Android: What You Need to Know
#
inspectelement
#
androidtesting
#
mobileapptesting
Comments
Add Comment
7 min read
The Art of Secure Test Data Management: Proven Best Practices
Bertha White
Bertha White
Bertha White
Follow
Sep 23
The Art of Secure Test Data Management: Proven Best Practices
#
testdatamanagement
#
testautomation
#
applicationtesting
#
softwaretesting
Comments
Add Comment
6 min read
The Potential of End-to-End Testing: Top Tools and Features
Bertha White
Bertha White
Bertha White
Follow
Sep 20
The Potential of End-to-End Testing: Top Tools and Features
#
endtoendtesting
#
endtoendtestingtools
#
testexecution
#
testautomation
Comments
Add Comment
4 min read
The Significance of Full SDLC Testing in Mobile App Development
Bertha White
Bertha White
Bertha White
Follow
Sep 19
The Significance of Full SDLC Testing in Mobile App Development
#
mobileapptesting
#
apptesting
#
mobileappdevelopment
Comments
Add Comment
6 min read
Maximizing Efficiency in an On-Premises Testing Lab: How?
Bertha White
Bertha White
Bertha White
Follow
Sep 16
Maximizing Efficiency in an On-Premises Testing Lab: How?
#
onpremise
#
testinglab
#
softwaretesting
#
onpremisetestinglab
Comments
Add Comment
7 min read
Testing Your Web Apps on the Right Devices: A Comprehensive Guide
Bertha White
Bertha White
Bertha White
Follow
Sep 10
Testing Your Web Apps on the Right Devices: A Comprehensive Guide
#
webapptesting
#
apptesting
#
webtesting
Comments
Add Comment
6 min read
Top Game Testing Tools Improving Game Performance
Bertha White
Bertha White
Bertha White
Follow
Sep 10
Top Game Testing Tools Improving Game Performance
#
automationtesting
#
gameperformance
#
gametesting
#
gametestingtools
Comments
Add Comment
5 min read
What is Appium Inspector? How to Use it? (Benefits and Use Cases)
Bertha White
Bertha White
Bertha White
Follow
Sep 9
What is Appium Inspector? How to Use it? (Benefits and Use Cases)
#
appium
#
appiyminspector
#
mobileautomationtesting
#
automationtesting
Comments
Add Comment
5 min read
How to Inspect Element on Mac: A Comprehensive Guide
Bertha White
Bertha White
Bertha White
Follow
Aug 20
How to Inspect Element on Mac: A Comprehensive Guide
#
inspectelement
#
headspin
#
performancemonitoring
#
softwaretesting
Comments
Add Comment
7 min read
Front-end Development With Acceptance Test-Driven Development
Bertha White
Bertha White
Bertha White
Follow
Aug 14
Front-end Development With Acceptance Test-Driven Development
#
agilesoftwaretesting
#
frontend
Comments
Add Comment
4 min read
5 Popular Automation Tools Used to Test React Native Apps
Bertha White
Bertha White
Bertha White
Follow
Aug 13
5 Popular Automation Tools Used to Test React Native Apps
#
reactnativeapps
#
automationtools
#
automationtesting
1
reaction
Comments
Add Comment
9 min read
Network Performance Testing: A Comprehensive Guide
Bertha White
Bertha White
Bertha White
Follow
Aug 12
Network Performance Testing: A Comprehensive Guide
#
performancetesting
#
networktesting
#
networkperformancetesting
2
reactions
Comments
Add Comment
8 min read
Top Software Testing Trends: What to Expect
Bertha White
Bertha White
Bertha White
Follow
Aug 9
Top Software Testing Trends: What to Expect
#
softwaretesting
#
testingtrends
#
testing
1
reaction
Comments
Add Comment
6 min read
Can AI Video Game Testing Boost Quality Assurance?
Bertha White
Bertha White
Bertha White
Follow
Aug 2
Can AI Video Game Testing Boost Quality Assurance?
#
videogametesting
#
qualityassurance
#
softwaretesting
#
gametesting
Comments
Add Comment
6 min read
How to Use Generative AI for Exploratory Testing
Bertha White
Bertha White
Bertha White
Follow
Jul 31
How to Use Generative AI for Exploratory Testing
#
generativeai
#
exploratorytesting
#
chatgpt
Comments
Add Comment
7 min read
Cross Browser Compatibility Testing - Know all about
Bertha White
Bertha White
Bertha White
Follow
Jul 30
Cross Browser Compatibility Testing - Know all about
#
testautomation
#
crossbrowsertesting
#
browsercompatibilitytesting
#
crossbrowsercompatibility
2
reactions
Comments
Add Comment
8 min read
Top Test Automation Tools for Salesforce
Bertha White
Bertha White
Bertha White
Follow
Jul 29
Top Test Automation Tools for Salesforce
#
testautomation
#
automationtools
#
salesforce
#
testautomationtools
Comments
Add Comment
8 min read
Mastering Audio/Video Testing on Authentic Devices: Ensuring Seamless Performance
Bertha White
Bertha White
Bertha White
Follow
Jul 24
Mastering Audio/Video Testing on Authentic Devices: Ensuring Seamless Performance
#
audiovideotesting
#
audiovisualtesting
#
performancetesting
Comments
Add Comment
5 min read
Guide to Automated Testing Tools for Web Apps
Bertha White
Bertha White
Bertha White
Follow
Jul 23
Guide to Automated Testing Tools for Web Apps
#
webapptesting
#
automatedtestingtools
#
automatedtesting
Comments
Add Comment
7 min read
9 Essential Considerations for Performance testing tools
Bertha White
Bertha White
Bertha White
Follow
Jul 19
9 Essential Considerations for Performance testing tools
#
performancetesting
#
performancetestingtools
Comments
Add Comment
4 min read
Salesforce Regression Testing: Definition, Benefits, and Best Practices
Bertha White
Bertha White
Bertha White
Follow
Jul 18
Salesforce Regression Testing: Definition, Benefits, and Best Practices
#
salesforce
#
regressiontesting
#
salesforceregressiontesting
Comments
Add Comment
6 min read
How to Use Appium for Fire TV App Test Automation
Bertha White
Bertha White
Bertha White
Follow
Jul 16
How to Use Appium for Fire TV App Test Automation
#
appium
#
testautomation
#
firetvapp
#
automationtesting
Comments
Add Comment
7 min read
An Ultimate Guide to Android App Testing and Debugging
Bertha White
Bertha White
Bertha White
Follow
Jul 15
An Ultimate Guide to Android App Testing and Debugging
#
androidapptesting
#
apptesting
#
applicationtesting
Comments
Add Comment
8 min read
Guide to Chrome Remote Debugging
Bertha White
Bertha White
Bertha White
Follow
Jul 10
Guide to Chrome Remote Debugging
#
chromeremotedebugging
#
remotedebugging
#
webapptesting
5
reactions
Comments
Add Comment
7 min read
Mastering Automation Testing: Low Code vs No Code
Bertha White
Bertha White
Bertha White
Follow
Jul 9
Mastering Automation Testing: Low Code vs No Code
#
automationtesting
#
lowcodetesting
#
nocodetesting
#
lowcodeautomation
Comments
Add Comment
5 min read
Top 10 QA Automation Tools: A Comprehensive Guide
Bertha White
Bertha White
Bertha White
Follow
Jul 8
Top 10 QA Automation Tools: A Comprehensive Guide
#
qaautomation
#
automationtools
#
automationtesting
1
reaction
Comments
Add Comment
6 min read
Why HeadSpin Is The Best BrowserStack Alternative
Bertha White
Bertha White
Bertha White
Follow
Jul 2
Why HeadSpin Is The Best BrowserStack Alternative
#
headspin
#
testingplatform
#
testingsolution
#
automatedtesting
Comments
Add Comment
5 min read
Mastering the HS Tunnel Tool Guide
Bertha White
Bertha White
Bertha White
Follow
Jun 28
Mastering the HS Tunnel Tool Guide
#
hstunneltool
#
headspintunnel
#
testautomation
#
automationtesting
Comments
Add Comment
6 min read
Mastering Enterprise Test Automation: Key Strategies for Enhancing Digital Quality
Bertha White
Bertha White
Bertha White
Follow
Jun 24
Mastering Enterprise Test Automation: Key Strategies for Enhancing Digital Quality
Comments
Add Comment
5 min read
A Deep Dive into End-to-End Testing Frameworks
Bertha White
Bertha White
Bertha White
Follow
Jun 10
A Deep Dive into End-to-End Testing Frameworks
#
endtoendtesting
#
testingframeworks
#
testautomatio
1
reaction
Comments
Add Comment
4 min read
Mastering Test Case Prioritization for Regression Testing
Bertha White
Bertha White
Bertha White
Follow
Jun 4
Mastering Test Case Prioritization for Regression Testing
#
regressiontesting
#
softwaretesting
#
testautomation
#
mobileapptesting
Comments
Add Comment
4 min read
Exploring the Depths of Functional and Non-Functional Testing
Bertha White
Bertha White
Bertha White
Follow
May 27
Exploring the Depths of Functional and Non-Functional Testing
#
functionaltesting
#
nonfunctionaltesting
#
testautomation
#
automatedtesting
Comments
Add Comment
4 min read
Future Trends of Web Application Testing
Bertha White
Bertha White
Bertha White
Follow
May 17
Future Trends of Web Application Testing
#
webapplicationtesting
#
webapps
#
applicationtesting
#
softwaretesting
Comments
Add Comment
6 min read
Unlocking Software Excellence: A Guide to Effective Functional Testing
Bertha White
Bertha White
Bertha White
Follow
May 9
Unlocking Software Excellence: A Guide to Effective Functional Testing
#
functionaltesting
#
softwaretesting
#
testautomation
#
automatedtesting
Comments
Add Comment
5 min read
Comprehensive Guide to XCUITest Framework for Beginners
Bertha White
Bertha White
Bertha White
Follow
Apr 30
Comprehensive Guide to XCUITest Framework for Beginners
#
xcuitest
#
testframework
#
uitesting
#
automationtesting
2
reactions
Comments
Add Comment
9 min read
Elevating Game Performance: Comprehensive Guide to Unity Game Testing
Bertha White
Bertha White
Bertha White
Follow
Apr 29
Elevating Game Performance: Comprehensive Guide to Unity Game Testing
#
gametesting
#
unittesting
#
unitygametesting
#
gametestingtools
Comments
1
comment
7 min read
Mastering the Art of Codeless Test Automation: Strategies for Seamless Software Quality Assurance
Bertha White
Bertha White
Bertha White
Follow
Apr 17
Mastering the Art of Codeless Test Automation: Strategies for Seamless Software Quality Assurance
#
codelesstestautomation
#
testautomation
#
functionaltestin
Comments
Add Comment
5 min read
Improving UX in Retail Through Cognitive Automation Testing
Bertha White
Bertha White
Bertha White
Follow
Apr 10
Improving UX in Retail Through Cognitive Automation Testing
#
automationtesting
#
retailapptesting
#
retailapps
#
apptesting
Comments
Add Comment
6 min read
The Future of Testing: Navigating Codeless Automation Tools for Efficiency
Bertha White
Bertha White
Bertha White
Follow
Mar 26
The Future of Testing: Navigating Codeless Automation Tools for Efficiency
#
automationtools
#
codeless
#
automationtesting
Comments
Add Comment
4 min read
Low Code vs. No Code Test Automation: Comparative Analysis
Bertha White
Bertha White
Bertha White
Follow
Mar 18
Low Code vs. No Code Test Automation: Comparative Analysis
#
lowcodeautomation
#
nocodeautomation
#
testinganalysis
Comments
Add Comment
8 min read
Boosting Digital Performance with End-user Experience Testing
Bertha White
Bertha White
Bertha White
Follow
Mar 14
Boosting Digital Performance with End-user Experience Testing
#
digitalperformance
#
appperformance
#
experiencetesting
Comments
Add Comment
7 min read
Physical and Real Device Cloud Testing Guide
Bertha White
Bertha White
Bertha White
Follow
Mar 7
Physical and Real Device Cloud Testing Guide
#
realdevicecloud
#
realdevicetesting
#
cloudtesting
Comments
1
comment
7 min read
Decoding the Software Testing Life Cycle (STLC):Phases, Benefits, and Strategic Importance
Bertha White
Bertha White
Bertha White
Follow
Feb 2
Decoding the Software Testing Life Cycle (STLC):Phases, Benefits, and Strategic Importance
#
softwaretestinglifecycle
#
softwaretesting
#
testinglifecycle
Comments
Add Comment
4 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
What is Performance Testing?Types of Performance Testing - DEV Community,"What is Performance Testing?Types of Performance Testing - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Testscenario
Posted on
Jun 6
What is Performance Testing?Types of Performance Testing
#
testing
Performance testing is a crucial aspect of the software development lifecycle, aimed at ensuring that applications function correctly under expected workloads. It involves evaluating various performance metrics such as speed, responsiveness, stability, and scalability to determine how a system behaves under different conditions. This article explores what performance testing is, its significance, and the various types of performance testing used to ensure optimal software performance.
Understanding Performance Testing
Definition
Performance testing is a non-functional testing technique performed to determine how a system performs in terms of responsiveness and stability under a particular workload. It helps identify and eliminate performance bottlenecks in the software application.
Importance of Performance Testing
Ensures Stability: Verifies that the application remains stable under varying loads.
Enhances User Experience: Ensures that users have a smooth and responsive experience.
Identifies Bottlenecks: Helps in identifying performance issues and bottlenecks that can affect the user experience.
Validates Scalability: Ensures the application can scale to meet the demands of increasing users and data volume.
Improves Optimization: Assists in optimizing resource usage, such as CPU, memory, and network bandwidth.
Types of Performance Testing
There are several types of performance testing, each serving a specific purpose in ensuring the software performs optimally under various conditions.
Load Testing
Overview
Load testing involves testing the system's performance under expected load conditions. The primary goal is to identify performance bottlenecks before the software application goes live.
Key Aspects
Simulated User Load: Tests how the application behaves under a specific number of users.
Response Time: Measures the time taken for the application to respond under load.
Throughput: Evaluates the amount of data processed by the application in a given time frame.
Use Cases
E-commerce Websites: Ensuring the website can handle high traffic during peak shopping seasons.
Online Services: Testing online services like banking and booking systems to handle a large number of transactions simultaneously.
Stress Testing
Overview
Stress testing involves testing the system beyond its normal operational capacity to determine its breaking point. The goal is to identify how the system behaves under extreme conditions and to ensure it fails gracefully.
Key Aspects
Extreme Conditions: Tests the system under extreme user loads, data volumes, or resource constraints.
Failure Points: Identifies the point at which the system fails or degrades in performance.
Recovery: Evaluates how well the system recovers after failure.
Use Cases
Financial Systems: Ensuring banking applications can handle extreme transaction loads during financial crises.
Critical Applications: Testing critical applications like healthcare systems to ensure they perform under extreme conditions.
Endurance Testing (Soak Testing)
Overview
Endurance testing, also known as soak testing, involves testing the system over an extended period to identify performance issues that may arise from prolonged usage.
Key Aspects
Long-term Performance: Evaluates the system’s performance over an extended period.
Resource Leaks: Identifies memory leaks and other resource depletion issues.
Stability: Ensures the system remains stable over time.
Use Cases
Streaming Services: Testing streaming services like Netflix to ensure continuous performance over long viewing sessions.
Enterprise Applications: Evaluating enterprise applications that are used continuously over long periods.
Spike Testing
Overview
Spike testing involves testing the system’s performance under sudden and extreme changes in load. The goal is to determine how the system handles unexpected spikes in user load.
Key Aspects
Sudden Load Changes: Tests the system’s response to sudden, extreme increases in load.
System Behavior: Evaluates the system’s ability to handle and recover from sudden spikes.
Use Cases
Ticket Booking Systems: Ensuring ticket booking systems can handle sudden spikes in traffic during popular events.
Social Media Platforms: Testing social media platforms to handle viral content that generates sudden traffic spikes.
Volume Testing
Overview
Volume testing involves testing the system with a large volume of data to determine its performance and behavior. The primary goal is to identify any issues related to data handling and processing.
Key Aspects
Data Volume: Evaluates the system’s performance with a large volume of data.
Data Integrity: Ensures data integrity and accuracy under high data volumes.
Throughput: Measures the system’s ability to process large data volumes efficiently.
Use Cases
Big Data Applications: Testing big data applications to handle and process large datasets.
Database Systems: Evaluating database systems for performance with large data volumes.
Scalability Testing
Overview
Scalability testing involves testing the system’s ability to scale up or down to meet changing user loads. The goal is to ensure the application can handle growth without compromising performance.
Key Aspects
Horizontal Scaling: Evaluates the system’s ability to scale horizontally by adding more nodes.
Vertical Scaling: Tests the system’s performance when scaled vertically by adding more resources to existing nodes.
Performance Metrics: Measures key performance metrics such as response time, throughput, and resource usage under scaled conditions.
Use Cases
Cloud Applications: Testing cloud-based applications to ensure they can scale to meet increasing user demand.
Distributed Systems: Evaluating distributed systems for scalability and performance.
Configuration Testing
Overview
Configuration testing involves testing the system’s performance under various configuration settings. The goal is to determine the optimal configuration for the best performance.
Key Aspects
Configuration Settings: Evaluates different configuration settings and their impact on performance.
Optimal Configuration: Identifies the optimal configuration for maximum performance.
Use Cases
Web Servers: Testing web server configurations to determine the best settings for performance.
Database Systems: Evaluating database configurations for optimal performance.
Best Practices for Performance Testing
To ensure effective performance testing, it is essential to follow best practices that cover planning, execution, and analysis.
Define Clear Objectives
Specific Goals: Define specific performance goals and objectives.
Key Metrics: Identify key performance metrics such as response time, throughput, and resource usage.
Create a Realistic Test Environment
Production-like Environment: Create a test environment that closely resembles the production environment.
Resource Allocation: Ensure adequate resources are allocated for performance testing.
Use Appropriate Tools
Performance Testing Tools: Utilize appropriate tools such as JMeter, LoadRunner, and Gatling.
Monitoring Tools: Use monitoring tools to track performance metrics during testing.
Develop Detailed Test Plans
Test Scenarios: Develop detailed test scenarios covering various load conditions.
Test Data: Prepare realistic test data to simulate actual usage.
Execute Tests Thoroughly
Multiple Runs: Execute performance tests multiple times to ensure consistent results.
Monitor Metrics: Continuously monitor performance metrics during test execution.
Analyze Results
Identify Bottlenecks: Analyze test results to identify performance bottlenecks.
Optimize Performance: Implement optimizations based on test results to improve performance.
Continuous Improvement
Iterative Testing: Conduct performance testing iteratively to continuously improve performance.
Update Tests: Regularly update performance tests to reflect changes in the application.
Tools for Performance Testing
Several tools are available for performance testing, each offering unique features and capabilities.
JMeter
Features: Open-source tool, supports load and performance testing, extensive reporting.
Use Cases: Load testing, stress testing, spike testing.
LoadRunner
Features: Comprehensive performance testing tool, supports a wide range of protocols, detailed analysis.
Use Cases: Load testing, endurance testing, scalability testing.
Gatling
Features: Open-source tool, high-performance testing, real-time monitoring.
Use Cases: Load testing, stress testing, performance testing.
Neoload
Features: Continuous testing, integrates with CI/CD pipelines, detailed analysis.
Use Cases: Load testing, scalability testing, performance testing.
BlazeMeter
Features: Cloud-based testing, integrates with JMeter, real-time reporting.
Use Cases: Load testing, stress testing, performance testing.
AppDynamics
Features: Application performance monitoring, real-time visibility, detailed diagnostics.
Use Cases: Performance monitoring, bottleneck identification, optimization.
Dynatrace
Features: Full-stack monitoring, AI-powered insights, automatic root cause analysis.
Use Cases: Performance monitoring, anomaly detection, optimization.
Conclusion
Performance testing is an essential aspect of software development, ensuring that applications are stable, responsive, and scalable under various conditions. By understanding the different types of performance testing and following best practices, organizations can identify and resolve performance issues before they impact users. Utilizing the right tools and continuously improving the testing process will help deliver high-quality applications that meet user expectations and perform optimally in production environments.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Part 4 - Using Cursor and Claude to Create Automated Tests with Playwright
Joe Giglio, Chief Remote Officer -
Nov 5
Raku Fall Issue Cleanup
Elizabeth Mattijsen -
Oct 23
TVP - Thinnest Viable Platform
Paradith -
Oct 21
Beyond Traditional Testing: Addressing the Challenges of Non-Deterministic Software
Danilo Poccia -
Oct 15
Testscenario
Follow
Joined
Mar 14, 2024
More from
Testscenario
What is Regression Testing?
#
testing
The Ultimate Guide to Android App Testing: Ensuring Quality with Testscenario
#
testing
Web Application Testing Tools
#
testing
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Tinybird - DEV Community,"Tinybird - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Follow
Organization actions
Tinybird
Tinybird is a serverless analytical backend for developers. Build low-latency APIs in minutes with nothing but SQL.
Joined
Joined on
Sep 20, 2022
Twitter logo
External link icon
Meet the team
Post
28 posts published
Member
3 members
How to choose the right type of database
Cameron Archer
Cameron Archer
Cameron Archer
Follow
Feb 28
How to choose the right type of database
#
database
#
beginners
#
learning
2
reactions
Comments
Add Comment
13 min read
Build a real-time dashboard with Next.js, Tremor, and Tinybird
Cameron Archer
Cameron Archer
Cameron Archer
Follow
Sep 6 '23
Build a real-time dashboard with Next.js, Tremor, and Tinybird
#
react
#
nextjs
#
javascript
#
database
6
reactions
Comments
Add Comment
24 min read
Announcing the ""St. Albnas"" Hackathon! Clean some data, win $20 of swag.
Cameron Archer
Cameron Archer
Cameron Archer
Follow
Jul 13 '23
Announcing the ""St. Albnas"" Hackathon! Clean some data, win $20 of swag.
#
showdev
#
programming
#
github
#
ai
Comments
Add Comment
1 min read
Designing and implementing a weather data API
Cameron Archer
Cameron Archer
Cameron Archer
Follow
Jun 20 '23
Designing and implementing a weather data API
#
api
#
database
#
tutorial
#
webdev
2
reactions
Comments
Add Comment
16 min read
5 criteria for data quality and how to test for them
Cameron Archer
Cameron Archer
Cameron Archer
Follow
Jun 12 '23
5 criteria for data quality and how to test for them
#
database
#
sql
#
datascience
#
testing
Comments
Add Comment
6 min read
A FOSS mock data generator for your next data project
Cameron Archer
Cameron Archer
Cameron Archer
Follow
May 17 '23
A FOSS mock data generator for your next data project
#
data
#
analytics
#
productivity
#
opensource
Comments
Add Comment
6 min read
The 8 considerations for designing public data APIs
Cameron Archer
Cameron Archer
Cameron Archer
Follow
May 15 '23
The 8 considerations for designing public data APIs
#
api
#
database
#
webdev
#
programming
Comments
Add Comment
10 min read
The 5 rules for writing faster SQL
Cameron Archer
Cameron Archer
Cameron Archer
Follow
May 8 '23
The 5 rules for writing faster SQL
#
sql
#
database
#
analytics
#
backend
3
reactions
Comments
1
comment
8 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Test Data Generator: Enhancing Software Testing Efficiency - DEV Community,"Test Data Generator: Enhancing Software Testing Efficiency - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
keploy
Posted on
Nov 9
Test Data Generator: Enhancing Software Testing Efficiency
#
webdev
#
javascript
#
programming
#
tutorial
In software testing, the accuracy and consistency of test data are crucial for reliable results. However, manually creating and managing test data is time-consuming and often error-prone, especially for large applications. This is where test data generators come in handy.
Test data generator
automatically create data needed for testing, saving time, reducing manual errors, and ensuring the testing process is as seamless as possible. This post explores what test data generator are, why they're essential, and how to integrate them into your testing pipeline.
What is a Test Data Generator?
A test data generator is a tool or process that automates the creation of data for software testing. Instead of developers spending hours manually creating datasets, a test data generator can produce data with specific attributes or in a specific format, allowing for quick and consistent data creation. Test data generators are particularly valuable for testing scenarios that need large datasets, randomized inputs, or data with specific patterns, making it easier to isolate variables and test features without depending on live data.
Types of Test Data Generators
Test data generators vary in functionality, with different types suited for specific testing needs and environments:
• Random Data Generators: These create random values for data fields, ideal for testing scenarios where variations are key, such as load testing.
• Static Data Generators: Generate consistent, repeatable data, often used for regression testing where stable inputs are needed.
• Format-Specific Generators: Some tools specialize in generating specific formats like JSON, XML, or CSV, making them useful for integration tests and API testing.
Each of these types serves unique purposes, ensuring that the generated data aligns with the testing requirements, ultimately improving the relevance and effectiveness of each test case.
Why Test Data Generators are Essential for Modern Testing
With the increasing complexity of applications, test data generation has become crucial to maintain testing efficiency and accuracy. Automated data generation speeds up the testing process, allowing teams to handle complex test cases and variations without additional manual input. For example, performance testing requires large volumes of data to simulate real-world scenarios. Manually creating this data is impractical, but a data generator can easily handle it. Test data generators also contribute to accurate, repeatable tests that improve code quality and help developers quickly identify issues.
Features to Look for in a Test Data Generator
Not all test data generators are created equal, and selecting the right one involves evaluating several features:
• Data Customization: The tool should allow customization to create data that aligns with the test case requirements, such as specific patterns or range restrictions.
• Support for Complex Data Structures: For advanced tests, the generator should handle complex data, such as nested structures, arrays, and cross-referenced data fields.
• Scalability: As applications grow, test data needs will increase. A robust test data generator should scale to handle large datasets without performance issues.
• Data Privacy and Compliance: For applications that handle sensitive data, it’s essential that the generator supports anonymized or synthetic data that adheres to privacy regulations.
Evaluating these features helps ensure the chosen test data generator meets your testing needs while ensuring efficiency and compliance.
Implementing Test Data Generators in Your Testing Pipeline
Integrating a test data generator into your testing pipeline can streamline workflows and reduce the manual effort required for preparing test data. Here’s how to implement it effectively:
Identify Testing Needs: Determine where automated data would benefit your testing process, such as in unit, integration, or performance tests.
Choose a Tool: Select a test data generator based on your project’s needs and the features outlined earlier.
Configure Data Specifications: Set up the generator to produce data that meets the criteria for each test scenario.
Integrate with Your CI/CD Pipeline: For continuous testing, configure the generator to create fresh data with each test run.
Following these steps will help you integrate test data generators into your testing pipeline, enhancing both efficiency and accuracy in your tests.
Popular Test Data Generator Tools
There are several powerful tools available to help generate realistic and varied test data:
• Faker: A popular library in various languages (JavaScript, Python, PHP) for generating random data like names, addresses, and dates.
• Mockaroo: An online tool for generating large datasets, supporting multiple formats like JSON, CSV, and SQL.
• JFairy: A Java-based library that generates real-world data, useful for scenarios requiring natural-looking data sets.
Each of these tools has unique strengths, allowing users to choose the best fit for their testing requirements and simplify the process of creating robust test datasets.
Challenges in Using Test Data Generators and How to Overcome Them
Despite their advantages, test data generators can present challenges:
• Data Relevance: Randomized data may not always reflect real-world conditions accurately, leading to irrelevant test results.
o   Solution: Configure the generator to align closely with production data, ensuring tests are realistic.
• Integration Complexity: Integrating some data generators into existing testing frameworks or data sources can be challenging.
o   Solution: Choose tools that offer API support or native integrations to simplify data loading and usage.
By addressing these issues, teams can maximize the effectiveness of test data generators and ensure tests are as accurate as possible.
Best Practices for Effective Test Data Generation
To get the most out of test data generators, follow these best practices:
• Manage Data Volume: Avoid excessive data generation to keep tests manageable and efficient.
• Select Relevant Data Sets: Use data that reflects realistic scenarios to improve test reliability.
• Ensure Consistency Across Tests: Keep data consistent for tests that require repeatable results, especially for regression testing.
• Prioritize Data Privacy: Use anonymized data in test environments, especially for applications with sensitive information, to comply with regulations.
These best practices help improve test data quality, ensuring that generated data is both relevant and compliant with industry standards.
Future Trends in Test Data Generation
As testing continues to evolve, new trends in test data generation are emerging. AI-driven test data generators are becoming more prevalent, creating sophisticated, realistic data that can adapt based on testing patterns and needs. Additionally, intelligent data sampling and predictive data generation are on the rise, allowing data generation tools to understand and anticipate data requirements, further automating and optimizing the testing process. These trends show promising advancements in how test data is generated, making it even easier to ensure high-quality testing in the future.
Conclusion
Test data generators have become essential tools for efficient and scalable software testing. They save time, reduce errors, and allow testers to focus on creating quality test cases rather than generating data manually. By automating data creation and following best practices, teams can improve test coverage, accuracy, and consistency, ultimately leading to more reliable software. As test data generation continues to evolve, incorporating AI and intelligent sampling, testers will have even more tools at their disposal to ensure comprehensive, high-quality testing.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Writing APIs Without Servant vs. Using Servant
Suraj Vatsya -
Dec 8
Horóscopo com IA: Uma Experiência com Next.js e Gemini
Frank William -
Dec 8
Day 12: Pico-co-colors 🐥
Valeria -
Dec 12
JavaScript vs. Deno, Vite 6, Astro 5, and more
James -
Dec 8
keploy
Follow
Keploy is an AI-powered testing tool that specializes in creating test cases and generating stubs/mocks for end-to-end testing. It can achieve an impressive 90% test coverage in just a min.
Joined
Sep 23, 2023
More from
keploy
React Testing: A Comprehensive Guide
#
webdev
#
javascript
#
beginners
#
tutorial
Webhooks vs APIs: Understanding the Differences
#
webdev
#
javascript
#
beginners
#
tutorial
GraphQL vs REST: A Comprehensive Comparison
#
javascript
#
programming
#
beginners
#
tutorial
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Introduction to Big Data Analysis - DEV Community,"Introduction to Big Data Analysis - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Madhav Ganesan
Posted on
Nov 17
Introduction to Big Data Analysis
#
bigdata
#
aws
#
hadoop
#
coding
Data
refers to raw, unprocessed facts, statistics, or information collected for reference, analysis, and processing.
They are of different formats:
Structured Data:
Organized in a defined format, such as rows and columns in a database
Unstructured Data:
No predefined structure, such as text, emails, images, videos, and social media posts.
(
Note:
If 20 percent of the data available to enterprises is structured data, the other 80 percent is unstructured.)
Semi-Structured Data:
A hybrid of structured and unstructured data, like JSON or XML documents that have tags or markers to organize the data but don’t fit into a traditional database schema.
Sources of structural data
Machine-Generated Structured Data:
Sensor Data
Web Log Data
Point-of-Sale Data
Financial Data
Human-Generated Structured Data:
Input Data
Click-Stream Data
Gaming-Related Data
Sources of Unstructured data
Machine-Generated Unstructured Data:
Satellite Images
Scientific Data
Photographs and Video
Radar or Sonar Data
Human-Generated Unstructured Data:
Text Internal to Your Company
Social Media Data
Mobile Data
Website Content
Bigdata
It refers to large and complex datasets that are difficult to process, analyze, and manage using traditional data processing tools and techniques.
Key Characteristics of Big Data
1) Volume:
This refers to the size/magnitude of the data, typically in the range of terabytes, petabytes, or even exabytes.
Limitations of Traditional RDBMS:
Traditional RDBMS are designed primarily for structured data organized in tables. They struggle with unstructured or semi-structured data
RDBMS have limitations in scaling out horizontally (adding more servers) to handle increased data volume. They often rely on vertical scaling, which is not always feasible for big data scenarios.
IoT devices and sensors generate continuous data streams that traditional databases are not equipped to handle efficiently.
2) Velocity:
Big Data is generated at high speed and often requires real-time or near-real-time processing to derive meaningful insights.
3) Variety:
This refers to different types and sources of data that need to be integrated and analyzed
Advantages:
Competitive Advantage
Enhanced Analytics
Ex. Structured, Unstructured and Semi-structured data
4) Veracity:
(4th V) (Coined by IBM)
It refers to the reliability, accuracy, and truthfulness of data.
Data must be accurate and reliable to make informed decisions. False or misleading data can lead to incorrect conclusions and misguided actions.
Ensure that data is complete, accurate, and timely.
High-quality data leads to more reliable analysis outcomes.
Implement processes to verify data accuracy and consistency.
Assess the reliability of data sources. Data from reputable and validated sources is generally more trustworthy.
The data must be relevant and contextually appropriate for the specific problem
5) Variability:
It refers to the fluctuations in the data flow rates and the changing patterns of data over time. It highlights the inconsistency in how data is generated, stored, and processed.
*
6) Visualization: *
It is a crucial aspect of big data that involves converting complex data into graphical formats such as graphs, charts, and maps. This transformation allows for easier comprehension and actionable insights from large and complex datasets.
Ex.
Nanocubes is a visualization tool developed by AT&T that supports high-dimensional data visualization.
7) Value:
This refers to the usefulness of the data
Examples of Big data:
Social Media:
Posts, tweets, comments, likes, shares, and multimedia content.
Internet of Things (IoT):
Data from sensors, devices, wearables, and smart systems.
E-commerce:
Customer behavior, transaction logs, and purchase history.
Healthcare:
Medical records, imaging data, and genomic data.
Finance:
Stock market data, transaction data, and financial records.
Web Analytics:
Clickstreams, browsing history, and user interactions on websites.
Data warehouse
It is a centralized repository that stores large volumes of structured and sometimes semi-structured data from various sources
Key Characteristics of Data Warehouse
Subject-Oriented:
It is organized around key business subjects such as sales, finance, inventory, etc.
Integrated:
Data from different sources (e.g., transactional systems, databases, flat files, cloud applications) is cleaned, transformed, and integrated into a unified format
Non-Volatile:
Once data is loaded into the data warehouse, it is not modified or deleted. This ensures historical data is preserved for long-term analysis.
Time-Variant:
Data in a data warehouse is stored with time stamps, allowing users to track and analyze trends over time.
Data Mart
It is a specialized subset of a data warehouse that is designed to focus on a specific business department, such as sales, marketing, finance, or operations.
Tools for building data warehouse
Amazon Redshift (AWS)
Google BigQuery
Snowflake
Apache Hive
Microsoft Azure Synapse Analytics
IBM Db2 Warehouse on Cloud
Snapshots of data
A snapshot is a static view of the data at a particular point in time. Organizations might store snapshots to preserve a record of important information without keeping a continuously updated dataset.
Big data management cycle
Big data architecture
Layer 0:
Redundant Physical Infrastructure
An infrastructure should be resilient to failure or changes when sufficient redundant resources are in-place, ready to jump into action.
""elastic"" refers to the ability of a system or infrastructure to dynamically scale its resources up or down in response to varying demands. Elasticity ensures that the system can handle fluctuations in workload efficiently without significant performance degradation or wasted resources.
It utilizes multiple machines to handle large-scale data processing tasks. Redundancy helps in managing failures and scaling operations efficiently.
Layer 1: Security Infrastructure
Security infrastructure is vital to protect sensitive data and ensure compliance with regulations. Security measures should be integrated into the architecture from the beginning, including encryption, access control, and monitoring.
It should adhere to industry standards and regulations (e.g., GDPR, HIPAA) is crucial for data security and privacy.
Layer 2: Operational Databases
Database Engines store and manage data collections relevant to your business. It must be fast, scalable, and reliable. Different engines may be more suitable depending on the big data environment, often requiring a mix of database technologies.
Structured Data: Relational Database
Unstructured Data: No SQL Database
Layer 3: Organizing Data Services and Tools
Addressing challenges of handling large volumes of unstructured data and ensuring continuous data capture without relying on snapshots.
Key Characteristics of Virtualization for Big Data
Partitioning
For big data environments, we can run multiple applications or instances of data processing frameworks (like Hadoop) on the same physical hardware
Isolation
Each virtual machine (VM) is isolated from others and from the host system. This means that if one VM fails or encounters issues, it doesn’t impact others.
Encapsulation
A VM can be encapsulated into a single file or set of files, which includes the virtual machine’s operating system, applications, and data.
Virtualization
It is the process of creating a simulated version of something rather than a physical version. This can include hardware platforms, storage devices, and network resources.
Virtualization allows multiple virtual systems to run on a single physical machine by separating the virtual environments from the underlying hardware
Imagine a small company with only one physical server. The company needs to run several different applications, such as a web server, a database server, and an email server. Instead of buying multiple physical servers for each application, they use virtualization to run multiple virtual servers on a single physical server.
How It Works:
Physical Server: The company has one physical server with sufficient hardware resources (CPU, RAM, disk space).
Virtualization Software: The company installs virtualization software (such as VMware, Hyper-V, or VirtualBox) on this physical server.
Creating Virtual Machines (VMs):
VM 1: Runs the web server application.
VM 2: Runs the database server application.
VM 3: Runs the email server application.
Each VM operates as if it were a separate physical server, with its own operating system and applications.
Resource Allocation: The virtualization software allocates the physical server’s resources (CPU, RAM, disk space) to each VM as needed. For instance, if the web server needs more resources, the virtualization software can allocate additional resources from the pool.
Applications
Server Consolidation
Software Virtualization
Storage Virtualization
Network Virtualization
Hypervisor (virtual machine monitor)
It is a software layer that allows multiple virtual machines (VMs) to run on a single physical host machine by managing and allocating hardware resources among them. It sits between the physical hardware and the virtual machines. It manages and allocates physical resources to each VM.
Types of Hypervisors
Type 1 Hypervisor
(Bare-Metal Hypervisor)
It runs directly on the physical hardware of the host machine, without an underlying operating system. It interacts directly with the hardware and manages the VMs directly.
Ex.
VMware ESXi, Microsoft Hyper-V, Xen, KVM (Kernel-based Virtual Machine)
Type 2 Hypervisor
(Hosted Hypervisor)
It runs on top of a host operating system, which then interacts with the physical hardware. The hypervisor relies on the host OS to manage hardware resources.
Ex.
VMware Workstation, Oracle VirtualBox, Parallels Desktop
Data Team Structure
Data Engineers
They manage the infrastructure, ensuring data pipelines, databases, and computing resources are effectively set up and maintained.
Data Modelers
They focus on analyzing data, developing models, and creating predictive / inferential products.
Subject Matter Experts (SMEs)
They provide deep knowledge about the specific domain or industry to guide data-driven decision-making.
How Software Development can be compared with Big Data Analysis?
Tools: Version control (like Git) parallels distributed file systems (HDFS) for managing data across nodes. IDEs are like big data platforms (e.g., Spark) for efficient processing.
Processes: Modularization in development is like data pipelines (e.g., Apache NiFi) for organizing data steps. Continuous integration mirrors data streaming tools (Kafka) for real-time updates.
Algorithms: Sorting/searching algorithms compare to MapReduce in big data, distributing processing across clusters. Testing in dev is similar to data cleaning for reliable results.
Use Cases: Debugging matches anomaly detection, optimizing code resembles query optimization, and developing features aligns with predictive analytics for actionable insights.
Team Structure: Cross-functional dev teams are like big data teams (engineers, analysts) working together for robust, insightful products.
Distributed File System (DFS)
A distributed file system (DFS) is a storage system that allows files to be stored across multiple servers or nodes in a way that makes them accessible as if they were on a single device.
Example Systems
Hadoop Distributed File System (HDFS), Google File System (GFS), Amazon S3
Running HDFS on laptop?
(This is only for testing purpose as this is for large scale parallel processing)
It works in a pseudo-distributed mode, which simulates a distributed environment on a single machine. All Hadoop components (NameNode, DataNode, ResourceManager, and NodeManager) run on the same machine. Laptop will act as both the master and worker node.
Why DFS?
Redundancy
: Data is replicated across multiple nodes, ensuring that if one node fails, the data is still available from another node.
Scalability
: DFS can easily scale out by adding more nodes to the cluster. This allows organizations to increase storage capacity and processing power as needed without significant changes to the existing infrastructure.
Parallel Processing
: By distributing data across multiple nodes, a DFS allows for parallel processing of data.
How file storage happens in DFS?
Scenario: Suppose we have a large video file that we want to store in a DFS.
1. Chunking the File
The video file is divided into chunks of 64 MB each. For instance, if the video file is 192 MB, it will be divided into three chunks:
Chunk 1: 64 MB
Chunk 2: 64 MB
Chunk 3: 64 MB
2. Replication of Chunks
Each chunk is replicated three times for redundancy. The replicas are distributed across different compute nodes to ensure that data remains available even if some nodes fail.
Example:
Chunk 1:
Replica 1: Node A (Rack 1)
Replica 2: Node C (Rack 2)
Replica 3: Node D (Rack 2)
Chunk 2:
Replica 1: Node B (Rack 1)
Replica 2: Node A (Rack 1)
Replica 3: Node C (Rack 2)
Chunk 3:
Replica 1: Node D (Rack 2)
Replica 2: Node B (Rack 1)
Replica 3: Node A (Rack 1)
3) Master Node (NameNode)
There is a master node (also known as NameNode) that keeps track of where each chunk is located. For our example, the NameNode maintains a mapping of:
Chunk 1: Node A, Node C, Node D
Chunk 2: Node B, Node A, Node C
Chunk 3: Node D, Node B, Node A
The NameNode itself is also replicated across different nodes for fault tolerance, ensuring that if one NameNode fails, others can take over.
MapReduce
It is a
programming model
and processing framework designed for processing and generating
large data sets
in parallel across a
distributed computing environment
. It was introduced by
Google
to handle massive amounts of unstructured data efficiently.
Core concepts of MapReduce
Map Phase:
The map function processes input data in parallel. It divides the data into smaller chunks and applies a specific function to each chunk to produce intermediate key-value pairs.
Ex. {""word"":1}
Shuffle and Sort:
After the map phase, the framework sorts and groups the intermediate key-value pairs by key.
Ex. In the word count application, all pairs with the same word key are grouped together
Reduce Phase:
The reduce function processes the grouped intermediate data. It takes each group of key-value pairs and aggregates them into a final result.
Work of master and worker nodes
Master (JobTracker)
Task Management: Coordinates the execution of MapReduce jobs, creating Map and Reduce tasks based on user input.
Task Assignment: Assigns tasks to Workers based on availability and load, monitoring their health.
Status Tracking: Keeps track of each task's status (idle, in progress, completed) and provides job progress updates.
Intermediate Data Handling: Manages the creation and flow of intermediate files between Map and Reduce tasks.
Job Scheduling: Schedules task execution, optimizing resource use and data locality.
Enter fullscreen mode
Exit fullscreen mode
Worker (TaskTracker)
Task Execution:
Executes either Map or Reduce tasks, processing data assigned by the Master.
Map Tasks: Processes input chunks, generates intermediate key-value pairs, and writes them to local disk.
Reduce Tasks: Retrieves intermediate data, applies the reduce function, and produces final output results.
Communication with Master: Reports task status to the Master and requests new tasks or notifications of completions.
Applications
Google used it for large matrix-vector multiplications for calculating PageRank.
Amazon used it for performing analytical queries to identify users with similar buying patterns.
What is Hadoop?
A comprehensive open-source framework part of Apache Software Foundation that includes both distributed storage (HDFS) and processing (MapReduce) components for managing and analyzing large-scale datasets.
Master node
NameNode:
It stores the directory tree of the file system, file metadata, and the locations of each file in the cluster.
SecondaryNameNode(Master):
It performs housekeeping tasks and checkpointing on behalf of the NameNode. (Note: Not a backup of Namenode)
ResourceManager (Master)(YARN)
It allocates and monitors available cluster resources to applications as well as handling scheduling of jobs on the cluster.
ApplicationMaster(Master)(YARN):
It coordinates a particular application being run on the cluster as scheduled by the ResourceManager.
Worker Nodes
DataNode:
It stores and manages HDFS blocks on the local disk. It also reports health and status of individual data stores back to the NameNode.
NodeManager:
It runs and manages processing tasks on an individual node as well as reports the health and status of tasks as they’re running.
Working
Data Access in HDFS
Client Request: The client application sends a request to the NameNode to locate the data.
Metadata Retrieval: The NameNode replies with a list of DataNodes that store the requested data blocks.
Direct Data Access: The client directly requests each data block from the respective DataNodes.
NameNode Role: The NameNode acts as a traffic cop, managing metadata and directing clients but does not store or transfer data.
Job Execution in YARN
Resource Request: Clients submit job requests to the ResourceManager for resource allocation.
ApplicationMaster Creation: The ResourceManager assigns an ApplicationMaster specific to the job for its duration.
Job Tracking: The ApplicationMaster tracks job execution and resource requirements.
Node Management: The ResourceManager monitors the status of the cluster nodes, while each NodeManager handles resource allocation and task execution within containers.
Task Execution: NodeManagers create containers and execute tasks as specified by the ApplicationMaster.
Components
1) Hadoop Distributed File System (HDFS)
2) MapReduce
3) YARN (Yet Another Resource Negotiator)
A cluster resource management layer for Hadoop that manages and schedules resources across the cluster.
4) Hadoop Ecosystem Components
(Apache Hive, Apache HBase, Apache Pig, Apache Spark, Apache Flume, Apache Sqoop, Apache Zookeeper)
Workflow Summary
1) Job Submission: User submits a job.
2) Task Creation: Master creates Map and Reduce tasks.
3) Task Assignment: Master assigns tasks to Workers.
4) Map Execution: Workers process data and generate intermediate results.
5) Intermediate File Creation: Workers store intermediate data locally.
6) Reduce Execution: Master assigns Reduce tasks; Workers process intermediate data.
7) Completion Notification: Workers notify the Master upon task completion.
8) Output Storage: Final results are saved to HDFS.
HDFS Blocks
HDFS files are divided into blocks, typically 64 MB or 128 MB, with high-performance systems often using 256 MB. The block size is configurable and represents the minimum data unit for read/write operations. By default, each block is replicated three times to ensure data availability in case of node failure. This replication factor is configurable at runtime.
Hadoop Streaming
It is a utility that allows developers to create and run MapReduce jobs using any executable or script as the mapper and/or reducer. This provides a way to process data in Hadoop without needing to write Java code, making it accessible to a broader range of programming languages.
hadoop jar /path/to/hadoop-streaming.jar \
  -input /path/to/input.txt \
  -output /path/to/output \
  -mapper /path/to/mapper.py \
  -reducer /path/to/reducer.py \
Enter fullscreen mode
Exit fullscreen mode
Example 1: Word Count
#!/usr/bin/env python
import
sys
if
__name__
==
""
__main__
""
:
for
line
in
sys
.
stdin
:
for
word
in
line
.
split
():
sys
.
stdout
.
write
(
""
{}
\t
i
\n
""
.
format
(
word
))
Enter fullscreen mode
Exit fullscreen mode
#!/usr/bin/env python
import
sys
if
__name__
==
'
__main__
'
:
curkey
=
None
total
=
0
for
line
in
sys
.
stdin
:
key
,
val
=
line
.
split
(
""
\t
""
)
# Corrected syntax here
val
=
int
(
val
)
if
key
==
curkey
:
total
+=
val
else
:
if
curkey
is
not
None
:
sys
.
stdout
.
write
(
""
{}
\t
{}
\n
""
.
format
(
curkey
,
total
))
curkey
=
key
total
=
val
Enter fullscreen mode
Exit fullscreen mode
Advanced MapReduce:
Job Chaining:
It allows for the execution of multiple MapReduce jobs in sequence, where the output of one job can serve as the input for the next.
Types:
1) Linear Job Chaining:
This involves a straight sequence of jobs, where each job runs after the previous one completes.
2) Data Flow Job Chaining:
This involves more complex workflows where the output of one job may feed into multiple subsequent jobs, allowing for more intricate data processing pipelines.
Ex. ETL (Extract, Transform, Load)
Combiners:
It is an optional component in the MapReduce process that acts as a mini-reducer to optimize the data flow between the Mapper and Reducer phases. It processes the output of the Mapper to reduce the amount of data transferred to the Reducer by performing local aggregation (e.g., summing values) before sending it over the network.
Partitioners:
It determines how the output from the Mapper is distributed to the Reducer tasks. It controls the partitioning of the intermediate key-value pairs into different Reducers based on the key.
Stay Connected!
If you enjoyed this post, don’t forget to follow me on social media for more updates and insights:
Twitter:
madhavganesan
Instagram:
madhavganesan
LinkedIn:
madhavganesan
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Amazon S3 Tables: A Game Changer in Analytics and Data Lake Space
Asanka Boteju -
Dec 6
🚀 Key Takeaways from Dr. Werner Vogels' Keynote at AWS re:Invent 2024 🌍
Bhavesh Gohel -
Dec 6
Streaming of Desktop Applications Securely on Web Browser Using Amazon AppStream 2.0
GargeeBhatnagar -
Nov 17
⚙️ ""Dynamic Scaling & Performance: EKS Auto Mode Insights"" 🔄
Sarvar Nadaf -
Dec 4
Madhav Ganesan
Follow
Always hungry to learn and contribute
Location
Chennai, Tamilnadu
Joined
Jun 14, 2024
More from
Madhav Ganesan
Cloud Computing
#
cloud
#
aws
#
azure
#
gcp
Introduction to Hadoop:)
#
hadoop
#
bigdata
#
nlp
#
llm
Recursion (DSA - 9)
#
cpp
#
coding
#
programming
#
leetcode
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
How to write unit tests for your SQL queries - DEV Community,"How to write unit tests for your SQL queries - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
BenBirt
for
Dataform
Posted on
Jul 15, 2019
• Edited on
Jul 25, 2019
• Originally published at
dataform.co
How to write unit tests for your SQL queries
#
sql
#
testing
I’ve previously
written
about how I think we should prefer writing processing pipelines in pure SQL. However, a big difference between SQL and more widely-used languages is that those other languages generally have a strong tradition of unit testing.
Usually, when we talk about ‘tests’ in the context of SQL, we don’t actually mean unit tests. Instead, the term generally refers to data tests, which are really
assertions
that the data itself conforms to some test criteria.
Unit tests are not assertions.
Unit tests verify the logic of a SQL query by running that query on some fixed set of inputs. Assertions necessarily depend upon the real datasets which they validate, while unit tests should never depend on any real data.
The benefits of unit tests
Unit testing is a standard practice in software engineering. Unit tests help ensure that difficult pieces of logic or complex interactions between components work as expected - and continue to work as expected as the surrounding code changes.
Unit tests should not have any external dependencies; tests run the code in question on some faked inputs, ensuring that changes outside of that unit of code do not affect the test. This means that the success or failure of the test comes down purely to the code’s logic. Thus, if the test fails, you know exactly where to start debugging.
Why isn’t SQL unit testing widespread?
In standard languages, a unit test typically consists of injecting fake input into the code under test and checking that the output matches some expected result. However, SQL scripts don’t label their input datasets - typically, they’re just defined statically inline in a FROM clause. This makes it difficult to inject fake input test data into your SQL code.
The result of this is that most SQL code goes untested.
The solution
Various SQL frameworks let you define layers of indirection between your SQL and its input(s); i.e. you declare and label the input datasets upon which a query depends. Unit testing frameworks can use this indirection to replace real input data with faked versions.
We can then run the code under test, using some faked input, and compare the output result rows against a set of expected outputs. If the actual output of the code under test matches the expected output, the test passes; if not, it fails.
This technique is simple and gives you real power to verify that a SQL script does what you think it does. You can pass faked inputs to your SQL that your real data may not currently contain, giving you confidence that it can robustly handle a wide range of data.
Test case support in Dataform
When using Dataform’s
enriched SQL
, you reference input datasets using either the
ref()
or
resolve()
function. This functionality gives us an easy way to inject fake input datasets into a script, thus enabling users to write unit tests.
We have defined a new type of Dataform script:
test
. In a
test
query, you specify:
The query you’re testing
The faked inputs, each labeled with their referenced name
The expected output of running the query on the faked inputs
Behind the scenes, when you run the test, we dynamically replace the inputs to the
dataset
query with your faked input data. We then run the
dataset
query, along with the query that defines your expected output, and check that the resulting rows match. Simple!
An example
Here’s a worked example written using Dataform’s JavaScript API.
// First, define a dataset - we’ll follow this up with the unit test.
publish
(
""
age_groups
""
).
query
(
ctx
=>
`
      SELECT
      FLOOR(age / 5) * 5 AS age_group,
      COUNT(1) AS user_count
      FROM
${
ctx
.
ref
(
""
ages
""
)}
GROUP BY age_group
    `
);
// Now, define the unit test.
test
(
""
test_age_groups
""
)
// Specify the name of the dataset under test.
.
dataset
(
""
age_groups
""
)
// Provide the fake input “ages” dataset.
.
input
(
""
ages
""
,
`
      SELECT 15 AS age UNION ALL
      SELECT 21 AS age UNION ALL
      SELECT 24 AS age UNION ALL
      SELECT 34 AS age
    `
)
// Provide the expected output of running “age_groups” on the “ages” dataset.
.
expect
(
`
      SELECT 15 AS age_group, 1 AS user_count UNION ALL
      SELECT 20 AS age_group, 2 AS user_count UNION ALL
      SELECT 30 AS age_group, 1 AS user_count
    `
);
Alternatively, if you prefer to use Dataform’s enriched SQL, the unit test would look as follows (note that publishing the dataset is elided for simplicity):
config
{
type
:
""
test
""
,
dataset
:
""
age_groups
""
}
input
""
ages
""
{
SELECT
15
AS
age
UNION
ALL
SELECT
21
AS
age
UNION
ALL
SELECT
24
AS
age
UNION
ALL
SELECT
34
AS
age
}
SELECT
15
AS
age_group
,
1
AS
user_count
UNION
ALL
SELECT
20
AS
age_group
,
2
AS
user_count
UNION
ALL
SELECT
30
AS
age_group
,
1
AS
user_count
For more details, see our
documentation
.
We’ve released this functionality as part of the v1.0.0 release of our
@dataform NPM packages
. Dataform Web will soon support test cases, too. Let us know what you think!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
On-Demand Refresh | Materialized Views (Mviews) in ORACLE SQL
Pranav Bakare -
Nov 13
Configure Vitest, MSW and Playwright in a React project with Vite and TS - Part 2
Juan de Tomaso -
Oct 10
How to Generate Extent Reports in Selenium
Steve Wortham -
Oct 10
TIL: joining with LIKE
Augusts Bautra -
Nov 1
Dataform
Follow
Dataform makes it easy to develop and deploy SQL-based operations in your cloud data warehouse. Publish tables, write data tests and automate complex SQL workflows in a few minutes.
Sign up to Dataform
More from
Dataform
Building an end to end Machine Learning Pipeline in Bigquery
#
machinelearning
#
sql
Consider SQL when writing your next processing pipeline
#
sql
Testing data quality with SQL assertions
#
sql
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Local WordPress plugin development with docker-compose - DEV Community,"Local WordPress plugin development with docker-compose - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Simon Frey
Posted on
May 27, 2020
• Originally published at
simon-frey.com
Local WordPress plugin development with docker-compose
#
wordpress
#
docker
As my WordPress plugin development process was quite flawed (Write code locally => upload via FTP => test on server => repeat :’D) I had to search for a better solution for being able to local develop plugins and test them.
I used this awful process before as I did not want to install all required software for running WordPress (Apache, MySQL, PHP) on my local machine as I like it being clean and only having services running I really need at that certain moment.
The better sysadmins of you would claim that I still could easily start/stop the services for only having them running when I am developing…but boy that still sounds like a way to big hassle for me.
And as you always do this nowadays: Docker for the rescue 😉
After researching and working my way to build a docker-compose file I now can say: This time this approach worked like a charm.
It now only is docker-compose up and WordPress is running. I can live edit the files of my plugins, and they are loaded and used instantly.
docker-compose.yml
I assume you know about docker and docker-compose, if not give this article a read.
version
:
'
3.3'
services
:
db
:
image
:
mysql:5.7
volumes
:
-
db_data:/var/lib/mysql
restart
:
always
environment
:
MYSQL_ROOT_PASSWORD
:
rootpw
MYSQL_DATABASE
:
wordpress
MYSQL_USER
:
wordpress
MYSQL_PASSWORD
:
wordpress
wordpress
:
depends_on
:
-
db
image
:
wordpress:latest
ports
:
-
""
8080:80""
restart
:
always
volumes
:
-
../mastodon_wordpress_autopost/autopost-to-mastodon/trunk:/var/www/html/wp-content/plugins/autopost-to-mastodon:ro
environment
:
WORDPRESS_DB_HOST
:
db:3306
WORDPRESS_DB_USER
:
wordpress
WORDPRESS_DB_PASSWORD
:
wordpress
WORDPRESS_DB_NAME
:
wordpress
volumes
:
db_data
:
{}
After executing
docker-compose up
in the directory of the
docker-compose.yml
all the services are started up and you are greeted with a WordPress installer on
localhost:8080
(as I mapped the WordPress port to 8080).
Click trough the WordPress installer and you got a working instance running.
Via the volumes section you can map outside directories into the installation.
E.g. The following config maps my local developer directory
../mastodon_wordpress_autopost/autopost-to-mastodon/trunk
(which I have open in my editor) into the plugins directory of WordPress.
volumes
:
-
../mastodon_wordpress_autopost/autopost-to-mastodon/trunk:/var/www/html/wp-content/plugins/autopost-to-mastodon:ro
You can map as many directories as you want. Keep in mind that the path is relative to the location of the
docker-compose.yml
After activating the plugin in the WordPress GUI the plugin is up and running and I can live edit at in my editor. As it is mapped and not copied into the docker container all changes are reflected live.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
From Zero to Hero: Dockerizing My Go App and Hosting It on Render like a Pro 🚀
Allan Githaiga -
Nov 6
How I used an OpenSource LLM in Wordpress
Med Marrouchi -
Nov 5
Simple steps to debug docker build
Le Vuong -
Nov 6
How to integrate Queue in an Express application
Le Quan Phat -
Oct 26
Simon Frey
Follow
Joined
Feb 21, 2020
Trending on
DEV Community
Hot
Day 8: Week 1 Quiz
#
docker
#
devops
#
beginners
#
tutorial
Atomic Note-Taking Guide
#
productivity
#
vim
#
neovim
#
obsidian
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Top Trends in Software Testing using AI & ML in 2020 - DEV Community,"Top Trends in Software Testing using AI & ML in 2020 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
glensmith088
Posted on
Jul 29, 2020
• Originally published at
cloudqa.io
Top Trends in Software Testing using AI & ML in 2020
#
software
#
testing
#
automation
#
machinelearning
Artificial Intelligence (AI) has made some fantastic progress since its exploratory presentation as a PC program intended to beat chess grandmasters. The main colossal accomplishment was IBM’s Deep Blue, which beat world chess champion, Garry Kasparov. That episode was, in a genuine sense, an expression point in AI innovation.
AI is just on a par with the information that is sustained into its motor. The AI approach is to assemble frameworks and applications that learn and develop themselves, which is known as AI. The productivity of AI calculations relies upon the registering intensity of IT frameworks.
Another age of applications that can talk, tune in, sense, reason, think, and act is accessible to us on our cell phones and work areas. With the approach of AI, there has been a complete change in outlook in programming improvement and programming testing as far as the nature of applications and the speed at which they are conveyed to clients.
From a product testing point of view, AI can be utilized to integrate a colossal measure of information to foresee the correct system and to anticipate future disappointments in programming conveyance.
Computer-based intelligence procedures are influencing all parts of programming testing. The utilization cases in the accompanying table are on the whole observing improvement because of AI.
Machine Learning (ML) Subpart of AI. It depends on working with enormous datasets (Big Data), by social affairs, looking at, and investigating the information to find basic examples and investigating contrasts.
Consequently, AI and ML both include information and endeavors to drive essential leadership utilizing information, yet they are not something very similar.
More or less, it’s this: We can utilize AI/ML strategies to accumulate, look at, and watch creation client information to produce a more brilliant kind of relapse testing.
Organizations are, as of now, gathering vast volumes of information to comprehend clients use each time they visit frameworks. It turns into a part of their AI datasets to fabricate models that expect to take care of issues.
There’s much more to AI than simply creating AI calculations. An AI framework includes a critical number of segments to gather, look at, and concentrate highlights used by clients.
To guarantee the framework has no quality holes, we have to utilize similar information gathered for testing. We are nearer than at any time in recent memory to killing the weight of physically seeing how clients use the whole framework, which will enable us to create tests naturally.
Moving towards AI/ML assembles the correct sort of value inclusion — no all the more think about how to test your framework.
Usually, there is a large number of possible reasons for software testing to become a part of AI and ML. Some of them are discussed below:
Software testing used to be a primary and direct assignment. For whatever length of time that we knew how the framework was to carry on being used cases, it was generally simple to enter info and contrast the outcomes and the desires. A match would mean the test is passed. If there were a confound, cautions would go off as we had a potential bug and expected to fix it by starting from the very beginning once more.
In such a normal situation, an analyzer would glance through the agenda to guarantee that potential clients’ means and activities were altogether secured and issues settled. Be that as it may, since shoppers have become all the more requesting and less patient, one might say, conventional testing strategies frequently can’t stay aware of them.
The primary issue lies in the sheer measure of information that analyzers need to deal with in a constrained timeframe they, for the most part, have nowadays. This by itself removes conventional testing techniques from the condition and requires a progressively critical methodology. That is, the one fueled by human-made reasoning, AI, and prescient examination.
Improved Accuracy
To fail is human. Indeed, even the most careful analyzer will undoubtedly commit errors while doing dreary manual testing. This is the place mechanized testing helps by playing out similar advances precisely every time they are executed and never pass up recording itemized results. Developers from the various technologies, day-by-day basis makes their new opportunities for dealing with the Software Testing for making their Application more responsively.
Going Beyond the Limitations of Manual Testing
It is almost unimaginable for the most critical programming/QA offices to execute a controlled web application test with 1, 000+ clients. By involving the use of Software Testing in the Application, many software developers can easily create multiple selections of the coding making the applications work with the supportive OS, Coding, and many more.
An Aid for Developers and Testers
The process that has to be sent to the Quality Assurance (QA) team, a specific standard mechanism for testing, is being first approved by the developer’s side. Once the test has been created, these test can efficiently be run at various platform devices and is being checked for any issues. If the problems are not there, it will be redirected to the developer team to make the applications move to the next stage.
Increment in Overall Test Coverage
With robotized testing, one can expand the general profundity, and the extent of tests is bringing about by and significant improvement of programming quality. Computerized programming testing can investigate memory and document substance, interior program states, and information tables to decide whether the product is carrying on as it is relied upon. Test mechanization can execute 1,000+ diverse experiments in each trial furnishing inclusion that is beyond the realm of imagination with manual tests.
Conclusion
For the present, utilizing AI or ML to improve programming testing remains, for the most part, hypothetical. It’s not something associations are doing well at this point. Yet, that is valid for most AI or ML innovations. They stay in their outset regarding what engineers trust they’ll in the end become.
The advantages of applying AI and ML to programming testing are clear enough. Presently, it’s only an issue of dispensing the assets essential to manufacture the calculations and schedules. On the off chance that your organization is now taking a gander at AI/ML activities in different territories, I’d propose they consider extending them to programming testing, as well, so as not to be abandoned when the AI and ML upset turns out to be a piece of this specialty.
To know more, visit Cloud QA website or email at
info@cloudqa.io
.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Critical Path Software for Project Teams: Top 3 Solutions
Thomas H. Young -
Nov 27
Building a movie suggestion Bot using AWS Bedrock, Amazon Lex, and OpenSearch
Salam Shaik -
Nov 26
Writing Jest tests for searchParams in Next 15
Peter Jacxsens -
Oct 28
How to build a RAG model from scratch?
Hakeem Abbas -
Oct 28
glensmith088
Follow
I am an enthusiastic about advancing technology. I love to learn about the advances in automation. I research and read on various topics on technology and cars.
Education
Software Engineer
Work
Senior Engineer at Cloud QA
Joined
Jul 16, 2020
More from
glensmith088
What every QA must know about Selenium 4?
#
selenium
#
qa
#
automation
#
cloudqa
Case Study - Bright-Interactive
#
brightinteractive
#
assetmanagement
#
automationtesting
#
testing
Using Containers and Google cloud in Automation Testing
#
containers
#
automation
#
googlecloud
#
testing
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Android Chart Performance Comparison - DEV Community,"Android Chart Performance Comparison - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Andrew Bt
Posted on
May 2
• Originally published at
scichart.com
Android Chart Performance Comparison
#
chart
#
android
Android Chart Performance: TLDR
In this post, we compare the performance of five of the major Android Charting Libraries vs. our own brand new & upcoming
Android Chart: SciChart for Android
. In order to conduct a fair comparison, we've developed a comparison application which generates several different chart types and measures refresh rate (FPS - Frames per Second) on each chart library.
Our findings were that:
AChartEngine
,
GraphView
and
AndroidPlot
had poor real-time performance for an Android Chart Library and were too slow for use in an App which required real-time updates.
MPAndroidChart
had better performance and was suitable for some cases, e.g. when data updates were infrequent or number of points on the chart were less than a few thousand, but once dataset became larger (above a few thousand points), it also started to slow down.
One popular commercial Android Chart library was tested (labelled CompetitorA) which had comparable performance to
MPAndroidChart
.
Finally, the fastest Android Chart was
SciChart for Android
, which beat all available Android Chart Controls by by a wide margin.
At smaller datasets all the charts appeared fine, however once we increased the volume of data and rate of updating the Android Chart control, all of the above slowed down significantly except for SciChart.
In some tests SciChart for Android registered between 2-5x faster than the nearest competitor, and in one, a staggering 255x faster.
For this reason we propose
SciChart's Android Chart
as the best Android Chart Library where real-time updates, or visualisation of big-data is required.
How do we Measure Android Chart Performance?
To avoid uncorroborated statements, we've tried find out what the most popular Android Charts can offer in terms of speedf. To our big surprise, we
haven't found
any performance
analysis
of the existing charting packages! And that in-spite of the fact that similar questions have been asked a lot at
StackOverflow
,
Quora
, etc.! Which means that this question is topical among the Android developer community.
So we've created an Android application which performs a comparison of different charting packages. It was designed to help evaluating their performance under different test conditions, ranging from mild to extreme. The following aspects were considered to be the most important for such analysis:
Whether a chart crashes under a test-case
Measuring FPS on the user interface when a chart is under test conditions
The initial launch time of a test
Rate of change of data
General visual smoothness and UI responsiveness
Who did We Compare Against?
We have compared the performance of
Android Charts by SciChart
against the most popular and renowned Android charting libraries, including:
AChartEngine
, an open source charting software library for Android
MPAndroid
, an open source, cross-platform chart library for Android and iOS
Competitor A
, a popular commercial cross-platform Android and iOS charting software
We also had a look at
GraphView
, an open source graph plotting library for Android
AndroidPlot
, an open source Android library for creating charts
However due to time constraints, plus poor stability and performance of these libraries when large data was appended, we have omitted them from the results - for now.
The Test Setup
What Tests are Run?
There are four different test cases run in our Android Charts Performance Comparison app, under different conditions. They are designed to really really stress the chart, by having huge numbers of series, or points, or many updates per second, with varying thickness of pens / number of pixels filled.
Test 1: NxM Series Test
In this test, N series of M points are appended to series, then the chart redraws (the same data) as fast as possible by just resizing the Y-Axis for 10 seconds per test.
The FPS is measured using
the Choreographer class
from the Android API.
Areas Stressed
: Iterating Data Series, Coordinate Transformation and Drawing.
Test 2: Scatter Series Test
In this test, N Scatter points (ellipses) are appended to a series, then the chart redraws. Immediately after, the points are updated in a Brownian motion fashion and the chart is drawn again.
The FPS is measured using
the Choreographer class
from the Android API.
Areas Stressed:
Coordinate Transformation, Geometry Generation (Ellipse) and Drawing.
Test 3: FIFO (Scrolling) Series Test
In this test N points are appended to a series, but scrolled from right to left (as new points arrive, old are discarded). SciChart for Android has a built-in mode to handle this called Fifo data series which is implemented as a circular buffer. Other chart components you simply remove a point for each point added.
The FPS is measured using
the Choreographer class
from the Android API.
Areas Stressed:
Copying Circular Buffers (FIFO Series), Resampling and Drawing, scrolling.
Test 4: Append Data Test
In this test N/3 points are appended to 3 series, then M points are appended between each draw of the chart. The data is random-walk but we vary noise to create more, or less noisy waves. This has the effect of stressing the actual drawing engine (when more noisy) vs. the update rate when less noisy.
The FPS is measured using
the Choreographer class
from the Android API.
Areas stressed:
Appending Data, Resampling, Auto-Ranging and Drawing.
Test Hardware
We've used SciChart Android v.1.0 for this purpose and compared it with the other Android Charts on a number of devices and emulators, among these Samsung Galaxy S6 Edge and Google Nexus 4.
The Test Results
You can find the test results of our Android Chart Performance Comparison below. All test result
numbers are in FPS
(Frames per second -
Higher is Better
), meaning, the average number of redraws per second during the test, as measured by
the Choreographer class
from the Android API.
If a number is missed (blank), it means that a chart failed to run (crash) under a particular test-case.
The final column (SciChart Speedup) is the speed increase of SciChart Android vs. the other Android Charting Packages, e.g. 2.5 means 2.5x faster.
Test Results in Chart Form
What Android charting performance comparison would be complete without the results in chart form?
You can find the test results plotted below:
Append Points Test Results
SciChart Android is green, results are FPS (Refresh rate) so higher is better.
Fifo Scrolling Test Results
SciChart Android is green, results are FPS (Refresh rate) so higher is better.
Scatter Series Test Results
SciChart Android is green, results are FPS (Refresh rate) so higher is better.
NxM Series Test Results
SciChart Android is green, results are FPS (Refresh rate) so higher is better.
Performance Comparison Conclusions
According to the Performance Comparison Results, Most Android Chart libraries are unsuitable for real-time updates on the chart. Most perform poorly once the amount of data reaches a few thousand points.
By contrast, the
Android Charts by SciChart
are extremely fast and show robust performance across a number of test cases including scatter charts, scrolling line series, appending line series and multiple series.
SciChart should be considered a very strong contender when evaluating real-time Android Charting Libraries for use in either Android Apps or Embedded systems which run the Android operating system.
A version of the test-application is available on request, if you would like to
contact us
. For legal reasons we can't publish the names of
CompetitorA
but if you believe you are that competitor and wish to verify the results or submit a modification to optimize your code, please feel free.
Finally let us know what you think in the comments! If there's anything else you want us to test, please ask!
Best regards,
Yuriy, SciChart Team
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
AR Surface Detection and Virtual Object Placement in Android Apps
Parth -
Sep 18
How to implemented pull to refresh in Jetpack Compose
Yauheni Mokich -
Sep 27
Receiving Push Notifications from Firebase in Android MAUI
Laura Pučkoriūtė -
Sep 27
Digging Deep to Find the Right Balance Between DDD, Clean and Hexagonal Architectures
Vadym Yaroshchuk -
Sep 21
Andrew Bt
Follow
Former electronic engineer who works in software. With experience in languages from C/C++ to C#, JavaScript and TypeScript, and now specialises in performance optimisation and data visualisation
Location
United Kingdom
Joined
Dec 14, 2023
More from
Andrew Bt
How Fast is SciChart’s WPF Chart? DirectX vs. Software Comparison
#
chart
SciChart is the fastest JS Chart library available
#
javascript
#
chart
#
library
#
webdev
Open Source Stock Chart library for iOS & Android
#
ios
#
android
#
javascript
#
chart
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
"Java has such an enormous eco-system built around itself, it's hard to even c... - DEV Community","Java has such an enormous eco-system built around itself, it's hard to even c... - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Discussion on:
Python vs Java
View post
Collapse
Expand
Raphael Habereder
Raphael Habereder
Raphael Habereder
Follow
A Freelance DevOps doing container stuff and automating unhealthy amounts of software.
Need something automated or containerized? Feel free to hit me up :)
Joined
Mar 6, 2017
•
Jun 2 '20
• Edited on
Jun 3
• Edited
Dropdown menu
Copy link
Hide
Java has such an enormous eco-system built around itself, it's hard to even compare it to any other language. It probably can do almost anything, setting aside the age-old topic of ""if we can do it, should we?"".
Pythons ecosystem on the other hand is rather small in comparison, but it got a nice niche in AI/Big Data it dominates handily.
To be honest, it's not the language java itself, that I love. It's what Maven and Gradle make out of it that really eases the process of developing with java. Once you get used to your build-tool of choice, it's just copy-pasting dependencies and bam, you get everything you may need, including the appropriate build plugins for your workload.
Fire up a simple build command and you get a nice deliverable artifact. How amazing is that?
While Java is heavily restricted by it's conventions, you get a lot out of it for free. Getting back to maven, for example. Add your pom.xml with the typical boilerplate stuff and you essentially get the whole build-chain for free.
Python doesn't have as many conventions, but that freedom costs you the effort of doing most things yourself.
Edit: I'll refrain from editing out my wrong statements, but absolutely recommend reading the answers to my comment that correct them!
Like comment:
Like comment:
11
likes
Like
Comment button
Reply
Collapse
Expand
Jason C. McDonald
Jason C. McDonald
Jason C. McDonald
Follow
Author. Speaker. Time Lord. (Views are my own)
Email
codemouse92@outlook.com
Location
Time Vortex
Pronouns
he/him
Work
Author of ""Dead Simple Python"" (No Starch Press)
Joined
Jan 31, 2017
•
Jun 3 '20
Dropdown menu
Copy link
Hide
Pythons ecosystem on the other hand is rather small in comparison, but it got a nice niche in AI/Big Data it dominates handily.
I hear about this purported ""niche"" a lot, but in nearly a decade of Python coding, I've never observed it. Python has a fairly broad ecosystem when you factor in libraries (installable via pip or poetry in one line): it handles not only AI and data, but is excellent at GUI, networking, system interoperability, and just about anything you can throw at it. The only area Python really doesn't excel at is true multiprocessing, thanks to the Global Interpreter Lock (GIL), but there's work being done to resolve that too.
Python doesn't have as many conventions, but that freedom costs you the effort of doing most things yourself.
Uhm, this is most certainly
not
true. Python has a greater emphasis on convention and The One Obvious Way (as we put it) than most other languages, Java included. We have no fear of boilerplate — cargo cult programming, as Guido calls it — but we do avoid
mindless
boilerplate when we can (except, perhaps, in
setup.py
). Testing, packaging, deployment, all of these work well once you know where a couple of pieces fit, to the same degree as Java and Maven.
Like comment:
Like comment:
10
likes
Like
Comment button
Reply
Collapse
Expand
Raphael Habereder
Raphael Habereder
Raphael Habereder
Follow
A Freelance DevOps doing container stuff and automating unhealthy amounts of software.
Need something automated or containerized? Feel free to hit me up :)
Joined
Mar 6, 2017
•
Jun 3 '20
Dropdown menu
Copy link
Hide
Interesting, I never really observed Python like that, though judging by your comments, it's absolutely certain you are far more knowledgeable with Python than I am.
With eco-system I meant more the tooling around the languages, like CI/CD, Security Scanning, Lifecycle-Management and the likes.
I would assume a big portion of that is probably not even necessary for Python. What would your take be on that?
So thank you for your input and correcting my wrong statements.
It seems I have to do more Research next time
Like comment:
Like comment:
2
likes
Like
Thread
Thread
Jason C. McDonald
Jason C. McDonald
Jason C. McDonald
Follow
Author. Speaker. Time Lord. (Views are my own)
Email
codemouse92@outlook.com
Location
Time Vortex
Pronouns
he/him
Work
Author of ""Dead Simple Python"" (No Starch Press)
Joined
Jan 31, 2017
•
Jun 3 '20
• Edited on
Jun 3
• Edited
Dropdown menu
Copy link
Hide
CI/CD: There are a number of robust testing frameworks, including PyTest, Nose, and the newer Ward. Python also integrates beautifully into most CI/CD frameworks, especially given the relative ease of deploying a package. (On that note, many Python projects even automate publishing their package to the Python Package Index using GitHub Actions or the like.)
Security Scanning: Bandit is one prominent example, and I know there are others.
I don't know about Lifecycle-Management, as I haven't used it, but a casual glance through the Python Package Index, or
PyPI
, it looks like there are plenty of options.
It's a common myth that Python isn't suitable for large projects. In fact, there are a number of
very
large applications and libraries built entirely in Python. It has its flaws, but Python is often excellent for software architecture and deployment because it ""just works"" in most situations, and aims to be intuitive while staying out of your way.
Like comment:
Like comment:
3
likes
Like
Thread
Thread
Raphael Habereder
Raphael Habereder
Raphael Habereder
Follow
A Freelance DevOps doing container stuff and automating unhealthy amounts of software.
Need something automated or containerized? Feel free to hit me up :)
Joined
Mar 6, 2017
•
Jun 3 '20
Dropdown menu
Copy link
Hide
Thank you for your take, you never stop learning!
I guess I'll spend this weekend on Python and taking a look at the tools you mentioned.
The last time I had any python on my screen was when I wrote custom ansible filters and a few shallow dips into Django, which confused the hell out of me.
It's about time for a refresher :)
Like comment:
Like comment:
1
like
Like
Thread
Thread
Jason C. McDonald
Jason C. McDonald
Jason C. McDonald
Follow
Author. Speaker. Time Lord. (Views are my own)
Email
codemouse92@outlook.com
Location
Time Vortex
Pronouns
he/him
Work
Author of ""Dead Simple Python"" (No Starch Press)
Joined
Jan 31, 2017
•
Jun 3 '20
Dropdown menu
Copy link
Hide
You may appreciate this, then:
Introducing ""Dead Simple Python""
Jason C. McDonald ・ Jan 13 '19 ・ 3 min read
#python
#beginners
#coding
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Sergiy Yevtushenko
Sergiy Yevtushenko
Sergiy Yevtushenko
Follow
Writing code for 35+ years and still enjoy it...
Location
Krakow, Poland
Work
Senior Software Engineer
Joined
Mar 14, 2019
•
Jun 3 '20
Dropdown menu
Copy link
Hide
It turned out that many java limitations are very useful in long run. For example, quite strict linking between class and it's file name and location allows to keep at least some level of order in project codebase. Once this restriction is removed (Kotlin) maintaining order became a sensible effort,  since devs tend to make shortcuts.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Raphael Habereder
Raphael Habereder
Raphael Habereder
Follow
A Freelance DevOps doing container stuff and automating unhealthy amounts of software.
Need something automated or containerized? Feel free to hit me up :)
Joined
Mar 6, 2017
•
Jun 3 '20
Dropdown menu
Copy link
Hide
I absolutely agree.
Though I am more focused on the conventions part that comes with Java, since that is probably the first thing a Newcomer encounters.
Most frameworks, especially in the EE Version are convention over configuration. Conventions are just that, they can be ignored.
Which you absolutely shouldn't, but hey, you could even use pointers and allocate memory yourself if you want, so theres that.
The conventions are, mostly, of intelligent design and as a bonus it saves you a lot of time you would otherwise spend on cumbersome configuration, by having sensible defaults.
CDI/JPA (especially compared to their initial/earlier versions) would be a great example of convention over configuration and how much is done in the background for you.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Litmus Tests in Tech - DEV Community,"Litmus Tests in Tech - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Adam Nathaniel Davis
Posted on
Apr 9, 2021
• Edited on
Apr 11, 2021
Litmus Tests in Tech
#
webdev
#
career
#
hiring
#
javascript
Career Fear (18 Part Series)
1
Coding Tests Eliminate Some of the Best Candidates
2
The Contentious Art of Pull Requests
...
14 more parts...
3
The Unbearable Whiteness of Coding
4
Tech Decisions with Zenn Diagrams
5
One Crazy Trick to Become a Programmer
6
Why Older People Struggle In Programming Jobs
7
The Perils of Remote Work
8
How Employers Sabotage Remote Workers
9
Devs Shouldn't Report to PMs
10
Lie - To Advance Your Career
11
Applying To Facebook
12
Litmus Tests in Tech
13
Rejected by Facebook
14
How To Hire Programmers
15
Codility === Sadness
16
Notes On The Job Market For Frontend Programmers
17
Unit Tests Should NOT Be In Your Coding Evaluations
18
How to Get Your First Job as a Self-Taught Programmer
Since I began blogging on Dev.to, one of my primary subjects has been the (arbitrary) standards that we apply to others when we evaluate them as potential candidates for jobs.  In fact, it was the focus of my
first
blog on this platform, and I've addressed it on several different levels since.  But this really became much clearer (in
my
mind, at least) after I posted my last article - an article about applying to Facebook.
In that article, I explained that I'd been told to expect questions about
.call()
and
.apply()
.  And because I don't feel any need to ""bolster"" my image in this forum, I also mentioned that I had to immediately Google those concepts - because I wasn't familiar with them.
To be absolutely clear, I've been coding, quite literally, since I was a kid.  I've been coding
professionally
for almost a quarter-century.  I've been doing JavaScript development quite heavily for about... a decade or so.  And I've been doing React development, specifically, for the last five+ years.
Despite this mountain of real-world experience, I wasn't terribly familiar with the ideas of
.call()
or
.apply()
.  After I looked them up, I realized exactly
why
I'm not familiar with them - because I don't use them
at all
in my regular work.  And given ""modern"" JS norms, I'm not even sure how or why I would use them going forward.
As a React dev, I'm intimately familiar with
.bind()
.  I'm familiar with it because there was a time when I used it frequently.  That time has passed.  In my current dev (and by ""current"", I mean, the last 2-3 years), I haven't used it at all.  Quite frankly, for me,
.bind()
is basically...
deprecated
.  And when I looked up
.call()
and
.apply()
, I similarly felt that these concepts are
deprecated
.
You see, I don't even
use
this
anymore.  And I'm not saying that there aren't some potentially-valid use-cases out there for
this
.  But for the most part, if you're repeatedly falling back on
this
, I gotta wonder how ""modern"" your JS dev really is?  And if you're
not
using
this
, then the use-cases for
.bind()
,
.call()
, and
.apply()
become incredibly...
scant
.
But this article is
NOT
about
.bind()
or
.call()
or
.apply()
.  I truly couldn't care less whether you agree with my take on them.  In fact, this article isn't about any particular language construct at all.  This article is about the arrogance of ""tech folks"" (like myself), and
litmus tests
, and
shibboleths
.
Shibboleths
To truly understand someone's abilities is...
hard
.  And most of us can't really put in the time to do a ""deep dive"" on someone else's credentials.  So we take shortcuts.
I'm not pointing fingers here.
I
do this.  We
all
do this.  We create an informal list of slang that we can use to determine whether someone is ""one of
us
"" or ""one of
them
"".
In other words, we create (consciously or subconsciously)
shibboleths
.  We create shortcuts to separate the
real
devs - from the pretenders.
What do shibboleths look/sound like in the current JavaScript environment?  Here are some common examples:
If a JS dev says ""class"" or ""constructor"" or ""OOP"" without the appropriate level of derision, he's one of
them
.
If a JS dev says ""declarative"" or ""pure"" or ""immutable"" or ""functional programming"", he's one of
us
.
If a frontend dev (of nearly any language) uses tabs, he's one of
them
.
If he uses spaces (and only
two-space
indents!), he's one of
us
.
If a JS dev refers to object properties via dot-notation, he's one of
them
.
If a JS dev relentlessly destructures all of his object properties into standalone variables, he one of
us
.
If a JS dev uses the
function
keyword, he's one of
them
.
If he uses arrow syntax, he's one of
us
.
If a JS dev uses
.then()
/
.catch()
, he's one of
them
.
If he uses
async
/
await
, he's one of
us
.
I could go on and on here, but I think you get the point.  Because we don't have the time to do a ""deep dive"" on everyone's skillset, we resort to using these shorthand call signs to swiftly label a dev as one of
us
, or one of
them
.
The (massive) problem with this approach is that it's lazy.  And it frequently leads to wildly-inaccurate assessments.
If I can perfectly pronounce
shibbólet
, does that mean I'm Jewish?  Possibly.  But it could also mean that I was raised in an environment with heavy Jewish influences.  Or it could mean that I study languages and I'm fluent in Hebrew.  Or it could even mean that I knew you'd use this silly test as a sorting mechanism, so I studied, beforehand, exactly how to pronounce
shibbólet
.
Similarly, the shibboleths we employ when evaluating fellow coders are prone to serious miscalculations.  More importantly, they're also rife with our own personal biases.
I've met some coders who absolutely adore
async
/
await
.  And that's fine.  But sometimes they are so enamored with it that they'll look down their nose at anyone who uses
.then()
/
.catch()
.  Or they scoff at anyone who uses the
function
keyword.  Or they snigger when they see a
class
in your code.  In other words, they're treating these concepts as shibboleths to sort out the poor coders.  But what they're
really
sorting out is: people who don't code
like them
.
Litmus Snobbery
A litmus test is similar to a shibboleth - but not entirely.  Shibboleths are more about
communication
- concepts that we
want
to hear from other coders, and concepts that we do
not
want to hear.
But litmus tests are more about tactics that you've either mastered - or you haven't.  Here's how a litmus test typically works in coding environments:
A candidate comes in for an interview and the team positions the nervous, sweaty soul at the whiteboard.  Then they ask him to code an example of doing a search with a binary tree.  The candidate has some extensive experience, and a solid grasp of many coding concepts.  But he's never studied or implemented a binary tree.  So he fails to provide any satisfactory solution to the problem.
At this point, the interview is
functionally
over.  They might afford him the courtesy of continuing the interview for another 45 minutes or so.  But the devs in that room who are evaluating the candidate have already decided that this guy's an imposter.  And they
know
he's an imposter because he can't do a binary tree!
I mean... the
nerve
of this guy!  Whatever made him believe that he was worthy of writing code if he hasn't already mastered the concept of binary trees?!  Not only should he be eliminated from consideration for the job, but he should also have his hands chopped off, so he's never tempted to sit down at a keyboard again!
Ammiright???
Of course, the
reality
is quite different.  In a quarter century of professional coding, a binary tree has been the ""right tool for the job"", for
me
, exactly...
ONCE
.  I know what a binary tree
is
, and I generally know
where
they should be used.  But if I had to set up a binary tree search today, I'd first start by hitting up Google for a few minutes, because I haven't actually
written
anything related to a binary tree in about 15 years.
But this article isn't about binary trees.  It's about the fact that we latch onto some programming technique that
we're
familiar with, and then we use that technique as a litmus test to eliminate potential candidates.
Thinly-Veiled Arrogance
Litmus tests reek of
arrogance
.  They function from the perspective that
you
know how to do this thing, so anyone else who claims to be a ""real"" coder should also know how to do this thing.  And if they can't do that thing?  Then no amount of experience will allow you to overlook the fact that this person - who
claims
to know how to code - couldn't do this ""thing"" that you yourself can do just fine.  So clearly... they must royally suck at life!
This is where the Arrogants get all in a lather.  They start hyperventilating and flailing their arms, and they say:
But, but,
but
... if this guy can't do a binary tree, he's obviously not a dev with broad and deep knowledge!!
Take it from me.  When a litmus tester reaches
this
point, there's really no benefit in talking to them about it any longer.  Because at
this
point, there is no amount of knowledge or skill or experience that will, in their mind, erase the fact that this guy didn't know how to do
this one thing
.  You can tell them, ""But... he wrote Tesla's entire self-driving application.  By himself.  In assembly code.  Over a long weekend.""  And their only response will be, ""But he didn't know how to write that binary tree search, so he obviously can't be too dedicated to his craft!""
You see, once someone gets it in their head that you should know some particular bit of programming knowledge, it won't matter to them whether you can prove that you know
ALL THE (other) THINGS!!!
They will always come back to the fact that you didn't know
this
thing - and therefore, you suck.
Defenders of this mindset will tell you that there's nothing wrong with expecting a candidate to have certain skills - like programming a binary tree search.  And if they can't do that, then how is it
arrogant
to eliminate candidates on that basis???  But the answer to that is incredibly simple:
For those who rely on litmus tests, their basis for what is-or-is-not ""standard"" is: Do
they
know how to do it themselves???  If they know how to do it, then they automatically assume that any ""real"" dev should also know how to do it.
Let me say that again, in a slightly different way.  Because I want this to really
sink in
for you.
When we apply litmus tests to other coders, we generally assume that whatever
we
are comfortable with is ""common knowledge"" - and any ""real"" coder should be able to do the same.  And we assume that whatever we
aren't
comfortable with is... esoteric.  Rare.  Arcane.
The Arrogants who are judging you based upon your binary tree skills are doing so either because A) their environment just so happens to use a lot of binary tree searches (thus,
to them
, it's become a commonplace coding task), or B) it's been established in their dev shop that this is a Sacred Coding Test and all of their current devs either knew how to do binary tree searches before they came onboard, or they quickly learned after it became a ""standard"" in their environment.
Karma is a...
In case you can't hear them yourself, those Arrogants are still squawking in the background.  They're fidgeting and arguing that:
I don't care what you say, if someone isn't intimately familiar with binary tree searches, they're not a
real
programmer!!!
To which I will only respond with this:
I wasn't familiar with
.call()
and
.apply()
until a few weeks ago.  I guess that means I'm not a ""real"" coder.  I'd need to do a little googling before I'd be ready to sit down and write out a binary tree search algorithm from scratch.  I guess that also means that I'm not a ""real"" coder.
But I've been doing this now for a quarter-century.  Do you
really
believe that, if
I'm
the one conducting the interview, I couldn't possibly come up with a litmus test that you would
FAIL
???  Are you so certain in the incredibly-broad and impressively-deep scope of your knowledge, that you're sure I couldn't possibly stump you???
And if I
could
stump you on any particular ""litmus test"", how will you feel when you get summarily dismissed from consideration just because you hadn't mastered the one technique that I happened to ask you to illustrate on the whiteboard???
Career Fear (18 Part Series)
1
Coding Tests Eliminate Some of the Best Candidates
2
The Contentious Art of Pull Requests
...
14 more parts...
3
The Unbearable Whiteness of Coding
4
Tech Decisions with Zenn Diagrams
5
One Crazy Trick to Become a Programmer
6
Why Older People Struggle In Programming Jobs
7
The Perils of Remote Work
8
How Employers Sabotage Remote Workers
9
Devs Shouldn't Report to PMs
10
Lie - To Advance Your Career
11
Applying To Facebook
12
Litmus Tests in Tech
13
Rejected by Facebook
14
How To Hire Programmers
15
Codility === Sadness
16
Notes On The Job Market For Frontend Programmers
17
Unit Tests Should NOT Be In Your Coding Evaluations
18
How to Get Your First Job as a Self-Taught Programmer
Top comments
(16)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Kevin Gilpin
Kevin Gilpin
Kevin Gilpin
Follow
Teacher in training. Founder @ AppLand.com. Flying, hockey, family fun.
Location
Boston, MA
Joined
Jan 7, 2021
•
Apr 9 '21
Dropdown menu
Copy link
Hide
What’s wrong with
this
? Seems like the classes support in JS is a lot better now than it used to be and
this
is a pretty essential part of that. I like a functional style too but sometimes a class is a good option.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Adam Nathaniel Davis
Adam Nathaniel Davis
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Email
me@adamdavis.codes
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
•
Apr 9 '21
Dropdown menu
Copy link
Hide
I don't disagree with you.  Maybe I should've been a little clearer in stating that, for the last 4-5 years, I've been almost exclusively a
React
dev.  And in React, everything seems to be trending
away
from classes and
toward
functions.  I'm on record as not ""believing"" in that trend - but I'm also on record as wanting to keep getting paychecks.  So, for some time now, I haven't written a single class.  Anywhere.
To be clear, the
this
keyword still ""exists"" in functions.  But if you're writing purely in functions, I think it's a rather dodgy practice (obviously, just my opinion there) to keep using
this
.
Like comment:
Like comment:
6
likes
Like
Comment button
Reply
Collapse
Expand
Kevin Gilpin
Kevin Gilpin
Kevin Gilpin
Follow
Teacher in training. Founder @ AppLand.com. Flying, hockey, family fun.
Location
Boston, MA
Joined
Jan 7, 2021
•
Apr 9 '21
Dropdown menu
Copy link
Hide
Yeah that makes sense.
this
in a functional style would be weird.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Liam Edwards
Liam Edwards
Liam Edwards
Follow
I am a Web & Software developer on the Central Coast, NSW.
Location
Central Coast, NSW, Australia
Work
Web Developer at Milestone-Belanova
Joined
Apr 12, 2021
•
Apr 12 '21
Dropdown menu
Copy link
Hide
I'm not sure if I'm
one of them
or
one of us
. I'm a fan of using classes in JavaScript, I use tabs, and I prefer object dot notation, however I abhor the
function
keyword and I love
async
/
await
.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Adam Nathaniel Davis
Adam Nathaniel Davis
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Email
me@adamdavis.codes
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
•
Apr 12 '21
Dropdown menu
Copy link
Hide
If you're not hung up on tech ""shibboleths"", then it doesn't matter where you fall in that combination of traits.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Liam Edwards
Liam Edwards
Liam Edwards
Follow
I am a Web & Software developer on the Central Coast, NSW.
Location
Central Coast, NSW, Australia
Work
Web Developer at Milestone-Belanova
Joined
Apr 12, 2021
•
Apr 12 '21
Dropdown menu
Copy link
Hide
Heh, true I guess.
I'm definitely guilty of judging people on how they write their code though. I do try to avoid it but sometimes it cannot be helped. Seeing
var
everywhere will change a person.
Like comment:
Like comment:
2
likes
Like
Thread
Thread
Adam Nathaniel Davis
Adam Nathaniel Davis
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Email
me@adamdavis.codes
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
•
Apr 12 '21
• Edited on
Apr 12
• Edited
Dropdown menu
Copy link
Hide
I agree, but
var
has
quantifiable
reasons why it's potentially harmful.  (Of course,
var
isn't
necessarily
harmful - it's just
potentially
harmful, depending upon how you use it.)  So I understand the aversion to seeing any new code written with
var
.
Contrast that with those who shudder every time they see
class
in JS code.  I have yet to hear a single
quantifiable
reason why this is ""bad"".  All of their arguments fall back on fuzzy bromides and vague notions of ""best practices"".  Yet they'll still use the appearance of that
class
keyword as a shibboleth to eliminate you from consideration.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Kimmo Sääskilahti
Kimmo Sääskilahti
Kimmo Sääskilahti
Follow
ML Software Developer enthusiastic about Python, TypeScript, Scala, machine learning and functional programming.
Location
Helsinki
Education
D.Sc. (computational science)
Work
Lead Software Developer
Joined
Feb 11, 2019
•
Apr 10 '21
Dropdown menu
Copy link
Hide
Thanks for the great article! I totally agree with you and can admit of being guilty of sometimes carrying out ""litmus tests"" myself. It is total nonsense to discard a candidate if they don't know a specific thing or two in a given language.
Could such tests bring more value when there are sufficiently many of them? If the applicant doesn't know async-await, cannot explain ""this"" in JS, cannot code a binary search tree, and doesn't know what a pure function component is, isn't it then reasonable to be able to make the assumption that this candidate may not be the ideal candidate for our company? We cannot rank the candidates by how many they got ""right"", but we can say whether they passed or failed. Does this make any sense?
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Adam Nathaniel Davis
Adam Nathaniel Davis
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Email
me@adamdavis.codes
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
•
Apr 10 '21
Dropdown menu
Copy link
Hide
Yes, I totally agree.  It's incredibly valuable to ask a candidate a range of questions on a range of topics - or to ask them to attempt a range of different whiteboard-style challenges.  But as you point out, the key is to not treat any one of those questions/challenges as a ""knockout"" issue.  Instead, look at the
aggregate
value of all the answers combined.
Also, I think it's important to differentiate between ""the candidate can't code this from scratch off the top of his head"" versus ""the candidate has no idea what this concept is, when to use it, or where to find information about it.""
In the example I gave regarding binary trees, I know dang well that I can't sit down, right
now
, and code it up from my head.  But I know what a binary tree
is
.  I know when you generally consider using one and what benefits (and downsides) it provides.  And I know that, if I had to go code one up tomorrow, I could absolutely be ""up to speed"" on it in a very short period of time.  That's far different than dealing with a candidate who's never even heard of a binary tree and has no clue what purpose it would serve.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Garrett van Wageningen
Garrett van Wageningen
Garrett van Wageningen
Follow
Location
Dallas, TX
Work
Software Developer at Halff Associates
Joined
Aug 5, 2020
•
Apr 11 '21
Dropdown menu
Copy link
Hide
Such a great job of framing the issue. I love apply, I wish people knew it so I could use it. But is
add.apply(null, [10, 20])
more readable or understandable than
const [x, y] = [10, 20]; add(x, y)
or some other variant? Maybe, maybe not. It depend on who you're working with! And if I'm being honest, it probably isn't more readable in any situation. And only once in a blue moon do I use it.
Then the functional vs OO, and promise chain vs async await is so great. I caught myself thinking, ""but async await is more imperative. Even if it is the new shiny!"" And there lies the problem.
This is a great article. I'm going to keep it in my back pocket the next time we do an interview.
Aside: putting together a good interview seems to be an art in itself, I'd love to hear some new ideas in that space
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Abdur Rehman Khalid
Abdur Rehman Khalid
Abdur Rehman Khalid
Follow
A Senior Computer Science Student, passionate Data Visualization, Data Science, Big Data, Development with Java, Angular, and React.Js.
Location
Lahore
Education
MS Computer Science
Work
Senior Computer Science Student
Joined
Apr 17, 2019
•
May 19 '21
Dropdown menu
Copy link
Hide
An excellent article that totally describes my feelings of becoming failure in many interviews.
They ask question that seems very simple to them, but when you give them a answer that they do not know becomes a very new and unique thing, I will continue with a very simple example.
let's say you are at an interview and someone asks you ""What is Node.JS?"" and You answer, ""It is a Run Time Environment for JavaScript."" if you stick to this question, they will assume you a good candidate but if you say,  ""It is a Run Time Environment for JavaScript, that uses operating system's I/O operations along side with POSIX."" they will consider you an alien and it is quite possible that they will not think you as developer as well.
One more thing, the devs that are taking the interview often forgets that there are multiple ways of solving the same problem, and programming is a very vast field and there are many to infinite things that a person still requires to learn as well. So, becoming a good developer is a long and exciting journey.
There should be a good and effective manner of judging the candidates, rather than these boring hard-coded interview questions, because they represent the fragment of the memory not the knowledge about the programming or a mind filled with passion to solve the problems.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Adam Nathaniel Davis
Adam Nathaniel Davis
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Email
me@adamdavis.codes
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
•
May 19 '21
Dropdown menu
Copy link
Hide
Very well said!
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Kelly Stannard
Kelly Stannard
Kelly Stannard
Follow
Developer. Design pattern enthusiast. Tilting at Agile practices.
Work
Lead Engineer at Andros.co
Joined
Sep 9, 2019
•
Apr 25 '21
Dropdown menu
Copy link
Hide
I think the problem is that it is impossible to gage overall skill based off any arbitrary number of fixed questions. I usually ask open ended questions so that candidates can show me what they are most skilled in.
“Tell me about an interesting problem you solved.”
“What is something exciting you learned lately”
If I have to do whiteboarding I would go with some kind of general knowledge use case. For example, measurements.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Jonathan Experton
Jonathan Experton
Jonathan Experton
Follow
I gather and mentor people to deliver well crafted softwares
Location
Montreal, Canada
Education
Jeffrey Way, Adam Wathan, MPJ, Uncle Bob and many more
Work
Deterministic Disorder Manager
Joined
Nov 13, 2017
•
Apr 11 '21
• Edited on
Apr 11
• Edited
Dropdown menu
Copy link
Hide
As an occasional technical interviewer I find it difficult to separate the knowledge I value from what matters.
Most candidates I’ve interviewed in webdev have little experience and zero theoretical knowledge about the tools they use, which is fine for junior entry level. What I find more problematic is for instance when an intermediate React developer has never heard about the event loop, or has never read the official React documentation.
Sometimes, even though I don’t value the answer, I ask basic shibboleth questions, like code syntax, on purpose to check if the candidate took the time to read things like “how to prepare for a React job interview” to check his level of commitment in his job search.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Isaac Hagoel
Isaac Hagoel
Isaac Hagoel
Follow
Location
Sydney, Australia
Work
Principal Engineer at Atlassian (ex. Pearson, TripAdvisor, Intel)
Joined
Feb 4, 2020
•
Apr 11 '21
Dropdown menu
Copy link
Hide
The insight about the one thing vs. Tesla's self driving system is pure gold.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Adam Nathaniel Davis
Adam Nathaniel Davis
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Email
me@adamdavis.codes
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
•
Apr 11 '21
Dropdown menu
Copy link
Hide
Hahaha - thank you!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
View full discussion (16 comments)
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Why Monorepo Projects Sucks: Performance Considerations with Nx
Jeferson F Silva -
Dec 11
UX/UI Laws Every Frontend Developer Should Know
Yuliana Sepúlveda Marín -
Dec 10
Mastering Modern JavaScript: A Deep Dive into ES6 Function Creation and Best Practices
Kissel James Paalaman -
Dec 11
Never code lines on the HTML canvas again
UnionForever -
Dec 10
Adam Nathaniel Davis
Follow
React acolyte, jack-of-all-(programming)trades, full-stack developer
Location
New Orleans, LA
Work
Frontend Software Engineer
Joined
Feb 18, 2020
More from
Adam Nathaniel Davis
Use Cases for IIFEs
#
javascript
#
typescript
#
webdev
#
tutorial
The Communication Shortcomings of Programmers
#
webdev
#
programming
#
career
Most Managers Have No Clue What Programmers Actually Do
#
webdev
#
programming
#
career
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
GCP Professional Data Engineer Guide - September 2020 - DEV Community,"GCP Professional Data Engineer Guide - September 2020 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Adolfo Estevez
Posted on
Oct 6, 2020
• Edited on
Oct 9, 2020
GCP Professional Data Engineer Guide - September 2020
#
googlecloud
#
career
#
datascience
#
machinelearning
I have recently recalled
my first experience with GCP
. It was in London, shortly before the 2012 Olympics, in an
online gaming project
, initially thought for
AWS
, that was migrated to App Engine -
PAAS platform that would evolve to the current GCP
.
My initial impression was good,
although the platform
imposed a number of development limitations
, which would be reduced later with the release of
App Engine Flexible
.
Coinciding with the launch of
Tensor Flow
as an Open Source framework in 2015, I was lucky enough to
attend a workshop on neural networks
- given by one of the AI scientists from
Google Seattle
- where I had my second experience with the platform. I was very surprised by the
simplicity of configuration and deployment
, the
NoOps concept and a Machine Learning / AI offering
, without competition at the time.
Do Androids Dream of Electric Sheep? Philip K. Dick would have ""hallucinated"" with the electrical dreams of neural networks - powered by Tensor Flow.
Exam
The
structure of the exam
is the usual one in GCP exams:
2 hours and 50 questions
, with a format  directed towards scenario-type questions, mixing
questions of high difficulty
with simpler ones of
medium-low difficulty
.
In general,
to choose the correct answer
, you
have to apply technical and business criteria
. Therefore, it is necessary a deep knowledge of the services from the technological point of view, as well as skill / experience to apply the business criteria in a contextual way, depending on the question, type of environment, sector, application, etc .. .
Image #1, Data Lake, the ubiquitous architecture - Image owned by GCP
We can group the relevant services according to the states (and substates) of the data cycle:
Management, Storage, Transformation and Analysis.
Ingestion Batch
/
Data Lake
: Cloud Storage.
Ingestion
Streaming
: Kafka, Pub/Sub, Computing Services, Cloud IoT Core.
Migrations:
Transfer Appliance, Transfer Service, Interconnect, gsutil.
Transformation
s: Dataflow, Dataproc, Cloud Dataprep, Hadoop, Apache Beam.
Computing:
Kubernetes Engine, Compute Instances, Cloud Functions, App Engine.
Storage
: Cloud SQL, Cloud Spanner, Datastore / Firebase, BigQuery, BigTable, HBase, MongoDB, Cassandra.
Cache
: Cloud Memorystore, Redis.
Analysis / Data Operations:
BigQuery, Cloud Datalab, Data Studio, DataPrep, Cloud Composer, Apache Airflow.
Machine Learning:
AI Platform, BigQueryML, Cloud AutoML, Tensor Flow, Cloud Text-to-Speech API, Cloud Speech-to-Text, Cloud Vision API, Cloud Video AI, Translations, Recommendations API, Cloud Inference API,  Natural Language,  DialogFlow,  Spark MLib.
IoT
: Cloud IoT Core, Cloud IoT Edge.
Security & encryption
: IAM, Roles, Encryption, KMS, Data Prevention API, Compliance ...
Opera
t
ions:
Kubeflow, AI Platform, Cloud Deployment Manager ...
Monitorization:
Cloud Stackdriver Logging, Stackdriver Monitoring.
Optimization:
Cost control, Autoscaling, Preemptive instances ...
Pre-requisites and recommendations
At this level of certification, the questions do not refer, in general, to a single topic
. That is, a question from the Analytics domain may require more or less advanced knowledge of
Computing, Security, Networking or DevOps
to be able to solve it successfully. I´d recommend having the
GCP Associate Cloud Engineer
certification or have equivalent knowledge.
GCP experience at the architectural level.
The exam is focused, in part, on the architecture solution, design and deployment of data pipelines; selection of technologies to solve business problems, and to a lesser extent development. I´d recommend studying as many
reference architectures
as possible, such as the ones I show in this guide.
GCP experience at the development level.
Although no explicit programming questions appeared in my question set, or in the mock test, the exam requires technical knowledge of services and APIS:
SQL, Python, REST, algorithms, Map-Reduce, Spark, Apache Beam (Dataflow)
…
GCP experience at Security level.
Domain that appears transversally in all certifications - I´d  recommend knowledge at the level of
Associate Engineer
.
GCP experience at Networking level.
Another domain that appears transversely -  I´d recommend knowledge at the level of Associate Engineer.
Knowledge of Data Analytics.
It's a no-brainer, but some domain knowledge is essential. Otherwise, I´d recommend studying books like
“Data Analytics with Hadoop”
or taking courses like Specialized Program:
Data Engineering, Big Data and ML on Google Cloud in Coursera
.
Likewise, practicing with laboratories or pet projects is essential to obtain some practical experience.
Knowledge of the Hadoop - Spark ecosystem
. Connected with the previous point. High-level knowledge of the ecosystem is necessary:
Map Reduce, Spark, Hive, Hdfs, Pig
…
Knowledge of Machine Learning and IoT.
Advanced knowledge in
Data Science and Machine Learning
is essential, apart from specific knowledge of GCP products. There are questions exclusively about this domain - at the level of certifications like
AWS Machine Learning
or higher. IoT appears on the exam in a lighter form, but it is essential to know the architecture and services of reference.
DevOps experience.
Concepts such as CI / CD, infrastructure or configuration as code, are of great importance today, and this is reflected in the exam, although they do not have a great specific weight.
Standard questions
Representative question of the level of difficulty of the
exam
.
Image property of GCP
Practical migration scenario question, that includes cloud services and the
Hadoop ecosystem
, as well as concepts from the
Analytics domain
.
Services to study in detail
Image #2 - property of GCP
Cloud Storage
- Core service that appears consistently in all certifications, and is central in the
Data Lake
systems. I´d recommend its study in detail at an architectural level - see Image 1 -, configurations according to the data temperature, and as an integration / storage element between the different services
BigQuery
- Core service in the
Analytics GCP
domain as a BI and storage element. Extremely important in the exam, so have to be studied in detail: architecture, configuration, backups, export / import, streaming, batch, security, partitioning, sharding, projects, datasets, views, integration with other services, cost, queries and optimization SQL (legacy and standard) at table levels, keys …
Pub / Sub
- Core service as an element of ingestion and integration. Its in-depth study is highly recommended: use cases, architecture, configuration, API, security and integration with other services (eg
Dataflow, Cloud Storage
) - Kafka's native cloud mirror service.
Dataflow
- Core service in the
Analytics GCP domain
as a process and transformation element. Implementation based on
Apache Beam
that is necessary to know at a high level and pipeline design. Use cases, architecture, configuration, API and integration with other services.
Dataproc
- Core service in the
Analytics GCP domain
as a process and transformation element. It is a service based on Hadoop, and therefore, it is the indicated service for a migration to the cloud. In this case, not only knowledge of Dataproc is required, but also in native services:
Spark, HDFS, HBase, Pig
… use cases, architecture, configuration, import / export, reliability, optimization, cost, API and integration with other services.
Cloud SQL, Cloud Spanner
- Cloud native relational databases. Use cases, architecture, configuration, security, performance, reliability, cost and optimization: clusters, transactionality, disaster recovery, backups, export / import, SQL performance and optimization, tables, queries, keys and debugging. Integration with other services.
Cloud Bigtable
- Low latency
NoSQL
managed database, suitable for time series, IoT… ideal to replace a
HBase
installation on premise. Use cases, architecture, configuration, security, performance, reliability and optimization: clusters, CAP, backups, export / import, partitioning, performance, and optimization of tables, queries, keys. Integration with other services.
Machine Learning
- One of the strengths of the certification is the domain ""
Operationalizing machine learning models""
. Much more dense and complex than it may seem at first, since it not only includes the operability and knowledge of the relevant GCP services; likewise, it includes the knowledge of the Data Science fundamentals: algorithm selection, optimization, metrics … The level of difficulty of the questions is variable, but comparable to that of specific certifications, such as
AWS Certified Machine Learning - Specialty
. Most important services:
BigQuery ML, Cloud Vision API, Cloud Video Intelligence, Cloud AutoML, Tensor Flow, Dialogflow, GPU´s, TPU´s
…
Security
- Security is a transversal concern across all domains, and appears consistently in all certifications. In this case, it appears as an independent technical topic, crosscutting concern or as a business requirement:
KMS, IAM, Policies, Roles, Encryption, Data Prevention API …
Image #3, IoT Reference Architecture - owned by GCP
Very important services to consider
Networking
- Cross-domain that can appear in the form of separate technical issues, cross cutting concerns, or as business requirements:
VPC, Direct Interconnect, Multi Region / Zone, Hybrid connectivity, Firewall rules, Load Balancing, Network Security, Container Networking, API Access ( private / public) …
Hadoop -
The exam covers ecosystems and third-party services like
Hadoop, Spark, HDFS, Hive, Pig
… use cases, architecture, functionality, integration and migration to GCP.
Apache Kafka
- Alternative service to
Pub / Sub
, so it is advisable to study it at a high level: use cases, operational characteristics, configuration, migration and integration with GCP - plugins, connectors.
IoT
- It can appear in various questions at the architectural level: use cases, reference architecture and integration with other services.
IoT core, Edge Computing.
Datastore / Firebase
- Document database. Use cases, configuration, performance, entity model, keys and index optimization, transactions, backups, export / import and integration with other services. It doesn't carry as much weight as the other data repositories.
Cloud Memory Store / Redis
- Structured data cache repository. Use cases, architecture, configuration, performance, reliability and optimization: clusters, backups, export / import and integration with other services.
Cloud Dataprep
- Use cases, console and general operation, supported formats, and Dataflow integration.
Cloud Stackdriver
- Use cases, monitoring and logging, both at the system and application level:
Cloud Stackdriver Logging, Cloud Stackdriver Monitoring, Stackdriver Agent and plugins.
Other services
MongoDB, Cassandra -
NoSQL
databases that can appear in different scenarios. Use cases, architecture and integration with other services
.
Cloud Composer
- Use cases, general operation and web console, configuration of diagram types, supported formats, import / export, integration with other services, connectors.
Cloud Data Studio
- Use cases, configuration, networking, security, general operation and environment, and integration with other services.
Cloud Data Lab -
Use cases, general operation and web console, types of diagrams, supported formats, import / export and integration with other services.
Kubernetes  Engine -
Use cases, architecture, clustering and integration with other services.
Kubeflow -
Use cases, architecture, environment configuration, Kubernetes.
Apache Airflow
- Use cases, architecture and general operation.
Cloud Functions
-
Use cases, architecture, configuration and integration with other services - such as
Cloud Storage and Pub / Sub, in Push / Pull mode.
Compute Engine
- Use cases, architecture, configuration, high availability, reliability and integration with other services.
App Engine -
Use cases, architecture and integration with other services.
Bibliography & essential resources
Google provides
a large number of
resources for the preparation of this certification
, in the form of courses, official guide book, documentation and mock exams. These resources are
highly recommended
, and in some cases, I would say essential.
The
Certification Preparation Course
, contained in the
Data Engineering Specialized Program
, includes an extra exam, lots of additional tips and materials and labs - using the external Qwik Labs tool.
GCP Certification Guide
Google Doc
s
Practice exam
Readiness course
–
highly recommended, includes an additional practice test.
Data Engineering, Big Data and ML on Google Cloud
Thirteen GCP Reference Architectures
Bibliography (selection) that I have used for the preparation of the certification
As I have previously indicated,
I find the Google courses on Coursera to be excellen
t, as they combine a series of
short videos, reading material, labs, and test questions
, thus creating
a very dynamic experience
. In any case, they
should only be considered as a starting point
, being necessary the deepening - according to experience - in each one of the domains using, for instance,
the excellent GCP documentation.
But you should not limit yourself to online courses. I can't hide the fact that I love books in general, and IT books in particular. In fact, I have a huge collection of books dating back to the 80s, which at some point I will donate to a local Cervantina bookstore.
Books provide
a deeper and more
dynamic experience than videos
, which can be a bit monotonous if they are too long - as well as being a much more passive experience - like watching TV. The ideal is the
combination of audiovisual and written media
,
thus creating your own learning path
.
Laboratories
Data Science on GCP Quest
Data Engineering Quest
Image #4  - Data Lake based upon Cloud Storage - owned by GCP
Part of the job as a
Data Engineer
consists of creating, integrating, deploying and maintaining
data pipelines
, both in batch and streaming mode.
The
Data Engineering Quest
contains several labs that introduce
the creation of different data transformation,  IoT, and Machine Learning pipelines
, so I find them excellent exercises - and not just for certification.
Is it worth?
The level of certification is advanced,
and in general, it should not be the first cloud certification to obtain. It
covers a large amount of material and domains
, so tackling it without a certain level of prior knowledge
can be quite a complex task.
If we compare it with the mirror certification on the AWS platform, it covers almost twice as much material, mainly due to the
inclusion of questions about the Machine Learning / Data Science domain
- which in the case of AWS have been eliminated, to be included in its own certification. Therefore, it is like taking two certifications in one.
Is it worth? of course, but not as a first certification - depending on the experience provided.
Certifications
are a good way, not only to
validate knowledge externally
, but to
collect updated information,
validate good practices and consolidate knowledge
with real practical cases (or almost).
Good luck to you all!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Burnout to Breakthrough: Rediscovering My Passion in Tech and Writing
Kasie Udoka Success -
Nov 22
How to find businesses without websites
Amin I. -
Oct 20
The Last Saree: Connoisseurship in the Age of AI
Jen Looper -
Nov 19
The intersection of architecture and coding
Agbo, Daniel Onuoha  -
Nov 23
Adolfo Estevez
Follow
AWS Evangelist | 13 x AWS Certified | 3x GCP | Serverless | Analytics | Strategist |
Location
Spain
Education
Electronics & Computer Sciences Degree
Work
AWS Evangelist at Everis / NTTData
Joined
Mar 28, 2020
More from
Adolfo Estevez
AWS Data Analytics Certification, is it worth?
#
aws
#
architecture
#
career
#
machinelearning
The Reference Architecture Disappointment
#
architecture
#
career
#
productivity
#
aws
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
One assert per test - What is that about? - DEV Community,"One assert per test - What is that about? - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Christian Baer
Posted on
May 16, 2018
• Edited on
Mar 16, 2019
One assert per test - What is that about?
#
unittests
#
review
#
codequalitity
#
collaboration
A colleague outside my team asked me and other colleagues to review a merge request. Although we try to hone our reviewing skills, it doesn't happen all too often that we invite colleagues outside of our specific teams to review. This was one of the cases and I appreciated the opportunity to give my two cents.
I was in the middle of my review when the colleague came by and asked for a first impression. I had looked briefly into the sources so my answer was brief as well: ""I don't like your unit tests"". When he inquired further why I didn't like them my answer was ""Because they are shit."" I know it sounds harsh - I am not a mean person, but I'm blunt and tend to exaggerate. Thankfully my colleague knows that and we agreed to talk again when I've had enough time to figure out why I didn't like the tests.
I sat down and took some time to figure out what bothered me and I want to share it with you.
More Than One Assert Per Test
One thing that was clear to me when I looked at the code, was the use of more that one assert per unit test. I'm not a fan of rules without reason so I had to have a closer look to make sure that it wasn't gut feeling. This is a broadly discussed topic, but I want to add my two cents.
One reason that seems to come up every time the internet discusses this topic is:
You won't see if there is something wrong with the second assert if the first one fails.
See here
Although the argument is true, I find it to be weak and tend not to use it. I don't consider it a problem.
I have a different gripe with it, and I've drawn two examples that I want to share with you to illustrate.
For privacy and brevity I simplified the tests, but let the essence intact.
public
void
testIfRegistryGetsValuesCorrectly
()
{
Registry
a
=
new
Registry
();
// Imagine some setup code here.
// originally there was a before-each method
assertThat
(
a
.
getEntryByKey
(
""RandOMstRIng""
),
equals
(
a
.
getEntryByKey
(
""randomstring""
)));
assertThat
(
a
.
getEntryByKey
(
""   rand   oms.tri ng""
),
equals
(
a
.
getEntryByKey
(
""randomstring""
)));
assertThat
(
a
.
getEntryByKey
(
""randomst37ring""
),
equals
(
a
.
getEntryByKey
(
""randomstring""
)));
}
The test works fine and meets the requirements. Cut out all non-characters of the key and make it lowercase before you ask for the value. Everything seems fine.
The problem becomes clear when you touch the tested method and break the 'testIfRegistryGetsValuesCorrectly' test.
As a developer working on the implementation, you're not going to find out what doesn't work any more. At least not at first sight.
You will understand that the test broke, this is what the name tells you.
I suggested refactoring the test to the below code.
public
void
testIfRegistryIgnoresCases
()
{
Registry
a
=
new
Registry
();
// Imagine some setup code here.
// originally there was a before-each method
assertThat
(
a
.
getEntryByKey
(
""RandOMstRIng""
),
equals
(
a
.
getEntryByKey
(
""randomstring""
)));
}
public
void
testIfRegistryIgnoresNonCharacters
()
{
Registry
a
=
new
Registry
();
// Imagine some setup code here.
// originally there was a before-each method
assertThat
(
a
.
getEntryByKey
(
"" 3  ra8nd   oms.tri ng""
),
equals
(
a
.
getEntryByKey
(
""randomstring""
)));
}
With these changes in place, you will understand the problematic behavior with a quick glance.
Dots are now allowed? You know what to do:
You broke one well defined test
You fix the test according to new requirements
You
rename
the test
Explicit Naming
Before continuing I'd like to present a second example to illustrate. Mind you: I simplified the tests, but let the essence intact.
public
void
testConstructorOneWorksCorrectly
()
{
SpecialObjectClass
a
=
new
SpecialObjectClass
(
""someID""
,
""attribute1""
,
""attribute2""
);
assertThat
(
a
.
getID
(),
equals
(
""someID""
));
assertThat
(
a
.
getFirstAttribute
(),
equals
(
""attribute1""
));
assertThat
(
a
.
getSecondAttribute
(),
equals
(
""attribute2""
));
}
public
void
testConstructorTwoWorksCorrectly
()
{
SpecialObjectClass
a
=
new
SpecialObjectClass
(
""someID""
,
""attribute1:attribute2""
);
assertThat
(
a
.
getID
(),
equals
(
""someID""
));
assertThat
(
a
.
getFirstAttribute
(),
equals
(
""attribute1""
));
assertThat
(
a
.
getSecondAttribute
(),
equals
(
""attribute2""
));
}
public
void
testConstructorTwoWorksCorrectlyWithoutSecondParameter
()
{
SpecialObjectClass
b
=
new
SpecialObjectClass
(
""someID""
,
""attribute1""
);
assertThat
(
b
.
getID
(),
equals
(
""someID""
));
assertThat
(
b
.
getFirstAttribute
(),
equals
(
""attribute1""
));
assertThat
(
b
.
getSecondAttribute
(),
equals
(
""attribute1""
));
}
The problem with the above example is similar, but not as obvious. The complex logic lies in the second and optional third parameter. The ID parameter (""someID"") is trivial and distracts more than it helps in each of the test cases. If something breaks in the handling of the ID parameter the three unit tests will break for the same reason.
I have two problems with that:
It violates the idea behind a unit test, which is to test single isolated units
It confuses the poor developer changing the tested code in the future. (Who could also be the author itself after a couple of months)
I suggested to refactor to the following form:
public
void
testConstructorSetsID
()
{
SpecialObjectClass
a
=
new
SpecialObjectClass
(
""someID""
,
""attribute1""
,
""attribute2""
);
assertThat
(
a
.
getID
(),
equals
(
""someID""
));
}
public
void
testConstructorSetsBothAttributes
()
{
SpecialObjectClass
a
=
new
SpecialObjectClass
(
""someID""
,
""attribute1""
,
""attribute2""
);
assertThat
(
a
.
getFirstAttribute
(),
equals
(
""attribute1""
));
assertThat
(
a
.
getSecondAttribute
(),
equals
(
""attribute2""
));
}
public
void
testConstructorTwoSplitsAttributesAtTheColon
()
{
SpecialObjectClass
a
=
new
SpecialObjectClass
(
""someID""
,
""attribute1:attribute2""
);
assertThat
(
a
.
getFirstAttribute
(),
equals
(
""attribute1""
));
assertThat
(
a
.
getSecondAttribute
(),
equals
(
""attribute2""
));
}
public
void
testConstructorTwoSetsSecondAttributeSameValueAsTheFirstIfNotGiven
()
{
SpecialObjectClass
b
=
new
SpecialObjectClass
(
""someID""
,
""attribute1""
);
assertThat
(
b
.
getFirstAttribute
(),
equals
(
""attribute1""
));
assertThat
(
b
.
getSecondAttribute
(),
equals
(
""attribute1""
));
}
My naming here may not be perfect, but it gets the point across. And as you may have noticed, the
one
word that was present in
all
the unit tests is now gone: ""
Correctly
""
This was my small epiphany.
As a word, ""Correctly"" should never be part of a test case's name. It does not provide any information at all.
Having found out my two gripes with the tests, I came up with a pattern for myself:
test case with multiple assertions -> look out for this dangerous word (""
Correctly
"") and change test case accordingly
dangerous word in the title -> look at the assertions to figure out what this test does and change the name
That was my learning of the day: There are words that should not be part of any test case's name, and it's: ""Correctly"".
There might be more. Look out for them!
P.S.: The colleague that I spoke of in this article, was asked to proofread the article. Besides helping a lot with proper English, he also added a thought of his own. ""As developers we often think about naming, and sometimes we seem to miss the most obvious points - or we tend to become lazy.""
Top comments
(3)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Alain Van Hout
Alain Van Hout
Alain Van Hout
Follow
A software developer with a passion for architecture and an affinity for Java and JavaScript
Location
Antwerp
Joined
Jun 17, 2017
•
May 16 '18
Dropdown menu
Copy link
Hide
I generally try to keep a (unit) test focused on ‘one concern’. Sometimes though, that concern is ‘this input flow should result in that bundle of outputs’.
Couldn’t agree more about the importance of naming. Semantics matter.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Christian Baer
Christian Baer
Christian Baer
Follow
Professional Software Developer, mainly in Java. Also a little bit of JavaScript, CSS, HTML, XML and nearly everything else needed in WebDev
Joined
Mar 30, 2017
•
May 16 '18
Dropdown menu
Copy link
Hide
Thanks for your comment. Sometimes this might be the right choice. As I tried to express I am not an extremist when it comes to this topic. This 'bundle of outputs' might just come in the way of having clear tests with having clear names. So look out for your bundle size ;)
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Alain Van Hout
Alain Van Hout
Alain Van Hout
Follow
A software developer with a passion for architecture and an affinity for Java and JavaScript
Location
Antwerp
Joined
Jun 17, 2017
•
May 17 '18
Dropdown menu
Copy link
Hide
Yes indeed :).
For context, I'm mostly talking about (fairly extensive) API tests, which generally involve setting up some things (via API calls), making a single specific API call with parameters that reflect the test's concern, and then verifying that the call has had the effect it needed to.
The latter can relate to for example
when updating a resource, you'd expect the value(s) that were updated to have in fact been updated and the remainder to have stayed unaltered, with potentially some minor side-effects like the resource's timestamp having changed or its version to have incremented
when moving a resource from one collection to another, you'd expect it to no longer be present in the first collection, to be present in the second collection, and possibly that the resource itself has a reference to the second collection rather than the first
Both of those involve multiple asserts, but really only a single concern.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Hare and Tortoise - To Find Start of Loop in Linked List
Lahari -
Dec 12
Calendly and Accessibility Problems
Chris  -
Dec 12
Native Observables, Rx7, Rx8 and the observable that doesn't exist yet
Dario Mannu -
Dec 12
the Accessibility Overlay Scandal
Chris  -
Dec 12
Christian Baer
Follow
Professional Software Developer, mainly in Java. Also a little bit of JavaScript, CSS, HTML, XML and nearly everything else needed in WebDev
Joined
Mar 30, 2017
More from
Christian Baer
Don't let programmers alone
#
codereview
#
collaboration
#
codequality
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
List of 54 UK companies hiring for tech internships now - DEV Community,"List of 54 UK companies hiring for tech internships now - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Piotr Bryła
Posted on
Feb 24, 2023
List of 54 UK companies hiring for tech internships now
While crunching a lot of tech market data for my side-project –
www.techleonardo.com
, I had an opportunity to create a list of 54 UK companies hiring now for tech internships, grouped by city and tagged with tech stack by my ML model.
London
Adarga
Keywords: Artificial Intelligence, Data Science, Machine Learning, Natural Language Processing, Security
psKINETIC
Keywords: Agile, Artificial Intelligence, Data Analysis, Data Analytics, Automation, Data Modelling, Object-Oriented Programming, Problem Solving, Quality, SQL, Unit Testing
Numberly
Keywords: Data Analysis, Cloud, Deployment, Digital Marketing, Marketing, Salesforce
Siemens
Keywords: Automation, Documentation, Mathematics, XML
S&P Global
Keywords: Agile, Data Analytics, Big Data, C#, Deployment, Java, Object-Oriented Programming, Pair Programming, Quality, Scala, Test, Testing
3H Partners
Keywords: Data Analysis, Data Analysis, Databases, Microsoft Excel, Go, Microsoft Excel, PowerPoint, Problem Solving, Research, SAAS, VBA
Thentia
Keywords: Artificial Intelligence, Angular, Automation, Big Data, C#, Cloud, Databases, Deployment, Design Patterns, ETL, Git, Java, Javascript, JavaScript, MongoDB, MySql, PHP, Predictive Analytics, Python, Quality, SAAS, SQL Server, Test, Version Control System
Cirrus Logic
Keywords: Algorithms, Algorithms, Automation, C#, C++, Continuous Integration, Data Structures, Deployment, Git, Java, JavaFX, Python, Ruby, Testing, User Interfaces, Version Control
Carto
Keywords: Data Analysis, CRM Systems, Data Science, Marketing, Research, SAAS, Salesforce
Commerzbank AG United Kingdom
Keywords: Mathematical, Python, Quality, VBA
Spotify
Keywords: Authorization, Backend, Java, Mathematics, React, Statistics, User Experience
TradingHub
Keywords: Data Analytics, Big Data, C#, Data Analysis, Object-Oriented Programming, Python, SQL
Jane Street
Keywords: Automation, Debugging, Express, Research
BAE Systems Digital Intelligence
Keywords: Agile, Android, Automated Testing, C#, Espresso, Go, Java, Marketing, Mdm, Oop, Security, Test, User Interfaces
Sports Interactive
Keywords: Architecture, C++, Debugging, Problem Solving, Version Control
Coveo
Keywords: Backend Development, Flow, Full Stack, Fullstack, Go, Jump, React, Scrum, SQL, Test, TypeScript, User Experience
Beacon Platform
Keywords: Algorithms, Data Analytics, Architecture, AWS, Azure, Cloud, Cloud Computing, Data Structures, Databases, Documentation, Full Stack, NoSql, Python., Quality, Risk Management, SQL, Test
OpenFin
Keywords: Agile, Automation, Cross-Platform Development, Deployment, Documentation, Electron, Progressive Enhancement, React, Security, Styled Components, Testing, TypeScript, User Interfaces, User Experience, Version Control System, Web Development, Webpack
Talan
Keywords: Artificial Intelligence, Big Data, Internet of Things, Marketing, Quality
Pangaea Data
Keywords: AWS, CSS, Databases, Full Stack Development, HTML, Javascript, Natural Language Processing, Python, React, Research, Test, User Interfaces, User Experience, User Interface, User Experience
The Bank of New York Mellon Corporation
Keywords: Agile, Artificial Intelligence, Data Analytics, CSS, Data Warehouses, Databases, Java, Javascript, Mathematics, Networking, Networks, Problem Solving, Python, Robotics, Test, Test Plan
Crédit Agricole CIB
Keywords: C#, Documentation, Microsoft Excel, Security, SQL, VBA
Amazon UK Services
Keywords: Architecture, CSS, Design Patterns, Experience Design, HTML, Javascript, Marketing, Networking, Security, User Interfaces, User Experience
Bitpanda
Keywords: Agile, Data Analytics, Architecture, Automation, AWS, Bitcoin, Cloud, DevOps, Documentation, Ethereum, Go, Graphql, Infrastructure As Code, Javascript, Microservices, Node.Js, Quality, Security, Terraform, TypeScript
Click Do
Keywords: Digital Marketing, Marketing, Search Engine Optimization
Softwire
Keywords: Artificial Intelligence, Go, Spring, Testing
Circle
Keywords: API, Architecture, AWS, Bitcoin, Cloud, Continuous Delivery, Data Engineering, Deployment, Ethereum, Flow, Go, Google Cloud, Java, Kubernetes, Microservices, Microsoft Azure, NoSql, Quality, Rest, Scaling, SQL, Test, User Experience
Northampton
Barclays
Keywords: Networking, Networks
Bristol
Immersive Labs
Keywords: Agile, Critical Thinking, Research, Security, Test
Arup
Keywords: Data Analytics, Architecture, Express, Flair, Mathematics, Networks, Project Management, Research, Statistics
K2 Management
Keywords: Data Analysis, Flow, Mathematics, Python, Quality, Research
Uxbridge
Canon
Keywords: CRM Systems, Go, HTML, Marketing, PowerPoint
Remote
NodeReal
Keywords: AWS, CRM Systems, Data Analysis, Marketing
RusHour
Keywords: Marketing, Problem-Solving, SAAS, Test
NodeReal
Keywords: Data Analytics, AWS, Marketing, Research
Oxford
Adaptix
Keywords: Data Analysis, Matlab, Problem Solving, Python, Quality, Research, Scikit
Brighton
Pensions Regulator
Keywords: Agile, Agile Methodologies, C#, DevOps, Embedded, Problem Solving, Python, Security, SQL
Edinburgh
Infographics UK
Keywords: .Net, .Net Core, Active Directory, Agile, Architecture, Asp.Net, Azure, Azure DevOps, C#, Cloud, Cloud Computing, DevOps, Docker, Git, Grasp, HTML5, Hyper-V, IaaS, Javascript, Load Balancing, Microservices, Microsoft Azure, Networking, PaaS, Quality Assurance, Quality Assurance, Scrum, Spring, SQL Server, Testing, TypeScript, User Interfaces, Windows Server
Codeplay
Keywords: Artificial Intelligence, C++, Computer Vision, Configuration Management, CPU, GitHub, Research, Tensorflow, Test
Glasgow
MathWorks
Keywords: Deep Learning, Documentation, Matlab, Quality, Research
Stantec
Keywords: Architecture, Project Management, Quality, Research
Liverpool
Lucid Games
Keywords: Unreal Engine, C#, Games Programming
Ipswich
RSM
Keywords: .Net, Asp.Net, C#, CSS, Design Patterns, Git, HTML, Integration Testing, Javascript, MVC, Networks, Quality, Research, Solid Principles, SQL Server, Test, Version Control System
Romford
ALPHANOVE CONSULTANCY
Keywords: Algorithms, Architecture, Data Structures, Java, Javascript, Microservices, Problem Solving, Test, Web Development
Sheffield
Autodesk
Keywords: Agile, Cloud, Continuous Integration, Cypress, Deployment, Javascript, Jenkins, Quality Assurance, Quality, Quality Assurance, React, Redux, Requirements Gathering, Test, Testing, User Interface
Cambridge
VividQ
Keywords: Analytical Skills, Augmented Reality, Linux, Problem Solving, Quality Assurance, Quality, Research, Testing, Unity
ValMIND
Keywords: Agile, HTML, JQuery, MVC
ARM
Keywords: Architecture, Automation, C++, Embedded, Git, Linux, Mathematics, Networking, Python, Quality, Research, Robotics, Testing, Version Control
Cadence Design Systems
Keywords: Algorithms, C++, Cloud, Design Systems, Internet of Things, Problem Solving, Python, Research
Horsham
Honeywell - Many internships positions
Keywords: Agile, Algorithms, C#, C++, Containers, Docker, Embedded, HTML5, Internet of Things, Java, Quality, Scrum, Security, Security, Testing, Web Components
Leeds
Arup
Keywords: Data Analytics, Express, Flair, Mathematics, Networks, Research, Statistics
Bath
3ADAPT
Keywords: Data Analysis, Benchmarking, Carbon, Flow, Full Stack, Research
Southampton
British American Tobacco
Keywords: Data Analysis, Documentation, Microsoft Excel, Networking, Project Management, Research, Test, Testing
Saltaire
Cimlogic
Keywords: .Net, Data Analytics, ASP .Net, C#, Code Review, CSS, HTML, Problem Solving, Quality, React, SQL Server
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Just a Number please
KooiInc -
Dec 12
Cool post that looks at a couple of things that CouchDB does best. Highly recommend if you’re curious about CouchDB’s unique replication capabilities.
Maddy -
Dec 12
A High-Level Overview of Reflector Oracle Protocol
Francis Ifegwu -
Dec 12
Reflector Oracle Protocol Documentation Improvement Suggestions
Francis Ifegwu -
Dec 12
Piotr Bryła
Follow
🇵🇱 🇬🇧 JavaScript ❤️ Software Engineer 💻 Creator of https://www.planyoursprint.com
Location
London
Work
Software Engineer
Joined
Feb 21, 2023
More from
Piotr Bryła
Are you new to coding? 💻 Here's a list of 5 essential skills you should learn before getting your first job.
#
webdev
#
beginners
#
programming
#
codenewbie
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
The New Property Technology & How Big Data Disrupts Real Estate - DEV Community,"The New Property Technology & How Big Data Disrupts Real Estate - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
AscendixTech
Posted on
Jan 17, 2022
• Edited on
Mar 15, 2022
• Originally published at
ascendixtech.com
The New Property Technology & How Big Data Disrupts Real Estate
#
todayilearned
#
ai
#
productivity
#
learning
Originally published at
Ascendix Tech's Blog
.
Real Estate has always been a stranger in innovation and technology. Nevertheless, the Covid-driven urge for new practices and a number of tech-minded property tech startups, popping up here and there, have started to bridge the decades-long gap between Real Estate and Tech.
Today, let’s have a look at how Proptech has changed the Real Estate industry and what new solutions it has brought.
Proptech Overview 2022: What Companies Say
Before getting into a detailed discussion of the key technology trends in Real Estate, let us outline the up-to-date state of Property tech and what it has already brought to companies worldwide:
80 percent
of businesses using Property technologies have seen
a boost in the quality and efficiency
of services and operations, while 70 percent have shown the increase in decision-making and finances.
The total investments
in Proptech are steadily growing, with a staggering
$9.7 billion in H1 of 2021
against $8.8 billion in H1 2020, despite the recent decrease in the number of real estate tech startups founded.
Businesses pour their funds into PropTech innovations because they expect
streamlined efficiencies (65%), cost-reduction (47%), and improved decision-making (44%)
.
42% of
Statista respondents
believe that the Smart Building solutions are the future of Proptech Real Estate.
Surprisingly, CRE (short for Commercial Real Estate) is slightly behind the curve, as
78 percent
of brokers and agents still handle their back-office activities manually.
Proptech Disruption: The New Technology
As you can see, most companies report a positive impact of property technology innovations on their efficiency and, as a result, profitability. Let’s now check what are the key technology trends that contributed to such a shift in the industry.
To get a clear picture of the new technology in property management in 2022, we’ve analyzed a few reports for the Proptech real estate sector made by the world’s thought leaders, like Forbes, KPMG, and JLL. Here are the technologies they single out:
Proptech Big Data Analytics & digitalization of property data sets
Big Data is about collecting and structuring big data sets about all kinds of assents, from residential properties to massive corporate facilities and telecommunications. Having a structured view of all the information, real estate companies and individual Proptech realtors can come up with more accurate pricing, more insightful market research, and better potential risk management.
AI and machine learning for back-office automation
Whereas Big Data algorithms are there to structure your data, AI and machine learning are all about making your data sets more actionable. Whether it be a sequence of personalized e-mails to your clients based on their personal details or advanced search prediction algorithms, AI will help you do the job!
Virtual reality for better home buying experience with no geographic boundaries
Virtual reality is actively entering a rather traditional Real Estate industry as the new intelligent way of showing homes to tenants or augmenting the existing interior with furniture (like in roOomy).
Internet of Things (IoT) for predictive maintenance
IoT, or The Internet of Things, means intelligent ecosystems of devices and sensors that are constantly exchanging information about an asset to track its overall condition and predict defects. Just to illustrate, property owners can monitor a building’s temperature, security, and maintenance from the screen of their mobile devices and computers.
New technologies for sustainable construction
Sustainability is on everybody’s lips today, so our construction, and especially our appliances, need to get smarter every day. There already is a great pool of sustainable technologies taking up the central place in our home, including SmartGrid refrigerators, washing machines, dishwashers, and micro ovens, however, there is still a lot to come!
Chatbots for better customer experience
Developed on top of smart AI algorithms, chatbots play a vital role almost on every website. Chatbots’ core goal is to improve customer satisfaction, but they can also offload piles of manual work and give actionable insights into your customers’ pain points.
Proptech Big Data Analytics & How It Has Changed the Real Estate Industry
Many confess that from the above-mentioned technologies, the one on the top of the industry disruption will be Big Data. Here is the main reason why the world’s key visionary leaders choose Proptech Big Data:
as your business grows, your data is inevitably expanding, transforming into Big Data sets
. The way you utilize Big Data can become the key standpoint for your success in Real Estate.
We outlined some of the key benefits this new technology can bring to your business:
Better Analytics
(property’s future risks and opportunities based on geography and other variables).
Precise Property Evaluations
(forecasts of property prices in x-year time with more than 90% accuracy).
Improved Home Prospecting, Marketing, and Sales
(target audience refinement through granular market research),
Data-driven property development
(predictions of where it is best to build a new building and what amenities would fit the potential residents)
Data-driven risk prediction
(clear predictive analyses of the risks of investing in certain buildings and projects).
Automation of back-office processes
(digitalization of records and tons of paperwork).
Final Thoughts
Real Estate is stepping into the new era of technology and innovation, and we’re lucky to witness this revolution. More than that, you can commit to the industry-wide technological shift by employing the above-listed solutions, whether it be robust VR for home tours or Big Data for managing tons of records. Give it a try, and maybe you’ll be the next to report a spike in your sales or efficiency.
If you want to get more insights into Proptech, check the whole article on
What is Proptech and What It Has Brought to the Real Estate Industry
.
If you already have one of these technologies in your Real Estate arsenal, please share your personal experience of how the new prop tech solution transformed your business.
Latest comments
(2)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Salu Bhai
Salu Bhai
Salu Bhai
Follow
Joined
Sep 28, 2024
•
Nov 28
Dropdown menu
Copy link
Hide
The impact of New Property Technology and Big Data on the real estate industry is truly transformative. With the ability to analyze vast amounts of data, technology is revolutionizing how properties are bought, sold, and managed. For instance, tools like those outlined on this
linkedin.com/pulse/how-transfer-fi...
website demonstrate how seamless technology integration can streamline operations. Big Data enables predictive analytics, improving market forecasting, while Property Technology enhances user experiences and investment strategies. This shift is not only increasing efficiency but also helping real estate professionals make data-driven decisions that were once unimaginable. Exciting times ahead!
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
kendel
kendel
kendel
Follow
fdlgjdflgjlkdfsj;glk;lfkdj;ldkfj;lkdvj;lkfdjfdlkjkfld
Joined
Apr 26, 2024
•
Apr 26
Dropdown menu
Copy link
Hide
The convergence of new property technology and big data is revolutionizing the real estate industry. With advancements in data analytics and artificial intelligence, stakeholders can now make more informed decisions about property investments, pricing strategies, and market trends. For instance, platforms utilizing big data can offer insights into buyer preferences, enabling developers to tailor their offerings accordingly. In the realm of vacation rentals, such as
Corfu villas
, big data can optimize marketing efforts by identifying target demographics and refining advertising strategies. Ultimately, this integration of technology and data not only enhances operational efficiency but also facilitates more personalized and satisfying experiences for both buyers and renters in the real estate market.
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
New AI Breakthrough Makes Self-Driving Cars 15x Faster and Safer with Truncated Diffusion Model
Mike Young -
Dec 1
Why Are We Still Learning in 2D When the World is 3D?
Jeffrey Nwankwo -
Nov 30
10 Types of AI - Detailed Guide
Vijendra Yadav -
Nov 26
How to Toggle Auto-Completion in Cursor Editor 🎉
Takehiro_Yamazaki -
Nov 26
AscendixTech
Follow
We are Proptech software developers with 17-year experience in building custom apps for real estate. Cloud, data science & analytics, location & proximity, etc. Our GitHub: https://github.com/ascendix
Location
Dallas, Texas
Joined
Jan 28, 2021
More from
AscendixTech
Optimizing Commercial Property Value with Effective Space Management and Planning
#
productivity
#
devops
#
database
#
automation
A Case Study in Developing a Recruitment App: Creating an Applicant Tracking System
#
webdev
#
tutorial
#
productivity
#
devops
Digital Transformation Roadmap for Legacy Real Estate Systems
#
webdev
#
todayilearned
#
writing
#
proptech
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Embracing Failure: A journey from Tester to Tech Lead - DEV Community,"Embracing Failure: A journey from Tester to Tech Lead - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Craig de Gouveia
Posted on
Aug 17, 2021
• Edited on
Aug 23, 2021
Embracing Failure: A journey from Tester to Tech Lead
#
career
#
dev
#
test
#
techlead
It started with one of the biggest failures of the last few decades, the sub-prime mortgage market, and several other factors that caused the failure of the global financial system. This isn't ideal when you're contracting for companies that earn most of their money installing network infrastructure for banks 🙋🏻‍♂️.
It was 2008, and I lived in London, 24 years old and recently married (too young), having been in the U.K. since 2005. I effectively worked as a labourer, just with extra steps, meaning I installed network cabling on various building sites. The job was pretty great for someone without a university education; it paid well. I got to work in some exciting places (the McLaren F1 development facility was definitely a highlight), and it was technically demanding. The point is, I enjoyed the job and was good at it. Unfortunately, the economy had other plans. Right, okay, cool, but...
Isn't this supposed to be about the tech industry?
So we managed to stay in London until the end of 2008 through the recession. I had not worked for about 4 months at this point, and remaining in London on a single salary for two people just wasn't feasible. We somewhat randomly decided to move to Newark-on-Trent, having never set foot in the place; it's a great little town, though. This put me in the Midlands with no work and minimal savings; it was very hard not to feel like I had failed at doing London in my 20's at this point. I needed to find a job fast, so I wasn't expecting anything significant and kept my expectations very low. I guess this is where the story actually begins ✨.
I interviewed for a job in Lincoln at a software company as a support engineer. Honestly, I wasn't expecting to get the job. After being stonewalled by pretty much everyone, I was happy to just get the opportunity to interview. I got the job, possibly because I have spent most of my life tinkering with computers and was already providing tech support to my family for free. I started on £12k, which was a £13k pay cut 😅.
I had just begun doing something that I loved. I highly doubt that would have happened without leaving London; all it took was a global financial crisis, easy. It was clear that I had developed a skill for this kind of work. I moved from 1st to 2nd line support pretty quickly due to the technical nature of the cases I was dealing with. Shortly after that, I moved to the Quality Assurance team. And...
What does testing have to do with failure?
So besides the obvious ""this thing failed x tests for y reasons"", there's a subtler and more insidious issue that most new testers experience. When you're new to a job, new-ish to a culture (I'm South African) and new to a project, understanding how to navigate the conversation of ""there is a problem with x"" is hard. Approaching the devs feels a lot like the response is going to be ""
Oh? You're Approaching Me?
"" regardless of what the person is actually like, it feels like you're guaranteed to fail. It is, however, your literal job to communicate these issues, so embracing that feeling is necessary for everyone to succeed.
Suppose I didn't welcome that initial discomfort as motivation to learn how to present helpful, actionable information as transparently as possible? I wouldn't have lasted very long as a tester. I believe it is this skill that separates good testers from great ones. I decided that the best way for me to do this would be to tell people precisely what part of the application code failed and for what reason. It meant getting much more in-depth and hands-on with code than many testers ever do.
It's around 2013, I have been testing desktop applications initially written in Delphi, and we are most of the way through migrating everything to C# and .Net. These applications are integrated with large supplier catalogues using SOAP web services backed by mainframe systems. I learnt a lot here, but I didn't realise at the time that the most valuable lesson was not to allow complacency to prevent me from moving on from something comfortable. I would need to learn this a second time before it sunk in.
Next, I take a job in Nottingham with a company that is SaaS-based with some mobile offerings. The tech is a lot more exciting, and the pay is much better. The commute to the new place kind of sucks, though. Lincoln had low salaries for tech jobs, which is probably why
Rockstar had its testing dungeon there
. Over the next year, I mostly get to grips with new ways of working (using ""proper"" agile methods) and a complex system with challenging release cycles.
This is also around the time my
marriage fails
; which was another source of valuable lessons and an opportunity to develop and grow as a person. One of the driving factors for moving to the city centre where I worked was having a commute that didn't suck. As a result, I had more time to evaluate where I was in my career. This evaluation kicked off a doom spiral (🤘🏻) that made me enrol on a part-time course with the Open University in Computer Science & Statistics. I also started spending a lot more of my spare time writing software; I wasn't getting a chance to do this as a tester. I felt like I had failed to make opportunities for myself and was course-correcting pretty violently.  I also began to look for new jobs, again much later than I should have.
In 2016 I started a new testing role for a Big Data company; I was absolutely stoked that I got the job. It came with a hefty pay rise, and I thought it was out of my league; they didn't; this was a recurring theme with how I valued myself. It was the largest company I had worked for by a
significant
margin. I had exposure to some incredible tech and phenomenal talent; the access to training was top-notch, and the pay was excellent. At this point, I was focusing on massive scale performance tests; I was writing more code than performing manual testing. Those years of testing had developed my ability to keep the big-picture structure of large-scale distributed systems in my mind while focusing on low-level detail. It is unconditionally one of my most valuable skills in designing systems.
I absolutely despised working there and only just forced myself to eke out a year despite all of what I have just mentioned. The problem is, large companies have a lot of space for people coast in. When you're going through a phase of immense self-development, it really kills your buzz to be met with that resistance to improvement. I felt like I had failed to crack working in a big corporation, but I learned what kind of culture I need to thrive. So...
What does software development have to do with failure?
After the foray into the corporate world, I started a limited company planning on contracting. I began working for a small insure-tech company instead, which was awesome.  I wanted to build cool things with exciting tech and was not disappointed; it was a great cultural fit. I created a test automation framework, contributed to open-source projects and contributed components to the platform that we were building. I learnt so much here, but more than anything, it just reinforced my love of solving problems with creative software solutions. I think it was here where I really stopped fitting into a ""Tester"" or ""Developer"" box.
REPL driven development is my preferred way to work, which is conveniently explained in
this article
where John Stevenson mentions getting fast feedback as a core benefit. This, in my opinion, is closely related to a systems design concept called
fail-fast
. To be horribly reductive in the pursuit of a point, detecting failure early on helps you avoid
sunk cost bias
. Acknowledging something isn't working, identifying elements that did work and moving on early is almost always, in my experience, the best approach. I view software development as a process of continuous failure that ultimately culminates in success. This approach is core to how I work. Back then, it meant building tools to enable people to get feedback on whether something was working as quickly and unobtrusively as possible. Today, it means evaluating options and making decisions as pragmatically as possible and communicating this in a way that lets people buy into the strategy.
So at this startup, the whole team was super motivated and passionate; the product and the timing were both excellent, which meant we had many interested investors and clients. However, the eagerness to please and the number of ""suitors"" we had for growth meant simply
too many
opportunities. We got pulled in multiple directions, ultimately leading to the company failing to capitalise on any of them, which is a shame. I had already moved on whilst the final steps towards this were being taken, which brings us to...
What does seemingly abandoning your career have to do with failure?
It was scary to decide to walk away from a company that I loved working for, where I was doing something that I loved. The prospect of going from a decent salary to a student loan was
terrifying
. I think that I must have gone back and forth about whether to do this about a thousand times before ultimately taking steps to do it. Having just finished the first year (two years part-time) of my Open University course, I weighed up my choices. I did not enjoy distance learning. I love learning, but something about doing it remotely just doesn't work for me. So I tentatively decided to send some applications out to universities offering full-time courses with in-person lectures.
I felt like getting a degree was somehow necessary to legitimise my transition from testing to software development. Even though I had already proven this by doing my job and doing it well, I often fail (or refuse?) to see evidence of my own ability. Of the 5 universities I applied to, 4 got back to me, 3 offered me a place, and 1 said no almost immediately. Amusingly, the one that rejected me was also the only one I contacted ahead of time for advice on whether it was a futile attempt; thanks, Nottingham 😘. Without that rejection, I wouldn't get the chance to spend my time focusing on deepening my knowledge and understanding of computer science or discovering a love for psychology, neuroscience and philosophy. I also wouldn't be living in Sheffield, easily one of my favourite places in the U.K. so far.
My enthusiasm didn't stop me from failing and needing to resit a maths module in my first year to progress. It turns out not doing ""serious"" maths for 14 years and then suddenly diving in is hard. There is nothing like failing at something you've staked your career on to light a fire under your arse. Going to university for the first time when you're in your 30's is tough. However, I feel like you're a lot more motivated to engage with the subject matter; perhaps you're in less of a rush to get out there and start working and earning money.
Speaking of earning money and my worries about not being able to eat on a student budget 😬. I built relationships with a few clients doing freelance work whilst working at the startup and used this to supplement my income. The work ranged from various web dev tasks to building a service that would run performance tests on point-of-sale systems at multiple sites worldwide and continually send results to a central hub. All fun challenges with unique quirks and constraints, with many failed attempts at building things, but ultimately successful. Having let go of the security of a fixed income really allowed me to reframe my thinking around how I earned money. This led to me finally making that move into contracting...
What does working for yourself have to do with failure?
Having a
Personal Service Company
requires some curious mental acrobatics. You are not an individual; you are a business, but at the same time, you are the owner and sole service deliverer for that business. As a creative (yes, we are), you put a lot of yourself into your work; it can be hard to separate them. Engaging with feedback on what you create and building on it separates good developers from great ones. When that feedback is delivered to a business and not an individual, well, let's just say the delivery of it is a
little less restrained
. This is, of course, a good thing once you get over the discomfort. Even better, it hones that ability to determine what is a good fit for you and how to walk away when something isn't.
Leading up to the summer break at university, a former client contacted me about some new work. I had helped them build a performance testing framework the previous year. They wanted me back to help define and build an automation strategy for a new stream of work. Unfortunately, the people initially involved in bringing me back moved on after a year to other projects. What we had discussed and agreed on changed quite drastically. New stakeholders with wildly different expectations got involved, and the relationship soured not long after that. In retrospect, that engagement should have ended after that initial year when everyone was happy. The following contract aligned perfectly in the end; it was a
much
better fit, and I continued to work with this client for some time on a few projects.
Managing a business throws several interesting new things in your way. It gives you a taste of the challenges of running something much larger and more successful and perhaps a much healthier appreciation of the work involved in doing that. Most importantly, though, it gives you an appreciation of what failure means and how it can shape your future positively.
I have shelved contracting and have taken a permanent role as a Principal Engineer and Tech Lead for a former client of my business. I did this because I believe that what they are doing is incredibly exciting. I get to work on a diverse set of projects solving unique and complex problems. I could have easily carried on contracting, but I absolutely love the people I work with. Every day, I am reminded how worthwhile the change was. Being somewhere because you want to be, not because you have to, is excellent. Cool story, bro, but...
What does failure have to do with me?
Increasingly we are becoming more comfortable with failure as an industry and a society, treating it as a learning opportunity, and this is a remarkable thing. That being said, I believe it is still important to share your experiences to possibly help others. What it means for you is one of two things. Either it will serve as a valuable source of inspiration if you are someone along a similar path and unsure about some risk or perception of things going wrong. Or you are in a similar position but feel that you have your own story to share, which could help someone, and the text above may inspire.
Writing this article has helped me gain a new appreciation for some aspects of my journey that I had forgotten about. It's also the first thing I have ever written for a public forum, so any feedback you can frame as a JoJo reference is gladly welcomed.
Failure never feels good while it is happening. I hope that this retelling of my career's meandering path will provide some compelling examples of all the ways seeming failure often results in new opportunities. Whether in understanding your role, direction, or self, embracing failure and allowing it to shape you will pay off. Just perhaps not as quickly or in the way you expect. It will inevitably lead to you being more resilient, capable and accepting of the interesting little curveballs life throws at you.
And remember:
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Some comments may only be visible to logged-in visitors.
Sign in
to view all comments.
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
The intersection of architecture and coding
Agbo, Daniel Onuoha  -
Nov 23
The best way to get better at writing code is...
Jake Lundberg -
Nov 28
Turn Job Descriptions Into Recruitment Video With AI
Dalu46 -
Nov 19
Believe in what you know already
Michelle Sanseverino -
Nov 19
Craig de Gouveia
Follow
A [design | tech | food | travel | outdoors] enthusiast; prone to opinions, they are my own.
Location
Sheffield, UK
Work
Principal Engineer and Tech Lead
Joined
Apr 26, 2017
Trending on
DEV Community
Hot
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
What is First Person Narrative on LinkedIn?
#
career
#
socialmedia
#
networking
#
careerdevelopment
Financial Post Mortem For The Commit Your Code Conference
#
webdev
#
javascript
#
opensource
#
career
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Best Practices for Data Security in Big Data Projects - DEV Community,"Best Practices for Data Security in Big Data Projects - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Aditya Pratap Bhuyan
Posted on
Oct 24
Best Practices for Data Security in Big Data Projects
#
bestpractices
#
bigdata
#
datasecurity
In today’s data-driven world, big data projects are becoming essential for organizations seeking to gain insights, enhance decision-making, and drive innovation. However, with the increased volume, variety, and velocity of data comes the heightened risk of data breaches and security vulnerabilities. This article outlines best practices for ensuring data security in big data projects, helping organizations protect their valuable information while complying with relevant regulations.
1. Data Classification: Understanding Your Data
Data classification
is the foundational step in any data security strategy. It involves categorizing data based on its sensitivity, value, and compliance requirements. By classifying data, organizations can determine which security measures are necessary.
Why It Matters
Classifying data helps prioritize resources and efforts. For instance, sensitive data such as personal identifiable information (PII) or financial records should be protected with more stringent security measures than less sensitive data.
How to Implement
Identify Data Types
: Catalog all data assets within your organization.
Create Categories
: Establish categories such as public, internal, confidential, and regulated data.
Assign Security Controls
: Based on the classification, implement appropriate security controls for each category.
2. Access Control: Limiting Data Exposure
Access control
is critical in preventing unauthorized access to sensitive information. By ensuring that only authorized personnel can access specific data, organizations can mitigate risks associated with data breaches.
Why It Matters
Implementing effective access controls reduces the risk of insider threats and ensures that users have the minimum level of access necessary to perform their job functions.
How to Implement
Role-Based Access Control (RBAC)
: Assign permissions based on user roles within the organization. Each role should have a defined set of permissions aligned with job responsibilities.
Multi-Factor Authentication (MFA)
: Enhance security by requiring additional verification methods for accessing sensitive data.
Regular Audits
: Periodically review access controls to ensure that only current employees have access and that permissions are up to date.
3. Encryption: Protecting Data Integrity
Encryption
is a critical component of data security, both at rest and in transit. It transforms readable data into an unreadable format, ensuring that even if data is intercepted, it cannot be understood by unauthorized parties.
Why It Matters
Data breaches can have severe consequences, including financial loss and reputational damage. Encryption serves as a safeguard, protecting sensitive information even if it falls into the wrong hands.
How to Implement
Data at Rest
: Use strong encryption algorithms (like AES-256) to encrypt data stored in databases or file systems.
Data in Transit
: Secure data as it travels across networks using protocols such as TLS (Transport Layer Security).
Key Management
: Implement robust key management practices to protect encryption keys, ensuring they are stored securely and rotated regularly.
4. Audit Logging: Tracking Data Access
Audit logging
involves maintaining detailed records of all data access and modifications. These logs provide valuable insights into user activities and can help identify unusual behavior.
Why It Matters
Audit logs are essential for compliance and forensic analysis. In the event of a data breach, logs can help determine how the breach occurred and which data was affected.
How to Implement
Comprehensive Logging
: Capture logs for all critical data access and changes, including user identification, timestamps, and actions performed.
Log Monitoring
: Use automated tools to monitor logs for suspicious activity, such as repeated failed login attempts or access to sensitive data by unauthorized users.
Regular Reviews
: Conduct regular reviews of audit logs to identify patterns or anomalies that may indicate security issues.
5. Data Masking and Tokenization: Protecting Sensitive Information
Data masking
and
tokenization
are techniques used to protect sensitive information, particularly in non-production environments. They allow organizations to use realistic data without exposing actual sensitive data.
Why It Matters
These methods help ensure that sensitive information is not exposed during development, testing, or analysis, reducing the risk of data breaches.
How to Implement
Data Masking
: Replace sensitive data with masked values while preserving the data's format and usability. For example, change real credit card numbers to a format like XXXX-XXXX-XXXX-1234.
Tokenization
: Replace sensitive data with unique identification symbols (tokens) that retain essential information without compromising security.
Non-Production Environments
: Use masked or tokenized data in non-production environments to minimize the risk of exposing sensitive information.
6. Secure Configuration: Hardening Your Systems
Secure configuration
involves setting up systems and applications with security best practices in mind. This includes both initial configurations and ongoing maintenance.
Why It Matters
Misconfigured systems are a common entry point for attackers. By ensuring that systems are securely configured, organizations can significantly reduce their attack surface.
How to Implement
Default Settings
: Change default passwords and settings on all devices and applications to reduce vulnerabilities.
Security Benchmarks
: Follow industry standards and benchmarks (such as CIS Benchmarks) for configuring systems securely.
Regular Updates and Patches
: Stay up to date with security patches and updates to address known vulnerabilities.
7. Network Security: Building a Secure Infrastructure
Network security
involves protecting the network infrastructure from threats that could compromise data integrity and availability.
Why It Matters
A secure network prevents unauthorized access and ensures that data remains confidential and intact.
How to Implement
Firewalls
: Deploy firewalls to monitor and control incoming and outgoing network traffic based on predetermined security rules.
Intrusion Detection Systems (IDS)
: Use IDS to monitor network traffic for suspicious activities and potential threats.
Segmentation
: Segment the network to isolate sensitive data and applications, reducing the impact of potential breaches.
8. Data Minimization: Reducing Risk Exposure
Data minimization
is the practice of collecting and retaining only the data that is necessary for a specific purpose.
Why It Matters
The less data an organization holds, the lower the risk of exposure in the event of a breach. Minimizing data collection also aids in compliance with regulations.
How to Implement
Assess Data Needs
: Regularly evaluate what data is essential for your operations and eliminate unnecessary data collection.
Retention Policies
: Establish and enforce data retention policies to determine how long data should be stored and when it should be deleted.
Regular Audits
: Conduct periodic audits to ensure compliance with data minimization practices.
9. Compliance: Adhering to Regulations
Compliance
with data protection regulations (such as GDPR, HIPAA, and CCPA) is not only a legal requirement but also a critical aspect of data security.
Why It Matters
Failing to comply with regulations can lead to significant fines and damage to an organization’s reputation. Compliance ensures that data is handled responsibly and ethically.
How to Implement
Understand Requirements
: Familiarize yourself with the relevant data protection regulations that apply to your organization.
Implement Compliance Measures
: Establish policies and practices that align with regulatory requirements, including data access controls and incident reporting.
Regular Training
: Provide ongoing training for employees on compliance requirements and best practices for data security.
10. Training and Awareness: Building a Security Culture
Training and awareness
are vital components of a comprehensive data security strategy. Educating employees about data security risks and best practices can significantly reduce human errors.
Why It Matters
Human error is a leading cause of data breaches. Regular training helps employees recognize potential threats and understand their role in protecting sensitive information.
How to Implement
Security Awareness Programs
: Develop training programs that cover topics such as phishing, password security, and data handling best practices.
Simulated Attacks
: Conduct simulated phishing attacks to help employees recognize and respond to real threats.
Ongoing Education
: Provide regular updates and refresher courses to keep security knowledge current.
11. Incident Response Plan: Preparing for Breaches
An
incident response plan
outlines the steps to take in the event of a data breach or security incident. Having a well-defined plan can minimize damage and restore normal operations quickly.
Why It Matters
An effective incident response plan enables organizations to respond swiftly to breaches, reducing the impact on operations and reputation.
How to Implement
Define Roles and Responsibilities
: Assign specific roles to team members in the event of a data breach.
Establish Communication Protocols
: Outline how to communicate with stakeholders, regulatory bodies, and the public during and after an incident.
Regular Testing
: Conduct drills and tabletop exercises to test the effectiveness of the incident response plan.
12. Third-party Risk Management: Vetting Vendors
With many organizations relying on third-party vendors for data processing and storage, managing
third-party risks
is essential for data security.
Why It Matters
Third-party vendors can introduce vulnerabilities that may compromise an organization’s data security. Proper vetting and management are critical to mitigating these risks.
How to Implement
Vendor Assessments
: Conduct thorough assessments of third-party vendors' security practices before engaging their services.
Contracts and SLAs
: Establish clear contracts that outline security expectations and responsibilities, including data protection measures and incident reporting.
Ongoing Monitoring
: Regularly review and monitor third-party vendors for compliance with security standards and contractual obligations.
Conclusion
Data security in big data projects is a multifaceted challenge that requires a comprehensive approach. By implementing best practices such as data classification, access control, encryption, and incident response planning, organizations can protect their sensitive information and minimize the risks associated with data breaches. As data continues to grow in volume and complexity, maintaining robust security measures will be essential for ensuring compliance and safeguarding organizational assets.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
The Need for Security in Firms:
Favour ogungbade -
Dec 10
Doctrine’s Collection filter method - a double-edged sword
Andrejus Voitovas -
Dec 11
Open Source LLMOps LangSmith Alternatives: LangFuse vs. Lunary.ai
Dmitrii -
Dec 11
Why Monorepo Projects Sucks: Performance Considerations with Nx
Jeferson F Silva -
Dec 11
Aditya Pratap Bhuyan
Follow
Aditya Pratap Bhuyan is an experienced IT professional with over 20 years in enterprise and cloud applications. With more than 40 industry certifications, he specializes in DevOps, cloud computing.
Location
Bangalore, India
Pronouns
He/Him
Work
Cloud Native Journey
Joined
Mar 24, 2024
More from
Aditya Pratap Bhuyan
Best Practices for Developing and Integrating REST APIs into Web Applications
#
rest
#
restapi
#
bestpractices
#
webapp
Best Practices for Using GROUP BY in MySQL for Converting Vertical Data to JSON
#
sql
#
mysql
#
json
#
bestpractices
Best Practices in Software Architecture for Scalable, Secure, and Maintainable Systems
#
architecture
#
attributes
#
bestpractices
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Apache Spark Unit Testing Strategies - DEV Community,"Apache Spark Unit Testing Strategies - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Sukumaar Mane
Posted on
Feb 28, 2022
• Edited on
Dec 5
Apache Spark Unit Testing Strategies
#
scala
#
programming
#
apachespark
#
bigdata
Hello, this article is moved to new link due to formatting issues on dev.to, new link ->
https://rovingdev.com/p/apache-spark-unit-testing-strategies
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Looking for some feedback on my team's new open-source secrets vault project.
Felipe Araujo -
Dec 11
Un senior destaca por su forma de pensar, no de codear
Khriztian Moreno -
Dec 10
Top 5 Open Source Projects You Must Explore Before 2025
Arindam Majumder  -
Dec 9
Why CSS Still Rocks in 2024
sharath mohan -
Dec 10
Sukumaar Mane
Follow
Coder (mostly backend with Big Data and cloud frameworks)
Joined
Feb 27, 2022
More from
Sukumaar Mane
What is '_spark_metadata' Directory in Spark Structured Streaming ?
#
apachespark
#
scala
#
java
#
bigdata
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Top Automation Testing Trends To Look Out In 2020 - DEV Community,"Top Automation Testing Trends To Look Out In 2020 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
himanshuseth004
for
LambdaTest
Posted on
Feb 28, 2023
• Originally published at
lambdatest.com
Top Automation Testing Trends To Look Out In 2020
#
qaautomationtools
#
mobileapptestingtools
#
softwaretesting
#
automation
Quality Assurance (QA) is at the point of inflection and it is an exciting time to be in the field of QA as advanced digital technologies are influencing QA practices. As per a press release by Gartner, The encouraging part is that IT and automation will play a major role in transformation as the IT industry will spend close to $3.87 trillion in 2020, up from $3.76 trillion in 2019.
Test automation is always at the forefront when it comes to software testing and is evolving rapidly. Every year we are being introduced to new
test automation
frameworks, test runners, libraries, methodologies, and more! In this article, we will look at the top automation testing trends for 2020.
If you’re new to Selenium and wondering what it is then we recommend checking out our guide —
What is Selenium?
Selenium 4 Will Be At The Centre-Stage Of Automation Testing
The selenium test framework is synonymously used for automated browser testing and
Selenium 4
is one of the most awaited versions of the framework. Selenium 4 not only has a number of feature improvements like better docker support, identification of parent frame, and better-formatted commands to access a Selenium Grid but also has a bulk of new features.
Some of the new features in Selenium 4 are the support of
relative locators,
better windows & tab management of web browsers, support for native Chromium driver for Microsoft Edge browser, and more. We have already tested the
alpha version of Selenium 4
and are quite bullish about what the stable release of Selenium 4 will bring to the table. The
latest Selenium version
should be the top automation testing trends in 2020 as it will be eagerly watched by all test automation enthusiasts.
Now you can perform manual or automated cross browser testing, including browser tests, on 3000+ browsers online with the best browser testing tool available. Our tool allows you to deploy and scale faster with the most powerful cross-browser testing tool online, designed specifically for comprehensive browser testing including
browser tests
.
Codeless Automation Testing To Rise Again In Popularity
Selenium automation testing is widely used as the Selenium framework is supported by popular programming languages like C#, Python, Ruby, Java, JavaScript, etc. Hence, the development of test cases related to automated browser testing requires programming knowledge to work with Selenium WebDriver or Selenium Grid.
Codeless automation testing
tools are built on AI technology and visual modeling allows accelerated creation of test cases that cater to Selenium automation testing. Using these tools, testers can create test scenarios without any coding knowledge and minimize the time spent on repetitive test cases. Increased adoption of codeless automation tools will be one of the automation testing trends in 2020 to watch out for.
LambdaTest also provides integrations with Codeless automation tools.
Need a great solution for cross browser testing on Safari? Forget about emulators or simulators - use real online browsers. Try LambdaTest to test on
safari browse online
Test Automation Frameworks To Enable Testing At Multiple-Tiers
The advantages of using test automation frameworks that enable testing of applications at multiple layers are multifold. These frameworks not only accelerate the activity associated with automation testing but also provide integration into the DevOps build environments. Selenified is one such popular test automation framework that has powerful features like traceable reporting, the ability to invoke dynamic waits, custom reporting, and much more.
Selenium Waits: Implicit, Explicit, Fluent And Sleep
It can be used for extensive testing of a web application, be it for
automated browser testing
or testing at an API level. As the tool is built on top of TestNG, it is ideal for beginners as well as experienced professionals who are into automation testing. The other major advantage of using Selenified is that it can be easily integrated into the DevOps build environments as there is built-in support for tools like Maven, Gradle, Ant, etc.
As organizations that are into the development of web apps/websites become more aggressive on the automation testing front, tools like Selenified will gain prominence in 2020. The primary reasons are the powerful features of the tools and seamless integration with cloud-based cross browser testing tools such as LambdaTest. If you are aware of Selenified then be sure to check out
LambdaTest integration with Selenified.
Test your native, hybrid, and web apps across all legacy and latest mobile operating systems on the most powerful
Android emulator online
.
QAOps To Be Adopted At A Wider Scale
QAOps is a new flavor of DevOps where the software QA team and operations team work together in close collaboration to deliver a quality software product. QAOps improves the overall software delivery process resulting in a more integrated and seamless operational software model. QAOps is about applying the core fundamentals & learnings of DevOps to the QA process.
QAOps results in better collaboration between the developers, testers, and the product team as the testers would no longer work in silos.
What Is QAOps? And Why It Matters For Your Web Application?
In 2020, we expect more software product organizations to use QAOps to streamline their QA process and reap the benefits of DevOps in the Quality Assurance/Software Testing department.
Run Selenium, Cypress, and Appium tests on LambdaTest to scale with the demand of your website and web apps.
Test your native, hybrid, and web apps on the most powerful
online emulator Android
, allowing you to run your applications across all legacy and latest mobile operating systems with ease.
Self-Healing Test Automation Will Gain Traction
Artificial Intelligence (AI) and Machine Learning (ML) are being widely used to not only solve a distinct problem but also to optimize the process of development & testing. AI is leveraged in testing to reduce the test lifecycle and is used in all aspects of testing including automation testing, performance testing, functional testing,
regression testing
, etc.
To achieve the goal of autonomous testing and harnessing the power of AI in testing, AI has to become an integral part of software testing tools. Each test cycle generates tons of data and that data can be used for identifying and resolving test failures. After each test run, the data/learning can be fed back to the AI/ML algorithms. This data will train the algorithm and that learning will help the test tool to identify the ‘
expected behavior
’ for those tests. This enables decision-making at runtime as the tool is in a position to flag a test as ‘failed’ in case there is any deviation from the expected behavior.
Such scenarios are very frequent in automated browser testing, where
Selenium
scripts are written to interact with a particular element (on a web page) identified by elementID/XPath. However, the absence of that element on the page can break the test. With AI-based learning, the tool can identify an ‘alternate’ element on the page that can be used in place of the expected element so that automated browser testing can proceed. With the increased usage of AI/ML in the development of test tools, the tools can self-heal at runtime. Self-healing automation testing will be one of the biggest automation testing trends in 2020.
Distributed Cloud For Cross Browser Testing To Become A Lifeline
Selenium automation testing approach on the local Selenium Grid is not scalable and this shortcoming has prompted many enterprises to shift their automated browser testing activities to cloud-based platforms. With the rise in the number of devices (with different screen sizes and form factors) being released each year,
cross browser testing on-cloud
seems a more viable option.
With a growing customer base spanning across multiple countries & continents, there could be intermittent delays in the test execution if the tests are being executed on a cloud-hosted in a faraway location. The adoption of distributed cloud will rise exponentially in 2020 thereby improving the speed and efficiency of test execution.
LambdaTest has been leveraging the advantages of distributed cloud computing so that its users can perform web automation testing in an efficient and scalable manner. Usage of the distributed cloud for automated browser testing is one of the automation testing trends in 2020 that will be on the radar of cloud technology and automation testing enthusiasts.
Are you using
Playwright
for automation testing? Run your Playwright test scripts instantly on 50+ browser/OS combinations using the LambdaTest cloud.
AI and ML Will Be At The Forefront Of Automation Testing
During the process of product development, there might be a number of test scenarios that fail when there are underlying changes in the UI of the web product. This sight is very common for any type of web product (website/web application). The downside is that developers need to spend a considerable amount of effort to accommodate changes in the test code.
AI will also be used to tackle challenges related to image recognition as AI algorithms are capable of capturing and analyzing data on the DOM. Machine Learning algorithms will go mainstream with UI testing as the user interface can be broken down at pixel level. The image patterns obtained via AI/ML algorithms can accelerate the testing of UI interfaces.
Hyper Automation To Be Used Extensively
Hyper Automation is a combination of tools including AI, ML, Natural Language Processing (NLP), Robotic Process Automation (RPA) that are used to leverage the advantages of AI-driven decision making. Hyper automation tools enhance human productivity by automating mundane processes that do not require any human intelligence.
Organizations are expected to use a combination of AI and RPA for test automation and analysis of automation reports. Hyper automation is one of the automation testing trends in 2020 that will be in huge demand.
AutoScaling CI/CD Runners To Support Parallel Testing
Automation testing is an effort to save time by fast-tracking the test cycles but the only way to unleash maximum benefits is by parallel test execution. The ability to run a test script over multiple test environments simultaneously, or testing multiple test scripts simultaneously. Enterprizes in 2020 will look forward to implementing automation testing through frameworks such as Selenium Grid which supports parallel testing. You can opt for on-premise parallel testing or can opt for a cloud-based Selenium Grid provider such as LambdaTest. LambdaTest will allow you to run your test cases in parallel on 3000+ real browsers.
The DevOps teams will look forward to autoscaling CI/CD runners to match up the pace of agile as the testing department will leverage the ability to run test cases in parallel. These autoscaling runners will manage the ups & downs over various servers on their own to efficiently manage queues in case of parallel testing. That way, teams can go for optimal consumption of resources without having their developers to wait for builds.
GitLab provides an autoscaling runner. You can read about it in their
official documentation
. LambdaTest offers integrations with GitLab for project management and continuous integration.
Integrate LambdaTest with GitLab
Integrate LambdaTest with GitLab CI
More Enterprizes To Lean Towards Shift-Left Testing
With the evolution of Agile and other methodologies, testing is no longer performed at the end of the development lifecycle. On the other hand, the shift-left testing approach is used extensively as it enables testing to be performed much earlier in the product life cycle. There are a number of benefits of shift-left testing — reduced costs in development & testing, early detection of bugs, and timely communication between the development & testing teams.
In the shift-left testing approach, the development and testing teams work in tandem to achieve common goals. Shift-left testing techniques are instrumental in detecting bugs at an early-stage of product development as their detection at a later-stage can hamper the timelines of product development and release. Shift-left testing will be one of the key automation testing trends in 2020 as more organizations plan to leverage the benefits of this testing approach.
Selenium IDE To Make Record and Playback Testing Much Convenient
‘How many of you use the Selenium IDE for automation?’ might not look like a relevant question in today’s times when record & playback testing seems a thing of passé. As it is rightly said — ‘There is light at the end of the tunnel’ and the same applies for the Selenium IDE. Selenium IDE is not dead, in fact, the revamped IDE and its code are freely available on GitHub. The Selenium IDE project is managed by the Selenium Community.
Upgrade your software testing with our
end-to-end testing
tool! Test your applications across multiple platforms and devices easily and quickly. Streamline your testing process and improve your team’s productivity with our user-friendly interface and comprehensive reporting
If you are into Selenium automation testing, the news that Selenium IDE is cross-browser i.e. available as extensions/add-ons on Chrome, Firefox, etc. will sound like sweet music to your ears☺. Selenium IDE Runner lets you execute tests on Selenium WebDriver servers and also supports Parallel execution of test cases/test suites. That’s not all; Selenium IDE has been redesigned to support Continuous Integration and Codeless Automation.
*Image Source: @*SeleniumConf
Dave Haeffner who has been actively contributing to the Selenium IDE project envisions incorporation of ML/AI algorithms in a future release of Selenium IDE. With Selenium IDE again gaining traction, Record and Playback testing are likely to also gain more prominence in 2020. To summarize, ‘development of
Selenium IDE’
and ‘revival of playback & record testing methodology’ will one of the eagerly watched automation testing trends in 2020.
Crowd-Sourced Testing To Gain Prominence For Automation Testing
The advent of multi-experience has prompted organizations to look beyond in-house testing resources. Though efficient test automation techniques are being widely used by companies (large enterprises and startups), the field of automation testing is rapidly undergoing democratization.
Crowd-sourced testing is gaining prominence as organizations look at improving user experience to stay competitive and relevant in today’s global market. To accelerate the delivery time, organizations are using crowd-sourced testing to gain user insights and assure quality via testing in a more cost-effective manner. As per a report, the global crowd-sourced testing market is expected to grow at a CAGR of 9.9% rising to $2 billion in 2024, up from $1.3 billion in 2019.
Though there are security concerns, privacy issues, management issues, etc.; the crowd-sourced automation testing market is expected to grow big-time in 2020. The increased usage of AI, ML, and other advanced technologies has opened up a new era in test automation called ‘test democratization’. The evolution of democratization in the field of automation testing will be one of the key talking points as part of the automation testing trends in 2020.
More Programming Languages To Emergence For Test Automation
Developers and testers are always on the lookout for tools and programming languages that can enhance their productivity. Such tools when used to their fullest potential can accelerate test case development and execution. What if there was a mechanism through which you can execute automation testing scenarios much faster? Smashtest is a programming language that has been specifically designed to accelerate the task of automation testing as the test cycles are 10x faster with the Smashtest.
The test cases in Smashtest are written in a tree-like format. This makes it easier to add as well as maintain test cases/test suites. The format of the test cases is such that trees represent how we think when we are testing. The major advantage of using Smashtest for Selenium automation testing is that the tests can be executed in parallel, easily integrates with the CI/CD tools, tests represent human- readable steps, and enables the generation of in-depth live reports.
Smashtest Is Changing The Way Selenium Tests Are Written
Smashtest is expected to take-off in a big way in 2020 and Selenium automation testers should count Smashtest as one of the most anticipated automation testing trends in 2020.
Exploratory Testing Will Become A Routine For Every QA
With an increased emphasis on automation testing implementation, across various organizations, Exploratory testing will become a routine of every QA on-board in a project. As automation testing will help to take care of the repetitive tasks, automation testers will be expected to find more unique test cases around their web application, in their added bandwidth.
Exploratory Testing: It’s All About Discovery
Automation testing will help them to emphasizes more on exploratory testing, so that they could come up with critical test scenarios that were unnoticed by the scripted testing so far. Due to faster go-to-market and the growing importance of Agile & DevOps on the development & testing departments, exploratory testing will become inevitable as a consequence of automation testing trends in 2020.
Effortless Maintenance Of Test Automation Scripts
Regular updations and maintenance of test automation scripts can become a humongous task if there are frequent changes in the product UI. Just imagine a scenario where your automation tests are based on a particular XPATH (or element ID or some other web locator). Any slightest change in XPATH (or element ID) can break all the test scenarios. Even if you make use of the Page Object Model (POM) for test case development, you cannot get rid of this problem!
Fortunately, new AI and ML solutions address such problems that ease the maintenance of test scripts. With such solutions in place, developers and testers no longer have to update their test scripts with slight changes in product UI (or web locators).
These tools are well-equipped to do self-correction during runtime. Test cases can now automatically self-adjust/self-heal as per the learning and test results. Maintenance of test automation scripts will become much easier as AI and ML will gain emphasis in 2020.
Selenium Alternatives Are Witnessing Increased Adoption Rate
When there is a discussion about automation testing, especially automated browser testing; Selenium will always be the major focus of discussion. It is hard to imagine automation testing without Selenium as it is the most preferred framework for cross browser testing.
The usage of Selenium might not witness a downturn but organizations might look for alternate tools to perform automation testing. These tools can either be open-source or commercially licensed tools. One reason to look out for alternative tools could be different project requirements and team preferences related to the task of automation.
Though there are a number of test automation tools/frameworks that are built using Selenium WebDriver, commercially available AI-based test automation frameworks might be a preferential automation testing choice for many projects.
Cypress.io, Jest, and few other test automation tools will be the trend-setters in 2020 as far as AI-based automation is concerned. Though the Selenium framework will be the ideal choice for test automation but the offerings of AI-based tools will shape the automation testing industry in 2020.
Agile and Test Automation Will Go Hand-in-Hand
Many industry experts predicted the death of Scrum but the bottom line is that the majority of the organization’s world over still make extensive usage of Agile methodologies.
Inclusion of automation testing has always been challenging as organizations are always looking out for tools that can complement Agile and test automation. AI and ML-based test automation tools could make this possible as it will ease the integration (and usage) of Agile with test automation.
Bottom Line
Automated browser testing is not only limited to Selenium automation testing as enterprises need to think beyond one particular automation framework. The year 2019 will be known for the evolution of different types of devices i.e. mobile phones, AR/VR, devices, etc.with every segment targeting a specific customer segment.
IoT (Internet of Things) is also playing a major role in preparing the automation testing industry for the next big challenge. With the emergence of AI/ML in automated browser testing, test script maintenance has become easier and AI (in a combination of Big Data) is helping testers by performing automated test runs and analyzing them at a quicker pace.
Though not many enterprises make use of ‘Playback and Record testing’, the positive development of Selenium IDE will bring the focus back on that testing approach. Visual validation automation testing, test case writing using AI, and self-healing automation testing were some of the defining moments of 2019 that will take shape as the pivotal automation testing trends in 2020.
What would be your pick for automation testing trends in 2020? Let me know in the comment section below. Happy testing! 🙂
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Build docker images with this Jenkins pipeline. Check it out.
Joshua Muriki -
Dec 12
5 Tech Innovations That Will Change Everyday Life
Codic Labs -
Dec 12
Trivy Vulnerability Scans Adnvanced Filtering
Artem -
Dec 12
Gone back to learn C Programming 23 years later.
eustone malaya -
Dec 12
LambdaTest
Follow
LambdaTest is a cloud based Cross Browser Testing Platform.
Visit Us
More from
LambdaTest
CSS Color-Contrast(): A Step-By-Step Guide
#
css
#
automation
#
cloudtesting
It’s Time For A TRIM(S): Richard Bradshaw [Testμ 2022]
#
webdev
#
testing
#
browser
#
automation
36 DevOps Testing Tools [2024]
#
devops
#
testing
#
tools
#
automation
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
The Adaptive Big Data layer with Pentaho Data Integration in the Market - DEV Community,"The Adaptive Big Data layer with Pentaho Data Integration in the Market - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Dhrumit Shukla
Posted on
Mar 8, 2018
The Adaptive Big Data layer with Pentaho Data Integration in the Market
#
pentaho
Businesses are growing in leaps and bounds since last decades, which result in a lot of systems that generate mountainous data at individual and organizational levels. This created a pressing need to consolidate all data in one platform and do an in-depth analysis that could help organizations take the right decisions.
The Pentaho BI Suite is the most popular business intelligence suite in the world that is used for reporting, analyzing, dash boarding, data mining, workflow and ETL or Export Transform Load capabilities. There are numerous software development service providers who are experts in
Pentaho business analytics and integration development
and catered to all the stacks that offer state-of-the-art solutions to esteemed customers.
THE ADAPTIVE BIG DATA LAYER
The
adaptive big data layer (ABDL) provides Pentaho users
the ability of working with any big data source, offering a completely functioning bugger to insulate IT developers and analysts from complexity of data. As a core component within the Pentaho Data Integration, the ABDL insulates a data developer from the shifting sands of data analytics and allows the ‘create once, run anywhere’ transformation process to work against any big data shop.
The current Pentaho data integration improvements help big data projects to deliver value in a rapid manner. The combination of more integrations with Spark, a new
level of Hadoop security compatibility
, as well as expanded metadata injection features allow companies to manage enterprise big data supply in a more effective manner, while accelerating and operationalizing innovations to drive Return on Investment.
OVERVIEW OF THE PENTAHO DATA INTEGRATION PLATFORM
The integration platform of Pentaho allows companies to integrate, blend, convert and transform data from any source of data across the whole enterprise. The platform offers extracting, transforming and loading the necessary functionality to integrate a wide range of data sources, which include enterprise apps, relational databases, files and big data.
The current PDI version 6.1, offers the following:
▪ Provides graphical ETL designer that enable data integration teams for designing, testing and deploying integration processes, notifications, workflows and alerts.
▪ Offers an extensive library of prebuilt data integration transformations, which support complicated process workflows.
▪ Allows connectivity to a wide range of big data stores, relational databases, files and enterprise apps as sources or targets in integration projects.
▪ Provides repository-based development tools, which manage the design, creation, testing, the deployment and operation of supporting metadata and integration processes.
▪ Allows users to visualize data during preparation of data and publishing metadata models to analytics tools.
Also, the version provides users the ability of converting data transformations to data services, which enable query results from the services to be analyzed as virtual data tables. Moreover, the latest versions also provide enhanced big data capabilities through supporting Cloudera Distribution for Hadoop as well as connecting to the Hadoop cluster with the use of Spoon.
WHO BENEFITS FROM THE PENTAHO INTEGRATION PLATFORM?
Small, medium as well as big enterprises use the platform to provide a cohesive and comprehensive data integration and business analytics platform. Aside from direct sales,
Pentaho has embedded OEM network
, allowing the vendors to extend their products with data integration and analytics capacities. Aside from the commercial versions, Pentaho also offers an open source version of a data integration product known as Kettle. A lot of companies initially begin working with the open source tool Kettle for exploring integration capabilities or for limited integration workloads.
Enjoy these Advantages
Pentaho is a sturdy data analytics platform, offering an array of advantages for businesses that want to acquire more from their data, such as complete and powerful visualizations that let users see data in a clear manner and zoom in on information as well as other relevant details far beyond statistical figures. Get real-time analysis of information through in-memory data caching. Exercise full control with customizable as well as interactive drag-and-drop dashboards that are web based, and library that is full of filter functionalities. The data integration system lets users to blend in information sourced from other pools of information including NoSQL, Hadoop, relational databases, and analytical databases.
BENEFIT FROM A COMPREHENSIVE ANALYTICS TOOL
The dashboards and reports of Pentaho offer deep analysis of data. Furthermore, even big volumes of data could be analyzed at lightning-speed, thanks to the extreme in-memory data caching. One could use the Pentaho data integration tool as well as other tools for accessing, blending and managing data from various sources. Additionally, one could incorporate business analytics with other software apps, like Google Maps. The analytics tools could be used to acquire actionable insights from data and make decisions that are information-driven.
The big data solutions of Pentaho are ideal for enterprises in the financial, government, retails, services and healthcare sectors. They allow users to access, combine and manage data from various sources.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
the Accessibility Overlay Scandal
Chris  -
Dec 12
Hare and Tortoise - To Find Start of Loop in Linked List
Lahari -
Dec 12
20 objections to accessibility and ways to counter them
Chris  -
Dec 12
Calendly and Accessibility Problems
Chris  -
Dec 12
Dhrumit Shukla
Follow
Dhrumit Shukla has been working as Business Development Manager in a software development company named TatvaSoft since 5 years.
Joined
Sep 18, 2017
Trending on
DEV Community
Hot
Meme Monday
#
discuss
#
jokes
#
watercooler
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Building a Big Data Playground Sandbox for Learning - DEV Community,"Building a Big Data Playground Sandbox for Learning - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Abdullah Haggag
Posted on
Oct 17
Building a Big Data Playground Sandbox for Learning
#
dataengineering
#
bigdata
#
opensource
Introduction
As a data engineer, I'm always seeking opportunities to experiment with different data solutions. Whether it's learning a new tool, practicing a solution, or testing ideas in a safe environment, the desire to innovate never ceases. To facilitate this, I've created a personal sandbox using Docker containers, featuring various big data tools. This setup, which I call the ""Big-data Ecosystem Sandbox (BES),"" leverages open-source big data tools orchestrated within Docker using custom-built images.
Sandbox Components
The BES includes a comprehensive set of tools essential for big data processing and analysis:
Data Storage and Management
PostgreSQL: An open-source relational database for structured data storage and complex queries.
MinIO: A high-performance, distributed object storage system compatible with Amazon S3 API.
Hadoop: An open-source framework for distributed storage and processing of large datasets.
Data Processing and Analytics
Hive: A data warehouse infrastructure built on Hadoop for querying and managing large datasets.
Spark: A fast, distributed computing system for large-scale data processing.
Trino: A distributed SQL query engine for querying data across various sources.
Streaming and Real-time Processing
Kafka: A distributed event streaming platform for building real-time data pipelines.
Flink: A stream processing framework for real-time data processing and event-driven applications.
Data Orchestration and Management
NiFi: An easy to use, powerful, and reliable system to process and distribute data.
Airflow: A platform to programmatically author, schedule, and monitor workflows.
Getting Started
You can find the GitHub Repo through the following link:
https://github.com/amhhaggag/bigdata-ecosystem-sandbox
Setup ALL the Sandbox Tools
“Make sure that you have enough CPU and RAM”
To Setup all the sandbox tools use the following script
git clone https://github.com/amhhaggag/bigdata-ecosystem-sandbox.git
cd
bigdata-ecosystem-sandbox

./bes-setup.sh
Enter fullscreen mode
Exit fullscreen mode
This script will do the following:
Pull the necessary Docker images from Docker Hub
amhhaggag/hadoop-base-3.1.1
amhhaggag/hive-base-3.1.2
amhhaggag/spark-3.5.1
Prepare the PostgreSQL Database for Hive-Metastore Service
Add the Trino Configurations to it’s specific mounted volume (Local Directory)
Create & Start all the containers
Now, let’s explain what is included in this repository:
Sandbox Architecture
The BES uses a combination of official Docker images and custom-built images to ensure compatibility and integration between tools. The custom images include Apache Hadoop, Hive, Spark, Airflow, and Trino, built in a hierarchical manner to maintain dependencies and ensure smooth integration.
Below is a diagram illustrating the dependencies between the custom built images.
Docker Compose Overview
To be able to use the sandbox efficiently you need to have at least basic knowledge of Docker and Docker Compose. Here is a quick overview on Docker Compose
A Docker Compose file, typically named
docker-compose.yml
, is a YAML file that defines, configures, and runs multi-container Docker applications. It allows you to manage all your application's services, networks, and volumes in a single place, streamlining deployment and scaling processes.
Here's the general structure of a Docker Compose file:
services
:
service_name
:
image
:
image_name:tag
# Use an existing image
build
:
context
:
./path
# Path to the build context
dockerfile
:
Dockerfile
# Dockerfile to use for building the image
ports
:
-
""
host_port:container_port""
# Map host ports to container ports
environment
:
-
VARIABLE=value
# Set environment variables
volumes
:
-
host_path:container_path
# Mount host paths or volumes
networks
:
-
network_name
# Connect to specified networks
depends_on
:
-
other_service
# Specify service dependencies
networks
:
network_name
:
driver
:
bridge
# Specify the network driver
volumes
:
volume_name
:
driver
:
local
# Specify the volume driver
Enter fullscreen mode
Exit fullscreen mode
Key Components Explained:
services
: Defines individual services (containers) that make up your application.
service_name
: A unique identifier for each service.
image
: Specifies the Docker image to deploy.
build
: Instructions for building a Docker image from a Dockerfile.
ports
: Exposes container ports to the host machine.
environment
: Sets environment variables within the container.
volumes
: Mounts host directories or named volumes into the container.
networks
: Connects the service to one or more networks.
depends_on
: Specifies service dependencies to control startup order.
networks
: (Optional) Defines custom networks for your services to communicate.
network_name
: The name of the network.
driver
: The network driver to use (e.g., bridge, overlay).
volumes
: (Optional) Defines named volumes for persistent data storage.
volume_name
: The name of the volume.
driver
: The volume driver to use.
Practical Example
Below is an example of a Docker Compose file of the PostgreSQL Service:
services
:
postgres
:
image
:
postgres:14
container_name
:
postgres
volumes
:
-
./mnt/postgres:/var/lib/postgresql/data
environment
:
POSTGRES_DB
:
""
admin""
POSTGRES_USER
:
""
admin""
POSTGRES_PASSWORD
:
""
admin""
ports
:
-
""
5432:5432""
networks
:
default
:
name
:
bes-network
Enter fullscreen mode
Exit fullscreen mode
Explanation of the Example:
Services
Service Name: postgres
image: the image that the container will use and deploy
container_name: the container will be created with this name “postgres”
volumes: the local directory “mnt/postgres” will be mounted and synced with the container directory “/var/lib/postgresql/data” to persist the data of the container in case we removed the container and started it again.
environment: specifies the environment variables that will be passed to the container
ports: the local port 5432 (on the left) will be mapped to the container port 5432 (on the right)
Networks
defining a network called “bes-network” through which all the related containers on the same network will be able to communicate together.
Basic Docker Commands
Here are some fundamental Docker commands to help you interact with containers:
docker ps
: List running containers
Example:
docker ps
docker-compose up
: Create and start containers defined in docker-compose.yml
Example:
docker-compose up -d
docker start
: Start a stopped container
Example:
docker start my_container
docker exec
: Execute a command in a running container
Example:
docker exec -it my_container bash
docker logs
: View the logs of a container
Example:
docker logs my_container
docker cp
: Copy files/folders between a container and the local filesystem
Example:
docker cp my_container:/path/to/file.txt /local/path/
docker stop
: Stop a running container
Example:
docker stop my_container
docker rm
: Remove a container
Example:
docker rm my_container
docker-compose down
: Stop and remove containers, networks, and volumes defined in docker-compose.yml
Example:
docker-compose down
These commands will help you manage your Docker containers effectively in the Big-data Ecosystem Sandbox.
Practical Applications
The BES opens up a world of possibilities for data engineering experiments and learning. Some potential use cases include:
Setting up a data lake using MinIO and processing it with Spark
Creating real-time data pipelines with Kafka and Flink
Orchestrating complex data workflows using Airflow
Performing distributed SQL queries across multiple data sources with Trino
Conclusion
The Big-data Ecosystem Sandbox provides a comprehensive environment for learning and experimenting with various big data tools. By leveraging Docker and custom integrations, it offers a flexible and powerful platform for data engineers to enhance their skills and explore new ideas.
In future posts, we'll dive deeper into specific use cases and advanced configurations to help you get the most out of your BES. Stay tuned, and happy data engineering!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
How to deploy Granite Dense 2B and 8B on a Virtual Machine in the Cloud?
Ayush kumar -
Oct 28
Edit HTML files visually.
Hayato Takenaka -
Oct 19
OptimizeIt Is Unleashed!
Majd Al Mnayer -
Nov 17
configuration management tools (Ansible/Chef/Puppet)
Vivesh  -
Nov 17
Abdullah Haggag
Follow
Data Engineer
Location
Riyadh, Saudi Arabia
Work
Data Engineer @ Qeema
Joined
Oct 15, 2024
More from
Abdullah Haggag
The Journey From a CSV File to Apache Hive Table
#
hadoop
#
hive
#
bigdata
#
dataengineering
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Installing Zsh and Oh-my-zsh on Windows 11 with WSL2 - DEV Community,"Installing Zsh and Oh-my-zsh on Windows 11 with WSL2 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Sunny Srinidhi
Posted on
Nov 6, 2021
Installing Zsh and Oh-my-zsh on Windows 11 with WSL2
#
windows
#
ohmyzsh
#
zsh
#
wsl
Before we begin, you might ask, why am I writing on something this trivial? I sold off my old MacBook Pro because I’m super excited about the new M1 Pro MacBook Pros. I have pre-ordered one of those and am waiting for it to come. Till then, I’m left with my gaming PC which is an Asus Zephyrus. So I thought I’ll make the best of it till I get my Mac. This will be series of posts in which I show to setup a Windows 11 PC to work with big data tools and technologies, mostly Hadoop. So let’s get into it.
Now that Windows 11 is officially available to the public, and given that Windows is the most popular operating system used by developers (according to stackoverflow, refer screenshot below), I thought it would be fun to install a new WSL system on my Windows 11 PC and try to set it up for big data tools. But before I could do that, I had to install Zsh and oh-my-zsh on it. I switched to using Zsh over half a decade ago and haven’t used Bash or any other shell ever. I just love how zsh look, the ability to theme it, and the community surrounding it. So, I started with zsh, and here is how you can install it too.
Source: stackoverflow
Installing WSL2 on Windows 11
The process of installing WSL on Windows 11 is not different than that on Windows 10. And if you are upgrading from a Windows 10 PC which already had WSL installed, you don’t even have to do anything extra. I already had mine setup on Windows 10 with Ubuntu 20.04 LTS. So that was readily available for me. But I wanted to setup everything fresh just to see if anything has changed (spoiler, it hasn’t). So I installed Debian this time.
To install any distro of Linux on Windows 11, just open up the Microsoft Store and search for your favorite Linux distro. In the screenshot below, you can see that if I search for Ubuntu, I get multiple versions of Ubuntu listed in the store.
Just select the version that you want to the install and click the Get button. After the installation is complete, the shell should open up automatically to complete the installation.
Soon after the installation is complete, you’ll see the bash shell something like the screenshot below.
As you can see, the prompt is pretty ugly, at least to me. I know I can customize it, but not to the extent of what’s already built into Zsh. Now that we have Debian installed, let’s start updating the packages and installing Zsh and oh-my-zsh.
Installing Zsh and Oh-my-zsh
Updating package list and upgrade packages
Before we start installing any packages, we first to have to update the package list so that we can then upgrade all the packages that we have already installed, and also fetch the references to the latest version of all the dependencies. This is pretty simple and will take just a few seconds. Run the following command in the terminal to update the references:
sudo
apt update
Enter fullscreen mode
Exit fullscreen mode
Once you update the package list, if you find any updates available, most probably you will, you can run the following command to upgrade all packages already installed. But let me also mention that this step is optional.
sudo
apt upgrade
Enter fullscreen mode
Exit fullscreen mode
Installing dependencies
Now that we have updated the package list, we have to install the dependencies that we’ll need to install Zsh and oh-my-zsh. The dependency list isn’t really exhaustive, it’s just two packages. If I’m not mistaken, these two packages must come pre-installed if you install a full fat version of Linux, but because this is a WSL version, I think it’s pretty stripped down.
Anyway, the two packages that we need to install are wget and git. And just to be clear, these are not required for installing Zsh, but for oh-my-zsh. And to be completely honest, you can technically install oh-my-zsh without these packages as well, but there are benefits if you do install this way. For instance, oh-my-zsh will automatically check for updates if you install it using git.
To install these two packages, run the following two commands one after another in your terminal:
sudo
apt
install
wget
sudo
apt
install
git
Enter fullscreen mode
Exit fullscreen mode
Together, it shouldn’t take more than a minute to install. These are pretty small packages. Once you’re done with these two, we can finally move on to installing Zsh.
Installing Zsh
In some cases, Zsh should already be installed even in the WSL version of Ubuntu or Debian. But I’m not 100% sure because I already had Ubuntu installed on WSL, as I mentioned. But if it’s not installed, it’s just one little command:
sudo
apt
install
zsh
Enter fullscreen mode
Exit fullscreen mode
And that’s it, you have Zsh installed on your Windows 11 PC using WSL. This excites me very much for some reason. But we’re not yet done. Let’s install oh-my-zsh.
Installing oh-my-zsh
If you don’t know what oh-my-zsh is, you can read all about it here. Installing this is another simple command. But it’s not using the apt package manager, but we’ll use wget and git to basically download the install script from the Git repo and run that on our machine. To install oh-my-zsh, run the following command in your terminal:
sh
-c
""
$(
wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh
-O
-
)
""
Enter fullscreen mode
Exit fullscreen mode
Because it’s an open source package, you can just to the link in that command and look at what the script is doing yourself. The whole process should look similar to what you see in the screenshot below.
Soon after the install, you can see the prompt change in your terminal. The default theme is applied. You can explore on how to get more themes and change it or customize it to your heart’s content.
Adding Useful Aliases
If you’re a developer and use Git a lot for your projects, there are some commands that you’ll be typing everyday. This shouldn’t come as a surprise, but most developers have handy aliases for these commands to make life a bit easier in the terminal. To add aliases, you’ll have to edit the .zshrc file in your home directory. For this, run the following command to open up the file:
vi ~/.zshrc
Enter fullscreen mode
Exit fullscreen mode
This will open the file in the vi editor. Scroll to the end of the file and the following lines:
alias
ll
=
""ls -ltra""
alias
gd
=
""git diff""
alias
gcmsg
=
""git commit -m""
alias
gitc
=
""git checkout""
alias
gitm
=
""git checkout master""
Enter fullscreen mode
Exit fullscreen mode
As you can see, these are pretty simple aliases. But also, they reduce typing a lot everyday when you add up the number of keystrokes at the end of the day. So, that’s pretty much it.
And if you like what you see here, or on my
Medium blog
and
personal blog
, and would like to see more of such helpful technical posts in the future, consider supporting me on
Patreon
and
Github
.
Top comments
(2)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
jramirez857
jramirez857
jramirez857
Follow
Education
Northeastern University
Work
Software Engineer in test
Joined
Oct 13, 2021
•
May 4 '22
Dropdown menu
Copy link
Hide
Just used this as I got a new Windows 11 laptop. Thanks for the guide! 🙌🙌
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Prayag Bhatt
Prayag Bhatt
Prayag Bhatt
Follow
Location
Vadodara, India
Pronouns
He/Him
Joined
Jun 9, 2023
•
Apr 13
Dropdown menu
Copy link
Hide
Really helpful. Thanks for the guide 🙌🏻✨
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Python Day-20 String functions logic using loops,Task
Guru prasanna -
Dec 11
Code Generated Architecture Diagram using Azure DevOps
FAB Builder | Code Generation  -
Dec 11
Webhooks vs APIs: Understanding the Differences
keploy -
Dec 11
Recommended Settings for VS Code for react js developers
Midhun Krishnan R -
Dec 11
Sunny Srinidhi
Follow
Coding, reading, machine learning, sleeping, listening, watching, potato. INDIA. 

http://blog.contactsunny.com

http://patreon.com/contactsunny

http://tiny.cc/7er8iz
Location
Bangalore
Education
Master's in Embedded Systems
Work
Lead Data Engineer
Joined
Aug 1, 2017
Trending on
DEV Community
Hot
🔥10 Useful Extensions For VS Code
#
programming
#
vscode
#
productivity
#
development
Meme Monday
#
discuss
#
jokes
#
watercooler
What was your win this week?
#
weeklyretro
#
discuss
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Unlock 10 Secrets to 90% Data Migration Success - DEV Community,"Unlock 10 Secrets to 90% Data Migration Success - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Emily Johnson
Posted on
Oct 11
Unlock 10 Secrets to 90% Data Migration Success
#
data
#
computing
#
migration
It's almost certain that your business will face a data migration process at some point. This process involves transferring existing data from one storage system or computer to another, a crucial step in ensuring business continuity.
Data migration is a complex task that requires careful planning and execution. In this article, we'll provide you with proven strategies to help you navigate this process successfully.
You'll learn what a data migration strategy entails, what to include in it, and what to consider when planning migration. We'll also outline common issues that arise during and after migration, enabling you to avoid unexpected surprises. Additionally, you'll discover how to conduct thorough tests after the migration.
Businesses undertake data migration for various reasons, such as replacing servers, transitioning their on-premise IT infrastructure to a cloud computing environment, updating their current database with new data due to a merger or acquisition, or moving their data to a new CRM. According to statistics, between 70-90% of data migration projects fail to meet expectations. Therefore, it's essential to follow data migration best practices to ensure a seamless process. For more information on seamless data migration, visit
t8tech.com
.
Key Strategies for a Successful Data Migration
Here are 10 essential data migration best practices to help you achieve a successful data transfer.
Back up Your Data
In the event of unforeseen circumstances, having a data backup ensures you can avoid potential data loss. If problems arise, such as file corruption, loss, or incompleteness, you can restore your data to its original state.
Verify Data Complexity and Quality
Another crucial best practice for data migration is verifying data complexity to determine the best approach. Assess different forms of organizational data, identify what data to migrate, its current location, storage, and format after transfer.
Evaluate the cleanliness of your current data and determine if it requires updates. Conducting a data quality assessment helps detect the quality of legacy data, implement firewalls to separate good data from bad data, and eliminate duplicates.
Agree on Data Standards
Once you understand the complexity of your data, establish standards to identify potential problem areas and avoid unexpected issues at the project's final stage. As data is constantly evolving, setting standards in place ensures successful data consolidation and future use.
Define Future and Current Business Regulations
To ensure regulatory compliance, it is essential to establish current and future business regulations for your data migration process. These regulations must align with various validation and business rules to facilitate consistent data transfer, achievable only through the development of data migration policies.
To ensure a seamless data migration, it's essential to establish a set of preliminary guidelines for your data before initiating the migration process, and then reassess and refine these guidelines to enhance their complexity and relevance to your data post-migration.
Formulate a Comprehensive Data Migration Plan
A well-defined strategy is crucial for successful data migration. You can adopt one of two approaches: a ""big bang"" migration, where the entire data transfer is completed within a specific timeframe, or a ""trickle"" migration, which involves a phased data migration process.
The ""big bang"" migration approach involves completing the entire data transfer within a short timeframe, such as 24 hours, during which live systems are taken offline while data undergoes ETL processing and is transferred to a new database. Although this approach is faster, it's also riskier.
In contrast, the ""trickle"" migration approach splits the data migration process into stages, allowing both the old and new systems to run concurrently, thereby eliminating downtime. While this approach is more complex, it's also safer, as data is migrated continuously.
Clearly Communicate Your Data Migration Process
The data migration process typically involves multiple teams, making effective communication a critical best practice. It's essential to inform teams about the process, assign tasks and responsibilities, and ensure they understand their roles and expectations. This includes listing all tasks and deliverables, assigning roles to activities, and verifying the availability of necessary resources.
Key considerations include:
identifying the ultimate authority responsible for overseeing the data migration process
determining who has the power to decide whether the migration was successful
assigning responsibility for data validation post-migration
Failing to establish a clear division of tasks and responsibilities can lead to organizational chaos, delays, or even migration failure.
Leverage the Right Tools for Data Migration
Manual scripting and data migration is not the most efficient approach. Utilizing the right tools can significantly expedite and streamline the data migration process, enabling data profiling, discovery, data quality verification, and testing.
Selecting the right migration tools should be a critical aspect of your planning process, guided by the organization's specific use case and business requirements.
Develop a Risk Management Strategy
Risk management is a critical consideration during the data migration process. Identifying potential challenges and devising strategies to mitigate or prevent them is essential for a successful outcome. Key factors to consider include deprecated data values, security concerns, user testing, and application dependencies.
Embrace Agility in Your Data Migration Strategy
By adopting an agile mindset during data migration, you can ensure the highest level of data quality through iterative testing, swiftly identify and rectify errors as they arise, and maintain a transparent process. This approach also facilitates more accurate cost and schedule forecasting, as it necessitates a clear allocation of tasks and responsibilities and adherence to deadlines.
Key Considerations for Testing
Deferring testing until the data transfer is complete can result in significant expenses. Instead, integrate testing into each phase of your data migration: planning, design, implementation, and maintenance. This will enable you to achieve your desired outcome in a timely and efficient manner.
Final Thoughts
Data migration can be a intricate process, but it is an unavoidable step for your organization. To mitigate the risk of data loss, ensure you have a reliable backup in place, as unforeseen issues can arise. Developing a comprehensive risk management strategy is crucial – identify potential pitfalls and develop solutions to rapidly resolve them.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
What is Ambient Computing and How It Will Transform Our Interaction with Technology
Aditya Pratap Bhuyan -
Nov 5
Planning and creating a database before you start working on your backend can save you time
dgloriaweb -
Nov 9
Data Traceability: Key Concepts and Best Practices
BuzzGK -
Nov 4
How to use migrations with Golang
Albert Colom -
Nov 6
Emily Johnson
Follow
My name is Emily Johnson, and I’ve spent the past decade at the intersection of technology and storytelling. By day, I work as a software developer, specializing in full-stack development for startups
Joined
Sep 16, 2024
More from
Emily Johnson
Master Selenium Testing with Python: 5 Reasons to Use Pytest!
#
command
#
computing
#
framework
#
open
Build a Web Server in 5 Minutes with Go
#
data
#
computing
#
distribution
#
differential
Build a Scalable AMQP-Based Messaging Framework on MongoDB in 5 Steps
#
application
#
data
#
computing
#
database
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
DevOps toolchain - DEV Community,"DevOps toolchain - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Megha Sharma
Posted on
Oct 1
DevOps toolchain
#
devops
#
development
#
devrel
#
developer
What is DevOps Toolchain?
A DevOps toolchain is a set of tools, practices, and processes that automate and optimize various stages of the software development and deployment lifecycle.
DevOps toolchain is a collection of instruments that makes it easier to develop, manage, and deploy software. Throughout the whole software development lifecycle, organizations control and automate DevOps operations using DevOps tools.
A DevOps toolchain consists of the technology and tools that enable development as well as operations teams to work together throughout the whole software lifecycle. The core DevOps elements are automation, collaboration, continuous integration, and delivery.
Software development teams can complete difficult tasks with the aid of a toolchain. A toolchain's components typically operate one after the other, with the output of one tool acting as the input for the subsequent one. Some DevOps technologies, however, are utilized independently of the DevOps pipeline.
Why Do You Need DevOps Toolchain?
In today’s fast-paced environment, software development demands agility and speed to keep up with ever-evolving customer expectations. This is where the DevOps toolchain becomes essential. As Steve Jobs once said, “Innovation distinguishes between a leader and a follower,” and in modern tech, innovation is tied directly to how fast and efficiently teams can deliver.
The DevOps toolchain bridges the gap between development and operations teams, enabling seamless collaboration to reduce the time between writing code and deploying it in production. By using these tools, you streamline processes like continuous integration, automated testing, and infrastructure management. This ultimately leads to faster releases, fewer errors, and better scalability.
A DevOps toolchain is the secret to attaining optimal tool automation or enabling an integrated software development lifecycle. The DevOps toolchain coordinates and automates the many skills required to produce high-quality software fast and consistently.
In the competitive tech market, the ability to iterate rapidly can be the difference between staying ahead or being left behind. It’s not just about building software quickly—it’s about building it well, over and over again, without compromising quality.
Typically, a DevOps toolchain includes a variety of tools and technologies that cover the following key areas:
1. Version Control Systems (VCS)
Git:
A widely-used distributed version control system that allows developers to track changes in code and collaborate.
GitHub / GitLab / Bitbucket:
Platforms that provide hosting for Git repositories with additional collaboration tools like pull requests and issue tracking.
2. Continuous Integration (CI) / Continuous Delivery (CD)
Jenkins:
An open-source automation server that supports building, testing, and deploying applications.
CircleCI
: A continuous integration platform that automates the build, test, and deployment process.
GitLab CI/CD:
Integrated CI/CD pipeline features within GitLab that enable automation of the DevOps lifecycle.
Travis CI:
A cloud-based CI/CD tool designed for GitHub projects.
3. Containerization and Orchestration
Docker:
A tool for containerizing applications to ensure consistency across environments.
Kubernetes:
An orchestration platform for automating the deployment, scaling, and management of containerized applications.
Docker Compose:
A tool for defining and running multi-container Docker applications.
4. Configuration Management
Ansible:
An open-source tool for automating configuration management, provisioning, and deployment.
Chef / Puppet:
Configuration management tools that allow you to define infrastructure as code.
5. Infrastructure-as-code
Terraform:
An infrastructure-as-code tool that allows you to define cloud and on-premise resources in a declarative language.
6. Monitoring and Logging
Prometheus:
An open-source system monitoring and alerting toolkit designed for reliability.
Grafana:
A multi-platform open-source analytics and interactive visualization web application.
ELK Stack (Elasticsearch, Logstash, Kibana):
A set of tools for logging, visualizing, and analyzing data.
Splunk:
A platform for searching, monitoring, and analyzing machine-generated big data.
7. Testing
Selenium:
An open-source automated testing framework for web applications.
JUnit:
A framework for testing Java applications.
TestNG:
Another testing framework inspired by JUnit, designed for a wide range of tests.
SonarQube:
A tool for continuous inspection of code quality, focusing on bug detection and vulnerability management.
8. Collaboration and Communication
Slack:
A popular communication tool for team collaboration.
Microsoft Teams:
A collaboration and communication platform integrated with Office 365.
Jira:
A project management tool often used in agile software development and DevOps workflows.
Trello:
A simple, visual tool for project management.
9. Security Tools (DevSecOps)
Aqua Security:
A tool for securing containerized applications.
Snyk:
A tool that automatically finds and fixes vulnerabilities in code, open-source libraries, and containers.
Twistlock:
A security platform designed for containers and cloud-native applications.
10. Cloud Providers
Amazon Web Services (AWS):
Cloud computing services with various offerings for DevOps, including Elastic Beanstalk, CloudFormation, and CodePipeline.
Microsoft Azure:
Cloud services from Microsoft, offering Azure DevOps for full CI/CD pipeline management.
Google Cloud Platform (GCP):
Google’s cloud offering with tools like Cloud Build, Cloud Run, and Kubernetes Engine.
11. Artifact Repositories
Nexus Repository:
A repository manager for storing and retrieving build artifacts, dependencies, and containers.
JFrog Artifactory:
A universal artifact repository manager supporting all major package types.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Your Path to Cloud Mastery: A Guide to AWS Certifications
Mohammed Nasser -
Nov 26
Application Performance Monitoring (APM) Guide for DevOps Teams in 2024
Alexandr Bandurchin -
Nov 27
Unlocking the Power of App Intents: Prepare Your App for the AI Revolution
Lin Dane -
Oct 24
UiPath and the Future of Robotic Process Automation
Vivek Garg -
Nov 27
Megha Sharma
Follow
👋 Hi there! I'm a DevOps enthusiast with a deep passion for all Cloud Native things. I thrive on learning and exploring new technologies, always eager to expand my knowledge and skills.
Location
Remote
Joined
Dec 2, 2023
More from
Megha Sharma
Creating a Dockerfile
#
docker
#
opensource
#
devops
#
cloud
Dockerfile Explain
#
docker
#
opensource
#
dockerfile
#
devops
Docker Commands - Part 3
#
docker
#
opensource
#
devops
#
cloud
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Top 6 PHP code quality tools 2023 - DEV Community,"Top 6 PHP code quality tools 2023 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Bentil Shadrack
for
Documatic
Posted on
May 30, 2023
Top 6 PHP code quality tools 2023
#
php
#
webdev
#
tooling
PHP is a popular programming language for web development that powers a large number of websites and internet-based applications. Code quality is extremely important when working as a PHP developer. Tools for evaluating the quality of PHP code can be useful here.
Developers can find errors, spot vulnerabilities, and enforce coding standards thanks to their automated code review and analysis processes. These tools allow beginning and intermediate developers to improve their coding techniques, generate cleaner code, and ultimately raise the caliber of their PHP projects as a whole.
I will delve into the world of PHP code quality tools in this post and examine their relevance, significance, and useful applications. Additionally, I will introduce you to the best PHP code quality tools on the market and provide details on their features and functionalities. By the conclusion, you will have a thorough knowledge of these tools and how they may greatly aid in the creation of reliable and dependable PHP applications. Let's explore the top PHP code quality tools now!
Relevance of PHP
PHP has long been a cornerstone of web development, gaining popularity and widespread usage among developers worldwide. Its versatility, ease of use, and extensive community support have contributed to its enduring presence in the web development landscape.
One of the primary reasons for PHP's popularity is its ability to seamlessly integrate with various web technologies and database systems. It powers numerous content management systems (CMS) such as WordPress, Drupal, and Joomla, enabling developers to create dynamic and interactive websites with ease. Additionally, PHP boasts a vast array of frameworks, including Laravel, Symfony, and CodeIgniter, which streamline the development process and promote code reusability.
With the immense adoption of PHP in web development, it becomes crucial to prioritize the quality of the PHP code itself. Efficient and well-structured PHP code ensures optimal performance, maintainability, and scalability of web applications. It minimizes the risk of errors, enhances code readability, and facilitates collaboration among developers working on the same project.
By adhering to coding best practices and leveraging PHP code quality tools, developers can ensure that their code meets industry standards, follows coding conventions, and avoids common pitfalls. This not only improves the overall quality of the PHP codebase but also contributes to faster development cycles, easier debugging, and reduced maintenance efforts.
In the next sections, we will explore the significance of code review and analysis, as well as the role of code quality tools in maintaining the integrity of PHP code.
PHP code quality tools
Code quality tools are software applications or libraries specifically designed to analyze, review, and assess the quality of code. These tools automate the process of code inspection, providing developers with valuable insights, recommendations, and metrics to improve the overall quality, maintainability, and efficiency of their codebase.
In PHP development, code quality tools play a crucial role in ensuring that the code adheres to established coding standards, follows best practices, and meets the requirements of a high-quality PHP application. These tools can detect various types of issues, ranging from simple syntax errors to complex architectural problems, security vulnerabilities, and performance bottlenecks.
I have summarized the benefits of these tools as follows:
Code Analysis: Code quality tools perform static analysis of PHP code, scanning the entire codebase or specific files to identify potential issues. They examine the code structure, syntax, and semantics, checking for errors, inconsistencies, and non-compliance with coding standards.
Automated Testing: Code quality tools often integrate with testing frameworks to facilitate automated testing. They can execute unit tests, integration tests, and other types of tests, providing feedback on test coverage, assertion failures, and overall code reliability.
Code Style Enforcement: Code quality tools enforce coding style guidelines, ensuring consistent formatting, naming conventions, and indentation throughout the codebase. They help maintain a uniform code style, improving code readability and making it easier for multiple developers to collaborate on the same project.
Performance Analysis: Some code quality tools analyze the performance aspects of PHP code. They identify potential bottlenecks, inefficient algorithms, and resource-heavy operations, enabling developers to optimize the code for better performance.
Security Vulnerability Detection: Code quality tools can identify common security vulnerabilities in PHP code, such as SQL injection, cross-site scripting (XSS), and insecure data handling. By highlighting these vulnerabilities, developers can take appropriate measures to address them and ensure the security of their applications.
Top 6 PHP Code quality tools
Now that we've covered the basics of PHP code quality tools, let's take a look at some of the most popular tools available today. I've compiled a list of the top PHP code quality tools, along with their key features and functionalities, to help you choose the right tool for your project.
PHP_CodeSniffer:
PHP_CodeSniffer
PHP_CodeSniffer is a widely-used tool for enforcing coding standards in PHP. It checks the code against a set of predefined coding standards (such as PSR-12, PSR-2, and many others) and provides reports with violations and recommendations for improvement.
Documentation: You can find more information about PHP_CodeSniffer and its usage in the official documentation:
PHP_CodeSniffer Repository
PHPMD Mess Detector:
PHP Mess Detector
PHP Mess Detector identifies potential problems and ""code smells"" in PHP code. It detects complex code, unused variables, duplicated code blocks, and other issues that may indicate poor code quality. It helps developers refactor and clean up their codebase.
Documentation: To explore PHP Mess Detector and understand its usage, refer to the official repository:
PHP Mess Detector Repository
PHPStan:
PHP Static Analysis Tool
PHPStan is a powerful static analysis tool that performs comprehensive type checking and detects potential errors in PHP code. It analyzes the codebase and provides detailed reports, highlighting type mismatches, undefined variables, and other issues.
Documentation: The official repository of PHPStan contains detailed information on installation, configuration, and usage:
PHPStan Repository
PHPUnit:
PHPUnit
PHPUnit is a testing framework for PHP. It enables developers to write unit tests, integration tests, and functional tests to verify the correctness of their code. It offers a wide range of assertions, test runners, and mocking capabilities.
Documentation: Explore PHPUnit's capabilities and learn how to write effective tests in the official documentation:
PHPUnit Repository
PHP_CodeCoverage:
PHP_CodeCoverage
PHP_CodeCoverage is a library that enables developers to measure the code coverage of their tests. It collects data on which parts of the code are executed during test runs, allowing developers to assess the effectiveness and completeness of their test suite.
Documentation: Learn how to integrate and utilize PHP_CodeCoverage effectively by referring to the official repository: PHP_CodeCoverage Repository
Psalm:
Psalm
Psalm is a static analysis tool specifically designed for PHP. It performs advanced type inference and checks for various types of errors, including type errors, undefined variables, incorrect function calls, and more. It provides comprehensive code analysis and helps improve code quality and maintainability.
Documentation: You can find more information about Psalm, including installation instructions and usage details, in the official repository:
Psalm Repository
Feel free to explore the repositories and documentation of these tools to gain a deeper understanding of their features, installation procedures, and configuration options. These tools can significantly enhance your PHP development workflow by promoting code quality, adherence to standards, and effective testing practices.
Code Review and Analysis:
Code review is the process of examining code to identify errors, improve quality, and ensure adherence to coding standards. Code analysis on the other hand is the automated examination of code using specialized tools to detect syntax errors, coding style violations, unused variables, code complexity, and security vulnerabilities.
The process of code review and analysis plays a vital role in maintaining the integrity and quality of PHP code.
Here are some key reasons why it is important:
Error Detection: Code review and analysis help identify errors, bugs, and logical flaws in the code. Detecting and fixing these issues early in the development process minimizes the risk of encountering critical errors in production.
Security Vulnerability Identification: Code review and analysis can uncover security vulnerabilities in the codebase. By identifying potential security risks, developers can address them proactively and ensure that the application is robust and protected against potential threats.
Performance Optimization: Through code review and analysis, developers can identify areas of the code that may impact performance, such as inefficient algorithms or resource-intensive operations. By optimizing these areas, developers can improve the overall performance and responsiveness of the application.
Adherence to Coding Standards: Consistent and well-structured code improves readability, maintainability, and collaboration among developers.
Continuous Improvement: Developers can gain insights into better coding practices, patterns, and techniques, leading to ongoing enhancement of their skills and the codebase.
Incorporating regular code review and analysis practices, coupled with the use of code quality tools, is instrumental in maintaining a high-quality PHP codebase and delivering reliable and efficient applications.
Integrating Code Quality Tools into the Workflow
Integrating PHP code quality tools into development workflow is essential for successful project delivery. It helps to maintain high standards of code quality, regularly running code analysis, addressing reported issues, and iterating on the codebase based on the tool's feedback.
Let's look as some key things to consider for incorporating these tools effectively.
IDE and Editor Integration: Code quality tools provide plugins or extensions for IDEs and text editors, allowing for real-time feedback and suggestions as code is written.
Build and Continuous Integration Systems: Integrate code quality tools into build and continuous integration (CI) systems to catch issues early and prevent potential problems from merging into the main codebase.
Automated Testing: Combining code quality tools with automated testing frameworks ensures code quality checks are performed alongside unit tests and integration tests for comprehensive quality assurance.
Happy Hacking!
Bentil here🚀
Are you a PHP developer, which of the code quality tools above have you used before? Which ones would you like to add? Kindly share links to it or the repository. This can help others as well.
Kindly Like, Share and follow us for more.
Top comments
(11)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Joe Bordes
Joe Bordes
Joe Bordes
Follow
Location
Valencia, Spain
Education
Valencia
Work
CTO
Joined
Oct 20, 2020
•
Jun 1 '23
Dropdown menu
Copy link
Hide
I agree with this list, essential tools to work with PHP, but I have to recommend sonarqube. Combined with these it is a another level of programming experience.
Note; no affiliation with them.
HTH
Like comment:
Like comment:
6
likes
Like
Comment button
Reply
Collapse
Expand
Bentil Shadrack
Bentil Shadrack
Bentil Shadrack
Follow
Software Engineer ||
Promoting & sharing educative tips & resources from Devs💻 and for Devs💻| Buy me a ☕
https://www.buymeacoffee.com/qbentil
Email
bentilshadrack72@gmail.com
Location
Accra, Ghana
Education
University of Ghana, Legon
Pronouns
He/Him/His
Work
Software Engineer
Joined
Sep 2, 2021
•
Jun 1 '23
Dropdown menu
Copy link
Hide
Thank you very much Joe🙌
I will try that
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Marius Posthumus
Marius Posthumus
Marius Posthumus
Follow
Work
Team Lead
Joined
Jun 1, 2023
•
Jun 1 '23
Dropdown menu
Copy link
Hide
I would like to add Infection to the list to fully test if your tests are ACTUALLY testing the correct things.
It's a very interesting tool that changes your code and reruns the tests to see if you missed certain branches / situations.
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Bentil Shadrack
Bentil Shadrack
Bentil Shadrack
Follow
Software Engineer ||
Promoting & sharing educative tips & resources from Devs💻 and for Devs💻| Buy me a ☕
https://www.buymeacoffee.com/qbentil
Email
bentilshadrack72@gmail.com
Location
Accra, Ghana
Education
University of Ghana, Legon
Pronouns
He/Him/His
Work
Software Engineer
Joined
Sep 2, 2021
•
Jun 1 '23
Dropdown menu
Copy link
Hide
Awesome 👏
Thank you Marius
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Roman Pronskiy
Roman Pronskiy
Roman Pronskiy
Follow
Developer advocate @JetBrains, @PhpStorm team, PHP Annotated author, board member at The PHP Foundation, PHP enthusiast, yet another DJ/Producer.
Joined
Aug 11, 2019
•
Jun 9 '23
Dropdown menu
Copy link
Hide
Would you be interested to try Qodana? It's a static code analysis tool by JetBrains with support for PHP.
jetbrains.com/qodana/
Like comment:
Like comment:
3
likes
Like
Comment button
Reply
Collapse
Expand
Bentil Shadrack
Bentil Shadrack
Bentil Shadrack
Follow
Software Engineer ||
Promoting & sharing educative tips & resources from Devs💻 and for Devs💻| Buy me a ☕
https://www.buymeacoffee.com/qbentil
Email
bentilshadrack72@gmail.com
Location
Accra, Ghana
Education
University of Ghana, Legon
Pronouns
He/Him/His
Work
Software Engineer
Joined
Sep 2, 2021
•
Jun 10 '23
Dropdown menu
Copy link
Hide
Great
Thank you Pronskiy. I will check it out
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Christophe Avonture
Christophe Avonture
Christophe Avonture
Follow
Markdown, WSL and Docker lover ~ PHP developer ~ Insatiable curious.
Location
Racour, Belgium
Joined
May 18, 2020
•
Jun 4 '23
• Edited on
Jun 4
• Edited
Dropdown menu
Copy link
Hide
There is also a.o.t phpcpd (php copy paste detector) and, for Laravel, larastan. And, on top of phpunit, Pest, so much better.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Bentil Shadrack
Bentil Shadrack
Bentil Shadrack
Follow
Software Engineer ||
Promoting & sharing educative tips & resources from Devs💻 and for Devs💻| Buy me a ☕
https://www.buymeacoffee.com/qbentil
Email
bentilshadrack72@gmail.com
Location
Accra, Ghana
Education
University of Ghana, Legon
Pronouns
He/Him/His
Work
Software Engineer
Joined
Sep 2, 2021
•
Jun 5 '23
Dropdown menu
Copy link
Hide
Oh greatt🙌
Thank you Chris
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Konadu Akwasi Akuoko
Konadu Akwasi Akuoko
Konadu Akwasi Akuoko
Follow
Building software (Software and DevOps Eng) 🏗️ Technical writer ✍️ About me: konadu.dev/about 🛠Built konadu.dev 

Talking about software engineering, devops

AWS Certified
Location
Kumasi, Ghana
Joined
Jul 28, 2021
•
Jun 2 '23
Dropdown menu
Copy link
Hide
Awesome read Bentil, unfortunately I've not used PHP before, maybe I'll try in the future
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Collapse
Expand
Bentil Shadrack
Bentil Shadrack
Bentil Shadrack
Follow
Software Engineer ||
Promoting & sharing educative tips & resources from Devs💻 and for Devs💻| Buy me a ☕
https://www.buymeacoffee.com/qbentil
Email
bentilshadrack72@gmail.com
Location
Accra, Ghana
Education
University of Ghana, Legon
Pronouns
He/Him/His
Work
Software Engineer
Joined
Sep 2, 2021
•
Jun 2 '23
Dropdown menu
Copy link
Hide
Thank you Akwasi
Like comment:
Like comment:
1
like
Like
Comment button
Reply
Collapse
Expand
Jean-François
Jean-François
Jean-François
Follow
CTO - PHP, DevOps, and Software Quality Expert
Joined
Mar 28, 2024
•
Mar 28
Dropdown menu
Copy link
Hide
Thanks for this article. I don't know if you are aware of
this list
, it's quite handy.
I am the creator of PhpMetrics. For a few weeks now, I have been working on an analyzer written in Go, which is very efficient and provides a lot of useful metrics. It is still experimental, but very promising, I hope. If you want to test it, I am open to feedback and suggestions:
github.com/Halleck45/ast-metrics
Like comment:
Like comment:
1
like
Like
Comment button
Reply
View full discussion (11 comments)
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Pathfinding: Solving the N-Queens Problem Using Backtracking Introduction:
TAMIL SELVAN M IT -
Nov 22
CSS - display: flex vs inline-flex
Jen C. -
Nov 22
My AWS Journey
Ijay -
Nov 22
Master CSS Selectors: The Complete Beginner-to-Expert Guide
chintanonweb -
Nov 26
Documatic
Follow
More from
Documatic
Transitioning to a Microservices Architecture: Overcoming Obstacles
#
javascript
#
beginners
#
programming
#
webdev
Data Privacy Laws: Navigating Compliance in the Age of Big Data
#
beginners
#
programming
#
webdev
#
database
The Future of Cloud Computing: Predictions and Trends
#
webdev
#
javascript
#
beginners
#
tutorial
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Testing Page 73 - DEV Community,"Testing Page 73 - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Testing
Follow
Hide
Find those bugs before your users do! 🐛
Create Post
tag moderators
9906 Posts Published
Older #testing posts
70
71
72
73
74
75
76
77
78
Posts
Left menu
👋
Sign in
for the ability to sort posts by
relevant
,
latest
, or
top
.
Right menu
A Complete Guide To Flutter Testing
Harish Rajora
Harish Rajora
Harish Rajora
Follow
for
LambdaTest
Apr 25 '22
A Complete Guide To Flutter Testing
#
showdev
#
flutter
#
testing
#
tutorial
10
reactions
Comments
Add Comment
12 min read
Obteniendo el mayor valor de nuestros tests en Angular
ng-content
ng-content
ng-content
Follow
Apr 24 '22
Obteniendo el mayor valor de nuestros tests en Angular
#
angular
#
testing
#
typescript
#
programming
8
reactions
Comments
Add Comment
12 min read
Getting started with WDM(WebDriverManager)
Arvind Choudhary
Arvind Choudhary
Arvind Choudhary
Follow
Apr 24 '22
Getting started with WDM(WebDriverManager)
#
testing
#
selenium
#
testautomation
#
browserautomation
5
reactions
Comments
Add Comment
2 min read
Testing a React Application: The Modern Approach
Diego (Relatable Code)
Diego (Relatable Code)
Diego (Relatable Code)
Follow
Apr 23 '22
Testing a React Application: The Modern Approach
#
programming
#
beginners
#
testing
#
react
21
reactions
Comments
4
comments
4 min read
Mocking AWS Dynamo Db calls during testing
Syed Mohammad Ibrahim
Syed Mohammad Ibrahim
Syed Mohammad Ibrahim
Follow
Apr 24 '22
Mocking AWS Dynamo Db calls during testing
#
dynamodb
#
moto
#
testing
#
python
11
reactions
Comments
Add Comment
9 min read
Apache Spark, Hive, and Spring Boot — Testing Guide
Semyon Kirekov
Semyon Kirekov
Semyon Kirekov
Follow
Apr 22 '22
Apache Spark, Hive, and Spring Boot — Testing Guide
#
bigdata
#
testing
#
java
#
docker
16
reactions
Comments
4
comments
18 min read
An easy way to automate running Rails test (or any command) on file changes
Nick Ang
Nick Ang
Nick Ang
Follow
Apr 12 '22
An easy way to automate running Rails test (or any command) on file changes
#
ruby
#
rails
#
testing
8
reactions
Comments
Add Comment
2 min read
Test Kernel changes on QEMU
Carlo Lobrano
Carlo Lobrano
Carlo Lobrano
Follow
Apr 23 '22
Test Kernel changes on QEMU
#
linux
#
kernel
#
qemu
#
testing
12
reactions
Comments
Add Comment
3 min read
Testing with Jest
Famini-ProDev
Famini-ProDev
Famini-ProDev
Follow
Apr 23 '22
Testing with Jest
#
javascript
#
testing
7
reactions
Comments
Add Comment
6 min read
Adding PHPUnit Test Log and Coverage to GitLab CI/CD Pipeline
Muhammad Hassan
Muhammad Hassan
Muhammad Hassan
Follow
Apr 22 '22
Adding PHPUnit Test Log and Coverage to GitLab CI/CD Pipeline
#
cicd
#
php
#
testing
#
docker
18
reactions
Comments
4
comments
7 min read
How to Inspect or Locate Element using UIAutomatorViewer in Appium: Tutorial
Garima Tiwari
Garima Tiwari
Garima Tiwari
Follow
Apr 22 '22
How to Inspect or Locate Element using UIAutomatorViewer in Appium: Tutorial
#
testing
#
ux
#
tutorial
#
uiweekly
7
reactions
Comments
Add Comment
3 min read
The Use of CI/CD
Aaron Darwish
Aaron Darwish
Aaron Darwish
Follow
Apr 22 '22
The Use of CI/CD
#
productivity
#
devops
#
agile
#
testing
9
reactions
Comments
Add Comment
3 min read
Top Trends in Software Testing: Infographic
deekshalt
deekshalt
deekshalt
Follow
for
LambdaTest
Apr 22 '22
Top Trends in Software Testing: Infographic
#
showdev
#
testing
#
webdev
#
programming
11
reactions
Comments
Add Comment
2 min read
How real-time autonomous unit testing can help you develop better software
Symflower
Symflower
Symflower
Follow
Mar 20 '22
How real-time autonomous unit testing can help you develop better software
#
devops
#
testing
2
reactions
Comments
Add Comment
4 min read
How To Test Internet Explorer For Mac?
Harish Rajora
Harish Rajora
Harish Rajora
Follow
for
LambdaTest
Apr 22 '22
How To Test Internet Explorer For Mac?
#
showdev
#
webdev
#
testing
#
discuss
10
reactions
Comments
Add Comment
12 min read
Register now for upcoming webinar ft. MaestroQA's Infrastructure Engineering Lead, Amrisha Sinha
Shipyard DevRel
Shipyard DevRel
Shipyard DevRel
Follow
Apr 22 '22
Register now for upcoming webinar ft. MaestroQA's Infrastructure Engineering Lead, Amrisha Sinha
#
devops
#
programming
#
testing
#
productivity
4
reactions
Comments
Add Comment
1 min read
Better table-driven tests: generating Symflower-style unit tests
Symflower
Symflower
Symflower
Follow
Mar 20 '22
Better table-driven tests: generating Symflower-style unit tests
#
devops
#
testing
2
reactions
Comments
Add Comment
4 min read
Everything You Need To Know about API testing
deekshalt
deekshalt
deekshalt
Follow
for
LambdaTest
Apr 22 '22
Everything You Need To Know about API testing
#
showdev
#
testing
#
webdev
#
database
10
reactions
Comments
Add Comment
5 min read
Testing in Laravel
Morcos Gad
Morcos Gad
Morcos Gad
Follow
Apr 22 '22
Testing in Laravel
#
php
#
laravel
#
webdev
#
testing
9
reactions
Comments
1
comment
1 min read
A Complete Guide to Optimizing Slow Tests
Tomas Fernandez
Tomas Fernandez
Tomas Fernandez
Follow
for
Semaphore
Apr 22 '22
A Complete Guide to Optimizing Slow Tests
#
testing
#
productivity
#
cicd
#
devops
4
reactions
Comments
Add Comment
1 min read
Why You Need To Understand Test Management Strategy To Become Pro?
arnabroychowdhury
arnabroychowdhury
arnabroychowdhury
Follow
for
LambdaTest
Apr 22 '22
Why You Need To Understand Test Management Strategy To Become Pro?
#
discuss
#
management
#
programming
#
testing
11
reactions
Comments
Add Comment
7 min read
How To Use iPhone Simulators On Windows
Harish Rajora
Harish Rajora
Harish Rajora
Follow
for
LambdaTest
Apr 21 '22
How To Use iPhone Simulators On Windows
#
showdev
#
webdev
#
testing
#
ios
10
reactions
Comments
Add Comment
11 min read
24 Things You Might Be Doing Wrong In Website Testing!
arnabroychowdhury
arnabroychowdhury
arnabroychowdhury
Follow
for
LambdaTest
Apr 21 '22
24 Things You Might Be Doing Wrong In Website Testing!
#
showdev
#
javascript
#
testing
#
website
12
reactions
Comments
Add Comment
16 min read
A GitOps-Powered Kubernetes Testing Machine with ArgoCD and Testkube — Kubeshop
Abdallah Abedraba
Abdallah Abedraba
Abdallah Abedraba
Follow
for
Kubeshop
Apr 21 '22
A GitOps-Powered Kubernetes Testing Machine with ArgoCD and Testkube — Kubeshop
#
kubernetes
#
argo
#
testing
#
gitops
7
reactions
Comments
Add Comment
6 min read
E2E Testing an App with Clerk Authentication in Cypress
Lynn Romich
Lynn Romich
Lynn Romich
Follow
Apr 21 '22
E2E Testing an App with Clerk Authentication in Cypress
#
javascript
#
webdev
#
testing
#
nextjs
6
reactions
Comments
Add Comment
2 min read
Using testing-playground with React Testing Library
Katie Raby
Katie Raby
Katie Raby
Follow
Apr 18 '22
Using testing-playground with React Testing Library
#
testing
#
react
#
webdev
79
reactions
Comments
3
comments
2 min read
All about unit testing: 11 best practices and overview
ericaeducative
ericaeducative
ericaeducative
Follow
for
Educative
Apr 19 '22
All about unit testing: 11 best practices and overview
#
codenewbie
#
beginners
#
testing
#
debugging
40
reactions
Comments
Add Comment
8 min read
What makes a good QA Engineer Part 1: Tester Mindset
Moaaz Adel
Moaaz Adel
Moaaz Adel
Follow
Apr 20 '22
What makes a good QA Engineer Part 1: Tester Mindset
#
testing
#
qualityassurance
#
testautomation
#
softwaretesting
10
reactions
Comments
Add Comment
2 min read
Testing Electron Apps with Playwright
Kubeshop
Kubeshop
Kubeshop
Follow
for
Kubeshop
Apr 20 '22
Testing Electron Apps with Playwright
#
testing
#
kubernetes
#
programming
#
opensource
10
reactions
Comments
Add Comment
6 min read
Visual Testing With Cypress
Phil
Phil
Phil
Follow
Apr 20 '22
Visual Testing With Cypress
#
webdev
#
testing
#
javascript
#
tutorial
6
reactions
Comments
Add Comment
5 min read
What Is Cross Browser Compatibility And Why We Need It?
Deeksha Agarwal
Deeksha Agarwal
Deeksha Agarwal
Follow
for
LambdaTest
Apr 20 '22
What Is Cross Browser Compatibility And Why We Need It?
#
techtalks
#
testing
#
webdev
#
database
6
reactions
Comments
Add Comment
8 min read
Automation Testing Tutorial: A Starter’s Guide With Example
Riadayal
Riadayal
Riadayal
Follow
for
LambdaTest
Apr 20 '22
Automation Testing Tutorial: A Starter’s Guide With Example
#
testing
#
programming
#
cloud
#
webdev
8
reactions
Comments
Add Comment
18 min read
4 Most Common Myths Around Test Automation
DEEPIKA K
DEEPIKA K
DEEPIKA K
Follow
Apr 19 '22
4 Most Common Myths Around Test Automation
#
testing
#
testautomation
#
testautomationmyths
#
softwaretesting
7
reactions
Comments
Add Comment
5 min read
Introducing Ladle for React Stories
Vojtech Miksu
Vojtech Miksu
Vojtech Miksu
Follow
Mar 29 '22
Introducing Ladle for React Stories
#
react
#
javascript
#
testing
#
vite
8
reactions
Comments
Add Comment
3 min read
Test Verification vs Validation in Website Testing
arnabroychowdhury
arnabroychowdhury
arnabroychowdhury
Follow
for
LambdaTest
Apr 19 '22
Test Verification vs Validation in Website Testing
#
testing
#
website
#
automation
#
javascript
9
reactions
Comments
Add Comment
11 min read
How To Use TestNG Reporter Log In Selenium
Riadayal
Riadayal
Riadayal
Follow
for
LambdaTest
Apr 19 '22
How To Use TestNG Reporter Log In Selenium
#
testing
#
javascript
#
webdev
#
programming
7
reactions
Comments
Add Comment
13 min read
Unit Testing ArcGIS API for JavaScript Apps
Rene Rubalcava
Rene Rubalcava
Rene Rubalcava
Follow
Apr 18 '22
Unit Testing ArcGIS API for JavaScript Apps
#
javascript
#
webdev
#
beginners
#
testing
9
reactions
Comments
Add Comment
5 min read
Do you use cypress.io to test your UI against your apis?
kryptobi
kryptobi
kryptobi
Follow
Apr 18 '22
Do you use cypress.io to test your UI against your apis?
#
testing
#
cypress
#
api
3
reactions
Comments
Add Comment
1 min read
Finding Cross Browser Compatibility Issues in HTML and CSS
Deeksha Agarwal
Deeksha Agarwal
Deeksha Agarwal
Follow
for
LambdaTest
Apr 18 '22
Finding Cross Browser Compatibility Issues in HTML and CSS
#
webdev
#
css
#
html
#
testing
10
reactions
Comments
Add Comment
5 min read
Dogfooding At Armory
Dan Peach
Dan Peach
Dan Peach
Follow
for
Armory
Apr 26 '22
Dogfooding At Armory
#
dogfooding
#
qa
#
testing
1
reaction
Comments
Add Comment
4 min read
How To Set Test Case Priority In TestNG With Selenium
Riadayal
Riadayal
Riadayal
Follow
for
LambdaTest
Apr 18 '22
How To Set Test Case Priority In TestNG With Selenium
#
testing
#
tutorial
#
test
#
javascript
10
reactions
Comments
Add Comment
21 min read
🚀 Valgrind automatic test suite
❮ ZI ❯
❮ ZI ❯
❮ ZI ❯
Follow
for
Z-Shell
Apr 18 '22
🚀 Valgrind automatic test suite
#
zsh
#
testing
#
hacktoberfest
#
tooling
6
reactions
Comments
Add Comment
3 min read
Talkingavatar.la AI-powered Video Creator
Talking Avatar
Talking Avatar
Talking Avatar
Follow
Apr 18 '22
Talkingavatar.la AI-powered Video Creator
#
help
#
testing
#
productivity
#
css
8
reactions
Comments
Add Comment
1 min read
Running a Postman Collection in a CI Pipeline
Dennis Whalen
Dennis Whalen
Dennis Whalen
Follow
for
Leading EDJE
Apr 18 '22
Running a Postman Collection in a CI Pipeline
#
postman
#
testing
#
api
#
devops
8
reactions
Comments
Add Comment
5 min read
Changing the default testing container
Chris Bongers
Chris Bongers
Chris Bongers
Follow
Apr 17 '22
Changing the default testing container
#
testing
14
reactions
Comments
Add Comment
2 min read
Recovering Data From RAW Flash Drive or Memory Card (SD, MicroSD)
Michael Mirosnichenko
Michael Mirosnichenko
Michael Mirosnichenko
Follow
Apr 17 '22
Recovering Data From RAW Flash Drive or Memory Card (SD, MicroSD)
#
tutorial
#
testing
#
test
#
beginners
4
reactions
Comments
Add Comment
8 min read
How to mock node-fetch with Vitest
Akira Kashihara
Akira Kashihara
Akira Kashihara
Follow
Apr 17 '22
How to mock node-fetch with Vitest
#
javascript
#
vitest
#
node
#
testing
7
reactions
Comments
Add Comment
2 min read
Testing library and React context
Chris Bongers
Chris Bongers
Chris Bongers
Follow
Apr 16 '22
Testing library and React context
#
react
#
testing
20
reactions
Comments
2
comments
3 min read
Check 100% Working Final Exam Preparation Tips Suggested By Teachers
statanalytica
statanalytica
statanalytica
Follow
Apr 16 '22
Check 100% Working Final Exam Preparation Tips Suggested By Teachers
#
study
#
tips
#
exams
#
testing
4
reactions
Comments
1
comment
3 min read
Authentication in Tests with DRF
Mangabo Kolawole
Mangabo Kolawole
Mangabo Kolawole
Follow
Apr 15 '22
Authentication in Tests with DRF
#
django
#
testing
#
python
12
reactions
Comments
3
comments
1 min read
Debugging testing library tests
Chris Bongers
Chris Bongers
Chris Bongers
Follow
Apr 15 '22
Debugging testing library tests
#
testing
19
reactions
Comments
Add Comment
2 min read
Buddies! We need your help!
Viber
Viber
Viber
Follow
Apr 15 '22
Buddies! We need your help!
#
discuss
#
beginners
#
webdev
#
testing
4
reactions
Comments
1
comment
1 min read
Testes de integração para API com Typescript, mocha, chai e sinon
Matheus Santos
Matheus Santos
Matheus Santos
Follow
Apr 14 '22
Testes de integração para API com Typescript, mocha, chai e sinon
#
typescript
#
testing
#
backend
32
reactions
Comments
3
comments
15 min read
Selenium Grid Setup with Docker
Arvind Choudhary
Arvind Choudhary
Arvind Choudhary
Follow
Apr 15 '22
Selenium Grid Setup with Docker
#
automation
#
docker
#
selenium
#
testing
6
reactions
Comments
Add Comment
2 min read
Benefits Of Cloud Testing And Best Practices
RileenaSanyal1
RileenaSanyal1
RileenaSanyal1
Follow
for
LambdaTest
Apr 4 '22
Benefits Of Cloud Testing And Best Practices
#
cloud
#
testing
#
security
#
tooling
5
reactions
Comments
Add Comment
13 min read
A Better Way To Code: Documentation Driven Development
Corbin Crutchley
Corbin Crutchley
Corbin Crutchley
Follow
for
This is Learning
Apr 8 '22
A Better Way To Code: Documentation Driven Development
#
testing
#
tdd
#
programming
194
reactions
Comments
18
comments
7 min read
Automated Cross Browser Testing
Deeksha Agarwal
Deeksha Agarwal
Deeksha Agarwal
Follow
for
LambdaTest
Apr 14 '22
Automated Cross Browser Testing
#
automation
#
webdev
#
productivity
#
testing
9
reactions
Comments
Add Comment
6 min read
2Captcha Review 2022 | Principles & Usage & Getting Started
openHacking
openHacking
openHacking
Follow
Apr 14 '22
2Captcha Review 2022 | Principles & Usage & Getting Started
#
javascript
#
node
#
testing
3
reactions
Comments
1
comment
5 min read
How to Recover Files Deleted Without Using the Recycle Bin?
Michael Mirosnichenko
Michael Mirosnichenko
Michael Mirosnichenko
Follow
Apr 14 '22
How to Recover Files Deleted Without Using the Recycle Bin?
#
beginners
#
testing
#
test
#
tutorial
4
reactions
Comments
Add Comment
7 min read
Testing library awaiting queries
Chris Bongers
Chris Bongers
Chris Bongers
Follow
Apr 14 '22
Testing library awaiting queries
#
testing
14
reactions
Comments
Add Comment
2 min read
loading...
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
SciChart.js Performance Demo: 1 Million Datapoints in under 15ms - DEV Community,"SciChart.js Performance Demo: 1 Million Datapoints in under 15ms - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Andrew Bt
Posted on
Apr 5
• Originally published at
scichart.com
SciChart.js Performance Demo: 1 Million Datapoints in under 15ms
#
webdev
#
javascript
#
datapoints
//
Are you looking for the Fastest JavaScript Charts?
SciChart.js, our
Ultra-High Performance JavaScript Chart library
is now released!
“With WebAssembly and WebGL hardware acceleration and an optimized render pipeline,
SciChart provides extremely fast JavaScript charts capable of drawing millions of data-points in realtime …
”
[read more]
WHY SCICHART: THE FASTEST JAVASCRIPT CHARTS
See our live demos in the SciChart.js examples suite:
demo.scichart.com
If you haven't read already, at
SciChart
we're developing a
Javascript chart component which specialises in fast 2D/3D charts & graphs
for your
Javascript
,
Typescript
,
React
,
Angular
and possibly even
Blazor
applications. SciChart already exists on other platforms, such as
Windows (WPF)
,
iOS
and
Android
and boasts outstanding performance: able to draw millions of data-points in real-time on mobile, or even
plot billions of data-points
in real-time on Windows. This opens up a whole host of previously impossible applications, such as: visualising real-time ECG waveforms on a mobile device, big data analysis of telemetry on Windows PCs, or field
lidar-visualisation
on an iPad.
So what about Javascript? Can we match the incredible performance of SciChart on this platform? Turns out we can! We've taken our 3D engine - the result of years of Research and Development and compiled it to
WebAssembly
with
WebGL
.
In a previous post we talked about our progress
toward SciChart.js and showcased some real-time charts, and also showcased some
Javascript 3D data-visualisation
that we're building for web browsers.
In this post I'm going to show you a very quick demo of the speed & performance of SciChart for Javascript: loading 1 million data-points in under 15 milliseconds.
This is a simple test which shows how fast SciChart.js is at loading, processing and drawing data. Results below!
The Performance Test Source Code
SciChart.js is still in development, and we don't have a version released for testing just yet, but we wanted to share the source-code for the example so you can see how we're logging time.
First of all we have to create the
SciChartSurface
. At the moment in the API there is a helper function which initializes and returns a webassembly context. We might change this before the first release. Then there is code to add XAxis, YAxis and set RenderableSeries. SciChart has the concept of separating out RenderableSeries and DataSeries so you can update an object in real-time and change how it draws or renders separately.
// Create the WebAssembly context and SciChartSurface
const { wasmContext, sciChartSurface } = await createSciChartSurface(divElementId, WIDTH, HEIGHT);

// Apply a theme
sciChartSurface.applyTheme(new SciChartJSDarkTheme());

// Create an XAxis and add to the chart
const xAxis = new NumericAxis(wasmContext, {
    axisAlignment: EAxisAlignment.Top,
    visibleRange: new NumberRange(0, 1000000),
    autoRange: EAutoRange.Never
});
sciChartSurface.xAxes.add(xAxis);
// Create a YAxis and add to the chart
const yAxis = new NumericAxis(wasmContext, {
    axisAlignment: EAxisAlignment.Left,
    visibleRange: new NumberRange(-5000, 5000),
    autoRange: EAutoRange.Never
});
sciChartSurface.yAxes.add(yAxis);

// Create a DataSeries of Xy data
const dataSeries = new XyDataSeries(wasmContext);
// Create the Line series and set properties
const lineSeries = new FastLineRenderableSeries(wasmContext, { dataSeries, strokeThickness: 2 });
// Add the line series to the chart
sciChartSurface.renderableSeries.add(lineSeries);
lineSeries.stroke = ""#42b649"";

// Set interactivity modifiers: Zoom, pan, mousewheel zoom
sciChartSurface.chartModifiers.add(
    new ZoomExtentsModifier(), 
    new ZoomPanModifier(), 
    new MouseWheelZoomModifier());
This code creates a SciChartSurface, adds the axis and renders the empty chart on the screen. The output so far can be seen below:
The empty SciChartSurface. Now to create a click handler which loads some data.
Next we add a button click handler to load the data and perform the performance test. The click handler code looks like this:
// Buttons for chart
const loadPoints = () => {
    console.log(""load points"");

    // Clear state
    dataSeries.clear();
    const newTimeSpans: TTimeSpan[] = [];

    // Start counting Points generation time
    const generateTimestamp = Date.now();

    // Generate 1,000,000 points as a Random walk 
    const POINTS = 1000000;
    const xValues = new Array(POINTS);
    const yValues = new Array(POINTS);
    let prevYValue = 0;
    for (let i = 0; i < POINTS; i++) {
        const curYValue = Math.random() * 10 - 5;
        xValues[i] = i;
        yValues[i] = prevYValue + curYValue;
        prevYValue += curYValue;
    }

    // Log time: Generating 1M data points
    newTimeSpans.push({
        title: ""Generate 1M Data Points"",
        durationMs: Date.now() - generateTimestamp
    });

    // Start counting data append time
    const appendTimestamp = Date.now();
    dataSeries.appendRange(xValues, yValues);

    // Log time: Appending data to SciCHart
    newTimeSpans.push({
        title: ""Append 1M Data Points"",
        durationMs: Date.now() - appendTimestamp
    });

    // Subscribe to sciChartSurface.rendered event,
    // and calculate time duration for drawing
    const firstFrameTimestamp = Date.now();
    let frameIndex: number = 0;
    // We're going to measure 10 frames 
    const numberFramesToMeasure: number = 10;
    let nextFramesTimestamp: number;
    const handler = () => {
        if (frameIndex === 0) {            
            newTimeSpans.push({
                title: ""Render the first frame"",
                durationMs: Date.now() - firstFrameTimestamp
            });
            nextFramesTimestamp = Date.now();
            // sciChartSurface.zoomExtents();
        } else if (frameIndex === numberFramesToMeasure + 1) {            
            newTimeSpans.push({
                title: ""Render next frames average"",
                durationMs: (Date.now() - nextFramesTimestamp) / numberFramesToMeasure
            });

            // After 10 frames update the UI and unsubscribe from sciChartSurface.rendered
            updateTimeSpans(newTimeSpans);
            sciChartSurface.rendered.unsubscribe(handler);
        }
        // Trigger a redraw immediately 
        setTimeout(sciChartSurface.invalidateElement, 0);
        // Increment frame index
        frameIndex++;
    };
    sciChartSurface.rendered.subscribe(handler);
};

document.getElementById(""loadPoints"").addEventListener(""click"", loadPoints);
When we click the button the code above is going to create a 1-million point (x,y values) of a Random walk, and then set it into SciChart.js using dataSeries.appendRange(xValues, yValues). Finally we use the SciChartSurface.rendered event (callback) to measure actual time between draws and calculate the time to render the first frame, and subsequent 10 frames to compute an average.
Here are the results!
In the performance test, SciChart.js renders 1,000,000 data-points in 35ms (first draw), and 15ms (subsequent draw). Data generation takes 27ms and appending to the chart takes 35ms.
Test 1 (first click after showing a chart)
Generate 1M points of random walk: 27ms
Append 1M points to Scichart: 34ms
Render the first frame: 35ms
Total data-append and draw time: 69ms
Test 2 (subsequent click)
Generate 1M points of random walk: 28ms
Append 1M points to Scichart: 37ms
Render the first frame: 15ms
Total data-append and draw time: 52ms
Fully Interactive Big Data Charts
What's more, this data is all there. We haven't down sampled it or pre-processed it at all. It's not a static image and it's not server-side rendered. You can actually zoom and pan right into the chart and see all the data. Take a look:
SciChart.js allows you to browse big data-sets statically or in real-time and does not pre-process or downsample data.
Using Emscripten Compiler Options for further Performance Improvements
But wait! 50-70 milliseconds is pretty good but can it go any further? Turns out that it can. When I did the above test
I forgot to enable Emscripten compiler options for maximum performance
😀.
There's a rundown on what the different compiler options do at the
Emscripten compiler documentation.
For the following test I enabled
flag -O3 Maximum Speed optimisation
. This also reduces wasm / WebAssembly file size also making it blazing fast for use on the web. Here is the result:
SciChart.js Performance Test: Loading 1 Million Points in under 15 milliseconds. This is the world's fastest Javascript chart!
Wow!! 🚀🚀 That's just incredible. One million points appended to the chart, and rendered in 13 milliseconds when full optimisations are enabled. There is not a Javascript chart library in the world that can match this performance. The speed, power and performance of SciChat with WebAssembly/WebGL is just unmatched...
Incredible Performance in a Javascript Chart
SciChart.js in the test above is able to draw add and draw million data-points in line charts in under 15 milliseconds, with a redraw time as fast as 10ms once the data has been loaded. This incredible performance of SciChart.js opens up a whole host of applications that were previously impossible, now made possible.
For example:
Could you imagine a
streaming dashboard of patient ECG data on a doctors computer or mobile device written in Javascript
?
Creating a
field scientific or surveying device that plots data from multiple sensors
in real-time in Javascript?
Plotting
telemetry from formula one vehicles or machinery
in an automated process to devices and applications written in Javascript.
All the above were previously impossible projects, but will be made possible by
SciChart.js - High Performance Realtime Javascript Charts
.
SciChart.js webinar in a bit more detail: ECG, Realtime Stock Charts, 1 Million Points & more!
SciChart.js Release Date
SciChart.js - our GPU Accelerated 2D/3D
JavaScript Chart
has now been released! In fact, we have just released a version 2 (November 2021) which is a Huge upgrade to v1.
If you want to learn more about SciChart.js or start a trial, or test drive the features by looking at our
Js Charts Demo
, click the link below.
SciChart.js JavaScript Chart Library
About SciChart: High Performance Realtime Charts
SciChart provides high performance realtime chart components on the WPF (Windows), iOS, Android
and JavaScript
platforms. It is our goal to create the best cross-platform Native WPF, iOS, Android and JavaScript 2D & 3D Charts in the world, focusing on performance, developer productivity, ease of use, depth of features and enterprise-grade tech support.
If you have a question about what SciChart's WebGL / WebAssembly Charts can offer you, or if you would like to get a quote, please
contact us
. Our friendly, helpful support team will be glad to help!
Contact Us
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Setup Mixpanel Analytics in a NextJS Application
Mohammad Faisal -
Dec 1
I Built a Chrome Extension to Bring Back the Google Maps Tab
Lukas Karsch -
Nov 26
Building a Single Page Weather Application in JavaScript
CourtneyKerr -
Nov 26
Easy Animation with Alpine.js
Abhishek Deshpande -
Nov 27
Andrew Bt
Follow
Former electronic engineer who works in software. With experience in languages from C/C++ to C#, JavaScript and TypeScript, and now specialises in performance optimisation and data visualisation
Location
United Kingdom
Joined
Dec 14, 2023
More from
Andrew Bt
How Fast is SciChart’s iOS Chart?
#
webdev
#
javascript
SciChart.js Javascript 3D Charts with WebGL & WebAssembly
#
webdev
#
javascript
#
charts
#
webassembly
Algorithmic Trading with SciChart
#
webdev
#
javascript
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Full-cycle testing: hands-on tips to troubleshoot QA hurdles - DEV Community,"Full-cycle testing: hands-on tips to troubleshoot QA hurdles - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Dzmitry Lubneuski
Posted on
Aug 30
• Edited on
Sep 3
Full-cycle testing: hands-on tips to troubleshoot QA hurdles
#
qa
#
webdev
#
testautomation
#
programming
With the rapid advancement of digitization and cutting-edge technologies, modern IT products are becoming increasingly complex. Open banking systems, eCommerce platforms, CRMs, ERPs, CMSs, AR/VR-based games, dApps, and virtual fitting rooms are all marked by intricate business logic, multifaceted architectures, numerous dependencies, integrations, and even the incorporation of big data. These products often include sophisticated features, such as biometric authentication. For example, virtual fitting rooms may offer 3D body scanning, augmented reality integration, fabric simulation, size recommendation engines, and virtual try-on capabilities, enhancing the online shopping experience.
Ensuring their failsafe operation is also becoming more challenging. Sporadic or chaotic testing activities can only aggravate a situation and cause insufficient test coverage, issues slipping into the production environment, missed deadlines, and even increased costs because of rework.
Luckily, these can be mitigated with
full-cycle testing.
But how to ensure its effectiveness and efficiency? In this article, we provide a closer look on this concept, its ubiquitous obstacles, and approaches for surmounting them.
The what, when, and why of full-cycle testing
Full-cycle testing can be characterized as an all-encompassing QA approach that starts from the early phases of the Software Development Life Cycle (SDLC) and continues through post-release support. This continuous, holistic testing process consists of the following essential, sequential activities: delving into requirements, planning, preparing a test approach or creating a test plan, creating a test model, developing test cases (including automated scripts), generating high-quality test data, setting up environments, conducting tests, creating reports, and post-release monitoring.
Companies should consider implementing Full cycle testing in
several scenarios.
Complex IT products that possess various integrations and rapidly accrete new components should be continuously verified to prevent new defects and crashes. Ongoing releases also require regular QA supervision to ensure they operate as intended. Full cycle testing is also a good choice when organizations need to make sure their software complies with regulatory demands. Finally, development projects that span for years and incorporate legacy code should also be supported with QA.
When applied correctly, full cycle testing provides
multiple benefits
to project teams, allowing them to significantly extend test coverage, find errors early, improve software quality, and minimize the chances of rework. In addition, when IT products operate like clockwork, end users will probably like them and may even advertise it by word-of-mouth recommendations.
Full-cycle testing challenges: things aren’t always simple
Testing is a momentous step vital for ensuring correct software operation. However, project teams can confront QA challenges that may complicate reaching set objectives. It’s important to be aware of them and know how to address them with a well-tuned test strategy.
Pressing deadlines
Almost half of all respondents of the latest
report
stated that the shortage of time has become one of the key problems when ensuring software quality. We all understand that meeting milestones is hardly possible without sticking to a set schedule. However, when QA engineers follow a tight release timetable and perform a myriad of tasks in no time, of course, software quality can be compromised, and overtime can become QA engineers' constant companion.
Poor interaction
There's a risk that when collaboration between QA engineers, software developers, business analysts, and other stakeholders is insufficient—particularly in globally distributed teams—the work environment can become unstable, and confusion may arise. This can slow down decision-making and compromise the quality of work. To mitigate these risks, it's important to adopt clear communication strategies and utilize collaboration tools that facilitate thorough discussion and planning of software requirements, potential challenges, and user journeys. By ensuring effective coordination, teams can maintain project stability and deliver high-quality outcomes, regardless of team location.
Shortage of experts
The ratio of QA engineers to software developers is of high importance, if an imbalance occurs, risks related to small test coverage may arise, leading to late identification of defects, security vulnerabilities, software inability to cope with high loads, and other problems that can negatively affect end-user experience.
Issues with requirements
Vague, missing, or constantly changing requirements introduce significant complexity, as QA engineers rely on them as the foundation for planning the scope of work and defining necessary activities. When requirements are unclear or unstable, project teams may end up with incomplete test cases, which can lead to issues in the production environment.
Continuous technological upgrade
IT ecosystems are changing at a fast pace. State-of-the-art development solutions, such as frameworks, libraries, or tools, appear quite often. It means that if QA engineers don’t upgrade their competencies and QA techniques on an ongoing basis, they likely won’t manage to ensure efficient quality control activities and may skip critical or major issues.
Incorrect priorities
Prioritizing test cases based on the criticality of features for business success is crucial in any software development project. This ensures that key functionalities, such as payment processing and product catalogs in eCommerce solutions or billing, service activation, and bandwidth management in telecom software, operate as required. If priorities are incorrectly defined, there is a risk of unoptimized testing efforts, which can lead to insufficient focus on the most critical areas, potentially compromising the overall quality and reliability of the product.
Problems with test environments
QA engineers are responsible for properly setting up testing environments, ensuring they closely resemble the production environment to the greatest extent possible, and contributing to their stability. Working with test data is another essential activity—in fact, according to the World Quality Report 2023-2024 (WQR),
69% of organizations are working on implementing a company-wide test data provisioning strategy.
QA specialists must adopt a specific approach to ensure that the data used is relevant, diverse, complete, and protected. Without effective management of these activities, there is a risk of invalid testing outcomes, which can affect the software's quality.
Small test coverage
Regardless of the type of software testing, test coverage is one of the most crucial aspects that QA engineers must focus on. It represents the extent of code covered by various QA verifications, serving as a key indicator of overall IT product quality. When test coverage is insufficient, the risk of undetected issues increases, leading to potential problems in production and higher maintenance costs. Therefore, ensuring adequate test coverage is essential to minimizing risks and maintaining software quality.
Low effectiveness of automated workflows
Test automation serves as a powerful tool to improve the capabilities of manual QA engineers. The WQR interviewees stated that it helps
minimize risks (54%), boost test efficiency (52%), decrease the number of live defects (51%), and speed up time to market (42%).
Its improper introduction or ineffectiveness can cause issues with stability and further maintenance.
QA solutions to surmount full-cycle testing challenges
Considering above-mentioned risks, it’s a good idea to understand how they can be mitigated by testing best practices.
1. Ensure productive communication
The success of any activity involving more than two people relies heavily on open and effective communication, particularly in software development projects where QA engineers, developers, business analysts, product owners, and other stakeholders work together as distributed teams to release robust IT products.
Establishing stable interaction channels for both synchronous and asynchronous communication is crucial, along with introducing tools that facilitate transparent and easy monitoring of all project activities. Regular team meetings are also essential to discuss questions, progress, and problems, ensuring that everyone is informed about relevant project aspects and milestones. Team members should practice active listening, maintain respectful behavior, and work to prevent escalation, especially during conflicts, to foster a collaborative and productive environment.
2. Improve requirements-related processes
To ensure consistent test cases and prevent defects in the production environment, it's crucial that requirements are always complete and up to date. QA engineers should actively clarify any ambiguities with other team members and promptly update test documentation as changes arise. While documenting the requirements is primarily the responsibility of business analysts, QA specialists play a critical role in analyzing the impact of any changes or omissions in the requirements. They must assess how these changes affect the software and the existing test cases, ensuring that test documentation is accurately adjusted to reflect the current state of the project..
3. Decide what tests to execute first
QA managers should apply several critical practices to prioritize testing effectively. First, they need to analyze existing requirements and create a traceability matrix to ensure each requirement is covered by one or more test cases. Based on the criticality of the requirements to the business, managers can then decide which cases should be executed first. Regular meetings should be held to reassess priorities, especially if there are changes to the requirements, ensuring that testing efforts remain aligned with project goals.
4. Cultivate a culture of continuous education
Even if a company faces temporary constraints in hiring new project team members, it can still invest in improving the skills of its existing staff. Beyond smart prioritization of test cases and enhancing test automation, companies can focus on upskilling their current resources. By offering practical-oriented courses, facilitating knowledge transfer, and establishing Centers of Excellence (CoEs) and Research and Development (R&D) initiatives, companies can significantly boost the efficiency of their QA engineers and overall project outcomes.
5. Introduce continuous testing (CT)
Continuous Testing (CT) is essential for minimizing challenges in full-cycle testing  CT ensures ongoing, early, and automated testing throughout the entire SDLC. This approach enables QA engineers to increase test coverage, identify issues of varying severity earlier, and reduce the risk of defects in the production environment. CT also provides continuous feedback on software quality, helping to make timely improvements and increasing the velocity of the testing process, which contributes to meeting quality gates on time.
6. Consider agile testing approaches
Unlike the Waterfall model, where testing is conducted near the end of the development cycle, Agile emphasizes continuous quality assurance activities throughout the process. Agile testing involves close and open communication among all project members, high flexibility to adapt to changing conditions, and a focus on customer needs. By adopting Agile testing approaches, QA engineers can enhance team collaboration, test software in parallel with development, manage evolving requirements more effectively, and reduce the risk of software issues.
7. Boost capabilities of automated workflows
To ensure the effective adoption of test automation and achieve reliable results, QA engineers should carefully identify which tests are best suited for automation, such as regression, performance, and cybersecurity tests that require frequent execution. They should prioritize the scope of automation and consider both functional and non-functional requirements to maximize test coverage. Additionally, ensuring the stability of test environments and selecting a toolkit that aligns with the team's competencies and project specifics are crucial. As software functionality expands over time, it’s advisable to write short, independent automated scripts and regularly update them to facilitate more effective maintenance.
Closing thoughts
Although full-cycle testing provides various benefits, such as sped up time to market, minimized chances of production defects, early identification of issues, or lessened probability of rework, QA teams can still confront challenges, impeding its effectiveness.
To successfully overcome them, companies can introduce CT and Agile testing, improve test automation effectiveness, establish a communication framework, ensure high quality of requirements, prioritize test cases, and stick to continuous learning.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
The AI Revolution: How Generative AI is Reshaping Our World
IvanDev -
Dec 11
Every Student Needs to Join a Tech Community
Iqra Firdose  -
Dec 10
See what's new in Gimli Tailwind 4.4
Gimli -
Nov 27
JavaScript Object Destructuring
Tyn -
Dec 10
Dzmitry Lubneuski
Follow
Dzmitry Lubneuski is the CIO at a1qa, a next-generation pure-play software testing company. With a strong IT background and a passion for innovation, he focuses on enhancing the company’s IT strategy.
Work
CIO at a1qa
Joined
Aug 30, 2024
Trending on
DEV Community
Hot
Code Smarter, Not Harder: Tips You Wish You Knew Yesterday
#
webdev
#
javascript
#
programming
#
beginners
What is your favorite IDE?
#
discuss
#
webdev
#
vscode
🌐 100+ Free APIs for Developers in 2024 🚀
#
api
#
productivity
#
discuss
#
opensource
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
Scala vs. Java: The Superior Choice for Big Data and Machine Learning - DEV Community,"Scala vs. Java: The Superior Choice for Big Data and Machine Learning - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Aditya Pratap Bhuyan
Posted on
Oct 1
Scala vs. Java: The Superior Choice for Big Data and Machine Learning
#
java
#
scala
#
machinelearning
#
bigdata
In the rapidly evolving landscapes of big data and machine learning, selecting the right programming language is crucial for efficiency, scalability, and innovation. While Java has long been a staple in enterprise environments, Scala emerges as a powerful alternative, offering distinct advantages that make it particularly well-suited for these demanding fields. This article delves into the reasons why Scala outperforms Java in big data and machine learning applications, providing a comprehensive analysis for developers and organizations seeking to leverage the full potential of their data-driven projects.
Introduction
Big data and machine learning have transformed industries by enabling organizations to harness vast amounts of data to drive decision-making, optimize operations, and innovate products and services. Central to these advancements is the choice of programming language, which can significantly impact the development process, performance, and scalability of data-driven applications. Scala, a modern programming language that seamlessly integrates object-oriented and functional programming paradigms, has gained prominence as a preferred language over Java in these domains. This article explores the key advantages of using Scala over Java in big data and machine learning, providing detailed insights into why Scala might be the optimal choice for your next project.
1. Functional Programming Paradigm
Scala's Functional Nature
Scala is inherently a functional programming language, which promotes immutability, higher-order functions, and concise code. This paradigm is particularly beneficial in big data and machine learning for several reasons:
Immutability:
Immutable data structures prevent unintended side effects, making code more predictable and easier to debug. In big data processing, where operations on large datasets are common, immutability ensures consistency and reliability.
Higher-Order Functions:
Functions that take other functions as parameters or return them as results enable more abstract and reusable code. This flexibility is advantageous in machine learning algorithms that often require complex transformations and data manipulations.
Conciseness:
Functional programming allows developers to write less code to achieve the same functionality, reducing the likelihood of errors and improving maintainability. In large-scale data projects, concise code can lead to significant time savings.
Java’s Object-Oriented Approach
Java primarily follows an object-oriented programming (OOP) paradigm, which, while powerful, can be more verbose and less flexible for certain data processing tasks. Although Java has introduced functional programming features in recent versions (e.g., lambdas and streams), Scala's design deeply integrates functional programming, offering a more seamless and efficient experience.
2. Interoperability with Java
Seamless Integration
One of Scala’s standout features is its seamless interoperability with Java. Since Scala runs on the Java Virtual Machine (JVM), it can directly utilize Java libraries and frameworks without any performance overhead. This compatibility allows developers to leverage existing Java ecosystems while enjoying the modern features of Scala.
Library Access:
Scala developers can access and use a vast array of Java libraries, tools, and frameworks, ensuring that they are not limited by the availability of Scala-specific resources.
Ease of Transition:
Teams with existing Java codebases can incrementally adopt Scala, integrating it with their current projects without a complete overhaul. This flexibility is particularly beneficial for large organizations looking to modernize their systems.
Enhanced Productivity
By combining Scala’s expressive syntax with Java’s robust libraries, developers can achieve higher productivity. Tasks that might require more boilerplate code in Java can often be accomplished with fewer lines in Scala, accelerating development cycles and reducing the potential for bugs.
3. Advanced Type System
Type Inference
Scala boasts an advanced type system with powerful type inference capabilities. This means that the compiler can often deduce the types of variables and expressions without explicit type declarations, leading to more readable and maintainable code.
Reduced Boilerplate:
Developers can write cleaner code without repeatedly specifying types, which is especially useful in complex data processing pipelines common in big data applications.
Type Safety:
Scala’s type system ensures that many errors are caught at compile-time rather than at runtime, enhancing the reliability of machine learning models and big data applications.
Generic Programming
Scala supports generic programming, allowing developers to write highly reusable and type-safe code. This is advantageous in machine learning where generic algorithms and data structures are frequently utilized.
4. Concurrency and Parallelism
Akka Framework
Scala’s compatibility with the Akka framework facilitates building highly concurrent and distributed systems. Akka employs the actor model, which simplifies the development of scalable and fault-tolerant applications.
Scalability:
In big data environments where processing large volumes of data concurrently is essential, Akka provides the tools to efficiently manage resources and scale applications seamlessly.
Fault Tolerance:
Akka’s design inherently supports resilience, ensuring that applications can recover gracefully from failures, a critical aspect in large-scale data processing.
Parallel Collections
Scala offers parallel collections out of the box, enabling effortless parallelism in data processing tasks. This feature allows developers to leverage multi-core processors effectively, improving the performance of big data and machine learning applications.
Ease of Use:
Implementing parallelism in Scala is straightforward, requiring minimal changes to existing code. This simplicity accelerates the development process and enhances performance without significant overhead.
5. Expressive and Concise Syntax
Code Readability
Scala’s syntax is more expressive and concise compared to Java, enabling developers to write clearer and more maintainable code. This expressiveness is particularly beneficial in complex data processing and machine learning algorithms, where readability is paramount.
Less Boilerplate:
Scala reduces the amount of boilerplate code needed, allowing developers to focus on the core logic rather than repetitive syntax.
Enhanced Expressiveness:
Features like case classes, pattern matching, and for-comprehensions enable developers to express complex operations succinctly and elegantly.
DSL Support
Scala’s flexibility allows the creation of domain-specific languages (DSLs), which can be tailored to specific tasks in big data and machine learning. DSLs enhance productivity by providing more intuitive and higher-level abstractions for complex operations.
Custom DSLs:
Developers can create custom DSLs to simplify interactions with big data frameworks or machine learning libraries, streamlining workflows and improving developer experience.
6. Spark Integration
Native Support for Apache Spark
Scala is the native language for Apache Spark, one of the most popular big data processing frameworks. While Spark provides APIs for Java, Scala’s integration is more seamless and feature-rich.
Performance:
Scala’s compatibility with Spark allows for more efficient code execution, leveraging Spark’s full capabilities without the limitations sometimes encountered with Java APIs.
Advanced Features:
Scala’s functional programming features enable the use of Spark’s advanced functionalities more naturally, such as transformation operations and lazy evaluation.
Community and Ecosystem
The Scala community has been instrumental in developing Spark’s ecosystem, ensuring that Scala remains at the forefront of big data innovations. This active community support translates to better resources, documentation, and third-party libraries for Scala-based big data projects.
7. Immutability and Concurrency
Immutable Data Structures
In big data and machine learning, handling immutable data structures is crucial for ensuring data integrity and consistency across distributed systems. Scala inherently emphasizes immutability, making it easier to develop robust and error-free applications.
Thread Safety:
Immutable data structures are inherently thread-safe, simplifying the development of concurrent applications and reducing the risk of race conditions and deadlocks.
Predictable Behavior:
With immutability, the state of data does not change unexpectedly, leading to more predictable and maintainable codebases.
Concurrency Models
Scala’s support for advanced concurrency models, such as the aforementioned actor model in Akka, provides developers with powerful tools to manage parallel processing efficiently. This capability is essential in big data environments where tasks need to be executed simultaneously across multiple nodes.
8. Performance and Optimization
JVM Performance
Scala runs on the JVM, benefiting from the same performance optimizations and just-in-time (JIT) compilation as Java. However, Scala’s more concise and expressive code can lead to performance improvements by reducing overhead and enhancing code efficiency.
Optimized Bytecode:
Scala compiles to optimized JVM bytecode, ensuring that applications run efficiently without sacrificing performance.
Garbage Collection:
Scala leverages the JVM’s garbage collection mechanisms, providing robust memory management that is critical in large-scale data processing.
Tail Recursion and Lazy Evaluation
Scala’s support for tail recursion and lazy evaluation allows for optimized memory usage and performance improvements, particularly in recursive algorithms and large data transformations common in machine learning.
Tail Recursion:
Enables the compiler to optimize recursive calls, preventing stack overflow errors and improving performance in recursive data processing tasks.
Lazy Evaluation:
Defers computation until necessary, reducing memory consumption and improving the efficiency of data pipelines.
9. Advanced Language Features
Pattern Matching
Scala’s powerful pattern matching capabilities allow for elegant and efficient handling of complex data structures. This feature is particularly useful in machine learning for tasks such as data preprocessing, feature extraction, and model evaluation.
Simplified Code:
Pattern matching can replace lengthy conditional statements, making code more readable and maintainable.
Expressiveness:
Enables concise and expressive handling of diverse data types and structures, facilitating more intuitive data manipulation.
Case Classes
Case classes in Scala provide a convenient way to define immutable data structures with built-in support for pattern matching and serialization. They are ideal for representing data models in big data and machine learning applications.
Boilerplate Reduction:
Automatically generates boilerplate code for common operations like equals, hashCode, and toString, saving development time.
Seamless Integration:
Easily integrates with frameworks like Spark for efficient data processing and manipulation.
10. Developer Productivity and Tooling
Interactive Development with REPL
Scala’s Read-Eval-Print Loop (REPL) allows developers to interactively test and debug code snippets, enhancing productivity and facilitating rapid prototyping. This interactive environment is invaluable in machine learning for experimenting with algorithms and data transformations.
Immediate Feedback:
Developers receive instant feedback on code changes, enabling quicker iterations and refinements.
Ease of Testing:
Simplifies the testing of individual components and functions without the need for a complete application setup.
Robust IDE Support
Scala benefits from strong support in popular Integrated Development Environments (IDEs) like IntelliJ IDEA and Eclipse, offering features such as intelligent code completion, refactoring tools, and debugging capabilities.
Enhanced Development Experience:
Advanced IDE features streamline the development process, making it easier to write, test, and maintain Scala code.
Integration with Build Tools:
Seamless integration with build tools like SBT (Simple Build Tool) and Maven enhances project management and dependency handling.
11. Community and Ecosystem
Active Community
Scala has a vibrant and active community that continuously contributes to its growth and the development of libraries and frameworks tailored for big data and machine learning. This community support ensures that Scala remains up-to-date with the latest industry trends and technological advancements.
Open-Source Contributions:
A plethora of open-source projects and libraries are available, providing solutions for a wide range of big data and machine learning challenges.
Knowledge Sharing:
Active forums, conferences, and meetups facilitate knowledge sharing and collaboration among Scala developers.
Comprehensive Ecosystem
Scala’s ecosystem is rich with tools and libraries that cater specifically to big data and machine learning needs. From Spark and Akka to Breeze and Play Framework, Scala offers a comprehensive set of resources that streamline development and enhance application capabilities.
Specialized Libraries:
Libraries like Breeze for numerical processing and Spark for big data analytics provide powerful tools for developing sophisticated machine learning models and data processing pipelines.
Integration with Other Technologies:
Scala integrates seamlessly with other technologies commonly used in big data and machine learning, such as Hadoop, Kafka, and TensorFlow, enabling the creation of versatile and robust applications.
12. Scalability and Maintainability
Scalable Architecture
Scala’s design facilitates the development of scalable architectures, which is essential in big data environments where applications must handle growing data volumes and user demands. Features like immutability, concurrency support, and efficient memory management contribute to building scalable systems.
Distributed Processing:
Scala’s compatibility with distributed processing frameworks like Spark and Akka enables the creation of applications that can scale horizontally across multiple nodes.
Modular Design:
Scala encourages modular and composable code structures, making it easier to scale individual components as needed without affecting the entire system.
Maintainable Codebase
Scala’s concise and expressive syntax, coupled with its advanced type system, leads to more maintainable codebases. In large-scale data projects, maintainability is crucial for ensuring that applications can evolve and adapt to changing requirements without significant overhead.
Readable Code:
Clear and concise code is easier to understand, review, and modify, reducing the time and effort required for maintenance and updates.
Type Safety:
Scala’s type system minimizes runtime errors, enhancing the stability and reliability of applications, which is particularly important in mission-critical big data and machine learning systems.
13. Case Studies and Industry Adoption
Prominent Use Cases
Numerous companies across various industries have adopted Scala for their big data and machine learning projects, citing its advantages over Java as a primary reason. For instance, LinkedIn uses Scala extensively for data processing and analytics, leveraging its functional programming capabilities and seamless integration with Spark.
Financial Services:
Organizations in the financial sector utilize Scala for real-time data analysis and algorithmic trading, benefiting from its performance and concurrency support.
E-commerce:
E-commerce giants employ Scala to process large volumes of transaction data and personalize user experiences through machine learning models.
Success Stories
Case studies highlight the tangible benefits of choosing Scala over Java. For example, Twitter migrated parts of its backend services from Java to Scala, achieving improved developer productivity, enhanced performance, and easier maintenance. These success stories underscore Scala’s effectiveness in handling the complexities of big data and machine learning applications.
14. Learning Curve and Developer Availability
Learning Curve
While Scala offers numerous advantages, it is often perceived as having a steeper learning curve compared to Java due to its functional programming features and advanced language constructs. However, for developers familiar with Java, transitioning to Scala is relatively straightforward, thanks to its interoperability and similar syntax in many aspects.
Educational Resources:
A wealth of tutorials, courses, and documentation are available to help developers learn Scala, mitigating the learning curve and accelerating proficiency.
Community Support:
The active Scala community provides ample support through forums, Q&A sites, and collaborative platforms, assisting new developers in overcoming challenges.
Developer Availability
The demand for Scala developers has grown alongside its adoption in big data and machine learning. While the talent pool may be smaller compared to Java, the specialized skills that Scala developers possess make them highly valuable for projects requiring expertise in functional programming and big data frameworks.
Recruitment Advantages:
Organizations can attract top talent by offering opportunities to work with Scala, appealing to developers seeking to work with modern and versatile technologies.
15. Future Prospects
Evolving Language Features
Scala continues to evolve, with ongoing developments aimed at enhancing its performance, scalability, and usability. Upcoming language features and improvements ensure that Scala remains relevant and competitive in the ever-changing tech landscape.
Scala 3:
The release of Scala 3 brings significant improvements in language simplicity, performance, and tooling, making it even more attractive for big data and machine learning applications.
Integration with Emerging Technologies:
Scala’s adaptability allows it to integrate with emerging technologies like cloud computing, artificial intelligence, and blockchain, ensuring its applicability in future data-driven innovations.
Industry Trends
The increasing emphasis on big data and machine learning across industries underscores the importance of languages that can efficiently handle large-scale data processing and complex algorithms. Scala’s strengths align well with these trends, positioning it as a leading language for current and future data-centric applications.
Adoption in Data Science:
As data science continues to grow, Scala’s capabilities in handling data-intensive tasks and integrating with machine learning frameworks make it a preferred choice for data scientists and engineers.
Support for Distributed Systems:
The rise of distributed computing reinforces Scala’s role in building scalable and resilient systems, essential for managing the complexities of big data environments.
Conclusion
In the realms of big data and machine learning, the choice of programming language can significantly influence the success of a project. Scala offers a compelling array of advantages over Java, including a functional programming paradigm, seamless Java interoperability, an advanced type system, superior concurrency support, and a more expressive syntax. These features collectively enhance developer productivity, application performance, and scalability, making Scala an optimal choice for data-intensive and machine learning applications.
Moreover, Scala’s robust integration with Apache Spark, active community, and comprehensive ecosystem further solidify its position as a superior alternative to Java in these fields. While the learning curve may be steeper, the long-term benefits in terms of maintainability, performance, and scalability make Scala a worthwhile investment for organizations aiming to leverage big data and machine learning effectively.
As industries continue to generate and rely on vast amounts of data, the demand for efficient, scalable, and maintainable solutions will only grow. Scala stands out as a language that not only meets these demands but also empowers developers to innovate and excel in the dynamic landscapes of big data and machine learning.
Top comments
(1)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Collapse
Expand
Silvestre Pitti
Silvestre Pitti
Silvestre Pitti
Follow
Joined
Feb 7, 2024
•
Oct 1
Dropdown menu
Copy link
Hide
Unfortunately the Scala plugin for Eclipse is now old and unsupported. I switched to Intellij Idea because of this but I'd switch back if someone starts to work on it again.
Like comment:
Like comment:
2
likes
Like
Comment button
Reply
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Collection in Java
Franciele B. de Oliveira -
Oct 17
Зоопарк Hibernate: N+1 запросов или как накормить жадного бегемота
Olga Lugacheva -
Nov 20
Swarm-Tuning AI Experts: Collaborative Fine-Tuning of Large Language Models
Mike Young -
Oct 17
Data Interpreter: LLM Agent Assisting Data Scientists in Workflow and Insight Generation
Mike Young -
Oct 17
Aditya Pratap Bhuyan
Follow
Aditya Pratap Bhuyan is an experienced IT professional with over 20 years in enterprise and cloud applications. With more than 40 industry certifications, he specializes in DevOps, cloud computing.
Location
Bangalore, India
Pronouns
He/Him
Work
Cloud Native Journey
Joined
Mar 24, 2024
More from
Aditya Pratap Bhuyan
How Java Solves the Diamond Problem?
#
java
#
diamondproblem
How Java Development Services Ensure Scalability and Performance in Modern Applications
#
java
#
scalability
#
performance
#
applications
How Java Development Services Ensure Robust Security for Enterprise-Level Applications
#
security
#
java
#
enterprises
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
The Faults of Algorithmic Coding Interview Tests for DevOps - DEV Community,"The Faults of Algorithmic Coding Interview Tests for DevOps - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Chris White
Posted on
Apr 25, 2023
The Faults of Algorithmic Coding Interview Tests for DevOps
#
interview
#
devops
#
job
When looking for potential DevOps candidates coding tests are not at all uncommon. Unfortunately many of them are remnants of older computer science class tests. I'd like to go over some of the points on why it may not be getting the best DevOps candidates.
Vertical and Horizontal Scaling
The traditional Big-O notation type algorithm theory tends to be more oriented towards vertical scaling patterns. Such optimizations are an attempt to squeeze as much as possible out of conventional hardware and reduce the need to add CPU, memory, etc. In modern DevOps based on a micro-services architecture you really don't see this as much.
This is because horizontal scaling is utilized and code tends to be compartmentalized into functions as a service or container units. Less code is applied to make services work. Performance optimization now deviates more towards how various services connect. Network flow in particular suddenly becomes an important factor in the performance of applications.
Managed Services
If you're using AWS in particular, there are a lot of services available to handle anything from the most basic to the most complex tasks. I wouldn't implement a queue system in my code as I'd just use AWS SQS instead. Crunching big data can be done through various service such as RedShift and Athena. As a DevOps engineer I won't be thinking about which binary tree to use but instead which service might meet my needs.
Higher Level Languages
Python and Ruby are some examples of very easy to work with languages. Modern languages abstract out a lot of the low level algorithmic work. This is essential in a modern business environment where customers aren't going to wait for a bubble sort implementation. Even if such algorithms were needed there are plenty of language packages and standard library solutions to handle it quickly. This means having such algorithms in the back of our heads isn't efficient use of time versus knowledge of python generators.
Performance Monitoring Tactics
How we deal with performance in a cloud based tooling era deals more with call patterns and inter-service latency. Even if a performance issue is found in the code the solution is very unlikely to be addressed by a low level algorithm. Instead it would be a language feature such as generators or refactoring object instantiation. DevOps engineers aren't thinking of this in terms of Big-O notation nor is there really a need to.
So What Should Be Tested?
Instead of testing for computer science courses, instead tests against the role. Programmatic questions should be related to how the role in question uses the appropriate programming language. I've personally done this by showing a somewhat stripped version of project code and having the candidate walk through it. If it doesn't work out I even go so far as to let them know what concepts to study up on.
It's also important to not forget about the actual infrastructure itself. Asking about Terraform or other IaC solutions is very crucial. It's also important to ask about the cloud architecture in the event it's a 100% required skill. Questions should be adjusted based on the amount of experience you expect.
Conclusion
I hope this was insightful on how programming tests in DevOps interviews should be handled. Again, it's important that what you're testing matches the role itself. Otherwise you'll get people really good at computer science tests and may be filtering out amazing candidates for the role.
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Demonstrating Persistence vs. Non-Persistence in Kubernetes with MongoDB
Dmitry Romanoff -
Nov 2
How to Install Minikube on Ubuntu Virtual Machine?
Ayush kumar -
Nov 6
How We Solved Infrastructure as Code
Tuomas -
Dec 3
Install Docker Desktop on Mac
Megha Sharma -
Nov 6
Chris White
Follow
Like what you see? I'm currently open for work opportunities!
Location
Austin, Texas
Work
Open for work
Joined
Apr 10, 2023
More from
Chris White
The Road To Kubernetes: How Older Technologies Add Up
#
devops
#
containers
#
docker
#
kubernetes
Implementing Quality Checks In Your Git Workflow With Hooks and pre-commit
#
git
#
tutorial
#
programming
#
devops
The Hacking Updates
#
github
#
devops
#
opensource
#
hacktoberfest
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
CPP VS Python Benchmark Testing - DEV Community,"CPP VS Python Benchmark Testing - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Itsuki Tatsuya (いつき たつや)
Posted on
Mar 3
CPP VS Python Benchmark Testing
#
python
#
cpp
#
testing
#
programming
This post will look at benchmark tests for CPP and Python programming.
The following is a partial list of the trials that will be conducted.
List:
Sorting Algorithm
Big Data Processing
Scientific Computing and Mathematical Computation
String Processing
Matrix and Vector Operations
Modeling and Simulation Tasks
Intensive Memory Usage
Repeated Code Execution
CPU Usage and Multithreading
Network Processing and Protocols
Results from benchmark testing
1. Sorting Algorithm
CPP
Python
Conclusion
:
C++
: Faster in sorting algorithms due to higher performance and more efficient memory usage.
Python
: Has powerful built-in libraries for sorting but typically slower than direct implementations in C++.
2. Big Data Processing
CPP
Python
Conclusion:
C++
: Faster and more efficient in processing big data.
Python
: Easy to use but tends to be slower in processing big data.
3. Scientific Computing and Mathematical Computation
CPP
Python
Conclusion:
C++
: Faster in mathematical computations due to compilation to machine code and performance optimization.
Python
: Easy to use and has efficient libraries like NumPy, but typically slower than C++.
4. String Processing
CPP
Python
Conclusion:
C++
: Faster in string processing due to higher performance and more efficient memory manipulation.
Python
: Easy to use and fast in string processing.
5. Matrix and Vector Operations
CPP
Python
Conclusion:
C++
: Faster in matrix and vector operations due to higher performance and more efficient memory usage.
Python
: Has efficient libraries like NumPy, but typically slower than C++ in matrix and vector operations.
6. Modeling and Simulation Tasks
CPP
Python
Conclusion:
C++
: Faster in modeling and simulation due to higher performance and more efficient memory usage.
Python
: Easy to use but may be slower in modeling and simulation.
7. Intensive Memory Usage
CPP
Python
Conclusion:
C++
: Better in intensive memory usage due to manual memory management and greater control.
Python
: Easy to use but may experience larger memory overhead.
8. Repeated Code Execution
CPP
Python
Conclusion:
C++
: Faster in repeated code execution due to higher performance.
Python
: Slower in repeated code execution due to code interpretation and automatic memory management.
9. CPU Usage and Multithreading
CPP
Python
Conclusion:
C++
: Better in CPU usage and multithreading due to higher performance and better control over thread management.
Python
: Has GIL limiting multithreading but still can use multiprocessing.
10. Network Processing and Protocols
CPP
Python
Conclusion:
C++
: Faster in network processing and protocols due to higher performance and better control over network management.
Python
: Easy to use but may be slower in network processing and protocols.
All benchmark testing has shown that CPP outperforms Python.
All Source Code Here:
🔗
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Systems Engineering: Free Learning Resources for Tech Enthusiasts
GetVM -
Dec 8
5 Essential JavaScript Design Patterns for Scalable Web Development
Aarav Joshi -
Dec 8
Enhancing the Developer Experience of Testing Part 2
Paul Mooney -
Dec 8
1760. Minimum Limit of Balls in a Bag
MD ARIFUL HAQUE -
Dec 7
Itsuki Tatsuya (いつき たつや)
Follow
バーチャルワールド探検家 🌐 | クリエイティブアーティスト 🎨 | テクノロジー＆アート ⚙️ | 他をインスパイア 🌟
Location
Yamatotakada-shi, Nara, Japan
Joined
Mar 3, 2024
More from
Itsuki Tatsuya (いつき たつや)
Boosting Performance with C++17 and C++20 Features
#
cpp
#
programming
#
tutorial
#
learning
Effective C++ Programming: Best Practices and Common Pitfalls
#
cpp
#
programming
#
tutorial
#
learning
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
An End-to-End Guide to dbt (Data Build Tool) with a Use Case Example - DEV Community,"An End-to-End Guide to dbt (Data Build Tool) with a Use Case Example - DEV Community
Skip to content
Navigation menu
Search
Powered by
Search
Algolia
Search
Log in
Create account
DEV Community
Close
Add reaction
Like
Unicorn
Exploding Head
Raised Hands
Fire
Jump to Comments
Save
Boost
More...
Copy link
Copy link
Copied to Clipboard
Share to X
Share to LinkedIn
Share to Facebook
Share to Mastodon
Report Abuse
Chetan Gupta
Posted on
Sep 16
• Edited on
Dec 11
An End-to-End Guide to dbt (Data Build Tool) with a Use Case Example
#
dataengineering
#
datatransformation
#
dbt
#
bigdata
Introduction
In modern data engineering, creating scalable and manageable data pipelines is a critical aspect of maintaining clean, organized, and reliable data in data warehouses. dbt (Data Build Tool) has emerged as a popular open-source tool for data transformation, enabling data teams to apply software engineering best practices like version control, testing, and modularity in SQL transformations. This blog will take you through an end-to-end guide to dbt, walking through key concepts with a use case example to help you understand how it works in practice.
What is dbt?
dbt (Data Build Tool) allows data analysts and engineers to transform data within a warehouse by writing SQL queries. It enables you to model your data, run tests, and create documentation. dbt can transform raw data into organized, actionable analytics datasets using SQL and Jinja (a templating language).
Core Features of dbt:
Modularity: Break down SQL code into reusable pieces.
Version Control: Use git to manage changes in your dbt project.
Testing: Automated testing for data quality.
Documentation: Automatically generate documentation from your models.
Dependency Management: Automatically order transformations with Directed Acyclic Graph (DAG).
dbt Architecture Overview
Here’s a simplified flow:
Data Extraction happens elsewhere (ETL tools like Airbyte or Fivetran).
Data Loading: Raw data lands in your data warehouse (e.g., Snowflake, BigQuery, Redshift).
dbt comes into play for data transformation, where it organizes, cleans, and models data.
The transformed data is used for reporting or analysis.
Use Case: Sales Analytics Pipeline
Imagine you're building a data pipeline to report on an e-commerce company’s sales performance. The raw data is available in the following format:
Customers table
Orders table
Products table
Our goal is to use dbt to transform this raw data into a Sales Report table that aggregates revenue per product and customer, with historical accuracy and data quality tests.
Setting Up Your dbt Project
Let’s walk through creating and managing a dbt project from scratch.
Install dbt
First, install dbt. This is done via Python’s package manager pip.
bash
pip install dbt
Once installed, you can create a new dbt project using the command:
bash
dbt init sales_analytics_project
Navigate to the new project directory:
bash
cd sales_analytics_project
Configure dbt Profiles
dbt needs to connect to your data warehouse (e.g., Snowflake, BigQuery, Redshift). Inside your
~/.dbt/profiles.yml
, configure the connection settings.
Example for Snowflake:
yaml
sales_analytics_project:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: your_snowflake_account
      user: your_username
      password: your_password
      role: your_role
      database: your_database
      warehouse: your_warehouse
      schema: analytics
      threads: 4
Enter fullscreen mode
Exit fullscreen mode
Organize Your dbt Models
In dbt, models are SQL files that represent transformations. Let’s build our Sales Report model step by step.
a. Create Models Directory
Inside your dbt project, you will have a models folder. Let’s create three models:
Customers model: customers.sql
Orders model: orders.sql
Products model: products.sql
Each model will transform the raw data, and you can reference raw tables using ref().
b. Write SQL Transformations
Customers Model:
sql
-- models/customers.sql
WITH customers_cleaned AS (
    SELECT
        customer_id,
        first_name,
        last_name,
        email,
        created_at
    FROM {{ ref('raw_customers') }}
)

SELECT * FROM customers_cleaned;
Enter fullscreen mode
Exit fullscreen mode
Orders Model:
sql
-- models/orders.sql
WITH orders_cleaned AS (
    SELECT
        order_id,
        customer_id,
        product_id,
        total_amount,
        order_date
    FROM {{ ref('raw_orders') }}
)

SELECT * FROM orders_cleaned;
Enter fullscreen mode
Exit fullscreen mode
Products Model:
sql
-- models/products.sql
WITH products_cleaned AS (
    SELECT
        product_id,
        product_name,
        category,
        price
    FROM {{ ref('raw_products') }}
)

SELECT * FROM products_cleaned;
Enter fullscreen mode
Exit fullscreen mode
c. Create Sales Report Model
Now, we’ll create a final Sales Report model by joining the above models.
sql
-- models/sales_report.sql
WITH sales AS (
    SELECT
        o.order_id,
        c.customer_id,
        c.first_name || ' ' || c.last_name AS customer_name,
        p.product_name,
        p.category,
        o.total_amount,
        o.order_date
    FROM {{ ref('orders') }} o
    JOIN {{ ref('customers') }} c
        ON o.customer_id = c.customer_id
    JOIN {{ ref('products') }} p
        ON o.product_id = p.product_id
)

SELECT
    product_name,
    category,
    SUM(total_amount) AS total_revenue,
    COUNT(order_id) AS total_orders
FROM sales
GROUP BY 1, 2
ORDER BY total_revenue DESC;
Enter fullscreen mode
Exit fullscreen mode
Testing Your dbt Models
Testing is crucial to maintain data quality. dbt allows you to write tests to validate data.
a. Write Basic Tests
Create a new file called schema.yml in the models folder:
yaml
version: 2

models:
  - name: customers
    tests:
      - unique:
          column: customer_id
      - not_null:
          column: email

  - name: orders
    tests:
      - unique:
          column: order_id
      - not_null:
          column: total_amount
Enter fullscreen mode
Exit fullscreen mode
These tests ensure that:
customer_id and order_id are unique.
email and total_amount are not null.
b. Run Tests
To run the tests, use the command:
bash
dbt test
If any tests fail, dbt will notify you and provide details.
Snapshots for Slowly Changing Dimensions (SCD)
Let’s implement a snapshot to track changes in the customer information over time (Slowly Changing Dimension Type 2).
a. Create a Snapshot
Inside the snapshots folder, create customers_snapshot.sql:
sql
-- snapshots/customers_snapshot.sql
{% snapshot customers_snapshot %}

    {{ config(
        target_schema='snapshots',
        unique_key='customer_id',
        strategy='check',
        check_cols=['first_name', 'last_name', 'email']
    ) }}

    SELECT * FROM {{ ref('raw_customers') }}

{% endsnapshot %}
Enter fullscreen mode
Exit fullscreen mode
This snapshot will track changes to the first_name, last_name, and email columns for each customer.
b. Run Snapshots
To capture the current state, use the command:
bash
dbt snapshot
Over time, this snapshot will record changes, allowing you to maintain a historical view of customer data.
Documentation in dbt
dbt can automatically generate documentation for your models, tests, and lineage. To generate and serve documentation:
bash
dbt docs generate
dbt docs serve
Enter fullscreen mode
Exit fullscreen mode
You’ll get an interactive website with details on your models, their columns, and dependencies.
Run dbt Models
Finally, run your dbt project to materialize the transformations and load the transformed data into your data warehouse.
bash
dbt run
This command will execute all models in the project, following the dependencies defined by the DAG (Directed Acyclic Graph).
Conclusion
This end-to-end guide provides an overview of setting up and using dbt in a real-world use case. We covered:
Project setup and configuration.
Writing transformations using models.
Running data tests to ensure quality.
Creating snapshots for tracking changes over time.
Generating documentation for better data lineage visibility.
By using dbt, you can simplify your data transformation workflows, ensure data quality, and build more maintainable, modular pipelines. With the sales analytics example, you now have a good foundation to explore other advanced dbt features like macros, incremental models, and CI/CD integration.
Happy Transforming!
Top comments
(0)
Subscribe
Personal
Trusted User
Create template
Templates let you quickly answer FAQs or store snippets for re-use.
Submit
Preview
Dismiss
Code of Conduct
•
Report abuse
Are you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's
permalink
.
Hide child comments as well
Confirm
For further actions, you may consider blocking this person and/or
reporting abuse
Read next
Create Your First AI-Powered C# App with Semantic Kernel: A Step-by-Step Guide
CodeStreet -
Dec 10
Important Javascript Topics for Interview
Akash Babu Kushwah -
Dec 10
Phoenix LiveView, hooks and push_event: json_view
Matt Whitworth -
Dec 10
first time doing a web page
Diogo Quintino -
Dec 10
Chetan Gupta
Follow
Developer by work, you can connect me on https://www.linkedin.com/in/chaets/
Location
New York
Joined
Jan 31, 2020
More from
Chetan Gupta
Journey Through Spark SQL
#
sparksql
#
spark
#
bigdata
#
datajourney
Tracking Data Over Time: Slowly Changing Dimensions (SCD)
#
bigdata
#
datatracking
#
scd
#
slowlychangingdimensions
Understanding OLTP and Choosing the Right Database
#
dataengineering
#
mongodb
#
postgressql
#
mysql
Thank you to our Diamond Sponsor
Neon
for supporting our community.
DEV Community
— A constructive and inclusive social network for software developers. With you every step of your journey.
Home
DEV++
Podcasts
Videos
Tags
DEV Help
Forem Shop
Advertise on DEV
DEV Challenges
DEV Showcase
About
Contact
Free Postgres Database
Guides
Software comparisons
Code of Conduct
Privacy Policy
Terms of use
Built on
Forem
— the
open source
software that powers
DEV
and other inclusive communities.
Made with love and
Ruby on Rails
. DEV Community
©
2016 - 2024.
We're a place where coders share, stay up-to-date and grow their careers.
Log in
Create account"
python - How can I stream big data to Google Cloud Storage? - Stack Overflow,"python - How can I stream big data to Google Cloud Storage? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How can I stream big data to Google Cloud Storage?
Ask Question
Asked
4 years, 8 months ago
Modified
1 year, 6 months ago
Viewed
2k times
Part of
Google Cloud
Collective
2
I am working on a system for analyzing data of any size and format streamed by the users to my private cloud based on Google Cloud Storage. Do you have any ideas how can I allow them to stream big data? At the moment I use Django API and I do this in this way:
def upload_blob(source_file_name, destination_blob_name):
    blob = bucket.blob(destination_blob_name)
    blob.upload_from_filename(source_file_name)
    print('File {} uploaded to {}.'.format(
        source_file_name,
        destination_blob_name))
It works correctly with small files however when I send for example large movie I get the error shown below. I am aware that this is not the optimal solution but I have no idea how can I solve this. As you can notice at the moment they send me requests with the blob format but with very large files it does not work. Do you have any ideas how can I solve my problem and send users data of any size to Google Cloud Storage?
Internal Server Error: /cloud/
Traceback (most recent call last):
 File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 672, in urlopen
  chunked=chunked,
 File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 387, in _make_request
  conn.request(method, url, **httplib_request_kw)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1252, in request
  self._send_request(method, url, body, headers, encode_chunked)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1298, in _send_request
  self.endheaders(body, encode_chunked=encode_chunked)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1247, in endheaders
  self._send_output(message_body, encode_chunked=encode_chunked)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1065, in _send_output
  self.send(chunk)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 987, in send
  self.sock.sendall(data)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 1034, in sendall
  v = self.send(byte_view[count:])
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 1003, in send
  return self._sslobj.write(data)
socket.timeout: The write operation timed out
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""/usr/local/lib/python3.7/site-packages/requests/adapters.py"", line 449, in send
  timeout=timeout
 File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 720, in urlopen
  method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
 File ""/usr/local/lib/python3.7/site-packages/urllib3/util/retry.py"", line 400, in increment
  raise six.reraise(type(error), error, _stacktrace)
 File ""/usr/local/lib/python3.7/site-packages/urllib3/packages/six.py"", line 734, in reraise
  raise value.with_traceback(tb)
 File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 672, in urlopen
  chunked=chunked,
 File ""/usr/local/lib/python3.7/site-packages/urllib3/connectionpool.py"", line 387, in _make_request
  conn.request(method, url, **httplib_request_kw)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1252, in request
  self._send_request(method, url, body, headers, encode_chunked)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1298, in _send_request
  self.endheaders(body, encode_chunked=encode_chunked)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1247, in endheaders
  self._send_output(message_body, encode_chunked=encode_chunked)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 1065, in _send_output
  self.send(chunk)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/http/client.py"", line 987, in send
  self.sock.sendall(data)
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 1034, in sendall
  v = self.send(byte_view[count:])
 File ""/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/ssl.py"", line 1003, in send
  return self._sslobj.write(data)
urllib3.exceptions.ProtocolError: ('Connection aborted.', timeout('The write operation timed out'))
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
 File ""/usr/local/lib/python3.7/site-packages/django/core/handlers/exception.py"", line 34, in inner
  response = get_response(request)
 File ""/usr/local/lib/python3.7/site-packages/django/core/handlers/base.py"", line 115, in _get_response
  response = self.process_exception_by_middleware(e, request)
 File ""/usr/local/lib/python3.7/site-packages/django/core/handlers/base.py"", line 113, in _get_response
  response = wrapped_callback(request, *callback_args, **callback_kwargs)
 File ""/usr/local/lib/python3.7/site-packages/django/views/decorators/csrf.py"", line 54, in wrapped_view
  return view_func(*args, **kwargs)
 File ""/usr/local/lib/python3.7/site-packages/django/views/generic/base.py"", line 71, in view
  return self.dispatch(request, *args, **kwargs)
 File ""/usr/local/lib/python3.7/site-packages/rest_framework/views.py"", line 505, in dispatch
  response = self.handle_exception(exc)
 File ""/usr/local/lib/python3.7/site-packages/rest_framework/views.py"", line 465, in handle_exception
  self.raise_uncaught_exception(exc)
 File ""/usr/local/lib/python3.7/site-packages/rest_framework/views.py"", line 476, in raise_uncaught_exception
  raise exc
 File ""/usr/local/lib/python3.7/site-packages/rest_framework/views.py"", line 502, in dispatch
  response = handler(request, *args, **kwargs)
 File ""/mypath/backend/views.py"", line 635, in post
  'user/' + str(user_name) + '/' + str(file))
 File ""/mypath/backend/views.py"", line 214, in upload_blob
  blob.upload_from_filename(source_file_name)
 File ""/usr/local/lib/python3.7/site-packages/google/cloud/storage/blob.py"", line 1318, in upload_from_filename
  predefined_acl=predefined_acl,
 File ""/usr/local/lib/python3.7/site-packages/google/cloud/storage/blob.py"", line 1263, in upload_from_file
  client, file_obj, content_type, size, num_retries, predefined_acl
 File ""/usr/local/lib/python3.7/site-packages/google/cloud/storage/blob.py"", line 1173, in _do_upload
  client, stream, content_type, size, num_retries, predefined_acl
 File ""/usr/local/lib/python3.7/site-packages/google/cloud/storage/blob.py"", line 1120, in _do_resumable_upload
  response = upload.transmit_next_chunk(transport)
 File ""/usr/local/lib/python3.7/site-packages/google/resumable_media/requests/upload.py"", line 425, in transmit_next_chunk
  retry_strategy=self._retry_strategy,
 File ""/usr/local/lib/python3.7/site-packages/google/resumable_media/requests/_helpers.py"", line 136, in http_request
  return _helpers.wait_and_retry(func, RequestsMixin._get_status_code, retry_strategy)
 File ""/usr/local/lib/python3.7/site-packages/google/resumable_media/_helpers.py"", line 150, in wait_and_retry
  response = func()
 File ""/usr/local/lib/python3.7/site-packages/google/auth/transport/requests.py"", line 216, in request
  method, url, data=data, headers=request_headers, **kwargs
 File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 533, in request
  resp = self.send(prep, **send_kwargs)
 File ""/usr/local/lib/python3.7/site-packages/requests/sessions.py"", line 646, in send
  r = adapter.send(request, **kwargs)
 File ""/usr/local/lib/python3.7/site-packages/requests/adapters.py"", line 498, in send
  raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out'))
[24/Mar/2020 19:17:26] ""POST /cloud/ HTTP/1.1"" 500 20879
python
django
python-3.x
google-cloud-platform
google-cloud-storage
Share
Improve this question
Follow
asked
Mar 28, 2020 at 10:50
John Smith
John Smith
333
2
2 silver badges
12
12 bronze badges
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
Have a look at
Resumable uploads
.
This option provides a resumable data transfer feature that lets you resume upload operations after a communication failure has interrupted the flow of data.
Especially useful if you are transferring large files, because the likelihood of a network interruption or some other transmission failures is high. In case of a failure, you do not have to restart large file uploads from the beginning when using this option.
Share
Improve this answer
Follow
answered
Mar 28, 2020 at 12:01
Deniss T.
Deniss T.
2,622
10
10 silver badges
22
22 bronze badges
Add a comment
|
0
If you look closely at the error message, you can see that the cause is a timeout. By default the timeout is 60 seconds, but you can specify a longer time.
blob.upload_from_filename(f'{filename}', timeout=300)
to wait for 300 seconds (or 5 minutes).
Share
Improve this answer
Follow
answered
Jun 2, 2023 at 16:35
Kristiaan
Kristiaan
406
4
4 silver badges
9
9 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
django
python-3.x
google-cloud-platform
google-cloud-storage
or
ask your own question
.
Google Cloud
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
5
Google App Engine: How to write large files to Google Cloud Storage
3
How to use Google Cloud Storage in a Django app hosted on Google Compute Engine?
1
Python Client for Google Cloud Storage and large files
33
Configure Django and Google Cloud Storage?
11
Writing data to google cloud storage using python
0
How can I stream data to Google Cloud storage?
0
How can I use Google cloud Storage with django-storages
12
Write-streaming to Google Cloud Storage in Python
2
Upload large file from django to cloud storage
0
Django with Google Cloud Storage
Hot Network Questions
What is ""B & S"" a reference to in Khartoum?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
When to start playing the chord when a measure starts with a rest symbol?
How does this Paypal guest checkout scam work?
When was ""to list"" meaning ""to wish"" lost?
Covering a smoke alarm horn
Didactic tool to play with deterministic and nondeterministic finite automata
Physical interpretation of selection rules for different multipole orders
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
Does an NEC load calculation overage mandate a service upgrade?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
How can dragons heat their breath?
What are these 16-Century Italian monetary symbols?
How can we be sure that the effects of gravity travel at most at the speed of light
Can two wrongs ever make a right?
Is outer space Radioactive?
When looking at the first DCM page, where is the next DCM page documented?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Why does it take so long to stop the rotor of a helicopter after landing?
Does DOS require partitions to be aligned at a cylinder boundary?
Challah dough bread machine recipe issues
Math contents does not align when subscripts are used
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
hiveql - Big data(spark sql and spark dataframes connection) - Stack Overflow,"hiveql - Big data(spark sql and spark dataframes connection) - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Big data(spark sql and spark dataframes connection)
Ask Question
Asked
4 years, 6 months ago
Modified
2 years ago
Viewed
44 times
0
I am new to big data platform. Could you please let me know the different ways:
1)how do we connect to spark for Spark sql?
2)how do we connect to sparrk for spark dataframes or datasets?
for hive,we connect through unix with beeline and if we have sql tool like: teradata we have to connect to hive through odbc with hive driver installed.
What about connecting to spark sql and spark dataframes or datasets?
Also if any one could provide me a good link or resource for a newbie like me to understand the concepts, commands and its use easily.That would really be of great help.
apache-spark-sql
hiveql
apache-spark-dataset
Share
Improve this question
Follow
edited
Jun 20, 2020 at 9:12
Community
Bot
1
1
1 silver badge
asked
Jun 8, 2020 at 17:20
soul shrestha
soul shrestha
125
5
5 silver badges
15
15 bronze badges
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
To use spark sql with SQL like querying you can simply type
spark-sql --master yarn # when you are testing on a Hadoop cluster
or
spark-sql --master local[2]   # when you are testing on local machine
and you can simply run all queries that you run on the
beeline>
shell in the
spark-sql>
shell.
Alternatively, if you wish to simply test some scala code:
spark-shell --master yarn # on cluster
or
spark-shell --master local[2] # on Local setup
Share
Improve this answer
Follow
answered
Jun 14, 2020 at 13:24
Yayati Sule
Yayati Sule
1,631
14
14 silver badges
25
25 bronze badges
Add a comment
|
0
spark=SparkSession.builder.master(""local"").appName(""wordCount"").getOrCreate()
spark.sql(""select * from table_name"").show()
you can refer below link
https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/8223668471254563/888847798980585/528896120252697/latest.html
Share
Improve this answer
Follow
answered
Dec 10, 2022 at 16:15
Shubham Tripathi
Shubham Tripathi
9
2
2 bronze badges
1
The link is about reading CSV from DBFS. It does not explain anything about connection to Spark and running Spark SQL. Please modify your answer to make corrections.
–
Azhar Khan
Commented
Dec 17, 2022 at 7:24
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
apache-spark-sql
hiveql
apache-spark-dataset
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Visit chat
Related
3
Spark as an engine for Hive
0
SparkSQL with HIVE
3
Apache Sqoop and Spark
10
Spark-sqlserver connection
0
Spark 1.6 SQL or Dataframe or Windows
1
Spark SQL DataFrame HAVING
2
Datasets in Apache Spark
1
Dataframes and Datasets in Spark
2
Spark SQL and using existing hive udfs
0
Spark-SQL plug in on HIVE
Hot Network Questions
Hole, YHWH and counterfactual present
What technique is used for the heads in this LEGO Halo Elite MOC?
Physical interpretation of selection rules for different multipole orders
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
Indian music video with over the top CGI
Protecting myself against costs for overnight weather-related cancellations
What should machining (turning, milling, grinding) in space look like
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
What is meaning of forms in ""they are even used as coil forms for inductors?""
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
What is ""B & S"" a reference to in Khartoum?
How does this Paypal guest checkout scam work?
Two types difinition of the distance function
Building a Statistically Sound ML Model
How can dragons heat their breath?
suspected stars and bars problem considered incorrect, in need for some further insight
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Why are Jersey and Guernsey not considered sovereign states?
Find all unique quintuplets in an array that sum to a given target
How should I connect a light fixture with UK wire colors to US wiring?
Should I expect a call from my future boss after signing the offer?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
nlp - Language detection in Python for big data - Stack Overflow,"nlp - Language detection in Python for big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Language detection in Python for big data
Ask Question
Asked
4 years, 1 month ago
Modified
2 years, 6 months ago
Viewed
4k times
Part of
NLP
Collective
2
I am trying to run language detection on a Series object in a pandas dataframe. However, I am dealing with millions of rows of string data, and the standard Python language detection libraries
langdetect
and
langid
are too slow, and after hours of running it still hasn't completed.
I set up my code as follows:
#function to detect language
def detect_language (cell):
    if len(cell) > 0:
        lan = langid.classify(cell)
    else:
        lan = ""NaN""
    return lan
#language detection using langid module

df['language'] = df.apply(lambda row: detect_language(row.Series), axis = 1)
Does anybody have suggestions on how to speed up my code or if there is another library out there?
python
nlp
bigdata
language-detection
Share
Improve this question
Follow
asked
Oct 30, 2020 at 8:34
P. Goodprice
P. Goodprice
41
1
1 silver badge
9
9 bronze badges
Add a comment
|
3 Answers
3
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
3
You could use
swifter
to make your
df.apply()
more efficient. In addition to that, you might want to try
whatthelang
library which should be more efficient than
langdetect
.
Share
Improve this answer
Follow
answered
Oct 30, 2020 at 8:42
Antoine Dubuis
Antoine Dubuis
5,294
1
1 gold badge
18
18 silver badges
32
32 bronze badges
2
1
swifter certainly seemed to help. In the end I stick to
langid
because I'm having problems installing the
whatthelang
library for Anaconda.
–
P. Goodprice
Commented
Oct 30, 2020 at 15:52
1
I did not knew about this issue with anaconda. Another idea to speed up the process would be to predict the lang only on a subset of words instead of the whole text.
–
Antoine Dubuis
Commented
Oct 30, 2020 at 16:00
Add a comment
|
1
Sample the data into multiple random sets and then use fasttext
https://fasttext.cc
to quickly get the results. It is fast and efficient.
import fasttext
path_to_pretrained_model = ""lid.176.bin""
fmodel = fasttext.load_model(path_to_pretrained_model)
from datetime import datetime
df['language']=''
for i in df.index:
    ln_cnt = i
    result = fmodel.predict(str(df['prcsd_title'][i]))
    #print(result[0])
    #detector.FindLanguage(text=str(df['prcsd_title'][i]))
    df['language'][i]=result[0]
    if i%10000==0:
        now = datetime.now()
        current_time = now.strftime(""%H:%M:%S"")
        print(""Current Time ="", current_time)
        print(i)
You have to install Ubuntu if you are having Windows OS. Refer this video link
https://www.youtube.com/watch?v=tQvghqdefTM&t=177s
Share
Improve this answer
Follow
answered
Dec 7, 2020 at 14:29
akash s
akash s
59
3
3 bronze badges
2
1
Thank you for your suggestion. However, it seems unnecessarily complicated to me to shift to Ubuntu for just a few lines of code. Using the swifter package alongside df.apply() did the trick well enough. Maybe this can be helpful to someone else
–
P. Goodprice
Commented
Dec 7, 2020 at 21:58
Hi Thanks for your response. In my case df.apply() was not efficient enough and as per my experience the code was still running even after ~9hrs for ~127 lakh records. But fasttext took ~7hrs to complete the task.   Please let me know your sample size and hardware configuration. It will help us to understand and learn better :).     Also, installing Ubuntu is not a complicated thing, please follow this link
windowscentral.com/…
–
akash s
Commented
Dec 9, 2020 at 4:23
Add a comment
|
0
Try:
from pandarallel import pandarallel
pandarallel.initialize(progress_bar=True)

""""""
then instead of apply, use parallel_apply to use all the cores of your CPU to parallelize it
""""""

df['new_column'] = df['your_column'].parallel_apply(lang_detect_func)
Share
Improve this answer
Follow
answered
May 20, 2022 at 8:01
Debjyoti Banerjee
Debjyoti Banerjee
90
6
6 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
nlp
bigdata
language-detection
or
ask your own question
.
NLP
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
43
NLTK and language detection
8
Detecting whether or not text is English (in bulk)
1
Python way to detect language ISO code
3
language detection code in python
10
Python langdetect: choose between one language or the other only
198
How to determine the language of a piece of text?
6
Text Language detection in python
1
Identifying natural languages from small samples in Python
1
Language detection using deepl's python library
3
Language detection for short user-generated string
Hot Network Questions
When to start playing the chord when a measure starts with a rest symbol?
What's a modern term for sucker or sap?
Why is the speed graph of a survey flight a square wave?
Topology of a horocycle
reverse engineering wire protocol
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
What is meaning of forms in ""they are even used as coil forms for inductors?""
What is ""B & S"" a reference to in Khartoum?
How to achieve infinite rage?
How to use a symbol as both function and head?
Time travelling paedo priest novel
Derailleur Hangar - Fastener torque & thread preparation
How manage inventory discrepancies due to measurement errors in warehouse management systems
When to use cards for communicating dietary restrictions in Japan
Protecting myself against costs for overnight weather-related cancellations
Useful aerial recon vehicles for newly colonized worlds
When was ""to list"" meaning ""to wish"" lost?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Is outer space Radioactive?
How did Jahnke and Emde create their plots
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"Storing big data on a mobile device, iOS and Android, with react native and expo SQLite - Stack Overflow","Storing big data on a mobile device, iOS and Android, with react native and expo SQLite - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Storing big data on a mobile device, iOS and Android, with react native and expo SQLite
Ask Question
Asked
4 years, 8 months ago
Modified
2 years, 11 months ago
Viewed
3k times
4
I am working on a mobile app with react native and expo. The critical point of the app is that it needs to store big data on a device because it will work in offline and online modes both. I'm thinking to try expo SQLite for this and I have few questions regarding this:
Is it appropriate for storing large data on the local device, both Android and iOS?
When deleting data, does the database file size decrease, or stay as is?
Are there any limitations to TEXT type?
Can you suggest any good documentation, because on expo.io is too short (i think )?
Thanks in advance!
sqlite
react-native
expo
Share
Improve this question
Follow
edited
Apr 13, 2020 at 1:45
aytek
1,932
24
24 silver badges
35
35 bronze badges
asked
Apr 12, 2020 at 16:46
E. Stoyanov
E. Stoyanov
43
1
1 silver badge
3
3 bronze badges
1
Did you resolve this issue? I'm curious what you used.
–
Ken Ingram
Commented
Oct 22, 2021 at 21:40
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
5
1) Storing large data on a react native app
Redux-persist and SQLite are not ideal ways to store large data on mobile devices. You can store your data on the application storage managed by the relevant OS. You can use
Documents
or
/Library/Caches
or
tmp
directories on
iOS
and external storage or external sd card on
Android
. You will need read-write permissions on Android, you can use
react-native-permissions
to handle it.
react-native-fs
is a good library to save, read or delete files.
2) Deleting data
If you store a binary or text in your database when you delete that record, it will reduce your database size. But if you store your file in the documents folder and add a reference to your database, deleting this reference won't remove the actual file from the device. You need to do this yourself.
3) SQLite limitations
I assume you are storing binaries. Regardless of the data type, there is a limit for SQLite Text type which is 2 GB. You can see more info
here
4) Suggestions
I would prefer storing data in local storage as explained above and keep the references in
Async Storage
managed by
redux-persist
or a local SQLite database.
Share
Improve this answer
Follow
answered
Apr 12, 2020 at 18:10
aytek
aytek
1,932
24
24 silver badges
35
35 bronze badges
0
Add a comment
|
1
I just use expo-file-system as all my data is stored in a single tree so it's easy to async write it to file after every update and load it when the app opens.
Share
Improve this answer
Follow
answered
Dec 15, 2021 at 21:03
Brian F
Brian F
1,650
1
1 gold badge
19
19 silver badges
24
24 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
sqlite
react-native
expo
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
0
Persisting data between app launches with Expo & React Native
0
How to use database with React native?
6
AsyncStorage vs Expo.SQLite
1
Storage in React Native
5
Offline database in Expo
13
expo sqlite use existing database
0
How can i persist data in react native application?
5
Accessing physical storage of Expo SQLite database
2
How to use sqlite file in expo react native
1
expo-sqlite don't keep data inserted in react native
Hot Network Questions
Why is the speed graph of a survey flight a square wave?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
Heating object in airless environment
What are these 16-Century Italian monetary symbols?
How can we be sure that the effects of gravity travel at most at the speed of light
Can a hyphen be a ""letter"" in some words?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
When was ""to list"" meaning ""to wish"" lost?
Is it possible that the committee contacts only one reference while applicants need to provide two?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
What technique is used for the heads in this LEGO Halo Elite MOC?
Time travelling paedo priest novel
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
What should machining (turning, milling, grinding) in space look like
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Passphrase entropy calculation, Wikipedia version
How to estimate the latency of communication?
When looking at the first DCM page, where is the next DCM page documented?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
Protecting myself against costs for overnight weather-related cancellations
Should I expect a call from my future boss after signing the offer?
Two types difinition of the distance function
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python - How to improve vectorized sliding window for big data? - Stack Overflow,"python - How to improve vectorized sliding window for big data? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to improve vectorized sliding window for big data?
Ask Question
Asked
4 years ago
Modified
3 years, 1 month ago
Viewed
179 times
0
I need to use a sliding window in python on a time series with 6 million time steps and 8 features per time step. I created an array of sliding windows using both a vectorized version and a version with a for loop. The for loop is substantially faster. I have highlighted the time consuming step in the vectorized version. Is there a good way to speed up the vectorized version?
Here is the vectorized version:
def vectorized_window(T, l: int, stride=1, start_idxs=None, output_type=""data""):
    """"""Takes a time series, T, and breakes it into subsequences of length l.

    This is a vectorized version of window creation. It should run faster because it uses
    matrix operations but for very large data, this is not true because the indexing operation
    at the end is slow.

    Args:
        T: A np.ndarray of shape (|T|, features) where |T| is the number of time steps.
        l: An int designating the length of the window.
        stride: The number of time steps to move the window forward by. Default is
            1 time step.
        start_idxs: A ndarray or None (default). If start_idxs is specified, these will be used as the start
            indices for each window. stride will be ignored. Default of None will
            sequentially slide the window by stride steps. Shape should be (num of indices,)
        output_type: ""data"" or ""idxs"". The default of ""data"" will compute and return the full window (ndarray)
            with the actual data values for each time step. If ""idxs"" is specified, it will return a ndarray
            of shape (num windows, 2) where windows[:,0] are the start indices and windows[:,1] are the end indices.

    Returns:
        windows: a list of ndarrays that represent windows, with length l, of the time series. The shape is
            either (num windows, l, num features) or (num windows, 2) depending on output_type.
    """"""
    window_idxs = np.expand_dims(np.arange(l), 0)
    if output_type != ""data"":
        window_idxs = window_idxs[[0, -1]]
    if start_idxs is None:
        start_idxs = np.expand_dims(np.arange(T.shape[0]-l, step=stride), 0).T
    else:
        start_idxs = np.expand_dims(start_idxs, 0).T
    
    if output_type != ""data"":
        windows = window_idxs + start_idxs
    else:
        sub_windows = (window_idxs + start_idxs)
        windows = T[sub_windows] # This is the slow step
    return windows
Here is the version with the for loop:
def create_window(T, l: int, stride=1):
    """"""Takes a time series, T, and breakes it into subsequences of length l.

    Args:
        T: A list or np.ndarray representing a univariate or multivariate time series.
            If it is a multivarite time series, it must be a numpy array of shape
            (time steps, features). If features is in axis 0, this will not work.
        l: An int designating the length of the window.
        stride: The number of time steps to move the window forward by. Default is
            1 time step.

    Returns:
        windows: a list of ndarrays that represent windows, with length l, of the time series.
    """"""
    if ""list"" in str(type(T)):
        T = np.asarray(T)

    n_T= T.shape[0]
    windows = []

    for i in range(0, n_T - l, stride):
        window = T[i:i+l]
        windows.append(window)
    
    return windows
The two versions don't do quite the same thing. The vectorized version will also return the bounding indices for each subsequence if the output_type is not ""data"". But, that difference does not significantly impact the overall speed.
Any suggestions to optimize this code is greatly appreciated!
python
algorithm
performance
optimization
vectorization
Share
Improve this question
Follow
edited
Nov 14, 2020 at 0:50
Emma
27.7k
11
11 gold badges
47
47 silver badges
71
71 bronze badges
asked
Nov 13, 2020 at 23:57
Nathaniel Strong
Nathaniel Strong
1
1
1 bronze badge
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
I would suggest you use
tsflex
, this package has
a very efficient
window-stride feature extraction
.
You can calculate your 8 features very conveniently, e.g., see
this example
:
import pandas as pd; import numpy as np; import scipy.stats as ss
from tsflex.features import MultipleFeatureDescriptors, FeatureCollection

# 1. -------- Get your time-indexed data --------
url = ""https://github.com/predict-idlab/tsflex/raw/main/examples/data/empatica/""
# Contains 1 column; [""TMP""] - 4 Hz sampling rate
data_tmp = pd.read_parquet(url+""tmp.parquet"").set_index(""timestamp"")
# Contains 3 columns; [""ACC_x"", ""ACC_y"", ""ACC_z""] - 32 Hz sampling rate
data_acc = pd.read_parquet(url+""acc.parquet"").set_index(""timestamp"")

# 2. -------- Construct your feature collection --------
fc = FeatureCollection(
    MultipleFeatureDescriptors(
          functions=[np.min, np.max, np.mean, np.std, np.median, ss.skew, ss.kurtosis],
          series_names=[""TMP"", ""ACC_x"", ""ACC_y""], # Use 3 multimodal signals 
          windows=[""5min"", ""7.5min""],  # Use 5 minutes and 7.5 minutes 
          strides=""2.5min"",  # With steps of 2.5 minutes
    )
)

# 3. -------- Calculate features --------
fc.calculate(data=[data_tmp, data_acc])
Note
: tsflex requires that your data has a time-index, and that the window & stride (= step) arguments are time-based (e.g., ""1min"", ""5s"", ""3h"").
You can check out other example of tsflex
here
.
Disclaimer: This library was created by me and some colleagues.
Share
Improve this answer
Follow
edited
Oct 26, 2021 at 7:40
answered
Sep 30, 2021 at 9:20
Jeroen Van Der Donckt
Jeroen Van Der Donckt
56
4
4 bronze badges
1
1
if you created the library you
must
disclose affiliation in the answer
–
Jean-François Fabre
♦
Commented
Oct 7, 2021 at 20:08
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
algorithm
performance
optimization
vectorization
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
6
Python - vectorizing a sliding window
0
How to vectorize this loop in python?
4
Improving moving-window computation in memory consumption and speed
7
Fast rolling-sum for list of data vectors (2d matrix)
3
Speeding up sliding windowed average calculations
3
Vectorize python code for improved performance
1
How to vectorize pandas operation to improve speed?
2
Python NumPy Vectorization to decrease processing time
2
Making the nested loop run faster, e.g. by vectorization in Python
2
How can I speed up this Python function?
Hot Network Questions
how do I correctly check that some aggregated results are correct?
Should I expect a call from my future boss after signing the offer?
UUID v7 Implementation
Physical interpretation of selection rules for different multipole orders
How to keep meat in a dungeon fresh, preserved, and hot?
Time travelling paedo priest novel
Two types difinition of the distance function
Humans try to help aliens deactivate their defensive barrier
What is meaning of forms in ""they are even used as coil forms for inductors?""
A Pandigital Multiplication
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
How to delete edges of curve based on their length
White perpetual check, where Black manages a check too?
How *exactly* is divisibility defined?
How to achieve infinite rage?
Does DOS require partitions to be aligned at a cylinder boundary?
Covering a smoke alarm horn
How much of a structural/syntactic difference is there between an oath and a promise?
Didactic tool to play with deterministic and nondeterministic finite automata
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How does this Paypal guest checkout scam work?
How to format numbers in monospaced (typewriter) font using siunitx?
How to estimate the latency of communication?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
electron - Efficient way to send big data between main process and renderer process - Stack Overflow,"electron - Efficient way to send big data between main process and renderer process - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Efficient way to send big data between main process and renderer process
Ask Question
Asked
4 years, 3 months ago
Modified
3 years, 1 month ago
Viewed
2k times
0
I need to read/stream big files (giga bytes) and display data in my renderers. The renderer may modify that data which then needs to be passed back to the main process. For security reasons I don't want to do that in the render process, but in the main process and pass the data to the renderer.
What is the most efficient way to pass that data?
Is it possible to have something like a shared memory where there is not even a copy of the data created?
electron
Share
Improve this question
Follow
asked
Aug 23, 2020 at 17:38
Fee
Fee
851
11
11 silver badges
26
26 bronze badges
4
You can use
ipcRenderer
and
ipcMain
–
Seblor
Commented
Aug 23, 2020 at 17:43
that will make a copy and IPC calls. Is that the most efficient way?
–
Fee
Commented
Aug 23, 2020 at 18:34
Unless you use an nAPI module with
nodeIntegration
activated, you cannot do without IPC calls and copied data. What is the issue with copying the data when passing it to the renderer ? If the passed data is not referenced anymore it will be dropped in the memory anyway.
–
Seblor
Commented
Aug 23, 2020 at 18:41
The issue is about copying the gigabyes of data from process to process which I would like to avoid very much. Thank you for pointing out N-API. I wasn't aware of that. However, for security reasons I was also trying to avoid to turn of nodeIntegration. Seems to be difficult to get both.
–
Fee
Commented
Aug 23, 2020 at 19:04
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
2
Maybe a bit late, but this is what I did for loading large binary files from the local file system:
I added a very basic web server to the main process, and the renderer requests the file's content using HTTP. This avoids the overhead of encoding/decoding the binary, but is still copying the data again and again.
Still, the performance is orders of magnitude better than with IPC.
Share
Improve this answer
Follow
answered
Oct 15, 2021 at 9:55
Erich Schreiner
Erich Schreiner
2,058
1
1 gold badge
14
14 silver badges
25
25 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
electron
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
36
How to pass parameters from main process to render processes in Electron
5
Communicating between two renderer processes in Electron
5
Send Data from render process to renderer process in Electron
0
renderer and main process communication in electron - the right way
54
How can we send messages from the main process to renderer process in Electron
0
Send data to renderer process
0
Communication between main and render processes
0
How to share data between Main/Renderer Processes
0
Electron communicate between class and main process with lots of tasks
6
Passing data from main to renderer (electron-js)
Hot Network Questions
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
Is there greater explanatory power in laws governing things rather than being descriptive?
Find all unique quintuplets in an array that sum to a given target
How could a city build a circular canal?
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
Realization of fundamental group endomorphism
What's the difference between '\ ' and tilde character (~)?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Why does it take so long to stop the rotor of a helicopter after landing?
Didactic tool to play with deterministic and nondeterministic finite automata
Useful aerial recon vehicles for newly colonized worlds
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Can two wrongs ever make a right?
Math contents does not align when subscripts are used
Does an NEC load calculation overage mandate a service upgrade?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
What's a modern term for sucker or sap?
Physical interpretation of selection rules for different multipole orders
How to remove clear adhesive tape from wooden kitchen cupboards?
how do I correctly check that some aggregated results are correct?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
reverse engineering wire protocol
What is meaning of forms in ""they are even used as coil forms for inductors?""
How to explain why I don't have a reference letter from my supervisor
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
postgresql - What are the best practices working with postgres replication slot for big data processing? - Stack Overflow,"postgresql - What are the best practices working with postgres replication slot for big data processing? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
What are the best practices working with postgres replication slot for big data processing?
Ask Question
Asked
4 years, 9 months ago
Modified
3 years, 7 months ago
Viewed
885 times
0
I am using kafka source connector which captures data from postgres and creates topics. I have tested it with thousands of records. But i want to know if the replication slot work same with even 10 million records. Do i need to take any additional precautions?
postgresql
apache-kafka
replication
database-replication
debezium
Share
Improve this question
Follow
edited
Mar 2, 2020 at 15:25
James Z
12.3k
10
10 gold badges
26
26 silver badges
47
47 bronze badges
asked
Mar 2, 2020 at 12:24
Joseph N
Joseph N
560
1
1 gold badge
13
13 silver badges
36
36 bronze badges
2
i don't understand: how can a replication
slot
fail? It is just a simple data structure on the primary server.
–
Laurenz Albe
Commented
Mar 2, 2020 at 12:34
Thanks Laurenz Albe, actually i am new to concept replication slot. I tried to explore about it. As per my knowledge, it keeps track of records being captured for replication / CDC. Can you please tell me how does it deal with data to keep track. What are the best practices to work with replication slots in production
–
Joseph N
Commented
Mar 2, 2020 at 12:46
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
So in versions 9.3 and prior there used to be a problem in cases such as mentioned below:
keeping it simple with 1 master and 1 replica, if for some reason the replica is down for a very long time in a stopped state and doesn't come up;
and if the primary's WAL segments required by the standby server have already been recycled, the standby cannot catch up with the primary server, to solve this problem we used to have something called wal_keep_segments. Set a larger value and it kind of bandaids the problem.
moving to the newer versions of postgres 9.4 and further, we got the
replication slot
.
So coming from official docs:
Replication slots provide an automated way to ensure that the master
does not remove WAL segments until they have been received by all standbys
, and that the master does not remove rows which could cause a recovery conflict even when the standby is disconnected.
So basically replication slots retain only the number of segments known to be needed. So basically it will function the way its designed to as mentioned above irrespective of the number of rows.
Hope this answers your question, see official docs for more
here
.
Share
Improve this answer
Follow
edited
May 9, 2021 at 3:13
OneCricketeer
191k
20
20 gold badges
141
141 silver badges
266
266 bronze badges
answered
Mar 2, 2020 at 15:43
Raj Verma
Raj Verma
1,172
1
1 gold badge
9
9 silver badges
22
22 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
postgresql
apache-kafka
replication
database-replication
debezium
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
11
PostgreSQL replication strategies
3
Which PostgreSQL replication solution to use for my specific scenario
1
Postgresql Replication solutions and their performance
4
PostgreSQL Streaming Replication
3
Which built-in Postgres replication fits my Django-based use-case best?
1
postgresql 9.4 high availability topology
2
Postgresql 9.6 Replication Recommendation
0
PostgreSQL streaming replication for high load
0
Streaming replication solution in Postgres
4
Postgres replication slot from kafka-connect is filling up
Hot Network Questions
How to explain why I don't have a reference letter from my supervisor
How to achieve infinite rage?
Only selecting Features that have another layers feature on top
The coherence of physicalism: are there any solutions to Hempel's dilemma?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
Challah dough bread machine recipe issues
How does this Paypal guest checkout scam work?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
How *exactly* is divisibility defined?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How should I connect a light fixture with UK wire colors to US wiring?
Is it possible that the committee contacts only one reference while applicants need to provide two?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
What is ""B & S"" a reference to in Khartoum?
suspected stars and bars problem considered incorrect, in need for some further insight
Why did Crimea’s parliament agree to join Ukraine?
How to estimate the latency of communication?
Time travelling paedo priest novel
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
How can dragons heat their breath?
How much of a structural/syntactic difference is there between an oath and a promise?
How could a city build a circular canal?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
dataframe - How to store BIG DATA as global variables in Dash Python? - Stack Overflow,"dataframe - How to store BIG DATA as global variables in Dash Python? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to store BIG DATA as global variables in Dash Python?
Ask Question
Asked
3 years, 11 months ago
Modified
3 years, 8 months ago
Viewed
5k times
3
I have a problem with my Dash application put in a server of a remote office. Two users running the app will experience interactions with each other due to table import followed by table pricing (the code for pricing is around 10,000 lines and pull out 8 tables). While looking on the internet, I saw that to solve this problem, it was enough to create html.Div preceded by the conversation of dataframes in JSON. However, this solution is not possible because I have to store 9 tables totaling 200,000 rows and 500 columns. So, I looked into the cache solution. However, this option does not create errors but increases the execution time of the program considerably. Going from a table of 20,000 vehicles to 200,000 it increases the compute time by almost * 1,000 and it is horrible every time I change the settings of the graphics.
I use cache filesystem and i used the exemple 4 of this :
https://dash.plotly.com/sharing-data-between-callbacks
. By doing some time calculations, I noticed that it is not accessing the cache that is the problem (about 1sec) but converting the JSON tables to dataframe (almost 60 seconds per callback). About 60 seconds is the time also corresponding to the pricing, so it is the same to call the cache in a callback as it is to price in a callback.
1/ do you have an idea that would save a dataframe not a JSON in the form of a cache or with a technique like the invisible html.Div or a cookie system or whatever other methods ?
2/ with the Redis or Memcached, we have to provide return json?
2/ If so, how do we set it up, taking example 4 from the previous link because I have an error ""redis.exceptions.ConnectionError: Error 10061 connecting to localhost: 6379. No connection could be established because l target computer expressly refused it. "" ?
3/ Do you also know if turning off the application automatically deletes the cache without following the default_timeout?
python
dataframe
caching
redis
plotly-dash
Share
Improve this question
Follow
edited
Feb 6, 2021 at 17:40
icedwater
4,877
3
3 gold badges
38
38 silver badges
53
53 bronze badges
asked
Dec 17, 2020 at 14:33
Pierre PRUDENT
Pierre PRUDENT
31
1
1 silver badge
2
2 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
-1
I think your issue can be solved using dash_extensions and specifically server side call back caches, might be worth a shot to implement.
https://community.plotly.com/t/show-and-tell-server-side-caching/42854
Share
Improve this answer
Follow
answered
Apr 6, 2021 at 13:08
Anthony1223
Anthony1223
359
2
2 silver badges
7
7 bronze badges
1
This is no longer available.
–
Sam
Commented
Oct 2, 2023 at 9:57
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
dataframe
caching
redis
plotly-dash
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
5
Python Dash App updating with new dataframe
7
How to make Dash app run faster if its slowed by large data imported
1
How to save plotly dash Input as global variable
0
how do you store variables in dash core components
8
Plotly dash refreshing global data on reload
4
Plotly dash server side caching
0
Plotly dash user dependent server side caching
4
How to persist state of plotly graph in Dash App
1
Live graphing in dash
1
how to use dcc.Store() for dictionary of dataframe
Hot Network Questions
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Two types difinition of the distance function
Does DOS require partitions to be aligned at a cylinder boundary?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
Indian music video with over the top CGI
How to keep meat in a dungeon fresh, preserved, and hot?
Need an advice to rig a spaceship with mechanicals part
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
What should machining (turning, milling, grinding) in space look like
What does “going off” mean in ""Going off the age of the statues""?
How to delete edges of curve based on their length
Convert pipe delimited column data to HTML table format for email
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Find all unique quintuplets in an array that sum to a given target
UUID v7 Implementation
Realization of fundamental group endomorphism
Physical interpretation of selection rules for different multipole orders
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
How did Jahnke and Emde create their plots
US phone service for long-term travel
Should I expect a call from my future boss after signing the offer?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
validation - Define Data Quality Rules for Big Data - Stack Overflow,"validation - Define Data Quality Rules for Big Data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Define Data Quality Rules for Big Data
Ask Question
Asked
4 years ago
Modified
3 years, 11 months ago
Viewed
341 times
1
Is there any way to define Data quality rules that can be applied over Dataframes.
The template to define the rule should be easy enough for any lay man to define and then we can take these rules and convert them to pyspark codes and run them over the data.
I was thinking in line as below.
ID  ProjectID   RuleID  Attribute1  Value1          Condition1  Attribute2  Value2          Condition2  Type    ModifyAttribute ModificationLogic   CustomUDF
1   1           1       SerialNum   6               EQUAL                                               MODIFY  SerialNum   SUBSTR(serialNum,1,6)   
2   1           2       DriverName  ['A','B','C']   VALUEMATCH  Source      ['D','E','F']   IN          REJECT
If there is any tools or Domain specific language to define the same it would help.
If there is any template to define rules which can be applied cross attribute and across multiple tables (join, example country lookup) is also helpful.
validation
pyspark
data-quality
Share
Improve this question
Follow
asked
Nov 27, 2020 at 7:47
Snehasish Das
Snehasish Das
291
3
3 silver badges
12
12 bronze badges
1
1
This could help:
github.com/great-expectations/great_expectations
–
blackbishop
Commented
Jan 10, 2021 at 21:30
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
Surprised no one gave a shot at answering this yet. Typically, for a use case like this, I would use
ConfigParser
. Based on what your architecture is, you can define sections and rules which can easily be read and executed. But that's something a developer would find easy to use rather than a normal user.
Now that's out of the way, for your use case, as python is a scripting language with a lot of flexibility, you can simply create an excel in the format you have given which will dictate the flow of your data manipulation. I hope this helps in some way. Let me know if you need more info.
Share
Improve this answer
Follow
answered
Jan 11, 2021 at 6:47
Jacob Celestine
Jacob Celestine
1,789
15
15 silver badges
23
23 bronze badges
2
Alternatively you configure these rules in some table (hive/HBase) and then broadcast these during execution or as mentioned by Jacob you can create a config file. Hope this helps.
–
Divyaansh Bajpai
Commented
Jan 12, 2021 at 5:43
@DivyaanshBajpai Hive is great! It helps with a lot of inconveniences you would face while doing file processing, but in this particular use case, he needs something user friendly. So it might not be a great choice here.
–
Jacob Celestine
Commented
Jan 12, 2021 at 16:52
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
validation
pyspark
data-quality
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
Effective Way to Validate Field Values Spark
1
Column data value consistency check PySpark SQL
3
Data Type validation in pyspark
1
Validate CSV file PySpark
0
Validate data from the same column in different rows with pyspark
0
Filter valid and invalid records in Spark
1
Schema validation in spark using python
1
Data Quality Process - defining rules
1
Column value length validation in pyspark
1
How to improve Column Validation For Dataframes Pyspark
Hot Network Questions
Covering a smoke alarm horn
How should I connect a light fixture with UK wire colors to US wiring?
Find a fraction's parent in the Stern-Brocot tree
When to use cards for communicating dietary restrictions in Japan
How to explain why I don't have a reference letter from my supervisor
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Does DOS require partitions to be aligned at a cylinder boundary?
Indian music video with over the top CGI
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
What does “going off” mean in ""Going off the age of the statues""?
How to use a symbol as both function and head?
Can two wrongs ever make a right?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
Topology of a horocycle
UUID v7 Implementation
Only selecting Features that have another layers feature on top
How did Jahnke and Emde create their plots
Does an NEC load calculation overage mandate a service upgrade?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Derailleur Hangar - Fastener torque & thread preparation
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Heating object in airless environment
Need an advice to rig a spaceship with mechanicals part
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
tensorflow - How to cache big data in memory (efficiently) in complex variables across executions of Python scripts? - Stack Overflow,"tensorflow - How to cache big data in memory (efficiently) in complex variables across executions of Python scripts? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to cache big data in memory (efficiently) in complex variables across executions of Python scripts?
Ask Question
Asked
3 years, 11 months ago
Modified
3 years, 11 months ago
Viewed
259 times
1
I am trying to call (from Java Spring beans) Python Pytorch scripts that contains trained neural networks for the use: my Pytorch neural networks are neural functions that accepts state, encodes it and returns the action, decodes all this is according to learned/trained policy.
So - each time when I am trying to invoke Python script I should construc torch.nn, load weights and biases from some external store (DB, file) and then execute this nn to get single answer, pretty expensive operation.
How can I keep torch.nn instance (with loaded weights and biases) in memory and make it available immediately for each execution of Python script?
Memcached is not the solution, because it can keep string or binary values only and it is quite expensive to serialized and deserialize torch.nn instance. One suggestion was Tensorflow Serving, I am currently researching it, so - I don't yet know whether this is the answer.
It is quite possible that Tensorflow has some caching technologies which I can use for the Pytroch as well?
python
tensorflow
caching
neural-network
pytorch
Share
Improve this question
Follow
asked
Dec 26, 2020 at 16:31
TomR
TomR
3,016
6
6 gold badges
42
42 silver badges
107
107 bronze badges
1
1
It's now possible to use the C++ API of PyTorch from Java with the JavaCPP Presets for PyTorch, so you may not need to use Python at all anyway:
github.com/bytedeco/javacpp-presets/tree/master/pytorch
–
Samuel Audet
Commented
Apr 16, 2021 at 1:47
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
If I understand your use case correctly, what you need is a model server that keeps the model loaded and ideally also handles any exceptions from incorrect data.
One rather straightforward way to transform your inference script into a tensorflow-serving-like callable service is the python library
flask
. Another way seems to be a new tool called
torchserve
.
Share
Improve this answer
Follow
edited
Dec 26, 2020 at 19:48
answered
Dec 26, 2020 at 17:59
Chris Holland
Chris Holland
579
4
4 silver badges
11
11 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
tensorflow
caching
neural-network
pytorch
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
76
Keep persistent variables in memory between runs of Python script
2
Smart caching of expensive objects in Python
169
Python in-memory cache with time to live
10
How to cache a large machine learning model in Flask?
4
Caching Computations in TensorFlow
19
Storing tensorflow models in memory
4
Reuse value of TensorFlow Variable between sessions without writing to disk
0
Keep track of caches in neural net
3
How to cache and iterate through a Dataset of unknown size?
0
Using .cache() with data preparation in python
Hot Network Questions
How to estimate the latency of communication?
How did Jahnke and Emde create their plots
Does an NEC load calculation overage mandate a service upgrade?
How can dragons heat their breath?
Physical interpretation of selection rules for different multipole orders
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
Should I expect a call from my future boss after signing the offer?
Humans try to help aliens deactivate their defensive barrier
Only selecting Features that have another layers feature on top
When to use cards for communicating dietary restrictions in Japan
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
What is ""B & S"" a reference to in Khartoum?
Convert pipe delimited column data to HTML table format for email
How to delete edges of curve based on their length
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Covering a smoke alarm horn
How can we be sure that the effects of gravity travel at most at the speed of light
How does this Paypal guest checkout scam work?
Heating object in airless environment
How to set individual columns in the siunitx package to boldface? It it a bug?
US phone service for long-term travel
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"sql - Laravel, query to show big data is slow - Stack Overflow","sql - Laravel, query to show big data is slow - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Laravel, query to show big data is slow
Ask Question
Asked
3 years, 11 months ago
Modified
3 years, 11 months ago
Viewed
2k times
-1
I have a page for user searching data from specific date range and it will show result in datatable e.g (
ID
,
Audit type
,
user
,
new value
,
old value
etc.) and it was from 2 table relationship.
Here is my query:
$audits = \OwenIt\Auditing\Models\Audit::with('user')
    ->orderBy('updated_at', 'DESC')
    ->where('created_at', '>=', $date1)
    ->where('created_at', '<=', $date2)
    ->get;
The problem is if amount of data is big, the process so slow. How to optimize the query?
I've tried to use
paginate(10)
or
take(10)
, but it only show 10 data not all data.
sql
laravel
performance
datatable
Share
Improve this question
Follow
edited
Dec 23, 2020 at 8:12
Rwd
35.2k
7
7 gold badges
67
67 silver badges
82
82 bronze badges
asked
Dec 23, 2020 at 2:46
Insko Malaysia
Insko Malaysia
21
9
9 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
To enhance performance at database level, create an index on the column which appears in where constraints.
So create an index on the
created_at
column.
Also to compare dates, why not use the
whereDate
rather than comparing string literals for dates
$audits = \OwenIt\Auditing\Models\Audit::with('user')
    ->orderBy('updated_at', 'DESC')
    ->whereDate('created_at','>=',$date1)
    ->whereDate('created_at','<=',$date2)
    ->paginate(25);
Once the query is getting paginated records, the view can then provide the pagination links for the visitors/users to loop through the paginated result sets
{{ $audit->links() }}
The
links()
will insert pagination link buttons on the view, which users/visitors can click to shuffle through the result sets.
Laravel docs:
https://laravel.com/docs/8.x/pagination#displaying-pagination-results
For large datasets/records in database tables, its always wise to query paginated result sets - to reduce memory usage.
Share
Improve this answer
Follow
answered
Dec 23, 2020 at 3:03
Donkarnash
Donkarnash
12.8k
5
5 gold badges
29
29 silver badges
42
42 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
sql
laravel
performance
datatable
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
How to optimize a big query?
1
Laravel, Datatables, inefficient queries
1
How to efficiently show reports and tables for huge dataset in laravel?
1
Why Datatables is very slow with laravel?
1
laravel datatable take long time to load data
0
Page lagging when query big data in database Laravel
0
How to reduce load time Retrieving large database table with 12,000+ rows in laravel
3
Laravel eloquent too slow with large amount of data
0
Laravel query slow to fetch data from database
2
Laravel Datatables load slow when more than 10k data
Hot Network Questions
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
What technique is used for the heads in this LEGO Halo Elite MOC?
How did Jahnke and Emde create their plots
Didactic tool to play with deterministic and nondeterministic finite automata
How can we be sure that the effects of gravity travel at most at the speed of light
Physical interpretation of selection rules for different multipole orders
Does an NEC load calculation overage mandate a service upgrade?
How to set individual columns in the siunitx package to boldface? It it a bug?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Can two wrongs ever make a right?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Challah dough bread machine recipe issues
How can dragons heat their breath?
Time travelling paedo priest novel
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Only selecting Features that have another layers feature on top
What is ""B & S"" a reference to in Khartoum?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
What does it mean when folks say that universe is not ""Locally real""?
Hole, YHWH and counterfactual present
Bash script that waits until GPU is free
Why does it take so long to stop the rotor of a helicopter after landing?
UUID v7 Implementation
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
default
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
How to plot visualization of missing values for big data in R? - Stack Overflow,"How to plot visualization of missing values for big data in R? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to plot visualization of missing values for big data in R?
Ask Question
Asked
3 years, 11 months ago
Modified
3 years, 11 months ago
Viewed
1k times
Part of
R Language
Collective
1
I would like to draw a plot of missing values for a big data (1000 variables), I tried vis_miss function as follows
library(naniar)
vis_miss(predictors, warn_large_data=TRUE)
However, it shows the names of the variables after drawing the plot which is barely readable as there are too many variables, I was wondering 1. if there is any way to remove variable names from the x axis
2. Is there any other beautiful way to draw a missing value plot for big data?
r
plot
missing-data
Share
Improve this question
Follow
edited
Dec 17, 2020 at 5:05
asked
Dec 17, 2020 at 0:06
user12035904
user12035904
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
The
vis_miss()
function is ggplot-based, so you can change it relatively easily.
Regarding your question:
if there is any way to remove variable names from the x axis
You can remove them using e.g.
vis_miss(predictors, warn_large_data=TRUE) +
  theme(axis.text.x = element_blank())
Or alter them using e.g.
vis_miss(predictors, warn_large_data=TRUE) +
  theme(axis.text.x = element_text(size = 6, angle = 60))
And for your other question:
Is there any other beautiful way to draw a missing value plot for big data?
Without a sample of your actual data it is hard to say what would be best, but there are some suggestions such as
gg_miss_upset()
here:
https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html
Share
Improve this answer
Follow
answered
Dec 17, 2020 at 0:30
jared_mamrot
jared_mamrot
26.1k
4
4 gold badges
25
25 silver badges
53
53 bronze badges
0
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
1
Plotting two columns against each other omitting missing values
10
How to connect dots where there are missing values?
0
Plotting historical data with missing values
3
Number of missing values in ggplot
2
Include number of missing values in ggplot
1
How to plot multiple columns of a data frame to see where data exists in each column?
1
How to avoid gaps due to missing values in matplot in R?
2
Grid plot of missing data
1
How to I draw a line plot and ignore missing values in R
1
R: Plot line chart using ggplot with missing values
Hot Network Questions
Math contents does not align when subscripts are used
US phone service for long-term travel
What's the justification for implicitly casting arrays to pointers (in the C language family)?
How to set individual columns in the siunitx package to boldface? It it a bug?
Humans try to help aliens deactivate their defensive barrier
What does it mean when folks say that universe is not ""Locally real""?
How to format numbers in monospaced (typewriter) font using siunitx?
Need an advice to rig a spaceship with mechanicals part
Can two wrongs ever make a right?
Longest bitonic subarray
What is meaning of forms in ""they are even used as coil forms for inductors?""
how do I correctly check that some aggregated results are correct?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
How could a city build a circular canal?
Does an NEC load calculation overage mandate a service upgrade?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Time travelling paedo priest novel
Find all unique quintuplets in an array that sum to a given target
How to remove clear adhesive tape from wooden kitchen cupboards?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
Didactic tool to play with deterministic and nondeterministic finite automata
Covering a smoke alarm horn
How to achieve infinite rage?
Physical interpretation of selection rules for different multipole orders
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python MySQL insert big data - Stack Overflow,"python MySQL insert big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
python MySQL insert big data
Ask Question
Asked
3 years, 11 months ago
Modified
3 years, 11 months ago
Viewed
240 times
0
using python,I am looping through csv file to read data, then I am ding some modifications on the readied row and call a save function to insert the modified data into MySQL.
def save(Id, modifiedData,):
    try:
       mydb = mysql.connector.connect(host=""localhost"",user=""use"",password=""pass"",database=""data"")
       sql = ""INSERT INTO data (Id, modifiedData)  VALUES (%s, %s)""
       recordTuple = (Id, modifiedData)
       mycursor = mydb.cursor()
       mycursor.execute(sql,recordTuple)
       mydb.commit()
       print(""Record inserted successfully into table"")

    except mysql.connector.Error as error:
        print(""Failed to insert into MySQL table {}"".format(error))

def main():
            for row in csv:    
             #modify row 
             #creat Id
             save(Id, modifiedData,)
but  I don't think this is good solution to do MYSQL connection and insert data with each iteration, it will be time and resources consuming , specially when I move to real server in production
how can I improve my solution?
python
mysql
Share
Improve this question
Follow
asked
Dec 14, 2020 at 12:32
matjar trk
matjar trk
97
1
1 gold badge
2
2 silver badges
7
7 bronze badges
4
Accumulate the modified data in a buffer, and send many values to MySQL at once (when the number of values reaches a certain limit, for example, 1000 rows, or when all rows are processed) in a single query. For example, with
cursor.executemany
.
–
Akina
Commented
Dec 14, 2020 at 12:45
The fastest way to do this is to save the modified data in a new csv then use a ""LOAD DATA LOCAL INFILE ..."" statement
–
Chechy Levas
Commented
Dec 14, 2020 at 12:54
@ChechyLevas This needs (1) the place accessable for both PHP and MySQL exists (2) LOAD DATA [LOCAL] INFILE allowed on MySQL.
–
Akina
Commented
Dec 14, 2020 at 13:16
@Akina fair point. But if those 2 conditions are satisfied then this will most performant. For very large csv files, might be worth pursuing.
–
Chechy Levas
Commented
Dec 14, 2020 at 13:22
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
Ideally, connections should be managed by connection pool, should be committed bulky. But amount of csv at most, need not to mind so much. Anyway, If you don't wanna bother it, I recommend using ORM like
SQLAlchemy
.
Share
Improve this answer
Follow
answered
Dec 14, 2020 at 12:47
Akihito KIRISAKI
Akihito KIRISAKI
1,333
9
9 silver badges
13
13 bronze badges
Add a comment
|
0
You only need to create the connection once, and that should be in function
main
, who then passes the connection to function
save
as follows:
def save(mydb, Id, modifiedData):
    try:
       sql = ""INSERT INTO data (Id, modifiedData)  VALUES (%s, %s)""
       recordTuple = (Id, modifiedData)
       mycursor = mydb.cursor()
       mycursor.execute(sql,recordTuple)
       mydb.commit()
       print(""Record inserted successfully into table"")
    except mysql.connector.Error as error:
        print(""Failed to insert into MySQL table {}"".format(error))

def main():
    try:
        mydb = mysql.connector.connect(host=""localhost"",user=""use"",password=""pass"",database=""data"")
    except mysql.connector.Error as error:
        print(""Failed to create connection: {}"".format(error))
        return            
    for row in csv:    
         #modify row 
         #creat Id
         save(mydb, Id, modifiedData)
For perhaps even greater performance you can try
executemany
:
def save(mydb, modified_records):
    try:
       sql = ""INSERT INTO data (Id, modifiedData)  VALUES (%s, %s)""
       mycursor = mydb.cursor()
       mycursor.executemany(sql, modified_records)
       mydb.commit()
       print(""Records inserted successfully into table"")
    except mysql.connector.Error as error:
        print(""Failed to insert into MySQL table {}"".format(error))

def main():
    try:
        mydb = mysql.connector.connect(host=""localhost"",user=""use"",password=""pass"",database=""data"")
    except mysql.connector.Error as error:
        print(""Failed to create connection: {}"".format(error))
        return
    modified_records = []
    for row in csv:    
        #modify row 
        #creat Id
        modified_records.append([id, modifiedData])
    save(mydb, modified_records)
Share
Improve this answer
Follow
edited
Dec 14, 2020 at 13:06
answered
Dec 14, 2020 at 12:56
Booboo
Booboo
43.4k
4
4 gold badges
41
41 silver badges
69
69 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
mysql
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
23
Python+MySQL - Bulk Insert
43
How to use python mysqldb to insert many rows at once
0
python best way to insert 60k rows in mysql
0
MySQL and Python-Insertion
0
Python insert data into MySQL
4
Inserting millions of records into MySQL database using Python
1
Inserting data into mysql using python
1
What is an efficient way to insert large amounts of data into a MySQL table using python?
1
Python insert to mysql
1
Fastest Way to insert data( millions of data) into mysql in python
Hot Network Questions
Hole, YHWH and counterfactual present
What does “going off” mean in ""Going off the age of the statues""?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Topology of a horocycle
How to remove clear adhesive tape from wooden kitchen cupboards?
How to achieve infinite rage?
how do I correctly check that some aggregated results are correct?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Didactic tool to play with deterministic and nondeterministic finite automata
How much of a structural/syntactic difference is there between an oath and a promise?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Why does it take so long to stop the rotor of a helicopter after landing?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
How did Jahnke and Emde create their plots
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
When was ""to list"" meaning ""to wish"" lost?
How can dragons heat their breath?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
Why is the speed graph of a survey flight a square wave?
Is it possible that the committee contacts only one reference while applicants need to provide two?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
default
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python 3.x - What are faster ways of reading big data set and apply row wise operations other than pandas and dask? - Stack Overflow,"python 3.x - What are faster ways of reading big data set and apply row wise operations other than pandas and dask? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
What are faster ways of reading big data set and apply row wise operations other than pandas and dask?
Ask Question
Asked
4 years, 1 month ago
Modified
4 years ago
Viewed
215 times
-1
I am working on a code where I need to populate a set of data structure based on each row of a big table. Right now, I am using pandas to read the data and do some elementary data validation preprocess. However, when I get to the rest of process and putting data in the corresponding data structure, it takes considerably long time for the loop to be completed and my data structures gets populated. For example, in the following code I have a table with 15 M records. Table has three columns and I create a foo() object base on each row and add it to a list.
# Profile.csv 
# Index    | Name | Family| DB
# ---------|------|-------|----------
# 0.       | Jane | Doe   | 08/23/1977
# ...
# 15000000 | Jhon | Doe   | 01/01/2000

class foo():
    def __init__(self, name, last, bd):
        self.name = name
        self.last = last
        self.bd = bd

def populate(row, my_list):
    my_list.append(foo(*row))

# reading the csv file and formatting the date column
df = pd.read_csv('Profile.csv')
df['DB'] = pd.to_datetime(df['DB'],'%Y-%m-%d')

# using apply to create an foo() object and add it to the list
my_list = []
gf.apply(populate, axis=1, args=(my_list,))
So the after using pandas to convert the string date to the date object, I just need to iterate over the DataFrame to creat my object and add them to the list. This process is very time taking (in my real example it is even taking more time since my data structure is more complex and I have more columns). So, I am wondering what is the best practice in this case to enhance my run time. Should I even use
pandas
to read my big tables and process through them row by row?
python-3.x
database
bigdata
apply
rowwise
Share
Improve this question
Follow
edited
Oct 26, 2020 at 7:20
Adrian
asked
Oct 26, 2020 at 4:04
Adrian
Adrian
213
4
4 silver badges
9
9 bronze badges
0
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
it would be simply faster using a file handle:
input_file = ""profile.csv""
sep="";""
my_list = []
with open(input_file) as fh:
    cols = {}
    for i, col in enumerate(fh.readline().strip().split(sep)):
        cols[col] = i
    for line in fh:
        line = line.strip().split(sep)
        date = line[cols[""DB""]].split(""/"")
        date = [date[2], date[0], date[1]]
        line[cols[""DB""]] = ""-"".join(date)
        populate(line, my_list)
Share
Improve this answer
Follow
answered
Oct 26, 2020 at 6:45
Wazaa
Wazaa
146
3
3 bronze badges
3
Thanks @Wazaa, that is a nice idea, however, if you want to convert your string date to date object, the conversion over each row takes a bit longer than pandas. If you have multiple date fields this excess conversion might compromise all the saving we gain with the file handler.
–
Adrian
Commented
Oct 26, 2020 at 7:27
1
this might help
peterbe.com/plog/fastest-python-datetime-parser
–
Wazaa
Commented
Oct 26, 2020 at 8:25
1
also
stackoverflow.com/questions/43726661/…
–
Wazaa
Commented
Oct 26, 2020 at 8:37
Add a comment
|
0
There are multiple approaches for this kind of situation, however, the fastest and most effective method is using vectorization if possible. The solution for the example I demonstrated in this post using vectorization could be as follows:
my_list = [foo(*args) for args in zip(df[""Name""],df[""Family""],df[""BD""])]
If the vectorization is not possible, converting the data framce to a dictionary could significantly improve the performance. For the current example if would be something like:
my_list = []
dc = df.to_dict()
for i, j in dc.items():
    my_list.append(foo(dc[""Name""][i], dc[""Family""][i], dc[""BD""][i]))
The last solution is particularly very effective if the type of structures and processes are more complex.
Share
Improve this answer
Follow
edited
Dec 10, 2020 at 22:13
answered
Nov 19, 2020 at 22:53
Adrian
Adrian
213
4
4 silver badges
9
9 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python-3.x
database
bigdata
apply
rowwise
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
5
Is there a way to improve speed of parsing date for large file?
Related
0
Speed up loop over each row for big dataset in Python
1
what is best way to do analysis over database table of million records?
2
Read Large data from database table in pandas or dask
1
Efficiently read in large csv files using pandas or dask in python
0
How to load and process dataset with million columns with Pandas or Pandas-like library?
1
How can I speed up these dataframe operations on 12k files/50gb?
0
Retrieve Records faster from whole dask dataframe
0
pandas, dataframe: If you need to process data row by row, how to do it faster than itertuples
1
handling million of rows for lookup operation using python
0
How can I work on a large dataset without having to use Pyspark?
Hot Network Questions
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
How can we be sure that the effects of gravity travel at most at the speed of light
Derailleur Hangar - Fastener torque & thread preparation
Find all unique quintuplets in an array that sum to a given target
Why did Crimea’s parliament agree to join Ukraine?
Math contents does not align when subscripts are used
Indian music video with over the top CGI
US phone service for long-term travel
What does “going off” mean in ""Going off the age of the statues""?
How *exactly* is divisibility defined?
Is there greater explanatory power in laws governing things rather than being descriptive?
How to explain why I don't have a reference letter from my supervisor
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Protecting myself against costs for overnight weather-related cancellations
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
When to use cards for communicating dietary restrictions in Japan
How can dragons heat their breath?
Why are Jersey and Guernsey not considered sovereign states?
What should machining (turning, milling, grinding) in space look like
Topology of a horocycle
Physical interpretation of selection rules for different multipole orders
How to estimate the latency of communication?
What's the difference between '\ ' and tilde character (~)?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python - Compute time difference according to a condition and for big data with Pyspark - Stack Overflow,"python - Compute time difference according to a condition and for big data with Pyspark - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Compute time difference according to a condition and for big data with Pyspark
Ask Question
Asked
4 years ago
Modified
4 years ago
Viewed
191 times
0
help please...I have data like this:
from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc = SparkContext.getOrCreate()
spark = SparkSession(sc)
from pyspark.sql.functions import substring, length
dept = [(""A"",1,""2020-11-07 23:19:12""), (""A"",1,""2020-11-07 23:19:16""), (""A"",1,""2020-11-07 23:19:56""), (""A"",0,""2020-11-07 23:20:37""), (""A"",0,""2020-11-07 23:21:06""), (""A"",0,""2020-11-07 23:21:47""), (""A"",1,""2020-11-07 23:22:05""), (""A"",1,""2020-11-07 23:22:30""),(""A"",1,""2020-11-07 23:23:00""), (""B"",1,""2020-11-07 22:19:12""), (""B"",1,""2020-11-07 22:20:10""), (""B"",0,""2020-11-07 22:21:31""), (""B"",0,""2020-11-07 22:22:01""), (""B"",0,""2020-11-07 22:22:45""), (""B"",1,""2020-11-07 22:23:52""), (""B"",1,""2020-11-07 22:24:10"")]
deptColumns = [""Id"",""BAP"",""Time""]
deptDF = spark.createDataFrame(data=dept, schema = deptColumns)
deptDF.show()
With Pyspark, how to get the time for the first 0 of the first series of zeros for each ID, and get the time of the first 1 just after the same series of zeros.Then make a time_stamp subtraction between the two. Something like this:
This must be done for each series of zeros which compose each ID. Thus, we can have several DeltaTime for the same ID if there are several series of zeros.
Actually, i can compute delta time between consecutive rows:
Delta=deptDF.withColumn(""DeltaTime"",(deptDF.Time.cast(""bigint"") - lag(deptDF.Time.cast(""bigint""),1).over(Window.partitionBy(""Id"").orderBy(""Time"")).cast(""bigint"")))
Delta.show()
it-is possible to add any condition to have expected result ?
python
dataframe
apache-spark
pyspark
apache-spark-sql
Share
Improve this question
Follow
edited
Dec 10, 2020 at 7:41
mck
42.3k
13
13 gold badges
40
40 silver badges
57
57 bronze badges
asked
Dec 9, 2020 at 21:31
Data Tao
Data Tao
23
4
4 bronze badges
2
Welcome to SO. Please show your efforts. SO is not a work-offloading system.
–
ShlomiF
Commented
Dec 9, 2020 at 22:32
Post edited.. check the new version
–
Data Tao
Commented
Dec 9, 2020 at 22:44
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
Add two columns begin0 and begin1 to help with parsing your data using window functions:
import pyspark.sql.functions as F

window = Window.partitionBy('Id').orderBy('Time')
Delta = deptDF.withColumn(
    'begin0',
    (F.lag('BAP').over(window) != 0) & (F.col('BAP') == 0)
).withColumn(
    'begin1',
    (F.lag('BAP').over(window) == 0) & (F.col('BAP') == 1)
).filter(
    'begin0 or begin1'
).withColumn(
    'DeltaTime',
    F.when(
        F.col('BAP') == 0,
        F.date_format(
            (
                F.lead('Time').over(window).cast('timestamp').cast('bigint') -
                F.col('Time').cast('timestamp').cast('bigint')
            ).cast('timestamp'),
           'HH:mm:ss'
       )
    ).otherwise(
        F.lit('00:00:00')
    )
).drop(
    'begin0', 'begin1'
).orderBy(
    'Id','Time'
)

Delta.show()
+---+---+-------------------+---------+
| Id|BAP|               Time|DeltaTime|
+---+---+-------------------+---------+
|  A|  0|2020-11-07 23:20:37| 00:01:28|
|  A|  1|2020-11-07 23:22:05| 00:00:00|
|  B|  0|2020-11-07 22:21:31| 00:02:21|
|  B|  1|2020-11-07 22:23:52| 00:00:00|
+---+---+-------------------+---------+
Share
Improve this answer
Follow
answered
Dec 10, 2020 at 8:02
mck
mck
42.3k
13
13 gold badges
40
40 silver badges
57
57 bronze badges
0
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
dataframe
apache-spark
pyspark
apache-spark-sql
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
6
SPARK, DataFrame: difference of Timestamp columns over consecutive rows
5
Calculate time between two dates in pyspark
33
How to calculate date difference in pyspark?
1
Filter pyspark dataframe based on time difference between two columns
0
Pyspark timestamp difference based on column values
0
timestamp difference between rows for each user - Pyspark Dataframe
0
pyspark: find time diffs between rows with conditions
3
Calculate time difference between consecutive rows in pairs per group in pyspark
0
Pyspark calculated field based off time difference
1
PySpark comparing time (only) against a time variable
Hot Network Questions
How to estimate the latency of communication?
How does this Paypal guest checkout scam work?
how do I correctly check that some aggregated results are correct?
Only selecting Features that have another layers feature on top
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
How to set individual columns in the siunitx package to boldface? It it a bug?
White perpetual check, where Black manages a check too?
How to keep meat in a dungeon fresh, preserved, and hot?
When to start playing the chord when a measure starts with a rest symbol?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
Building a Statistically Sound ML Model
Is outer space Radioactive?
Covering a smoke alarm horn
Heating object in airless environment
How to format numbers in monospaced (typewriter) font using siunitx?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Why is the speed graph of a survey flight a square wave?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
Hole, YHWH and counterfactual present
What technique is used for the heads in this LEGO Halo Elite MOC?
What's the difference between '\ ' and tilde character (~)?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
Topology of a horocycle
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
oracle database - How to generate big data volume to perform load test using JMeter? - Stack Overflow,"oracle database - How to generate big data volume to perform load test using JMeter? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to generate big data volume to perform load test using JMeter?
Ask Question
Asked
4 years ago
Modified
4 years ago
Viewed
495 times
0
We need to perform load test using JMeter, but the test database has few records. As you know we need a dataset which is similar to the production dataset which is huge. But there is no option to get the large data volume. We are using Oracle as database.
Does there any option in JMeter to simulate the large data volume?
What are the options do we have to perform a load test to meet the above criteria?
oracle-database
groovy
jmeter
jmeter-plugins
Share
Improve this question
Follow
asked
Dec 9, 2020 at 11:24
Flashmark
Flashmark
129
3
3 silver badges
11
11 bronze badges
1
1
Can you create a copy of the production dataset to your test database? Also, is your test database similar in terms of performance to production? If not, your load test results may be hard to interpret/extrapolate.
–
Martin Schapendonk
Commented
Dec 9, 2020 at 12:53
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
You can create a database dump, if necessary anonymize it removing the sensitive data and import it into your ""empty"" database, see
Moving Data Using Data Pump and Export/Import
for more details
If point it is not achievable for any reason you can use JMeter's
JDBC Request
sampler to directly add some fake data into the database using
INSERT Statements
If point 2 cannot be implemented as well there are
some aspects which you still can test against a scaled-down environment
like:
system(s) integration under the load
load allocation
monitoring the backend using
profiling tools
running a
soak test
Share
Improve this answer
Follow
answered
Dec 9, 2020 at 14:00
Dmitri T
Dmitri T
168k
5
5 gold badges
88
88 silver badges
146
146 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
oracle-database
groovy
jmeter
jmeter-plugins
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
Load Testing with JMeter
1
How to generate load testing reports using jmeter?
3
How to set up a large amount of request data for a jMeter load test
0
How do JMeter load test stream upload
0
Jmeter load distribution
0
Automating test data generation in Performance testing-Jmeter
1
Jmeter : how to get large number of rps in jmeter
1
Testing 10.000 VU in JMeter in 10 seconds
0
How to generate huge dynamic test data using jmeter to perform stress testing
0
Using JMeter testing tool
Hot Network Questions
What should machining (turning, milling, grinding) in space look like
How to set individual columns in the siunitx package to boldface? It it a bug?
How should I connect a light fixture with UK wire colors to US wiring?
Building a Statistically Sound ML Model
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
Does an NEC load calculation overage mandate a service upgrade?
Math contents does not align when subscripts are used
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
How can we be sure that the effects of gravity travel at most at the speed of light
Derailleur Hangar - Fastener torque & thread preparation
Why does it take so long to stop the rotor of a helicopter after landing?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
What are these 16-Century Italian monetary symbols?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Why is the speed graph of a survey flight a square wave?
reverse engineering wire protocol
Hole, YHWH and counterfactual present
Indian music video with over the top CGI
Find all unique quintuplets in an array that sum to a given target
How can dragons heat their breath?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
Is outer space Radioactive?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
default
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
elasticsearch - logstash - Jdbc input plugin doesn’t work with prepared statements enabled and with big data - Stack Overflow,"elasticsearch - logstash - Jdbc input plugin doesn’t work with prepared statements enabled and with big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
logstash - Jdbc input plugin doesn’t work with prepared statements enabled and with big data
Ask Question
Asked
4 years, 3 months ago
Modified
4 years ago
Viewed
1k times
0
I've a task to import data from Oracle database to Elasticsearch.
Of course I'm using JDBC input plugin for it.
Due to performance reasons I need to enable prepared statements for a plugin.
(It will reduce read operations on DB and will proper index usage)
My configuration looks as follows:
input {
  jdbc {
    jdbc_fetch_size => 999
    schedule => ""* * * * *""
    use_prepared_statements => true
    prepared_statement_name => ""foo""
    prepared_statement_bind_values => ["":sql_last_value""]
    statement => "" SELECT
      ......
      FROM table_name tbl
      JOIN ......
      JOIN ...
      LEFT JOIN ......
      WHERE tbl.id > ?
    ""
    use_column_value => true
    tracking_column => ""id""
  }
}
But here I hit a problem. After activating it:
no events are transmitted in logstash
no new documents in ELK are created
CPU usage and memory consumptions is 100%.
after some time logstash scrash with following error:
java.lang.BootstrapMethodError: call site initialization exception
Few important remarks:
it doesn't matter if I change jdbc_fetch_size to smaller or larger value (it only affects how fast memory will be consumed)
on smaller amount of data everything works fine - documents in ELK indexes are created but with slight delay which doesn't occur when prepared statement are disabled.
when prepared statement are disabled everything works fine even with large data and without any delay
I've tested it on two versions 7.8.0 and 7.9.0 - results on both is the same - not working.
Am I dealing here with a bug?
elasticsearch
logstash
logstash-jdbc
Share
Improve this question
Follow
edited
Sep 2, 2020 at 19:54
e1m7bo
asked
Sep 1, 2020 at 21:34
e1m7bo
e1m7bo
53
5
5 bronze badges
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
What value are you using for
jdbc_driver_class
?
What happens if you execute the query directly against the DB?
How many records is that sql statement returning from the DB?
Have you tried to return only a batch of rows from the DB? For that you can change your SQL statement to:
select * from (
 SELECT
      ......
      FROM table_name tbl
      JOIN ......
      JOIN ...
      LEFT JOIN ......
      WHERE tbl.id > ?
) where rownum = 100000;
Share
Improve this answer
Follow
answered
Nov 18, 2020 at 16:59
Luis Goncalves
Luis Goncalves
1
Add a comment
|
0
What value are you using for jdbc_driver_class?
Java::oracle.jdbc.driver.OracleDriver
(driver itself is oracle:oracle:11.2.0.4-jdk6-sp1)
What happens if you execute the query directly against the DB?
It is working (long, because of number of entries returned) but it didn't crash and statements were constantly displayed (checked on oracle console).
How many records is that sql statement returning from the DB?
193 millions of records
Have you tried to return only a batch of rows from the DB
Yes, with limited records it works but it is not an option for me - I need to fetch all records and not only part of it (using
rownum
doesn't ensure it)
Here problem with prepared statements is fact that it crash, I do not complain about long lasting import (I'm prepared for it - large number of records enforced it)
Share
Improve this answer
Follow
answered
Dec 7, 2020 at 14:22
e1m7bo
e1m7bo
53
5
5 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
elasticsearch
logstash
logstash-jdbc
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
1
Logstash JSON filter does not appear to be working with JDBC input
0
Logstash jdbc-input-plugin configuration for initial sql_last_value
1
Error in JDBC connection using logstash
0
Not able to create elasticsearch's mappings for the index created from logstash JDBC input plugin
0
Logstash JDBC input plugin execution failed with error : Expected one of #, {, } at line 9, column 60
0
Logstash jdbc not sending data
5
Running Logstash on multiple nodes with JDBC input plugin
0
Logstash as a service with JDBC input
0
Logstash JDBC: Update row issue
0
Problem of JDBC Plugin in logstash config
Hot Network Questions
How to keep meat in a dungeon fresh, preserved, and hot?
How to remove clear adhesive tape from wooden kitchen cupboards?
Humans try to help aliens deactivate their defensive barrier
How to use a symbol as both function and head?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Covering a smoke alarm horn
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
Realization of fundamental group endomorphism
Find a fraction's parent in the Stern-Brocot tree
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
how do I correctly check that some aggregated results are correct?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Physical interpretation of selection rules for different multipole orders
How to delete edges of curve based on their length
Why does it take so long to stop the rotor of a helicopter after landing?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Why are Jersey and Guernsey not considered sovereign states?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
How to set individual columns in the siunitx package to boldface? It it a bug?
How *exactly* is divisibility defined?
Does an NEC load calculation overage mandate a service upgrade?
How does this Paypal guest checkout scam work?
How to estimate the latency of communication?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
c++ - Shared array for big data - Stack Overflow,"c++ - Shared array for big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Shared array for big data
Ask Question
Asked
4 years ago
Modified
4 years ago
Viewed
69 times
0
I have two threads. The first one is writing data from a USB in a big array(main buffer). Every when it gets new data from USB, an offset (write offset) will increase. In second thread has a ""read offset"" and it will grow (too) when new data are extracting from the main buffer.
One thread is writing, and the next is reading from that.
But this solution is not optimized. I'll need a big and bigger array for my different requirements. I need a dynamic behavior for using a memory that is shared between two or more threads.
I'm sure so many useful algorithms for my problem.
Anybody can suggest some of them, please? [C(++) language]
c++
Share
Improve this question
Follow
asked
Nov 23, 2020 at 2:48
JaberRouhi
JaberRouhi
15
8
8 bronze badges
2
Do you know how to use
std::deque
? It'll likely manage your buffering much better than you can do yourself. All you have to do is make sure that access to the
std::deque
is properly synchronized between your execution threads.
–
Sam Varshavchik
Commented
Nov 23, 2020 at 2:50
thanks. let me check the std::deque. It seems good.
–
JaberRouhi
Commented
Nov 23, 2020 at 9:33
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
I've done something similar in C++. I started by building a list -- a free list -- of buffers, and populating it with two empty buffers I got from
malloc()
or
new
. Then my incoming data thread did this
if an empty buffer was in the list, take it off the list and start writing to it. If no buffers were in this list, I got another one from
malloc()
or
new
.
when the active buffer filled up, I put it into a queue and grabbed another.
my consumer thread, the other one, pulled buffers from the queued, did what was needed, and placed the buffer back in the free list.
This scheme worked better than the ring buffer scheme you described, because it could add buffers if needed.
You'll need to use thread-safe operations when you put buffers into the queue or the free list, or take them out. Use appropriate thread safe stdlib collections for the queue and the free list.
You could also use a ring buffer, and when it's not big enough use
realloc
to get a new one 1.5x the size and copy your data.
Share
Improve this answer
Follow
edited
Nov 23, 2020 at 3:09
answered
Nov 23, 2020 at 3:04
O. Jones
O. Jones
108k
17
17 gold badges
128
128 silver badges
180
180 bronze badges
1
the @Sam Varshavchik  method is easier. let me chack that first.
–
JaberRouhi
Commented
Nov 23, 2020 at 9:35
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
c++
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
C++: Array and shared memory
2
Store huge amount of data in memory
0
how to store data in a large double dimension array
0
Working with huge arrays in C++
2
Data structure for storing very large 2D data in C++
0
Sharing dynamically allocated arrays between C++ source files
1
Memory Organising for big data
1
Store shared pointer to array in vector
1
Storing 200kb of data in an array?
0
c++11 share fixed size array between different classes
Hot Network Questions
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How does this Paypal guest checkout scam work?
Covering a smoke alarm horn
Why does it take so long to stop the rotor of a helicopter after landing?
Realization of fundamental group endomorphism
How did Jahnke and Emde create their plots
Can a hyphen be a ""letter"" in some words?
Is outer space Radioactive?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Why are Jersey and Guernsey not considered sovereign states?
Challah dough bread machine recipe issues
Convert pipe delimited column data to HTML table format for email
How manage inventory discrepancies due to measurement errors in warehouse management systems
Find a fraction's parent in the Stern-Brocot tree
Useful aerial recon vehicles for newly colonized worlds
What should machining (turning, milling, grinding) in space look like
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
Derailleur Hangar - Fastener torque & thread preparation
How can dragons heat their breath?
How to estimate the latency of communication?
How to use a symbol as both function and head?
How to set individual columns in the siunitx package to boldface? It it a bug?
When looking at the first DCM page, where is the next DCM page documented?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-cpp
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
R analyse string in column of a big data frame and give value in a separate column - Stack Overflow,"R analyse string in column of a big data frame and give value in a separate column - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
R analyse string in column of a big data frame and give value in a separate column
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
102 times
Part of
R Language
Collective
0
I have a data frame with 20 columns. One of the columns is made up of strings. I would like to analyse the strings for positions of specific alphabets in R and then assign a value based on this in a different column. For example, if the strings are ABCDEF, AADFEG, I would like to create a new column with values, with 1 if A is in position 1. 1,2 if A is in position 1,2. 7 if G is in position 6. I have been trying to use str_locate and also if_else and then mutate using dplyr package but not sure if there is an easier way to do this.
Thanks!
r
string
Share
Improve this question
Follow
edited
Nov 12, 2020 at 14:05
Gregor Thomas
145k
22
22 gold badges
183
183 silver badges
313
313 bronze badges
asked
Nov 12, 2020 at 13:52
blablabla
blablabla
1
4
so shouldn't you have 26+1=27 columns, 1 for string and other 26 for 26 alphabets?
–
AnilGoyal
Commented
Nov 12, 2020 at 13:57
What is the output for the string ""AACCCG"" - A is in position 1 and 2, and G is in position 6. Should the result be
1
,
2
,
7
,
127
or something else?
–
Gregor Thomas
Commented
Nov 12, 2020 at 14:03
A small reproducible example with sample input and desired output would help clear the intent.
–
Gregor Thomas
Commented
Nov 12, 2020 at 14:16
Apologies for not making it clearer. But I wanted the output to be 1,2. The code below from mat.tho works. I can use it to add the other letters as well. Thanks!
–
blablabla
Commented
Nov 12, 2020 at 16:45
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
I've created a small code snippet:
df<-data.frame(a=c(""ABCDEF"",""AADFEG"",""TRETGTGA""))
df$b<-lapply(gregexpr(pattern = 'A', df$a), function(x) c(unlist(x)) )
Here, you apply a regex at each entry of the vector
df$a
and find the pattern
A
, resulting for the indices of all
A
s. With
lapply
the results (=indices) are extracted. At the end you assign the result to a new column in the dataframe.
note
If the pattern can't be found, -1 is shown.
Share
Improve this answer
Follow
edited
Nov 12, 2020 at 15:00
answered
Nov 12, 2020 at 14:54
mat.tho
mat.tho
204
1
1 silver badge
9
9 bronze badges
0
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
r
string
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
5
Extracting a string from a data frame
2
R - How to get at a string from a single column and row in a data frame
1
Extract Values stored in a certain pattern from a string in R and store in a data frame
0
R: extract data from one column based on string detected in another
5
Extract value from data frame column based on another column?
0
Split String to data frame
2
extract info from a column based on value from another column in data.frame r
2
How to retrieve value in one data frame by matching a string within an entire column from another data frame?
1
How to make new column values based on part of string from another column in R
2
get a value from column based on string in another column in data frame
Hot Network Questions
What are these 16-Century Italian monetary symbols?
How should I connect a light fixture with UK wire colors to US wiring?
Is it possible that the committee contacts only one reference while applicants need to provide two?
How to set individual columns in the siunitx package to boldface? It it a bug?
Heating object in airless environment
Useful aerial recon vehicles for newly colonized worlds
What does “going off” mean in ""Going off the age of the statues""?
Only selecting Features that have another layers feature on top
How can we be sure that the effects of gravity travel at most at the speed of light
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
How could a city build a circular canal?
Realization of fundamental group endomorphism
Passphrase entropy calculation, Wikipedia version
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
When was ""to list"" meaning ""to wish"" lost?
Indian music video with over the top CGI
Bash script that waits until GPU is free
Covering a smoke alarm horn
How to achieve infinite rage?
Does DOS require partitions to be aligned at a cylinder boundary?
Why is the speed graph of a survey flight a square wave?
Physical interpretation of selection rules for different multipole orders
How *exactly* is divisibility defined?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python - pd.read_sav and pyreadstat are so slow. how can i speed up pandas for big data if i have to use SAV/SPSS file format? - Stack Overflow,"python - pd.read_sav and pyreadstat are so slow. how can i speed up pandas for big data if i have to use SAV/SPSS file format? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
pd.read_sav and pyreadstat are so slow. how can i speed up pandas for big data if i have to use SAV/SPSS file format?
Ask Question
Asked
4 years, 3 months ago
Modified
4 years, 1 month ago
Viewed
2k times
0
I've been transitioning away from SPSS for syntax writing/data management where I work to python and pandas for higher levels of functionality and programming. The issue is, reading SPSS files into pandas is SO slow. i work with bigger datasets (1 million or more rows often with 100+ columns). it seems that there are some pretty cool plugins out there to speed up processing CSV files such as Dask and Modin, but i don't think these work with SPSS files. i'd like to continue using pandas, but i have to stick with the SPSS file format (it's what everyone else where i work uses).
Are there any tips on how to accomplish faster data processing outside of computer upgrades and or file chunking?
python
pandas
spss
Share
Improve this question
Follow
asked
Sep 2, 2020 at 19:16
Nate
Nate
157
12
12 bronze badges
1
there is currently an open issue about reading spss files performance: github.com/Roche/pyreadstat/issues/80. Please provide a sample file to investigate
–
Otto Fajardo
Commented
Oct 31, 2020 at 11:42
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
2
You can try to parallelize reading your file:
As an example I have a file ""big.sav"" which is 294000 rows x 666 columns. Reading the file with pyreadstat.read_sav (which is what pd.read_spss uses in the background) takes 115 seconds. By parallelizing it I get 29 seconds:
first I create a file worker.py:
def worker(inpt):
    import pyreadstat
    offset, chunksize, path = inpt
    df, meta = pyreadstat.read_sav(path, row_offset=offset, row_limit=chunksize)
    return df
and then in the main script I have this:
import multiprocessing as mp
from time import time

import pandas as pd
import pyreadstat

from worker import worker

# calculate the number of rows in the file
_, meta = pyreadstat.read_sav(""big.sav"", metadataonly=True)
numrows = meta.number_rows
# calculate number of cores in the machine, this could also be set manually to some number, i.e. 8
numcores = mp.cpu_count()
# calculate the chunksize and offsets
divs = [numrows // numcores + (1 if x < numrows % numcores else 0)  for x in range (numcores) ]
chunksize = divs[0]
offsets = [indx*chunksize for indx in range(numcores)] 
# pack the data for the jobs
jobs = [(x, chunksize, ""big.sav"") for x in offsets]

pool = mp.Pool(processes=numcores)
# let's go!
t0=time()
chunks = pool.map(worker, jobs)
t1=time()
print(t1-t0) # this prints 29 seconds
# chunks is a list of dataframes in the right order
# you can concatenate all the chunks into a single big dataframe if you like
final = pd.concat(chunks, axis=0, ignore_index=True)
EDIT:
pyreadstat version 1.0.3 has had a big improvement in performance of about 5x.
In addition a new function ""read_file_multiprocessing"" has been added that is a wrapper around the previous code shared in this answer. It can give up to another 3x improvement, making (up to) a 15 times improvement compared to the previous version!
You can use the function like this:
import pyreadstat

fpath = ""path/to/file.sav"" 
df, meta = pyreadstat.read_file_multiprocessing(pyreadstat.read_sav, fpath)
Share
Improve this answer
Follow
edited
Nov 6, 2020 at 16:59
answered
Nov 1, 2020 at 11:07
Otto Fajardo
Otto Fajardo
3,357
1
1 gold badge
19
19 silver badges
28
28 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
pandas
spss
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
4
Performance reading large SPSS file in pandas dataframe on Windows 7 (x64)
3
how to rapidaly load data into memory with python?
4
Converting .sav file to pandas dataframe
5
How to convert .sav file into csv file
10
Reading huge sas dataset in python
3
Is there any way of accelerating file read/write in Pandas?
2
Is it possible to use pandas and/or pyreadstat to read a large SPSS file in chunks, or does an alternative exist?
1
How to convert large .sav file into csv file
1
Python Pandas improving calculation time for large datasets currently taking ~400 mins to run
1
pyreadstat read and write spss without data loss
Hot Network Questions
Bash script that waits until GPU is free
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
A Pandigital Multiplication
Heating object in airless environment
Convert pipe delimited column data to HTML table format for email
Should I expect a call from my future boss after signing the offer?
How to set individual columns in the siunitx package to boldface? It it a bug?
Covering a smoke alarm horn
Indian music video with over the top CGI
Building a Statistically Sound ML Model
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Math contents does not align when subscripts are used
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
Time travelling paedo priest novel
What does it mean when folks say that universe is not ""Locally real""?
UUID v7 Implementation
How can dragons heat their breath?
Does DOS require partitions to be aligned at a cylinder boundary?
Hole, YHWH and counterfactual present
What technique is used for the heads in this LEGO Halo Elite MOC?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
r - Extract columns from big data table to small data tables and save in a list - Stack Overflow,"r - Extract columns from big data table to small data tables and save in a list - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Extract columns from big data table to small data tables and save in a list
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
92 times
Part of
R Language
Collective
0
I get a data table (time series for different products depending on dates) from an extern server, which can have the following maximal number of columns (date is always the first column, and all other columns can exists or not, or there are only two additional columns, or whatever):
set.seed(123)
dt.data <- data.table(date = seq(as.Date('2020-01-01'), by = '1 day', length.out = 365),
                      'DEB Cal-2019' = rnorm(365, 2, 1), 'DEB Cal-2021' = rnorm(365, 2, 1),
                      'DEB Cal-2022' = rnorm(365, 2, 1), 'DEB Cal-2023' = rnorm(365, 2, 1),
                      'ATB Cal-2019' = rnorm(365, 2, 1), 'ATB Cal-2021' = rnorm(365, 2, 1),
                      'ATB Cal-2022' = rnorm(365, 2, 1), 'ATB Cal-2023' = rnorm(365, 2, 1),
                      'TTF Cal-2019' = rnorm(365, 2, 1), 'TTF Cal-2021' = rnorm(365, 2, 1),
                      'TTF Cal-2022' = rnorm(365, 2, 1), 'TTF Cal-2023' = rnorm(365, 2, 1),
                      'NCG Cal-2019' = rnorm(365, 2, 1), 'NCG Cal-2021' = rnorm(365, 2, 1),
                      'NCG Cal-2022' = rnorm(365, 2, 1), 'NCG Cal-2023' = rnorm(365, 2, 1),
                      'AUTVTP Cal-2019' = rnorm(365, 2, 1), 'AUTVTP Cal-2021' = rnorm(365, 2, 1),
                      'AUTVTP Cal-2022' = rnorm(365, 2, 1), 'AUTVTP Cal-2023' = rnorm(365, 2, 1),
                      'ATW Cal-2019' = rnorm(365, 2, 1), 'ATW Cal-2021' = rnorm(365, 2, 1),
                      'ATW Cal-2022' = rnorm(365, 2, 1), 'ATW Cal-2023' = rnorm(365, 2, 1),
                      'BRN Cal-2019' = rnorm(365, 2, 1), 'BRN Cal-2021' = rnorm(365, 2, 1),
                      'BRN Cal-2022' = rnorm(365, 2, 1), 'BRN Cal-2023' = rnorm(365, 2, 1),
                      'FEUA MDEC1' = rnorm(365, 2, 1),
                      check.names = FALSE)
Now I would like to save / extract each occurring column with the date column in its own data table. Ideally, all extracted data tables are then added to a list. I know that I should somehow do this with a for loop, but I can't solve it.
After I have received individual data tables for each product, I would have to do the following for each of the data tables (an example data table is now used here for
AUTVTP Cal-2022
):
DT <- data.table(date = seq(as.Date('2020-01-01'), by = '1 day', length.out = 365),
                 'AUTVTP Cal-2022' = rnorm(365, 2, 1), check.names = FALSE)


DT <- DT %>%
  mutate(month = format(date, '%b'), 
         date = format(date, '%d')) %>%
  tidyr::pivot_wider(names_from = date, values_from = 'AUTVTP Cal-2022') %>%
  relocate(`01`, .after = month)

## Calculate monthly and quarterly mean values: ##
DT <- setDT(DT)[, monthAvg := rowMeans(.SD, na.rm = TRUE), .SDcols = -1]
DT <- DT[, quartAvg := mean(monthAvg), ceiling(seq_len(nrow(DT))/3)]
DT <- DT[, yearAvg := mean(monthAvg), ceiling(seq_len(nrow(DT))/12)]

## Round all values of the data table to 2 digits: ##
DT <- DT %>% mutate_if(is.numeric, round, 2)
HOW CAN I DO THIS?
r
list
data.table
extract
Share
Improve this question
Follow
edited
Oct 28, 2020 at 12:01
markus
26.3k
5
5 gold badges
44
44 silver badges
59
59 bronze badges
asked
Oct 28, 2020 at 11:51
Miko
Miko
496
8
8 silver badges
27
27 bronze badges
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
Reshape to long format, then split.
split(
  melt(dt.data, id.vars = ""date""),
  by = ""variable"", keep.by = FALSE)
You can then use
lapply
to iterate over the list and do whatever your tidyverse code does.
However, generally you shouldn't split a data.table. It's inefficient and often not necessary.
Edit:
I suggest you forget the splitting. Wrap your code in a function like this:
foo <- function(DT, colname) {
  DT <- DT[, c(""date"", colname), with = FALSE]
  DT <- DT %>%
    mutate(month = format(date, '%b'), 
           date = format(date, '%d')) %>%
    tidyr::pivot_wider(names_from = date, values_from = colname) %>%
    relocate(`01`, .after = month)
  
  ## Calculate monthly and quarterly mean values: ##
  DT <- setDT(DT)[, monthAvg := rowMeans(.SD, na.rm = TRUE), .SDcols = -1]
  DT <- DT[, quartAvg := mean(monthAvg), ceiling(seq_len(nrow(DT))/3)]
  DT <- DT[, yearAvg := mean(monthAvg), ceiling(seq_len(nrow(DT))/12)]
  
  ## Round all values of the data table to 2 digits: ##
  DT %>% mutate_if(is.numeric, round, 2)
}
Then, when you need the table for a specific column in your shiny app, you can simply call this function:
foo(dt.data, 'DEB Cal-2019')
If you insist on pre-computing the list:
lapply(names(dt.data)[names(dt.data) != ""date""], 
       foo, DT = dt.data)
Share
Improve this answer
Follow
edited
Oct 28, 2020 at 15:35
answered
Oct 28, 2020 at 11:59
Roland
Roland
132k
12
12 gold badges
198
198 silver badges
298
298 bronze badges
14
When do I use the last part of the code, where I calculate the the monthly, quarterly and yearly means for each data table of the list?
–
Miko
Commented
Oct 28, 2020 at 12:03
You do that before you split. You can use
lapply
within a data.table to iterate over the columns.
–
Roland
Commented
Oct 28, 2020 at 12:05
I'm not really familiar with the
apply
convention? As I need to calculate the monthly, quarterly and yearly means for each small data table, how can I do this before splitting?
–
Miko
Commented
Oct 28, 2020 at 12:09
1
Your example code is of no use to me because it is written in tidyverse. I'm an old school R user who doesn't use that. Your explanation is very unclear. You should not start with splitting the data.table. If you must, you can end with that. I have still not understood what your actual goal is. And please don't explain with the steps you
think
you need.
–
Roland
Commented
Oct 28, 2020 at 13:35
1
See
help(""setNames"")
.
–
Roland
Commented
Oct 29, 2020 at 7:20
|
Show
9
more comments
1
Create a list of dataframes using
split.default
and
cbind
the first column to each list.
lapply(split.default(dt.data[, -1], names(dt.data[, -1])), cbind, dt.data[, 1])
Share
Improve this answer
Follow
answered
Oct 29, 2020 at 4:18
Ronak Shah
Ronak Shah
388k
20
20 gold badges
168
168 silver badges
229
229 bronze badges
2
Thank you for your answer. But then I still have to apply the second part of the code from above to each individual data table in the list (monthly, quarterly, yearly mean of each data table and all data tables transforming in a wide format).
–
Miko
Commented
Oct 29, 2020 at 5:39
1
From your question title and description that is what I get that you want to divide the data.table into two column list of data tables which my answer gives. If you write a function which you want to apply to each individual list, you can apply the function using
lapply
i.e
lapply(result_from_above, fun)
.
–
Ronak Shah
Commented
Oct 29, 2020 at 6:43
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
r
list
data.table
extract
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
Data frame of tables from a list
4
How to convert a list of tables to a data frame in R
2
Converting list of data frames to a data table
2
How to extract all columns when using data.table?
0
Extract data.table as vector in case of many columns
0
convert big data into column list in r
2
Assign/Extract values from list in data.table to columns
1
Turning a column of a data.table into a list in the same data.table
2
Reshape long data table to list of wide data tables
0
create list from columns of data table expression
Hot Network Questions
Find all unique quintuplets in an array that sum to a given target
Should I expect a call from my future boss after signing the offer?
Building a Statistically Sound ML Model
UUID v7 Implementation
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
How should I connect a light fixture with UK wire colors to US wiring?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
Can a hyphen be a ""letter"" in some words?
Time travelling paedo priest novel
Heating object in airless environment
When to start playing the chord when a measure starts with a rest symbol?
How to set individual columns in the siunitx package to boldface? It it a bug?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
Is it possible that the committee contacts only one reference while applicants need to provide two?
Passphrase entropy calculation, Wikipedia version
How could a city build a circular canal?
What are these 16-Century Italian monetary symbols?
Topology of a horocycle
Useful aerial recon vehicles for newly colonized worlds
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
reverse engineering wire protocol
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What's a modern term for sucker or sap?
How to keep meat in a dungeon fresh, preserved, and hot?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
chart.js - Chartjs create chart with big data and fixed labels - Stack Overflow,"chart.js - Chartjs create chart with big data and fixed labels - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Chartjs create chart with big data and fixed labels
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
35 times
0
I want to create chart of type
line
with chatjs. The problem is, I have date formatted:
{ x: real number from 0 to 1, y: integers from 0 to infinite }
, and I want fixed labels to the chart in x-axis, something like:
[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
.
This is how I'd like to set the chart configuration:
type: 'line',
data: {
  labels: ['0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1'],
  datasets: [{
    label: 'data',
    data: [
      { x: 0.3, y: 446 },
      { x: 0.3331, y: 6863 },
      { x: 0.874, y: 12 },
      { x: 0.2244, y: 1565 },
      { x: 0.38899, y: 32221 },
      { x: 0.23685545, y: 3112324 },
      { x: 0.11332, y: 444423 },
      { x: 0.97694, y: 21212334 },
    ],
  }]
},
But this is obviously not working, How can I do it?
chart.js
Share
Improve this question
Follow
asked
Oct 28, 2020 at 17:23
shalvi muqta
shalvi muqta
193
1
1 gold badge
3
3 silver badges
19
19 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
So I found out there is another chart type of
scatter
. This is typically used to only points on axes chart, but it can configured to have lines between the points. Also, if marking the points on graph is unneccessary there is a configuration option to set those points' radius to
0
and this makes those hidden.
https://www.chartjs.org/docs/latest/charts/scatter.html#scatter-chart
Share
Improve this answer
Follow
answered
Oct 28, 2020 at 19:00
shalvi muqta
shalvi muqta
193
1
1 gold badge
3
3 silver badges
19
19 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
chart.js
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
6
Line chart with large number of labels on X axis
3
How to include many datapoints to plot in chart.js
7
Chartjs 2 scaling lots of data points
2
ChartJS unique datasets per label
3
Updating chart.js chart with dataset of different size
4
Different amount of label and data in Chart.js
0
Create data in data with Chartjs
0
Chart.js - combine data and label into a single object
1
How to use chart in JavaScript with large data set
1
create different labels for different data chart js
Hot Network Questions
How to format numbers in monospaced (typewriter) font using siunitx?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Challah dough bread machine recipe issues
Only selecting Features that have another layers feature on top
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Humans try to help aliens deactivate their defensive barrier
Find all unique quintuplets in an array that sum to a given target
How should I connect a light fixture with UK wire colors to US wiring?
Indian music video with over the top CGI
How to delete edges of curve based on their length
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Useful aerial recon vehicles for newly colonized worlds
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
What does it mean when folks say that universe is not ""Locally real""?
How did Jahnke and Emde create their plots
Can two wrongs ever make a right?
A Pandigital Multiplication
What does “going off” mean in ""Going off the age of the statues""?
What's the difference between '\ ' and tilde character (~)?
What should machining (turning, milling, grinding) in space look like
Is there greater explanatory power in laws governing things rather than being descriptive?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"c# - Grpc transfer big data, one unary call is slower than streaming, - Stack Overflow","c# - Grpc transfer big data, one unary call is slower than streaming, - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Grpc transfer big data, one unary call is slower than streaming,
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
1k times
2
I'm trying to transfer a big data between two services using grpc.
The data size is about 23M and is composed by 42 big List.
Then I test the performance using one unary call vs server side streaming(stream one list at a time).
The unary call takes 276.59 ms.
The streaming call takes 126.64 ms.
But if I change the data to contains 1000 small list, each list just have one number, the streaming call is much more slower than unary call.
Is the result normal? And Why?
Here is the server side code:
public override Task<MemDtoToWbs> GetLargeMEM(Empty request, ServerCallContext context)
{
    return Task.FromResult(MemData.GrpcLargeMem);
}
public override async Task StreamLargeMem(Empty request, IServerStreamWriter<LogDtoToWbs> responseStream, ServerCallContext context)
{
    foreach (var log in MemData.GrpcLargeMem.Logs)
    {
         await responseStream.WriteAsync(log);
    }
}
I use the .net core 3.1 and grpc nuget package 2.32.0.
Run test in aks cluster.
Thanks.
c#
.net
grpc
Share
Improve this question
Follow
edited
Oct 18, 2020 at 13:38
李冰涛
asked
Oct 18, 2020 at 13:16
李冰涛
李冰涛
21
3
3 bronze badges
3
Well, that's exactly why when talking about performance you may hear the phrase ""everything is a trade-off"". There's no one best approach, each situation requires doing things in certain ways.
–
Camilo Terevinto
Commented
Oct 18, 2020 at 13:22
Thanks @CamiloTerevinto for the reply. But I want to know what caused such a difference.
–
李冰涛
Commented
Oct 18, 2020 at 13:39
Basically, even though sending messages in streaming calls has very little overhead, when you send lots of small messages (such as in your case), the overhead for sending each message can add up and things can end up slower than if you sent everything in just one bunch (it of course depends how big that bunch is). Basically, streaming call is useful when your use case requires receiving data incrementally in portions (e.g you want to send data as soon as it becomes available), but it doesn't mean that splitting in many small messages is always faster (often it's not).
–
Jan Tattermusch
Commented
Oct 23, 2020 at 17:57
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
I think @Jan Tattermusch is right.
I test in localhost, the TCP segment size is 64K.
When small messages is too small, like 32K, it only have 6.5K payload length in each TCP segment. But if message is big, it can use all the 64K.  So yes the overhead for sending each message can add up and things can end up slower.
So it's reasonable to me that streaming is faster if message is big.
Because sending data in server side and process data in client side is running in parallel.
Share
Improve this answer
Follow
answered
Oct 27, 2020 at 6:29
李冰涛
李冰涛
21
3
3 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
c#
.net
grpc
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
gRPC - request response performance
0
grpc-java bi-di stream performance
3
grpc call time reduce overhead
26
Multiple unary rpc calls vs long-running bidirectional streaming in grpc?
11
Should I transmit large data sets via gRPC without manual chunking?
7
Python GRPC Server performance bottleneck
10
Use gRPC to share very large file
1
gRPC how to stream a huge string
3
gRPC intermittently has high delays
3
gRPC slow serialization on large dataset
Hot Network Questions
When looking at the first DCM page, where is the next DCM page documented?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
White perpetual check, where Black manages a check too?
What should machining (turning, milling, grinding) in space look like
Heating object in airless environment
Is there greater explanatory power in laws governing things rather than being descriptive?
US phone service for long-term travel
Physical interpretation of selection rules for different multipole orders
What does it mean when folks say that universe is not ""Locally real""?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
How to estimate the latency of communication?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Covering a smoke alarm horn
What's a modern term for sucker or sap?
Building a Statistically Sound ML Model
Hole, YHWH and counterfactual present
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Topology of a horocycle
Math contents does not align when subscripts are used
Derailleur Hangar - Fastener torque & thread preparation
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-cs
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
debugging - CoGroupByKey always failed on big data (PythonSDK) - Stack Overflow,"debugging - CoGroupByKey always failed on big data (PythonSDK) - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
CoGroupByKey always failed on big data (PythonSDK)
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
206 times
Part of
Google Cloud
Collective
1
I have about 4000 files (avg ~7MB each) input.
My pipeline always failed on the step CoGroupByKey when the data size reach about 4GB.
I tried to limit only use 300 file then it run just fine.
In case of fail, the logs on GCP dataflow only show:
Workflow failed. Causes: S24:CoGroup Geo data/GroupByKey/Read+CoGroup Geo data/GroupByKey/GroupByWindow+CoGroup Geo data/Map(_merge_tagged_vals_under_key) failed., The job failed because a work item has failed 4 times. Look in previous log entries for the cause of each one of the 4 failures. For more information, see https://cloud.google.com/dataflow/docs/guides/common-errors. The work item was attempted on these workers: 
  store-migration-10212040-aoi4-harness-m7j7
      Root cause: The worker lost contact with the service.,
  store-migration-xxxxx
      Root cause: The worker lost contact with the service.,
  store-migration-xxxxx
      Root cause: The worker lost contact with the service.,
  store-migration-xxxxx
      Root cause: The worker lost contact with the service.
I digging through all logs in Logs Explorer. Nothing else indicate error other than the above, even my
logging.info
and
try...except
code.
Think this relate to the memory of the instances but I didn't digging into that direction. Because it kindna what I don't want to worry about when I am using GCP services.
Thanks.
debugging
google-cloud-dataflow
google-dataflow
Share
Improve this question
Follow
asked
Oct 22, 2020 at 6:56
khiem.nix
khiem.nix
11
1
1 silver badge
1
1 bronze badge
3
that's interesting! Thanks for sharing.
The worker lost contact with the service.
messages are common when the worker is suffering high pressure on memory. Can you share more details about your pipeline, and about the function coming after the CoGBK?
–
Pablo
Commented
Oct 22, 2020 at 18:40
Agree with Pablo, it looks like a memory issue. Do you have hot keys? Have you tried machines with more memory?
–
Iñigo
Commented
Oct 23, 2020 at 23:14
@Pablo I tried
n1-highmem-4
and
-8
and it still crashed. The GroupByKey within it said it have ~15GB mem data, which is less then
-8
and it still crash there.
–
khiem.nix
Commented
Nov 2, 2020 at 4:55
Add a comment
|
Related questions
2
CoGbkResult has more than 10000 elements,reiteration (which may be slow) is required
1
no translator registered for GroupByKey.GroupByKeyOnly
2
Dataflow Batch Job Stuck in GroupByKey.create()
Related questions
2
CoGbkResult has more than 10000 elements,reiteration (which may be slow) is required
1
no translator registered for GroupByKey.GroupByKeyOnly
2
Dataflow Batch Job Stuck in GroupByKey.create()
4
Using CoGroupByKey with custom type ends up in a Coder error
1
Dataflow GCS to BQ Problems
3
Apache Beam GroupByKey() fails when running on Google DataFlow in Python
1
Dataflow GroupByKey and CoGroupByKey is very slow
0
CoGroupByKey not giving desired results Apache Beam(python)
0
Dataflow GroupByKey is very slow for streaming pipeline with small window (preferably 2 seconds)
1
Left join with CoGroupByKey sink to BigQuery using Dataflow
Load 7 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
debugging
google-cloud-dataflow
google-dataflow
or
ask your own question
.
Google Cloud
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
How manage inventory discrepancies due to measurement errors in warehouse management systems
Realization of fundamental group endomorphism
How could a city build a circular canal?
When to start playing the chord when a measure starts with a rest symbol?
Why does it take so long to stop the rotor of a helicopter after landing?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
Physical interpretation of selection rules for different multipole orders
What does “going off” mean in ""Going off the age of the statues""?
How to set individual columns in the siunitx package to boldface? It it a bug?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
How much of a structural/syntactic difference is there between an oath and a promise?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
How to achieve infinite rage?
White perpetual check, where Black manages a check too?
How can we be sure that the effects of gravity travel at most at the speed of light
How to estimate the latency of communication?
Protecting myself against costs for overnight weather-related cancellations
Is there greater explanatory power in laws governing things rather than being descriptive?
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
What does it mean when folks say that universe is not ""Locally real""?
How to explain why I don't have a reference letter from my supervisor
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
r - calculate daily mean of big data table depending on calendar year - Stack Overflow,"r - calculate daily mean of big data table depending on calendar year - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
calculate daily mean of big data table depending on calendar year
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
150 times
Part of
R Language
Collective
1
I get a data table from a server that shows price predictions depending on the selected month of a calendar year. Basically, data is downloaded from every month of the year. Here is an example data table:
set.seed(123)
dt.data <- data.table(Date = seq(as.Date('2020-01-01'), by = '1 day', length.out = 365),
                      'BRN Jan-2021' = rnorm(365, 2, 1), 'BRN Jan-2022' = rnorm(365, 2, 1),
                      'BRN Feb-2021' = rnorm(365, 2, 1), 'BRN Feb-2022' = rnorm(365, 2, 1),
                      'BRN Mar-2021' = rnorm(365, 2, 1), 'BRN Mar-2022' = rnorm(365, 2, 1),
                      'BRN Apr-2021' = rnorm(365, 2, 1), 'BRN Apr-2022' = rnorm(365, 2, 1),
                      'BRN May-2021' = rnorm(365, 2, 1), 'BRN May-2022' = rnorm(365, 2, 1),
                      'BRN Jun-2021' = rnorm(365, 2, 1), 'BRN Jun-2022' = rnorm(365, 2, 1),
                      'BRN Jul-2021' = rnorm(365, 2, 1), 'BRN Jul-2022' = rnorm(365, 2, 1),
                      'BRN Aug-2021' = rnorm(365, 2, 1), 'BRN Aug-2022' = rnorm(365, 2, 1),
                      'BRN Sep-2021' = rnorm(365, 2, 1), 'BRN Sep-2022' = rnorm(365, 2, 1),
                      'BRN Oct-2021' = rnorm(365, 2, 1), 'BRN Oct-2022' = rnorm(365, 2, 1),
                      'BRN Nov-2021' = rnorm(365, 2, 1), 'BRN Nov-2022' = rnorm(365, 2, 1),
                      'BRN Dec-2021' = rnorm(365, 2, 1), 'BRN Dec-2022' = rnorm(365, 2, 1),
                      check.names = FALSE)
This data table is quite small as I only created data for the years 2021 and 2022. But there can be several calendar years, or just one calendar year.
Now I would like to calculate daily mean values (based on the date column) for the year 2021 (i.e. the sum of all 12 values per day / date divided by 12 = number of months per calendar year) and save them in a new data table as a column. And now of course the same for 2022.
In this case, the new data table should have the following columns:
| Date | BRN Cal-2021 | BRN Cal-2022 |
where the date column remains unchanged.
The calculation and the column designation for the new data table should always be variable (depending on how many calendar years appear in
dt.data
). Basically, it might make sense to organize
dt.data
by calendar year at the beginning. But actually I don't really know how to keep the average calculation (daily) variable and general? Or maybe you should create an extra data table for each calendar year, then calculate the mean values and then merge the columns with the daily mean values back into a common data table? However, this should always remain automated (depending on how many calendar years there are). Unfortunately I have no idea how that could be done.
I hope I was able to ask my question accurately enough and someone can help me with my problem.
r
datatable
time-series
mean
Share
Improve this question
Follow
edited
Oct 22, 2020 at 5:24
Miko
asked
Oct 22, 2020 at 5:16
Miko
Miko
496
8
8 silver badges
27
27 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
2
Yes, it would be better to get data in separate columns for each year. We can use
pivot_longer
for that and create new column based on the pattern in the column names. Once we get that we can just take
mean
for each
Date
.
library(dplyr)

dt.data %>%
  tidyr::pivot_longer(cols = -Date, 
               names_to = c('month', '.value'), 
               names_pattern = c('(.*)-(\\d+)')) %>%
  group_by(Date) %>%
  summarise(across(c(matches('^\\d+$')), mean, na.rm  =TRUE))
A base R option without getting the data in long format would be to use
split.default
. We split the data based on year mentioned in the column names and take rowwise mean in each list.
result <- cbind(dt.data[, 1], sapply(split.default(dt.data[, -1], 
      sub('.*-', '', names(dt.data)[-1])), rowMeans, na.rm = TRUE))
names(result)[-1] <- paste0('BRN_Cal-', names(result)[-1])

#           Date BRN_Cal-2021 BRN_Cal-2022
#  1: 2020-01-01     1.974847     2.272833
#  2: 2020-01-02     2.241470     2.399902
#  3: 2020-01-03     1.988883     2.372697
#  4: 2020-01-04     2.057867     2.084504
#  5: 2020-01-05     2.012305     2.049808
# ---                                     
#361: 2020-12-26     2.038167     2.161655
#362: 2020-12-27     2.308974     2.215492
#363: 2020-12-28     2.001359     2.552923
#364: 2020-12-29     2.086283     1.773254
#365: 2020-12-30     1.802871     2.107373
Share
Improve this answer
Follow
edited
Oct 22, 2020 at 5:44
answered
Oct 22, 2020 at 5:30
Ronak Shah
Ronak Shah
388k
20
20 gold badges
168
168 silver badges
229
229 bronze badges
4
I get the error message:
across() must be only used inside dplyr vers.
In addition, it is not automatically recognized how many calendar years there are, because in the last line you specifically refer to 2021 and 2022. As already mentioned above, my data table can also contain other (other) calendar years.
–
Miko
Commented
Oct 22, 2020 at 5:40
The
base R
option works for me, the only thing I need in addition the following new column names ""BRN Cal-2021"" and ""BRN Cal-2022"".
–
Miko
Commented
Oct 22, 2020 at 5:43
You might have
plyr
loaded. Try with
dplyr::summarise
. We can use regex to select specific columns that we want in
across
. I have also added a way to rename the columns.
–
Ronak Shah
Commented
Oct 22, 2020 at 5:49
Thank you so much, I will try this with different calendar years, but I think this will work for me !!
–
Miko
Commented
Oct 22, 2020 at 5:50
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
r
datatable
time-series
mean
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
9
Compute monthly averages from daily data
14
Calculate average monthly total by groups from data.table in R
8
Calculating mean date by row
0
Computing weakly/daily mean in multiple time-series inside data table
0
R: How to calculate year-wise mean and other operations on daily data for elements in a column
2
Calculating monthly averages in R with a large dataset spanning several years
0
R: For a timespan of some years compute the mean of measured values in every calendary month
3
Computing mean of different columns depending on date
1
Mean of last year (halfyear, month) of observations using.data.table
1
Daily mean by date-column in data table R
Hot Network Questions
Find all unique quintuplets in an array that sum to a given target
Hole, YHWH and counterfactual present
A Pandigital Multiplication
Is there greater explanatory power in laws governing things rather than being descriptive?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
Does DOS require partitions to be aligned at a cylinder boundary?
Didactic tool to play with deterministic and nondeterministic finite automata
Covering a smoke alarm horn
how do I correctly check that some aggregated results are correct?
When looking at the first DCM page, where is the next DCM page documented?
What does “going off” mean in ""Going off the age of the statues""?
Passphrase entropy calculation, Wikipedia version
How to remove clear adhesive tape from wooden kitchen cupboards?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
How to explain why I don't have a reference letter from my supervisor
Why is the speed graph of a survey flight a square wave?
How to delete edges of curve based on their length
When to start playing the chord when a measure starts with a rest symbol?
Two types difinition of the distance function
Derailleur Hangar - Fastener torque & thread preparation
Longest bitonic subarray
Should I expect a call from my future boss after signing the offer?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
sql - string_agg is to slow with big data and i need a faster solution - Stack Overflow,"sql - string_agg is to slow with big data and i need a faster solution - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
string_agg is to slow with big data and i need a faster solution
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
2k times
0
I am working with contacting a string from a result and it is taking to long to execute
this is the solution I have:
declare @damagedParties table (caseid varchar(max),FirstName varchar(max),LastName varchar(max),damagedName varchar(max))

insert @damagedParties
select t.caseid,ped.FirstName as FirstName,ped.LastName,concat(ped.FirstName,' ',ped.LastName) as damagedName
from [Case] t
inner join [KCC].[dbo].[Party] p1 on p1.CaseId = t.caseid 
LEFT JOIN Person ped ON ped.PersonOrBusinessId = ped.PersonOrBusinessId and p1.PartyTypeRefData = 'kpcparty$PARTY_INJUREDPARTY_F'

select string_agg(d.damagedName,', ')
from @damagedParties d
group by d.caseid
sql
sql-server
Share
Improve this question
Follow
edited
Oct 21, 2020 at 7:51
jarlh
44.6k
8
8 gold badges
50
50 silver badges
67
67 bronze badges
asked
Oct 21, 2020 at 7:34
endrit sheholli
endrit sheholli
11
5
5 bronze badges
6
Which dbms are you using? (That code is product specific.)
–
jarlh
Commented
Oct 21, 2020 at 7:48
I am using an SQL server
–
endrit sheholli
Commented
Oct 21, 2020 at 7:50
How many rows in @damagedParties? Which part of query is slow - inserting into @damagedParties or string aggregating?
–
Arvo
Commented
Oct 21, 2020 at 7:54
every table is big and the string_agg is slowing the query execution
–
endrit sheholli
Commented
Oct 21, 2020 at 7:58
Try using For Xml Path clause, my friend.
–
Tomato32
Commented
Oct 21, 2020 at 8:23
|
Show
1
more comment
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
The LEFT JOIN condition in the first query joins the Person table to itself on PersonOrBusinessId.  Presumably, the JOIN should be from Person table to the Party table?  Something like this
insert @damagedParties
select t.caseid, ped.FirstName, ped.LastName,
       concat(ped.FirstName,' ',ped.LastName) as damagedName
from [Case] t
     join [KCC].[dbo].[Party] p1 on p1.CaseId = t.caseid 
     left join Person ped ON p1.PersonOrBusinessId = ped.PersonOrBusinessId
where p1.PartyTypeRefData = 'kpcparty$PARTY_INJUREDPARTY_F';
Share
Improve this answer
Follow
answered
Oct 21, 2020 at 12:14
SteveC
SteveC
6,005
2
2 gold badges
13
13 silver badges
25
25 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
sql
sql-server
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
0
Faster SQL query for over 2 million rows
0
Speed up SQL query billions of rows
0
T-SQL Poor performance when concatenating a large string
0
Looking for an efficient way of emulating LISTAGG(STRING_AGG) on Microsoft Analytics Platform System
1
Performance improvement of SQL Server table with millions of records
3
Alternative to STRING_AGG in with SQL
0
How to return from STRING_AGG to regular table?
0
SQL with string concatenation is slow
2
Improve the speed of this string_agg?
0
Sql server: limit string_agg result
Hot Network Questions
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
Useful aerial recon vehicles for newly colonized worlds
Why is the speed graph of a survey flight a square wave?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Longest bitonic subarray
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Why does it take so long to stop the rotor of a helicopter after landing?
How to achieve infinite rage?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
Covering a smoke alarm horn
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
When looking at the first DCM page, where is the next DCM page documented?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Time travelling paedo priest novel
Challah dough bread machine recipe issues
Heating object in airless environment
Building a Statistically Sound ML Model
reverse engineering wire protocol
How to use a symbol as both function and head?
Is it possible that the committee contacts only one reference while applicants need to provide two?
Passphrase entropy calculation, Wikipedia version
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Humans try to help aliens deactivate their defensive barrier
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
swift - Share data between main App and Widget in SwiftUI - Stack Overflow,"swift - Share data between main App and Widget in SwiftUI - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Share data between main App and Widget in SwiftUI
Ask Question
Asked
4 years, 2 months ago
Modified
7 months ago
Viewed
21k times
32
@main
struct ClockWidgetExt: Widget {
    private let kind: String = ""ClockWidgetExt""
    
    public var body: some WidgetConfiguration {
        StaticConfiguration(kind: kind, provider: Provider(), placeholder: PlaceholderView()) { entry in
            HomeTestView()
        }
        .configurationDisplayName(""My Widget"")
        .description(""This is an example widget."")
    }
}
How can I get data from my Main App to the widget?
swift
swiftui
widget
ios14
widgetkit
Share
Improve this question
Follow
edited
May 12 at 16:04
pawello2222
54k
23
23 gold badges
178
178 silver badges
237
237 bronze badges
asked
Sep 16, 2020 at 14:17
codingManMAx
codingManMAx
349
1
1 gold badge
3
3 silver badges
4
4 bronze badges
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
67
You can add the
AppGroup
capability for both your Widget and App (
here
is a very good explanation how to add it).
UserDefaults
Instead of
UserDefaults.standard
just use the shared
UserDefaults
for your AppGroup:
UserDefaults(suiteName: <your_app_group>)
Then you can read/write data like explained in
this answer
.
File Container
With the AppGroup entitlement you get access to the shared File Container:
let containerURL = FileManager.default.containerURL(forSecurityApplicationGroupIdentifier: <your_app_group>)!
and access an url like this:
let someFileURL = containerURL.appendingPathComponent(""SomeFile.txt"")
Then you can use your shared File Container like explained in this answer:
How to read files created by the app by iOS WidgetKit?
CoreData
You can create a shared CoreData container as well:
let storeURL = containerURL.appendingPathComponent(""DataModel.sqlite"")
let description = NSPersistentStoreDescription(url: storeURL)

let container = NSPersistentContainer(name: ""DataModel"")
container.persistentStoreDescriptions = [description]
container.loadPersistentStores { ... }
Then you can use your shared CoreData Container like explained in this answer:
Fetch data from CoreData for iOS 14 widget
Here is a
GitHub repository
with different Widget examples including the App Group Widget.
Share
Improve this answer
Follow
edited
Oct 15, 2020 at 20:03
answered
Sep 16, 2020 at 17:20
pawello2222
pawello2222
54k
23
23 gold badges
178
178 silver badges
237
237 bronze badges
2
Can you please tell me where to put that code (shared CoreData container). If you can, please share a code snippet which shows the fetching part in a widget.
–
umayanga
Commented
Sep 17, 2020 at 11:49
1
@umayanga There is really nothing Widget-specific here. You create a Persistent Container in the same way as in a standard App (you just explicitly specify the location of the sqlite file). The same applies to the fetching part. You access a context from a persistent container and perform a fetch request with it. But as this still might be confusing I added a more detailed explanation under your post.
–
pawello2222
Commented
Sep 17, 2020 at 20:18
Add a comment
|
3
One simple way to do this is by adding both the app and the widget to the same App Group, and then storing and retrieving data from UserDefaults storage located in that App Group's container instead of the default UserDefaults storage for the app.
You can access this shared storage for your App Group by initializing UserDefaults using
UserDefaults(suiteName: ""YOUR_APP_GROUP_NAME"")
instead of accessing it using
UserDefaults.standard
.
This article
gives a more thorough description of the details of this process.
Share
Improve this answer
Follow
answered
Feb 5, 2021 at 0:20
smapi_guru
smapi_guru
59
1
1 bronze badge
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
swift
swiftui
widget
ios14
widgetkit
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
1
Pass big data like images to widget?
19
Fetch data from CoreData for iOS 14 widget
9
Why don't interactive buttons in iOS 17 widget call AppIntent perform() when the app running?
8
How to read files created by the app by iOS WidgetKit?
5
Sharing Data with AppGroup
6
UIKit UserDefaults in SwiftUI widget
7
Communication between iOS app & Widgets for iOS 14
6
What is the reasonable way to display data from Core Data in iOS 14 widget?
4
iOS 17 Beta Shared AVAudioPlayer State Between App and Widget
3
Store an array on the app group for WidgetKit
See more linked questions
Related
3
Transfer data from project to widget in swift
8
Sharing UserDefaults between main app and Widget in iOS 14
12
Need help to make iOS 14 Widget show content from from REST api
7
Communication between iOS app & Widgets for iOS 14
5
Sharing Data with AppGroup
13
How to refresh Widget when Main App is used?
1
How to pass uiviewcontroller data to swiftui widget class
3
How to get different data for each widget
1
Using CoreData as dynamic Widget Intents in SwiftUI
2
How to share Core Data with Widget?
Hot Network Questions
Does an NEC load calculation overage mandate a service upgrade?
Didactic tool to play with deterministic and nondeterministic finite automata
How could a city build a circular canal?
Physical interpretation of selection rules for different multipole orders
How *exactly* is divisibility defined?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Challah dough bread machine recipe issues
Useful aerial recon vehicles for newly colonized worlds
When was ""to list"" meaning ""to wish"" lost?
Is it possible that the committee contacts only one reference while applicants need to provide two?
how do I correctly check that some aggregated results are correct?
Find a fraction's parent in the Stern-Brocot tree
US phone service for long-term travel
reverse engineering wire protocol
Building a Statistically Sound ML Model
How to format numbers in monospaced (typewriter) font using siunitx?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
What does it mean when folks say that universe is not ""Locally real""?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
What technique is used for the heads in this LEGO Halo Elite MOC?
Is outer space Radioactive?
Does DOS require partitions to be aligned at a cylinder boundary?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-swift
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
mysql - Any way to do this query faster with big data - Stack Overflow,"mysql - Any way to do this query faster with big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Any way to do this query faster with big data
Ask Question
Asked
4 years, 2 months ago
Modified
4 years, 1 month ago
Viewed
61 times
0
This query takes around 2.23seconds and feels a bit slow ... is there anyway to make it faster.
our member.id, member_id, membership_id, valid_to, valid_from has index as well.
select * 
from member 
where (member.id in ( select member_id from member_membership mm 
INNER JOIN membership m ON mm.membership_id = m.id 
where instr(organization_chain, 2513) and m.valid_to > NOW() and m.valid_from < NOW() ) ) 
order by id desc 
limit 10 offset 0
EXPLAIN FOR WHAT QUERY DOING: every member has many a member_memberships and and member_memberships connect with another table called membership there we have the membership details. so query will get all members that has valid memberships and where the organization id 2513 exist on member_membership.
Tables as following:
CREATE TABLE `member` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `first_name` varchar(255) DEFAULT NULL,
  `last_name` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=latin1;

CREATE TABLE `member_membership` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `membership_id` int(11) DEFAULT NULL,
  `member_id` int(11) DEFAULT NULL,
  `organization_chain` text DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `member_membership_to_membership` (`membership_id`),
  KEY `member_membership_to_member` (`member_id`)
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=latin1;

CREATE TABLE `membership` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) DEFAULT NULL,
  `valid_to` datetime DEFAULT NULL,
  `valid_from` datetime DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `valid_to` (`valid_to`),
  KEY `valid_from` (`valid_from`),
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=latin1;

ALTER TABLE `member_membership` ADD CONSTRAINT `member_membership_to_membership` FOREIGN KEY (`membership_id`) REFERENCES `membership` (`id`);

ALTER TABLE `member_membership` ADD CONSTRAINT `member_membership_to_member` FOREIGN KEY (`member_id`) REFERENCES `member` (`id`);
Here with EXPLAIN statement =>
https://i.ibb.co/xjrcYWR/EXPLAIN.png
Relations
member has many member_membership
membership has manymember_membership
So member_membership is like join for tables member and membership.
mysql
mariadb
Share
Improve this question
Follow
edited
Oct 14, 2020 at 19:40
Mohammed
asked
Oct 14, 2020 at 18:10
Mohammed
Mohammed
33
7
7 bronze badges
14
all optimize questions need a EXPLAIN your query and you must show tables and where their indexes are
–
nbk
Commented
Oct 14, 2020 at 18:17
I added EXPLAIN and tables too!
–
Mohammed
Commented
Oct 14, 2020 at 19:03
this is not good, images doesn't help and i can't see which indexes you have. and a explain should result in such a table
db-fiddle.com/f/4yPorU6k3SjQ5nmhgi1wGo/42
–
nbk
Commented
Oct 14, 2020 at 19:14
The indexes is there below each table!. I dont understand what is missing!
–
Mohammed
Commented
Oct 14, 2020 at 19:19
1
We disagree. It is general policy to not post images of what is essentially text. Most helpful would be to post the SQL statements to set up tables and data. That will increase the probability of getting a useful answer considerably.
–
trincot
Commented
Oct 14, 2020 at 19:27
|
Show
9
more comments
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
Well I found a way to make it less to 800ms ... like this. Is this good way or maybe there is more we can do?
select * 
from member  
where (member.id in ( select member_id from member_membership mm FORCE INDEX (PRIMARY)
INNER JOIN membership m ON mm.membership_id = m.id  
where instr(organization_chain, 2513) and m.valid_to > NOW() and m.valid_from < NOW() ) ) 
order by id desc 
limit 10 offset 0
NEW UPDATE.. and I think this solve the issue.. 15ms :)
I added FORCE INDEX..
The FORCE INDEX hint acts like USE INDEX (index_list), with the addition that a table scan is assumed to be very expensive. In other words, a table scan is used only if there is no way to use one of the named indexes to find rows in the table.
select * 
from member  
where (member.id in ( select member_id from member_membership mm FORCE INDEX (member_membership_to_member)
INNER JOIN membership m FORCE INDEX (organization_to_membership) ON mm.membership_id = m.id  
where instr(organization_chain, 2513) and m.valid_to > NOW() and m.valid_from < NOW() ) ) 
order by id desc 
limit 10 offset 0
Share
Improve this answer
Follow
edited
Oct 17, 2020 at 13:31
answered
Oct 14, 2020 at 20:53
Mohammed
Mohammed
33
7
7 bronze badges
1
If you would explain why it is faster and what you have optimized this could become a great learning resource.
–
jschnasse
Commented
Oct 15, 2020 at 6:34
Add a comment
|
0
How big is
organization_chain
?  If you don't need
TEXT
, use a reasonably sized
VARCHAR
so that it could be in an index.  Better yet, is there some way to get 2513 in a column by itself?
Don't use
id int(11) NOT NULL AUTO_INCREMENT,
in a many-to-many table; rather have the two columns in
PRIMARY KEY
.
Put the
ORDER BY
and
LIMIT
in the subquery.
Don't use
IN ( SELECT ...)
, use a
JOIN
.
Share
Improve this answer
Follow
answered
Oct 15, 2020 at 6:11
Rick James
Rick James
142k
14
14 gold badges
138
138 silver badges
243
243 bronze badges
3
organization_chain does not affact if its text or varchar. the big issue comes now when the data get bigger more and more.. so like 1million rows takes 15ms.. but when its 2million rows its 50ms .. thats another issue.
–
Mohammed
Commented
Oct 17, 2020 at 12:35
@Mohammed - A
TEXT
column forces a temp table to be on disk instead of in RAM -- this slows down the
ORDER BY
. Pulling the 2513 out, if possible, could lead to an index that would help
significantly
.
–
Rick James
Commented
Oct 17, 2020 at 15:45
Thank u Rick James for this info :) will check that and change it to varchar!
–
Mohammed
Commented
Oct 18, 2020 at 11:59
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
mysql
mariadb
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
Improve Query Performance of This SQL
0
Writing faster select queries on large databases
0
Optimize Query on mysql
0
How improve query speed with bigdata ? Mysql
1
How to optimize my mysql request to work with large database
0
SQL: Optimize the query on large table with indexing
2
How to optimize the following SELECT query
0
MySQL SELECT Query on Only 300k Rows Running Very Slow
0
Improving slow MariaDB query performance
0
Historical big data slow queries
Hot Network Questions
How *exactly* is divisibility defined?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Longest bitonic subarray
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
What should machining (turning, milling, grinding) in space look like
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Time travelling paedo priest novel
Heating object in airless environment
When looking at the first DCM page, where is the next DCM page documented?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Didactic tool to play with deterministic and nondeterministic finite automata
How can we be sure that the effects of gravity travel at most at the speed of light
Find a fraction's parent in the Stern-Brocot tree
How could a city build a circular canal?
Does DOS require partitions to be aligned at a cylinder boundary?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
Building a Statistically Sound ML Model
Two types difinition of the distance function
suspected stars and bars problem considered incorrect, in need for some further insight
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
r - How to create a scatter plot of a really big data - Stack Overflow,"r - How to create a scatter plot of a really big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to create a scatter plot of a really big data
Ask Question
Asked
4 years, 2 months ago
Modified
4 years, 2 months ago
Viewed
456 times
Part of
R Language
Collective
0
I have a really big data of size more than 3 gb (only X and Y variables). The data frame
IBD
has 300 million rows. Is there a better and faster way to plot this?
First, I read the dataframe:
IBD <- fread(""/40/AD/LL_Cohorts_MERGED-IBD.genome"", select = c(""X"", ""Y""))
and tried to plot, but it's been over 12 hours and I am not getting any output.
ggplot(IBD, aes(x=X, y=Y))+ 
  geom_point() + 
  ggtitle(""ADGC EOAD"") + 
  scale_x_continuous(limits=c(0,1)) + 
  scale_y_continuous(limits=c(0,1))
r
scatter-plot
Share
Improve this question
Follow
edited
Oct 13, 2020 at 14:20
Yamuna_dhungana
asked
Oct 13, 2020 at 14:15
Yamuna_dhungana
Yamuna_dhungana
663
5
5 silver badges
12
12 bronze badges
2
Calculate the clusters and use scatter plot for the centroids. Otherwise, I think you will be facing with a big full rectangular without any space. See
this
.
–
maydin
Commented
Oct 13, 2020 at 14:18
I tried with 1/3rd of the data and it gives me what I expect. So If I can plot it, I think it should be fne. I just need to see the outliers.
–
Yamuna_dhungana
Commented
Oct 13, 2020 at 14:22
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
One way to get the run time down is to play with smaller datasets and different code to see which would be faster.
Then you can use
system.time()
to see how long something takes and compare:
Measuring function execution time in R
For example:
size<-100000
IBD<-data.frame(X=rbeta(n = size,shape1=2,shape2 = 2),Y=rbeta(n = size,shape1=2,shape2 = 2))
Using your code on this fake dataset:
system.time(
ggplot(IBD, aes(x=X, y=Y))+ geom_point() + ggtitle(""ADGC EOAD"") + scale_x_continuous(limits=c(0,1)) + scale_y_continuous(limits=c(0,1))
)

   user  system elapsed 
   0.01    0.00    0.01
Using base
plot
as a comparison point:
system.time(
plot(Y~X, data=IBD)
)

   user  system elapsed 
   2.13    2.34    4.56
You can see that
plot
takes a lot longer. I realize this isn't a solution to making your code faster, but it is a tool that you can use to figure out what would be faster on such a large dataset.
Edit:
Adding in the methods from comments by @maydin:
cluster<-kmeans(x = IBD, centers = 1000)
Clus<-data.frame(cluster$centers)

system.time(
  ggplot(Clus, aes(x=X, y=Y))+ geom_point() + ggtitle(""ADGC EOAD"") + scale_x_continuous(limits=c(0,1)) + scale_y_continuous(limits=c(0,1))
)

   user  system elapsed 
      0       0       0
Share
Improve this answer
Follow
edited
Oct 13, 2020 at 14:47
answered
Oct 13, 2020 at 14:26
Dylan_Gomes
Dylan_Gomes
2,222
16
16 silver badges
36
36 bronze badges
2
Well, kmeans is also going to take time.
–
Yamuna_dhungana
Commented
Oct 13, 2020 at 15:25
absolutely it is. You can wrap that within
system.time()
to see if it would take longer or shorter.
–
Dylan_Gomes
Commented
Oct 13, 2020 at 18:15
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
r
scatter-plot
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
361
Measuring function execution time in R
Related
30
Plotting of very large data sets in R
149
Scatterplot with too many points
5
I have imported the data in R, how to make a scatter plot?
1
Graphing Scatter Plots in R
1
R: How to visualize large and clumped scatter plot
1
plotting pairwise scatter plot for high throughput data
3
Scatter plot in R with large overlap and 3000+ points
0
How to create a scatterplot with 1000 rows of data, indexed/plotted by one variable, with R?
2
plotting a scatter plot with wide range data R
0
scatter plot in R of many data 10 or more
Hot Network Questions
Time travelling paedo priest novel
Does DOS require partitions to be aligned at a cylinder boundary?
reverse engineering wire protocol
Why is the speed graph of a survey flight a square wave?
US phone service for long-term travel
Why does it take so long to stop the rotor of a helicopter after landing?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
How to explain why I don't have a reference letter from my supervisor
suspected stars and bars problem considered incorrect, in need for some further insight
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
How *exactly* is divisibility defined?
Useful aerial recon vehicles for newly colonized worlds
Find a fraction's parent in the Stern-Brocot tree
Does an NEC load calculation overage mandate a service upgrade?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How did Jahnke and Emde create their plots
Didactic tool to play with deterministic and nondeterministic finite automata
Convert pipe delimited column data to HTML table format for email
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
How could a city build a circular canal?
Passphrase entropy calculation, Wikipedia version
When to use cards for communicating dietary restrictions in Japan
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"python - Iterating through big data with pandas, large and small dataframes - Stack Overflow","python - Iterating through big data with pandas, large and small dataframes - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Iterating through big data with pandas, large and small dataframes
Ask Question
Asked
4 years, 2 months ago
Modified
4 years, 2 months ago
Viewed
50 times
-1
This is my first post here and it’s based upon an issue I’ve created and tried to solve at work. I’ll try to precisely summarize my issue as I’m having trouble wrapping my head around a preferred solution. #3 is a real stumper for me.
Grab a large data file based on a parquet - no problem
Select 5 columns from the parquet and create a dataframe - no problem
import pandas
df = pd.read_parquet(’/Users/marmicha/Downloads/sample.parquet’,
columns=[""ts"", ""session_id"", ""event"", ""duration"", ""sample_data""])
But here is where it gets a bit tricky for me.      One column(a key column) is  called ""session_id"" .     Many values are unique.     Many duplicate values(of session_id) exist and have multiple associated entry rows of data.     I wish to iterate through the master dataframe, create a unique dataframe per session_id.       Each of these unique (sub) dataframes would have a calculation done that simply gets the SUM of the ""duration"" column per session_id.   Again that SUM would be unique per unique session_id, so each sub dataframe would have it's own SUM with a row added with that total listed along with the session_id      I'm thinking there is a nested loop formula that will work for me but every effort has been a mess to date.
Ultimately, I'd like to have a final dataframe that is a collection of these unique sub dataframes.     I guess I'd need to define this final dataframe, and append it with each new sub dataframe as I iterate through the data.     I should be able to do that simply
Finally, write this final df to a new parquet file.     Should be simple enough so I won't need help with that.
But that is my challenge in a nutshell. The main design I’d need help with is #3. I’ve played with interuples and iterows
python
pandas
dataframe
Share
Improve this question
Follow
edited
Oct 9, 2020 at 5:19
Kyle Hardinge
95
10
10 bronze badges
asked
Oct 8, 2020 at 22:12
Mark Michaelsen
Mark Michaelsen
1
2
Do you need all the small files for anything else? Or do you only need the summary file? Seems like a lot of extra work for no reason
–
noah
Commented
Oct 8, 2020 at 22:17
Do you basically just need a summary of the total duration for each session ID?
–
noah
Commented
Oct 8, 2020 at 22:18
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
2
I think the
groupby
function will work:
df.groupby('session_id')['duration'].sum()
More info here:
https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html
Share
Improve this answer
Follow
answered
Oct 8, 2020 at 22:22
jsmart
jsmart
3,001
1
1 gold badge
8
8 silver badges
14
14 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
pandas
dataframe
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
0
Iterating over DataFrames
1
Efficient way to iterate through a large dataframe
1
Python pandas iterate through dataframe
0
Pandas and Large dataframe
3
Pandas - make a large DataFrame into several small DataFrames and run each through a function
2
Splitting large dataframe into list of smaller pandas dataframes
2
Iterating over very large dataframe efficiency in python pandas is too time consuming
2
How to avoid for loop in a Pandas DataFrame for large dataset
1
How do I iterate through two dataframes of different sizes?
0
How to iterate over very big dataframes in python?
Hot Network Questions
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Topology of a horocycle
White perpetual check, where Black manages a check too?
Challah dough bread machine recipe issues
How to keep meat in a dungeon fresh, preserved, and hot?
Math contents does not align when subscripts are used
When looking at the first DCM page, where is the next DCM page documented?
How to set individual columns in the siunitx package to boldface? It it a bug?
Realization of fundamental group endomorphism
Passphrase entropy calculation, Wikipedia version
When was ""to list"" meaning ""to wish"" lost?
Does DOS require partitions to be aligned at a cylinder boundary?
Why did Crimea’s parliament agree to join Ukraine?
Useful aerial recon vehicles for newly colonized worlds
Only selecting Features that have another layers feature on top
Is outer space Radioactive?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
What does it mean when folks say that universe is not ""Locally real""?
How to remove clear adhesive tape from wooden kitchen cupboards?
Indian music video with over the top CGI
Derailleur Hangar - Fastener torque & thread preparation
Heating object in airless environment
suspected stars and bars problem considered incorrect, in need for some further insight
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
dplyr - What are helpful optimizations in R for big data sets? - Stack Overflow,"dplyr - What are helpful optimizations in R for big data sets? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
What are helpful optimizations in R for big data sets?
Ask Question
Asked
4 years, 3 months ago
Modified
4 years, 2 months ago
Viewed
2k times
Part of
R Language
Collective
16
I built a script that works great with small data sets (<1 M rows) and performs very poorly with large datasets. I've heard of data table as being more performant than tibbles. I'm interested to know about other speed optimizations in addition to learn about data tables.
I'll share a couple of commands in the script for examples. In each of the examples, the datasets are 10 to 15 million rows and 10 to 15 columns.
Getting the lowest date for a dataframe grouped by nine variables
dataframe %>% 
      group_by(key_a, key_b, key_c,
               key_d, key_e, key_f,
               key_g, key_h, key_i) %>%
      summarize(min_date = min(date)) %>% 
      ungroup()
Doing a left join on two dataframes to add an additional column
merge(dataframe, 
          dataframe_two, 
          by = c(""key_a"", ""key_b"", ""key_c"",
               ""key_d"", ""key_e"", ""key_f"",
               ""key_g"", ""key_h"", ""key_i""),
          all.x = T) %>% 
      as_tibble()
Joining two dataframes on the
closest date
dataframe %>%
      left_join(dataframe_two, 
                  by = ""key_a"") %>%
      group_by(key_a, date.x) %>%
      summarise(key_z = key_z[which.min(abs(date.x - date.y))]) %>%
      arrange(date.x) %>%
      rename(day = date.x)
What best practices can I apply and, in particular, what can I do to make these types of functions optimized for large datasets?
--
This is an example dataset
set.seed(1010)
library(""conflicted"")
conflict_prefer(""days"", ""lubridate"")
bigint <- rep(
  sample(1238794320934:19082323109, 1*10^7)
)

key_a <-
  rep(c(""green"", ""blue"", ""orange""), 1*10^7/2)

key_b <-
  rep(c(""yellow"", ""purple"", ""red""), 1*10^7/2)

key_c <-
  rep(c(""hazel"", ""pink"", ""lilac""), 1*10^7/2)

key_d <-
  rep(c(""A"", ""B"", ""C""), 1*10^7/2)

key_e <-
  rep(c(""D"", ""E"", ""F"", ""G"", ""H"", ""I""), 1*10^7/5)

key_f <-
  rep(c(""Z"", ""M"", ""Q"", ""T"", ""X"", ""B""), 1*10^7/5)

key_g <-
  rep(c(""Z"", ""M"", ""Q"", ""T"", ""X"", ""B""), 1*10^7/5)

key_h <-
  rep(c(""tree"", ""plant"", ""animal"", ""forest""), 1*10^7/3)

key_i <-
  rep(c(""up"", ""up"", ""left"", ""left"", ""right"", ""right""), 1*10^7/5)

sequence <- 
  seq(ymd(""2010-01-01""), ymd(""2020-01-01""), by = ""1 day"")

date_sequence <-
  rep(sequence, 1*10^7/(length(sequence) - 1))

dataframe <-
  data.frame(
    bigint,
    date = date_sequence[1:(1*10^7)],
    key_a = key_a[1:(1*10^7)],
    key_b = key_b[1:(1*10^7)],
    key_c = key_c[1:(1*10^7)],
    key_d = key_d[1:(1*10^7)],
    key_e = key_e[1:(1*10^7)],
    key_f = key_f[1:(1*10^7)],
    key_g = key_g[1:(1*10^7)],
    key_h = key_h[1:(1*10^7)],
    key_i = key_i[1:(1*10^7)]
  )

dataframe_two <-
  dataframe %>%
      mutate(date_sequence = ymd(date_sequence) + days(1))

sequence_sixdays <-
  seq(ymd(""2010-01-01""), ymd(""2020-01-01""), by = ""6 days"")

date_sequence <-
  rep(sequence_sixdays, 3*10^6/(length(sequence_sixdays) - 1))

key_z <-
  sample(1:10000000, 3*10^6)

dataframe_three <-
  data.frame(
    key_a = sample(key_a, 3*10^6),
    date = date_sequence[1:(3*10^6)],
    key_z = key_z[1:(3*10^6)]
  )
r
dplyr
data.table
tidyverse
Share
Improve this question
Follow
edited
Sep 13, 2020 at 17:40
Cauder
asked
Sep 7, 2020 at 9:01
Cauder
Cauder
2,627
7
7 gold badges
42
42 silver badges
92
92 bronze badges
8
2
data.table could be
well suited to your needs
. Could you supply a script to create fake data to test this with
microbenchmark
?
–
Waldi
Commented
Sep 7, 2020 at 9:24
1
Check out the
tidyft::parse_fst
where you read fst file.
–
akrun
Commented
Sep 7, 2020 at 22:27
2
Yes, but the difference seems to be not so big:
iyarlin.github.io/2020/05/26/dtplyr_benchmarks
As this link explains, you can do it even faster by imposing
dataframe
to be a
data.table
–
iago
Commented
Sep 9, 2020 at 14:02
3
This list by Dirk Eddelbuettel has lots of tools for working with large datasets
cran.r-project.org/web/views/HighPerformanceComputing.html
–
Cauder
Commented
Sep 9, 2020 at 14:18
2
You should including loading
lubridate
package in your example, it uses
ymd
function. Question in general could be improved to be fully reproducible, that would be useful for answers to provide working code.
–
jangorecki
Commented
Sep 9, 2020 at 15:41
|
Show
3
more comments
3 Answers
3
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
11
+100
What best practices can I apply and, in particular, what can I do to make these types of functions optimized for large datasets?
use
data.table
package
library(data.table)
d1 = as.data.table(dataframe)
d2 = as.data.table(dataframe_two)
1
grouping by many columns is something that data.table is excellent at
see barchart at the very bottom of the second plot for comparison against dplyr spark and others for exactly this kind of grouping
https://h2oai.github.io/db-benchmark
by_cols = paste(""key"", c(""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i""), sep=""_"")
a1 = d1[, .(min_date = min(date_sequence)), by=by_cols]
note I changed
date
to
date_sequence
, I think you meant that as a column name
2
it is unclear on what fields you want to merge tables, dataframe_two does not have specified fields so the query is invalid
please clarify
3
data.table has very useful type of join called rolling join, which does exactly what you need
a3 = d2[d1, on=c(""key_a"",""date_sequence""), roll=""nearest""]
# Error in vecseq(f__, len__, if (allow.cartesian || notjoin || #!anyDuplicated(f__,  : 
#  Join results in more than 2^31 rows (internal vecseq reached #physical limit). Very likely misspecified join. Check for #duplicate key values in i each of which join to the same group in #x over and over again. If that's ok, try by=.EACHI to run j for #each group to avoid the large allocation. Otherwise, please search #for this error message in the FAQ, Wiki, Stack Overflow and #data.table issue tracker for advice.
It results an error. Error is in fact very useful. On your real data it may work perfectly fine, as the reason behind the error (cardinality of matching rows) may be related to process of generating sample data. It is very tricky to have good dummy data for joining.
If you are getting the same error on your real data you may want to review design of that query as it attempts to make row explosion by doing many-to-many join. Even after already considering only single
date_sequence
identity (taking
roll
into account). I don't see this kind of question to be valid for that data (cadrinalities of join fields strictly speaking). You may want to introduce data quality checks layer in your workflow to ensure there are no duplicates on
key_a
and
date_sequence
combined.
Share
Improve this answer
Follow
edited
Sep 9, 2020 at 15:52
answered
Sep 9, 2020 at 15:36
jangorecki
jangorecki
16.7k
5
5 gold badges
83
83 silver badges
167
167 bronze badges
7
2
This is a great article about how rolling joins work
gormanalysis.com/blog/r-data-table-rolling-joins
–
Cauder
Commented
Sep 9, 2020 at 15:45
point 2 still needs clarification. Your
merge
call specify columns to join on that don't exist in both tables which is an invalid usage. If I would know on which columns you want to merge those tables I could try to provide ready-to-use code.
–
jangorecki
Commented
Sep 13, 2020 at 17:25
Can you also mention that values of setting keys in joins with data tables?
–
Cauder
Commented
Sep 13, 2020 at 17:36
Unfortunately I am trying to reproduce your code and getting
dataframe_two <- +   dataframe %>% +       mutate(date_sequence = date_sequence + days(1)) ✖ Input
date_sequence` can't be recycled to size 10000000. ℹ Input
date_sequence
is
date_sequence + days(1)
. ℹ Input
date_sequence
must be size 10000000 or 1, not 10001914. Run
rlang::last_error()
to see where the error occurred.`
–
jangorecki
Commented
Sep 13, 2020 at 17:38
I'll use dput maybe that'll be easier. Can you please try again with library(conflicted) and conflict_prefer(""days"", ""lubridate""). data tables and lubridate both have a function called
days
and can you wrap date_sequence with ymd(). I'll update the description
–
Cauder
Commented
Sep 13, 2020 at 17:38
|
Show
2
more comments
5
Expanding on @jangorecki's answer.
Data:
library(lubridate)
library(dplyr)
library(conflicted)
library(data.table)

dataframe = data.frame(bigint,
    date_sequence = date_sequence[1:(1*10^7)],
    key_a = key_a[1:(1*10^7)],
    key_b = key_b[1:(1*10^7)],
    key_c = key_c[1:(1*10^7)],
    key_d = key_d[1:(1*10^7)],
    key_e = key_e[1:(1*10^7)],
    key_f = key_f[1:(1*10^7)],
    key_g = key_g[1:(1*10^7)],
    key_h = key_h[1:(1*10^7)],
    key_i = key_i[1:(1*10^7)])

dataframe_two = dataframe %>% mutate(date_sequence1 = ymd(date_sequence) + days(1))

dataframe_two$date_sequence = NULL
Benchmarks:
1.
dplyr result of 2 runs: 2.2639 secs; 2.2205 secs
st = Sys.time()
a1 = dataframe %>% 
  group_by(key_a, key_b, key_c,
           key_d, key_e, key_f,
           key_g, key_h, key_i) %>%
  summarize(min_date = min(date_sequence)) %>% ungroup()
Sys.time() - st
data.table results: 1.0987 secs; 0.9825 secs
setDT(dataframe)
by_cols = paste(""key"", c(""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i""), sep=""_"")
st = Sys.time()
a2 = dataframe[, .(min_date = min(date_sequence)), by=by_cols]
Sys.time() - st
2.
dplyr
setDF(dataframe)
st = Sys.time()
df3 = merge(dataframe, 
      dataframe_two, 
      by = c(""key_a"", ""key_b"", ""key_c"",
             ""key_d"", ""key_e"", ""key_f"",
             ""key_g"", ""key_h"", ""key_i""),
      all.x = T) %>% as_tibble()
Sys.time() - st
# Error in merge.data.frame(dataframe, dataframe_two, by = c(""key_a"", ""key_b"",  : 
#  negative length vectors are not allowed
data.table
setDT(dataframe)
setDT(dataframe_two)
st = Sys.time()
df3 = merge(dataframe, 
            dataframe_two, 
            by = c(""key_a"", ""key_b"", ""key_c"",
                   ""key_d"", ""key_e"", ""key_f"",
                   ""key_g"", ""key_h"", ""key_i""),
            all.x = T)
Sys.time() - st
# Error in vecseq(f__, len__, if (allow.cartesian || notjoin || !anyDuplicated(f__,  # : 
#  Join results in more than 2^31 rows (internal vecseq reached physical limit). 
# Very likely misspecified join. Check for duplicate key values in i each of which 
# join to the same group in x over and over again. If that's ok, try by=.EACHI to 
# run j for each group to avoid the large allocation. Otherwise, please search for 
# this error message in the FAQ, Wiki, Stack Overflow and data.table issue tracker 
# for advice.
This error is helpful and running the following:
uniqueN(dataframe_two, by = c(""key_a"", ""key_b"", ""key_c"",
                                    ""key_d"", ""key_e"", ""key_f"",
                                    ""key_g"", ""key_h"", ""key_i""))
gives
12
When I am working with datasets containing about 10 million rows and 15 columns, I convert strings to factors before merging and have seen performance gains from approx. 30 secs to 10 secs for an inner join. To my surprise, setkey() was not as effective as converting strings to factors in that particular case.
EDIT:
Reproducible example of data.table merge in 3 flavours (on character column, setkey, strings to factors)
Create tables:
x = 1e6
ids = x:(2*x-1)
chrs = rep(LETTERS[1:10], x)
quant_1 = sample(ids, x, replace = T)
quant_2 = sample(ids, x, replace = T)

ids_c = paste0(chrs, as.character(ids))

dt1 = data.table(unique(ids_c), quant_1)
dt2 = data.table(unique(ids_c), quant_2)
(i) on character column
system.time({result_chr = merge(dt1, dt2, by = 'V1')})
#   user  system elapsed 
#  10.66    5.18   18.64
(ii) using setkey
system.time(setkey(dt1, V1))
#   user  system elapsed 
#   3.37    1.55    5.66 
system.time(setkey(dt2, V1))
#   user  system elapsed 
#   3.42    1.67    5.85  
system.time({result_setkey = merge(dt1, dt2, by = 'V1')})
#   user  system elapsed 
#   0.17    0.00    0.16
(iii) strings to factors
dt3 = data.table(unique(ids_c), quant_1)
dt4 = data.table(unique(ids_c), quant_2)

system.time({dt3[, V1 := as.factor(V1)]})
#   user  system elapsed 
#   8.16    0.00    8.20 
system.time({dt4[, V1 := as.factor(V1)]})
#   user  system elapsed 
#   8.04    0.00    8.06 
system.time({result_fac = merge(dt3, dt4, by = 'V1')})
#   user  system elapsed 
#   0.32    0.01    0.28
In this case, setkey is overall the fastest with total 11.67 seconds. However, if data is ingested with strings to factors as true then no need to use setkey.
Example 2:
In case your data comes in one file with rows separated by an attribute, for example date, and you need to separate them first, then do a join.
Data:
dt5 = data.table(date = '202009', id = unique(ids_c), quant = quant_1)
dt6 = data.table(date = '202010', id = unique(ids_c), quant = quant_2)
# Original data comes combined
dt = rbindlist(list(dt5, dt6))
(i) setkey
system.time(setkey(dt, id))
#  user  system elapsed 
#  5.78    3.39   10.78 
dt5 = dt[date == '202009']
dt6 = dt[date == '202010']
system.time({result_setkey = merge(dt5, dt6, by = 'id')})
# user  system elapsed 
# 0.17    0.00    0.17
(ii) strings as factors
dt5 = data.table(date = '202009', id = unique(ids_c), quant = quant_1)
dt6 = data.table(date = '202010', id = unique(ids_c), quant = quant_2)
dt = rbindlist(list(dt5, dt6))
system.time({dt[, id := as.factor(id)]})
#   user  system elapsed 
#   8.17    0.00    8.20  
dt5 = dt[date == '202009']
dt6 = dt[date == '202010']
system.time({result_fac = merge(dt5, dt6, by = 'id')})
#   user  system elapsed 
#   0.34    0.00    0.33
In this case, strings to factors is faster at 8.53 seconds vs 10.95. However, when shuffling the keys before creating the tables
ids_c = sample(ids_c, replace = F)
, setkey performs 2x faster.
Also, note that not every function in data.table is faster than combination of base functions. For example:
# data.table    
system.time(uniqueN(ids_c))
#   user  system elapsed 
#  10.63    4.21   16.88 

# base R
system.time(length(unique(ids_c)))
#   user  system elapsed 
#   0.78    0.08    0.94
Important to note that uniqueN() consumes 4x less memory, so would be better to use if RAM size is a constraint. I've used profvis package for this flame graph (from a different run than above):
Finally, if working with datasets larger than RAM, have a look at
disk.frame
.
Share
Improve this answer
Follow
edited
Oct 4, 2020 at 10:37
answered
Sep 14, 2020 at 22:42
Zaki
Zaki
141
3
3 bronze badges
Add a comment
|
3
By default, R works with data in memory. When your data gets significantly larger R can throw out-of-memory errors, or depending on your setup use the pagefile (
see here
) but the pagefiles is slow as it involves reading and writing to disk.
1. batching
From just a computation perspective, you may find improvements by batching your processing. Your examples include summarising the dataset down, so presumably your summarised dataset is much smaller than your input (if not, it would be worth considering other approaches to producing the same final dataset). This means that you can batch by your grouping variables.
I often do this by taking modulo of a numeric index:
num_batches = 50
output = list()

for(i in 0:(num_batches-1)){
  subset = df %>% filter(numeric_key %% num_batches == i)

  this_summary = subset %>%
    group_by(numeric_key, other_keys) %>%
    summarise(result = min(col)

  output[[i]] = this_summary
}
final_output = bind_rows(output)
You can develop a similar approach for text-based keys.
2. reduce data size
Storing text requires more memory than storing numeric data. An easy option here is to replace strings with numeric codes, or store strings as factors. This will use less memory, and hence the computer has less information to read when grouping/joining.
Note that depending on your version of R,
stringsAsFactors
may default to
TRUE
or
FALSE
. So probably best to set it explicitly. (
discussed here
)
3. move to disk
Beyond some size it is worth having data on disk and letting R manage reading to and from disk. This is part of the idea behind several existing R packages including
bigmemory
,
ff and ffbase
, and a host of
parallelisation packages
.
Beyond just depending on R, you can push tasks to a database. While a database will never perform as quickly as in-memory data, they are designed for handling large quantities of data. PostgreSQL is free and open source (
getting started guide here
), and you can run this on the same machine as R - it does not have to be a dedicated server. R also has a package specifically for PostgreSQL (
RPostgreSQL
). There are also several other packages designed for working with databases including dbplyr, DBI, RODBC if you want other options for interacting with databases.
While there is some overhead setting up a database, dplyr and dbplyr will translate your R code into SQL for you, so you don't have to learn a new language. The downside is that you are limited to core dplyr commands as translations from R to SQL are only defined for the standard procedures.
Share
Improve this answer
Follow
answered
Sep 10, 2020 at 21:27
Simon.S.A.
Simon.S.A.
6,894
7
7 gold badges
25
25 silver badges
44
44 bronze badges
6
Can I spin up the PostgreSQL db from within my R instance or terminal?
–
Cauder
Commented
Sep 10, 2020 at 21:32
R can pass commands to the cmd prompt, and (almost) anything you can do with a mouse & keyboard interactively you could do from the terminal. So if you're very determined then I am sure you'd find a way. But I don't have a way to do this, and the tutorials I've found online involve some setup outside of R. Note that once a database is setup on the computer, you can access it and load data into it from within R.
–
Simon.S.A.
Commented
Sep 11, 2020 at 2:02
2
For your point 2, R uses
a global string pool
so storing strings as factors shouldn't bring any additional benefit.
–
Alexlok
Commented
Sep 11, 2020 at 21:20
@Alexlok makes a good point if working within R. If reading/writing to disk or a database then this still is worth considering.
–
Simon.S.A.
Commented
Sep 12, 2020 at 7:55
Adding to @Alexlok  factors are in general memory inefficient in
R
when compared to character vectors. One of the main points why
stringAsFactors
was changed to default to
FALSE
in ´R-4.0.0`.
–
Oliver
Commented
Sep 12, 2020 at 9:59
|
Show
1
more comment
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
r
dplyr
data.table
tidyverse
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
13
Forcing R (and Rstudio) to use the virtual memory on Windows
0
My group by doesn't appear to be working in disk frames
2
Join with fuzzy matching by date in R
0
How do I make my R script robust to crashing?
Related
2
Improvement in performance
1
Fast processing of large tables and table columns - apply too slow
2
How to optimize this for loop in R for a large data.table
2
Optimization of Data.Table in R?
1
Efficient way to handle big data in R
0
How to improve speed for table with millions of rows
1
Optimize `dplyr` group_by / summarize
0
How to speed up row operations over large dataset when applying specific function
0
Dealing with a huge amount of rows
0
What would be the best way to improve calcul performance in a big data.table?
Hot Network Questions
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Physical interpretation of selection rules for different multipole orders
How *exactly* is divisibility defined?
how do I correctly check that some aggregated results are correct?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Convert pipe delimited column data to HTML table format for email
Can a hyphen be a ""letter"" in some words?
Derailleur Hangar - Fastener torque & thread preparation
When looking at the first DCM page, where is the next DCM page documented?
How to delete edges of curve based on their length
reverse engineering wire protocol
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
Find a fraction's parent in the Stern-Brocot tree
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
What does “going off” mean in ""Going off the age of the statues""?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
How did Jahnke and Emde create their plots
Does an NEC load calculation overage mandate a service upgrade?
Challah dough bread machine recipe issues
Heating object in airless environment
What's the justification for implicitly casting arrays to pointers (in the C language family)?
How could a city build a circular canal?
How much of a structural/syntactic difference is there between an oath and a promise?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
How do I etl big data between 2 SQL Server? - Stack Overflow,"How do I etl big data between 2 SQL Server? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How do I etl big data between 2 SQL Server?
Ask Question
Asked
4 years, 3 months ago
Modified
4 years, 2 months ago
Viewed
152 times
0
My primary datasource get 50M records per day. I need view record max delay about 5 minutes.
How I have best way to transfer data from primary SQL Server datasource to report SQL Server datasource.
At this time, I user merge join every 30seconds. But it seems effect to primary datasource performance.
sql
sql-server
etl
Share
Improve this question
Follow
edited
Aug 19, 2020 at 9:33
Dale K
26.9k
15
15 gold badges
50
50 silver badges
81
81 bronze badges
asked
Aug 19, 2020 at 9:31
Thapld
Thapld
37
9
9 bronze badges
7
3
Replication or data sync, maybe?
–
DhruvJoshi
Commented
Aug 19, 2020 at 9:34
4
Replication would be an obvious choice. Have you done any of your own research?
–
ADyson
Commented
Aug 19, 2020 at 9:36
I don’t use data sync because The method play with trigger. I think it causes slow down insert performance in primary datasource.
–
Thapld
Commented
Aug 19, 2020 at 9:44
I also search replication method but I get more issues about performance.
–
Thapld
Commented
Aug 19, 2020 at 9:44
Replication is usually efficient if it's set up correctly
–
ADyson
Commented
Aug 19, 2020 at 10:18
|
Show
2
more comments
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
The most common approach to minimize the load on your source server is to do periodic extracts using a timestamp, i.e. a simple SELECT ...
WHERE timestamp > previous-max-timestamp-extracted
.
The source table(s) need to provide a column that allows you to filter on un-extracted records. If that's completely impossible, you might extract e.g. the last hour's data into staging tables, and deduplicate with previously extracted records.
Yes, you could use CDC, but that's often more involved, and usually adds some restrictions.
Cheers, Kristian
Share
Improve this answer
Follow
answered
Sep 24, 2020 at 11:50
Kristian Wedberg
Kristian Wedberg
475
4
4 silver badges
10
10 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
sql
sql-server
etl
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
0
SQL Server 2005 loading data from an external server
1
Data transfer from SQL Server to PostgreSQL
1
How can I move data from one SQL Server to other?
1
SSIS - ETL - Transfer tables/databases from many servers?
0
Best approach to move data between SQL Servers
1
Migrating data between two SQL Server 2014 DBs
3
Copying data from one database to another using SSIS
5
Best method to move a large SQL Server table from one database to another?
4
What is the best way to transfer data from SAP to SQL Server?
0
Push data to another SQL server
Hot Network Questions
Why did Crimea’s parliament agree to join Ukraine?
Is there greater explanatory power in laws governing things rather than being descriptive?
When was ""to list"" meaning ""to wish"" lost?
What should machining (turning, milling, grinding) in space look like
Is it possible that the committee contacts only one reference while applicants need to provide two?
What are these 16-Century Italian monetary symbols?
How to keep meat in a dungeon fresh, preserved, and hot?
How to estimate the latency of communication?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
Time travelling paedo priest novel
Why are Jersey and Guernsey not considered sovereign states?
Convert pipe delimited column data to HTML table format for email
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How did Jahnke and Emde create their plots
Need an advice to rig a spaceship with mechanicals part
Physical interpretation of selection rules for different multipole orders
How should I connect a light fixture with UK wire colors to US wiring?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Derailleur Hangar - Fastener torque & thread preparation
Building a Statistically Sound ML Model
how do I correctly check that some aggregated results are correct?
How to use a symbol as both function and head?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
Application Insights with big data - Stack Overflow,"Application Insights with big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Application Insights with big data
Ask Question
Asked
4 years, 2 months ago
Modified
4 years, 2 months ago
Viewed
198 times
Part of
Microsoft Azure
Collective
1
I am looking to use a jvm-profiler (
https://github.com/uber-common/jvm-profiler
) on my spark clusters and correlate this with my application logs. On a large cluster (about 1000 executors), this will produce a lot of data. I tried dumping it into an influxDb which worked well. However, then I have to manage the influxDb. My stack is all in Azure, and we use Application Insights.
Could application insights handle this kind of load, or is there a better Azure alternative. My plan would then be to use Power BI to create dashboards around this data.
A current alternative is to use influxDb with Chronograf to visualize the data.
azure-application-insights
Share
Improve this question
Follow
asked
Sep 22, 2020 at 17:12
terminatur
terminatur
678
1
1 gold badge
6
6 silver badges
25
25 bronze badges
1
For Time Series storage there is also
Azure Time Series Insights
–
Peter Bons
Commented
Sep 22, 2020 at 18:00
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
App Insights will ingest
a maximum of 32,ooo events a second. The default data cap is 100GB per day, but you can increase this to 1,000 GB on your own or request a higher cap from support.
Making use of sampling
will reduce the total volume and your bill- you can control how this is done so you only sample data where you want to. Generally, cumulative metrics are sent separately so things like request counts aren't affected by sampling.
Share
Improve this answer
Follow
answered
Sep 23, 2020 at 0:26
PerfectlyPanda
PerfectlyPanda
3,446
1
1 gold badge
7
7 silver badges
17
17 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
azure-application-insights
or
ask your own question
.
Microsoft Azure
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
Multi-Tenant Application Insights in MS Azure
5
Application Insights
1
Application Insights automation
0
Application Insight Analytics SDK
0
Azure Application Insights
1
Azure Application Insights Scalability
1
Application Insights analytics Let operator
3
Implement Application Insights for windows service
6
Application Insights data sampling
0
Application Insights on a large SaaS
Hot Network Questions
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Why are Jersey and Guernsey not considered sovereign states?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
how do I correctly check that some aggregated results are correct?
What's the difference between '\ ' and tilde character (~)?
What should machining (turning, milling, grinding) in space look like
Why did Crimea’s parliament agree to join Ukraine?
How can we be sure that the effects of gravity travel at most at the speed of light
Find all unique quintuplets in an array that sum to a given target
Bash script that waits until GPU is free
What are these 16-Century Italian monetary symbols?
Math contents does not align when subscripts are used
What is ""B & S"" a reference to in Khartoum?
Longest bitonic subarray
Building a Statistically Sound ML Model
Physical interpretation of selection rules for different multipole orders
A Pandigital Multiplication
How to explain why I don't have a reference letter from my supervisor
Need an advice to rig a spaceship with mechanicals part
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
US phone service for long-term travel
Can a hyphen be a ""letter"" in some words?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python 3.x - How to simplify text comparison for big data-set where text meaning is same but not exact - deduplicate text data - Stack Overflow,"python 3.x - How to simplify text comparison for big data-set where text meaning is same but not exact - deduplicate text data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to simplify text comparison for big data-set where text meaning is same but not exact - deduplicate text data
Ask Question
Asked
4 years, 3 months ago
Modified
4 years, 3 months ago
Viewed
145 times
Part of
NLP
Collective
0
I have text data set (different menu items like chocolate, cake, coke etc) of around 1.8 million records which belongs to 6 different categories (category A, B, C, D, E, F). one of the category has around 700k records. Most of the menu items are mixed up in multiple categories to which they doesn't belong to, for example: cake belongs to category 'A' but it is found in category 'B' & 'C' as well.
I want to identify those misclassified items and report to a personnel but the challenge is the item name is not always correct because it is totally human typed text. For example: Chocolate might be updated as hot chclt, sweet choklate, chocolat etc. There can also be items like chocolate cake ;)
so to handle this, I tried a simple method using cosine similarity to compare category-wise and identify those anomalies but it takes alot of time since I am comparing each items to 1.8 million records (Sample code is as shown below). Can anyone suggest a better way to deal with this problem?
#Function
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

def cos_similarity(a,b):
    X =a
    Y =b

    # tokenization 
    X_list = word_tokenize(X)  
    Y_list = word_tokenize(Y) 

    # sw contains the list of stopwords 
    sw = stopwords.words('english')  
    l1 =[];l2 =[] 

    # remove stop words from the string 
    X_set = {w for w in X_list if not w in sw}  
    Y_set = {w for w in Y_list if not w in sw} 

    # form a set containing keywords of both strings  
    rvector = X_set.union(Y_set)  
    for w in rvector: 
        if w in X_set: l1.append(1) # create a vector 
        else: l1.append(0) 
        if w in Y_set: l2.append(1) 
        else: l2.append(0) 
    c = 0

    # cosine formula  
    for i in range(len(rvector)): 
            c+= l1[i]*l2[i] 
    if float((sum(l1)*sum(l2))**0.5)>0:
        cosine = c / float((sum(l1)*sum(l2))**0.5) 
    else:
        cosine = 0
    return cosine

#Base code
cos_sim_list = []
for i in category_B.index:
    ln_cosdegree = 0
    ln_degsem = []
    for j in category_A.index:
        ln_j = str(category_A['item_name'][j])
        ln_i = str(category_B['item_name'][i])
        degreeOfSimilarity = cos_similarity(ln_j,ln_i)
        if degreeOfSimilarity>0.5:
            cos_sim_list.append([ln_j,ln_i,degreeOfSimilarity])
Consider text is already cleaned
python-3.x
machine-learning
nlp
duplicates
cosine-similarity
Share
Improve this question
Follow
edited
Sep 6, 2020 at 19:18
akash s
asked
Sep 4, 2020 at 4:50
akash s
akash s
59
3
3 bronze badges
2
I would be clearer if you show a full example (e.g. input -> output format)
–
Jason Angel
Commented
Sep 6, 2020 at 6:40
Hi James, I figured out a way to deal with this problem using KNeighbors and cosine similarity and it is working for my usecase. Though I am still comparing category by category, it is still effective for me. Please do let me know if you can find any better idea to handle this problem
–
akash s
Commented
Sep 6, 2020 at 19:08
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
I used KNeighbor and cosine similarity to solve this case. Though I am running the code multiple times to compare category by category; still it is effective because of lesser number of categories. Please suggest me if any better solution is available
cat_A_clean = category_A['item_name'].unique()

print('Vecorizing the data - this could take a few minutes for large datasets...')
vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)
tfidf = vectorizer.fit_transform(cat_A_clean)
print('Vecorizing completed...')

from sklearn.neighbors import NearestNeighbors
nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)

unique_B = set(category_B['item_name'].values) 

def getNearestN(query):
    queryTFIDF_ = vectorizer.transform(query)
    distances, indices = nbrs.kneighbors(queryTFIDF_)
    return distances, indices

import time
t1 = time.time()
print('getting nearest n...')
distances, indices = getNearestN(unique_B)
t = time.time()-t1
print(""COMPLETED IN:"", t)

unique_B = list(unique_B) 
print('finding matches...')
matches = []
for i,j in enumerate(indices):
    temp = [round(distances[i][0],2), cat_A_clean['item_name'].values[j],unique_B[i]]
    matches.append(temp)

print('Building data frame...')  
matches = pd.DataFrame(matches, columns=['Match confidence (lower is better)','ITEM_A','ITEM_B'])
print('Done') 

def clean_string(text):
        text = str(text)
        text = text.lower()
        return(text)
def cosine_sim_vectors(vec1,vec2):
    vec1 = vec1.reshape(1,-1)
    vec2 = vec2.reshape(1,-1)
    return cosine_similarity(vec1,vec2)[0][0]

def cos_similarity(sentences):
    cleaned = list(map(clean_string,sentences))
    print(cleaned)
    vectorizer = CountVectorizer().fit_transform(cleaned)
    vectors = vectorizer.toarray()
    print(vectors) 
    return(cosine_sim_vectors(vectors[0],vectors[1]))

cos_sim_list =[]
for ind in matches.index:
    a = matches['Match confidence (lower is better)'][ind]
    b = matches['ITEM_A'][ind]
    c = matches['ITEM_B'][ind]
    degreeOfSimilarity = cos_similarity([b,c])
    cos_sim_list.append([a,b,c,degreeOfSimilarity])
Share
Improve this answer
Follow
answered
Sep 6, 2020 at 19:13
akash s
akash s
59
3
3 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python-3.x
machine-learning
nlp
duplicates
cosine-similarity
or
ask your own question
.
NLP
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
How to detect duplicate text with some fuzzyness
3
Detect duplicated/similar text among large datasets?
3
How to compare metrics between two large texts - cosine, Jaccard similarities, Sim_MinEdit (Sim_String) and Sim_Simple in Python
12
Text normalization : Text similarity in Python. How to normalize Text spelling mismatch?
0
How to find text similarity within millions of entries?
4
what is the best method to calculate text similarity?
1
How to find text reuse with fuzzy match?
0
Text (cosine) similarity
1
increasing efficiency of cosine simarlity
1
Quickly performing cosine similarity with list of embeddings
Hot Network Questions
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Should I expect a call from my future boss after signing the offer?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How *exactly* is divisibility defined?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
Does DOS require partitions to be aligned at a cylinder boundary?
Convert pipe delimited column data to HTML table format for email
Does an NEC load calculation overage mandate a service upgrade?
Why did Crimea’s parliament agree to join Ukraine?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
How should I connect a light fixture with UK wire colors to US wiring?
How can we be sure that the effects of gravity travel at most at the speed of light
How could a city build a circular canal?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
Need an advice to rig a spaceship with mechanicals part
When was ""to list"" meaning ""to wish"" lost?
Passphrase entropy calculation, Wikipedia version
Didactic tool to play with deterministic and nondeterministic finite automata
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Indian music video with over the top CGI
How to remove clear adhesive tape from wooden kitchen cupboards?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Is outer space Radioactive?
Is there greater explanatory power in laws governing things rather than being descriptive?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"sql - ignite write big data in a pressure test, io write and read time tow high? - Stack Overflow","sql - ignite write big data in a pressure test, io write and read time tow high? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
ignite write big data in a pressure test, io write and read time tow high?
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 3 months ago
Viewed
90 times
0
I hava some write and find in ignite with sql.When I have a pressure test with 100 persons,  ignite server's cpu is low, but iowait is two heigh, and io write and read time is two heigh. So I have some ways to reduce iowait?
I use 2.7.6 version with two SSD machines as cluster.
this is iowait time picture
[1]:
https://i.sstatic.net/P0bka.png
[2]:
https://i.sstatic.net/hyfv1.png
this is iowait cpu picture
sql
ignite
iowait
Share
Improve this question
Follow
asked
Aug 14, 2020 at 13:33
zhangzl
zhangzl
3
2
2 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
In persistent scenarios, you are bound by WAL writes. You can try changing WAL Mode to
LOG_ONLY
to mitigate these peaks slightly.
I recommend making checkpoints further apart (by changing ""checkpoint frequency"" setting, in
ms
) and maybe increasing ""checkpoint page buffer"".
Share
Improve this answer
Follow
edited
Sep 4, 2020 at 9:23
answered
Aug 14, 2020 at 14:26
alamar
alamar
19.3k
4
4 gold badges
70
70 silver badges
105
105 bronze badges
3
Do you know good idea to improve WAL writes very much?
–
zhangzl
Commented
Aug 24, 2020 at 7:39
LOG_ONLY is default model，I don't change.
–
zhangzl
Commented
Aug 24, 2020 at 7:44
Then, I recommend making checkpoints further apart (by changing checkpoint frequency setting, in
ms
) and maybe increasing checkpoint page buffer.
–
alamar
Commented
Sep 4, 2020 at 9:23
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
sql
ignite
iowait
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
Many small writes to SQLite
7
How to minimize number of bytes writen to disk by sqlite?
1
Apache Ignite: How can I improve insertion performance?
0
Is high read but absolutely no writing on SQLite query normal?
0
Apache Ignite throughput, value size, and max cache count?
1
Apache Ignite - Problem with Transactions including 20K objects
2
Apache Ignite vs. SQL Server Performance
0
Ignite Configuration 2.6-IGFS Speed--do backups/disk writes Asynchronously?
0
Apache Ignite IGFS: Start reading large file before write completes
0
Simple SELECT(*) queries very slow in Apache Ignite
Hot Network Questions
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Can we judge morality?
Covering a smoke alarm horn
Protecting myself against costs for overnight weather-related cancellations
How to use a symbol as both function and head?
What should machining (turning, milling, grinding) in space look like
Is outer space Radioactive?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Physical interpretation of selection rules for different multipole orders
Only selecting Features that have another layers feature on top
White perpetual check, where Black manages a check too?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
UUID v7 Implementation
Does an NEC load calculation overage mandate a service upgrade?
Passphrase entropy calculation, Wikipedia version
Useful aerial recon vehicles for newly colonized worlds
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
How manage inventory discrepancies due to measurement errors in warehouse management systems
How to achieve infinite rage?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Indian music video with over the top CGI
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
c# - Blazor - asynchronously render big data - Stack Overflow,"c# - Blazor - asynchronously render big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Blazor - asynchronously render big data
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
669 times
0
on my Razor page I have an svg container .
Inside of the svg I draw the circles.
It could be thousands of circles in the SVG.
<svg>
  @foreach (var pixel in GetPixels())
  {
     <circle class=""pixel"" cx=""@pixel.CoordinateX"" cy=""@pixel.CoordinateY"" />                  
    }
</svg>
and GetPixels is
IEnumerable<Pixel> GetPixels()
        {
            var progress = 0;
            float oldPercent = 0;
            foreach (var p in Result.Pixels)
            {
                yield return p;

                var progressPercent = (float)Math.Ceiling(progress * 1F / (Result.TotalItems) * 100F);
                if (progressPercent % 1 == 0 && oldPercent != progressPercent)
                {
                    Js.InvokeVoidAsync(""updateProgress"", progressPercent);
                    oldPercent = progressPercent;
                }

                progress++;

            }
        }
The backend iteration works very fast, the problem occurs when there foreach statement is complete and the browser is trying to render thousands of these circles, so it literally hanging for a while.
I would like to implement some batch load of the circles, when after specific amount of circles I rerender the svg (let say 1000 circles - output to SVG and show partially a picture).
Could you please suggest what would be the better way of doing it?
c#
.net
asp.net-core
svg
blazor
Share
Improve this question
Follow
edited
Aug 13, 2020 at 19:53
DolceVita
asked
Aug 13, 2020 at 7:28
DolceVita
DolceVita
2,100
1
1 gold badge
23
23 silver badges
35
35 bronze badges
5
nope, I don't have any warning.
–
DolceVita
Commented
Aug 13, 2020 at 9:58
codedread.com/browser-tests/particle/particle.xhtml
–
Robert Longson
Commented
Aug 13, 2020 at 9:58
@RobertLongson they have 500 particles. I could have 5-200k
–
DolceVita
Commented
Aug 13, 2020 at 10:00
1
In that case you should be using canvas and not SVG.
–
Robert Longson
Commented
Aug 13, 2020 at 10:01
@RobertLongson ok, you could be right, but anyway, it would be interresting how to solve the issue with a big data rendering in Blazor... Below Marijn Pessers posted a very interresting approach, it is absolutely new and I think it could be cool to implement in this problem.
–
DolceVita
Commented
Aug 13, 2020 at 19:52
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
I have no answer for you regerding the current release version but if you look at .net 5 they have solved this (in the upcoming preview).
Look at this video of the
.net Community Standup
and go to 50m50s and you will see an implementation of your question using tables.
So IF you can use .net5 preview or you can wait till the end of the year when it is released I suggest you use that approach.
Share
Improve this answer
Follow
answered
Aug 13, 2020 at 9:58
Marijn Pessers
Marijn Pessers
824
1
1 gold badge
7
7 silver badges
13
13 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
c#
.net
asp.net-core
svg
blazor
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
9
How should data be loaded asynchronously in Blazor
32
Calling async methods in Blazor view
2
Loading a large list in Blazor
0
How to show a .svg file which is loaded in a variable on a html page (Blazor ServerSide)?
0
Blazor foreach does not render elements after OnInitializedAsync call
0
Blazor rendering after computation or call
0
Data Does Not Load In OnAfterRenderAsync() Blazor Server
0
Razor component - Load and display data asynchronously
2
Blazor / SVG Real Time Updates
0
Blazor Server Side - Load more 50 data
Hot Network Questions
How to keep meat in a dungeon fresh, preserved, and hot?
How could a city build a circular canal?
What does it mean when folks say that universe is not ""Locally real""?
How did Jahnke and Emde create their plots
Why is the speed graph of a survey flight a square wave?
How does this Paypal guest checkout scam work?
What's a modern term for sucker or sap?
A Pandigital Multiplication
What technique is used for the heads in this LEGO Halo Elite MOC?
What's the difference between '\ ' and tilde character (~)?
reverse engineering wire protocol
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Heating object in airless environment
Longest bitonic subarray
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
How *exactly* is divisibility defined?
Why are Jersey and Guernsey not considered sovereign states?
Convert pipe delimited column data to HTML table format for email
Physical interpretation of selection rules for different multipole orders
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
How to set individual columns in the siunitx package to boldface? It it a bug?
Building a Statistically Sound ML Model
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-cs
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
hadoop - Sqoop Big Data : How to import an address field with a comma using sqoop? - Stack Overflow,"hadoop - Sqoop Big Data : How to import an address field with a comma using sqoop? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Sqoop Big Data : How to import an address field with a comma using sqoop?
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
277 times
0
I have a csv data like:
id,name,address,city,state
111,'Mona','E103,abc society','Pune','Maharashtra'
How to import this file data to hive using sqoop?
hadoop
hive
sqoop
Share
Improve this question
Follow
edited
Aug 13, 2020 at 8:38
JGS
asked
Aug 12, 2020 at 14:03
JGS
JGS
369
2
2 gold badges
5
5 silver badges
17
17 bronze badges
0
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
It has to be with sqoop?
You can do this using hive directly.
CREATE EXTERNAL TABLE IF NOT EXISTS bdp.hv_csv_table
(id STRING,Code STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs://sandbox.hortonworks.com:8020/user/root/bdp/ld_csv_hv/ip';
Share
Improve this answer
Follow
edited
Aug 13, 2020 at 18:20
OneCricketeer
191k
20
20 gold badges
141
141 silver badges
266
266 bronze badges
answered
Aug 12, 2020 at 14:09
diogoramos
diogoramos
86
7
7 bronze badges
4
Do you know what the csv file means? It is  literally Comma-separated values. The question has been edited I think
–
diogoramos
Commented
Aug 13, 2020 at 8:06
Original post did not have commas
stackoverflow.com/revisions/…
–
OneCricketeer
Commented
Aug 13, 2020 at 16:01
You're right @OneCricketeer. Do you think the answer now applies?
–
diogoramos
Commented
Aug 13, 2020 at 16:33
Using the correct field names and skipping the header of the file, then sure
–
OneCricketeer
Commented
Aug 13, 2020 at 18:21
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
hadoop
hive
sqoop
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
52
Hive load CSV with commas in quoted fields
0
How to import a table with delimiter "";"" into hive?
0
sqoop importing string column of a dataset containing "","" in it
1
How to import csv file into hive table when it has delimiter value, say comma, as a field value?
0
How do I export a CSV file into Hive table with records value with comma?
0
How to specify fields delimiter when import MySQL into Hive with Sqoop?
2
Multiple character delimiter using apache sqoop import
1
How to create an external Hive table if the field value has comma separated values
0
Sqoop import using control character as delimiter
0
How to handle CSV file where address column data's are seperated by comma as a single column instead of multiple column in Spark
Hot Network Questions
How manage inventory discrepancies due to measurement errors in warehouse management systems
Does DOS require partitions to be aligned at a cylinder boundary?
What should machining (turning, milling, grinding) in space look like
Two types difinition of the distance function
How to achieve infinite rage?
How *exactly* is divisibility defined?
How to format numbers in monospaced (typewriter) font using siunitx?
A Pandigital Multiplication
Should I expect a call from my future boss after signing the offer?
When to use cards for communicating dietary restrictions in Japan
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What's the difference between '\ ' and tilde character (~)?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
How to delete edges of curve based on their length
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
What does “going off” mean in ""Going off the age of the statues""?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
How could a city build a circular canal?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
When was ""to list"" meaning ""to wish"" lost?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
UUID v7 Implementation
Need an advice to rig a spaceship with mechanicals part
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
r - Random forest for Big data - Stack Overflow,"r - Random forest for Big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Random forest for Big data
Ask Question
Asked
4 years, 8 months ago
Modified
4 years, 4 months ago
Viewed
136 times
Part of
R Language
Collective
0
I have tried to use ""rfsrc"" R package for a massive data set.
I got  AUC: NaN
Error rate: 0, 0, 0
Also, got Warning messages:
1: In length(x) * length(y) : NAs produced by integer overflow
2: In length(x) * length(y) : NAs produced by integer overflow
Traditional random forest failed to train the data. I was wondering if you could give me some insights on these issues.
My code:
str(df)
'data.frame':   250000 obs. of  32 variables:
 $ DER_mass_MMC               : num  138 161 -999 144 176 ...
 $ DER_mass_transverse_met_lep: num  51.7 68.8 162.2 81.4 16.9 ...
 $ DER_mass_vis               : num  97.8 103.2 126 80.9 134.8 ...
 $ DER_pt_h                   : num  27.98 48.146 35.635 0.414 16.405 ...
 $ DER_deltaeta_jet_jet       : num  0.91 -999 -999 -999 -999 ...
 $ DER_mass_jet_jet           : num  125 -999 -999 -999 -999 ...
 $ DER_prodeta_jet_jet        : num  2.67 -999 -999 -999 -999 ...
 $ DER_deltar_tau_lep         : num  3.06 3.47 3.15 3.31 3.89 ...
 $ DER_pt_tot                 : num  41.928 2.078 9.336 0.414 16.405 ...
 $ DER_sum_pt                 : num  198 125 198 76 58 ...
 $ DER_pt_ratio_lep_tau       : num  1.582 0.879 3.776 2.354 1.056 ...
 $ DER_met_phi_centrality     : num  1.4 1.41 1.41 -1.28 -1.38 ...
 $ DER_lep_eta_centrality     : num  0.2 -999 -999 -999 -999 0.975 0.791 -999 -999 -999 ...
 $ PRI_tau_pt                 : num  32.6 42 32.2 22.6 28.2 ...
 $ PRI_tau_eta                : num  1.017 2.039 -0.705 -1.655 -2.197 ...
 $ PRI_tau_phi                : num  0.381 -3.011 -2.093 0.01 -2.231 ...
 $ PRI_lep_pt                 : num  51.6 36.9 121.4 53.3 29.8 ...
 $ PRI_lep_eta                : num  2.273 0.501 -0.953 -0.522 0.798 ...
 $ PRI_lep_phi                : num  -2.414 0.103 1.052 -3.1 1.569 ...
 $ PRI_met                    : num  16.82 44.7 54.28 31.08 2.72 ...
 $ PRI_met_phi                : num  -0.277 -1.916 -2.186 0.06 -0.871 ...
 $ PRI_met_sumet              : num  258.7 164.5 260.4 86.1 53.1 ...
 $ PRI_jet_num                : int  2 1 1 0 0 3 2 1 0 1 ...
 $ PRI_jet_leading_pt         : num  67.4 46.2 44.3 -999 -999 ...
 $ PRI_jet_leading_eta        : num  2.15 0.725 2.053 -999 -999 ...
 $ PRI_jet_leading_phi        : num  0.444 1.158 -2.028 -999 -999 ...
 $ PRI_jet_subleading_pt      : num  46.1 -999 -999 -999 -999 ...
 $ PRI_jet_subleading_eta     : num  1.24 -999 -999 -999 -999 0.224 0.131 -999 -999 -999 ...
 $ PRI_jet_subleading_phi     : num  -2.48 -999 -999 -999 -999 ...
 $ PRI_jet_all_pt             : num  113.5 46.2 44.3 0 0 ...
 $ Weight                     : num  0.00265 2.23358 2.34739 5.44638 6.24533 ...
 $ Label                      : Factor w/ 2 levels ""b"",""s"": 2 1 1 1 1 1 2 2 1 2 ...



# Random forest Using rfsrc
> start_time <- Sys.time()
> RFSRC<-rfsrc(Label~.,data=df)
> end_time <- Sys.time()
> end_time - start_time
Time difference of 11.99594 mins
> print(RFSRC)
                         Sample size: 250000
           Frequency of class labels: 164333, 85667
                     Number of trees: 1000
           Forest terminal node size: 1
       Average no. of terminal nodes: 3298.9
No. of variables tried at each split: 6
              Total no. of variables: 31
       Resampling used to grow trees: swor
    Resample size used to grow trees: 158000
                            Analysis: RF-C
                              Family: class
                      Splitting rule: gini *random*
       Number of random split points: 10
              Normalized brier score: 0.75 
                                 AUC: NaN 
                          Error rate: 0, 0, 0

Confusion matrix:

          predicted
  observed      b     s class.error
         b 164333     0           0
         s      4 85663           0

    Overall error rate: 0% 
Warning messages:
1: In length(x) * length(y) : NAs produced by integer overflow
2: In length(x) * length(y) : NAs produced by integer overflow

# Traditional random forest
> start_time <- Sys.time()
> rf <-randomForest(Label~.,data=df)
Error: cannot allocate vector of size 1.9 Gb
> rf <-randomForest(Label~.,data=df)
Error: cannot allocate vector of size 1.9 Gb
r
machine-learning
random-forest
Share
Improve this question
Follow
edited
Aug 8, 2020 at 23:52
Christopher Moore
17.1k
11
11 gold badges
50
50 silver badges
61
61 bronze badges
asked
Apr 15, 2020 at 3:57
ForestGump
ForestGump
59
5
5 silver badges
23
23 bronze badges
4
250K is not that much, the ranger package can handle this without a problem. Don't forget to specify the number of cores you want to use. If you have need of more capabilities, you might want to look into h2o.
–
phiver
Commented
Apr 15, 2020 at 8:12
@phiver Why I am seeing this results, AUC: NaN  and Error rate: 0, 0, 0 from rfsrc? Am I missing anything?
–
ForestGump
Commented
Apr 15, 2020 at 13:11
For that I would need to have a reproducible example. But you might be better of creating an issue on the github page of
randomForestSRC
–
phiver
Commented
Apr 15, 2020 at 13:45
@phiver I will do it! Thank u so for your suggestions!
–
ForestGump
Commented
Apr 15, 2020 at 17:29
Add a comment
|
Related questions
7
Random forest on a big dataset
3
Random Forest in R - many classes
7
R randomForest for classification
Related questions
7
Random forest on a big dataset
3
Random Forest in R - many classes
7
R randomForest for classification
1
Random Forest in R
3
randomForest() machine learning in R
1
Random Forest algorithm in R
0
Random Forest run
0
Random forest model in r
1
Random Forest Tree for classification
8
Running Random Forest in Parallel
Load 7 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
r
machine-learning
random-forest
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
How to remove clear adhesive tape from wooden kitchen cupboards?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
How can we be sure that the effects of gravity travel at most at the speed of light
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
How to format numbers in monospaced (typewriter) font using siunitx?
What's a modern term for sucker or sap?
Two types difinition of the distance function
Can two wrongs ever make a right?
Covering a smoke alarm horn
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
Why is the speed graph of a survey flight a square wave?
Is there greater explanatory power in laws governing things rather than being descriptive?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How to use a symbol as both function and head?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Does an NEC load calculation overage mandate a service upgrade?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
How to explain why I don't have a reference letter from my supervisor
How *exactly* is divisibility defined?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
How to set individual columns in the siunitx package to boldface? It it a bug?
Does DOS require partitions to be aligned at a cylinder boundary?
White perpetual check, where Black manages a check too?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
performance - Can we do big data load testing by using Java request sampler? - Stack Overflow,"performance - Can we do big data load testing by using Java request sampler? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Can we do big data load testing by using Java request sampler?
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
32 times
0
We have created custom Hbase plugin and we are using sampler as ""java request"" in jmeter, here we are passing ""table name"", data, Zooker keeper location then we are inserting the data to Hbase then data is inserting to Hbase, Now my question is how we can update and delete the data ? by using the java request sampler?
Please advise me a solution, it will be great helpful.
performance
jmeter
load-testing
database-performance
Share
Improve this question
Follow
edited
Aug 4, 2020 at 16:53
user4157124
2,904
14
14 gold badges
30
30 silver badges
44
44 bronze badges
asked
Aug 4, 2020 at 8:22
D Md Siraj Ahmed
D Md Siraj Ahmed
1
1
1 bronze badge
3
Try YCSB rather than JMeter
–
OneCricketeer
Commented
Aug 4, 2020 at 16:32
what is this ""YCSB"" it is any tool ?,  pls send me a reference link.
–
D Md Siraj Ahmed
Commented
Aug 5, 2020 at 12:52
I trust your ability to search for it
–
OneCricketeer
Commented
Aug 5, 2020 at 14:12
Add a comment
|
Related questions
6
Jmeter - simulating more complex load scenarios?
3
Load Testing with JMeter
23
How to perform load testing for website using JMeter
Related questions
6
Jmeter - simulating more complex load scenarios?
3
Load Testing with JMeter
23
How to perform load testing for website using JMeter
1
jmeter regarding performance and load testing
3
How to set up a large amount of request data for a jMeter load test
0
What is Jmeter Test scenario for large request per second
1
Tools for doing load testing of Http request at high payload(500kb and more)
2
Jmeter bottleneck
1
Jmeter load test with 30K users?
3
How to test performance with 1 million users in Jmeter?
Load 7 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
performance
jmeter
load-testing
database-performance
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
Hole, YHWH and counterfactual present
What should machining (turning, milling, grinding) in space look like
Didactic tool to play with deterministic and nondeterministic finite automata
Is it possible that the committee contacts only one reference while applicants need to provide two?
US phone service for long-term travel
What's the difference between '\ ' and tilde character (~)?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
reverse engineering wire protocol
How can we be sure that the effects of gravity travel at most at the speed of light
suspected stars and bars problem considered incorrect, in need for some further insight
How could a city build a circular canal?
Why are Jersey and Guernsey not considered sovereign states?
Is there greater explanatory power in laws governing things rather than being descriptive?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
Can a hyphen be a ""letter"" in some words?
White perpetual check, where Black manages a check too?
Is outer space Radioactive?
Does an NEC load calculation overage mandate a service upgrade?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Bash script that waits until GPU is free
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Math contents does not align when subscripts are used
Useful aerial recon vehicles for newly colonized worlds
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
database - Realm migration with big data base - Stack Overflow,"database - Realm migration with big data base - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Realm migration with big data base
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
37 times
1
I need to implement a migration with processing a large amount of data (more than 2,000,000 records). When I check the migration the memory grows linearly to 2.4 GB (iPad Pro 12.9 1 gen) and the application crashes. How can I split the migration into several steps so that it doesn't consume so much memory? I tried to convert blob data to several realm models.
Before:
@interface RealmDataModel : RLMObject

@property NSData *data; // The data stored in JSON format with different types

@end
After:
@interface RealmDataModel : RLMObject

@property RLMArray<RealmNew1Model *>< RealmNew1Model > *type1;
@property RLMArray<RealmNew2Model *>< RealmNew2Model > *type2;
@property RLMArray<RealmNew3Model *>< RealmNew3Model > *type3;

@end
Migration:
[migration enumerateObjects:@""RealmDataModel"" block:^(RLMObject *oldObject, RLMObject *newObject) {  
    NSArray *dataArray = [NSJSONSerialization JSONObjectWithData:oldObject.data options:NSJSONReadingAllowFragments error:error];
    for (NSDictionary *someObject in dataArray) {
        if (someObject[@""type""] == @(1)) {
            RLMObject *new1Object = [migration createObject:@""RealmNew1Model"" withValue:@{}];
            // convert someObject to the RealmNew1Model
        } else if (someObject[@""type""] == @(2)) {
            RLMObject *new2Object = [migration createObject:@""RealmNew2Model"" withValue:@{}];
            // convert someObject to the RealmNew2Model
        } else if (someObject[@""type""] == @(3)) {
            RLMObject *new3Object = [migration createObject:@""RealmNew3Model"" withValue:@{}];
            // convert someObject to the RealmNew3Model
        }
    }
}];
database
migration
realm
Share
Improve this question
Follow
asked
Jul 31, 2020 at 11:08
Vladimir Goncharov
Vladimir Goncharov
35
6
6 bronze badges
Add a comment
|
Related questions
1
How to migrate Realm in Android
3
Realm model migration strategy
2
Performing a simple migration in Realm
Related questions
1
How to migrate Realm in Android
3
Realm model migration strategy
2
Performing a simple migration in Realm
1
Realm migration android
3
How to migrate data from realm when updating application
4
Realm migration with new schema
0
Android Realm migrating from 1.x to 2.x
1
Realm Migration: Migrating objects to another
3
how can i Migrate Realm without losing any data - Kotlin
0
Realm Swift Bundle Data
Load 7 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
database
migration
realm
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How much of a structural/syntactic difference is there between an oath and a promise?
Find a fraction's parent in the Stern-Brocot tree
What technique is used for the heads in this LEGO Halo Elite MOC?
How *exactly* is divisibility defined?
What does “going off” mean in ""Going off the age of the statues""?
reverse engineering wire protocol
What is meaning of forms in ""they are even used as coil forms for inductors?""
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
What's a modern term for sucker or sap?
UUID v7 Implementation
How to estimate the latency of communication?
How should I connect a light fixture with UK wire colors to US wiring?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Topology of a horocycle
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
Indian music video with over the top CGI
When was ""to list"" meaning ""to wish"" lost?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
A Pandigital Multiplication
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Is outer space Radioactive?
suspected stars and bars problem considered incorrect, in need for some further insight
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
mysql - Regarding NodeJs and big data - Stack Overflow,"mysql - Regarding NodeJs and big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Regarding NodeJs and big data [closed]
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
60 times
-2
Closed
. This question is
opinion-based
. It is not currently accepting answers.
Want to improve this question?
Update the question so it can be answered with facts and citations by
editing this post
.
Closed
4 years ago
.
Improve this question
Currently, I'm developing one project and there are lots of MySQL query operations with billions of records and also mathematic operation included and it takes more time to perform the query.
so I need your help in choosing technology for big data and DB operation
Currently, I'm using nodejs and MySQL DB
Thanks for giving me the right way to develop this.
mysql
node.js
bigdata
Share
Improve this question
Follow
asked
Jul 28, 2020 at 13:09
chirag sorathiya
chirag sorathiya
1,243
11
11 silver badges
31
31 bronze badges
1
That depends on the nature of your data. e.g. if you are having meta data regarding entities or you have unstructured data, but all in all you can improve backend storage of your data at this point.
–
Zeeshan
Commented
Jul 30, 2020 at 10:20
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
It depends on your data. If your data is homogeneous (most of the rows has the same number of columns) and you need to perform complex queries with tons of joins, using a relational database as MySQL is a good option. You can also try other relational databases like Oracle DB, MariaDB and others. It shouldn't be difficult to export your current database and try if the performance improves.
On the other way, if your data is heterogenous and you don't need to perform complex join queries, a NoSQL database can be your option. There are a lot of them but one of the most famous ones is MongoDB. Moreover, Mongo has very good integration with NodeJS. Your main problem would be to convert your actual relational database to a non-relational database.
Share
Improve this answer
Follow
edited
Jul 28, 2020 at 13:35
Dharman
♦
33.1k
27
27 gold badges
99
99 silver badges
146
146 bronze badges
answered
Jul 28, 2020 at 13:29
Pablo RP
Pablo RP
38
5
5 bronze badges
1
Hello @pabol rp thanks for answer. can we use spark SQL? it is good to perform for fast result ?
–
chirag sorathiya
Commented
Jul 28, 2020 at 16:20
Add a comment
|
Not the answer you're looking for? Browse other questions tagged
mysql
node.js
bigdata
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
5483
What's the difference between tilde(~) and caret(^) in package.json?
3203
Should I use the datetime or timestamp data type in MySQL?
3107
How do I pass command line arguments to a Node.js program and receive them?
2961
What's the difference between dependencies, devDependencies, and peerDependencies in NPM package.json file?
2201
How can I update Node.js and npm to their latest versions?
1781
How do I completely uninstall Node.js, and reinstall from beginning (Mac OS X)
1561
What is the purpose of Node.js module.exports and how do you use it?
722
What is ""require"" in JavaScript and NodeJS?
1414
Which MySQL data type to use for storing boolean values
Hot Network Questions
What technique is used for the heads in this LEGO Halo Elite MOC?
How could a city build a circular canal?
When was ""to list"" meaning ""to wish"" lost?
Why are Jersey and Guernsey not considered sovereign states?
A Pandigital Multiplication
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
How much of a structural/syntactic difference is there between an oath and a promise?
How to delete edges of curve based on their length
Passphrase entropy calculation, Wikipedia version
How to format numbers in monospaced (typewriter) font using siunitx?
Should I expect a call from my future boss after signing the offer?
When looking at the first DCM page, where is the next DCM page documented?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Challah dough bread machine recipe issues
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
How does this Paypal guest checkout scam work?
When to start playing the chord when a measure starts with a rest symbol?
Need an advice to rig a spaceship with mechanicals part
Physical interpretation of selection rules for different multipole orders
What should machining (turning, milling, grinding) in space look like
What is meaning of forms in ""they are even used as coil forms for inductors?""
How should I connect a light fixture with UK wire colors to US wiring?
Is there greater explanatory power in laws governing things rather than being descriptive?
Why does it take so long to stop the rotor of a helicopter after landing?
more hot questions
default
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
apache spark - Analyse input data and find errors in input in big data - Stack Overflow,"apache spark - Analyse input data and find errors in input in big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Analyse input data and find errors in input in big data
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
112 times
0
I have a large amount of data( about 20 Gig ) in some text files, and I have all 20 Gig data at once so I don't want to process them partially as they are created, each line in text file specify one record, but this record is in text format and attributes of data are concatenated to each other, this is the sample line in one text file:
00000000015tashkhisi---8686745428M.....
First 10 characters show the record number => 0000000001
Character at position 11 show type of record => 5
Next 12 characters shows owner of the record => tashkhisi---
Next 10 characters shows the identity of the owner => 8686745428
Next character shows Gender of the owner => M
and so on ......
This is only an example and you can consider my record has 100 columns or attributes in real data.
But like any other input in big data there are noise in input data and some records are incorrect and have problem, for instance we may have ++--0000001 as record number which is an invalid record number.
Now what I want:
I want to find all errors in my input data and report them in aggregated manner like this:
Record with tashkhisi as owner -> has x% of total Error
Record with Gender M -> has y% of total Error
Error in owner name where record type is 5 => 250000 (y% total)
Error in owner identity when record  type is 5 => 4000000(x% total)
Error in owner name where record type is 3 => 250000 (k% total)
Error in owner identity when record  type is 3 => 4000000( z% total)
......
Total number of name Error => 10000000 with percentage 6%
and so on ....
You can see attributes are interdependent and I need large amount of
group by
and
substring
in my program, Also I want the report to be generated very fast, the size of input data is 20 Gig and I can't load the whole data in memory to work with them.
I know I can use
Spark
,
Elasticsearch
and
logstash
,
mapreduce
and so on. But I want to know which one is better in doing this specific task and why? I want to know what is the best approach to generate this report, Is there any tool to generate this report very fast? Please give me your reason why you suggest that tool.
apache-spark
elasticsearch
pyspark
bigdata
data-analysis
Share
Improve this question
Follow
edited
Jul 28, 2020 at 5:29
Shubham Jain
5,506
2
2 gold badges
18
18 silver badges
41
41 bronze badges
asked
Jul 28, 2020 at 4:22
Tashkhisi
Tashkhisi
2,214
1
1 gold badge
9
9 silver badges
25
25 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
You can use spark to read the data as a dataframe and once the dataframe is ready yoou can perform the valid and invalid checks over that to generate the required statistics.
Now the tricky part is to generate the dataframe, to do so you can do the following.
df = spark.read.text(r""C:\stackoverflow\samples\fixedWidth.txt"")

df.select(
    df.value.substr(1,10).alias('RecordNumber'),
    df.value.substr(11,1).alias('Type'),
    df.value.substr(12,12).alias('Owner'),
    df.value.substr(24,10).alias('identity'),
    df.value.substr(34,1).alias('gender')
).show()

+------------+----+------------+----------+------+
|RecordNumber|Type|       Owner|  identity|gender|
+------------+----+------------+----------+------+
|  0000000001|   5|tashkhisi---|8686745428|     M|
|  ++--000001|   5|tashkhisi---|8686745428|     M|
|  _+00000001|   5|tashkhisi---|++86745428|     M|
|  0000000001|   5|tashkhisi---|8686745428|     M|
|  0000000001|   5|tashkhisi---|8686745428|     M|
|  0000000001|   5|tashkhisi---|8686745428|     M|
|  0000000001|   5|tashkhisi---|8686745428|     M|
+------------+----+------------+----------+------+
Now from this dataframe, there are multiple opportunities, you can directly generate the stats over this using spark only or if you have an active ES cluster then load into ES(not recommended though)
ES is good if your data is json structured and you want a fast search feature over your dataset. For aggregations and all ES provides approximate results.
Share
Improve this answer
Follow
answered
Jul 28, 2020 at 5:33
Shubham Jain
Shubham Jain
5,506
2
2 gold badges
18
18 silver badges
41
41 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
apache-spark
elasticsearch
pyspark
bigdata
data-analysis
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
1
Large Data Analytics
1
Apache Spark (scala) + python/R work flow for data analysis
3
Effective Way to Validate Field Values Spark
1
Prepare my bigdata with Spark via Python
30
How to handle an AnalysisException on Spark SQL?
0
Identifying changes in large amounts of data using pyspark
1
Define Data Quality Rules for Big Data
0
Evaluation of the method working with the DataFrame
1
How to collect bad rows/records in pyspark while processing?
0
Checking 200 million data using Pyspark, takes a lot of time
Hot Network Questions
How to achieve infinite rage?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Indian music video with over the top CGI
What technique is used for the heads in this LEGO Halo Elite MOC?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Didactic tool to play with deterministic and nondeterministic finite automata
When looking at the first DCM page, where is the next DCM page documented?
Is outer space Radioactive?
Math contents does not align when subscripts are used
How manage inventory discrepancies due to measurement errors in warehouse management systems
How to estimate the latency of communication?
Find all unique quintuplets in an array that sum to a given target
Should I expect a call from my future boss after signing the offer?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
How to format numbers in monospaced (typewriter) font using siunitx?
Topology of a horocycle
How should I connect a light fixture with UK wire colors to US wiring?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Why are Jersey and Guernsey not considered sovereign states?
suspected stars and bars problem considered incorrect, in need for some further insight
Challah dough bread machine recipe issues
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
r - How to fit hierarchical models on big data with repeated observations - Stack Overflow,"r - How to fit hierarchical models on big data with repeated observations - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to fit hierarchical models on big data with repeated observations
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
236 times
Part of
R Language
Collective
0
I am working with online behavioral data where each user has multiple Bernoulli trials. I am familiar with fitting hierarchical models using
lme4
in R, but now that my dataset has ~1MM unique users and 1-10 observations each, the
lme4
model is running endlessly on my Macbook Pro. I had previously only ever fit such models to a few thousand users and run time was manageable.
library(lme4)
glmer(outcome ~ treatment + (1|user_id), family = 'binomial', data = mydata)
How might I practically approach fitting a hierarchical model to such a large dataset?
r
bigdata
hierarchical-data
lme4
Share
Improve this question
Follow
asked
Jul 22, 2020 at 21:28
Joe
Joe
3,776
4
4 gold badges
23
23 silver badges
45
45 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
There are a few ways to speed up a
glmer
:
Try setting
nAGQ = 0
within the
glmer
call
Try specifying
""nloptwrap""
as your optimizer in
glmerControl
Try specifying
calc.derivs = F
in
glmerControl
More info here
# code example
glmer(
    outcome ~ condition + (1|user_id),
    family = ""binomial"", 
    data = mydata, 
    nAGQ = 0,
    control = glmerControl(optimizer = ""nloptwrap"", calc.derivs = FALSE)
)
Share
Improve this answer
Follow
edited
Jul 23, 2020 at 20:54
answered
Jul 23, 2020 at 1:58
Walker Harrison
Walker Harrison
537
3
3 silver badges
12
12 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
r
bigdata
hierarchical-data
lme4
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
7
using glmer for nested data
1
How do I code the individual in to an lme4 nested model?
10
fitting a linear mixed model to a very large data set
2
Simulation from a nested glmer model
5
Three-level partially nested model
1
R code if my random effect is nested under another random effect
6
Big data: generalized linear mixed-effects models
0
How to fit quickly mixed models by using an iterative method on a long format dataset
3
Multilevel model for repeated measures data using lme4 in R
2
R: Modeling random intercepts from lme4 or brms objects
Hot Network Questions
How can we be sure that the effects of gravity travel at most at the speed of light
Humans try to help aliens deactivate their defensive barrier
How to keep meat in a dungeon fresh, preserved, and hot?
Protecting myself against costs for overnight weather-related cancellations
Two types difinition of the distance function
How does this Paypal guest checkout scam work?
Is it possible that the committee contacts only one reference while applicants need to provide two?
Is there greater explanatory power in laws governing things rather than being descriptive?
What is ""B & S"" a reference to in Khartoum?
How much of a structural/syntactic difference is there between an oath and a promise?
Is outer space Radioactive?
How to explain why I don't have a reference letter from my supervisor
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
UUID v7 Implementation
What technique is used for the heads in this LEGO Halo Elite MOC?
Find all unique quintuplets in an array that sum to a given target
How can dragons heat their breath?
What does “going off” mean in ""Going off the age of the statues""?
Why are Jersey and Guernsey not considered sovereign states?
Covering a smoke alarm horn
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Why is the speed graph of a survey flight a square wave?
Math contents does not align when subscripts are used
What's the difference between '\ ' and tilde character (~)?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-r
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
esp idf - Sending big data amount to Google cloud IOT core - Stack Overflow,"esp idf - Sending big data amount to Google cloud IOT core - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Sending big data amount to Google cloud IOT core
Ask Question
Asked
4 years, 6 months ago
Modified
4 years, 4 months ago
Viewed
132 times
Part of
Google Cloud
Collective
1
I am developing a project with the esp32, that is measuring sensor data, saving this data to an sd and also publishes the data to the web (to Google cloud IOT core).
The measuring and saving of the data works already very well, but I am facing some issues regarding the sending to the web.
To measure the data (max frequency should be 500 Hz), I am using a timer, that measures the data periodically and sends the measured data to a queue. A separate task reads the data from the queue and then saves this data formatted to the sd card. So far so good.
I am using esp-idf for the development and therefore implemented the web framework esp-google-iot. Connecting to the cloud worked very well and I am able to receive commands already (in the first approach just a command to stop or start with measuring). Now I tried to create another queue, where the data, that should be published is hold. Then I created a delayed publish task, that should send all the data, stored in the queue every second to the cloud. Unfortunately I am always receiving the error ""heap out of memory"".
As I thought this could appear due to to big data amount, I removed the second queue and just sent the message ""dummy"" every second. This worked perfectly.
Does anyone know, how to handle these amount of data or had similar issues?
Probably I am also doing the completely wrong approach.... Probably sending more than just one value per publish, so that I do not have to send 500 requests per second?
Any help is appreciated
google-cloud-iot
esp-idf
Share
Improve this question
Follow
edited
Jun 30, 2020 at 17:55
Brian Tompsett - 汤莱恩
5,875
72
72 gold badges
60
60 silver badges
133
133 bronze badges
asked
Jun 10, 2020 at 23:50
ledtom
ledtom
19
1
1 bronze badge
1
Could you help us by posting some code which you use? Ideally, this should be a minimal working example, it doesn't need have real data, though. Yes, probably sending 500 requests/s may not be a good idea. Please have a look here if you need more information on your device's heap usage:
docs.espressif.com/projects/esp-idf/en/latest/esp32/…
–
StrawHat
Commented
Jun 30, 2020 at 6:15
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
Depending on what you're doing on IoT Core, you could send the whole queue in one JSON string and use a Cloud function to parse through the JSON and move the data to BigQuery. I think this might solve the problem that you're having although i suggest just publishing the the data whenever your device has read it.
Also you might want to space out your publishing to more then every second since as StrawHat pointed out sending 500 request/s is not a good idea and would probably cause IoT Core to reject the connection.
Share
Improve this answer
Follow
answered
Jul 23, 2020 at 17:13
Gal Zahavi
Gal Zahavi
66
2
2 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
google-cloud-iot
esp-idf
or
ask your own question
.
Google Cloud
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
1
How to upload milions object to google cloud storage
1
Send data from cloud to aws iot thing
1
How to transfer huge json from instance to Bigtable in goole cloud
1
Store large IoT data at high frequency to the cloud
0
GCP - IOT Core Monitoring messages
2
Google Cloud IoT Gateway Connection Out Of Memory
0
GCP IoT Error on sending new device configuration: ""Error 413 (Request Entity Too Large)!!""
0
How to send sensor data (like temperature data from DHT11 sensor) to Google Cloud IoT Core and store it
0
How to upload large files to Google Cloud Storage with GAE/SE go112
1
upload greater than 5TB object to google cloud storage bucket
Hot Network Questions
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
How much of a structural/syntactic difference is there between an oath and a promise?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
How to explain why I don't have a reference letter from my supervisor
How to keep meat in a dungeon fresh, preserved, and hot?
When looking at the first DCM page, where is the next DCM page documented?
suspected stars and bars problem considered incorrect, in need for some further insight
Convert pipe delimited column data to HTML table format for email
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Can a hyphen be a ""letter"" in some words?
Building a Statistically Sound ML Model
Heating object in airless environment
Useful aerial recon vehicles for newly colonized worlds
What does it mean when folks say that universe is not ""Locally real""?
Math contents does not align when subscripts are used
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
How to estimate the latency of communication?
What does “going off” mean in ""Going off the age of the statues""?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Topology of a horocycle
How does this Paypal guest checkout scam work?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
Query exceeded resource limits in BigQuery GROUP BY on big data - Stack Overflow,"Query exceeded resource limits in BigQuery GROUP BY on big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Query exceeded resource limits in BigQuery GROUP BY on big data
Ask Question
Asked
4 years, 4 months ago
Modified
4 years, 4 months ago
Viewed
989 times
Part of
Google Cloud
Collective
0
I have two tables -
StringList
which has only one column
SearchString
abc
def
ghi
lmn
I have one other table
Phrase
with one column
Phrase
xyzabcefg
ddflkd
fdjkd
abcdefghi
extreme
I need an output table which will have two columns -
SearchString
Phrases
Search string will be search string from first table and Phrases will be all data from 2nd table where Search string occurs in Phrase
Something like below -
SearchString Phrases
abc   [xyzabcefg, abcdefghi]
def   [abcdefghi]
ghi   [abcdefghi]
I am able to get the results but the query is not optimized at all, I have around 2.4 M records in 2nd table and 7.5K in 1st table.
I need to scale this query to 240M records in 2nd table but this query doesn't work even on 1.5 M records right now.
Gives following error -
Query exceeded resource limits. 13256.532365742061 CPU seconds were used, and this query must use less than 9200.0 CPU seconds.
I tried various things but no success. My current query is -
With first_batch as (
SELECT
a.Phrase,
b.SearchString
FROM `project-id.DS_WORK.Phrase`  a
join `project-id.DS_WORK.StringList` b
ON a.Phrase LIKE '%' || b.SearchString || '%' )
SELECT
first_batch.SearchString,
'[' || STRING_AGG(first_batch.Phrase) || ']' AS Phrases
FROM first_batch
GROUP BY first_batch.SearchString;
Any help will be appreciated.
I am thinking of using loops in User-defined functions but I first want to check if there is a simpler way to achieve this.
google-bigquery
query-optimization
Share
Improve this question
Follow
edited
Jul 20, 2020 at 21:02
y0j0
3,562
6
6 gold badges
37
37 silver badges
55
55 bronze badges
asked
Jul 18, 2020 at 13:37
Vivekanand Joshi
Vivekanand Joshi
117
7
7 bronze badges
7
Can you share the execution details for your query so we can check which part is consuming more processing power?
–
Alexandre Moraes
Commented
Jul 20, 2020 at 8:55
Edited the question to include image.
–
Vivekanand Joshi
Commented
Jul 20, 2020 at 17:48
I did not see any abnormal activity through this execution log. Although, there are practices that make your query perform better such as select only the columns your desire, filter your data before joining and more. Here is the link for [documentation][
cloud.google.com/bigquery/docs/…
. I created a new query which I believe it can be faster and thus do not exceed the CPU seconds, I will share in the next comment.
–
Alexandre Moraes
Commented
Jul 21, 2020 at 13:36
DECLARE str1 ARRAY<STRING>; SET str1 = ( SELECT  ARRAY_AGG(DISTINCT str order by str)  FROM test-proj-261014.bq_load_codelab.string );  Select str, ARRAY_AGG(phrase) as phrases from project_id.data_set.string t1 cross join  project_id.data_set.phrase  t2  where t1.str = REGEXp_EXTRACT(t2.phrase, t1.str) group by str
. Remember to put back the back quotes when referencing the tables, I had to remove them due to formatting. Did it work for you?
–
Alexandre Moraes
Commented
Jul 21, 2020 at 13:37
I had only 1 column in both the tables.
–
Vivekanand Joshi
Commented
Jul 22, 2020 at 17:55
|
Show
2
more comments
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
In the BigQuery you should avoid using
group by
on a big data. The reason is the data are processed on distributed nodes that are small. Functions like group by or order by need all the data on one node that is not possible.
Try use window function instead. I would rewrite your query like
With first_batch as (
SELECT
a.Phrase,
b.SearchString
FROM `project-id.DS_WORK.Phrase`  a
join `project-id.DS_WORK.StringList` b
ON a.Phrase LIKE '%' || b.SearchString || '%' )
SELECT 
a.SearchString,
'[' || a.Phrases || ']' AS Phrases
FROM ( SELECT 
first_batch.SearchString as SearchString,
ROW_NUMBER() OVER(PARTITION BY first_batch.Phrase) AS rn,
STRING_AGG(first_batch.Phrase)
  OVER (
    PARTITION BY first_batch.Phrase
  ) AS Phrases
FROM first_batch ) AS a
WHERE a.rn = 1
If you need to try it on simple query with real data use this:
#standardSQL
WITH `table` AS (
  SELECT '1' AS UserId, 'A' AS Name UNION ALL
  SELECT '1' AS UserId, 'B' AS Name UNION ALL
  SELECT '1' AS UserId, 'C' AS Name UNION ALL
  SELECT '2' AS UserId, 'A' AS Name UNION ALL
  SELECT '3' AS UserId, 'B' AS Name
)
SELECT a.UserId, a.Name FROM (SELECT
UserId,
ROW_NUMBER() OVER(PARTITION BY UserId) AS rn,
STRING_AGG(Name)
  OVER (
    PARTITION BY UserId
  ) AS Name
from `table`) as a
where a.rn=1
If window functions do not help you can compose your query similar as following with using distinct and nested select
#standardSQL
WITH `table` AS (
  SELECT '1' AS UserId, 'A' AS Name UNION ALL
  SELECT '1' AS UserId, 'B' AS Name UNION ALL
  SELECT '1' AS UserId, 'C' AS Name UNION ALL
  SELECT '2' AS UserId, 'A' AS Name UNION ALL
  SELECT '3' AS UserId, 'B' AS Name
)
SELECT a.UserId,
(SELECT STRING_AGG(Name) from `table` as t where a.UserId = t.UserId) as Names
FROM (SELECT
DISTINCT UserId,
from `table`) as a
Share
Improve this answer
Follow
edited
Jul 21, 2020 at 9:16
answered
Jul 20, 2020 at 20:59
y0j0
y0j0
3,562
6
6 gold badges
37
37 silver badges
55
55 bronze badges
2
This query actually takes more CPU than group one.  ""Query exceeded resource limits. 16258.025859160658 CPU seconds were used, and this query must use less than 9200.0 CPU seconds.""
–
Vivekanand Joshi
Commented
Jul 21, 2020 at 5:33
that means some of the first_batch.Phrase group is too big. I added one more way to my answer how to write such query with using of DISTINCT and nested SELECT. Can you rewrite your query in such way? Is it work?
–
y0j0
Commented
Jul 21, 2020 at 9:17
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
google-bigquery
query-optimization
or
ask your own question
.
Google Cloud
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
11
Understanding ""Resources exceeded during query execution"" with GROUP EACH BY in BigQuery
0
Optimize query to avoid ""Resources exceeded during query execution""
2
BigQuery GROUP BY/GROUP EACH BY resources exceeded error, but the query doesn't contain a GROUP BY operator
2
Solved Resources exceeded in GROUP BY, but I need to make it less expensive
0
Google BigQuery Error: Resources exceeded during query execution
1
BigQuery - Resources exceeded during query execution, with Allow Large Results set
2
BigQuery - Group By with multiple fields extremely slow
6
Query exceeded resource limits in Bigquery
2
Bigquery resources exceeded during query execution - optimization
1
BIGQUERY - Query Exceeded resource limit
Hot Network Questions
Heating object in airless environment
Should I expect a call from my future boss after signing the offer?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
How to explain why I don't have a reference letter from my supervisor
How could a city build a circular canal?
Why did Crimea’s parliament agree to join Ukraine?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Passphrase entropy calculation, Wikipedia version
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
How did Jahnke and Emde create their plots
How much of a structural/syntactic difference is there between an oath and a promise?
White perpetual check, where Black manages a check too?
Only selecting Features that have another layers feature on top
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Can a hyphen be a ""letter"" in some words?
Why is the speed graph of a survey flight a square wave?
reverse engineering wire protocol
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
UUID v7 Implementation
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
Time travelling paedo priest novel
What are these 16-Century Italian monetary symbols?
How to format numbers in monospaced (typewriter) font using siunitx?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
swift - how to share big data with detail view? - Stack Overflow,"swift - how to share big data with detail view? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
how to share big data with detail view?
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 4 months ago
Viewed
27 times
0
I'm writing a video player support subtitles, There are two Views, VideoView and PlayView, main code is like the following, I want to pass subtitles to PlayView,
but the program becomes very slow. If I load var titles in PlayView not expose it to VideoView the program works ok. Is the titles too large? which is about 100k.
struct VideoView: View {
    @ObservedObject var videoItem: VideoItem = VideoItem()
    @State var titles:[Title] = []
    var body: some View {
        ZStack{
            if videoItem.playerItem != nil {
                
                PlayerView(player: $videoItem.player,titles:$titles)
                    .frame(maxWidth: .infinity, maxHeight: .infinity)
            }
}
}
playview.swift
public struct PlayerView: NSViewRepresentable {
    
   
    @Binding var titles:[Title] = []
}
the following code works, but can't access titles in VideoView
playview.swift
public struct PlayerView: NSViewRepresentable {
    
   
   var titles:[Title] = []
   public init(subtitleFile:Binding<String>)
  {
   
    let subtitles = Subtitles(filePath: self.subtitleFile)
    self.titles = subtitles.titles!

   }
   }
swift
Share
Improve this question
Follow
edited
Jun 25, 2020 at 2:49
龙方哲
asked
Jun 25, 2020 at 2:32
龙方哲
龙方哲
125
1
1 silver badge
9
9 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
@Binding support any large big data, It is just an address , My problem is caused by my own algorithm is too slow, which searches a subtitle in an array by the time.
Share
Improve this answer
Follow
answered
Jul 18, 2020 at 2:34
龙方哲
龙方哲
125
1
1 silver badge
9
9 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
swift
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
0
How should I make data available to all of my views?
20
Send data from TableView to DetailView Swift
92
How do you share data between view controllers and other objects in Swift?
22
Sharing data in between apps in IOS
0
Sharing big chunks of data iOS
0
Best way to store big data in Swift
1
Passing TableView Data to DetailViewController in swift
2
iOS sharing data between viewcontrollers
2
Passing Core Data objects from DetailViewController to another View Controller
0
send data from tableviewcontroller to detailviewcontroller
Hot Network Questions
What's a modern term for sucker or sap?
What does “going off” mean in ""Going off the age of the statues""?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
How to use a symbol as both function and head?
how do I correctly check that some aggregated results are correct?
How can we be sure that the effects of gravity travel at most at the speed of light
How can dragons heat their breath?
Derailleur Hangar - Fastener torque & thread preparation
Time travelling paedo priest novel
White perpetual check, where Black manages a check too?
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
Hole, YHWH and counterfactual present
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Is there greater explanatory power in laws governing things rather than being descriptive?
Indian music video with over the top CGI
What are these 16-Century Italian monetary symbols?
Only selecting Features that have another layers feature on top
Humans try to help aliens deactivate their defensive barrier
Why does it take so long to stop the rotor of a helicopter after landing?
What is meaning of forms in ""they are even used as coil forms for inductors?""
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Passphrase entropy calculation, Wikipedia version
Can two wrongs ever make a right?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-swift
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
linux - bash script optimization for big data - Stack Overflow,"linux - bash script optimization for big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
bash script optimization for big data
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
90 times
0
I have the follogwing litte Skript that forms date.
The Problem i face is that the Skript is used for big data log files and it takes far to long.
I changed the for loop from read to cat because its faster. But the data program is still far to slow. How can I improve this ?
IFS='
'
for i in $(cat ist.log | egrep -o ""^.{19}""); do 
        olddatum=${i}
        newdatum=`date -d ""${olddatum}"" +'%b %e %T'`
        echo ${newdatum} >> soll.log.fixed
done
linux
bash
shell
optimization
Share
Improve this question
Follow
asked
Jul 15, 2020 at 9:54
Nico
Nico
11
2
2 bronze badges
1
what is the format of
olddatum
?
–
jhnc
Commented
Jul 15, 2020 at 10:02
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
With the shell and
sed
and the
date
utility (most probably GNU
date
)
#!/bin/sh

search=$(sed ""s/^\(.\{19\}\).*/\1/"" file.txt)
replace=$(date -d ""$search"" '+%b %e %T')
sed ""s/^$seach/$replace /"" file.txt
With a while read loop using the shell. Don't read lines with
for
#!/usr/bin/env bash

while IFS= read -r lines; do
 if [[ $lines =~ ^(.{19})(.+) ]]; then
   printf '%s %s\n' ""$(date -d ""${BASH_REMATCH[1]}"" +'%b %e %T')"" ""${BASH_REMATCH[2]}""
 else
   printf '%s\n' ""$lines""
 fi
done < file.txt
Share
Improve this answer
Follow
edited
Jul 15, 2020 at 11:50
answered
Jul 15, 2020 at 11:12
Jetchisel
Jetchisel
7,771
2
2 gold badges
20
20 silver badges
18
18 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
linux
bash
shell
optimization
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
1
Bash script optimisation
1
bash script optimization
0
Bash running time optimization
0
Speeding Up Bash Scripts
1
Optimizing performance of an IO-heavy shell script
2
performance issues in shell script
1
Performance issues with bash script
1
Optimise Bash scripting for large data
0
Faster execution time for bash script
0
Speed up shell script/Performance enhancement of shell script
Hot Network Questions
Realization of fundamental group endomorphism
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
How can we be sure that the effects of gravity travel at most at the speed of light
What technique is used for the heads in this LEGO Halo Elite MOC?
What does “going off” mean in ""Going off the age of the statues""?
What is meaning of forms in ""they are even used as coil forms for inductors?""
How to delete edges of curve based on their length
How to explain why I don't have a reference letter from my supervisor
How much of a structural/syntactic difference is there between an oath and a promise?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
Why is the speed graph of a survey flight a square wave?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
Passphrase entropy calculation, Wikipedia version
Two types difinition of the distance function
How *exactly* is divisibility defined?
Why does it take so long to stop the rotor of a helicopter after landing?
Hole, YHWH and counterfactual present
Derailleur Hangar - Fastener torque & thread preparation
Topology of a horocycle
UUID v7 Implementation
How does this Paypal guest checkout scam work?
Convert pipe delimited column data to HTML table format for email
Does an NEC load calculation overage mandate a service upgrade?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-bash
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python - How to reduce the time taken working on a big Data Frame - Stack Overflow,"python - How to reduce the time taken working on a big Data Frame - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to reduce the time taken working on a big Data Frame
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
36 times
0
I think my code is inefficient and I think there may be a way to do it better.
The objective of the code is that it takes an Excel listing and has to relate each element of a column to the rest of the elements of the same column. Depending on some conditions store it in a new data frame with the joint information, in my case the file has more than 16000 rows, so when doing the exercise it must perform (16.000 x 16.000) 256.000.000 iterations. But it takes days processing.
The code I have is the following:
import pandas as pd                                         
import numpy as np


excel1=""Cs.xlsx""                                                 

dataframe1=pd.read_excel(excel1)                                

col_names=[""Eb"",""Eb_n"",""Eb_Eb"",""L1"",""Ll1"",""L2"",""Ll2"",""D""]
my_df =pd.DataFrame(columns=col_names)                          

count_row = dataframe1.shape[0] 

print(count_row)

for n in range(0,count_row):

    for p in range(0,count_row):
        if abs(dataframe1.iloc[n,1] - dataframe1.iloc[p,1]) < 0.27 and abs(dataframe1.iloc[n,2] - 
            dataframe1.iloc[p,2]) < 0.27:           
            Nb_Nb=dataframe1.iloc[n,0]+""_""+dataframe1.iloc[p,0]     
            myrow=pd.Series([dataframe1.iloc[n,0],dataframe1.iloc[p,0],Nb_Nb,dataframe1.iloc[n,1],
            dataframe1.iloc[n,2],dataframe1.iloc[p,1],dataframe1.iloc[p,2]],   
            index=[""Eb"",""Eb_n"",""Eb_Eb"",""L1"",""Ll1"",""L2"",""Ll2""])
            my_df = my_df.append(myrow, ignore_index=True)          
    
print(my_df.head(5))
python
pandas
performance
Share
Improve this question
Follow
edited
Jul 15, 2020 at 8:17
mkrieger1
22.7k
6
6 gold badges
63
63 silver badges
79
79 bronze badges
asked
Jul 15, 2020 at 1:22
Centella
Centella
9
1
1 bronze badge
1
What does ""573/5000"" mean?
–
mkrieger1
Commented
Jul 15, 2020 at 8:15
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
To start with, you can try using a different python structure. Dataframes take a lot of memory and are slower to process.
Order from simple structures and more efficient processing to complex structures and less efficient processing
Lists
Dictionaries
Numpy Arrays
Pandas Series
Pandas Dataframes
Share
Improve this answer
Follow
answered
Jul 15, 2020 at 4:13
Donald S
Donald S
1,753
1
1 gold badge
12
12 silver badges
26
26 bronze badges
2
Depending on what they are used for, NumPy arrays and Pandas dataframes are more efficient to work with than lists and dictionaries.
–
mkrieger1
Commented
Jul 15, 2020 at 8:17
@mkrieger1 - you are correct, the simplicity and efficiency comes with a cost. It will be necessary to weigh the pros and cons of either choice.
–
Donald S
Commented
Jul 15, 2020 at 8:39
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
pandas
performance
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
5
Faster processing of Dataframe in Pandas
0
Making Dataframe Analysis faster
0
Reduce execution time of pandas code
2
Iterating over very large dataframe efficiency in python pandas is too time consuming
1
How to improve performance while iterating through a pandas data frame?
2
How to speed up DataFrame setting?
0
Make piece of code efficient for big data
1
Python Pandas improving calculation time for large datasets currently taking ~400 mins to run
0
Optimize Python code for faster processing
1
Panda DataFrame, How to make it faster?
Hot Network Questions
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Covering a smoke alarm horn
Convert pipe delimited column data to HTML table format for email
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
When to start playing the chord when a measure starts with a rest symbol?
How to achieve infinite rage?
How to set individual columns in the siunitx package to boldface? It it a bug?
What technique is used for the heads in this LEGO Halo Elite MOC?
When to use cards for communicating dietary restrictions in Japan
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
How can dragons heat their breath?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
Time travelling paedo priest novel
Challah dough bread machine recipe issues
What is meaning of forms in ""they are even used as coil forms for inductors?""
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
UUID v7 Implementation
Is outer space Radioactive?
How does this Paypal guest checkout scam work?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
What is ""B & S"" a reference to in Khartoum?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
How to update Teradata driver in Talend Big Data 7.0 - Stack Overflow,"How to update Teradata driver in Talend Big Data 7.0 - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to update Teradata driver in Talend Big Data 7.0
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
110 times
0
I am using Talend Big Data 7.0.1 , while connecting to teradata using teradataInput it is always selecting teradata driver 15.0.1.
How can i change teradata driver from 15.0.1 to 17.0.1 ?
teradata
talend
Share
Improve this question
Follow
asked
Jul 13, 2020 at 9:34
SUNNY
SUNNY
125
1
1 gold badge
4
4 silver badges
13
13 bronze badges
3
Far from an expert on Talend, but isn't the driver specified in the database connection?
–
Andrew
Commented
Jul 13, 2020 at 19:25
Import the new .jar file using the ""Modules"" view?
–
Fred
Commented
Jul 14, 2020 at 19:07
Importing jars in ""modules"" did not work for me . But I removed the old jars from .m2 repository and created a new job. When I add new teradataInput it gave me an option to download missing jars. After downloading and restart, it worked
–
SUNNY
Commented
Jul 15, 2020 at 20:56
Add a comment
|
Related questions
8
How can I use tFileExist component in Talend?
1
Talend open studio - how to proceed with next step
0
Talend's tOracleInput does not read data
Related questions
8
How can I use tFileExist component in Talend?
1
Talend open studio - how to proceed with next step
0
Talend's tOracleInput does not read data
2
Talend tMSSQLInput
1
Installing Teradata Database
0
export jobs from talend DI to talend big data
1
How to update key value in talend with tmysqloutput component
0
Talend BigData Error on Run
1
Talend Installtion on Ubuntu
0
Talend Open Studio : timpala component missing
Load 7 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
teradata
talend
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Why does it take so long to stop the rotor of a helicopter after landing?
suspected stars and bars problem considered incorrect, in need for some further insight
Topology of a horocycle
Realization of fundamental group endomorphism
Passphrase entropy calculation, Wikipedia version
Can two wrongs ever make a right?
What does it mean when folks say that universe is not ""Locally real""?
Can we judge morality?
Why did Crimea’s parliament agree to join Ukraine?
how do I correctly check that some aggregated results are correct?
Protecting myself against costs for overnight weather-related cancellations
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Why is the speed graph of a survey flight a square wave?
How can dragons heat their breath?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
When to start playing the chord when a measure starts with a rest symbol?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
When looking at the first DCM page, where is the next DCM page documented?
Need an advice to rig a spaceship with mechanicals part
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How to estimate the latency of communication?
What is meaning of forms in ""they are even used as coil forms for inductors?""
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
firebase - Firestore Deadline Exceeded Node - Stack Overflow,"firebase - Firestore Deadline Exceeded Node - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Firestore Deadline Exceeded Node
Ask Question
Asked
5 years, 9 months ago
Modified
5 years, 9 months ago
Viewed
2k times
Part of
Google Cloud
Collective
2
I would like to load collection that is ~30k records. I.e load it via.
const db = admin.firestore();
let documentsArray: Array<{}> = [];
db.collection(collection)
  .get()
  .then(snap => {
    snap.forEach(doc => {
      documentsArray.push(doc);
    });
  })
  .catch(err => console.log(err));
This will always throw
Deadline Exceeded
error. I have searched for some sorts of mechanism that will allow me to paginate trough it but I find it unbelievable not to be able to query for not that big amount in one go.
I was thinking that it may be that due to my rather slow machine I was hitting the limit but then I deployed simple express app that would do the fetching to app engine and still had no luck.
Alternatively I could also export the collection with
gcloud beta firestore export
but it does not provide JSON data.
firebase
google-app-engine
google-cloud-firestore
gcloud
Share
Improve this question
Follow
edited
Mar 12, 2019 at 10:12
Dan Cornilescu
39.8k
12
12 gold badges
60
60 silver badges
101
101 bronze badges
asked
Mar 12, 2019 at 8:39
ukaric
ukaric
428
1
1 gold badge
7
7 silver badges
22
22 bronze badges
3
Why do you find it unbelievable? It's expected on GAE:
cloud.google.com/appengine/articles/deadlineexceedederrors
. What environment?
–
Dan Cornilescu
Commented
Mar 12, 2019 at 10:10
Correct but ~30k recrods where each is aprox few kb's should not be an issue. On the other hand on much more beefy machine with 64gb of ram it still hangs.
–
ukaric
Commented
Mar 12, 2019 at 10:42
I'm having this issue with my localhost node app while reading only 1 single document.
–
kevinius
Commented
Jun 18, 2019 at 17:04
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
I'm not sure about firestore, but on datastore i was never able to fetch that much data in one shot, I'd always have fetch pages of about 1000 records at a time and build it up in memory before processing it. You said:
I have searched for some sorts of mechanism that will allow me to paginate trough
Perhaps you missed this page
https://cloud.google.com/firestore/docs/query-data/query-cursors
Share
Improve this answer
Follow
answered
Mar 13, 2019 at 20:23
Alex
Alex
5,276
13
13 silver badges
29
29 bronze badges
1
I was able to fetch the 30k records but on much beefier machine. If you get the response back under 60 sec you are ok.
–
ukaric
Commented
Mar 14, 2019 at 12:06
Add a comment
|
1
In the end the issue was that machine that was processing the 30k records from the Firestore was not powerful enough to get the data needed in time. Solved by using, GCE with
n1-standard-4
GCE.
Share
Improve this answer
Follow
answered
Mar 14, 2019 at 12:07
ukaric
ukaric
428
1
1 gold badge
7
7 silver badges
22
22 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
firebase
google-app-engine
google-cloud-firestore
gcloud
or
ask your own question
.
Google Cloud
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
0
Cloud Firestore big data error - Deadline Exceeded
Related
7
Cloud Functions with Firestore error ""Deadline Exceeded""
1
Firestore .get() resulting in deadline exceeded
5
Timeout for firestore operations
2
Error: deadline-exceeded when working with firebase cloud functions
10
Python Google cloud firestore error 504 Deadline Exceeded
9
Firestore: Error: 4 DEADLINE_EXCEEDED: Deadline exceeded
0
Firebase Cloud Function Errors: ""Deadline exceeded"" and ""Error code 16""
1
google-cloud/firestore: Error: 4 DEADLINE_EXCEEDED: Deadline Exceeded while creating document
3
Firebase Cloud Functions: ""4 DEADLINE_EXCEEDED: Deadline exceeded""
0
Handle Deadline exceeded Firebase Cloud Functions
Hot Network Questions
How to keep meat in a dungeon fresh, preserved, and hot?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
reverse engineering wire protocol
How to remove clear adhesive tape from wooden kitchen cupboards?
How *exactly* is divisibility defined?
How should I connect a light fixture with UK wire colors to US wiring?
Why does it take so long to stop the rotor of a helicopter after landing?
how do I correctly check that some aggregated results are correct?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Time travelling paedo priest novel
How manage inventory discrepancies due to measurement errors in warehouse management systems
Is outer space Radioactive?
Derailleur Hangar - Fastener torque & thread preparation
Passphrase entropy calculation, Wikipedia version
Topology of a horocycle
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Indian music video with over the top CGI
Can we judge morality?
Can two wrongs ever make a right?
Why is the speed graph of a survey flight a square wave?
Challah dough bread machine recipe issues
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Find all unique quintuplets in an array that sum to a given target
Does an NEC load calculation overage mandate a service upgrade?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
mongodb - Non-Relational Database Design For Big Data Warehouse - Stack Overflow,"mongodb - Non-Relational Database Design For Big Data Warehouse - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Non-Relational Database Design For Big Data Warehouse
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
144 times
0
Suppose I need to design a table for Spotify where I need to quickly retrieve what items (song or album) a user already purchased so it can play for the user. The scenario is straightforward: when users click to buy the song, the database needs to quickly update a particular song being purchased to the user account.
Since it really requires near real-time response and the table could be increased exponentially, on the other hand, the access format is quite simple and fix, a non-relational database is designed for this use case. That's why I am thinking about using HBase, Cassandra, or MongoDB.
I would like to use UserId as the primary key for this Purchase Table, will Wide Column Stores like (HBase or Cassandra) or Document databases like MongoDB work better for this scenario?
The input is just a user_id and the database table response with all available purchased items. What is the best database table design strategy?
{user_id:int
   {purchased_item: item1
                    item2
                    item3
   }
}
The second table will be used for searching for specific artists, albums, genres, and songs that are available for purchase.
Appreciate if you can share any examples of best practice from the real-world application. Or any good article/document/blogs I can read.
mongodb
cassandra
hbase
non-relational-database
Share
Improve this question
Follow
asked
Jul 11, 2020 at 12:52
Chelseajcole
Chelseajcole
547
2
2 gold badges
9
9 silver badges
18
18 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
If you are considering near real-time I would definitely consider using Cassandra especially for history detailed storage!
What I would do using Cassandra is the folowing:
CREATE TABLE purchases( user_id uuid, purchase_id uuid, item_id uuid, item_details text, item_name text, time_of purchase timestamp, PRIMARY KEY((user_id), purchase_id, item_id));
This will let you cluster the data in several ways first using the user_id then using the purchase_id to keep all items recorded per purchase!
By having the the Primary key formed of Partition key the user_id the clustering key the purchase_id and item_id we are able to group the items in the purchase_id and then in the user_id.
https://cassandra.apache.org/doc/latest/data_modeling/intro.html
https://docs.datastax.com/en/landing_page/doc/landing_page/current.html
Share
Improve this answer
Follow
answered
Jul 12, 2020 at 6:56
Planet 9
Planet 9
26
2
2 bronze badges
0
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
mongodb
cassandra
hbase
non-relational-database
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
Hbase schema design suggestion
1
cassandra highscore
2
Which nosql solution fits my application HBase OR Hypertable OR Cassandra
3
nosql: MongoDB, Cassandra or alternative for data warehousing
0
How to design a RDBMS for big data
0
Cassandra data modeling query
0
Data modeling in NoSQL Databases ( Cassandra DB )
8
No-SQL (Cassandra) data modelling of user data
0
What ""BigData"" solution should I use for this problem? Hbase? MongoDB? Others?
0
Best database for this use case? MongoDB does not appear to scale
Hot Network Questions
Covering a smoke alarm horn
What should machining (turning, milling, grinding) in space look like
Didactic tool to play with deterministic and nondeterministic finite automata
When was ""to list"" meaning ""to wish"" lost?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Protecting myself against costs for overnight weather-related cancellations
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
Bash script that waits until GPU is free
suspected stars and bars problem considered incorrect, in need for some further insight
How should I connect a light fixture with UK wire colors to US wiring?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
How does this Paypal guest checkout scam work?
Hole, YHWH and counterfactual present
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
UUID v7 Implementation
What does “going off” mean in ""Going off the age of the statues""?
How to use a symbol as both function and head?
How *exactly* is divisibility defined?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
What does it mean when folks say that universe is not ""Locally real""?
Can we judge morality?
Need an advice to rig a spaceship with mechanicals part
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-js
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
python - Make piece of code efficient for big data - Stack Overflow,"python - Make piece of code efficient for big data - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Make piece of code efficient for big data
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
56 times
0
I have the following code:
new_df = pd.DataFrame(columns=df.columns)
for i in list:
    temp = df[df[""customer id""]==i]
    new_df = new_df.append(temp)
where
list
is a list of customer id's for the customers that meet a criteria chosen before. I use the
temp
dataframe because there are multiple rows for the same customer.
I consider that I know how to code, but I have never learnt how to code for big data efficiency. In this case, the
df
has around 3 million rows and
list
contains around 100,000 items. This code ran for more than 24h and it was still not done, so I need to ask, am I doing something terribly wrong? Is there a way to make this code more efficient?
python
pandas
performance
Share
Improve this question
Follow
asked
Jul 11, 2020 at 23:33
user12195705
user12195705
147
2
2 silver badges
11
11 bronze badges
2
yes,
new_df = new_df.append(temp)
is very inefficient. It makes your algorithm quadratic time,
pandas.Dataframe.append
always creates whole-new dataframe. The most efficient way would probably be to make
'customer id'
column an index and simply select with your list
–
juanpa.arrivillaga
Commented
Jul 11, 2020 at 23:40
I simulated with 3 million record and 100000 customer ids. It takes only a few seconds with isin.
–
Pramote Kuacharoen
Commented
Jul 12, 2020 at 0:23
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
list
is a type in Python. You should avoid naming your variables with built-in types or functions. I simulated the problem with 3 million rows and a list of customer id of size 100000. It took only a few seconds using isin.
new_df = df[ df['customer id'].isin(customer_list) ]
Share
Improve this answer
Follow
edited
Jul 12, 2020 at 1:44
answered
Jul 11, 2020 at 23:46
Pramote Kuacharoen
Pramote Kuacharoen
1,541
1
1 gold badge
6
6 silver badges
6
6 bronze badges
1
You're absolutely right, this worked perfectly in just seconds. I didn't realize
isin
could be used this way, so thank you! The huge difference in time between the two codes still baffles me but I get it. Thanks again
–
user12195705
Commented
Jul 12, 2020 at 11:07
Add a comment
|
1
You can try this code below, which should make things faster.
new_df = df.loc[df['customer id'].isin(list)]
Share
Improve this answer
Follow
answered
Jul 11, 2020 at 23:46
rhug123
rhug123
8,758
1
1 gold badge
10
10 silver badges
26
26 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
python
pandas
performance
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
3
How can filter and retrieve specific records from big data efficiently using Python/Pyspark in Google Colab medium?
Related
0
Improving the speed/optimizing this code
0
Reduce execution time of pandas code
0
How to optimize below code to run faster, size of my dataframe is almost 100,000 data points
1
Can I make this code more efficient? It currently takes ~6 hours to run on ~1m entries
1
How can I make my pandas code more efficient?
0
Improving performance for my code with Pandas dataframe (column operation )
0
How to make code more efficient in terms of speed
0
Optimize Python code for faster processing
0
How can I make this python code more efficient?
0
Improving time efficiency of code, working with a big Data Set using Python
Hot Network Questions
Only selecting Features that have another layers feature on top
Realization of fundamental group endomorphism
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
When to start playing the chord when a measure starts with a rest symbol?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
Protecting myself against costs for overnight weather-related cancellations
How manage inventory discrepancies due to measurement errors in warehouse management systems
Topology of a horocycle
Can we judge morality?
How to use a symbol as both function and head?
How should I connect a light fixture with UK wire colors to US wiring?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Covering a smoke alarm horn
Should I expect a call from my future boss after signing the offer?
Indian music video with over the top CGI
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
White perpetual check, where Black manages a check too?
When looking at the first DCM page, where is the next DCM page documented?
UUID v7 Implementation
Find all unique quintuplets in an array that sum to a given target
When was ""to list"" meaning ""to wish"" lost?
Building a Statistically Sound ML Model
Bash script that waits until GPU is free
reverse engineering wire protocol
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
Database restoration problem on SQL Server Big Data Cluster - Stack Overflow,"Database restoration problem on SQL Server Big Data Cluster - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Database restoration problem on SQL Server Big Data Cluster
Ask Question
Asked
4 years, 6 months ago
Modified
4 years, 5 months ago
Viewed
191 times
1
I deployed a SQL Server Big Data Cluster (in Active Directory mode) on an on premise Kubernetes cluster (Kubernetes version = 1.18.2).
https://learn.microsoft.com/en-us/sql/big-data-cluster/deploy-active-directory?view=sql-server-ver15
The Kubernetes Cluster is made up of 3 (vSphere) VMs, 1 Master and 2 worker.
Also a 4th (NFS server) VM is configure as a Dynamic Storage Provider.
I managed to copy and restore databases from
.bak
files, but only when the database size is less than 15 GB.
https://learn.microsoft.com/en-us/sql/big-data-cluster/data-ingestion-restore-database?view=sql-server-ver15#copy-the-backup-file
My initial hunch was that the default value for (data) Persistent Volumes was 15 GB; so I customized that value to 100 GB and re-deployed the SQL Server BDC.
https://learn.microsoft.com/en-us/sql/big-data-cluster/concept-data-persistence?view=sql-server-ver15#configure-big-data-cluster-storage-settings
But still restoration of Databases larger than 15 GB fails with the following error...
PMStarted executing query at Line 1
  Msg 42019, Level 16, State 4, Line 2
RESTORE MANAGED DATABASE operation failed. Internal service error.
  Msg 3013, Level 16, State 1, Line 2
RESTORE DATABASE is terminating abnormally.
A side (2nd) question, is it possible to the change the edition/License of the deployed pods in BDC..???
Thanks in advance :)
sql-server
Share
Improve this question
Follow
edited
Jun 11, 2020 at 8:02
Emon
asked
Jun 8, 2020 at 17:18
Emon
Emon
19
2
2 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
Did you check if the volumes are created with the correct size (using kubectl)?
Regarding licensing, all pods will have the same edition/license as your master instance - unless I misunderstand your question?
Share
Improve this answer
Follow
answered
Jul 8, 2020 at 5:22
Ben Weissman
Ben Weissman
11
1
1 bronze badge
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
sql-server
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
Restore a SQL Server database from single instance to cluster
2
restoring database
1
problem with restore database - sql server
2
Some data loss with backup and restore in SQL Server 2008
55
SqlServer restore database error
0
Difficulty Restoring Database in SQL Server 2008 R2
0
Database Restore
0
Restore db in Microsoft SQL Server
1
DB restoration issue
1
SQL Server database restoration error
Hot Network Questions
Didactic tool to play with deterministic and nondeterministic finite automata
How did Jahnke and Emde create their plots
How can we be sure that the effects of gravity travel at most at the speed of light
White perpetual check, where Black manages a check too?
Math contents does not align when subscripts are used
US phone service for long-term travel
Physical interpretation of selection rules for different multipole orders
Is it possible that the committee contacts only one reference while applicants need to provide two?
How does this Paypal guest checkout scam work?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
When to use cards for communicating dietary restrictions in Japan
Can a hyphen be a ""letter"" in some words?
Convert pipe delimited column data to HTML table format for email
Can we judge morality?
Protecting myself against costs for overnight weather-related cancellations
Realization of fundamental group endomorphism
How much of a structural/syntactic difference is there between an oath and a promise?
How to achieve infinite rage?
How manage inventory discrepancies due to measurement errors in warehouse management systems
What are these 16-Century Italian monetary symbols?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
What's a modern term for sucker or sap?
Does an NEC load calculation overage mandate a service upgrade?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-sql
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
r - How to get some subset of data from a csv file for big-data(comparing csv's)? - Stack Overflow,"r - How to get some subset of data from a csv file for big-data(comparing csv's)? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to get some subset of data from a csv file for big-data(comparing csv's)? [duplicate]
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
338 times
Part of
R Language
Collective
0
This question already has answers here
:
How to join (merge) data frames (inner, outer, left, right)
(14 answers)
Inner join on two text files
(5 answers)
Closed
4 years ago
.
I had 2 csv files, first csv file has 16749 rows,with SYMBOLS and log as a columns. In the first csv it contains some junk data, which is not needed at all.
Second, is a clean csv file (no LOC11*) has 14107 rows, with SYMBOLS as only one column. I'd like to add for each SYMBOL ID it should find the corresponding logFC value from the first csv file.
I'm breaking my head, newbie to programming, unable to do it.
Some help please.
Example:
First csv
SYMBOL          log
LOC117745510   -0.35
LOC117741824   -0.54
ipmkb          -0.46
prrc2b

Second csv (log column to be added from 1st csv as reference)
SYMBOL          log
hs6st1a
ipmkb
prrc2b
r
bash
Share
Improve this question
Follow
asked
Jul 3, 2020 at 21:09
park
park
49
6
6 bronze badges
7
There is a command line utility called
join
in GNU coreutils. Just do on command line:
join <(sort first.csv) <(sort second.csv)
–
M. Nejat Aydin
Commented
Jul 3, 2020 at 22:08
Can you show some example, that I'd be great. Thanks.
–
park
Commented
Jul 4, 2020 at 0:31
I did already:
join <(sort first.csv) <(sort second.csv)
. first.csv and second.csv are filenames which contain the data as you specified in the question.
–
M. Nejat Aydin
Commented
Jul 4, 2020 at 0:38
Yes,I tried it not working. Tables looks same, I tried to write the output to the new file, it empty.
–
park
Commented
Jul 4, 2020 at 1:05
What character is the field separator in your data files?
–
M. Nejat Aydin
Commented
Jul 4, 2020 at 1:13
|
Show
2
more comments
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
Read in your data with an appropriate import function like
read.csv
and then join it.
library(dplyr)

## read your files (possibly you need to adjust some arguments in read.csv)
file1 <- read.csv(""path/to/file1.csv"", header = TRUE)
file2 <- read.csv(""path/to/file2.csv"", header = TRUE)

file2 %>%
  left_join(file1, by = ""SYMBOL)
Share
Improve this answer
Follow
edited
Jul 4, 2020 at 19:57
answered
Jul 3, 2020 at 21:28
eastclintw00d
eastclintw00d
2,364
1
1 gold badge
10
10 silver badges
19
19 bronze badges
0
Add a comment
|
Not the answer you're looking for? Browse other questions tagged
r
bash
or
ask your own question
.
R Language
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Linked
1558
How to join (merge) data frames (inner, outer, left, right)
13
Inner join on two text files
Related
6330
How do I get the directory where a Bash script is located from within the script itself?
888
Bash tool to get nth line from a file
615
How to extract directory path from file path?
664
How can I get a recursive full-path listing, one line per file?
638
How to get a password from a shell script without echoing
1
How to assign a ""keyword"" for the values starting with negative number for a column in R?
557
How to urlencode data for curl command?
446
How to limit depth for recursive file list?
Hot Network Questions
Does DOS require partitions to be aligned at a cylinder boundary?
A Pandigital Multiplication
How can dragons heat their breath?
Why did Crimea’s parliament agree to join Ukraine?
How should I connect a light fixture with UK wire colors to US wiring?
What are these 16-Century Italian monetary symbols?
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
What is ""B & S"" a reference to in Khartoum?
Protecting myself against costs for overnight weather-related cancellations
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Should I expect a call from my future boss after signing the offer?
White perpetual check, where Black manages a check too?
Realization of fundamental group endomorphism
How to explain why I don't have a reference letter from my supervisor
Is there greater explanatory power in laws governing things rather than being descriptive?
Heating object in airless environment
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Why is the speed graph of a survey flight a square wave?
Hole, YHWH and counterfactual present
Two types difinition of the distance function
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Topology of a horocycle
more hot questions
default
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"bigdata - Jmeter Hbase testing , How to preform the load testing for Big data? - Stack Overflow","bigdata - Jmeter Hbase testing , How to preform the load testing for Big data? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Jmeter Hbase testing , How to preform the load testing for Big data?
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
335 times
-1
I am new to jmeter Hbase load testing, i have install all plugins related to Hbase load testing, can any one help what exactly i need to do load testing on Hbase and what all need to capture the details related to Hbase load testing.
what are the scenarios i need to execute and how i can proceed the Hbase load testing.
jmeter
bigdata
performance-testing
load-testing
query-performance
Share
Improve this question
Follow
edited
Jun 29, 2020 at 13:16
D Md Siraj Ahmed
asked
Jun 29, 2020 at 12:45
D Md Siraj Ahmed
D Md Siraj Ahmed
1
1
1 bronze badge
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
JMeter doesn't support HBase testing out of the box, first of all you will need to install
Hadoop/HBase Testing
plugins, you can do this using
JMeter Plugins Manager
what are the scenarios i need to execute
- we don't know, you mention
load testing
and it means that you should put your HBase instance under
anticipated
load, in other words your test must represent real life HBase usage. You should ask your team for the details. If no one knows or unwilling to share the information you can execute the real life scenario using your system user interface or API or whatever and check HBase logs to get the relevant queries from there.
how i can proceed the Hbase load testing
- given you have the plugin and the queries you can use the relevant HBase test elements, most probably HBase Connection Config is a must and HBase CRUD Sampler will cover 99% of your needs. You might also be interested in
How to Load Test HBase with JMeter
article
Share
Improve this answer
Follow
answered
Jun 29, 2020 at 16:24
Dmitri T
Dmitri T
168k
5
5 gold badges
88
88 silver badges
146
146 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
jmeter
bigdata
performance-testing
load-testing
query-performance
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
Load Testing with JMeter
23
How to perform load testing for website using JMeter
1
jmeter regarding performance and load testing
3
How to set up a large amount of request data for a jMeter load test
0
What is Jmeter Test scenario for large request per second
2
Jmeter bottleneck
1
Jmeter load test with 30K users?
3
How to test performance with 1 million users in Jmeter?
1
Better jmeter report
0
How to generate huge dynamic test data using jmeter to perform stress testing
Hot Network Questions
A Pandigital Multiplication
How should I connect a light fixture with UK wire colors to US wiring?
Protecting myself against costs for overnight weather-related cancellations
Convert pipe delimited column data to HTML table format for email
When was ""to list"" meaning ""to wish"" lost?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
How could a city build a circular canal?
When looking at the first DCM page, where is the next DCM page documented?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
how do I correctly check that some aggregated results are correct?
Topology of a horocycle
Why does it take so long to stop the rotor of a helicopter after landing?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
How to remove clear adhesive tape from wooden kitchen cupboards?
Hole, YHWH and counterfactual present
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How much of a structural/syntactic difference is there between an oath and a promise?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
Heating object in airless environment
What's a modern term for sucker or sap?
Useful aerial recon vehicles for newly colonized worlds
How manage inventory discrepancies due to measurement errors in warehouse management systems
Can two wrongs ever make a right?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
How to rename mongoDB columns big data? - Stack Overflow,"How to rename mongoDB columns big data? - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
How to rename mongoDB columns big data?
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
308 times
0
I have a database with 15,574,934 records in mongo
I want to rename some columns to:
db.offerPhotos.files.update({}, {$rename: { 'orgFilename': 'metadata.orgFilename', 'offerId': 'metadata.offerId', 'batch': 'metadata.batch', 'group': 'metadata.group', 'size': 'metadata.size', 'mimeType': 'metadata.mimeType'}}, false, true)
I do it by mongo CLI but I'm waiting and waiting and nothing happens
How to do it better?
mongodb
rename
Share
Improve this question
Follow
edited
Jun 27, 2020 at 9:38
thammada.ts
5,235
3
3 gold badges
24
24 silver badges
34
34 bronze badges
asked
Jun 27, 2020 at 9:23
Robert M
Robert M
1
1
1 bronze badge
1
why you are using dot in your new name like metadata.offerId?
–
Anku Singh
Commented
Jun 27, 2020 at 9:43
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
2
As per
MongDB naming conventions
Field names
cannot
contain the null character.
Top-level field names
cannot
start with the dollar sign ($) character.
The use of
$
and
.
in field names is not recommended and is not supported by the official MongoDB drivers
Otherwise, starting in MongoDB 3.6, the server permits storage of
field names that contain dots (i.e. .) and dollar signs (i.e. $).
Share
Improve this answer
Follow
edited
Jun 27, 2020 at 9:58
answered
Jun 27, 2020 at 9:51
Swatantra Kumar
Swatantra Kumar
1,330
6
6 gold badges
25
25 silver badges
33
33 bronze badges
6
I don't understand the reason. Hope you need to recheck your answer.
–
Gibbs
Commented
Jun 27, 2020 at 10:06
The question doesn't mention the version of MongoDB. And there are restrictions of using
.
in the column name with the v3.5 and earlier versions.
–
Swatantra Kumar
Commented
Jun 27, 2020 at 10:10
Op clearly says it takes forever. I assume which means it's >v3.5. No problem.
–
Gibbs
Commented
Jun 27, 2020 at 10:13
It also says, how to rename... nothing happens
–
Swatantra Kumar
Commented
Jun 27, 2020 at 10:14
I haven't used older versions. Out of curiosity, shouldn't it throw syntax error when name contains dot character.
–
Gibbs
Commented
Jun 27, 2020 at 10:21
|
Show
1
more comment
0
Reason for the slowness:
The $rename operator logically performs an $unset of both the old name and the new name, and then performs a $set operation with the new name.
It does two operation first on a document
unset
on old and new name. Then it performs
set
operation. So totally three operations per document.
Reference
And you are actually doing more worse. Because you are converting a field to
nested
one.
Input doc:
{
""_id"":"""",
""key"":1
}
Rename:
db.test.update({}, {
  ""$rename"": {
    ""key"": ""key1.name""
  }
})
Output:
/* 1 */
{
    ""_id"" : ObjectId(""5ef718a290c7f76c305aa21c""),
    ""key1"" : {
        ""name"" : 1
    }
}
It's converted to nested one. You are doing worse because each rename results in 3 operations on a nested doc which in turn contains many fields.
Approximately you are doing
1.6million * 3 * 6
operations on a doc. Hence it is slow.
Share
Improve this answer
Follow
answered
Jun 27, 2020 at 10:06
Gibbs
Gibbs
22.9k
14
14 gold badges
83
83 silver badges
145
145 bronze badges
1
I checked it today and all records have changed name. I thought it killed the query because it was too big. Thank you for your answer
–
Robert M
Commented
Jun 28, 2020 at 7:00
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
mongodb
rename
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Visit chat
Related
17
MongoDB 1.6.5: how to rename field in collection
56
MongoDB rename database field within array
302
How can I rename a field for all documents in MongoDB?
1
How to rename a column in a collection in Mongodb using MongoMapper?
2
Renaming long MongoDB column names to shorter ones, in a live app
2
How to rename all the key in mongodb?
1
Renaming Field Within an Array in MongoDB
1
How to rename the columnname in mongodb using node js
1
How to rename a field inside an array with database commands?
1
How to rename mongodb field without overwriting?
Hot Network Questions
How to achieve infinite rage?
Is there greater explanatory power in laws governing things rather than being descriptive?
What's the difference between '\ ' and tilde character (~)?
Need an advice to rig a spaceship with mechanicals part
US phone service for long-term travel
Didactic tool to play with deterministic and nondeterministic finite automata
How could a city build a circular canal?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Heating object in airless environment
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
When was ""to list"" meaning ""to wish"" lost?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
How to explain why I don't have a reference letter from my supervisor
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
How to remove clear adhesive tape from wooden kitchen cupboards?
What technique is used for the heads in this LEGO Halo Elite MOC?
Two types difinition of the distance function
How to use a symbol as both function and head?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
Indian music video with over the top CGI
Building a Statistically Sound ML Model
Find all unique quintuplets in an array that sum to a given target
Why is the speed graph of a survey flight a square wave?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-js
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
Django and Amazon Lambda: Best solution for big data with Amazon RDS or GraphQL or Amazon AppSync - Stack Overflow,"Django and Amazon Lambda: Best solution for big data with Amazon RDS or GraphQL or Amazon AppSync - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Django and Amazon Lambda: Best solution for big data with Amazon RDS or GraphQL or Amazon AppSync
Ask Question
Asked
4 years, 5 months ago
Modified
4 years, 5 months ago
Viewed
228 times
Part of
AWS
Collective
1
We have a system with large data (about 10 million rows in on a table). We developed it in Django framework and also we want to use Amazon Lambda for serving it. Now I have some question about it:
1- If we want to use Amazon RDS (MySql, PostgresSQL), which one is better? And relational database is a good solution for doing this?
2- I read somewhere, If we want to use a relational database in Amazon Lambda, Django for each instance, opens a new connection to the DB and it is awful. Is this correct?
3- If we want to use GraphQL and Graph database, Is that a good solution? Or we can combine Django Rest-API and GraphQL together?
4- If we don't use Django and use Amazon AppSync, Is better or not? What are our limitations for use this.
Please help me.
Thanks
django
amazon-web-services
aws-lambda
graphql
aws-appsync
Share
Improve this question
Follow
asked
Jun 17, 2020 at 10:41
Soheil Tayyeb
Soheil Tayyeb
315
4
4 silver badges
14
14 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
3
GraphQL is very useful for graph data, not timeseries. Your choice will depend on the growth factor, not the actual rows. I currently run an RDS instance with 5 billion rows just fine, but the problem is how it will increase over time. I suggest looking into archival strategies using things like S3 or IoT-analytics (this one is really cool).
I wouldn't worry about concurrent connections until you have a proper reason too (+50's per second). Your DB will be the largest server you have anyway.
Share
Improve this answer
Follow
answered
Jun 17, 2020 at 10:48
Exelian
Exelian
5,878
1
1 gold badge
32
32 silver badges
51
51 bronze badges
1
Thanks for your answer, We are importing about 10000 rows in 24 hours, but we are reading or querying data about 10000 times in a minute.
–
Soheil Tayyeb
Commented
Jun 17, 2020 at 12:20
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
django
amazon-web-services
aws-lambda
graphql
aws-appsync
or
ask your own question
.
AWS
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
5
Cassandra or MongoDB for good scaling and big amount of queries
5
Best database solution for Django on AWS
2
Amazon RDS, lambda postgres
0
Suggestion on handling realtime data in AWS
1
AWS - Best way to transfer and retreive large scale data from/to AWS storage
0
Best approach to handle graphql for aws lambda?
0
Lambda architecture on AWS: choose database for batch layer
6
AWS DynamoDB vs RDS for Lambda serverless architecture
2
Which instance is the best to create an Apollo Graphql microservice on AWS?
3
Use graphQL with API Gateway (apollo-server-lambda) vs. AppSync
Hot Network Questions
Only selecting Features that have another layers feature on top
US phone service for long-term travel
How does this Paypal guest checkout scam work?
Is outer space Radioactive?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
How to estimate the latency of communication?
How manage inventory discrepancies due to measurement errors in warehouse management systems
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
Does DOS require partitions to be aligned at a cylinder boundary?
What does it mean when folks say that universe is not ""Locally real""?
Why are Jersey and Guernsey not considered sovereign states?
How to explain why I don't have a reference letter from my supervisor
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
How to delete edges of curve based on their length
reverse engineering wire protocol
Find a fraction's parent in the Stern-Brocot tree
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
How to set individual columns in the siunitx package to boldface? It it a bug?
Didactic tool to play with deterministic and nondeterministic finite automata
Can a hyphen be a ""letter"" in some words?
Math contents does not align when subscripts are used
Time travelling paedo priest novel
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
default
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
database - Python: Creating big data base with arrays and dictionary - Stack Overflow,"database - Python: Creating big data base with arrays and dictionary - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Python: Creating big data base with arrays and dictionary
Ask Question
Asked
4 years, 6 months ago
Modified
4 years, 6 months ago
Viewed
125 times
1
I am trying to set up a database to be used in python for analysis of physics signals that can be updated when more data are acquired.
I am not experienced with database creation nor handling as I moved recently from ROOT to python for commodity in sharing with colleagues. I used to loop over ROOT files with different attributes (particle name, energy, date,...) to perform the analysis I wanted to be performed. Each of these files contained several signals called events that allow statistics analysis. Currently those files amounts to 20GB total but will increase.
I want to transpose my analysis into python codes but using a data base rather than keeping split files. I should be able to select certain attributes and retrieve the associated signals to performs some analysis. This data base should also be able to be updated in the futur.
I learned about pickle, panda, sqlalchemy as great tools for what I want to do and I tried to implement different solutions starting with the creation of the data base. (Next stage being the search and retrieve of data from the data frame).
Currently I tried different structures to organise the data:
_nested dictionary:
dataDict[""particle_name""][""energy_value""][...][""event_ID""]=signal (type= np.array)
_ structured array:
simtype = [(""particle_name"", 'U13'),(""energy_value"", 'f8'),...,(""event_ID"",'i8'),('signal', 'f8', amplisize)]
x=np.array((proton,13.8,...,156,signal),dtype=simtype)
Then writing to a file i tried both dumping to a pickle file and json file. I had to use the following encoder for the json.
from json import JSONEncoder
import json
class NumpyArrayEncoder(JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return JSONEncoder.default(self, obj)
I don't know why the json file was heavier than my original scattered files by a factor of 3. I thus focused on pickle files which seemed to work more easily. Should i reconsider json files for my case?
My issue then comes on the updating process. I first consider loading the pickle data then updating it.
For dictionary case when using:
class NestedDict(dict):
    def __getitem__(self, key):
        if key in self: return self.get(key)
        return self.setdefault(key, NestedDict())
it allows me to easily loop over new files (files in a directory) that are to be added to the database and I append them to the loaded NestedDict then, after reverting the data to dict type, I dump it by overwriting the pickle file. However, the solution of structured arrays face the issue that numpy.append is using a copy, thus doubling the memory required.
I recently found out that you can use the 'ab' in pickle to append and thus be able to just add to the database without needing to reload everything. However, I am not familiar how this 'ab' will affect the next stage of analysis and search in the database.
And this comes with another issue, if I want to rerun an event and update the stored signal,loading the full dictionary would automatically update the value but appending to the file makes me store another dictionary.
I am looking for advice on structure, on libraries to use and clarification on some steps I might be doing completely wrong. I tried to look at examples online but most of them use dummy examples with csv,pickles tables or dictionary then loading in panda + sqlalchemy to perform search, thus I am not sure how my try will affect how to handle the next stage.
Solutions I am looking for need to cope with heavy final database size (>>20GB) both in writing/updating the data file and then retrieving signals for certain attributes (usual sql examples).
python
database
dictionary
Share
Improve this question
Follow
asked
Jun 15, 2020 at 17:27
flogoru
flogoru
31
4
4 bronze badges
0
Add a comment
|
Related questions
46
Python Disk-Based Dictionary
1
Constructing a dictionary with large volume of data in a variable
4
Storing a 7millions keys python dictionary in a database
Related questions
46
Python Disk-Based Dictionary
1
Constructing a dictionary with large volume of data in a variable
4
Storing a 7millions keys python dictionary in a database
1
Store dictionary in database
0
How to read data and create a dictionary of dictionaries in Python?
0
Python dictionary / database in memory
4
Which database to store very large nested Python dicts?
2
create student database in python
1
Proper choice for a database consisting of a dictionary
0
Need to create a Python Level structure of dictionaries using key value pair
Load 7 more related questions
Show fewer related questions
0
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
python
database
dictionary
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
What's a modern term for sucker or sap?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
Why does it take so long to stop the rotor of a helicopter after landing?
how do I correctly check that some aggregated results are correct?
Longest bitonic subarray
Does an NEC load calculation overage mandate a service upgrade?
How does this Paypal guest checkout scam work?
How can we be sure that the effects of gravity travel at most at the speed of light
Can two wrongs ever make a right?
What does it mean when folks say that universe is not ""Locally real""?
What should machining (turning, milling, grinding) in space look like
Heating object in airless environment
Useful aerial recon vehicles for newly colonized worlds
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
Is there greater explanatory power in laws governing things rather than being descriptive?
A Pandigital Multiplication
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Indian music video with over the top CGI
Can a hyphen be a ""letter"" in some words?
Time travelling paedo priest novel
How to set individual columns in the siunitx package to boldface? It it a bug?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-py
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
amazon dynamodb - Need some advice on Big Data ETL job cost effective design - Stack Overflow,"amazon dynamodb - Need some advice on Big Data ETL job cost effective design - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
Need some advice on Big Data ETL job cost effective design
Ask Question
Asked
4 years, 6 months ago
Modified
4 years, 6 months ago
Viewed
69 times
Part of
AWS
Collective
-1
I need some advice on designing a big data ETL job that processes hourly/daily. I need a system that runs periodically over 1.8TB metadata to transform the data into a billing report for downstream usage.
Requirements:
Primary DataSource: DynamoDB has the core metadata, 1.8TB and growing unbounded, metadata would be updated all the time.

Latency: Need to finish the job within 12 hours.

Availability: Critical business logic is running on the DDB tables, I cannot afford to impact business when trying to run the ETL job to produce billing reports.
I am looking into the AWS EMR, but I could not find a cost effective way to dump the DDB data into EMR, even if I am dumping daily, scan through the entire table each time end up costing $60000:
1 RCU would be 4KB:
1.8TB * 1024^3 / 4 = 483183820.8 * (0.00013/RCU) = $62813
Given the size of the data, my initial thought is to dump the data daily as a snapshot and compute the hourly business report of the previous day. But still, doing a daily dump would come down to an overwhelming $2M every month.
Could I get some advice on cost effective way for this job?
amazon-dynamodb
bigdata
amazon-emr
Share
Improve this question
Follow
edited
Jun 10, 2020 at 7:07
Jialun Liu
asked
Jun 10, 2020 at 5:20
Jialun Liu
Jialun Liu
331
1
1 gold badge
5
5 silver badges
16
16 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
0
Why do you need to dump data on EMR? You just need to run your compute on EMR, your source can still be dynamodb.
For the first iteration, just read all the data present in dynamodb and create your report. From next iteration onwards, just read the delta from dynamodb and update your report.
Share
Improve this answer
Follow
answered
Jun 11, 2020 at 2:11
srikanth holur
srikanth holur
770
4
4 silver badges
11
11 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
amazon-dynamodb
bigdata
amazon-emr
or
ask your own question
.
AWS
Collective
Join the discussion
This question is in a collective:
a subcommunity defined by tags with relevant content and experts.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
3
Recommendation for a large-scale data warehousing system
4
Amazon DynamoDB - Scale and benefits, but is it a right fit
2
Is Amazon DynamoDB at all appropriate for small datasets?
0
Designing an Analytics System with Hadoop
0
Move from MySQL to AWS DynamoDB? 5m rows, 4 tables, 1k writes p/s
3
Suggestion regarding Cost reduction on Using Dynamodb
0
Architecture? Business logic separate from big data
1
Dynamodb design recommendation
0
Optimal ETL process and platform
2
Is DynamoDB suitable for high transaction application?
Hot Network Questions
When to start playing the chord when a measure starts with a rest symbol?
Find all unique quintuplets in an array that sum to a given target
How should I connect a light fixture with UK wire colors to US wiring?
What technique is used for the heads in this LEGO Halo Elite MOC?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
Are pigs effective intermediate hosts of new viruses, due to being susceptible to human and avian influenza viruses?
Protecting myself against costs for overnight weather-related cancellations
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
How to achieve infinite rage?
Why does this simple and small Java code runs 30x faster in all Graal JVMs but not on any Oracle JVMs?
how do I correctly check that some aggregated results are correct?
What does “going off” mean in ""Going off the age of the statues""?
When was ""to list"" meaning ""to wish"" lost?
Time travelling paedo priest novel
What's the justification for implicitly casting arrays to pointers (in the C language family)?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
Is there greater explanatory power in laws governing things rather than being descriptive?
Is outer space Radioactive?
What is meaning of forms in ""they are even used as coil forms for inductors?""
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
What are these 16-Century Italian monetary symbols?
A Pandigital Multiplication
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
Find a fraction's parent in the Stern-Brocot tree
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
how can I split a big data set to small tables in sas - Stack Overflow,"how can I split a big data set to small tables in sas - Stack Overflow
Skip to main content
Stack Overflow
About
Products
OverflowAI
Stack Overflow for Teams
Where developers & technologists share private knowledge with coworkers
Advertising & Talent
Reach devs & technologists worldwide about your product, service or employer brand
OverflowAI
GenAI features for Teams
OverflowAPI
Train & fine-tune LLMs
Labs
The future of collective knowledge sharing
About the company
Visit the blog
Loading…
current community
Stack Overflow
help
chat
Meta Stack Overflow
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Just browsing Stack Overflow? Help us improve your experience.
Sign up for research
Home
Questions
Tags
Users
Companies
Labs
Jobs
Discussions
Collectives
Communities for your favorite technologies.
Explore all Collectives
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Collectives™ on Stack Overflow
Find centralized, trusted content and collaborate around the technologies you use most.
Learn more about Collectives
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Get early access and see previews of new features.
Learn more about Labs
how can I split a big data set to small tables in sas
Ask Question
Asked
4 years, 6 months ago
Modified
4 years, 6 months ago
Viewed
2k times
1
I have a large data set of branches and accounts. I would like to split the data set into to smaller tables by the variable BRANCH. Is there a way to do so, even by PROC TABULATE or PROC REPORT ?
My code:
PROC SQL ;
    CREATE TABLE Branch_trans as
    SELECT  Branch,
            account_id
    FROM work.BRANCH
;
QUIT ;
split
sas
Share
Improve this question
Follow
asked
Jun 9, 2020 at 14:33
Eyal Marom
Eyal Marom
301
6
6 silver badges
21
21 bronze badges
1
Tabulate and Report display the data. Do you need to split your data out to view it or to different data sets?
–
Reeza
Commented
Jun 9, 2020 at 15:41
Add a comment
|
3 Answers
3
Sorted by:
Reset to default
Highest score (default)
Trending (recent votes count more)
Date modified (newest first)
Date created (oldest first)
1
If you want to create separate datasets by branch, you can use a macro to do so. The below macro will get the distinct number of branches and subset the data into individual files suffixed 1, 2, 3, etc.
You will need to know the distinct number of branches. If your dataset is large, this will take some time to complete. You can run these all in parallel to make it run faster, but the code will increase in complexity.
%macro splitData(group=, data=, out=);

    proc sql noprint;
        select distinct &group.
        into :groupvalues separated by '|'
        from &data.
        ;
    quit;

    %do i = 1 %to %sysfunc(countw(&groupvalues., |));
        %let groupvalue = %scan(&groupvalues., &i., |);

        data &out._&i.;
            set &data.;
            where &group. = ""&groupvalue."";
        run;
    %end;

%mend;
%splitData(data=sashelp.cars, group=origin, out=want);
Share
Improve this answer
Follow
answered
Jun 9, 2020 at 17:07
Stu Sztukowski
Stu Sztukowski
12.8k
1
1 gold badge
15
15 silver badges
24
24 bronze badges
1
I saw a similar solution, thanks. Proc print with ods does it easier.
–
Eyal Marom
Commented
Jun 9, 2020 at 17:09
Add a comment
|
0
Using PROC PRINT with BY statement as such:
PROC PRINT DATA=have ;
BY Branch ;
RUN ;
Share
Improve this answer
Follow
answered
Jun 9, 2020 at 15:35
Eyal Marom
Eyal Marom
301
6
6 silver badges
21
21 bronze badges
Add a comment
|
0
maybe it help for you but more then your question. This soultion split data by branch so you can modify report by branch if do you want:
/* this is an example table, the branch 3 has 2 row*/
    data fulldata;
    branchid=1; a=""aaa"";output;
    branchid=2; a=""bbb"";output;
    branchid=3; a=""ccc"";output;
    branchid=3; a=""ddd"";output;
    run;

    %macro x;
    /*sort for the distinct branch number*/
    proc sort data=fulldata out=temptable nodupkey;
    by branchid;
    run;

    %let branchcount=0;

    /*save the banch number, branchid and branch count into macro variables */
    data _null_;
    set temptable end=x;
    call symput(""branch"" || strip(_n_),strip(branchid));
    if x then call symput('branchcount',_n_);
    run;

    /* cycle in the branch count and split the table by brancid */
    %let i = 1;
    %do %while (&i<=&branchcount);
        data branch&i;
        set fulldata;
        where branchid=&&branch&i;
        run;
        proc report data=branch&i; /* you can modify if you want */
        quit;
        %let i=%eval(&i.+1);
    %end;
    %mend;
    %x;

    /* it make 3 table and the third has 2 rows */
    /*important : the branchid is numeric you need use like : 
     where branchid=""&&branch&i"";*/
Share
Improve this answer
Follow
answered
Jun 9, 2020 at 17:24
imnotarobot
imnotarobot
141
4
4 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Stack Overflow.
Learn more
Thanks for contributing an answer to Stack Overflow!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
split
sas
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
0
Split SAS dataset
5
Split large SAS dataset into smaller datasets
1
divide data into groups in SAS proc-sql
0
SAS- How to split a large dataset by certain requirements?
0
SAS: Spliting data groups into its own rows
0
Conditional splitting a huge dataset in SAS
1
Separate data into different subsets resulted in not enough space error in SAS
0
Splitting a table into three tables
0
How to split a dataset into a certain number of rows in SAS?
1
How can I split a dataset into multiple datasets based on a column (which consists of groups) in sas
Hot Network Questions
What is ""B & S"" a reference to in Khartoum?
When was ""to list"" meaning ""to wish"" lost?
Bash script that waits until GPU is free
Protecting myself against costs for overnight weather-related cancellations
how do I correctly check that some aggregated results are correct?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
Does DOS require partitions to be aligned at a cylinder boundary?
What technique is used for the heads in this LEGO Halo Elite MOC?
Time travelling paedo priest novel
How much of a structural/syntactic difference is there between an oath and a promise?
Topology of a horocycle
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Why does it take so long to stop the rotor of a helicopter after landing?
Does an NEC load calculation overage mandate a service upgrade?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
How *exactly* is divisibility defined?
What's a modern term for sucker or sap?
Why are Jersey and Guernsey not considered sovereign states?
Is outer space Radioactive?
Only selecting Features that have another layers feature on top
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Challah dough bread machine recipe issues
How to keep meat in a dungeon fresh, preserved, and hot?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Stack Overflow
Questions
Help
Chat
Products
Teams
Advertising
Talent
Company
About
Press
Work Here
Legal
Privacy Policy
Terms of Service
Contact Us
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
architecture - Handle big data sets in a web application in combination with real-time communication between web-clients - Software Engineering Stack Exchange,"architecture - Handle big data sets in a web application in combination with real-time communication between web-clients - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Handle big data sets in a web application in combination with real-time communication between web-clients
Ask Question
Asked
4 years, 1 month ago
Modified
4 years, 1 month ago
Viewed
95 times
0
We have the following use case to be developing:
A web application that shows a data table / data grid with up to 100000 rows and 30-50 columns. Each column is filterable and sortable. The web application is based on Angular and PrimeNG (with PrimeNG Table) and the backend is based on ASP.Net Core WebApi, but i think that doesn't matter.
The users a very picky, so performance is a top priority and the application should be scalable. Filtering a 100000 rows table should take ideally less than 5 seconds.
There are two special things in our environment:
The data set for the web application is generated in a SQL-Stored procedure. So the complex logic to prepare the Data is not in the API itself
The web application will implement real-time communication via SignalR. So when a user changes some data, all other users get this changes instant
What we can't do
Currently i think that it is not a good idea to download the whole data set from the API and let the client handle all the filtering / sorting stuff. Javascript is slow and not scalable.
So we need some of the good old paging stuff.
Our idea
I have this process in mind:
Web Client sends request to API -> API calls SQL-Stored Procedure and caches the result using a Cache Store like Redis or something and then returns the first page (like the first 100 rows) to the client
Now when the user filters / sorting the data or switches to another data page:
Web Client sends request to API with the ""CacheId"" (or something to identify the data set in the cache) -> When the API gets a CacheId it first tries to load the data from the cache instead of calling the SQL Stored Procedure again. -> The API applies filter, sorting, paging etc. and then returns the result to the client
This way the SQL-Stored Procedure (the expensive part) is not called every time and also only small amount of the data is transfered to the client.
Our problem:
How to handle the real-time communication with this architecture? When someone changes something in the data all the cached data sets must be updated. It is not an option to invalidate the data sets in the cache and generate them new from scratch because data changes happen very often (every minute). If we invalidate the whole cache every time, this would be very expensive.
So we need something like this:
User changes data and send ""DataChanged-Event"" -> Cached data sets are read from the cache -> Cached data sets will be updated and then are written back to the cache
Our questions
Do you think we are on the right path with our solution or are we doing something completly wrong?
Where should we handle the cache-updating process? Maybe a Azure Function / Console Application or something that receives all the ""DataChanges Events"" and then updates the cache?
architecture
web-development
api
performance
caching
Share
Improve this question
Follow
edited
Nov 4, 2020 at 15:06
OPunktSchmidt
asked
Nov 4, 2020 at 8:38
OPunktSchmidt
OPunktSchmidt
125
4
4 bronze badges
4
1
Getting 100 records from a database table with 100,000 rows doesn't sound all that expensive.  A properly written query on any reasonably good database system should take less than a second to run.  Why not just run the query for every page request, and dispense with the cache?
–
Robert Harvey
Commented
Nov 4, 2020 at 15:27
Alternatively, use an in-memory data store that is periodically saved to disk.
–
Robert Harvey
Commented
Nov 4, 2020 at 15:32
The SQL Query takes place in a Stored Procedure and this Stored Procedure is complex. The data we display to the user is thus not directly available in a separate database table. We collect the data from several tables and transform them. This is very expensive
–
OPunktSchmidt
Commented
Nov 5, 2020 at 7:35
Your requirements seem impossible.  The users expect instant filter changes and instant data updates, against an expensive analytical query.  It can't be done, unless you can find some new way to optimize the query or give up something.
–
Robert Harvey
Commented
Nov 5, 2020 at 13:46
Add a comment
|
0
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
architecture
web-development
api
performance
caching
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
4
What's the best way to cache a growing database table for html generation?
1
When should I invalidate a cache of a user's credentials?
2
AngularJS dealing with large data sets (Strategy)
2
filtering data from db
3
Request Caching for Symfony 3 application with > 1 million combination of query parameters
1
Two way data synchronization between web application and REST API server
3
Architecture for real time updates from the data in file system
0
Caching or in-memory table in Azure for performance
Hot Network Questions
How to delete edges of curve based on their length
Can we judge morality?
How to format numbers in monospaced (typewriter) font using siunitx?
How to use a symbol as both function and head?
What technique is used for the heads in this LEGO Halo Elite MOC?
How does this Paypal guest checkout scam work?
How to remove clear adhesive tape from wooden kitchen cupboards?
Derailleur Hangar - Fastener torque & thread preparation
What should machining (turning, milling, grinding) in space look like
Physical interpretation of selection rules for different multipole orders
Protecting myself against costs for overnight weather-related cancellations
How could a city build a circular canal?
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
Is there greater explanatory power in laws governing things rather than being descriptive?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Can two wrongs ever make a right?
Should I expect a call from my future boss after signing the offer?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Need an advice to rig a spaceship with mechanicals part
reverse engineering wire protocol
How to set individual columns in the siunitx package to boldface? It it a bug?
Is outer space Radioactive?
What is meaning of forms in ""they are even used as coil forms for inductors?""
When to start playing the chord when a measure starts with a rest symbol?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
Data Quality with DBT Tests and Great Expectations | by Shehroz Abdullah | Medium,"Data Quality with DBT Tests and Great Expectations | by Shehroz Abdullah | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality with DBT Tests and Great Expectations
Shehroz Abdullah
·
Follow
5 min read
·
Aug 27, 2023
--
Listen
Share
Data quality is a critical aspect of any data engineering or data science project. Ensuring that your data is accurate, consistent, and reliable is essential for making informed decisions and deriving meaningful insights. Dbt provides a comprehensive framework for implementing data quality checks through its testing capabilities.
By incorporating data quality tests into your dbt workflow, you establish a process that not only ensures the reliability of your data but also increases confidence in the insights and decisions derived from it. Regularly running these tests helps catch data quality issues early, reducing the risk of faulty analyses and incorrect decision-making based on inaccurate or inconsistent data.
In this article, we will go through an introduction of two types of tests in dbt, singular and generic. We will also understand the process of incorporating tests from great expectations package in our dbt test workflow. We will be working with public Airbnb data, which is available at
s3://dbtlearn.
To follow along, make sure that you have completed the pre-requisites of installing core dbt and establishing connection with a supported datastore of your choice.
Generic Tests in Dbt
Let us first gain an understanding of Generic Tests in dbt, which are built-in modules that are added with the core dbt installation. The four generic tests included in the core dbt installation are unique, not_null, accepted_values and relationships. External packages such as great expectations, which we will cover later on, can be included to add more generic tests. Generic tests are extremely simple to include in our test workflows. Convention states that we define our generic states in a schema.yml file in our project folder, however the name and destination of the YAML file is up to the user.
In this YAML file we can not only define table and column level tests but also add descriptions as an important piece of business metadata for any downstream operations.
version: 2
models:
- name: dim_hosts_cleansed
description: Cleansed table for the Airbnb hosts
columns:
- name: host_id
description: The id of the host. This is the primary key.
tests:
- not_null
- unique
- name: host_name
description: The name of the host
tests:
- not_null
- name: is_superhost
description: Defines whether the hosts is a superhost.
tests:
- accepted_values:
values: ['t', 'f']
In the above YAML file example, we have defined generic tests for different columns of the dim_hosts_cleansed model. For purposes of understanding, we can think of a model as database table. For the first column
host_id
we have defined the
not_null
and
unique
tests. For the second column
host_name
, the
not_null
test has been defined. Lastly, the
is_superhost
column has the
accepted_values
test defined, where any values other than ‘t’ or ‘f’ will fail the test.
To execute all four tests defined for the dim_hosts_cleansed model, use the following command:
dbt test --select dim_hosts_cleansed
If all four tests pass, the results should be as follows:
Importing more Generic Tests with Great Expectations
Dbt allows users to create their own custom tests to serve specific use cases, using singluar tests, which we will cover later on. However, before creating you own tests it is a good idea to search for the availability of these tests from external open source packages. Dbt allows for importing tests from various packages, one of which is
calogica/dbt-expectations
. This is an open-source port for great expectations python package but is not a straight one-to-one mapping with the core python package. However, it goes a long way in bringing multiple great expectation tests to dbt.
To import this package, create a packages.yml file in the project folder and add the following lines:
packages:
- package: calogica/dbt_expectations
version: ["">=0.8.0"", ""<0.9.0""]
Now to install this package, use the following command:
dbt deps
Granted that the package was installed correctly, we are now ready to import various tests from the newly imported package. To use these tests, we simply add them to our schema YAML file as defined earlier for the core dbt generic tests.
version: 2
models:
- name: dim_listings_w_hosts
tests:
- dbt_expectations.expect_table_row_count_to_equal_other_table:
compare_model: source('airbnb', 'listings')
columns:
- name: price
tests:
- dbt_expectations.expect_column_values_to_be_of_type:
column_type: number
- dbt_expectations.expect_column_quantile_values_to_be_between:
quantile: .99
min_value: 50
max_value: 500
- dbt_expectations.expect_column_max_to_be_between:
max_value: 10000
config:
severity: warn
Here is an example of using table and column level great expectations tests. Take note that we are referencing the tests with the dbt_expectations pre-text for our tests. These tests can now be executed like the core dbt generic tests with the following command:
dbt test --select dim_listings_w_hosts
To learn more about the different types of available tests in this package explore their GitHub repository,
expectations/great_expectations
and the official documentation,
Great Expectations
.
Singular Tests in Dbt
In situation where we have to create own own custom tests, Singular Tests come into the picture. Singular tests are sql definitions that reference a particular model. These are expected to return an empty result set if the test passes. Singular tests give users the freedom to write tests for their custom use cases.
Cotinuing working with Airbnb data, let us assume we have a scenario where we want to ensure that the review date for each listings is added after the create date for that listing. Naturally, the review date should be added after the creation date of that listing.
SELECT * FROM {{ ref('dim_listings_cleansed') }} l
INNER JOIN {{ ref('fct_reviews') }} r
USING (listing_id)
WHERE l.created_at >= r.review_date
Here is an example of a singular test named,
consistent_created_at
. The sql defined for the test above should return an empty result set if the test passes. Any non-empty result set means that the test has failed. In order to run this test, the same process which was defined earlier for generic tests is followed.
Dbt also allows for users to use their singular tests as generic tests with the help of macros. Dbt macros are similar to functions in programming languages like Python or Java. Dbt uses macros in a templating language called Jinja to avoid repeated code across multiple models.
{% macro no_nulls_in_columns(model) %}
SELECT * FROM {{ model }} WHERE
{% for col in adapter.get_columns_in_relation(model) -%}
{{ col.column }} IS NULL OR
{% endfor %}
FALSE
{% endmacro %}
As observed in the example above, for the test
no_nulls_in_columns
, we are using the Jinja syntax to create our logic for the test. Macros are defined as sql files in the macros folder (by default) in the main project folder. This allows us to reference this test in our YAML file(s) as described earlier and treat it like a generic test.
Conclusion
In this article, we understood the power of good data quality and how dbt tests allows for resolving major problems with transformation workflows. The importance of data quality cannot be overstated in today’s data-driven world. High-quality data is the foundation for informed decision-making, accurate analysis, reliable insights, and the successful implementation of various business processes.
Dbt allows users to now include the best software engineering practices in transformation workflows. Incorporating dbt tests into your data transformation processes helps establish a culture of data quality and reliability, making your data pipelines more robust and trustworthy.
References
dbt documentation on tests
dbt severity level config for tests
dbt getting started with data tests
--
--
Follow
Written by
Shehroz Abdullah
7 Followers
·
3 Following
data geek
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Preventing Data Quality Issues with Unit Testing | by Seckin Dinc | Medium,"Preventing Data Quality Issues with Unit Testing | by Seckin Dinc | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Preventing Data Quality Issues with Unit Testing
Seckin Dinc
·
Follow
8 min read
·
Mar 22, 2023
--
Listen
Share
Using Pytest to write robust unit tests to prevent data quality issues
Photo by
Jeswin Thomas
on
Unsplash
Unit testing is a program testing technique that involves testing individual units of an application in isolation from the rest of the system. It ensures that each unit of code functions as expected and meets its design specifications.
During unit testing, software engineers, data engineers, data scientists, data analysts, or analytical engineers write test cases that evaluate the functionality of a single class or function within the code, using mock data to simulate input and output. The tests are designed to check for errors, bugs, and other issues that could cause the unit to fail, and they are run repeatedly as code changes are made.
Why is unit testing important for data quality?
Data quality is all about ensuring that data is accurate, consistent, and reliable. Unit testing is an essential practice in data quality because it helps to ensure that data processing and transformation functions which are the fundamentals of the data processing pipelines are working correctly and producing accurate results. Here are some reasons why unit testing is important in data quality:
Ensuring accuracy:
By testing individual data processing and transformation functions, we can ensure that they are producing the expected results for a given input.
Detecting data anomalies:
Unit testing can help detect data anomalies, such as missing or duplicate data, in the early stages of data processing.
Reducing errors:
Unit testing can help reduce the number of errors in data processing by identifying and fixing defects in the code.
Enhancing data quality:
By ensuring that each data processing and transformation function is working correctly, we can enhance the overall quality of the data.
Pytest
Pytest is a Python testing framework that originated from the PyPy project. It can be used to write various types of software tests, including unit tests, integration tests, end-to-end tests, and functional tests. Its features include parametrized testing, fixtures, and assert re-writing.
Screenshot from
https://pypi.org/project/pytest/
Pytest is not a built-in Python package. You need to install it within your terminal with the
pip install pytest
command.
Best Practises
Before we dive into examples I want to share some best practices in unit testing and pytest that will help you to develop better unit tests with pytest.
Unit Testing Best Practices
Write test cases for both positive and negative scenarios:
Ensure that the test cases cover both expected and unexpected scenarios, including edge cases. As the product evolves the code base needs to change as well. Do not forget to main both of the scenarios in the meantime.
Test only one unit at a time:
Individual units should be tested in isolation from other units to ensure that any issues or errors can be traced back to that specific unit. Some units will be more complex than others. Isolating units will grant focus on each one of them.
Run tests automatically:
Automating tests can save time and reduce the chances of human error. Automatically executing unit tests in the CI/CD pipelines, make sure that unit tests are evaluated before the code is deployed to production.
Keep tests small and focused:
Tests should be small and focused on a specific aspect of the code. The intention of unit testing is to test certain behaviors at a time. The smaller the test can be the more detailed it will be.
Pytest Best Practices
Split your source code from test code:
Having separate folders for source code and test code supports a cleaner and scalable code base.
Organize your tests into test functions:
Pytest uses Python functions to represent test cases. Each function should test a specific behavior or functionality of your code.
Use descriptive function names:
Use descriptive names that explain what each test function does. The best approach is getting a source functional name and adding “test_” as a prefix. This will help everyone to understand which source code is being tested.
Use test fixtures:
Test fixtures are functions that set up the environment for your tests. They can be used to initialize variables, create test data, or set up the test environment. Use fixtures to avoid code duplication and to ensure that tests run consistently.
Use parameterized tests:
Pytest allows you to define parameterized tests, which are tests that run multiple times with different inputs. Use parameterized tests to test different input scenarios and to avoid code duplication.
Use assertions:
Assertions are statements that check if a condition is true or false. Use assertions to check that your code behaves as expected. If an assertion fails, Pytest will report an error.
Pytest Examples
Example 1 — Testing the source function with one test function
In this example, I created a basic function to create the upper case of a given string.
def upper_case(sample_text: str): #sample_text is the input text object
output = sample_text.upper() #sample_text is converted into upper case
return output #the new object is returned
At the unit test, I will create a mock string to test my function.
from source import source_code #importing source_code from other folder
def test_upper_case():
test_text = ""AbCd123"" #mock text object to be used at testing
test_output = source_code.upper_case(test_text) #applying function
assert test_output == test_text.upper() #asserting function
I am executing the test function over my terminal.
pytest test/test.py
The output shows that the test case passed successfully;
Screenshot from the author
In order to showcase the error statement I will change the test function.
from source import source_code
def test_upper_case():
test_text = ""AbCd123""
test_output = source_code.upper_case(test_text)
assert test_output != test_text.upper() #this assertion will fail
The output shows that the test case failed;
Screenshot from the author
Example 2 — Testing the source function with two test functions
In this example, I am going to use the same source code and the test code above but I will create a new test function for it and test it with two test functions. With a purpose, I will make the first test fail and the second pass to show the difference in the outputs.
from source import source_code
def test_upper_case():
test_text = ""AbCd123""
test_output = source_code.upper_case(test_text)
assert test_output != test_text.upper() #this assertion will fail
def test_upper_case_string_length():
test_text = ""AbCd123""
test_output = source_code.upper_case(test_text)
assert len(test_output) > 0
The output shows that one test case passed and one test case failed;
Screenshot from the author
Example 3 — Parametrize testing with a list of test cases
In the previous examples, I created a single test object to test a single function. If I want to test multiple test objects with a single test function, I can use
pytest.mark.parametrize()
.
I am creating a
test_text
list object to contain two text objects and passing them to the test function;
from source import source_code
import pytest
test_text = [""AbCd123"", """"]
@pytest.mark.parametrize(""sample_test_text"", test_text)
def test_upper_case_string_length(sample_test_text):
test_output = source_code.upper_case(sample_test_text)
assert len(test_output) > 0
In this example, I am also going to test only a single function in my text file. In this regard am executing the test function over my terminal.
pytest test/test.py::test_upper_case_string_length
The output shows that one case passed and one test case failed;
Screenshot from the author
Example 4 — Using fixtures to test multiple test functions on a single test object
In pytest, fixtures are functions that provide a fixed baseline of data or test objects that are used as a basis for testing other code. Fixtures are essentially a way of defining and managing resources that are needed by tests.
Fixtures are defined using the
@pytest.fixture
decorator. The decorated function provides the setup code that is needed to create the fixture. When a test function requests the fixture as an argument, pytest automatically calls the fixture function and passes its return value to the test function.
In this example, I will pass the test text in a function and use fixtures to pass it to multiple test cases at once.
from source import source_code
import pytest
@pytest.fixture
def test_text():
return ""AbCd123""
def test_upper_case(test_text):
test_output = source_code.upper_case(test_text)
assert test_output == test_text.upper()
def test_upper_case_string_length(test_text):
test_output = source_code.upper_case(test_text)
assert len(test_output) > 0
The output shows that both cases passed;
Screenshot from the author
Example 5 — Pandas data frame testing
In the real-world from data scientists to data engineers are mostly working on Pandas data frames on certain data preparation operations; e.g. data clearing, feature engineering, etc.
In this regard, I will dive into more on the data frame test cases. In the examples, I will use the
movies.csv
file. A quick look into the data set;
Screenshot from the author
Below I will run multiple tests to validate the data frame;
import pytest
import pandas as pd
@pytest.fixture
def sample_data_frame():
return pd.read_csv(""movies.csv"") #uploading dataset as a fixture
def test_column_names(sample_data_frame): #testing column names matching with the given list
assert list(sample_data_frame.columns) == [""title"", ""rating"", ""year"", ""runtime""]
def test_unique_ratings(sample_data_frame): #testing rating types matching with the given list
assert list(sample_data_frame.rating.unique()) == [
""R"",
""PG"",
""PG-13"",
""GP"",
""Not Rated"",
""NC-17"",
""G"",
]
def test_year_minimum_year(sample_data_frame): #testing minimum value for the year column
assert sample_data_frame.year.min() >= 1900
def test_year_maximum_year(sample_data_frame): #testing maximum value for the year column
assert sample_data_frame.year.max() <= 2023
def test_missing_values(sample_data_frame): #testing null values
assert sample_data_frame.isna().sum().sum() == 0
Screenshot from the author
Conclusion
Unit testing is the first line of defense to prevent data quality issues. If we can’t ensure that our programs are working as expected in the first place, whatever data quality, data observability, or data reliability product will not solve our problems. In this regard, every data professional who is using Python should test their codes before deploying to production.
Thanks a lot for reading 🙏
If you liked the article, check out my
other articles
.
If you want to get in touch, you can find me on
Linkedin
and
Mentoring Club
!
Data
Machine Learning
Data Science
Data Quality
Python
--
--
Follow
Written by
Seckin Dinc
664 Followers
·
124 Following
Building successful data teams to develop great data products
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How to download really big data sets for big data testing | by Sandipan Ghosh | Medium,"How to download really big data sets for big data testing | by Sandipan Ghosh | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to download really big data sets for big data testing
Sandipan Ghosh
·
Follow
3 min read
·
Aug 3, 2020
--
Listen
Share
For a long time, I have been working with big data technologies, like MapReduce, Spark, Hive, and very recently I have started working on AI/ML. For different types of bigdata framework testing and text analysis, I do have to do a large amount of data processing. We have a Hadoop cluster, where we usually do this.
However recently, I had a situation where I had to crunch 100 GBs of data on my laptop. I didn't have the opportunity to put this data to our cluster, since it would require a lot of approval, working with admin to get space, opening up the firewall, etc.
So I took up the challenge to get it done using my laptop. My system only has 16 Gb of ram and i5 processor. Another challenge was I do not have admin access, so I can not install any required software without approval. However, luckily I had Docker installed.
For processing the data I can use Spark on local mode as spark support parallel processing using CPU cores. As i5 has 4 cores and 4 threads, the spark could run the entire process on 8 parallel processes.
How to get the Data: Yellow cab data
Now to the real topic, where to get the really big opensource data, which is 100GB in size? We need both structure(CSV) and semistructured(JSON)data
Source1:- After little research, I found out that we can download entire yellow cab data from the NYC gov data site. Here is the
link
This does need a little bit of effort to download the data, as all the data are split in monthly CSV. Each CSV is 2 GB in size. So I wrote a python program that will download each month CSV for the website into a local directory and will also show a little progress bar on the screen.
import urllib.request
from tqdm import tqdm
class DownloadProgressBar(tqdm):
def update_to(self, b=1, bsize=1, tsize=None):
if tsize is not None:
self.total = tsize
self.update(b * bsize - self.n)
def download_url(url, output_path):
with DownloadProgressBar(unit=
'B'
, unit_scale=True,
miniters=1, desc=url.split(
'/'
)[-1]) as t:
urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)
year = list(range(2009, 2020))
month = [
'01'
,
'02'
,
'03'
,
'04'
,
'05'
,
'06'
,
'07'
,
'08'
,
'09'
,
'10'
,
'11'
,
'12'
]
for x, y in [(x, y) for x in year for y in month]:
print(
""fetching data for %s, %s""
% (x, y))
link =
""https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_%s-%s.csv""
% (x, y)
file_name =
'/home/sandipan/Documents/yellow_taxi/yellow_taxi_data_%s-%s.csv'
% (x, y)
print(link, file_name)
download_url(link, file_name)
JSON data
How about the semi-structured data? well, we can use ‘Open Library’ data. The Open Library is an initiative intended to create “one web page for every book ever published.” You can download their dataset which is about 20GB of compressed data.
we can download the data very easily using wget. Then unzip it using unzip command.
wget — continue http://openlibrary.org/data/ol_cdump_latest.txt.gz
Well, that's all, have fun with all the data.
In my next post, I will post how to process the data locally.
Big Data Analytics
Data Science
Large Datasets
--
--
Follow
Written by
Sandipan Ghosh
35 Followers
·
16 Following
Bigdata solution architect & Lead data engg with experience in building data-intensive applications,tackling challenging architectural and scalability problems.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How to do data quality testing for free using dbt | by Hugo Lu | Medium,"How to do data quality testing for free using dbt | by Hugo Lu | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to do data quality testing for free using dbt
Hugo Lu
·
Follow
5 min read
·
May 11, 2023
--
1
Listen
Share
Anomalo, Monte Carlo, dbt test, when will it end??
I’ve always been a big believer in dbt test because it’s so simple.
You don’t run a model that is upstream if a test fails if you run
dbt build
Which is awesome. You get integrated testing in a production environment so bad data never gets bubbled up into end systems.
So why is this complicated? Well, writing tests is hard. As any good software engineer will tell you, writing good tests is its own profession (“Quality Assurance Engineering”).
The problem with software tests, however, is that some of them are often specific to software apps. They aren’t always generalisable. This means they cannot be commoditised.
In data, it’s not the case. Data tests people want are all the same. Some popular ones:
Check a PK is not null
Check a PK is unique
Check data is up-to-date
Check values in the data do not significantly differ from the mean
Check the rows inserted into data align with what is expected
What is interesting about (4) and (5) is that software companies purporting to offer sophisticated tools that ensure data quality say that testing for these is very difficult. Take the data in
this article
for example.
There is a clear seasonal component to the blue series. Therefore to calculate is incoming new data is vs. expectation requires us to model the green series. This is hard to implement in a test.
Or is it?
Solution 1: just use a dbt python test
Taking the code in the article above, I was able to produce this piece of code that admittedly is untested but also took me 20 minutes to write. It essentially allows you to SARIMA-rise any series and therefore compare a green value to blue value in the diagram above and flag a test to fire if the delta is above a threshold. THIS IS A PYTHON DBT TEST:
from datetime import datetime, timedelta
import itertools
import statsmodels.api as sm
def model(dbt, rolling_window, date_column, field, threshold):
def sarima_grid_search(y, seasonal_period):
p = d = q = range(0, 2)
pdq = list(itertools.product(p, d, q))
seasonal_pdq = [(x[0], x[1], x[2], seasonal_period) for x in list(itertools.product(p, d, q))]
mini = float('+inf')
for param in pdq:
for param_seasonal in seasonal_pdq:
try:
mod = sm.tsa.statespace.SARIMAX(y,
order=param,
seasonal_order=param_seasonal,
enforce_stationarity=False,
enforce_invertibility=False)
results = mod.fit()
if results.aic < mini:
mini = results.aic
param_mini = param
param_seasonal_mini = param_seasonal
print('SARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))
except:
continue
print('The set of parameters with the minimum AIC is: SARIMA{}x{} - AIC:{}'.format(param_mini,
param_seasonal_mini, mini))
return [param_mini, param_seasonal_mini, mini]
def generate_predictions(data, param, param_seasonal, window_cutoff):
mod = sm.tsa.statespace.SARIMAX(data,
order=param,
seasonal_order=param_seasonal,
enforce_stationarity=False,
enforce_invertibility=False)
results = mod.fit()
pred = results.get_prediction(start=window_cutoff, dynamic=False)
y_forecasted = pred.predicted_mean
return y_forecasted
my_sql_model_df = dbt.ref(""model"")
window_cutoff = datetime.now() - timedelta(days=rolling_window)
first_part = my_sql_model_df[my_sql_model_df[date_column] >= window_cutoff]
second_part = my_sql_model_df[my_sql_model_df[date_column] < window_cutoff]
train_series = second_part.groupby(date_column)[field].sum().sort_values(by=date_column, ascending=False)
test_series = first_part.groupby(date_column)[field].sum().sort_values(by=date_column, ascending=False)
param_mini, param_seasonal_mini, mini  = sarima_grid_search(list(train_series[field]), 7)
forecasted = generate_predictions(list(train_series[field]), param_mini, param_seasonal_mini)
test_series['forecasted'] = forecasted
test_series['delta'] = test_series['forecasted'] - test_series[field]
final_df = test_series[test_series['delta'] > threshold]
return final_df
Really not that hard
Solution 2: just use a moving average
If you have a broadly stationary dataset that shows some seasonality, it’s quite easy to model a rolling average test that is correct 99% of the time.
In this example, we generate a series that has a value of 10 on weekdays and 40 on weekends. There is some randomness introduced; a uniformly distributed variable between 0 and 10. This creates the blue series in the top RHS and has a 7 day rolling average represented by the orange line. We can see that if we test the series vs. the 7 day average with a threshold of 8%, there are no alerts.
We can “screw up the data” by throwing in some hardcodes:
So clearly something has screwed up on the 28th of Jan. As long as the testing sensitivity isn’t too great, you can more or less get the same result as having a fancy daily SARIMA test. We get an alert based on the 7 day average when we need it. This is super easy to implement in dbt (a very simple percentage implementation below that can easily be extended to use a rolling average):
{% test falls_below_threshold(model, column_name, metric_column_name, threshold, num_days, aggregation_type) %}
WITH BASE AS (
SELECT cast({{column_name}} as date ) date_,
case when {{aggregation_type}} = 1 then sum({{metric_column_name}})
when {{aggregation_type}} = 2 then count(distinct {{metric_column_name}})
when {{aggregation_type}} = 3 then count( {{metric_column_name}})
else 0 end all_metric
FROM {{model}}
WHERE {{column_name}} >= DATEADD(DAY, -{{num_days}}, current_timestamp ()) and {{column_name}} <= current_date()
group by 1
),
STAGING AS (
SELECT
date_,
all_metric,
lag(all_metric, -1) over (order by date_ desc) lag_metric,
div0(all_metric , lag_metric)-1 reduction
from BASE
)
select * FROM STAGING
WHERE REDUCTION < -{{threshold}} or REDUCTION > {{threshold}}
{% endtest %}
The fancy SARIMA test is only not equivalent to a well-specificed moving average test in cases where say, a value
is just beyond the xth percentile of what you expect
. I would say these are relatively few and far between. Engineering screw ups are pretty catastrophic in terms of rows affected.
For example, if a table breaks, there will be 0 rows inserted. If there are normally 1000 rows give or take a 100, then this will easily be flagged by a moving average test. The 5th percentile is bout 800, so 0 is well below that!
Typically, observability tools let you define “accepted thresholds” which, mathematically, is pretty much the same as re-jigging the moving average test in the dbt snippet above anyway. Looks great in a chart, but mathematically it’s a very simple op.
Conclusions
I still struggle to understand why data engineers are so averse to writing tests on their SQL. Dbt provides a perfect framework to integrate data quality tests into production workflows. True, it may not be obvious how to write some of these tools but dbt-expectations is pretty good, and the ability to run python dags makes data science testing a breeze. No need for more tools, just get your data science hat on.
Data Quality
Engineering
Data Science
Data Engineering
Anomaly Detection
--
--
1
Follow
Written by
Hugo Lu
9.5K Followers
·
51 Following
I write on Data engineering and the coolest data stuff. CEO@ Orchestra, the best-in-class unified control plane for dataops.
https://app.getorchestra.io/signup
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
4 Tips for Data Quality Validations with Pytest and PySpark | by Taylor Wagner | Slalom Build | Medium,"4 Tips for Data Quality Validations with Pytest and PySpark | by Taylor Wagner | Slalom Build | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
4 Tips for Data Quality Validations with Pytest and PySpark
Testing transformed data to yield a high-quality and dependable result
Taylor Wagner
·
Follow
Published in
Slalom Build
·
11 min read
·
Jun 3, 2024
--
Listen
Share
This article is written in collaboration with
Likitha Lokesh
.
Photo by
Lukas Blazek
on
Unsplash
Background
I recently contributed to a data software project as a quality engineer, which required a lot of testing on transformed data. In this project,
AWS Glue
was used to transform data from one
Amazon S3
bucket to another. The data was transformed using Python, specifically
PySpark
; thus, the test automation framework for testing these transformations leaned on the same tech stack, with the addition of
Pytest
, in an effort to bridge cohesiveness.
To learn more about getting started with Pytest, PySpark, and AWS, check out this awesome blog written by my colleague,
Likitha Lokesh
,
here
.
Introduction
For the project mentioned above, my team was transforming data to be ingested by a third-party software tool. To ensure successful importing of data, each target file had a list of requirements. Each target file must have met the requirements otherwise the software wouldn’t ingest the data, and then the data wouldn’t be accessible for analysis.
The outlined requirements helped our team determine what the scripts should look like to transform the data from source files to target files, but this didn’t guarantee that all of the data in each of the target files would meet every requirement.
Discrepancies in data can cause skewed results when analyzing or using the data for other purposes. In the case of this project, which used financial data, the level of confidence in the data was an absolute must.
The following question then arises:
How do we know the transformed data meets expectations?
Not all data software project solutions are one-size-fits-all; however, there are a few techniques that I acquired from working on this project that could lend themselves to efficient engineering and thought processes across a variety of data-specific projects.
I’ve created a list of the four key takeaways from this experience for quality data testing:
Wrap assertions as conditionals for logging invalid data
Determine common data tests and parametrize
Leverage Pytest warnings and XFail for known data concerns
Manage environment variables with
-E
Flag
I will go through each of the four takeaways in this article explaining what each one means in more detail and highlighting why each point made the list.
Wrap Assertions as Conditionals
When performing automated testing on a traditional software project, the visibility into a bug is slightly more clear than with debugging data. With a traditional software project or application, I can pull up the app to inspect it or review the API, but data can be incredibly exhaustive. The magnifying glass to gain visibility into issues with data in PySpark is
DataFrames
.
In my project, I was testing data in .csv files. Many of the tests had the same outline:
Read the .csv file and create a DataFrame
Use a DataFrame method to analyze the data (as applicable to the requirements)
Wrap the assertion as a conditional
Let’s say that for one column in the file, the data is ZIP codes and the requirement is that each value should be a length of exactly 5. This is how that test could look:
import length
import logging
def test_zipcode_data_length(spark_source, csv_file_path: str):
## Read the .csv file and Create a DF
dataframe = spark_source.read.csv(csv_file_path)
## Use the filter method on DF to analyze column values
## (and create another DF)
invalid_rows = dataframe.filter(length(dataframe['Zipcode']) != 5)
## Wrap Assertion in a Conditional for Logging/Debugging
if invalid_rows.count() == 0:
logging.info(""All of the values in the 'Zipcode' column are equal
to a length of 5 as expected!"")
assert True
else:
logging.error(""All of the values in the 'Zipcode' column should be
equal to a length of 5 and there are some values present that don't
meet those expectations!"")
## Print the filtered DF with the rows that don't meet the requirements
invalid_rows.show(truncate=False)
assert False
Please note: There are many code snippets throughout the article. Keep in mind that it is best practice to use test helper functions to mediate code duplication among test methods, but it is out of scope for the purposes of this article.
As you can see in the example above, the assertion is not just a flat True/False assertion but rather placed within a conditional so that the appropriate logging can take place upon a failure to debug and locate the specific place where the data is not meeting expectations. The DataFrame with invalid rows could tell me specifically where to find the data that would need addressing for the software to ingest the file.
While I showed you an example of where a specific column would need an exact length of 5 for every value, the principle of the general outline/flow of the test remains the same. Without wrapping the assertion as a conditional, what would the other options be? Downloading transformed .csv files and manually reviewing/filtering to find mistake rows? That is not a very appealing option.
Regardless of what it is you’re testing, the need to quickly assess the root cause of the failure will always be a component of a quality testing strategy.
Wrapping assertions as conditionals answers that need.
The automated approach of wrapping the assertions in conditionals is efficient and provides a fail-fast approach to addressing data concerns quickly. This approach removes a lot of the guesswork and provides specifics needed to maintain and/or improve the quality of a system/pipeline.
Parametrize Common Data Tests
While data can be intimidating because it’s so vast — the upside is that it’s typically less ambiguous when it comes to testing. The requirements are pretty straightforward, and from my experience, easier to gather than more traditional software projects. Often common data requirements can overlap between different sets of data.
In the case of my project, many of the target files that were to be created had similar requirements for different files, and even different columns within the same file carried the same requirements. To keep it simple, one requirement was that each file contained data — and while that may seem like an obvious requirement, it’s a pretty easy and fast automated test that can be run on every file, which can come in handy for unexpected, edge-case data transformation scenarios.
But what is the best approach here so that the same simple test isn’t written for every single target file? How can code reusability best be leveraged? The answer was to
parametrize
the test using
Pytest’s Parametrize marking
.
import pytest
@pytest.mark.parametrize(VALUES HERE)
def test_data_present(spark_source, csv_file_path: str):
## Read the .csv file and Create a DF
dataframe = spark_source.read.csv(csv_file_path)
## Assert that the DF is not empty
assert dataframe.first() is not None
Through the parametrize marking in Pytest, all target files can be run through this same test to ensure that each file being created contains some kind of data at a minimum. There were several instances where this became important in my previous project, especially when it came to checking for additional comma delimiters within the data or columns requiring unique data.
The caveat here is that there may be a little more effort on the front end of defining the test scenarios — to look for those common requirements and strategize how to best reuse code — but test development will be sped up exponentially in the long term.
What I learned from this experience was to better understand all of the requirements and map out the commonalities in those requirements first — before developing tests
.
Create a plan of the different types of tests that will be created based on the requirements such as data types, uniqueness, formatting, validating no additional delimiters/proper column separation in the data, etc.
Analyze the plan for patterns and try to consolidate tests as much as possible.
Establishing this understanding of patterns in the requirements and creating a stronger plan before developing tests helps speed up development as well as test execution.
Leverage Warnings and XFail
When it comes to interacting with data, and more specifically developing new software that uses sensitive data (such as financial data), many times development is scaffolded among lower environments first before interacting with real data. This process helps to protect real (production/prod) data while all the kinks are worked out in development; however,
a problem that often occurs with non-production environment data, such as QC/QE/DEV data, is the data can often get become unmanaged or mismanaged
.
Mismanagement of non-production environment data can be tricky to work around and cause skewed results. Luckily, there are two capabilities embedded into Pytest that I would recommend leveraging when testing known data issues:
Pytest Warnings
and
Pytest XFail
. I don’t necessarily have a preference or recommendation between these two options. Choose the tool that you feel works best for your situation.
If you are already implementing, or plan to implement, my first suggestion in this article for wrapping your assertions as conditionals — you can easily start embedding Pytest Warnings into that same concept. The difference here is that instead of a false assertion, there will be a warning. It’ll look something like this:
import warnings
import logging
def test_email_data_unique(spark_source, csv_file_path: str):
## Read the .csv file and Create a DF
dataframe = spark_source.read.csv(csv_file_path)
## Use the count method on DF to capture the number of total rows
num_rows = dataframe.count()
## Use the select method - paired with the distinct and count methods
## on DF to analyze column values for uniqueness
num_unique_rows = dataframe.select(dataframe['Email'].distinct().count())
## Wrap Assertion in a Conditional and Leverage WARNINGS
if num_rows == num_unique_rows:
logging.info(""All of the values in the 'Email' column are unique
as expected!"")
assert True
else:
## Print the rows that don't meet the requirements
dataframe.groupBy(dataframe['Email']).count().where(""count > 1"").drop(
""count"").show(truncate=False)
## Warn instead of fail
warnings.warn(UserWarning(""Some of the data in the 'Email' column is
not meeting the uniqueness requirement!"")
As a quality engineer, the color red is a sign that something needs attention. The best thing about using warnings for
known issues
is that it’s going to print yellow, which helps convey a message of “this is something known and doesn’t require immediate attention or concern.” This message is also conveyed to the other contributors when they go to execute the test suite, which
provides the test team a sense of ease because efforts can be put forth toward the team’s velocity instead of acting as a messenger
.
Another solid option for handling known data issues is the Pytest XFail marking. Similar to the Pytest Warnings capability, if the test fails, the result will come out yellow instead of red. I can’t double down on how helpful it is to be greeted by a color other than red, when applicable.
Image provided by the author
As you can see in the screenshot above, it’ll indicate the test including the XFail with a yellow ‘x’ in the test execution. However, the XFail indication is not marked in the assertion section of the test. XFail is marked at the top of the function similar to how we marked a test in the previous section for parametrization. Check out the XFail implementation below:
import pytest
@pytest.mark.xfail(reason=""Known data issue, expected to fail temporarily"")
def test_date_format():
## Rest of the test here
Again, like the warning option explored above, this helpful XFail marking allows the quality engineer to make the appropriate documentation as well as leave some contextual information for other teammates. A noteworthy additional benefit of XFail is that when a test that was expected to fail passes (i.e., the bug fix is addressed); then, the test fails which lets the quality engineer know it is now time to fix/alter the test.
Use with caution:
Please consider when/how/why these options would best be used for specific project needs. It’s important to look into any discrepancies in the data that are not expected, and then only after
determining the root cause of the failure
assessing the priority/severity of the failure with your team
should you explore employing these capabilities of Pytest.
On the other side of the coin, having the option to indicate certain tests with YELLOW flags makes it so that the necessary automated tests are documented, present in the test suite, and accessible as the project progresses towards production, where
hopefully
the data issues are no longer present.
Dynamically Manage Environment Variables
To wrap up the four key takeaways, I want to continue to address the nuances of testing data between different environments. At the beginning of the article, I mentioned how the data that I was testing was hosted in Amazon S3 buckets. The bucket names and paths were listed in an INI configuration file in the repository and fed to the various tests; however, the bucket names were altered slightly based on the specified environment.
[BUCKET]
S3 = my-dev-environment-bucket
[PATH]
FILE-PATH = pathway/to/dev/environment
The variations of the bucket names and associated paths were things that I was looking to manage dynamically from the terminal upon each test session, and that’s where the
Pytest
-E
Flag
came in handy. By using fixtures and the
pytest_addoption
function, I was able to indicate the desired environment with the
-E
flag along with the standard Pytest command to flip-flop between environments with each test execution.
The INI file was transformed slightly from its original state with the hard-coded variables, but it didn’t impact variable names being fed into the tests, making this adjustment very low impact to the suite. Then, the INI file itself became a temporary template during each test run and was restored upon completion of test runs. The variation looked like this:
[BUCKET]
S3 = my-{env}-environment-bucket
[PATH]
FILE-PATH = pathway/to/{env}/environment
With this adjustment to the INI file, along with some methods to manage the file during test sessions, the command became
pytest -E=dev
or
pytest -E=qc
or
pytest -E=prod
based on the desired environment. This alteration made the complication of switching between environments due to varying bucket names extremely simple. There was
no longer the dependency of a human having to remember to change the variable names in the INI file
each time there was a desire to perform test execution in a different environment. Any team member with access could now easily switch between environments from the command line.
To learn more about how to implement this flexibility in INI files with Pytest, you can check out my how-to article
here
.
Parting Thoughts
As a quick refresher, the four key takeaways for data quality testing that I highlighted in this article were:
Wrap Assertions as Conditionals for Logging Invalid Data
Determine Common Data Tests and Parametrize
Leverage Pytest Warnings and XFail for Known Data Concerns
Manage Environment Variables with
-E
Flag
The overall goal is to increase the level of confidence in the quality of the transformed data as a result of these strategies. I hope that these takeaways were enlightening and you were able to pick up some new tips and strategies that can come in handy for your next data project.
I would love to know how these ideas were received, so feel free to drop some feedback in the comments!
#QE4DE
Resources
AWS Glue Information
Amazon S3 Information
PySpark Documentation
Pytest Documentation
Getting Started with PySpark, Pytest, and Amazon S3 by Likitha Lokesh
PySpark DataFrames
Pytest Parametrize
Pytest Warnings
Pytest XFail
Pytest -E Flag
How to Manipulate INI File Variables by Taylor Wagner
Pytest
Python
Pyspark
Data Validation
AWS
--
--
Published in
Slalom Build
1.8K Followers
·
Last published
Nov 8, 2024
The Build Blog is a collection of perspectives and viewpoints on the craft of building digital products today, written by the technologists that build them. By builders, for builders.
Follow
Written by
Taylor Wagner
212 Followers
·
14 Following
Senior Data Engineer, Slalom Build
https://www.linkedin.com/in/taylorawagner/
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Leveraging the Power of OpenMetadata Data Quality Framework | by Teddy Crépineau | OpenMetadata,"Leveraging the Power of OpenMetadata Data Quality Framework | by Teddy Crépineau | OpenMetadata
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Leveraging the Power of OpenMetadata Data Quality Framework
Teddy Crépineau
·
Follow
Published in
OpenMetadata
·
6 min read
·
Nov 17, 2023
--
Listen
Share
OpenMetadata offers the possibility to execute data quality tests directly from the UI, making it a powerful tool for everyone in the company. With close to 30 table and column tests, OpenMetadata makes it easy for everyone to own data quality.
figure 1 — Creating a test from OpenMetadata UI
We understand that teams can use third-party tools, or have custom data quality rules. That is why we offer-third party integration with dbt and Great Expectations, and comprehensive API integration to ingest all your test results into the platform.
Now you may ask yourself, “What if I want to extend OpenMetadata tests, and define custom tests that can be set up and executed directly from the UI”? Then look no further. With OpenMetadata Data Quality Framework, you can define your data quality tests and make them available to all your users.
Disclaimer:
You will need to work with the OpenMetadata API and write some Python code. You won’t need extensive knowledge of these two concepts; only basic familiarity with them will be enough.
Configure your Test Definition
The first step is to create your test definition. A test definition describes the different elements of your test (e.g., name, description, parameter, etc.). You will define this new test definition as shown below.
{
""description"": ""<you test definition description>"",
""entityType"": ""<TABLE|COLUMN>"",
""name"": ""<your_test_name>"",
""displayName"": ""<your display name>"",
""testPlatforms"": [""OpenMetadata""],
""supportedDataTypes"": [""NUMBER|TEXT|CHAR""],
""parameterDefinition"": [
{
""name"": ""<name>"",
""displayName"": ""<display name>"",
""description"": ""<desription of the test>"",
""optionValues"": [""optionOne"", ""optionTwo""],
},
{
""name"": ""<name>"",
""displayName"": ""<display name>"",
""description"": ""<desription of the test>"",
""optionValues"": [""optionOne"", ""optionTwo""],
}
]
}
Let’s go over each element one by one:
description
: this is the description of your test. It will provide your users with more information and details
entityType
: set
TABLE
if you are defining a table-level test or
COLUMN
if you are defining a column-level test
name
: the name of your test definition. It should be unique across all test definitions
displayName
: the name of your test definition as it will appear in OpenMetadata UI
testPlatforms
: this is an array of test platforms. To make your test executable through OpenMetadata, this should be set to
OpenMetadata
supportedDataTypes
: (optional) pass a list of supported data types for your test definition. This will limit the ability for users to create the test only against columns with supported data types. The complete list of data type values can be found
here
.
parameterDefinition
: a list of parameters that will be used to configure a test
name
: the name of your parameter
displayName
: the display name of your parameter as it will appear in the UI
description
: the description for your parameter. This will be used to populate the tooltip in the UI
optionValues
: If you want to make your parameter value a selectable value from a dropdown menu
Creating your Test Definition in OpenMetadata
Create your Test Definition in OpenMetadata
Once you have defined your test definition using the JSON template above, you must make the test available in OpenMetadata. Making it available in OpenMetadata will make your test usable through the UI. Users can navigate to the test creation page and select it from the dropdown menu.
For this step, you will need a way to send an API
POST
request to your OpenMetadata server endpoint
/api/v1/dataQuality/testDefinitions
. For this example, we’ll use
curl
(more info about
curl
here
).
Test Definition
{
""description"": ""Compute entropy for a given column"",
""displayName"": ""Column Entropy To Be Between"",
""entityType"": ""COLUMN"",
""name"": ""columnEntropyToBeBetween"",
""testPlatforms"": [""OpenMetadata""],
""parameterDefinition"": [
{
""name"": ""minEntropy"",
""displayName"": ""Min"",
""dataType"": ""INT"",
""required"": true,
""description"": ""minimum entropy""
},
{
""name"": ""maxEntropy"",
""displayName"": ""Max"",
""dataType"": ""INT"",
""required"": true,
""description"": ""maximum entropy""
}
]
}
curl
Request
curl --location --request POST '<yourServerHost>/api/v1/dataQuality/testDefinitions' \
--header 'Authorization: Bearer <yourJWTToken>' \
--header 'Content-Type: application/json' \
--data-raw '{
""description"": ""Compute entropy for a given column"",
""displayName"": ""Column Entropy To Be Between"",
""entityType"": ""COLUMN"",
""name"": ""columnEntropyToBeBetween"",
""testPlatforms"": [""OpenMetadata""],
""parameterDefinition"": [
{
""name"": ""minEntropy"",
""displayName"": ""Min"",
""dataType"": ""INT"",
""required"": true,
""description"": ""minimum entropy""
},
{
""name"": ""maxEntropy"",
""displayName"": ""Max"",
""dataType"": ""INT"",
""required"": true,
""description"": ""maximum entropy""
}
]
}'
You will need to replace
<yourServerHost>
with the URL where the OpenMetadata server is hosted (e.g.,
http://localhost:8558
if you are working with a local installation). You will also need to replace
<yourJWTToken>
with your personal JWT Token (i.e., go to
settings > bot
to find your JWT Token or generate one).
figure 2 — Our custom entropy test in OpenMetadata UI
Define the Execution Logic
Now that we have created our new test definition, we’ll need to tell OpenMetadata how we want to process and execute this test. To accomplish this, we can leverage OpenMetadata Data Quality Framework.
Step 1 — Create a Python Package
The first step is to create a new Python package. This will allow you to version control your custom tests and add unit tests. As we’ll be leveraging Python
namesapce
package, our package needs to have a specific structure.
For a column level test:
<yourFolderName>/metadata/data_quality/validations/columns/<sqlalchemy|pandas>/yourTestName.py
<yourFolderName>/metadata/data_quality/validations/table/<sqlalchemy|pandas>/yourTestName.py
Use
pandas
if you are writing a test that will be used for data lake connectors; otherwise, use
sqlalchemy
for all other database connectors
For our column entropy test, our folder structure will look like this
folderName/
├── setup.py
└── metadata/
├── __init__.py
└── data_quality/
├── __init__.py
└── validations/
├── __init__.py
└── column/
├── __init__.py
└── sqlalchemy/
├── __init__.py
└── columnEntropyToBeBetween.py
Important:
note how our test file name matches the above test definition name. This is important to respect.
Step 2 —
setup.py
and
__init__.py
files
For your
setup.py
file, you will, at minimum, need to call the
setup
function calling
find_packages()
for the
packages
argument.
For our entropy test, we have added the requirements needed to execute our test (e.g.,
scipy
)
from setuptools import find_packages, setup
base_requirements = [""scipy""]
setup(name='custom-tests',
version='0.0.1',
description='OpenMetadata Custom Tests',
packages=find_packages(),
install_requires=list(base_requirements)
)
Finally, you must add the line below to all your
__init__.py
files. This tells Python that we are working with
namespace
packages (this is at the core of OpenMetadata flexible Data Quality Framework).
__path__ = __import__('pkgutil').extend_path(__path__, __name__)
Step 3 — Defining your Test Execution Logic
One of the last two missing pieces to our puzzle is adding the logic to execute our test. You will need to create a new class name
<YourTestName>Validator
that will inherit from
BaseTestValidator
.
You will then need to implement the
run_validation
method where your core logic is being executed.
from itertools import chain
from sqlalchemy import Column, inspect
from scipy import stats
from metadata.data_quality.validations.base_test_handler import BaseTestValidator
from metadata.generated.schema.tests.basic import TestCaseResult, TestResultValue
from metadata.utils.entity_link import get_decoded_column
class ColumnEntropyToBeBetweenValidator(BaseTestValidator):
""""""Implements custom test validator for OpenMetadata.
Args:
BaseTestValidator (_type_): inherits from BaseTestValidator
""""""
def _get_column_name(self) -> Column:
""""""get column name from the test case entity link
Returns:
Column: column
""""""
column = get_decoded_column(self.test_case.entityLink.__root__)
columns = inspect(self.runner.table).c
column_obj = next(
(col for col in columns if col.name == column),
None,
)
if column_obj is None:
raise ValueError(f""Cannot find column {column}"")
return column_obj
def run_validation(self) -> TestCaseResult:
""""""Run test validation""""""
rows = self.runner.select_all_from_table(self._get_column_name())
list_of_values = list(chain.from_iterable(rows))
entropy = stats.entropy(list_of_values)
min_bound = self.get_min_bound(""minEntropy"")
max_bound = self.get_max_bound(""maxEntropy"")
return self.get_test_case_result_object(
self.execution_date,
self.get_test_case_status(min_bound <= entropy <= max_bound),
f""Found entropy={entropy} vs. the expected  min={min_bound} and max={max_bound}]."",
[TestResultValue(name=""entropy"", value=str(entropy))],
)
For our entropy test above, we created our new class ColumnEntropyToBeBetweenValidator, which inherits from
BaseTestValidator
. We then implemented
run_validation,
fetching the rows from our table column with
self.runner.select_all_from_tabl(self._get_column_name())
.
self._get_column_name()
fetches the column for which our test was created. We then compute the entropy for the column
stats.entropy(list_of_values)
. Finally, we fetch the min and max bounds that were set
self.get_min_bound(""minEntropy"")
— note how the argument we pass to
get_<min|max>_bound
corresponds to the
name
of our test definition. Finally, we call
self.get_test_case_result_object(...)
which will return the status of our test alongside other information.
Step 4 — Install your package in your environment
Finally, you must install your package where your OpenMetadata ingestion is running using
pip install <...>
.
If you enjoy OpenMetadata and want to contribute new tests to the community, you can head to our
GitHub Repo
, browse the
Data Quality code
, and open a new PR. 😊
Data Governance
Data Engineering
Data Catalog
Data Quality
Data Quality Management
--
--
Follow
Published in
OpenMetadata
581 Followers
·
Last published
Oct 15, 2024
OpenMetadata is an open-source project that is driving Open Metadata standards for data. It unifies all the metadata in a single place in a Centralized Metadata store and helps people Discover, Collaborate, and Get their data right.
Follow
Follow
Written by
Teddy Crépineau
160 Followers
·
203 Following
Three o’clock is always too late or too early for anything you want to do — Jean-Paul Sartre.
http://www.teddycrepineau.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Automating data quality checks in Google BigQuery | by Michaël Scherding | Medium,"Automating data quality checks in Google BigQuery | by Michaël Scherding | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Automating data quality checks in Google BigQuery
Michaël Scherding
·
Follow
5 min read
·
Feb 29, 2024
--
3
Listen
Share
In today’s data-driven world, ensuring the quality of your data is paramount. This article outlines a comprehensive approach to automate data quality checks across multiple tables within a Google Cloud Platform (GCP) project, utilizing BigQuery, Cloud Functions, and Cloud Scheduler.
Overview of the System
The system comprises several key components:
Centralized test definition table
: Holds configurations for checks.
Centralized rejection log table
: Logs issues found during checks.
Stored Procedures
: Perform the data quality checks.
Cloud Function (“Executor”)
: Reads test definitions and triggers checks.
Cloud Scheduler Jobs
: Scheduled jobs that trigger the Cloud Function.
Step 1: Designing BigQuery tables
Centralized test definition table
Create a BigQuery table with fields to specify the project, dataset, table, column for checks, the type of check (e.g., “check_not_null”), and the check’s schedule.
Schema Example:
project_name
: STRING
dataset_name
: STRING
table_name
: STRING
column_name
: STRING
check_type
: STRING
schedule
: STRING
To enhance the centralized test definition table for dashboarding purposes, allowing you to track the status and execution times of your data quality checks, you would add fields to record the last run timestamp and the last status of each check. This addition not only helps in monitoring and auditing the health of your data but also enables quick identification of issues and trends over time with:
last_run
: TIMESTAMP - The timestamp of the last time the check was executed.
last_status
: STRING - The outcome of the last check (""Success"", ""Failure"", or any other relevant status).
Centralized rejection log table
This table will record details of data quality issues.
Schema Example:
project_name
: STRING
dataset_name
: STRING
table_name
: STRING
column_name
: STRING
rejection_reason
: STRING
rejected_at
: TIMESTAMP
data
: STRING
Integrating a
data
column in JSON format into your centralized rejection log table is a strategic approach to capturing and analyzing the specifics of each data quality issue. This column not only stores the entire row of data that failed a quality check but also enables detailed monitoring and analysis of rejections by dataset, table, column, etc.
With the centralized rejection log table, creating a dashboard to monitor data quality becomes straightforward.
Rejection over time
: Display the number of rejections over time to identify trends or patterns. This can help in spotting when particular changes lead to increased data quality issues.
Top offenders
: Aggregate rejection data by
dataset_name
,
table_name
, or
column_name
to identify which areas of your data warehouse are most prone to issues. This can help prioritize efforts to improve data quality.
Detailed drill-downs
: Use the JSON details in the
data
column to create drill-down reports that allow users to explore the specifics of rejected rows. This can be invaluable for debugging and rectifying source data issues.
Rejection reasons breakdown
: Analyze the
rejection_reason
field to understand the most common causes of data quality issues, informing potential areas for improvement in data collection or processing pipelines.
Step 2: Implementing stored procedures
Stored procedures in BigQuery allow for the encapsulation of complex logic that can be reused. Here’s an example of a stored procedure for checking for null values in a specified column of a table. This procedure logs any findings to a centralized rejection log.
For deploying a Cloud Function that reads from a test definition table and creates Cloud Scheduler jobs, consider this Python example using Flask framework for the Cloud Function:
Step 4: Setting up cloud scheduler jobs
While the Cloud Function
schedule_checks
dynamically manages Cloud Scheduler jobs based on entries in the test definitions table, understanding how these jobs trigger the function is essential. Each job, when executed, makes an HTTP POST request to the Cloud Function endpoint with the necessary parameters extracted from the test definitions. This is illustrated in the Cloud Function code above where each job's HTTP target is constructed.
Step 5: The executor function
The executor function is designed to:
Receive parameters from a Cloud Scheduler job via an HTTP POST request.
Parse these parameters to determine the specifics of the data quality check to be performed, such as which table and column to check and what kind of check to perform (e.g., checking for null values).
Call the appropriate stored procedure in BigQuery to perform the specified data quality check.
Handle the response from the stored procedure, including logging any errors or issues detected during the check.
How it will be organized in BigQuery
This data quality control logic is designed to be project-specific within Google Cloud’s BigQuery. Each project will have a dedicated dataset named
quality
, containing two central elements: a table for test definitions (
test_definitions
) and a table for logging rejected rows (
rejection_log
). Additionally, the dataset will host various routines (stored procedures) for performing the quality checks.
The entire setup can be efficiently deployed across one or multiple projects using Terraform, allowing for quick and consistent implementation of data quality testing at the project level. This approach not only standardizes data quality checks across an organization but also simplifies management and scalability of data governance practices.
Conclusion
The executor function plays a pivotal role in the automated data quality control system. It acts as the bridge between the Cloud Scheduler jobs (which schedule and trigger data quality checks) and the BigQuery stored procedures (which perform the actual checks). By dynamically parsing parameters from incoming requests and executing the corresponding stored procedures, this function enables automated, scheduled data quality checks across multiple tables and datasets within a GCP project.
Hope it was fun, see ya 🤟.
Bigquery
Data Quality
Data Quality Management
Clean Data
--
--
3
Follow
Written by
Michaël Scherding
198 Followers
·
19 Following
Bio :
https://bio.link/michael_scherding
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality and Testing Frameworks | by Kovid Rathee | Cognizant Servian,"Data Quality and Testing Frameworks | by Kovid Rathee | Cognizant Servian
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Photo by
Sigmund
on
Unsplash
Data Quality and Testing Frameworks
A short introduction to open-source data quality & testing tools — dbt, Deequ, and Great Expectations
Kovid Rathee
·
Follow
Published in
Cognizant Servian
·
8 min read
·
May 11, 2022
--
3
Listen
Share
Background
D
ata quality has always been an important theme while building data systems. Still, data engineering teams of the past have had bigger fish to fry, making sure the data lands up from one place to another (in whatever way or form). Data quality often surfaces as a problem when the data has already reached its destination in a data pipeline you engineered.
It soon becomes a pain because the data you so painstakingly moved between two worlds cannot help you extract the value you expected. Why? Because some columns have null values, some have had a variety of date formats, while others have a mixed bag of data quality issues that couldn’t have been caught by manually checking a sample of the data.
The data quality problem is offloaded to the data analysts and analytics engineers to solve. I believe it is not their problem to solve. It would be best if you built quality checks and tests into the pipeline that moves the data around, reshapes it, and, in some cases, summarizes it. The moment you detach quality and testing from the pipeline, issues arise.
Getting Started with Data Quality
It won’t be presumptuous to assume if you have a reasonably mature deployment system in place, you’d already have some unit and integration tests for your code. They might not be related to the quality of data at all. Adding the capability to test data within your pipeline involves choosing the right tools. You’ll be tempted to write SQL queries or Python functions to test data. I know because I have been that person. If you write custom tests for all of your data, it will become its own separate project and eat up quite a lot of your time. Don’t do that.
Use templates and try to write the minimum number of tests to cover the highest number of use cases while still maintaining the individuality of the tests. Tools like dbt, Great Expectations, PyDeequ, a combination of Cucumber, Gherkin, and Jinja2 with pytest, etc., allow you to do that. You can go the commercial route and use something like Ataccama ONE, Informatica Data Quality (IDQ), etc. Still, in this article, I’ll take you through some of the most widely adopted open-source data quality frameworks, such as Deequ, Great Expectations, dbt, etc. At the end of the article, you’ll hopefully be more aware of the DQ space and possibly in a better position to make data quality central to your data pipelines.
Great Expectations
This tool is built on the core abstraction of an
Expectation
, which, essentially, describes what the expected shape of the data (data type, null-ness and otherwise, precision, etc.) should be. The reason for this tool’s existence is that most of the automation testing tools at the time only supported tests for code. None of them offered tests for data. This guiding principle is quite apparent from
the article announcing the release of Great Expectations back in 2018
:
Instead of just testing code, we should be testing data. After all, that’s where the complexity lives.
If you wanted to test the data, you’d probably have to write custom SQL queries on every layer of your data pipeline. We all know how much fun that can turn out to be. Data engineers and analysts, more or less, come to terms with messy data after a while and write SQL to fix their data pipeline jobs and reports (while crying their hearts out). A lot of other less technical and business-facing roles might not be able to fix data with SQL queries, worsening the already terrible situation. From simple data pipelines to reports and dashboards, from MLOps to transactional databases, Great Expectations can fit anywhere, which is why it has seen widespread adoption in the last few years.
Some Expectations
Great Expectations allows you to define expectations in a JSON file or inline with your code. Below are some examples of the in-line Expectations from a survey data set, where you’ll see the number of data quality aspects being checked.
Some types of Expectations.
The above example uses pre-defined Expectations for Pandas. Great Expectations has backend support for Pandas, Spark, and SQLAlchemy. You can see which expectations are available for your choice of backend
here
, as shown in the image below:
Backend Support for Great Expectations
If you don’t meet your Expectations in the list of pre-defined Expectations, you can always
create your own
.
Integrating Great Expectations Into Your Data Workflow
Whether feeding data into a
SageMaker model
, a
feature validation engine
, a set of reports on top of a
Snowflake data warehouse
, an
ML workflow engine
, a
data orchestrator like Airflow
, a data lake on Databricks, or even a traditional
transactional database
, or a
non-traditional transactional database
, the only reasonable way to use Great Expectations is to run it as part of your data or CI pipeline.
Whatever your tools and workflow, you can find a way to integrate Great Expectations into it. I was recently using Great Expectations with PySpark in a JupyterHub setup. I found
one of my ex-colleagues' blogpost
and
a Docker-based quick start tutorial by Dataroots
very useful.
dbt
In
this blog post
, dbt presents a good analogy of a full-scale data pipeline and the different check posts where your pipeline needs testing, data quality checks, and validation. Ideally, such a check post should exist
in your data pipeline where data is being ingested, pushed out, or transformed
. Using
dbt utils
, you can perform both structural and data tests for your models and transformations. You can
start here
with a post by my colleague where she talks about setting up basic tests and data quality checks using dbt.
Some Data Quality & Testing Examples using dbt
Using the
survey_data
data set as an example, you can define, more or less, the same tests that you specified using Great Expectations. dbt uses YAML files to define tests. Here’s an example covering four types of data quality tests using the
dbt_utils
package. Here’s how you can define tests in dbt:
accepted_range
— To validate whether the column value lies within the expected range.
not_accepted_values
— To check whether the column contains unacceptable values.
not_null_where
— A simple
IS NOT NULL
quality check for any given column of your table represented as a dbt model.
unique_combination_of_columns
— As dbt integrates tightly with data warehouses and lakes, it also provides data quality checks supporting specific types of data models that you can use generically. Engineers use this check to ensure the uniqueness of columns to create surrogate keys for data warehouses.
GitHub Gist for a sample YAML file for defining tests in dbt.
dbt Helps with Testing & Data Quality of Transformation Layers
dbt offers a deeper level of testing with very useful predefined tests for data warehouse-modeling techniques like Dimensional modeling and Data Vault 2.0. Tests and data quality checks involving aggregates, case statements, table comparisons, and summaries are extremely useful in the transformation layers of the data pipeline. A few examples of what you’d want to check in a hypothetical scenario where you have a data lake in S3, a staging layer, and a data warehouse in Snowflake are as follows:
When moving data from S3 to the
Snowflake staging layer
, you should perform basic data quality checks like
checking for NULLs
, column formats, data types, etc.
When moving data from the
Snowflake staging layer to DataVault
, you should also check for column counts and other aggregates for loose integrity checks.
When moving data from the Snowflake DataVault to a dimensional schema, you should be checking again for column counts in the source and the target. The more complex individual, row-level tests are performed with the help of introspective macros, which allow you to iterate over the result of a query.
dbt provides
several helper libraries
that you can use to achieve this testing and data quality functionality.
Deequ
Suppose you’re working with Spark in any way or form with EMR,
Glue
,
Databricks
, Jupyter notebooks, etc. In that case, you’d have heard of the Spark-native
library for unit testing
and measuring data quality called
Deequ
. This utility comes from AWS Labs. They have also released a Python avatar for Deequ called PyDeequ, given the popularity of PySpark. Deequ takes a slightly different approach than dbt and Great Expectations.
Analyze, Suggest, Verify, and Store
Deequ lets you profile and validate data using Analyzers. In addition to the profile summaries, Deequ also helps you with field-level data validation. Based on the profiles of your data and some heuristic rules, Deequ’s
Constraint Suggestion functionality
suggests what constraints you should be running in your Verification suite to enforce a quality check on your data. Here are a couple of examples of the suggestions Deequ might make for your data set:
Completeness
— Measures the presence of null values, for example
isComplete(""email"")
or
isComplete(""phone"")
.
Consistency
— Consistency of data types and value ranges, for example,
isContainedIn(""state""), (Array(""VIC"",""NSW"",""ACT"",""QLD""))
.
For a more detailed explanation of the data validation and verification process, and the philosophy behind the design choices and types of constraints, read
this paper
which contains research done at
Amazon
for making Deequ possible.
In addition to the suggested checks, there are a
host of checks
available that you can add to the
Constraint Verification phase
. You can write your checks too. Here’s a simple example of how you’d run checks using the
VerificationSuite
in PyDeequ using the
survey_data
dataset and similar tests as performed using dbt and Great Expectations:
Once the constraint verification process completes, PyDeequ will write a summary report on a path of your choosing; in this case, we’re printing the results DataFrame,
survey_df_check_result
, to the console. It helps to see if your fixes to correct data somewhere in your data pipeline or at the source have worked or not. PyDeequ also allows you to define a Metrics Repository, which can help you track the quality of your data over time.
Different Approaches to Tackling the Data Quality Problem
Deequ provides is a great way to work with your Spark code, whether written in Scala, Python, or SparkSQL. As long as Deequ receives data in a DataFrame (or a DynamicFrame, in the case of AWS Glue), it will be fine. Note how the three tools we’ve discussed have approached the same problem with different techniques. That’s happened because of two main reasons — the processes that these tools support are different, and the types of tests those processes need are different. dbt, as they sell it, is for transformations, Great Expectations is a general-purpose data quality suite, and Deequ is for large-scale data quality checks when you’re underlying infrastructure is on Spark.
Conclusion
Most of the time, your data engineering project timelines don’t allow you to add in data quality checks is a valid excuse. Not reviewing code, not writing tests, and not taking care of data quality won’t hurt at the time. However, it will eventually catch up with your project and diminish the return on investment that the business will get from the whole exercise.
So what do you have to do not to make that excuse? Push the data quality agenda from the word go. Initially, no one would listen (been there). Keep pushing until one day you get the budget to do it. And if all the efforts fail, forgive everyone for they know not what they do, and maybe work at some other place that values and appreciates data for what it’s worth.
P.S. — Shoutout to
Deepank Chopra
,
Marat Levit
, and
Adam Weisser
for reviewing and helping out.
Data Engineering
Data Quality
Automation Testing
Data Observability
Data
--
--
3
Follow
Published in
Cognizant Servian
1.93K Followers
·
Last published
Aug 14, 2023
Cognizant Servian was formed on the deep expertise of two acquisitions: Servian and Contino. Together, we are experts in innovative data and analytics, digital, customer engagement and cloud solutions to evolve your competitive advantage.
Follow
Follow
Written by
Kovid Rathee
3.4K Followers
·
270 Following
I write about tech, Indian classical music, literature, and the workplace among other things. 1x engineer on weekdays.
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
A guide to open-source data quality tools in late 2023 | by Bruno Gonzalez | Medium,"A guide to open-source data quality tools in late 2023 | by Bruno Gonzalez | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
A guide to open-source data quality tools in late 2023
Bruno Gonzalez
·
Follow
7 min read
·
Sep 11, 2023
--
1
Listen
Share
Photo by
Konstantin Evdokimov
on
Unsplash
If you are wondering why data quality matters and why it is important, the idea can be summarized with the concept of “garbage in, garbage out”, and how caring and improving data quality will lead to better decisions and outcomes.
How can you improve data quality?
There are known unknowns. That is to say, there are things that we know we don’t know.
Donald Rumsfeld
The answer doesn’t fit in one article, but everything starts with testing. One of your “known unknowns” is the quality of your data, and you can leverage data quality tools to tackle the first stage of data quality improvement.
There are plenty of tools and many articles describing their features,
Gartner reports
, and the excellent
MAD (ML/AI/Data) landscape
that has a special section for “Data Quality & Observability” (it’s a great starting point if you’re looking for options)
In this article, I want to focus on open-source tools you can integrate into the so-called “modern data stack”.
The tools
There are many tools out there, but sadly, there are few options in the open-source ecosystem. You can “force” some of the tools to enter the podium, but from my perspective, there are only two general-purpose open-source tools for data quality:
Great Expectations
and
Soda Core
.
Let me know if you want another article about the “other tools” I disregarded and why, but I did it mainly because they’re platform-specific or not maintained. Please leave me your thoughts in the comments.
About the selected tools, I have hands-on experience with both, integrating them into our data pipelines. Both have their nuances, and none of them covered 100% of our use cases, but they’re mature tools you can use in production workloads without question.
Let’s do a quick overview, and then I will review each tool’s different aspects and features. I will focus on the definition of data quality metrics. You can comment if you want me to dig deeper into other features.
Great Expectations:
Python library (retired
CLI
since
April 2023
)
Allows you to define assertions about your data (named
expectations
)
Provides a declarative language for describing constraints (Python + JSON)
Provides
expectations gallery
with 300+ pre-defined assertions (50+
core
)
A long
list of integrations
, including data catalogs, data integration tools, data sources (files, in-memory, SQL databases), orchestrators, and notebooks
Runs data validation using
Checkpoints
Subject matter expert friendly for expectations definition using
data assistant
Automatically generates
documentation
to display validation results (HTML)
No official docker image
Cloud version
available
Great
community
regarding contributions (
GitHub
), knowledge exchange and Q&A (
Slack
)
Soda Core:
CLI tool and Python library
Allows you to define assertions about your data (named
checks
)
Provides a human-readable, domain-specific language for data reliability called Soda Checks Language (YAML)
Includes
25+ built-in metrics
, plus the ability to create user-defined checks (
SQL queries
)
Compatible with
20+ data sources
(files, in-memory, SQL databases)
Runs data validation using
scans
Display scan results in the
CLI
(save to file available) or access them
programmatically
Collects usage statistics (you can
opt out
)
Docker image
available
Cloud version
available
Decent community regarding contributions (
GitHub
), knowledge exchange and Q&A (
Slack
)
Now that the overview is done let’s dive into some aspects I consider important:
Data source integration
Available assertions
Custom assertions (extensibility)
Validation execution
Bonus: GitHub statistics
Data source integration
When you’re looking for a data quality tool, you want to be able to use it in different contexts, especially with various data sources.
Both tools provide a reasonable set of compatible data sources, including the more prominent data warehouse solutions (Snowflake, BigQuery, Redshift), SQL OLTP databases (PostgreSQL, MySQL, MS SQL Server), query engines (Athena, Trino), and text files (through Pandas).
Great Expectations has a longer list of compatible data sources, but I don’t think it makes a huge difference (it does, in case the data source you use is not listed).
Check the complete list from
Great Expectations
and
Soda Core
.
Available assertions
An assertion is a statement about your data, something you
expect
to be true, and you need to
check
to assess the quality of your data.
Again, both tools provide a set of predefined expectations/checks that you can test against attributes in your data source. Most common: validate missing counts, row counts, schema validation, regex match, reference check, and numeric metrics (min, max).
If you look at the numbers, you may think Great Expectations is way beyond Soda Core, but it’s not an apples-to-apples comparison.
For Great Expectations, an expectation is an assertion about the data, and you could need many of them to define a data quality metric. On the other hand, Soda Core combines metrics and thresholds to define a check.
Example: NULL count check
Let’s see a simple example of a
NULL
count check on a column, starting with the assertion “
column_to_check
shouldn’t have
NULL
values”.
Great Expectations:
validator.expect_column_values_to_not_be_null(""column_to_check"")
Soda Core:
checks for table_to_check:
- missing_count(column_to_check) = 0
But what if now I realized I want to consider “NA” as
NULL
as well?
Great Expectations:
validator.expect_column_values_to_not_be_null(""column_to_check"")
validator.expect_column_values_to_not_be_in_set(""column_to_check"", [""NA""])
Soda Core:
checks for table_to_check:
- missing_count(column_to_check) = 0:
missing values: [NA]
In SodaCL (
Soda Checks Language
), you use the same metric and threshold and add a column config. In Great Expectations, you need to add another expectation. The conclusion is that you cannot compare the expectations number with the metrics number because conceptually they are different things.
You need to understand the logic behind each paradigm and define your metrics based on your needs.
Custom assertions
Your business, your rules.
Both tools provide an extensive list of assertions, but depending on your needs, you must define your own custom assertions. The paradigm is slightly different, so creating custom assertions involves different steps.
Great Expectations (
documentation
):
Choose between creating private expectations or contribute to the open source project
Understand the different
expectation classes
Code the Python class that implements the metric and validation
Once defined it can be used anywhere
Example:
here
Soda Core (
documentation
):
Use common table expressions (CTE) or SQL queries (a file can be referenced)
Defined for a specific table
Example:
checks for dim_reseller:
- avg_order_span between 5 and 10:
avg_order_span expression: AVG(last_order_year - first_order_year)
Validation execution
Both tools provide a CLI and programmatic way to run validations. In Great Expectations they call them
Checkpoints
, and the most common way to run them is programmatically. In Soda Core you run a
scan
that executes checks, and the most common way to run it is using the CLI.
As I mentioned before, Great Expectations retired their CLI, but you can still use it to run checkpoints. Soda Core allows you to run
programmatic scans
using Python, providing an API to access logs and results.
You can execute the validations using your local environment, but the idea is to integrate them into an orchestration tool that could run them without your assistance, leaving the results in a repository or database. Documentation available for
Soda Core
and under
integrations
for Great Expectations.
GitHub statistics
Given they’re open-source tools, it’s worth checking GitHub statistics. Great Expectations is more community driven and has way more activity than Soda Core.
The statistics were collected on September 11, 2023.
Source:
https://github-stats.com/great-expectations/great_expectations
Source:
https://github-stats.com/sodadata/soda-core
Conclusions
As with any tool, you need to understand your use cases to see if they fit your needs and the gaps. As I mentioned, we used both in my current job for different use cases, and you need to do the homework to figure that out.
Let me do a quick summary of the aspects we analyzed.
Great Expectations:
Bigger community and contributions
Extensive list integrations with data sources and tools
The open-source version is incredibly complete
A lot of Python code (I love it, but it could be a difficulty) that could be alleviated using
data assistant
Programmatic validations are preferred
Steep learning curve to extend
Soda Core:
Most common data source integrations available
Easier to define assertions in
YAML
format
SodaCL is complete and powerful
Execute validations with CLI is preferred
SQL code for custom assertions
Both have their wins and nuances, so if you want me to continue analyzing these tools, consider subscribing and leave a comment with what you want to read next.
This article was originally posted in Data Quality Guru Substack:
https://dataqualityguru.substack.com/p/open-source-data-quality-tools-comparison
Follow me to support my work!
Data Quality
Data
Open Source
Data Engineering
--
--
1
Follow
Written by
Bruno Gonzalez
220 Followers
·
72 Following
Technical writer at Data Quality Guru | Senior Data Engineer |
https://dataqualityguru.substack.com/
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Why an open standard for data quality is important | by Teddy Crépineau | OpenMetadata,"Why an open standard for data quality is important | by Teddy Crépineau | OpenMetadata
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Why an open standard for data quality is important
Teddy Crépineau
·
Follow
Published in
OpenMetadata
·
6 min read
·
Jul 12, 2022
--
Listen
Share
Written by:
Pere Miquel Brull
and
Teddy Crépineau
Data is at the heart of every organization, whether it’s to improve business strategies and operations, product innovation, improve customer experience, or build new efficiencies, the role of data is becoming more and more important. Considering this new reality, driven and backed by data, an important question arises -
Can we trust the data?
Data Quality issues are plaguing organizations and preventing them from realizing the full potential of data.
From Data to Data Products
Data is becoming the end goal, not just the means with which to gain business insights. As a result, companies invest more resources to ensure that data is available, discoverable, and usable for all the teams. One key factor driving this trend is building
Trust In Data
.
Data-related teams should focus on finding insights, building solutions, or other productive activities rather than searching and validating data. Therefore, teams producing data are no longer just concentrating on gathering, but also testing data like any other piece of software. This new requirement brings the need for innovative solutions in the Data Quality and Testing space.
The Current Landscape
When there’s a lack of a clear winner for solving Data Quality issues in the market, one can usually observe two scenarios:
Custom and in-house Solutions
Third-party Solutions
Custom Solutions
Many organizations build in-house solutions, as existing tools either don’t cover specific use cases or can’t easily be integrated within their current processes or architectures. These proprietary solutions need to constantly evolve to keep up with the growing needs and changing use cases and require a sizable investment in both people and resources.
Third-party Solutions
Many tools exist today to solve data quality issues each focusing on a different set of problems. Given no tool is addressing the problem comprehensively, multiple tools are required by an organization to address its needs. These tools also have an overlapping feature set that creates confusion for the users. With a growing number of tools organizations are suffering from data architecture complexity.
In the current crop of Data Quality software, one solution stands out from the crowd by providing a framework for data contracts and a platform to enable and automate them -
Great Expectations
.
Metadata Standard for Data Quality
At OpenMetadata, we believe that Open Standards for metadata is a must to make metadata fuel innovation and for true interoperability in the metadata space. Data quality metadata lacks standard schemas to capture the quality tests and results. With standardized schemas, teams see the benefit of tool portability and choose the best one for the job freely. Also, the tests that are created in an organization can be freely exchanged and reused across teams.
Great Expectations has been doing an excellent job in defining Data Quality tests; thanks to the OpenSource community behind the project. Opening the doors to a community of users with different backgrounds, use cases, and degrees of experience brings in the richness of diversity. This can be seen by the comprehensive suite of data quality tests called
Expectations
, developed by the community.
This is a goal we share in the OpenMetadata community. Building a metadata standard is not something that a few can drive, which is why we are basing our Data Quality efforts on top of what Great Expectations brings to the table.
Data Quality Integration
Scattered tools and broken user workflows are one of the biggest blockers to unlocking the true potential of data. That’s why we are bringing the data profiling and validation information together with tables’ definitions, schemas, lineage, and ownership.
Providing users with an overall understanding of the different facets of a data asset, and how it interacts with the rest of the entities within a Data Platform, OpenMetadata is a one-stop solution for finding answers and fueling collaboration.
Porting Great Expectations’ test case definitions to the metadata standard as
JSON Schemas
has helped us build a robust and validated approach to data quality as part of our Ingestion Framework.
Moreover, following the API-centric design, any tests adhering to the standard can be collected with their results shown directly in the UI. In the upcoming releases, we are applying this thought aimed directly at Great Expectations, providing an action that will allow current GX users to push tests to OpenMetadata.
How to Ingest Great Expectations Test Results to OpenMetadata?
With the Actions feature, GX offers the ability to trigger actions after running your Expectations. OpenMetadata takes advantage of this feature to collect metadata generated by GX and centralize it with the other existing metadata helping users from both communities.
Here are the steps:
The steps below assume you have a running OpenMetadata instance and have created Expectations and checkpoint files in GX.
Step 1: Pip Install
First,
pip install
our submodule containing the Great Expectations action.
pip install openmetadata-ingestion[great-expectations]
This command will download the latest version of
openmetadata-ingestion
. You need to make sure to download the same version as your running OpenMetadata server. For example, if you have OpenMetadata server 0.12.2 running, you will need to download
openmetadata-ingestion
version 0.12.2.x using the below command
pip install 'openmetadata-ingestion~=0.12.2.0[great-expectations]'
Step 2: Configure your Checkpoint File
Next, configure the checkpoint file with the validator to run against your data. It is also where you will configure your custom actions that will be performed after the validation. You can find out more about checkpoints
here
.
[…]
action:
module_name: metadata.great_expectations.action
class_name: OpenMetadataValidationAction
config_file_path: path/to/ometa/config/file/
ometa_service_name: my_service_name
[…]
ometa_service_name
is optional. If you don’t specify it, when looking for the table entity, it will look for the service name where the table entity name exists. If the same table entity name exists in more than one service name, it will raise an error.
Step 3: Create your OpenMetadata Config File
config.yaml
To ingest GX results into OpenMetadata, you will need to specify your OpenMetadata security configuration for the REST endpoint. This configuration file needs to be located inside the
config_file_path
referenced in step 2 and named
config.yaml
.
hostPort: http://localhost:8585/api
authProvider: azure
apiVersion: v1
securityConfig:
clientSecret: {{ env('CLIENT_SECRET') }}
authority: my
clientId: 123
scopes:
- a
- b
You can use environment variables in your configuration file by simply using
{{ env('<MY_ENV_VAR>') }}
. These will be parsed and rendered at runtime allowing you to securely create your configuration and commit it to your favorite version control tool.
As we support multiple security configurations, you can check out
the documentation page
for more information on specific providers.
Step 4: Run your Checkpoint File
With everything set up, it is now time to run your checkpoint file.
great_expectations checkpoint run <my_checkpoint>
What’s Next?
As we strive to always provide the best user experience, we are working on providing generic test definitions. This feature will allow users to ingest all data quality tests supported by GX, DBT, or other data quality tools. Bringing test results from third-party tools in OpenMetadata will allow data consumers to have a 360 view of their data assets while accommodating existing infrastructure across teams.
We are also working on improving how data quality tests are displayed in the platform by providing a dedicated dashboard at the column level. With this feature, users will be able to quickly understand what type of data they are working on and also the quality of the data. With the historical view, it’ll be possible to understand the evolution of the data quality for a specific asset.
These next steps are focused on providing an improved user experience to increase trust and understanding of the data; ultimately reducing the time to gain crucial insights.
Data Quality
Metadata
Big Data
Data Discovery
--
--
Follow
Published in
OpenMetadata
581 Followers
·
Last published
Oct 15, 2024
OpenMetadata is an open-source project that is driving Open Metadata standards for data. It unifies all the metadata in a single place in a Centralized Metadata store and helps people Discover, Collaborate, and Get their data right.
Follow
Follow
Written by
Teddy Crépineau
160 Followers
·
203 Following
Three o’clock is always too late or too early for anything you want to do — Jean-Paul Sartre.
http://www.teddycrepineau.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Another Approach to Testing Big Data | by Tom Harrison | Tom Harrison’s Blog,"Another Approach to Testing Big Data | by Tom Harrison | Tom Harrison’s Blog
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Another Approach to Testing Big Data
Tom Harrison
·
Follow
Published in
Tom Harrison’s Blog
·
9 min read
·
Feb 13, 2016
--
Listen
Share
There are some good strategies for how to test in a big data context, in this case Hadoop. Most rightly point out that you need to test:
data that has been staged on Hadoop from its source
the logic and process of transformation
the correctness and completeness of the target data sets
There are challenges in big data that are not present in well-structured, relational DB contexts. Perhaps most notable is when data comes from external sources you don’t control or whose content is unknown — at Direct Hit, Ask Jeeves and Digital Advisor the sources of data we got were completely non-deterministic. At Pubget, we got data in XML which was “kinda structured” … except that publishers who sent us abstracts of their journals often don’t follow their own data schemas. So this is one kind of problem: unknown data progeny.
At Paytronix, some of our data has this issue, but much of it comes directly from our OLTP systems so is of known progeny and at least we’re in the same company, so know when our schemas change. Other data, such as exports from third party systems fall into the unknown data progeny class.
Garbage In
Much as we would like to believe otherwise, there’s really no such thing as pure data in any sufficiently complex system. There’s a high cost to validate and normalize all data … and there may be a higher payoff. But the cost is that data that does not fit strict structure and content presence and type rules is either
excluded from the dataset, or
must be treated as a known class of exception by all downstream consumers
The one option that we just cannot accept is adding data that is known to be dirty.
Examples of dirty data
A customer of ours sends data having date/time values. For reasons I couldn’t fully articulate, the data is sent in a 12 hour format that does not include AM/PM indicators — this is a long-standing bug in the system that produces this date data. It will be fixed once thousands of point-of-sale terminals in thousands of stores all have the new version of the POS software updated. So this is a case we simply have to accept. We use heuristics based on when we receive the data, and other rules to guess, and “correct” the data. On the edges, or in certain known cases (e.g. a delay in sending the data) we simply cannot know.
Another case involves a customer who uses our software differently than others. Their use-case is perfectly valid, but just not one that we had yet supported in the design of the software. We’re a customer-first organization, and so we have cases where a data set having a given id can change its meaning over time. In January of last year, a particular id like 12345 was associated with a discount, for example “Free coke on your 10th visit”. This year, the same item 12345 might be associated with “10% off purchases over $25”. We know at the time how to account for transactions, so our OLTP and accounting part of the system is fine. But as we flatten it out over a number of years, the relationship is lost. To deal with this, we need to augment the data we import, effectively adding the enhancement we need in our OLTP system in the downstream Hadoop data.
These are a few cases that need to be wired into our ETL. We need to understand the exception case, manage it as well as possible, and ensure that reporting based upon it is properly qualified to know: garbage in results in garbage out.
Garbage data naturally confounds any automated testing. Do we have the same number of transactions sent on a given date as we recorded in our OLTP system? Nope — in a given day or month, we’re going to find a few edge cases. When we compare how the different discounts performed over years, we’re using different keys than we use in our OLTP system.
This is reality. Purists would say “Don’t do that”. Most purists are teaching, having failed in creating successful businesses!
Boiling the Ocean
Most of the testing we do today builds queries against source data, performing effectively the same transformation as the “real” software does to see if it comes out the same. Other tests are aggregate checks — we imported 987,654,321 transactions for 2015 — is that the same number as there are in the OLTP database?
These are brute force tests. They take forever to write … often longer than it takes to write the original. And they take forever to run. But what are we actually testing?
Reconciling the Checkbook
Such tests are similar to the processes used in accounting: in effect, use a different method to add up data and see if you come out with the same results. That works for accounting because there’s just one way to do it. For big data, especially data that may not be pure, this is a fool’s errand. The process is something like, write two gnarly queries, one against source, one against output target and run them against some sample of data (since running them against everything would take days in some cases).
So one problem is, we might not have picked the customer whose dates didn’t have AM/PM indicators, or the customer who was reusing the id’s for their promotions. Works great for most customers, but not for those guys.
Moving Target
Typically these tests are done in the context of a given release. We freshen our QA database, and freeze it for testing, run the processes and run the tests. When we’re all passed, we release.
Then time passes and we repeat. But the tests that we may have run before may not work any more — perhaps bugs have been fixed (or introduced) in the source system. Maybe the schema was extended. Maybe there’s a new case we should have caught in our first test that appears now that the system is live.
At Pubget, they used the same strategy as I outlined above, on data that just changed all the time, and the test suite just added new stuff to old. This was all run with a continuous integration process, and at some point, tests would fail several times a day because some external dependency (e.g. a third party service) would be down or fail. In most cases, we just resorted to commenting out the tests that failed because they were in old code that was not even used. Oy. (Eventually we rewrote the test suite to use real fixtures, even it the data was an HTML page or XML document.)
It’s Hard to Know
So we end up having tests that get us high confidence that the data we have was, at some point, correct. We assume a lot here but it’s just the nature of the beast.
Or is it?
Boiling the Teapot
I want to try something crazy: let’s use the same techniques we used in non-big-data situations to execute tests. The xUnit test framework defines a notion of data Fixtures — manually composed samples of data that are expressly designed to be minimal and test specific aspects of the system.
For each test, a setup process loaded one ore more fixtures into an empty database, started a transaction, ran the tests, and then rolled back the transaction. Tests were even run in different order each time so that any tests that depended on the previous state of data from a prior test would eventually fail. Each test was atomic.
The whole test suite (unit, functional, system) ran in minutes and was completely internally consistent.
What makes big data so different?
There are many aspects of big data that are different than simple application data scenarios. Just the scale is important, of course and both load-time and run-time performance are critical factors. And it’s not just performance — sometimes it’s a question whether loads or queries will run at all — do they run out of resources?
But that’s a different thing to test. Let’s not conflate them.
We’re far less interested in testing the specific methods that manage data in a big data context — the tricky transactional stuff or workflow testing, unit and functional tests have already been done upstream (right?).
In this case, we start with a known or presumed state of input data. The focus is on whether the full process properly executes the ETL process, leaving us with a known state of output data. We can easily assert that each expected state is true if our tests are build in parallel with building the fixtures of input data that will be transformed.
Data is data
Instead, we can use the same strategy of Fixtures. Instead of picking a sample of real data, we
compose
a sample that has the minimal set of conditions we need to test. Create 10 records, not 10,000. Pick 3 (fake) customers, not 200.
Built up the conditions that exercise all of the things that happen — clean and dirty, so yes, create a date that represents an edge case that could happen in our dirty-data examples above, and make sure the system still does the right thing. We can do this in a single record, and chances are, we can use that record, and the others it is related to in the OLTP system to compose a tiny set of simple “happy path” cases, and bizarre edge cases that our software is supposed to handle.
Anne, Betty and Carlos
A great strategy for this is to build a scenario that describes the data entities in human terms that we can remember. With just a few records, you “get to know” the scenario you have composed. Good old Anne, Betty and Carlos have gone to Acme Store and bought Apples, Bananas and Cigarettes.
Oh, we know discounts can’t be applied to cigarettes, so there’s an edge case. Acme Store closes at 4am, but they want transactions at 4am Tuesday to be counted against Monday’s. Hey, let’s make their visit happen on the same day that we switch between standard and daylight savings time, and have Anne come in at 11pm, Betty at 2am and Carlos at 4am!
As we build up our test suite, the fixtures are get committed to the source control repo along with the tests that verify the correct outcome. All we need to test are simple assertions of expected outcome in the final dataset. As the source data changes, we change our fixtures manually to match. As we find bugs or new edge cases, we find an existing fixture that can be the “carrier” of that case, or add a new one. It all goes in source control, and in the same repo as the code we’re testing.
Continuous Integration for Big Data
With tiny data sets, we can run what are effectively system tests that start in the middle of the whole pipeline … but at the edge of the big data universe … all in a matter of minutes. Data sets are checked in, and can be loaded and run on developer workstations, and be part of the CI build and code-review process. Developers create the input dataset, and QA may decide to extend it with edge cases.
Tests are simple: run the setup to load fixtures (from scratch, deleting anything already there), run the ETL processing against this known state, then check a specific known record for a known cases or cases with simple queries.
The great part of this is that the small number of records can be easily visually inspected. While this is just one aspect of validation, it’s nearly pointless with thousands or millions of records. And when it comes to isolating problems found in production, it’s simple to go to the test set, find or create a representative record, and see how it works there.
Instead of duplicating the effort of writing queries and software comparing source and target by replicating specified logic for transformation, we create a static and known source, manually, and once we have it working the first time, extending it is incremental and does not require extensive programming skills (and the time to do it) of usual big-data testing techniques.
Not Everything, But By Far the Most Important
This approach does not exercise scale, performance, resource consumption, or a bunch of other things. And typically these are the most important things to test from a systems standpoint. It’s what makes big data so different.
But this kind of testing is more straightforward. If we have all of the data validations done, we can spin up a cluster that is sufficiently large to replicate production, make a copy of production source data once, and run these tests against real data with our new code. We’re not looking for logic errors, we’re looking for system configuration, sizing, and stability issues.
It’s a hell of a lot easier to separate these two kinds of testing.
Photo credit:
Pixabay
Process
Technology
Big Data
Continuous Integration
Hadoop
--
--
Follow
Published in
Tom Harrison’s Blog
534 Followers
·
Last published
Jul 30, 2024
AWS Certified Solutions Architect Professional
Follow
Follow
Written by
Tom Harrison
2K Followers
·
162 Following
Software architecture, infrastructure, design, and good ol' writin' code.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Automating Data Quality Checks with Great Expectations | by Data@Urban | Medium,"Automating Data Quality Checks with Great Expectations | by Data@Urban | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Illustration by Anna Minkina/Shutterstock
Automating Data Quality Checks with Great Expectations
Data@Urban
·
Follow
6 min read
·
May 5, 2021
--
1
Listen
Share
Data management and quality assurance is key to delivering rigorous and reliable policy analysis. The Urban Institute’s Housing Finance Policy Center uses multiple property records data sources, including Agency Mortgage-Backed Security data and Home Mortgage Disclosure Act data, to study structural barriers homeowners of color face. Our data science team aims to support the Housing Finance Policy team, and all Urban researchers, in building a robust data quality assurance system, which makes quality checks automated, iterative, and fast.
Before statistical analysis or building machine learning models, cleaning data can take up a lot of time in the project cycle. Without automated and systematic data quality checks, we may spend most of our time cleaning data and hand-coding one-off quality checks. As most data scientists know, this process can be both tedious and error prone.
Having an automated quality check system is critical to project efficiency and data integrity. Such systems help us understand data quality expectations before beginning the project, know what to expect in our data analysis, and make communicating the data’s intricacies much easier. For example, a good metadata system in which the researchers define the variables included in the data, the variable values (e.g., “1” indicates male and “2” indicates female), and even the data types (e.g., string or integer), sets rules for how data should look, helps quickly find errors, and facilitates researcher collaboration. Further, regularly updated data, such as most published datasets Urban uses, benefit from automated refreshes and automated data monitors, reducing the need for continuous human intervention.
To help our colleagues in the Housing Finance Policy Center, we developed a comprehensive data quality system. The system consists of four parts: a data pipeline infrastructure, a metadata system, an open-source data quality tool called Great Expectations, and a front-end web page that lists all the datasets available to researchers, along with their quality check history and reports. To connect all the parts, we created an automated process that connects code updates, data file generation, quality checks, and shares notifications of the results. This allows researchers to conduct data checks without having to interact with complex data infrastructure. First, a researcher can write up code for generating data files and push the code to Github. Once the code updates are approved, it gets pulled into a local server and executes. Data files are then generated and sent to an S3 (
Amazon Web Services’ data storage service
) bucket. This triggers a quality check pipeline that automatically runs on the cloud. Once the check finishes, the reports are stored in S3 and displayed on the website, where researchers can easily find, view, and share the reports.
Great Expectations
is a key part of the quality check system. It offers automated unit tests for datasets (also known as pipeline testing) to help guard against upstream data changes and monitor data quality. At a high level, it conducts checks on a researcher’s dataset against the data quality “expectations,” the parameters researchers provide to the system. This system has the following main advantages:
-
Compatibility with tools and systems we already use.
Great Expectations is a Python-based application, which is applicable to various data forms, including in-memory Pandas or Spark data frames, data files stored in S3, or Relational Database Service such as MySQL and PostgreSQL. It also allows you to invoke checks in the command line without using Python.
-
Comprehensive unit tests.
Great Expectations offers a variety of expectations researchers can configure. The tool defines expectations as statements describing verifiable properties of a dataset. Some commonly used expectations include the following:
· verifying column names and row counts
· identifying missing cells
· checks on data types
· checks on expected values
· reporting statistical characteristics (mean, median, and minimum and maximum values) of certain columns
These expectations work as functions in Python. For example, to check if a column named “student_id” exists in a dataset, we simply read data into memory in Python, initialize a Great Expectations session, and call the following function:
expect_column_to_exist(“student_id”).
Not only does it offer more than 50 built-in expectations (
full list here
), it also allows data engineers and researchers to write custom expectation functions. For our project, we wrote a custom function to check all positive unique values for certain columns in a dataset.
-
Easy to use without complex coding:
Great Expectations applies tests directly on data. Some coding is needed at the beginning to fit Great Expectations into a data pipeline (such as writing custom expectations). But once the system is set up, checks can be automatically conducted whenever the new data comes in.
-
Results delivery with HTML reports and Slack notifications:
Great Expectations generates an HTML report that compiles the quality check outcomes for a dataset. It can be configured to display very detailed information, such as the unexpected values for a column, that can simplify debugging. The HTML reports are neat, organized, and easy to understand.
An example of the Great Expectations HTML report.
If a researcher is looking to get a quick answer on how many checks passed or failed, Great Expectations can be plugged into a Slack channel and will post messages that summarize check results.
-
The profiling functionality automates generating summary statistics:
When new datasets come in, researchers may want a quick sense of how the data look. Instead of trying to set up everything in Pandas to describe the dataset, automatic profiling creates a summary statistics report, which provides a comprehensive overview, including the number of variables, observations, missing cells, and data types. It then provides a summary of value counts and distribution for each column. Below are two examples of how profiling presents this overview and descriptive summary.
An Example of the Great Expectations Profiling Report
Implementing Great Expectations is fairly simple. Three main elements are needed to conduct the check.
Expectation suite: This consists of a collection of expectations in JSON format, which helps keep the expectations organized and reusable. An example expectation suite looks like this:
{
""data_asset_type"": ""hmda"",
""expectation_suite_name"": ""hmda_1998_processed_suite"",
""expectations"": [
{
""expectation_type"": ""expect_column_to_exist"",
""kwargs"": {
""column"": ""my_var_1""
}
},
{
""expectation_type"": ""expect_column_values_to_be_of_type"",
""kwargs"": {
""column"": ""my_var_2"",
""type_"": ""str""
}
}
]
}
1. Data: Our data are stored in S3. Once the data get updated in S3, the quality check pipeline will be triggered.
Data context: This serves as a central place to document all resources and configurations for the quality check, including expectation suites, data sources, and notification settings. The data context is configured via the .yml file named
great_expectation.yml.
In our case, we also automated the .yml file to update through its metadata. The data source section of the .yml file looks like this:
datasources:
embs_fhl_aggrhist:
batch_kwargs_generators:
summary_statistics:
assets:
embs_fhl_aggrhist:
path: /dev/data/embs_fhl_aggrhist.csv
class_name: ManualBatchKwargsGenerator
class_name: PandasDatasource
data_asset_type:
class_name: CustomPandasDataset
module_name: custom_module.custom_dataset
module_name: great_expectations.datasource
embs_fhl_cpnhist:
batch_kwargs_generators:
summary_statistics:
assets:
embs_fhl_cpnhist:
path: /dev/data/embs_fhl_cpnhist.csv
class_name: ManualBatchKwargsGenerator
class_name: PandasDatasource
data_asset_type:
class_name: CustomPandasDataset
module_name: custom_module.custom_dataset
module_name: great_expectations.datasource
The automated quality check system ensures the data we have are what we expect and are ready for downstream policy analysis. We are currently working to make the system easily applicable to other datasets available at Urban so more data users and Urban’s policy work can benefit from this system.
- Vivian Sihan Zheng
Want to learn more? Sign up for the Data@Urban newsletter.
Quality Assurance
Policy Analysis
Data Science
--
--
1
Follow
Written by
Data@Urban
2.3K Followers
·
58 Following
Data@Urban is a place to explore the code, data, products, and processes that bring Urban Institute research to life.
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Why Data Quality Is Harder than Code Quality | by Ari Bajo | Towards Data Science,"Why Data Quality Is Harder than Code Quality | by Ari Bajo | Towards Data Science
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Why Data Quality Is Harder than Code Quality
How to detect, understand, fix, and reduce data quality issues.
Ari Bajo
·
Follow
Published in
Towards Data Science
·
10 min read
·
Nov 14, 2022
--
1
Share
Photo by
Patrick Fore
on
Unsplash
As a data engineer, I always feel less confident about the quality of data I handle than the quality of code I write. Code, at least, I can run it interactively and write tests before deploying to production. Data, I most often have to wait for it to flow through the system and be used to encounter data quality issues. And it’s not only the errors that are raised. It’s also this feeling that there are more unknown data quality issues than code bugs waiting to be discovered. But, is data quality a more complex problem to solve than code quality?
Code quality is the process of ensuring code meets expectations. Likewise,
data quality is the process of ensuring data meets expectations
. In this article, I want to abstract away from expectations (known as
data quality dimensions
when talking about data) as these will likely be different based on your usage. Instead, I discuss the different steps to handle data quality issues:
detecting, understanding, fixing, and reducing quality issues
. Then I argue why I find each one of these steps harder to implement when applied to data than when applied to code.
Why is detecting data quality issues hard?
--
--
1
Follow
Published in
Towards Data Science
774K Followers
·
Last published
3 hours ago
Your home for data science and AI. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.
Follow
Follow
Written by
Ari Bajo
367 Followers
·
2.3K Following
Freelance Data Engineer & Technical Writer. I digest 50+ tech blogs by French companies →
guriosity.com
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Your data quality strategy should be automated — here’s where to start | by Barr Moses | Medium,"Your data quality strategy should be automated — here’s where to start | by Barr Moses | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Your data quality strategy should be automated — here’s where to start
Barr Moses
·
Follow
3 min read
·
Sep 27, 2024
--
Listen
Share
Did you know that data analysts often spend more time writing tests for data than they do driving value from it?
Data quality is a team sport, involving analysts, engineers, and governance teams to hand off the baton across issue detection, triage, and resolution. For most data teams, the first step is 1) understanding what data matters most to your business and 2) hand-writing data quality checks to cover these critical assets. The problem is that the burden of writing rules will always grow with the scale of your data environment. The more data you have, the more rules you’ll need to write to validate it. It’s tedious. It’s expensive. And worst of all — it isn’t enough.
At a certain point, a manual approach to testing just doesn’t make sense. In my opinion, automating data quality checks is the only way for modern data teams to effectively manage data reliability at scale.
Now, I know what you’re thinking — not every rule can be automated — and I agree with you. Some business rules will always rely on the expertise of domain-specific analysts and SMEs to define them. But here are five baseline data quality rules that can and should be automated with AI:
Uniqueness rules — If it’s a routine rule-type that you find yourself writing often, it’s a rule you shouldn’t be writing by hand. But uniqueness isn’t all things to all fields — or even all tables. Instead of profiling each field separately and guessing at thresholds, you can use a good machine learning monitor that allows for a UNIQUE _RATE to be applied across an entire table and then programmatically defines appropriate uniqueness rates by field.
Validity rules and dimension drift — Validating low-cardinality fields is always tedious. Instead of hand-writing rules to validate multiple possible values or the limitless potential permutations of drift, ML-powered dimension tracking can be used to apply distribution analysis to the values of a given field, giving analysts the distribution of certain values and their percentage based on historic values and their relative frequency.
Timeliness rules — As arguably THE most common data quality rule, timeliness checks are the premier candidate for automation — and a massive burden when you don’t. Instead of cloning this test over and over again with slightly different standards, you can deploy an automated monitor like ‘TIME_SINCE_LAST_ROW_COUNT_CHANGE,’ to alert you if a load of new rows breaches historic thresholds — and save yourself a whole lot of time.
Accuracy rules — In general, manual accuracy rules require profiling each numeric column separately to define its accepted ranges — and if you have hundreds of tables to define, you have a whole lot of rules to write. What’s worse, this data is highly sensitive to changes. Automated distribution monitors for things like mean, negative%, and standard deviation can programmatically check for shifts in the numeric profile of your data across tables and proactively maintain those thresholds over time.
Rules for unforeseen issues — You don’t know what you don’t know, and this applies to your data environment. Analysts need some form of advanced automation that can offer coverage for all the issues you can’t anticipate. Automated machine learning monitors for ‘unknown unknowns’ should cover your entire environment at the data, system, and code levels to detect those issues that aren’t being monitored by a specific test — like a code change that causes an API to stop collecting data or a JSON schema change that removes a critical column. This type of coverage should tell you not only not only when a break happens, but where and why as well.
In 2024, a modern approach to data quality is an absolute necessity. Without strategic automation for routine manual data testing (and an operational strategy for rolling it out), you’re not just sacrificing valuable insights — you’re getting left behind.
Stay reliable,
Barr Moses
Data Quality
Data Testing
Data Observability
--
--
Follow
Written by
Barr Moses
20K Followers
·
227 Following
Co-Founder and CEO, Monte Carlo (
www.montecarlodata.com
). @BM_DataDowntime #datadowntime
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
The challenge of testing Data Pipelines | by Blake Norrish | Slalom Build | Medium,"The challenge of testing Data Pipelines | by Blake Norrish | Slalom Build | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
The challenge of testing Data Pipelines
How the Oracle Problem, quality thresholds, walled gardens, multi-dimensional quality, and the very purpose of tests makes the challenge of testing data pipelines different from testing traditional software.
Blake Norrish
·
Follow
Published in
Slalom Build
·
14 min read
·
Nov 19, 2020
--
4
Listen
Share
Software just moves data around inside of computers. Data pipelines also just move data around inside of computers. So how different can testing data pipelines really be from testing any other form of software? Shouldn’t all the practices, approaches, and accumulated expertise learned from testing traditional software also apply to data pipelines?
Unfortunately, it’s complicated. Yes, there is overlap between testing data pipelines and testing traditional software. However, there are characteristics of data pipelines and pipeline development that are unique, and create peculiar testing challenges that the seasoned quality engineer might not expect. Ignoring these challenges and approaching data pipelines as if they are no different than any other type of software will result in frustration and inefficiency. In this article, I’ll walk through several of the most interesting challenges and provide alternative approaches appropriate for data pipelines.
While this article will focus on these differences, it won’t be an exhaustive “how to test data pipelines” tutorial; testing data pipelines is a broad topic that can’t be covered adequately in one blog post. However, understanding these challenges will be valuable to everyone working on data pipelines.
First, a bit of context on data pipelines: I’m using the term broadly to describe a wide range of systems, including both ETL and ELT processes and all types of data lake, data mart, and data warehouse architectures. It doesn’t matter if data is processed in batches, micro-batch, or streamed — everything here still applies. These applications often transform the data during processing, but this isn’t mandatory. Data pipelines usually work with large amounts of data, but not everything has to be “Big” data, as even small pipelines may suffer from the challenges discussed below.
If what I describe above seems like a lot, well, that’s because it is. Applying a single term like data pipeline to all these applications necessarily oversimplifies their variety and diversity. However, I believe the below challenges are common across the broad category, even if their exact manifestation is unique to a particular architecture or organization.
Data Quality “Tests”
Let’s start by talking about tests, and how even this simple term can be confusing within the context of a data pipeline. Here’s a hypothetical situation:
You join a team expanding an existing data pipeline. This team, like many data pipeline development teams, has not traditionally used a quality engineer, so they are not quite sure what you should be doing. You ask about any automated tests they have written, and to your relief, they tell you they have a lot. Excellent!
They show you some tests written in PyTest that ensure data conforms to certain rules. For example, there are checks for null, some referential integrity checks, and some other checks on “counts and amounts” as they put it. However, you’re not sure exactly how these tests are used. Are they unit tests, or some other form of automated validation?
You ask when these tests are executed. “Oh, they are run with the pipeline,” they say. Well, that’s great, they have automated CI/CD pipelines here. You ask about their promotion process but get… blank stares.
“The tests that are executed in the pipeline, are they executed on merge to trunk, after deployment to a test environment, something like that?” you ask.
More confused looks.
This brings us to the first big difference between data pipeline testing and testing standard applications: Data Pipelines often have “tests” built into the data pipeline itself, executed as data is processed, to ensure data quality. These tests are part of the production functionality of the pipeline. They are usually defined by data stewards or data engineers, and ensure that bad data is identified, then blocked, scrubbed, fixed, or just logged as the pipeline is run. These tests are necessary because data that flows into data pipelines is often from untrusted systems and of low quality. These tests are
not
the type of test we normally think about when using the term in the context of traditional software, and the term “test” is often a source of confusion.
The closest parallel to this in traditional software is the input validation that happens against incoming message payloads in a publicly exposed API. If I build a REST service that accepts a POST request from external consumers, I need to check the content of this request before processing it. If the request is invalid the system should respond with an HTTP 400(bad request), or an application-level error message. This check of incoming data can be thought of as a runtime test — part of the production system and executed every time a POST message is received to validate (or “test”) the correctness of the input. However, we would not consider this input validation test to be a test in the same sense that quality engineers use the term.
Similar to the REST service, data pipelines receive data from external, untrusted sources. They must test the data and react appropriately when invalid data is found. This type validation is often referred to as “testing” to ensure “data quality”, which is run in the “pipeline” (the
data
pipeline!), but like the REST service, it is not performing the same function as an actual functional test. Data quality issues are so pervasive in data pipelines that the term “test” is usually assumed to mean the runtime validation test, not the type of test or automation we are familiar with.
It often helps to visualize data pipelines as two, orthogonal paths. The horizontal path (blue arrows) is the flow of data through the data pipeline. The vertical path (orange) is the promotion of code and logic across environments in the CI/CD pipeline, as new code or features are developed. Data flows horizontally across each of these environments, and code or other pipeline changes flow vertically between environments. Data quality tests, aka runtime tests, are executed in the horizontal flow, to validate data quality, and transformation validation or other normal tests are executed in the vertical flow, to validate code an logic before promotion to the next environment. They are both tests, but test different things for different reasons.
The flow of data (horizontal) and the promotion of code (vertical) in data pipelines
The confusion between runtime data quality tests and development code quality tests is exacerbated by the fact that the two types of tests can be written in the same framework, and a single test could find both a data quality issue as well as a code logic issue, depending on how it is being used. In addition, runtime data quality tests are themselves functionality, so should be tested
by logic tests.
The need for both extensive runtime data quality tests as well as standard code validation tests and the overlap of terms used for each is a huge source of confusion testing data pipelines. Mature data pipeline development teams leverage both automated data quality validation, as well as automated logic tests, ensuring only quality code is promoted to production and data has the highest possible value to the organization.
Key takeaway
: Be clear about what your tests are testing and ensure that you are sufficiently testing both data quality and logic. Find and use precise terms for each that have meaning for your team.
GUI Tools and Walled Gardens
Enterprise software comes in a lot of flavors. On one hand we have modern architectures built on open tools, platforms, and languages. On the other, we have all forms of legacy and proprietary systems. Proprietary doesn’t necessarily mean legacy (think SAP, Salesforce, etc.), but still makes modern test automation and continuous deployment practices challenging. These systems often provide instance management, configuration, deployment and testing functionality from within their walled gardens, requiring the use of their tools and adherence to their processes. Want a new environment? Pay another $50k/month for an instance and deploy it using this proprietary GUI tool! Want to set up ephemeral environments deployed with your existing GitHub Actions or Jenkins pipelines while executing PyTest suites on deployment? That could be a problem.
Data pipelines suffer from an egregious form of the walled garden, GUI-centric problem. ETL tools are often targeted at database developers — people with deep expertise in SQL and data, but not in application development. Asking these people to write python, C#, or similar language would be problematic, so tool vendors built powerful drag-and-drop GUIs to insulate SQL developers from this type of development. These tools work great for writing data transformations, but are incredibly cumbersome to integrate into external, modern development pipelines. Creating arbitrary environment promotion paths, with fully customized, automated quality gates between environments, is often impossible.
For example, let’s say I have a data pipeline with transformations implemented in Informatica. Informatica provides a GUI tool for implementing the transformations, so this is where developers work. The data pipeline is getting more complex, with more developers working simultaneously, so I want to set up a feature branch-based model that allows developers to develop and test on isolated environments, before merging to trunk. The merge should automatically deploy to a shared test environment, and kick off additional, higher level tests. If any tests fail, a build failure message should be sent to the appropriate channels.
This would be an easy thing for a DevOps and Quality Engineer to set up if we were building something like a React web app and deploying to AWS. Unfortunately, there is no native way to “branch” development within an Informatica instance. All developers working in the same instance are technically using the same account, and can step on each others’ toes. It is very challenging to replicate instances in the same way it is to replicate AWS environments to get the same functionality as our hypothetical react-on-aws application.
Because of the prevalence of proprietary walled garden and GUI-centric development tools, it is often challenging or impossible to implement the types of development and test automation processes we rely in traditional application development.
Key takeaway:
Understand the limitations of your tools. If your organization is moving away from legacy ETL development to modern data engineering, adopt code-first tools (like
dbt
) over GUI-first, walled garden type tools, regardless of how nice the UI looks.
The (many) Dimensions of Quality
Data pipelines are all about… well… data. Data is pulled (or pushed, or streamed) from different source systems, data is transformed and processed, and data is saved, exported, or otherwise “egressed” to where it needs to go. Data “quality” measures how usable this data is for the intended application. There are many dimensions in which data can lack quality, and this multi-dimensional aspect is something less common in normal software.
Many data engineers have attempted to define the “dimensions of data quality” which you can find with a quick Google search. However, no two lists look exactly alike. The point is that data in a data pipeline isn’t just correct or incorrect, but rather has higher or lower quality — and low quality data can be low quality for a variety of reasons. Often low quality data is, on the surface, indistinguishable from high quality data (sneak peek: the Oracle Problem!).
Here is a list I’ve used that makes sense to me. Again, exactly how you enumerate and define these dimensions is a personal preference, and if expressed too loudly in the wrong part of the office, can get you into unnecessary arguments.
Completeness:
When whole records within the data set or values within a record that should be present are missing (think null columns in a DB row).
Uniqueness
: When data is duplicated. Duplicate or redundant data is common in pipelines that pull from a variety of upstream sources, and deduplication is often a big component of data pipeline processing.
Validity:
For data to be valid, it needs to be presented in a format that is consistent within the domain of values. For example, in a date attribute that assumes MM-DD-YY format, any date in a YYYY-MM-DD format would be invalid, even if the date is actually accurate.
Consistency:
When data that is expected to be equivalent is not. For example, the customer record from billing says Bob’s address is 123 Main St, but the account record from CMS says its 489 James St. This is a contrived example, and there are more subtle reasons why data could be considered inconsistent based on context and domain.
Referential Integrity
: When data refers to other data through foreign keys or other less explicit relationships, but that data is missing. This ties into the concept of Master Data Management (MDM).
Accuracy:
Data values that are just incorrect. It might have been pulled from the wrong source, been corrupted, or simply entered incorrectly in the source system.
Compliance:
Data might be all the above things, but be out of compliance. Doesn’t seem that important? Now you have a clear text credit card number flowing through your data pipeline, getting written into log files, etc. Yikes.
Timeliness:
When availability or latency issues prevent data from being valuable for its intended use. Depending on the use case and the data, timeliness expectations can vary widely from minutes to infinite.
Data problems in the above dimensions can be caused by both upstream issues and logic issues in the pipeline itself. Tests that verify data in these dimensions might be executed as part of the horizontal pipeline, or part of the vertical promotion, or both. Regardless of the reason, we must ensure that the data provided to our users is as valuable as possible, across all the dimensions of data quality.
Key Takeaway:
Understand all the ways your data is expected to have quality, and develop ways this quality will be tested.
The Oracle Problem
(no, not the company)
Every quality engineer is familiar with the
Oracle Problem
, even if they don’t know it by that name. You execute a test and get some result, this is your “actual” result. What did you expect to get? In some cases, this is easy to answer, either by accessing data or using common sense. You expected to get an HTTP 200(OK) response. You expected to not get an error page. You expected your cart to be emptied, and an e-mail to be sent with an order confirmation, per the acceptance criteria of the story. However, in many cases determining the “expected” result can be a challenge — this is the Oracle Problem. You log into the test environment for AcmeTravel.com. You search for a flight from Seattle to NYC, and get 302 results. Were you supposed to get 302 results? Were these the exact 302 results you were supposed to get? Without deep hooks into the underlying data sources, or full control of the integration points of this search system, this is probably impossible to answer. Again, this is the Oracle Problem: every test is a comparison of two results, an expected and an actual, and determining the expected result is often a challenge.
Data pipelines have a bad case of the Oracle Problem. A data pipeline pulls data from many source systems. It combines this data, transforms it, scrubs it, then stores it in some processed state for consumption by downstream systems, analytics platforms, or data scientists. These pipelines often process data from hundreds or even thousands of sources. If you run your pipeline, get several million new rows in your consumption layer, how do you know if it succeeded? How many rows were you supposed to get? Is every value in each column of each row correct? This problem is just as bad, if not worse, than the above flight search problem!
If you are not familiar with data pipelines, you will probably suggest mocking upstream data sources. With full control of the data sources, you know exactly what’s entering the pipeline, so you should know exactly what’s coming out. How is this any different that traditional software testing?
While this can sometimes be done, the specific nature of data pipelines can make it challenging or even impossible. Often source data is made up of unstructured data (think: blog posts, customer reports, log files, etc.) not rows in a relational database, and it is often challenging to derive representative mock data from unstructured data. Even if all source data was nicely structured database tables, these tables often number in the thousands, and have complex relationships between them. Attempting to set up even a single, consistent set of test data could be incredibly time consuming. Another challenge: this source data is often quite varied (The third “V” of big data), such that you cannot predict beforehand all the permutations that you should account for in your test data. The strategy of mocking, so useful in testing normal software systems, is often impotent in data pipelines.
While mocking is not always the answer, the Oracle Problem is not insurmountable. Depending on the architecture of the pipeline and application of the data, there are solutions (technological and process) that can help. For example, before-and-after comparisons, heuristic validation, production data set replication, trending and profiling, etc. Which is applicable for your testing challenge (both data quality testing, as well as logic validation testing!) will depend on the specific nature of your pipeline.
Key takeaway
: Think early about how you will obtain expected results for tests — both data quality and transformation logic — as this might be your biggest challenge. For data quality validation, leverage data stewards or other data science experts to develop statistical techniques for identifying data issues.
“Good Enough” Data — Data Quality Thresholds
We are used to high bars of quality in production in traditional enterprise software. A defect found in production is usually considered a very bad thing, and can result in process changes, architectural reviews, or reassessment to test priorities and automation investments.
In many applications of data pipelines, data is assumed to be less than perfect, even in production. The sources of data are so suspect, unstructured, volatile, and messy that it is impossible to achieve “fully cleansed” data in your output or target layer. In addition, downstream users may not even need fully clean data, so any energy invested to achieve it will be wasted.
For example, it may be that your data pipeline is transforming unstructured customer feedback data from multiple systems. The analysts are looking for insights into which customers would benefit most from proactive customer support calls (eww). Of the millions of customers, the analysts are just looking for a generally prioritized list. This output isn’t really a right/wrong situation, and if a customer is slightly higher or lower than they should be because some part of the data was invalid, it won’t significantly impact the business.
This “good enough for the purpose” mentality can be jarring for the seasoned quality engineer coming from traditional quality engineering domains, but is something that will need to be understood to be successful in data pipelines.
Key takeaway:
Know your data quality expectations and invest in quality assurance accordingly.
Data Stewards and other new roles
You probably understand the roles and responsibilities of the people around you in traditional software projects. In the data pipeline world, there is a whole new set of roles, and some of these roles might even have the term “quality” in their title. And that can be confusing, so learning who these new roles are, and how they impact quality, will be important.
A common example of this is the role of the Data Steward or sometimes Data Quality Steward. This person is not necessarily embedded within the pipeline team, but plays a critical role in ensuring data quality across the enterprise. Data Stewards are often closely aligned to a data governance organization, which plays a significant role determining how data is owned, moved, and used across the organization.
Every company is different, so I can’t define all the roles and how you should interact with them. However, you should walk in knowing that as a quality engineer (just like in normal application development) your role is highly collaborative, and you will need to interface with a large set of other individuals to be effective. In data pipeline projects, there will be new roles and new titles that you will not be familiar with.
Key takeaway
: Know how these new (to you) roles in the organization are related to quality, and establish good relationships.
Conclusion
This was just an introduction to some of the unique challenges of testing data pipelines. It is not exhaustive, and a lot of interesting problems were left out simply to keep this article a readable length, so this definitely does not qualify you to claim expertise in testing data pipelines. However, a data pipeline IS software, and despite the new terms, concepts, and tools, any engineer can successfully transition to working with them. Keeping the above differences in mind will accelerate your journey help avoid frustration as you apply quality engineering expertise to the domain of data pipelines.
Special thanks to
Sara Beck
for her technical review of this article.
Software Development
Data Engineering
Testing
Data Science
Test Automation
--
--
4
Published in
Slalom Build
1.8K Followers
·
Last published
Nov 8, 2024
The Build Blog is a collection of perspectives and viewpoints on the craft of building digital products today, written by the technologists that build them. By builders, for builders.
Follow
Written by
Blake Norrish
2.8K Followers
·
45 Following
Quality Engineer, Software Developer, Consultant, Pessimist — Currently Sr Director of Quality Engineering at Slalom Build.
Follow
Responses (
4
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Best Practices for Implementation of Testing in Big Data | by Xenonstack | Medium,"Best Practices for Implementation of Testing in Big Data | by Xenonstack | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Best Practices for Implementation of Testing in Big Data
Xenonstack
·
Follow
5 min read
·
Mar 30, 2019
--
Listen
Share
Big Data Testing Strategy?
There are several areas in Big Data where testing is required. There is various type of testing in
Big Data
projects such as Database testing, Infrastructure, and Performance Testing, and Functional testing. Big Data defined as a large volume of data structured or unstructured. Data may exist in any format like flat files, images, videos, etc. The primary Big data characteristics are three V’s — volume, velocity, and variety where volume represents the size of the data collected from various sources like sensors, transactions, velocity described as the speed(handle and process rates) and variety represents the formats of data. Learn more about
Continuous Load Testing
in this insight.
The primary example of Big Data is E-commerce sites such as Amazon, Flipkart, Snapdeal and any other E-commerce site which have millions of visitors and products.
Social Media Sites
Healthcare
How Does Big Data Testing Strategy Work?
1. Data Ingestion Testing
In this, data collected from multiple sources such as CSV, sensors, logs, social media, etc. and further, store it into HDFS. In this testing, the primary motive is to verify that the data adequately extracted and correctly loaded into HDFS or not. Tester has to ensure that the data properly ingests according to the defined schema and also have to verify that there is no data corruption. The tester validates the correctness of data by taking some little sample source data, and after ingestion, compares both source data and ingested data with each other. And further, data loaded into HDFS into desired locations.
Tools —
Zookeeper
,
Kafka
,
Sqoop
,
Flume
.
2. Data Processing Testing
In this type of testing, the primary focus is on aggregated data. Whenever the ingested data processes, validate whether the business logic is implemented correctly or not. And further, validate it by comparing the output files with input files.
Tools —
Hadoop
,
Hive
,
Pig
,
Oozie
3. Data Storage Testing
The output stored in HDFS or any other warehouse. The tester verifies the output data correctly loaded into the warehouse by comparing the output data with the warehouse data.
Tools —
HDFS
,
HBase
4. Data Migration Testing
Majorly, the need for Data Migration is only when an application moved to a different server or if there is any technology change. So basically data migration is a process where the entire data of the user migrated from the old system to the new system. Data Migration testing is a process of migration from the old system to the new system with minimal downtime, with no data loss. For smooth migration (elimination defects), it is essential to carry out Data Migration testing.
There are different phases of migration test -
Pre-Migration Testing
— In this phase, the scope of the data sets, what data included and excluded. Many tables, count of data and records are noted down.
Migration Testing
— This is the actual migration of the application. In this phase, all the hardware and software configurations checked adequately according to the new system. Moreover, verifies the connectivity between all the components of the application.
Post_Migration Testing
— In this phase, check whether all the data migrated or not in the new application, is there any data loss or not. Any functionality changed or not.
Interested in deploying or migrating an existing data center? See how to perform
Data Center Migration
5. Performance Testing Overview
All the Big Data Applications involve the processing of significant data in a very short interval of time due to which there is a requirement of vast computing resources. And for such type of projects, architecture also plays an important role here. Any architecture issue can lead to performance bottlenecks in the process. So it is necessary to use Performance Testing to avoid bottlenecks. Following are some points on which Performance Testing majorly focused:
6. Data Processing Speed
Sub-System Performance
— In this, the performance of the individual components tested which are the part of the overall application. Sometimes it is necessary to identify the bottlenecks.
Functional Testing / Integration Testing
Functional Testing performed by testing the front end application according to the user requirements to validate the application results produced by the front end applications compared with the expected results. This process will test the complete workflow from Data Ingestion to Data Visualization.
How to Adopt Big Data Testing?
Implement Live integration
— Live integration is important as data comes from different sources. Perform End — to — End Testing.
Data Validation
— It involves validation of data into Hadoop Distributed File System. It includes the comparison of source data with the added data.
Process Validation
— After comparison, process validation involves Mapreduce validation, Business Logic validation, Data Aggregation and Segregation, checks key-value pair generation.
Output Validation
— It involves the elimination of data corruption, successful data loading, maintenance of data integrity, comparing HDFS data with target data.
Top 5 Benefits of Big Data Testing Strategy
Data Accuracy
Improved Business Decisions
Minimizes losses and increases revenues
Quality Cost
Improved market targeting and Strategizing
Why Big Data Testing Strategy Matters?
Big Data Testing plays a vital role in Big Data Systems. If Big Data systems not appropriately tested, then it will affect business, and it will also become tough to understand the error, cause of the failure and where it occurs. Due to which finding the solution for the problem also becomes difficult. If Big Data Testing performed correctly, then it will prevent the wastage of resources in the future.
The revolution in Big Data is starting to transform how companies organize, operate, manage talent, and create value.
Source-
Big Data Testing
Big Data Testing Best Practices
Testing based on requirements
Prioritize the fixing of bugs
Stay connected with the context
To save time, automate it
Test objective should be clear
Communication
Technical skills
Key Big Data Testing Tools
There are various Big Data tools/components -
Concluding Big Data Testing Strategy
Big Data is the trend that is revolutionizing society and its organizations due to the capabilities it provides to take advantage of a wide variety of data, in large volumes and with speed. However, many organizations are taking their first steps to incorporate Big Data into their processes. Therefore, we compiled some best recommendations of Big Data Testing Tools start in the world of data.
Originally published at
https://www.xenonstack.com
on March 30, 2019.
Big Data
--
--
Follow
Written by
Xenonstack
222 Followers
·
10 Following
A Product Engineering and Technology Services company provides Digital enterprise services and solutions with DevOps , Big Data Analytics , Data Science and AI
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
What is Big Data Testing? | by Knoldus Inc. | Medium,"What is Big Data Testing? | by Knoldus Inc. | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
What is Big Data Testing?
Knoldus Inc.
·
Follow
3 min read
·
Feb 25, 2022
--
Listen
Share
This blog is all about Big data testing and what all scenarios we should keep in mind while performing. Big data testing is a process of performing quality analysis. Hence the big data testing can be functional, performance, database, infrastructure, security etc. Lets start.
What is Big Data?
Big data is a new word in the town. It is the data which is huge and is increasing exponentially with time. As data increases it becomes difficult to process, handle and manage the data.New Computing technologies have been created to handle, manage huge amount of data. Hence processing it quicker than the traditional system and technologies. Hence it is important to understand the tools and technologies used to handle big data.
Strategy behind testing Big Data
Testing such a huge amount of data will definitely need a astonishing strategy. The quality assurance team should focus on
Batch Data Processing Test
— Batch data processing is the processing of accessing large amount of data. Hence it involves running the application with huge data. As a result validating the volume of data. Some of the tools used are HDFS, HCL workload automation etc.
Real-Time Data Processing Test
— It deals with the data when the application is in Real-Time Data Processing mode. The tools used are Spark etc
I
nteractive Data Processing Test
— This involves peak detection, integration and quantitation. Hence the testing process checks modifying, retrieving and displaying of information which is in the form of result. The tools are
HiveSQL
etc.
Big data Testing also goes through these 3 stages:
Data Ingestion —
This means that the data loads from Big data system using some extraction tools. As a result the storage might be HDFS, MongoDB. Now the data is validating, as if the data which is storing is as per requirement.
Data Processing
— The data processes as per the requirement.
Validation of the Output
— The output generates and then sent to the data warehouse.
Tools used in Big data
Data Ingestion — Kafka, Zookeeper, Sqoop, Flume, Storm, Amazon Kinesis.
Data Processing — Hadoop (Map-Reduce), Cascading, Oozie, Hive, Pig.
Storage — HDFS (Hadoop Distributed File System), Amazon S3, HBase.
Benefits of Big data testing
Cost effective storage
Accurate data
Right data at right time
Increase revenue
Effective decision making and business strategy
Challenges while testing Big data
Testing big data is quite complicating. It requires high skills and technical knowledge.
Volume of data monitoring . It is a real challenge
There are multiple tools to test full end to end flow
Special environments requires while testing huge data.
References
Big Data Testing: A Perfect Guide You Need to Follow | Edureka
The never-ending surge for the Creation, Storage, Retrieval and Analysis of the colossal volumes of data, triggered the…
www.edureka.co
Automation Testing
Big Data Analytics
QA
Testing
--
--
Follow
Written by
Knoldus Inc.
2.3K Followers
·
15 Following
Group of smart Engineers with a Product mindset who partner with your business to drive competitive advantage |
www.knoldus.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"How to ensure data quality with Great Expectations | by Tomáš Sobotík | Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science | Medium","How to ensure data quality with Great Expectations | by Tomáš Sobotík | Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to ensure data quality with Great Expectations
Tomáš Sobotík
·
Follow
Published in
Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science
·
12 min read
·
Nov 2, 2021
--
2
Listen
Share
Data Quality — one of the crucial parts of the DWH development lifecycle but still I believe part which usually lies on the bottom of the backlog with the lowest priority. Simply because it does not bring immediate business value — no new dashboard or KPI is coming on stage because of DQ, no new customer intake, or canceled churn because of the data quality solution in place. But in long term I think DQ brings many benefits, starting with potential data issues being discovered before they are spotted and reported by business users which is already late because it can have an impact on business or decisions which are being made. Another unquestionable benefit of having DQ in place is building confidence. Confidence in data, confidence in the data team who is preparing them. Last but not least having DQ as part of the project can speed up the development and make it cheaper because once you have an automatic system which is ensuring that your business rules are applied correctly and data have the shape which they are supposed to have then you do not need to dedicate a time to do such task manually as part of development.
During my career and many projects which I have been part of we sooner or later have made it to the point when we have started thinking about some automatic DQ solution. But always we have started with some bits and pieces, custom-made code which was trying to ensure some minimal data quality. Many times also the target architecture has contained some info about the DQ solutions that should be implemented
some times
.
Today, I would like to have a look at Data Quality, not from an isolated view when developers try to ensure the DQ at least on the level of database constraints. But I would like to have a look at the matter from a complex view, introduce a great tool that has the word
GREAT
in the name and try to introduce complex architecture how it might be integrated into a solution including
Snowflake
and used for automatic data quality verification.
Today I am going to introduce a data quality tool called
Great Expectations
and solution which I have developed around it. We will cover parts like how to prepare infrastructure and development environment for the whole team and then we will have a look at how to integrate the tool with Snowflake and automate the data verification process through
Github Actions
. Let’s start! 🚀
Great Expectations introduction
The great
expectation
is an open-source tool built in Python. It has several major features including data validation, profiling, and documenting the whole DQ project. Automation together with proper logging and alerts on top of data quality solutions are essential parts of having a successful and reliable solution. Great Expectations (GE) offers many integrations (Airflow, Slack, Github Actions, etc.) which supports those tasks and makes them possible.
What are Great Expectations about
Expectations
Are one of the tool's key features used for data assertions. There is a lot of
prebuilt expectations
where not all of them have been implemented for all sources. If you need an expectation that does not exist you are free to develop it on your own! Meaning that the whole concept behind expectations is very flexible and easy to extend. How does such an expectation look? Please look below. Basically, they cover common data issues.
expect_column_values_to_not_be_null
expect_column_values_to_be_unique
expect_table_row_count_to_be_between
expect_column_to_exist
expect_column_values_to_be_in_set
Data docs
Automatically generated documentation about your expectations same as about result of their verification is called Data docs. It is generated into a human-readable format in form of web pages available to anyone who needs it. Documentation is always up to date because it is generated based on your tests and tests run against new data. Documenting is always a pain for each team so providing automatically generated documentation makes life easier for developers and provides value to data consumers and maintainers.
Expectations translation into a human-friendly format available in data docs
Automated data profiling
Great Expectations can also profile your data automatically and based on results generate the expectations based on profiling. This is a nice feature of how to make understanding the new data easier and faster. Of course, you still need the domain knowledge and verify the proposed expectations as not all of them will be relevant. But as I said it can help you especially with new datasets and maybe it will provide expectations which you would not think of at the early stage of development.
Profiling example
Various data sources
I want to highlight how many different data sources GE supports. Starting with various SQL databases like
MySQL
,
PostgreSQL
through cloud-based DBs including
Athena
,
BigQuery
,
Redshift,
and
Snowflake
. Next, you can connect to data on
S3
,
GCS
,
Azure Blob Storage
using
Spark
or
Pandas
. Last but not least you can of course validate data on a local filesystem.
Where to get more info and how to learn GE?
Let’s not go too much into details of all features and tool capabilities. If you want to learn more and possibly try it yourself I can strongly recommend their official
documentation
where you can find many tutorials starting with a simple „
hello world
“ application going through different use cases on how to use this and that feature of GE.
Community
I have to mention also the
community
around GE. There is a growing friendly community around and you can meet others on Slack or GitHub. Slack community is really helpful in cases you will be stuck somewhere. There are support and beginners channels and I have found help there many times.
❄️ Snowflake and Great Expectations
Now let’s focus on our own implementation of Great Expectations, how we have integrated it into our architecture, daily routine and what kind of challenges in relation to infrastructure and overall architecture we have been facing. We have DWH in Snowflake — 30 TB in size, a couple of thousand ETL jobs, various sources including relational DBs, flat files, APIs, etc. We have been aiming for Data Quality Solution which will be:
working as a standalone solution
independent from the current ETL tool (Dataiku)
code-based — Python preferably
working natively with Snowflake
supporting various orchestration tools like Apache Airflow or GitHub Actions
Great Expectations and Snowflake?
Great Expectations fulfill all the requirements. The first challenge we had to solve is how to prepare the development environment and whole GE set up for the data engineering team. GE is not a client-server solution where all the developers would be connecting to some sort of server and work there simultaneously on different tasks. The usual pattern is having GE installed locally and working there or using some prepared docker image. Because of internal rules we are not allowed to fetch data into local computers. In the end, we have decided to build the whole infrastructure around the GitHub repository because as an orchestration tool we use GitHub Actions. Our aim was to prepare as easy an infrastructure as possible for the whole team with no need to tweak, modify or build anything by developers on regular basis but simply having something that they can use right away and they don’t need to take care about any infra details at all. For development purposes we use AWS EC2 machine with prepared python’s virtual environments per developer or as a second option developers can use prepared docker image with preinstalled tools and dependencies. You can see the architecture on high level schema below. Let’s look at it in detail.
Way of working & Architecture
We had to figure out how to have development environment on one side and production (automated) environment on the other side. As I said for development we use an EC2 machine where each developer has its own Python virtual environment or use a prepared docker image which is available in corporate AWS ECR. As you can see in the picture the central point of the whole architecture is GitHub repository. When the developer starts working on the new GE-related task he needs to fetch the latest changes from the repository first to get his own virtual environment updated with the latest changes from others and then create a new branch where the new functionality will be developed.
High-level Data quality solution architecture
Once he is ready with his development locally (inside Python virtual environment or docker image) and everything is working as expected developer can test the automation part by pushing his own branch back into the repository. There is a prepared „DEV“ version of Github Actions workflow which is automatically triggered every time there is a push to a non-master branch. This will ensure that newly developed functionality is working also on GitHub Runners when it is triggered via GitHub Actions and it will also ensure that the new part does not break anything already developed.
When new functionality is verified also inside the GitHub environment (successful run of GitHub Actions workflow) developer can create a pull request to merge his changes into the master (production) branch. Pull request needs to be reviewed and approved by admins and then it is merged. Production workflow runs on schedule every morning after daily ETL loads are done to verify all the data in the production Snowflake database. What is not visible in the schema is notification system. We have integrated GE with our Slack and every checkpoint failure is sent into our channel dedicated to DQ issues.
Slack notification about expectation suite failure
Last but not least you can see also the integration of
AWS Secret Manager
where we keep needed Snowflake credentials which are rotated regularly. Of course it is because of security reasons as we do not want to have logins and passwords laying in different configuration files in EC-2 machine. And with integration of AWS Secret Manager we read our secrets directly from Secret Manager with no need to keep them anywhere locally. Credentials rotation is done by Lambda function which at the same time updates the credentials in the Manager. By integration of Secret Manager all processes have always access to up-to-date passwords, roles, warehouses, etc. without any difficult distribution of such valuable assets.
Data docs storage and presentation
As you can see we store our data docs on AWS S3. Then we have web servers running as docker images which represent the presentation layer of automatically generated documentation and fetch the content from defined paths in S3.
Example of data docs
What next?
We have been already running the whole solution in production mode for a couple of months and so we have some experience and know the weak spots, things to improve or do differently. I must say we are still learning how to do it properly, and how to do it in best way in relation to our data and processes. We already have tens or even maybe hundreds of expectations stacked into several checkpoints. Most of them verifies source data in terms of basic verifications like not null values and not empty datasets, having a value from the defined set, or checking the data types — e. g. date for all date values. For now, we have just a few expectations that verify business logic but it is slowly growing. Already now by checking source data regularly we can spot potential data issues in source systems very soon before they are reported by business users. Those basic expectations also make a small „pressure“ on data developers to be precise in defining correct data types otherwise it is immediately reported. Let me share a few points which I have in my mind how to improve the current solution and move it to the next level. 🚀
Tighter integration with ETL/ELT processes
Right know we run all the checkpoints and all the expectation suites on schedule when our daily loads are done. Processing time is slowly raising as we add more and more expectations. But we do not need to run all of them in same time. We should aim for more parallelism and run those expectations and checkpoints immediately when portion of the data which is going to be verified is refreshed. Thanks to that we will split the whole GE processing into several smaller chunks which will be tightly integrated with that concrete ETL/ELT process that prepares those data. We just need to look at how to trigger the GitHub actions workflow directly from our ETL/ELT tool right after the process is done. GitHub Actions offers also external triggers through their GitHub API so I believe this will be way to go.
Data quality issue classification
This relates to tighter integration with the ETL tool. I can imagine also reverse integration and in case of a critical issue that might corrupt the data in that way, it would have an impact on business the ETL processing could be stopped to preserve current (correct) data. Now we do not use any issue classification and reports everything as a data issue but we also do not stop data processing. I see at least two categories:
Critical issue = stop the processing
Data issue = just alert team on slack
More business logic oriented expectations
We have collected feedback from business users about the usual data issue and what they are looking at to ensure the data they use are correct. We should aim for transforming this into expectation suites to ensure analytical tables are correct and follow defined and developed business rules.
Skeleton expectation suite
There are expectations which repeats for majority of the tables. I would like to have in place some kind of skeleton that would contain those basic expectations like key columns are not null, they are unique and exists in the schema, data types verification, or dimensions values from a limited set.
Export area schema detection
As each ordinary DWH we are providing many data exports for our customers and partners. Those integrations are mainly system to system and our partners have built their ETL processes based on schema which we provide. Because of that, I see ensuring that schema is still the same (number of columns, their order, and data types) as a crucial part to keep transfers running smoothly. This is another area where we can deploy Great Expectations. 💪🏻
Wrap up 🎯
Let me share some final words and wrap up all these. We have been looking for a data quality solution that will natively work with Snowflake and fulfill all our other requirements. So far it looks Great Expectations is great candidate. We are still learning and pushing our solution towards being more automated, more reliable and easier to use. Speaking about Snowflake integration it works smoothly, GE is using Python’s SQLAlchemy for Snowflake.
From my perspective, there are several selling points why you should at least consider trying Great Expectations if you are looking for a data quality tool, no matter what data sources you have. Let me highlight the most relevant ones:
community around
great documentation, learning materials
integration with modern cloud services (Secret Managers, cloud storage, etc.) and messaging services like Slack
automatically generated documentation in human-readable format (Data Docs)
The python-based — easy learning curve
If there are pros, there should be also cons. Let me also share few points which I think could be improved or it is needed to have them in mind when you will be considering using Great Expectations.
There is rapid development behind the tool and new version is out very often. Even though the team is trying to keep continuity some times there are bigger changes in the architecture which will require also changes at your end, in your code. One of those is V2 API vs V3 API. We still have this migration ahead of us but it is good there is some transition period and now both versions are supported. Also sometimes it can happen that any particular part of your solution can be broken by a new library release or anything similar. This has happened to me with new version of snowflake-sqlaclchemy some time ago. But such things are usually solved very quickly by the community. As the new features are adding very quickly it also means that not all parts are updated at the same time and for instance, some new features are firstly available only via code before they will be accessible via Great Expectations CLI or wizards. What I am trying to say is that it should be obvious that the tool is not mature yet and there is very active development behind which requires you to test your code before moving to the newer version.
All in all I think
Great Expectations
is great tool for Data Quality. It is very versatile which make it usable for different use case or environments. Great community around and active development confirms that it is a tool that is worth trying! 👏🏻
Snowflake
Great Expectations
Data Quality
Data Quality Tool
Software Architecture
--
--
2
Published in
Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science
7K Followers
·
Last published
1 hour ago
Best practices, tips & tricks from Snowflake experts and community
Follow
Written by
Tomáš Sobotík
656 Followers
·
17 Following
Lead data engineer & architect, Snowflake Data Superhero and SME, O'Reilly instructor. Obsessed by cloud & data solutions. ☁️ ❄️
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Observable Data Quality with Elementary and DataHub | by Tom Swann | Inside Business Insider | Medium,"Observable Data Quality with Elementary and DataHub | by Tom Swann | Inside Business Insider | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Observable Data Quality with Elementary and DataHub
Tom Swann
·
Follow
Published in
Inside Business Insider
·
6 min read
·
Jul 27, 2023
--
Listen
Share
We on the Business Insider Data team are big fans of
dbt
and our engineers, analysts, and scientists use it extensively to handle data transformation within our pipelines.
Regardless of the specific technologies used, all ELT (Extract Load Transform) data pipelines tend to follow a similar, well-worn pattern. Data is firstly extracted from various data sources — such as an external database, API, or file-based storage system.
This data is then loaded into internal storage, in the raw, where it is modified through a sequence of transformations — cleansing, normalizing, joining, and modeling it for later use in reports, dashboards, data science, or enrichment in user-facing applications.
There are potentially a number of different architectural concerns at play here — data discovery, acquisition, ingestion, transformation (The latter is the role of dbt — it is very much the ‘T’ in ELT.
Fig. 1 — Example ELT pipeline with DBT transformations
Part of the reason for dbt’s popularity is that it leans on SQL templates as the means by which users define their transformations — an approach that a wide range of people are comfortable with.
The ‘template’ part comes from
Jinja
, which allows for a plentiful dash of creativity when it comes to developing SQL transformations that can be easily generalized, parameterised, and thus made re-usable.
Testing Data Pipelines at “Batch Time”
All software architecture ultimately comes down to a trade-off between the simplest way to solve the problem here and now, and anticipating what is likely to change — and designing accordingly.
It is the art of
just-enough-thinking-ahead
such that we are able to adapt to change cost effectively whilst avoiding falling into the trap of over-engineering. This is not easy, and it requires tooling and process to support it.
Engineering teams have adopted many practices to help insulate themselves from the impact of change, such as continuous integration and automated testing.
In recognising that requirements and context will shift underneath us, having automated unit, integration, and performance tests give us the confidence to make changes. To do otherwise is to operate without a safety net in place!
In data intensive systems, it is not only the source code, schemas, and infrastructure which change, but also the data itself. This presents an additional axis of change which data engineering teams need to handle and ideally observe at “batch time” — the time when the pipeline is actually scheduled to run in production.
For example, a compile-time CI pipeline task could determine if any schemas have been modified or it could run unit tests to check that a function returns expected outputs. However, if the inputs in a live system fail or change in unpredictable ways, that creates a big hole in the resilience of the process which CI checks cannot account for.
Enter DBT Test
In addition to intuitive SQL-based transformations, dbt has another nice feature which addresses this issue of batch time data observability —
dbt test
.
A dbt test is a data quality check which executes against a data pipeline at run time.
You can either use the out-of-the-box tests which dbt provides, or avail of third-party test libraries provided by plugins like
dbt-expectations
and
dbt-utils
. It is also possible to write completely bespoke tests using SQL in a manner similar to writing transformations, using all the power of Jinja templating.
Let’s look at our data pipeline when modified to include a dbt test step:
Fig. 2 — Adding run-time data quality tests
The dbt test step will execute all of the tests we have defined for our model after it runs and the Snowflake tables have been populated with new data.
Below is an example of a simple test which checks that new rows exist for a given run of the pipeline (
run_date
is a variable passed in to the dbt task instance from Airflow):
tests:
- rows_exist_for_date:
date_field: 'EVENT_DATE'
date_value: var('run_date')
Elementary Data
dbt test is great, but it lacks some of the features required of a more comprehensive monitoring and observability solution — features like tracking results, anomaly detection, alerts and notifications.
This is where
Elementary
comes in!
There are a few things I particularly love about Elementary:
It has high quality documentation which is well organized, easy to understand, and focuses on clearly explaining the core use cases of the tool and helping new users to get up and running quickly
It extends
dbt run
and
dbt test
in a transparent way. Elementary is itself a dbt model and it integrates with the execution of dbt run and dbt test using hooks. This means that the deployment is ‘non-invasive’ — you don’t have to modify existing pipelines in order to get elementary metrics captured for ‘free’ if they already execute those commands
The only thing you will need to do is decide where and when you want to call the
edr monitor
command which determines how frequently and to which destination you would like to send the alerts which it generates (email or slack channel — see below)
Now our pipeline includes monitoring which we achieved through dbt hooks (so no modifications to the Airflow job or dbt models were required!) and alert generation via the addition of the new elementary component:
Fig. 3 — Adding transparent monitoring with Elementary
Note that Elementary also stores all of its metrics in the data warehouse (Snowflake in our case) within its own ELEMENTARY schemas.
Putting it all together
Instrumenting data flows using dbt test and Elementary provides detailed diagnostic information that can help us more quickly eliminate the lines of investigation that lead down the wrong path and thus get to the root cause of problems more efficiently.
Triggering alerts when the test conditions fail mean that the engineering team can receive this diagnostic information to a Slack channel at the moment the pipeline fails. This puts us in a position of being able to react to problems closer to when they occur and to get ahead of issues before our users notice them.
A final piece of this puzzle is in how we can use the evidence of this rigorous data quality testing to give our community of data consumers better visibility of which data sets are in good condition.
DataHub
DataHub
is an open-source metadata platform integrating features such as a data catalog, data pipeline lineage graphs and — crucially for this use case — observability into the individual pipeline tools, including data quality metrics from dbt.
Whilst we proactively direct notifications to our engineers using Elementary, by pushing our dbt test results to DataHub we can also give analysts, business stakeholders, and other interested parties access to full historical information on a dataset’s quality over its entire lifetime.
This includes version information for the dbt model which is very useful for tracing periods of test failure and low quality to a specific model deployment.
Our final pipeline with a full end-to-end data quality monitoring solution in place, then looks something like this:
Fig. 4 — Integrating DataHub metadata collection
datahub ingest
is a configuration driven CLI tool which pushes the test results to the DataHub API endpoint.
Fig. 5 — Viewing data quality metrics in DataHub
The results of our dbt tests will now be visible for each dataset in the DataHub UI, allowing downstream consumers of our processes to see not only the full pipeline lineage, but also the fact that a series of quality control rules have been applied (and are hopefully passing!)
That’s all for our foray into data pipeline quality and metadata collection — hopefully you found some inspiration to consider for your own projects!
Additional Resources
Below are some resources which I have found very helpful in thinking about this problem space.
Integrating dbt + Airflow
: Our architecture relies on Airflow (scheduling) and dbt (transformations) working together in close concert. This article contains some useful documentation from dbt on the topic
Elementary Tutorial
— A really useful walkthrough on getting started with the elementary dbt extensions
DataHub Integrations Catalog
— a key consideration for metadata tools is obviously “Does it integrate with my stuff?” The answer to that question is here.
Data
Dbt
Data Quality
Snowflake
Data Engineering
--
--
Published in
Inside Business Insider
182 Followers
·
Last published
Feb 28, 2024
Get better every day.
Follow
Written by
Tom Swann
93 Followers
·
186 Following
Botherer of data, player of games. All my views are materialised.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Warehouse Testing Strategies for Better Data Quality | by Lily Chang | Policygenius | Medium,"Data Warehouse Testing Strategies for Better Data Quality | by Lily Chang | Policygenius | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Warehouse Testing Strategies for Better Data Quality
Lily Chang
·
Follow
Published in
Policygenius
·
10 min read
·
May 24, 2023
--
Listen
Share
Policygenius
is America’s leading online insurance marketplace. Our mission is to help people get insurance right by making it easy for them to understand their options, compare quotes, and buy a policy, all in one place.
Background
A
previous post
provided a high-level overview of the ELT (Extract, Load, Transform) architecture at Policygenius. As the number of transformation jobs has grown significantly over the last few years, an increasing number of data quality issues have emerged. These issues not only cost developers valuable time as they try to fire-fight but also impaired data stakeholders’ trust and ability in making effective business decisions. In this post, we will discuss the challenges associated with these data quality issues and strategies for tackling them.
Generally speaking, Data Engineers are responsible for extracting and loading data from various sources into the data warehouse (BigQuery), while the ownership of transformation jobs is shared among the Data Engineering and Data Science teams. The development workflow for these transformation jobs is relatively straightforward: add/modify SQL query, push to GitHub, test in the development environment, request a code review, merge the pull request, and deploy to production (See the workflow diagram below). However, this workflow is not particularly quality-oriented for the following reasons:
Data product developers are required to manually validate queries by gut-checking production results, as the development environment has limited data.
The QA process across the team is not consistent. Some data product developers are more meticulous than others, with some copying and pasting the validation queries in the pull request and others using an isolated view, which can make it difficult to track down and reproduce the results.
The data that was validated at the time of development can drift over time due to the constantly evolving production data.
Time-to-detect of data issues is often very long in production, estimated at around one week, and these issues are usually reported by a data stakeholder who notices the report is off.
Sometimes when the data product developers believe they have found the root cause of a data issue, they push a patch to production and hold their breath, hoping that it will solve the problem. However, regression often occurs, with patching one issue causing another. This creates a vicious cycle that I am sure every data professional can relate to to some degree.
Figure 1: Original Development Workflow.
Turning the Tide
In Q3 2022, we decided to address our data quality issues head-on. First, we need to understand the types of data quality issues we commonly face:
Bad logic/bugs in the data warehouse views
: Pretty common, ranging from a tiny typo to incorrect business logic. This problem is exacerbated by the fact that some transformation jobs contain hundreds of lines of legacy SQL code, and it can take hours to unravel what the code does. While we would prefer to write fewer lines of cleaner SQL code, sometimes we need to get the job done quickly to support business reporting.
Contextual change from upstream sources
: Contextual data changes, also known as semantic changes, refer to how data is stored in the database. These changes are difficult to detect because they don’t immediately break things, unlike changes to the data structure. However, when unexpected data creeps into the data warehouse and breaks the initial assumptions, datasets and reports can start to drift. For example, we experienced one incident where one unexpected duplicate record snuck into a low-level table seven months after the initial validation, and it was double-counted 320 times in the final table used for key business reporting.
There are two approaches we can take to tackle these quality issues, either from a process perspective or a technology perspective. This post will focus on the latter by introducing better tools with the following design requirements:
Requirement 1
: Data product developers should not write cumbersome custom scripts. We don’t want to add too much development overhead to the data product developers, so the tool needs to be generalized enough to cover various quality use cases.
Requirement 2
: Data product developers should be able to use an interface that is similar to their current workflow. Data product developers include Data Engineers, Data Scientists, and Data Analysts with varying degrees of skill-set, so we want to find a common set of tools. If new tools are introduced, it should be as easy as possible to reduce the cost of onboarding.
Requirement 3
: The cost of switching to other solutions, should it become necessary in the future, should be low. We want to abstract away the implementation or choice of tools behind the interface from Requirement 2.
With the requirements in mind, we have introduced two testing tools below to ensure development and post-development data state adhere to certain standards. The figure below illustrates the new steps in the developers’ workflow for integrating these two tools.
Figure 2: New Data Development Workflow. Blue: Current workflow. Yellow: Additions to the existing workflow.
SQL Unit Test
Given an input row ABC, we expect to see a row DEF after 500 lines of SQL transformation.
Most of the current open source Data Quality tools/vendors that we looked at, such as Great Expectation and dbt, focus on checking the state of the end products in an aggregated form. These types of checks are certainly important for monitoring the quality of production data, which we will address in the Production Observability section below. However, there is a missing link in the SQL development phase. We want to “
shift left
” by moving testing as early as possible to the development phase (See the first two additional steps in the New Data Development Workflow above). Ideally, we would like to be able to test “Given an input row ABC, we should expect to see a row DEF after 500 lines of SQL transformation.”
We believe that unit testing is a good fit here. It is a widely adopted testing method among software developers but has yet to gain traction among the data community due to a lack of mature DevOps culture and available tools. Many data product developers are used to diving straight into writing SQL, testing, and rewriting it if issues are found. Instead, we want them to create mock-up test cases
before
they write any SQL code so they can focus on expected behaviors/outcomes rather than diving straight into implementations. Unit testing is well suited for views with complex business logic and can help ensure that the code produces the results as expected and reduce the likelihood of regression down the road.
For example, one of the transformation jobs uses BigQuery
User-defined Function
(UDF) for amortizing fixed marketing spend based on a decay rate. UDF opens so many more possibilities for the SQL-based transformation jobs, but they could become difficult to understand and maintain for the Data team. In this case, a unit test is set up with an input spend record of $100 below:
[
{
""report_date"": ""2022–09–01"",
""spend"": 100
}
]
Assuming a daily decay rate of 0.4, we expect the following output to be returned after the UDF transformation in BigQuery:
[
{
""amortized_spend"": 40,
""remaining_spend"": 60,
""amortized_spend_date"": ""2022–09–01"",
""day"": 0
},
{
""amortized_spend"": 24,
""remaining_spend"": 36,
""amortized_spend_date"": ""2022–09–02"",
""day"": 1
},
{
""amortized_spend"": 14.4,
""remaining_spend"": 21.6,
""amortized_spend_date"": ""2022–09–03"",
""day"": 2
},
{
""amortized_spend"": 8.64,
""remaining_spend"": 12.96,
""amortized_spend_date"": ""2022–09–04"",
""day"": 3
},
{
""amortized_spend"": 5.184,
""remaining_spend"": 7.776,
""amortized_spend_date"": ""2022–09–05"",
""day"": 4
},
{
""amortized_spend"": 3.1104,
""remaining_spend"": 4.6656,
""amortized_spend_date"": ""2022–09–06"",
""day"": 5
},
{
""amortized_spend"": 1.86624,
""remaining_spend"": 2.79936,
""amortized_spend_date"": ""2022–09–07"",
""day"": 6
},
{
""amortized_spend"": 1.119744,
""remaining_spend"": 1.679616,
""amortized_spend_date"": ""2022–09–08"",
""day"": 7
},
{
""amortized_spend"": 1.679616,
""remaining_spend"": 0,
""amortized_spend_date"": ""2022–09–09"",
""day"": 8
}
]
Production Observability
Given a table A, we expect column B to be unique or column C’s value fall in the range from XXX to XXX.
In addition to the shift-left unit testing approach above, Data Observability focuses on post-deploy monitoring to ensure that the production data satisfies certain assumptions, such as “Given a table A, we expect column B to be unique or column C’s value fall in the range from XXX to XXX.” Since production data is constantly changing, it is nearly impossible for developers to catch all edge cases during the development phase. Regularly executing data quality checks in production can help detect issues earlier and more proactively, rather than having to rely on stakeholders to report issues days later. Below are the common types of data quality checks we have enabled:
Unique constraint: For example, the table `application` should be unique at (`product_type`, `application_id`).
Cross table check: Checks if a column in the current table aggregates to the same value of a column in another table, with a XX% margin of tolerance. For instance, the difference between SUM(`table_1.spend`) and SUM(`table_2.spend`) should be within the margin of +/-5%.
Nullable check: Checks whether a column can have null values or not.
% nullable check: Sets a threshold of the maximum percentage of null values expected to see in a column.
Enum check: Checks if the value is one of the expected values. For example, the column `product` should be one of [“life”, “disability”, …].
Distribution check: Checks if the value of numeric types, including integers and floats, matches a range. For example, annual premium should be in the range of 0–1,000,000 and the median of annual premium should be in the range of 1,000–10,000, etc.
Figure 3: An example of the transformation jobs with the data quality checks.
Any instance of data quality checks can have the following types of alerts.
Hard-fail: Sends a PagerDuty incident to Engineer On-Call (EOC) from the Data team. It’s commonly used in strong integrity checks like unique constraints.
Soft-fail: Sends an alert to a designated Slack channel without creating an incident, but the EOC can declare an incident if needs be. This type of alert is useful for failures that are good to keep an eye for, for example, % nullable check on a column.
Silent-fail: Logs in DataDog only and alerts if the failure exceeds a certain threshold, but it does not send to any Slack channel or create an incident. This is the most tolerant data failure and can be used during the initial prototyping and iterations so it doesn’t create much noise.
Implementation
Data product developers use YAML for configurations (see example below) and JSON for seeding the unit test cases. We chose YAML and JSON because they are declarative and commonly used among the Data team, which eliminates the need to write custom Python code for tests and satisfies the first two requirements mentioned earlier. We use pytest,
bq-test-kit
and some lightweight custom script to dynamically generate and execute the tests. Unit tests are automated as part of the CICD pipeline, and Data Observability checks are run periodically as part of the transformation jobs in production. In the future, we might consider migrating the testing framework to Great Expectations or other similar tools if there is a strong fit. However, that should not change the current interface that data product developers use, as directed in the third requirement.
When we initially launched the tools, we noticed that developers were spending a significant amount of time on tedious tasks such as creating folder structure and copying input data. This became time-consuming, especially for views that referenced 20 or more upstream tables. To address this issue, we developed a scaffolding tool that auto-fills the configuration files and input/output data, resulting in a significant reduction of development time. This tool received very positive feedback from users.
As expected, it took a while for the data product developers to adopt the test-first mindset. We scheduled several rounds of training and pairings to get them used to the new workflow, and they began to appreciate it more when they realized “Oh, it’s actually working.” For example, the unit test immediately revealed a calculation error in the initial launch. It also happened multiple times that when an update was deployed to production but the observability checks failed, we were immediately notified, rectified the situation, and contained the blast radius before it became too widespread.
An example of the view config file with unit test and observability checks enabled:
- -
description: ""One row represents an application and its information.""
priority: 1
frequency_in_cron: ""3 13 * * 1–5""
fields:
- name: application_id
description: PK - This unique identifier represents a single application.
type: INTEGER
- name: created_at
description: When the application was first created.
type: DATETIME
- name: customer_id
description: The customer that the application belongs to. A customer can have multiple apps.
type: INTEGER
- name: product_type
description: The business product (e.g. life, home and auto).
type: STRING
test:
input:
- table: ""{project}.policygenius.life_applications_view""
schema:
file_path: postgresql/schemas/policygenius/life_applications.json
- table: ""{project}.policygenius.users_view""
schema:
file_path: postgresql/schemas/policygenius/users.json
validation:
table:
- check_type: unique_constraint
expects: application_id, product_type
fail_type: hard_fail
columns:
product_type:
- check_type: nullable
expects: False
fail_type: hard_fail
- check_type: enum
expects: ['life', 'disability', 'homeandauto', 'renters']
fail_type: soft_fail
Closing
It’s been over 6 months since we initially introduced the toolings. We now have a consistent and automated data testing process that covers about half of the transformation jobs. This not only reduced the risk of data breakages and decreased time-to-detect from days to hours, but it has also increased confidence of data product developers in deploying their code. Although we have not completely eradicated all data quality issues, we are empowering data product developers with more tools and a better development experience.
We are continuously expanding our test coverage within the Data team and exploring opportunities to expand the Data Observability framework upstream to Software Engineering, thereby increasing the coverage of our data end-to-end. Our work is not finished yet as we continue to innovate and identify ways to improve data quality.
Data Engineering
Engineering
Data Quality
Data Warehouse
Google Cloud Platform
--
--
Published in
Policygenius
139 Followers
·
Last published
May 24, 2023
Policygenius is America’s leading online insurance marketplace.
Follow
Written by
Lily Chang
96 Followers
·
87 Following
Currently Data Engineering Manager at Justworks. Love tinkering with data/software/platform and building teams.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality with data-flare on Apache Spark | The Startup,"Data Quality with data-flare on Apache Spark | The Startup
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Why I Built an Opensource Tool for Big Data Testing and Quality Control
Tim Gent
·
Follow
Published in
The Startup
·
4 min read
·
Aug 11, 2020
--
2
Listen
Share
I’ve developed an open-source data testing and a quality tool called
data-flare
. It aims to help data engineers and data scientists assure the data quality of large datasets using Spark. In this post I’ll share why I wrote this tool, why the existing tools weren’t enough, and how this tool may be helpful to you.
Who spends their evenings writing a data quality tool?
In every data-driven organisation, we must always recognise that without confidence in the quality of our data, that data is useless. Despite that there are relatively few tools available to help us ensure our data quality stays high.
What I was looking for was a tool that:
Helped me write high performance checks on the key properties of my data, like the size of my datasets, the percentage of rows that comply with a condition, or the distinct values in my columns
Helped me track those key properties over time, so that I can see how my datasets are evolving, and spot problem areas easily
Enabled me to write more complex checks to check other facets of my data that weren’t simple to incorporate in a property, and enabled me to compare between different datasets
Would scale to huge volumes of data
The tools that I found were more limited, constraining me to simpler checks defined in yaml or json, or only letting me check simpler properties on a single dataset. I wrote
data-flare
to fill in these gaps, and provide a one-stop-shop for our data quality needs.
Show me the code
data-flare
is a Scala library built on top of Spark. It means you will need to write some Scala, but I’ve tried to keep the interface simple so that even a non-Scala developer could quickly pick it up.
Let’s look at a simple example. Imagine we have a dataset containing orders, with the following attributes:
CustomerId
OrderId
ItemId
OrderType
OrderValue
We can represent this in a Dataset[Order] in Spark, with our order being:
case class Order(customerId: String, orderId: String, itemId: String, orderType: String, orderValue: Int)
Checks on a single dataset
We want to check that our orders are all in order, including checking:
orderType is “Sale” at least 90% of the time
orderTypes of “Refund” have order values of less than 0
There are 20 different items that we sell, and we expect orders for each of those
We have at least 100 orders
We can do this as follows (here orders represents our Dataset[Order]):
As you can see from this code, everything starts with a ChecksSuite. You can then pass in all of your checks that operate on single datasets using the singleDsChecks. We’ve been able to do all of these checks using SingleMetricChecks — these are efficient and perform all checks in a single pass over the dataset.
What if we wanted to do something that we couldn’t easily express with a metric check? Let’s say we wanted to check that no customer had more than 5 orders with an orderType of “Flash Sale”. We could express that with an Arbitrary Check like so:
The ability to define arbitrary checks in this way gives you the power to define any check you want. They won’t be as efficient as the metric based checks, but the flexibility you get can make it a worthwhile trade-off.
Checks on a pair of datasets
Let’s imagine we have a machine learning algorithm that predicts which item each customer will order next. We are returned another Dataset[Order] with predicted orders in it.
We may want to compare metrics on our predicted orders with metrics on our original orders. Let’s say that we expect to have an entry in our predicted orders for every customer that has had a previous order. We could check this using Flare as follows:
We can pass in dualDsChecks to a ChecksSuite. Here we describe the datasets we want to compare, the metrics we want to calculate for each of those datasets, and a MetricComparator which describes how those metrics should be compared. In this case we want the number of distinct customerIds in each dataset to be equal.
What happens when you run your checks?
When you run your checks all metrics are calculated in a single pass over each dataset, and check results are calculated and returned. You can then decide yourself how to handle those results. For example if one of your checks gives an error you could fail the spark job, or send a failure notification.
What else can you do?
Store your metrics and check results by passing in a metricsPersister and qcResultsRepository to your ChecksSuite (ElasticSearch supported out the box, and it’s extendable to support any data store)
Graph metrics over time in Kibana so you can spot trends
Write arbitrary checks for pairs of datasets
For more information check out
the documentation
and the
code
!
Apache Spark
Spark
Data Quality
Scala
Open Source
--
--
2
Published in
The Startup
814K Followers
·
Last published
just now
Get smarter at building your thing. Follow to join The Startup’s +8 million monthly readers & +772K followers.
Follow
Written by
Tim Gent
99 Followers
·
5 Following
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Testing data quality with SQL assertions | by Lewis Hemens | Dataform | Medium,"Testing data quality with SQL assertions | by Lewis Hemens | Dataform | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Testing data quality with SQL assertions
Lewis Hemens
·
Follow
Published in
Dataform
·
4 min read
·
Jul 10, 2019
--
Listen
Share
Ensuring that data consumers can use their data to reliably answer questions is of paramount importance to any data analytics team. Having a mechanism to enforce high data quality across datasets is therefore a key requirement for these teams.
Often,
input data sources are missing rows, contain duplicates, or include just plain invalid data
. Over time, changes to business definitions or the underlying software which produces input data can cause drift in the meaning of columns — or even the overall structure of tables. Addressing these issues is critical to creating a successful data team and generating valuable, correct insights.
In this article we explain the concept of a SQL data assertion, look at some common data quality problems, how to detect them, and — most importantly —
how to fix them in a way that persists for all data consumers
.
The SQL snippets in this post apply to Google BigQuery but can be ported easily enough to Redshift, Postgres or Snowflake data warehouses.
What is a data assertion?
A data assertion is a query that looks for problems in a dataset
. If the query returns any rows then the assertion fails.
Data assertions are defined this way because it’s much easier to look for problems rather than the absence of them. It also means that assertion queries can themselves be used to quickly inspect the data causing the assertion to fail — making it easy to diagnose and fix the problem.
Checking field values
Let’s take a look at a simple example.
Assume that there is a
database.customers
table containing information about customers in the database. Some checks that we might want to verify on the table's contents include:
The field
email_address
is always set
The field
customer_type
is one of
""business""
or
""individual""
The following simple query will return any rows violating these rules:
SELECT customer_id
FROM database.customers
WHERE  email_address IS NULL
OR NOT customer_type IN (“business”, “individual”)
Checking for unique fields
We may also want to run checks across more than one row. For example, we might want to verify that the
customer_id
field is unique. A query like the following will return any duplicate
customer_id
values:
SELECT
customer_id,
SUM(1) AS count
FROM database.customers
GROUP BY 1
HAVING count > 1
Combinining multiple assertions into a single query
We can combine all of the above into a single query to quickly find any
customer_id
value violating one of our rules using
UNION ALL
:
SELECT customer_id, “missing_email” AS reason
FROM database.customers
WHERE email_address IS NULL
UNION ALL
SELECT customer_id, “invalid_customer_type” AS reason
WHERE not customer_type in (“business”, “individual”)
FROM database.customers
UNION ALL
SELECT customer_id, “duplicate_id” AS reason
FROM (
SELECT customer_id, SUM(1) AS count
FROM database.customers
GROUP BY 1
)
WHERE count > 1
We now have one query we can run to detect any problems in our table, and we can easily add another unioned
SELECT
statement if we want to add new conditions in the future.
Creating clean datasets
Now that we’ve detected the issues in our data, we need to clean them up. Ultimately choosing how to handle data quality issues depends on your business use case.
In this example we will:
Remove any rows that are missing the
email_address
field
Set a default customer type if it’s invalid
Remove rows with duplicate
customer_id
fields, retaining one row per
customer_id
value (we don't care which one)
Rather than editing the dataset directly, we can create a new clean copy of the dataset
— this gives us freedom to change or add rules in the future and avoids deleting any data.
The following SQL query defines a view of our
database.customers
table in which invalid rows are removed, default customer types are set, and duplicate rows for the same
customer_id
are removed:
SELECT
customer_id,
ANY_VALUE(email_address) AS email_address,
ANY_VALUE(
CASE customer_type
WHEN “individual” THEN “individual”
WHEN “business”   THEN “business”
ELSE “unknown”
END) AS customer_type
FROM database.customers
WHERE NOT email_address IS NULL
GROUP BY 1
This query can be used to create either a view or a table in our cloud data warehouse, perhaps called
database_clean.customers
, which can be consumed in dashboards or by analysts who want to query the data.
Now we’ve fixed the problem, we can check that the above query has correctly fixed the problems by re-running the original assertion on the new dataset.
Continuous data quality testing
Assertions should be run as part of any data pipelines to make sure breaking changes are picked up the moment they happen.
If an assertion returns any rows, future steps in a pipeline should either fail, or a notification delivered to the data owner.
Dataform
has built in support for data assertions, and provides a way to run them as part of a larger SQL pipeline.
These can be run at any frequency, and if an assertion fails an email will be sent to notify you of the problem. Dataform also provides a way to easily create new datasets in your warehouse, making managing the process of cleaning and testing your data extremely straightforward.
For more information on how to start writing data assertions with Dataform, check out the
assertions documentation
guide for Dataform’s
open-source framework
, or
create an account for free
and start using Dataform’s fully managed Web platform.
Originally published at
https://dataform.co
.
Sql
Big Data
Data Quality
Data Engineering
--
--
Published in
Dataform
199 Followers
·
Last published
Nov 10, 2020
Dataform is the simplest way to manage your data
Follow
Written by
Lewis Hemens
35 Followers
·
7 Following
Co-founder @
dataform.co
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Advanced Snowflake-Native Data Quality Testing using Orchestra | by Will Davies | Orchestra’s Data Release Pipeline Blog | Medium,"Advanced Snowflake-Native Data Quality Testing using Orchestra | by Will Davies | Orchestra’s Data Release Pipeline Blog | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Orchestra is the Unified Data Control Plane for Snowflake
Advanced Snowflake-Native Data Quality Testing using Orchestra
The need for third-party tools has vastly diminished due to Snowflake Cortex and Orchestra’s integration with it
Will Davies
·
Follow
Published in
Orchestra’s Data Release Pipeline Blog
·
7 min read
·
Apr 30, 2024
--
Listen
Share
Introduction
We’re excited to announce
Orchestra’s
Data Quality and Data Observability suite built for
Snowflake
.
My Co-Founder
Hugo
has long spoken about the possibility of consolidating Orchestration with Observability, and while in the early days of Orchestra we were focused on building a best-in-class Data Orchestration tool for
Modular Architecture
, providing visibility to Data Teams has been on our roadmap from Day 1.
With this new capability, Orchestra users can not only control any workflows in Snowflake (from Streams and Tasks to Stored Procedures and Anomaly Detection) but can leverage the Orchestra for monitoring cost and Data Quality.
This builds on Snowflake’s advanced Anomaly Detection capabilities (built out using
Snowflake Cortex
) and Data Quality measures. This represents a significant value-add for Data Teams as it allows Engineers to “shift quality left” but also empowers Analysts to proactively monitor Data Quality to raise incidents.
Having a robust data infrastructure is crucial prerequisite for AI-readyness, and we can’t wait to see how you build Data and AI products with Data Quality monitors engrained using Orchestra!
Why is Data Quality such a big problem?
While many executives acknowledge the importance of Data, this has never become more evident than with the rise of AI. However, AI trained on poor quality data is hardly AI at all, and that is the situation many companies find themselves in.
Gartner reports that around 80% of
Data and AI initiatives fail
. For the most part, this is due to poor data quality but also a lack of holistic and end-to-end management. Where Data Practitioners in SMEs struggle to articulate the value of Data and AI Products to Business Stakeholders, simply building and maintaining Data and AI Products across a 50–100 person data organization in a Large Enterprise represents an even greater challenge.
Challenges facing Data Teams, including Data Quality (dbt’s 2024 report)
The challenge, therefore, is ensuring a high degree of collaboration, visibility, and incident response times
across
data pipelines and
across
stakeholders. It is not enough to build a dashboard for a sales team — they must be kept in the loop if there are data quality issues, at risk of losing trust. It is not enough for engineering and analytics teams to operate in a pure “data mesh” — some overarching monitoring can help foster additional collaboration and improve trust.
Fundamental Problems
Business stakeholders do not trust the Data Team
Teams are the last to know about failures
Data Quality is poor
Organisations’ Data is not AI Ready
Indeed, trust is an important point apparently lacking in Analytics Leaders. According to the State of Analytics Engineering 2024 report, over 80% of Analytics Leaders report that increasing trust in the Analytics Team is a high priority.
This is hardly a surprise considering the modular and disconnected way Analytics Teams work with Engineering Teams. It is all the more unsurprising due to generally pervading poor data quality, arguably driven by the “Just get the data into Snowflake” Attitude.
How does Orchestra solve these problems?
Let’s start with Poor Data Quality.
Poor Data Quality can be solved by introducing blocking tests into Data Pipelines. With Orchestra, users can run Snowflake Queries as Blocking Data Quality Tests on Snowflake Source tables. This can be done directly, or by using a transformation framework like dbt or Coalesce.io.
Adopting rigorous source-testing policies will ensure the data in Snowflake is of an acceptable quality.
Monitoring Data Quality in Orchestra
Failure alerting
Orchestra is a platform for triggering and monitoring the entire stack. Users can be alerted when different parts of the Data Pipeline complete, while chaining pipelines together (the concept in Airflow of “Sub-Dags”) to enable more separate, cross-functional work.
With granular alerting enabled on the Orchestra lineage graph, stakeholders are no longer the first to find out about data quality issues — instead, it’s the Data Team.
Retrying from failed nodes — this was only recently introduced in AIrflow 2.9 smh
Furthermore, Data Teams can quickly recover incidents using the “Retry from Failed” functionality, which not only minimises compute but also allows relevant stakeholders to be notified once pipelines are complete and incidents resolved.
Business Stakeholders do not trust the Data Team
By allowing Analytics Teams to split up Data Pipelines into Sub-Pipelines, Business stakeholders can have their own, dedicated section of the pipeline.
For example, a Data Analyst who has built a sales report for a Head of Sales could run data quality tests before refreshing and distributing a report. They would then have a log of whether the dashboard was successfully refreshed
and if not, why not
. This helps to build trust in the Data Team by adopting a Product mindset.
Definitions
Blocking Data Quality Test
A check that is run as part of a Data Pipeline. If the Check fails, then the Pipeline Fails and downstream tasks do not run.
Non-blocking data quality Test
A check that is run after a pipeline completes. These results are stored and can alert stakeholders, but should not block a pipeline. These are typically used to inform analysts about the state of the data, rather than test for minimum viable data quality
Minimum Viable Data Quality
The lowest level of Data Quality required to productise a Data Asset. These are typically defined by a schema and conditions for date/time completeness, uniqueness, null conditions and recency.
Additional Features
Snowflake Credits monitoring
Compute Credits
are shown for every Query executed directly or indirectly from Orchestra. This enables users to monitor and react to cost where models run for longer than expected
Granular dbt and Coalesce integrations
We recognise that the market-leading ways to transform data in Snowflake are by using
dbt
or Coalesce. Orchestra partners closely with both dbt and Coalesce, and queries executed from dbt or
Coalesce
such as tests will render accordingly in Orchestra, with Snowflake Credits included (if selected) as well.
Support for Streams and Tasks
One of the most powerful aspects of Snowflake is its support for streaming data. By nature, these operations are not batch and therefore are not suited to a batch-based SQL Control mechanism like dbt. These operations are typically created using the Snowflake UI or Git-controlled SnowSQL.
A flaw of many
OSS workflow orchestration tools
is their inability to understand Sub-DAGs in Snowflake that result from Streams and Tasks. For example, if root task A triggers multiple downstream tasks, this lineage information is normally lost. This applies to things like Azure Data Factory, Matillion, Databricks Workflows, and so on.
Orchestra has partnered closely with Snowflake to ensure this information renders in the Orchestra UI, so
Streams and Tasks
can be used (and data quality-tested) accordingly.
Support for Snowflake Cortex
Anomaly Detection
no longer requires Data Scientists to write complicated code for calculating data quality or working out standard deviations. Snowflake users can leverage Cortex’ in-built Anomaly Tests and these can be rendered in Orchestra too.
Anomaly Detection in Orchestra that leverages Snowflake Cortex
This democratises Data Science so anyone with a vested interest can leverage Machine Learning to improve and maintain Data Quality using state-of-the-art Machine Learning and Data Quality systems.
Conclusion
Monitoring data quality starts with ensuring stakeholders can be alerted when data pipelines fail. This helps foster collaboration and increase trust in the Data Team.
Through the
Orchestra UI
, Data Teams and Analytics Practitioners can now easily run blocking or ad-hoc data quality and anomaly tests using Snowflake. The Orchestra UI offers a clean, elegant, and powerful way to bring Data Quality to the fore in many businesses.
This will be absolutely crucial in enabling a successful AI Strategy on Snowflake. While the possibilities presented by Cortex are tempting, it would be a gross mistake to aim for these use-cases without robust data infrastructure and data quality testing in place first (particularly as queries run using Snowflake Cortex are orders of magnitude more expensive).
For more information, please get in touch or see additional resources below. If you’re ready to get started, we’ve put together this tutorial here.
Will and the Orchestra Team ⭐️
Find out More
We have a lot of resources on Snowflake Data Quality for you to get started:
Video Tutorials
📹
Snowflake Blog
❄️
Find us on Linkedin
💼
Check out our Integrations
🔗
Start now
:D
Data
Data Engineering
Snowflake
Data Quality
Data Orchestration
--
--
Published in
Orchestra’s Data Release Pipeline Blog
88 Followers
·
Last published
Nov 12, 2024
A blog by Orchestra on Data Release Pipelines
Follow
Written by
Will Davies
19 Followers
·
2 Following
I write tech guides aimed at helping Data Engineers build cool data products. I am CTO at Orchestra, a data release pipeline management platform.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Using Soda Core to test data quality | by Maksim Kazartsev | Medium,"Using Soda Core to test data quality | by Maksim Kazartsev | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Using Soda Core to test data quality
Maksim Kazartsev
·
Follow
8 min read
·
Feb 1, 2024
--
Listen
Share
My name is Maksim, and I like learning and experimenting with new tools and technologies.
Today I will share how to use
Soda Core
(both CLI and Python package) to check
data quality
based on the Superstore dataset in Snowflake.
It’s important to note that the provided instructions can be applied to any data source supported by Soda Core, not exclusively to Snowflake.
Photo by
Agence Olloweb
on
Unsplash
Tools used
Soda Core
Python
Snowflake
git, GitHub
VS Code (IDE)
About Soda
Before we dive into project implementation details, let me tell you a few words about Soda.
Soda (
https://www.soda.io/
) is a tool used for data quality checking and monitoring. It helps data teams automate the search for bad-quality data.
Its strengths lie in the automation, scalability, and integration of data quality checks into broader data workflows. It is particularly effective in larger or more dynamic data environments where ongoing monitoring and alerting are crucial.
History
Soda appeared in 2018. The company behind Soda, also named Soda, was founded in Brussels, Belgium.
Company mission — help data teams catch, prevent, and resolve data issues before they wreak havoc downstream.
The tool was designed to provide users with the ability to write checks and validations for their data, set up alerts, and generate reports on data quality.
Some of the companies using SODA: HelloFresh, Group 1001, Lululemon, Panasonic, Zefr, Zendesk.
Soda Distribution
Soda comes with the following three deliverables:
Soda Core
: is a free, open-source Python library and CLI tool with documentation.
Soda Cloud
: a more comprehensive, cloud-based solution that provides a UI for monitoring data quality, setting up alerts, and visualizing issues.
Soda Library
is an extension of Soda Core, which enables users to connect to Soda Cloud and is compatible with complex SodaCL checks and offers other features and functionality not available with the open-source tool.
A bit more about Soda Core
Since the article is dedicated to Soda Core, there are a few things worth mentioning about it:
Available for free
Compatible with basic SodaCL checks and configurations
Supports connections to 18+ data sources
Github —
https://github.com/sodadata/soda-core
Docs —
https://github.com/sodadata/soda-core/blob/main/docs/overview-main.md
Key Concepts
While working with Soda, you will inevitably come across the following terms and concepts specific to it.
SodaCL
— Soda Checks Language is a YAML-based language used to write checks for data quality.
Soda Check
is a test that Soda performs when it scans a dataset in your data source (more in documentation -
https://docs.soda.io/soda-cl/soda-cl-overview.html
)
Soda Scan
executes the checks you write and returns a result for each check:
pass
: the values in the dataset match or fall within the thresholds you specified
fail
: the values in the dataset do not match or fall within the thresholds you specified
error
: the syntax of the check is invalid
warn
: something you can explicitly configure for individual checks.
Metric
is a property of the data in your dataset. They can be
built-in
or
custom
.
SodaCL includes over 25 built-in metrics that you can use to write checks, but you also have the option of writing your own SQL queries or expressions using SodaCL.
More about SodaCL metrics and checks —
https://docs.soda.io/soda-cl/metrics-and-checks.html
A
threshold
is the value for a metric that Soda checks against during a scan. Usually, you use both a metric and a threshold to define a SodaCL check in a checks YAML file, like the following example that checks that the dim_customer dataset is not empty.
Soda metrics
Usage scenarios
Soda data quality checks can be applied at every stage of a data engineering project lifecycle so that you can test:
source data
ingested or staging data
transformation models
data marts
Soda checks throughout DE project lifecycle
When using
dbt
, Soda can substitute
dbt tests
or can supplement them by ingesting the results of your
dbt tests
and pushing them to Soda Cloud (see
https://docs.soda.io/soda/integrate-dbt.html
for more details).
Project
Armed with the theoretical basics of Soda, it’s time to move on to the project.
As a quick recap, we will consider two project scenarios to check the data quality of Superstore dataset in Snowflake:
Using Soda Core CLI
Using Soda Core Python package
Project Scenario 1: Soda Core CLI + Snowflake
Use Soda Core CLI on a local Windows machine to check the data in Snowflake.
Prerequisites
Python
installed
pip
installed
venv
installed
Snowflake
account
Step 1 — Install Soda Core
Docs —
https://github.com/sodadata/soda-core/blob/main/docs/installation.md
Create a virtual environment
python -m venv soda_venv
Activate the virtual environment
.\soda_venv\Scripts\activate
Upgrade pip inside your new virtual environment
python.exe -m pip install --upgrade pip
Install the soda-core-snowflake package
pip install soda-core-snowflake
Step 2 — Prepare a “configuration.yml” file
In the project directory, create the
configuration.yml
file. It will store connection details for Snowflake.
Use this instruction —
https://docs.soda.io/soda/connect-snowflake.html
— as a reference to copy+paste the connection syntax into your file, then adjust the values to correspond with your data source’s details.
Create and use system variables for
username
,
password
,
account
attributes for security.
data_source snowflake:
type: snowflake
username:  ${SNOWFLAKE_USER}
password:  ${SNOWFLAKE_PASSWORD}
account:   ${SNOWFLAKE_ACCOUNT}
database:  SUPERSTORE
schema:    RAW
warehouse: WH_SUPERSTORE
Run the following command in the prompt to test the configured connection to Snowflake:
soda test-connection -d snowflake -c ./soda/configuration.yml -V
In case of successful configuration, you should see the desired
Connection ‘snowflake’ is valid
message in output.
Step 3 — Write checks in a checks.yml file
Let’s create checks YAML file for our superstore dataset. Since we are going to test the
Orders
and
Returned_Orders
tables in Snowflake, I created a separate folder called
checks
and the
orders.yml
and
returned_orders.yml
files in it.
Your checks define a passing state, what you expect to see in your dataset. Do not define a failed state
Useful links:
- SodaCL tutorial —
https://docs.soda.io/soda/quick-start-sodacl.html
- SodaCL metrics and checks —
https://docs.soda.io/soda-cl/metrics-and-checks.html
orders.yml
# Checks for ORDERS table in Snowflake
checks for ORDERS:
# Built-in metrics:
- row_count > 0
- missing_count(row_id) = 0
- duplicate_count(row_id) = 0
- missing_count(order_id) = 0
- missing_count(order_date) = 0
# Custom metric
- count_order_date_after_ship_date = 0:
count_order_date_after_ship_date query: |
SELECT COUNT(*)
FROM ORDERS
WHERE order_date > ship_date
# Schema validation
- schema:
fail:
when required column missing:
- ORDER_ID
- YEAR
when wrong column type:
ORDER_ID: Integer
returned_orders.yml
checks for RETURNED_ORDERS:
# Built-in metrics:
- row_count > 0
# Referential integrity
- values in (order_id) must exist in orders (order_id)
Step 4 — Run a scan
Let’s introduce intentional errors in the dataset in Snowflake to double-check if Soda indeed catches the errors:
-- delete order_id CA-2018-145492 from orders
-- it should break our Referential integrity check for RETURNED_ORDERS
delete from orders where order_id = 'CA-2018-145492'
-- change ship_date for one row so that ship_date will be sooner than order_date
-- it should break our custom metric for ORDERS
update orders
set ship_date = '2018-11-07'
where row_id = 1
Apart from that, in the
orders.yml
configuration file, we added a check that the
YEAR
column should exist in the dataset and the
ORDER_ID
column has the type of Integer that is all NOT true. So, let’s see the output.
To run a scan of the data in your data source, execute the following command:
soda scan -d snowflake  -c ./soda/configuration.yml ./soda/checks/
It runs checks based on all YAML check files available in the
/checks
directory. In our case, it’s
orders.yml
and
returned_orders.yml
The output:
[00:20:51] Soda Core 3.1.5
[00:20:53] Using DefaultSampler
[00:20:54] Scan summary:
[00:20:54] 6/9 checks PASSED:
[00:20:54]     RETURNED_ORDERS in snowflake
[00:20:54]       row_count > 0 [PASSED]
[00:20:54]     ORDERS in snowflake
[00:20:54]       row_count > 0 [PASSED]
[00:20:54]       missing_count(row_id) = 0 [PASSED]
[00:20:54]       duplicate_count(row_id) = 0 [PASSED]
[00:20:54]       missing_count(order_id) = 0 [PASSED]
[00:20:54]       missing_count(order_date) = 0 [PASSED]
[00:20:54] 3/9 checks FAILED:
[00:20:54]     RETURNED_ORDERS in snowflake
[00:20:54]       values in (order_id) must exist in orders (order_id) [FAILED]
[00:20:54]         value: 1
[00:20:54]     ORDERS in snowflake
[00:20:54]       count_order_date_after_ship_date = 0 [FAILED]
[00:20:54]         check_value: 1.0
[00:20:54]       Schema Check [FAILED]
[00:20:54]         fail_missing_column_names = [YEAR]
[00:20:54]         fail_column_type_mismatch[ORDER_ID] expected(Integer) actual(TEXT)
[00:20:54]         schema_measured = [ROW_ID NUMBER, ORDER_ID TEXT, ORDER_DATE DATE, SHIP_DATE DATE, SHIP_MODE TEXT, CUSTOMER_ID TEXT, CUSTOMER_NAME TEXT, SEGMENT TEXT, COUNTRY TEXT, CITY TEXT, STATE TEXT, POSTAL_CODE TEXT, REGION TEXT, PRODUCT_ID TEXT, CATEGORY TEXT, SUBCATEGORY TEXT, PRODUCT_NAME TEXT, SALES NUMBER, QUANTITY NUMBER, DISCOUNT NUMBER, PROFIT NUMBER]
[00:20:54] Oops! 3 failures. 0 warnings. 0 errors. 6 pass.
As we can see from the output, all our traps have worked and checks failed where expected.
Scenario 2: Soda Core Python library + Snowflake
This scenario includes Soda Core Python library on local machine to check the data in Snowflake.
Steps 1, 2, and 3 in this scenario are identical to the previous scenario. So, refer to the previous section to complete them unless you did already.
The only difference will be in step 4, where we will be running a scan using the
Soda Core Python library
instead of
Soda Core CLI
.
We will be using the following
soda_scan.py
script to run a scan programmatically
from soda.scan import Scan
scan = Scan()
scan.set_data_source_name(""snowflake"")
# Add configuration YAML files
scan.add_configuration_yaml_file(file_path=""./soda/configuration.yml"")
# Add variables
scan.add_variables({""date"": ""2024-01-28""})
# Add check YAML files
scan.add_sodacl_yaml_file(""./soda/checks/orders.yml"")
scan.add_sodacl_yaml_file(""./soda/checks/returned_orders.yml"")
# Execute the scan
exit_code = scan.execute()
print(exit_code)
# Set logs to verbose mode, equivalent to CLI -V option
scan.set_verbose(True)
# Print results of scan
print(scan.get_logs_text())
Run the following command to execute a scan using the python script:
python .\python\soda_pandas_scan.py
Output:
2 #it's the Soda Core exit code that means Soda issues a failure on a check(s)
INFO   | Soda Core 3.1.5
INFO   | Using DefaultSampler
INFO   | Scan summary:
INFO   | 6/9 checks PASSED:
INFO   |     ORDERS in snowflake
INFO   |       row_count > 0 [PASSED]
INFO   |       missing_count(row_id) = 0 [PASSED]
INFO   |       duplicate_count(row_id) = 0 [PASSED]
INFO   |       missing_count(order_id) = 0 [PASSED]
INFO   |       missing_count(order_date) = 0 [PASSED]
INFO   |     RETURNED_ORDERS in snowflake
INFO   |       row_count > 0 [PASSED]
INFO   | 3/9 checks FAILED:
INFO   |     ORDERS in snowflake
INFO   |       count_order_date_after_ship_date = 0 [FAILED]
INFO   |         check_value: 1.0
INFO   |       Schema Check [FAILED]
INFO   |         fail_missing_column_names = [YEAR]
INFO   |         fail_column_type_mismatch[ORDER_ID] expected(Integer) actual(TEXT)
INFO   |         schema_measured = [ROW_ID NUMBER, ORDER_ID TEXT, ORDER_DATE DATE, SHIP_DATE DATE, SHIP_MODE TEXT, CUSTOMER_ID TEXT, CUSTOMER_NAME TEXT, SEGMENT TEXT, COUNTRY TEXT, CITY TEXT, STATE TEXT, POSTAL_CODE TEXT, REGION TEXT, PRODUCT_ID TEXT, CATEGORY TEXT, SUBCATEGORY TEXT, PRODUCT_NAME TEXT, SALES NUMBER, QUANTITY NUMBER, DISCOUNT NUMBER, PROFIT NUMBER]
INFO   |     RETURNED_ORDERS in snowflake
INFO   |       values in (order_id) must exist in orders (order_id) [FAILED]
INFO   |         value: 1
INFO   | Oops! 3 failures. 0 warnings. 0 errors. 6 pass.
As we can see, it’s identical to what we saw when using CLI.
Conclusion
Soda Core
is a powerful and versatile tool for ensuring data quality, offering both command-line interface (CLI) and Python package options to suit different workflows and preferences. It is a valuable asset for data teams looking to maintain the integrity of their data.
In this article, we’ve touched on Soda’s purpose, a bit of its history, key concepts, and how to configure and run a project to check the data quality of a Superstore dataset in Snowflake using the Soda Core tool (CLI and Python package).
The project just scratches the surface and provides very basic samples for the configuration and usage scenarios of Soda. Nevertheless, it can be a good starting point for your data project. Snowflake can be easily swapped to any other data source supported by Soda Core.
Link the project repository on Github —
https://github.com/kazarmax/soda_core_snowflake
Soda project in VS Code
Follow me on LinkedIn —
https://www.linkedin.com/in/kazarmax/
Soda
Soda Core
Data Quality
Data Quality Management
Data Engineering
--
--
Follow
Written by
Maksim Kazartsev
74 Followers
·
11 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
“Big Data Testing Challenges”. Big data is a collection of large… | by Tahmina Naznin | Oceanize Lab Geeks | Medium,"“Big Data Testing Challenges”. Big data is a collection of large… | by Tahmina Naznin | Oceanize Lab Geeks | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
“Big Data Testing Challenges”
Tahmina Naznin
·
Follow
Published in
Oceanize Lab Geeks
·
4 min read
·
Sep 17, 2018
--
Listen
Share
Big data is a collection of large datasets that cannot be processed using traditional computing techniques. It has many different uses — real-time fraud detection, web display advertising and competitive analysis, call center optimization, social media and sentiment analysis, intelligent traffic management and smart power grids and so on.
Big Data Testing Strategy:
Testing Big Data application is more a verification of its data processing rather than testing the individual features of the software product. When it comes to Big data testing,
performance and functional testing
are the keys. In Big data testing QA engineers verify the successful processing of terabytes of data using commodity cluster and other supportive components. It demands a high level of testing skills as the processing is very fast. Processing may be of three types:
Along with this, data quality is also an important factor in big data testing. Before testing the application, it is necessary to check the quality of data and should be considered as a part of database testing.
It involves checking various characteristics like conformity, accuracy, duplication, consistency, validity, data completeness,
etc.
Big Data Testing can be categorized into three stages:
Step 1: Data Staging Validation
The first stage of big data testing is also known as a Pre-Hadoop stage which comprises of process validation.
Validation of data is very important so that the data collected from various source like RDBMS, weblogs etc is verified and then added to the system.
To ensure data match you should compare source data with the data added to the
Hadoop
system.
Make sure that the right data is taken out and loaded into the accurate HDFS location
Step 2: “Map Reduce” Validation
Validation of “Map Reduce” is the second stage. Business logic validation on every node is performed by the tester. Post that authentication is done by running them against multiple nodes, to make sure that the:
The process of Map Reduce works perfectly.
On the data, the data aggregation or segregation rules are imposed.
Creation of key-value pairs is there.
After the Map-Reduce process, Data validation is done.
Step 3: Output Validation Phase
The output validation process is the final or third stage involved in big data testing. The output data files are created and they are ready to be moved to an
EDW (Enterprise Data Warehouse)
or any other such system as per requirements. The third stage consisted of:
Checking on the transformation rules are accurately applied.
In the target system, it needs to ensure that data is loaded successfully and the integrity of data is maintained.
By comparing the target data with the
HDFS file system data
, it is checked that there is no data corruption.
Architecture Testing
Hadoop processes very large volumes of data and is highly resource intensive. Hence, architectural testing is crucial to ensure success of your Big Data project. Poorly or improper designed system may lead to performance degradation, and the system could fail to meet the requirement. Atleast,
Performance and Failover test
services should be done in a Hadoop environment.
Performance testing includes testing of job completion time, memory utilization, data throughput and similar system metrics. While the motive of Failover test service is to verify that data processing occurs seamlessly in case of failure of data nodes.
Performance Testing
Performance Testing for Big Data includes two main action
Data ingestion and Throughout
: In this stage, the tester verifies how the fast system can consume data from various data source. Testing involves identifying different message that the queue can process in a given time frame. It also includes how quickly data can be inserted into underlying data store for example insertion rate into a Mongo and Cassandra database.
Data Processing
: It involves verifying the speed with which the queries or map reduce jobs are executed. It also includes testing the data processing in isolation when the underlying data store is populated within the data sets. For example running Map Reduce jobs on the underlying HDFS
Sub-Component Performance
: These systems are made up of multiple components, and it is essential to test each of these components in isolation. For example, how quickly message is indexed and consumed, mapreduce jobs, query performance, search, etc.
Performance Testing Approach
Performance testing for big data application involves testing of huge volumes of structured and unstructured data, and it requires a specific testing approach to test such massive data.
Performance Testing is executed in this order
Process begins with the setting of the Big data cluster which is to be tested for performance
Identify and design corresponding workloads
Prepare individual clients (Custom Scripts are created)
Execute the test and analyzes the result (If objectives are not met then tune the component and re-execute)
Optimum Configuration
Tools used in Big Data Scenarios
Challenges in Big Data Testing
Automation
Automation testing for Big data requires someone with a technical expertise. Also, automated tools are not equipped to handle unexpected problems that arise during testing
Virtualization
It is one of the integral phases of testing. Virtual machine latency creates timing problems in real time big data testing. Also managing images in Big data is a hassle.
Large Dataset
Need to verify more data and need to do it faster
Need to automate the testing effort
Need to be able to test across different platform
Resource: Guru 99 & CABOT.
Big Data
--
--
Published in
Oceanize Lab Geeks
359 Followers
·
Last published
Jul 24, 2020
This will be used in technology, project management and others system development.
Follow
Written by
Tahmina Naznin
58 Followers
·
6 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality Test Using Machine Learning | by Dios Kurniawan | Medium,"Data Quality Test Using Machine Learning | by Dios Kurniawan | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality Test Using Machine Learning
Dios Kurniawan
·
Follow
7 min read
·
Oct 4, 2022
--
Listen
Share
Anomaly Detection (photo: Author)
This post continues my previous post on outlier detection as a statistical method to identify
Data Quality (DQ)
issues. If you haven’t read my previous article, click
here
. This time around, a different approach is taken by employing machine learning (ML) techniques.
As I explained in my previous post, a sudden change in the number of outliers in a dataset is a strong indicator that we may have a data quality issue. Outliers can be legitimate, but more often than not, they are
anomalies
which should be removed from the dataset. Finding anomalies is a crucial step before allowing the dataset to be used for further processing.
To demonstrate how the anomaly detection works, I took a sample dataset containing the
Daily Sales Revenue statistics
of a fictitious company, as shown below:
Our intention is to detect the day in which revenue numbers in any of the Sales Areas deviate from what we consider as normal. This is important because sudden change in revenue is generally not expected. However, eyeballing the data row-by-row to search for anomalies can be a daunting, error-prone task. We need a machine to do that for us. To achieve this, we can use an ML algorithm called
Isolation Forest
.
Isolation Forest is a classifier which traces its root from the popular Decision Tree algorithm. In short, Isolation Forest algorithm tries to build a tree and directly
isolate outliers
. This is precisely what we need to detect data quality issues (while it is an interesting topic, I may not be the right person to explain the mathematical background of this algorithm in detail. If you wish to understand the theory behind Isolation Forest, please visit
https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf
).
Isolation Forest allows us to detect
multivariate
outliers
, meaning we can find unusual values in more than one variable. For the first run, we will use two variables only, “NQ” and “Total” from the table. “NQ” is particularly interesting because it indicates revenue transactions which cannot be categorized into the correct Sales Area. A sudden jump in “NQ” is a clear sign that something has gone wrong. Let’s visualize the data in a scatter plot (with the values have been normalized):
By looking at the scatter plot above, outliers can be identified as dots which are distant away from the majority in the mid-lower part of the chart. There are both outliers in “NQ” and “Total” variables. Human eyes can easily see these, but computers need to be trained to achieve the same. Using Python’s
Scikit-learn
library, Isolation Forest can be implemented with few lines of code like the example below:
from sklearn.ensemble import IsolationForest
rng = np.random.RandomState(88)
X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state=rng)
dfTrain = pd.DataFrame(list(zip(X_train, y_train)), columns=['total_','nq_'])
dfTest = pd.DataFrame(list(zip(X_test, y_test)), columns=['total_','nq_'])
clf =
IsolationForest
(n_estimators=100, max_samples='auto', contamination=0.1, random_state=rng)
clf.fit(dfTrain)
y_pred_train = clf.predict(dfTrain)
y_pred_test = clf.predict(dfTest)
dfPredTest = pd.DataFrame(y_pred_test)
dfPredTest = dfTest.join(dfPredTest, how='left')
dfOutlier = dfPredTest.loc[dfPredTest[0] < 0]
print(""Outlier Count="", dfOutlier.count())
This algorithm needs a random number to start the work, and we can experiment with this number to see different result. One thing to note, the algorithm takes one parameter called
contamination
, which in the above example, is set to 0.1, meaning we expect outliers will be less than 10% of the population (this is because the dataset is small; in a larger dataset, 2% will be a more reasonable limit). The
estimator
parameter can be set to above 100 to gain some accuracy at the price of more processing time.
When I ran the program on a dataset of approximately 300 records, the model produced outliers count = 16. Let’s superimpose the outliers in the previous scatter plot, marking the 16 outliers with red color:
As we can see, most obvious outliers have successfully been detected, but not all. The detection performance can be improved by experimenting with the parameters and by adding more historical data (the above example splits data into training and test datasets with 80–20 ratio). To complete the detection, we should also perform another run with all 5 variables in the table (AREA1, AREA2, etc.), at the same time. This might result in a smaller set of outliers like the example below:
Since we already have the anomalies, they can then be visualized in a Tableau dashboard with different color, alerting us to immediately perform some investigation on them.
A sample Data Quality Dashboard
Other ML Algorithm
Another interesting alternative for outlier detection is clustering algorithm called
DBSCAN
. The good thing about DBSCAN is that it does the job almost autonomously, it does not require us to supply the number of clusters to be created unlike other algorithms such as K-Means. The implementation in Scikit-learn is also quite straightforward. The snippet of the code looks like below:
from sklearn.cluster import DBSCAN
db_default = DBSCAN(eps = 0.5, min_samples = 3).fit(dfRecharge)
labels = db_default.labels_
There are two important parametes to be supplied;
eps
and
min_samples
. It takes some experiments to find the right values. Running it on similar Daily Sales statistics table (with few months of historical data) results in two clusters, with the anomalies are flagged as CLUSTER = -1 as seen below:
Visualized in a scatter plot, anomalies are marked red:
Most anomalies can be detected pretty much effectively with this technique. However, DBSCAN tends to run slowly and seems to take a lot of computing resource. In many cases it simply fails to produce any useful output when the dataset becomes too large.
Doing It at Scale
The above programs cover datasets with size of only few hundred rows. What if we want to detect outliers in a much larger dataset, say, 1 million records? In real-world application, this will likely be the usual case. That’s the where challenge begins!
To search for outliers in a large dataset, we cannot use the standard Python because it would simply break after reaching a certain point. Python and Scikit-learn are not actually designed for big data. We would ideally have to employ
Spark MLLib
for data at this scale. However, at the time of writing, Spark MLLib does not support Isolation Forest or DBSCAN. Alas, we are stuck with Scikit-learn.
To make an attempt to see how far Scikit-learn could go, I created a sample of 1 million, 2 million, 10 million, 15 million and a gigantic 150 million records. In a hope to exploit all available computing resources, I tried creating
Spark UDF
(
user-defined function
) to make Isolation Forest as if it is a Spark function. Also, I used
sparkContext.broadcast()
function to distribute the data and model to all nodes, minimizing communication cost. Here is the code :
def get_outliers(a, b):
result = 0
x_pred = [(a, b)]
try:
x_pred = b_scaler.value.transform(x_pred)
result = b_model1.value.predict(x_pred)[0]
except:
print('{0} Error in {1} : '.format(dTime, x_pred))
return int(result)
udf_get_outliers = F.udf(get_outliers, IntegerType())
scaler = pp.StandardScaler(copy=True, with_mean=True, with_std=True)
model1 = IsolationForest(n_estimators=150, max_samples='auto', contamination=0.02, random_state=42)
X_train = scaler.fit_transform(X_train)
y_train = model1.fit(X_train)
b_scaler = spark.sparkContext.broadcast(scaler)
b_model1 = spark.sparkContext.broadcast(y_train)
df1 = df1.withColumn('prediction', udf_get_outliers('rev_mtd_', 'rev_m2_'))
nOutliers = df1.where(F.col('prediction') < 0).count()
Was it successful? Not really. The above program worked for up to 10 million records, but it crashed when faced with 15 million records. A far cry from 150 million records that I wanted! Running it on a sampled table with 10 million rows and 2 variables took 5 hours, resulting in around 200 outliers found.
As you can see, putting the code into production-level data is the real challenge. A true implementation using Spark remains something to wish for. After some Googling, I found a library that the creator claimed to use Isolation Forest in Spark (
https://github.com/titicaca/spark-iforest
). I would really like to try this, however I still don’t know how (and don’t have time) to make it work. Would anyone be interested in helping me? Drop me an email!
--
--
Follow
Written by
Dios Kurniawan
60 Followers
·
15 Following
Big Data Analytics, Data Warehousing, Machine Learning, Software Development, Data Governance, Privacy and Protection
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
The essential role of automated tests in data pipelines | by Bruno Gonzalez | Medium,"The essential role of automated tests in data pipelines | by Bruno Gonzalez | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
The essential role of automated tests in data pipelines
Bruno Gonzalez
·
Follow
12 min read
·
Oct 17, 2023
--
1
Listen
Share
Photo by
Sigmund
on
Unsplash
This article was originally posted in Data Quality Guru Substack:
https://dataqualityguru.substack.com/p/the-essential-role-of-automated-tests
Subscribe to support my work!
In the old days of data engineering, we were far from “normal” software engineers. We were using SSIS or Pentaho, and things like version control or testing weren’t part of our toolbox. Luckily, things have changed, and we incorporated a lot of concepts from software engineering.
Meme of the gray cat. Image by the author.
If there is something we struggle with as engineers, it is testing. We know it is important to write unit and integration tests, think about test cases, and improve coverage, but in the end we don’t do anything (or barely the minimum).
In the following lines, I will discuss why having automated tests for our data pipelines is crucial, what is different about testing data, and present an example with
pytest
, highlighting valuable features that could simplify our testing process.
Early in my career, as an on-call data engineer at Lyft, I accidentally introduced a breaking code change while attempting to ship a hotfix at 4 AM to a SQL job that computed tables for core business analytics. A seemingly small change in filtering logic ended up corrupting data for all downstream pipelines and breaking dashboards for the entire company. Gleb Mezhanskiy —
Source
We can relate to Gleb’s story (I can, at least). At some point in our career, we did something similar. The change seemed easy; we knew the pipeline, and the ad-hoc queries we ran to test our idea were okay, but somehow we didn’t consider a border case that broke something.
Everything would have been easier if we had something to verify our changes, understand if those affect downstream processes, and warn us. It’s called testing.
What’s the problem with testing?
Testing data is way more complicated than testing “normal” code. We must deal with schema changes, missing columns, null values, duplicates, values out of range, and the code orchestrating extraction, transformation, and loading. And I didn’t even mention the state of the data assets we interact with, which could also have inconsistencies and missing data.
Does that mean that we don’t need to build them? No. We face considerable disadvantages when we don’t have automated tests:
It’s harder to determine the impact of our changes. As we read in Gleb’s story, small and seemingly innocent changes can corrupt valuable downstream data assets.
Bugs can remain undetected for some time. Sometimes, the bug only applies to a subset of the data, or it’s hard to detect them looking at aggregated data, making the problem worse. The longer it takes to detect it, the harder it is to fix the consequences. In a talk from
Jacquline Bilston from Yelp at DATA+AI Summit 2022
, she mentioned that it took
five days to find a critical bug and three days to fix it
.
Errors have a high cost. If we introduce a bug in a data pipeline, rolling back the change and fixing the problem is usually expensive. We spend valuable time analyzing the root cause, the wrong data propagates to multiple data assets, and backfills are painful (even when pipelines are well-designed).
If we are now convinced to start testing our data pipelines, we will need to face these challenges that are unique to data testing:
Data in testing environments is way different than the data in production. Not only velocity (that maybe doesn’t matter if you’re not testing a streaming pipeline) or volume, but variety. We can have a subset of production data, but it will never be identical. There will be some nuances that aren’t captured. In addition to that, since we use that environment to test, we have a high chance that the data is corrupted or incomplete.
Setting up (and maintaining) the proper testing environment is costly. We might need to create a separate cluster, copy a subset of production data (or create fake data), run scripts to generate the state we want and think about the cases we want to test. Moving data in the cloud could be expensive, so don’t take that easy.
We have tools that can help with those challenges. We can create a “fork” of our production data using
lakefs
or
Nessie
(if using data lakes) or create fake data using
Faker
or
Mockaroo
(if you prefer a UI), but we still need to think about the test cases, build the infrastructure around the testing environment, and understand (and code) the nuances that could happen during the pipeline execution.
Solving the problem with testing
No, the title isn’t an error. I presented the challenges, but the disadvantages are still there. We still need to test our pipelines, so the solution to the “testing problem” is testing (I’m sorry).
Difficult decision. Image by the author.
There are different types of testing in the context of data pipelines, which I combined from software engineering and ideas from
this article
:
Unit tests: Validate each logical unit or functions that are part of the ETL process. If the pipeline consists of a group of transformations, those can be tested separately using a set of input values and expected output.
Contract tests: Applicable for assets consumed in downstream processes. I already presented some concepts in my
previous post
, but the idea is to test the items from the contract: schema, semantics, references, distribution, and freshness (SLAs).
Data quality tests: Audit the data stored in a data asset to check for accuracy, consistency, completeness, uniqueness, and timeliness.
Integration tests: The flow between different data assets is correct, and there are no communication, interaction, or compatibility problems.
Performance tests: Assesses the resource utilization and scalability of the pipeline. This is crucial for high-volume data pipelines to meet the required SLAs.
End-to-end tests: Test the data pipeline as a whole, from the source to the target or output. This could be categorized as “black box” testing because there is no need to know the internal structure of the pipeline, but only the expected output given the input.
There are many options, and each one has its challenges. For the rest of the post, I will focus on end-to-end testing with
pytest
, highlighting some features that will be useful in our journey of testing data pipelines.
Please consider subscribing to my
Substack
to support my writing.
Data pipeline example
We will use the following data pipeline: a simple ETL for product reviews.
Simple ETL. Image by the author.
API: REST service providing customer reviews.
Extract: HTTP client that extracts the reviews using a timestamp filter (reviews created from that timestamp).
Transformation: Flatten the structure to aggregate the engagements of the review (other users marked them as helpful or reported it), remove engagements from “trolls”, and create the entities to populate the data assets.
Load: Update the engagement database (relational DB) and insert product reviews in an object storage.
Here is an example of a product review:
{
""user_id"": ""U34567"",
""product_id"": ""P22334"",
""product_name"": ""SmartCool Air Conditioner"",
""title"": ""Confusing controls but works well"",
""review"": ""It's hard to figure out the controls initially. But once you get the hang of it, it cools the room quickly."",
""stars"": 3,
""timestamp"": ""2023-08-24T09:15:00Z"",
""helpful"": [
{""user_id"": ""U67890"", ""timestamp"": ""2023-08-24T11:10:00Z""},
{""user_id"": ""U11111"", ""timestamp"": ""2023-08-24T12:05:00Z""}
],
""reports"": [
{""user_id"": ""U23456"", ""timestamp"": ""2023-08-24T13:00:00Z""},
{""user_id"": ""U22222"", ""timestamp"": ""2023-08-24T14:30:00Z""},
{""user_id"": ""U88888"", ""timestamp"": ""2023-08-24T18:45:00Z""}
]
}
By the way, I didn’t write the example. I used the following prompt with GPT-4:
I'm preparing an ETL that consists of the analysis of product reviews. The reviews are provided in a REST API that includes ""user_id"", ""product_id"", ""product_name"", ""title"", ""review"", ""stars"", ""timestamp"", and two lists: ""helpful"" and ""reports"". The two lists contain the ""user_id"" and the user's ""timestamp"" engaging with the review as helpful or the report.
The reviews can have zero helpful marks and zero reports, but some will have up to 5 of them (usually, all are in the helpful or report side, not both).
Can you mock 4 product reviews with different scenarios? The ""user_id"" in the ""helpful"" and ""report"" lists must be other than the ones that made the review.
Here is a simple implementation in Python (yes, I asked ChatGPT to generate it):
def print_function_name(func):
def wrapper(*args, **kwargs):
print(f""Executing function: {func.__name__}"")
return func(*args, **kwargs)
return wrapper
class ETLProcess:
# Extraction related functions
@print_function_name
def extract_reviews(self, timestamp):
""""""
Extracts reviews using the timestamp filter.
""""""
pass
# Transformation related functions
@print_function_name
def flatten_reviews(self, reviews):
""""""
Flattens the reviews data structure and aggregates the engagements (helpful and reports).
""""""
pass
@print_function_name
def identify_trolls(self, reviews):
""""""
Queries the engagement table using the user_id from the engagements and identifies the trolls in the extracted reviews.
""""""
pass
@print_function_name
def filter_trolls(self, reviews, trolls):
""""""
Removes engagements from trolls.
A troll is a user who reports 5-star reviews or any review with more than 5 helpful marks.
""""""
pass
@print_function_name
def create_entities_for_data_assets(self, reviews):
""""""
Transforms reviews into entities to populate the data assets.
""""""
pass
# Load related functions
@print_function_name
def update_trolls_database(self, trolls):
""""""
Updates the relational database with identified trolls.
""""""
pass
@print_function_name
def load_reviews_to_object_storage(self, reviews):
""""""
Inserts the reviews into an object storage.
""""""
pass
# Main ETL process
def run_etl(self, timestamp):
""""""
Main ETL function that runs the extraction, transformation, and load processes in sequence.
""""""
# Extract
extracted_reviews = self.extract_reviews(timestamp)
# Transform
flattened_reviews = self.flatten_reviews(extracted_reviews)
trolls = self.identify_trolls(extracted_reviews)
non_troll_reviews = self.filter_trolls(flattened_reviews, trolls)
entities = self.create_entities_for_data_assets(non_troll_reviews)
# Load
self.update_trolls_database(trolls)
self.load_reviews_to_object_storage(entities)
etl = ETLProcess()
start_timestamp = ""2023-08-01T00:00:00Z""  # Example timestamp
etl.run_etl(start_timestamp)
Testing with pytest
pytest
is a popular testing framework for Python that allows us to write simple, scalable, and efficient tests. It provides a lot of features that are relevant for testing data pipelines, such as:
Test discovery. Automatically finds and runs all the tests in the project, which makes it easy to organize and maintain the test suite.
Fixtures. Functions that run before the tests that can be used to set up the test environment, create test data, and provide reusable objects.
Parameterized tests. Run the same test with different input data, which is helpful for testing data pipelines that process large volumes of data.
Mocking. Technique in which certain parts of the code are replaced with fake objects during testing.
Assertions. Test the correctness of the code with a wide range of assertions, such as
assert
,
assertEqual
,
assertAlmostEqual
, and more.
With
pytest
, we can write end-to-end tests that simulate the entire pipeline, from data ingestion to output.
To show the most relevant features, I will build an end-to-end test focusing on testing the transformations.
Test folder structure
This could be the folder structure:
etl_project
├── pyproject.toml
├── README.md
├── etl_project
│   └── __init__.py
│   └── etl_process.py
└── tests
└── __init__.py
└── test_extraction.py
└── test_transformation.py
└── test_load.py
We will separate each logic group into a different file. All the tests can be written in one file, but it is a good practice to separate them. I recommend separating them further if the transformation has different components, but we can have a generic transformation test file if we want to build an end-to-end test.
The test files will be discovered automatically when you run
pytest
. This
article
has a very good list of the most used command line flags.
Fixtures
Fixtures can be used to set up the test environment, create test data or reusable objects.
Let’s set up the environment: create the connection to the test database, and add some “trolls” to have something to filter in the transformation step.
import os
import psycopg2
import pytest
@pytest.fixture(scope=""session"")
def environment_setup():
# Database connection
test_conn_url = os.environ.get(""TEST_RELATIONAL_DB"")
connection = psycopg2.connect(test_conn_url)
cursor = connection.cursor()
# Setup code: Create table, insert rows
cursor.execute(""CREATE TABLE test.engagement (LIKE public.engagement INCLUDING ALL);"")
cursor.execute(""INSERT INTO test.engagement VALUES ('U22222', 10, 0, 10), ('U88888', 5, 1, 4);"")
connection.commit()
yield cursor  # This returns control to the test function
# Teardown code: Cleanup after tests
cursor.execute(""DROP TABLE test.engagement;"")
connection.commit()
cursor.close()
connection.close()
After the setup, we returned the control to the test function using
yield
to finally remove the table we created and close the connection. In this case, we defined the scope as “session” only to be invoked once per test session (because it requires network access and is “expensive” to create). You can read more about scope options
here
.
From
pytest
documentation
:
At a basic level, test functions request fixtures they require by declaring them as arguments.
When
pytest
runs a test, it looks at the parameters in that test function’s signature and then searches for fixtures that have the same names as those parameters. Once
pytest
finds them, it runs those fixtures, captures what they returned (if anything), and passes those objects into the test function as arguments.
If we want to create a test that will use the environment setup, we need to include the fixture name as a function parameter.
def test_transformations(environment_setup):
# We can use the cursor here using the environment_setup variable
cursor = environment_setup
# Test code here
cursor.execute(""..."")
Pro tip:
We can automatically request a fixture by passing in
autouse=True
to the fixture’s decorator. This can avoid a lot of redundant requests from different test functions.
We can also use fixtures to create test data (using the example from the previous section). This could be useful if we want to perform unit tests, for example, to the
flatten_reviews
function.
import pytest
@pytest.fixture(scope=""module"")
def api_response():
# Here we can use Faker if we want
return [
{
""user_id"": ""U34567"",
""product_id"": ""P22334"",
""product_name"": ""SmartCool Air Conditioner"",
""title"": ""Confusing controls but works well"",
""review"": ""It's hard to figure out the controls initially. But once you get the hang of it, it cools the room quickly."",
""stars"": 3,
""timestamp"": ""2023-08-24T09:15:00Z"",
""helpful"": [
{""user_id"": ""U67890"", ""timestamp"": ""2023-08-24T11:10:00Z""},
{""user_id"": ""U11111"", ""timestamp"": ""2023-08-24T12:05:00Z""}
],
""reports"": [
{""user_id"": ""U23456"", ""timestamp"": ""2023-08-24T13:00:00Z""},
{""user_id"": ""U22222"", ""timestamp"": ""2023-08-24T14:30:00Z""},
{""user_id"": ""U88888"", ""timestamp"": ""2023-08-24T18:45:00Z""}
]
}
]
Parameterized tests
We can run the same test with different input data. It could be helpful if we want to create different states of the database in an end-to-end testing or unit test one function using different input parameters.
Here is an example of how to use parametrized tests:
import pytest
from etl_project.etl_process import ETLProcess
# Sample data for testing
sample_reviews = [
{
""user_id"": ""U34567"",
""product_id"": ""P22334"",
""stars"": 3,
""helpful"": [
{""user_id"": ""U67890"", ""timestamp"": ""2023-08-24T11:10:00Z""},
{""user_id"": ""U11111"", ""timestamp"": ""2023-08-24T12:05:00Z""},
],
""reports"": [
{""user_id"": ""U23456"", ""timestamp"": ""2023-08-24T13:00:00Z""},
{""user_id"": ""U22222"", ""timestamp"": ""2023-08-24T14:30:00Z""},
{""user_id"": ""U88888"", ""timestamp"": ""2023-08-24T18:45:00Z""},
]
},
{
""user_id"": ""U67890"",
""product_id"": ""P45678"",
""stars"": 5,
""timestamp"": ""2023-08-27T15:20:00Z"",
""helpful"": [
{""user_id"": ""U11111"", ""timestamp"": ""2023-08-27T19:10:00Z""},
{""user_id"": ""U22222"", ""timestamp"": ""2023-08-27T20:15:00Z""},
],
""reports"": [
{""user_id"": ""U23456"", ""timestamp"": ""2023-08-27T17:05:00Z""},
]
}
]
@pytest.mark.parametrize(
""reviews, trolls, expected_reports_length"",
[
(sample_reviews, [""U23456""], [2, 0]),
(sample_reviews[1:2], [], [1])  # No trolls, should return all reports
]
)
def test_filter_trolls(reviews, trolls, expected_reports_length):
etl = ETLProcess()
filtered_reviews = etl.filter_trolls(reviews, trolls)
reports_length = [len(r[""reports""]) for r in filtered_reviews]
assert reports_length == expected_reports_length
The first argument to
pytest.mark.parametrize()
is a comma-delimited string of parameter names that will be replicated in the test function parameters. The second argument is a
list
of tuples or single values representing the parameter value(s).
Mocking
Here we replace certain parts of the code with fake objects during testing. For example, if we don’t want to test the object storage connection, we can mock some function to save the file to a local folder instead of uploading it to the cloud.
In our case, if we want to create an end-to-end test but prevent the
load_reviews_to_object_storage
function from uploading the reviews to the cloud, we can do something like this:
import json
import pytest
# Mock function that will save reviews to disk instead of object storage.
def mock_load_reviews_to_object_storage(self, reviews):
with open(""mock_reviews_output.json"", ""w"") as file:
json.dump(reviews, file, indent=4)
print(""Saved reviews to mock_reviews_output.json"")
def test_run_etl(monkeypatch):
# Use monkeypatch to temporarily replace the actual function with the mock function
monkeypatch.setattr(ETLProcess, ""load_reviews_to_object_storage"", mock_load_reviews_to_object_storage)
etl = ETLProcess()
start_timestamp = ""2023-08-01T00:00:00Z""  # Example timestamp
etl.run_etl(start_timestamp)
# Add assertions or any other verification logic here
with open(""mock_reviews_output.json"", ""r"") as file:
reviews = json.load(file)
assert isinstance(reviews, list)  # example assertion
When you run this test using
pytest
, the
load_reviews_to_object_storage
function of the
ETLProcess
class will be replaced with
mock_load_reviews_to_object_storage
only during the test's execution. The actual function remains unaltered outside of this testing context.
Conclusion
We discussed why testing data pipelines is a crucial tool for the modern data engineer. Testing data pipelines can be challenging, but tools like
pytest
can make it much easier.
End-to-end testing with
pytest
allows us to simulate the entire pipeline, from ingestion to loading. Using fixtures, parameterized tests, and mocking, we can create a comprehensive and efficient testing suite to ensure our data pipelines' correctness.
It takes some time to build the tests and maintain them, but it’s worth the effort. Let’s confidently push those changes by having the right assets to ensure data quality.
Data Quality
Testing
Python
Data
Data Science
--
--
1
Follow
Written by
Bruno Gonzalez
220 Followers
·
72 Following
Technical writer at Data Quality Guru | Senior Data Engineer |
https://dataqualityguru.substack.com/
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Test Big Data Waters | Medium,"Test Big Data Waters | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Test the Big Data Waters
Krupesh Desai
·
Follow
4 min read
·
Jan 9, 2022
--
Listen
Share
Big data success stories from across industries have induced substantial interest in executives and business managers. However, managers or executives, intrigued by the possibility of big data, can find it troublesome to get started. Blown away by new jargon every quarter describing some new big data technology with over-the-top promises, leading a big data initiative is more often than not confusing for managers. If you are intrigued by the possibility of big data but not sure about where to start, you have come to the right place.
Venus Informatics offers consulting services to organisations seeking the right strategy to initiate big data projects. Our ‘Test the Big Data Waters’ service is a six-to-eight week pilot project with clear deliverables. The objective of this project is to evaluate the big data readiness of all or pre-specified business units and identify the readiest business unit to be explored with big data technologies with minimum capital cost and visible ROI. Leveraging the deliverables of this project business can build a contemporary data analytics platform which forms the synthesis between traditional and big data analytics and enables the organisation to incrementally augment it with new and disparate data sources for more in-depth analysis. Deliverables of this project are:
Identify the right target
Define the project ownership
Estimate the total cost of ownership
Identify the right target
Big data presents new possibilities for the organisation to derive ROI in three business areas; cost reductions, business process improvements and new products and services offerings. The organisation needs to select where they are going to apply big data and analytics within their business. It is vital decision with cascading effects on identifying the business unit that should lead the initiative and manage the project. Our consultants examine three factors in the identification of the right target where business can gain faster ROI on big data.
The first factor is the existing problems which can be fixed with big data. Instead of pushing new technology out of the box, fixing something slow or broken will get support from the executive managers. An organisation may be sitting on a goldmine of data unaware of its analytical value which could transform business strategies.The second factor is identifying such hidden structured or unstructured data sources which the business can apply in the decision making. The third factor is comparing existing massive data storage, and computational-heavy data transformations cost with the big data environment built on MPP (Massive Parallel Processing) architecture. Replacing the entire enterprise data warehouse with big data technologies is possible. But, the ideal incremental path would be to offload ETL process on cost-effective MPP architecture first. Augmenting existing data warehouse with MPP technology enables the organisation to expand big data capabilities gradually.
Define the project ownership
Existing analytics group or architectural groups within IT organisations are the most likely organisational structures to initiate or accommodate big data technologies. Ideally, a big data initiative should neither be an independent IT project nor an isolated effort from the analytical group. Once defined what the organisation wants from big data, the business unit benefiting most from the initiative should take over the big data project ownership.
Big data initiatives can cause change management issues with changes in roles, process design, and skills either during the data capturing stage or by the result of the analysis of new data sources. Therefore, the collaboration between business experts, system matter experts, process owners from the project owner business unit, IT team, and the BI team is essential for big data success. The deliverable of the pilot project defines a clear RACI (Responsible, Accountable, Consult and Inform) matrix with existing roles and new roles which may include new roles such as data scientist or big data engineer.
Estimate the total cost of ownership
The total cost of ownership is vital for new acquisitions. The cost of installing, deploying, using, upgrading, and maintaining the big data environment can vary significantly between self-managed big data environments and the Platform-as-a-Service (PaaS/Cloud) offerings. Organisations with rigid data security policies would prefer an in-house big data solution. However, if the chosen big data target requires external data or less sensitive sensor data, entire data processing workload can be configured on the cloud with embedding the final processed and cleaned data view in the existing analytical solution.
The cloud factor can drastically reduce capital expenditures, but the TCO analysis over an extended period is required to make the best decision. Another key factor is up-skilling existing staff for big data tools or hiring new data scientists/engineers. The cost of acquiring big data skills may outweigh the savings on the hardware and software. Therefore, the last and the most critical deliverable of this pilot project includes concise TCO analysis of next five years for all possible variants (including different MPP technologies) of the proposed big data solution including hardware, software, and necessary skills.
Contact US
Your primary interest in big data could be the cost reduction, improve decision-making, create new products and services or fix an existing business problem. Our ‘Test The Big Data Waters’ pilot study will provide you with a future-proofed strategy for technology architecture, implementation skills and management, and the ownership cost to develop the big data solutions to fit your current needs with the ability to extend it with more disparate data sources incrementally.
Big Data
Big Data Analytics
Data Management
Data Management Services
Data Management Solution
--
--
Follow
Written by
Krupesh Desai
48 Followers
·
43 Following
A Certified Data Management Professional - CDMP Associate , solving data-intensive problems, creating value, sharing the Data View House™ school of thoughts.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"How to build an end-to-end testing pipeline with dbt on Databricks | by Databricks SQL SME | DBSQL SME Engineering | Oct, 2024 | Medium","How to build an end-to-end testing pipeline with dbt on Databricks | by Databricks SQL SME | DBSQL SME Engineering | Oct, 2024 | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to build an end-to-end testing pipeline with dbt on Databricks
Databricks SQL SME
·
Follow
Published in
DBSQL SME Engineering
·
12 min read
·
Oct 15, 2024
--
3
Listen
Share
Data Pipelines with Embedded Data Quality & Testing
Author:
Tobi Sam
, Solutions Architect @ Databricks
Introduction
In previous articles, the DBSQL SME group has introduced how to perform basic performant ETL on DBT for all things Databricks (
here
and
here
). Now we dive into the next stage: data quality & pipeline testing. Data quality is essential in any analytics pipeline. This blog post outlines a robust approach to ensuring data integrity throughout your dbt workflow. We will explore a series of tests such as anomaly detection, unit tests, and data contracts that will help you maintain high-quality data from the source to the final output.
Databricks provides a unified platform for data processing and analytics that allows users to build, test, deploy, and monitor data products all in one place! We will leverage
dbt
(data build tool) — an open-source command-line tool that helps analysts and engineers transform data in their warehouse more effectively — to implement robust testing techniques to our data pipeline.
We will use the medallion architecture, a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze/raw ⇒ Silver/cleaned ⇒ Gold/serving layer tables).
Prerequisites & Configurations
dbt comes in two flavors dbt-core and dbt-cloud, and setting either one up is pretty straightforward. We will use dbt-core in this article. Please follow the installation instructions
here
and you should have a dbt project enabled.
Databricks
Ensure you have a
Databricks SQL warehouse
created and ready to be used
Setup OAuth or Personal T
oken
Your Local Machine
On your local machine, create a virtual environment and install the
dbt-databricks
adapter
Clone this repository as we will use it throughout this article
https://github.com/kamalendubiswas-db/dbt-dss-demo/tree/dev
Regarding the
profiles.yml file
, the best practice is to gitignore this file, as it relates to an individuals configuration and is created automatically when you set up dbt locally. This dbt YAML file lives in the .dbt/ directory of your user/home directory. Update your profiles.yml file from the repo to point to the Databricks SQL warehouse you created above.
Now test that you can connect to your Databricks SQL warehouse from your terminal with dbt debug. If all goes well, you should see this output “All checks passed!” If not, please troubleshoot using the error messages (I find them very helpful actually).
Configure your dbt Project
Sync your dbt metadata with Unity Catalog
To ensure that all your dbt table and column-level descriptions are synchronized with Unity Catalog, you can enable persist_docs in your dbt_projects.yml file. This really enriches the Unity Catalog view, adds rich lineage, and enhances the
GenAI capabilities
of the Databricks platform.
How to sync table metadata from DBT to Unity Catalog
Once you enable this, all your metadata is synchronized with Unity Catalog in Databricks
Syncing metadata from DBT automatically to UC
Optimise your Databricks Compute Resources
In your profiles.yml file, you can increase the value of the “threads” configuration to increase your project’s parallelism and fully take advantage of compute concurrency on the Databricks Lakehouse.
Threads help parallelize node execution in the
dbt’s (DAG)
. The default number of threads is currently 4, but there is no maximum, and dbt will allow you to go up to your Databricks SQL maximum limit. As a starting point, increase this number to 10 with a medium SQL warehouse but check out this
in-depth analysis
for more details on the best combination. To see how Databricks SQL manages multiple queries, click
here
.
Testing your models in dbt
Now that we have Databricks and dbt setup, we will break down our testing strategy into 3 phases (you can customise this based on your use case).
Validating the source data
Validating the ingested data
Validating the data transformations and output
Validate Source Data
Before diving into transformations, it’s crucial to verify the integrity of your source data. This step ensures that you’re working with fresh, accurate data from the start.
Freshness Checks
One of the first things to check is the freshness of your data. To check the freshness of your source data in your pipeline in dbt, you can use dbt’s freshness check block. A freshness block is defined within your models/sources.yml file and is used to define the acceptable amount of time between the most recent record in the source, and now, for a table to be considered “fresh”.
Example record of freshness
Let’s assume that it is currently 15:00 on September 15th, 2024, and we expect one new row every day. We can configure the freshness block to throw an error if the most recent row is older than 1 day and warn us if we haven’t received a new row within the last 12 hours.
Freshness check on DBSQL
When we run
dbt source freshness
on the table, our check will fail as expected.
Freshness checks are very helpful as they notify you when your data becomes stale so you can address them before proceeding further in your pipeline.
Validate Ingested Data
Once you’ve confirmed the source data’s integrity, the next step is to ensure that data is loaded correctly into your warehouse. Validation at this stage ensures a solid foundation and eliminates a stage in debugging any subsequent issues.
Row Count Comparison
One of the first things I like to do at this stage is to compare the rows of the source data to the bronze layer. Note that this is only valid when doing a full data load or if the source data is never deleted. You could write custom SQL to compare this but there are robust testing packages such as
dbt-expectations
that have a lot of pre-built tests ready to use. Using the
expect_table_row_count_to_equal_other_table
test, all we have to do is specify the two tables to compare:
DBT tests
When you run
dbt test
dbt will pass if both tables have equal row counts or it would fail otherwise. Easy!
To install the dbt-expectations package, include this in your packages.yml file and run the following:
dbt init
Installing expectations packages
Feel free to add other tests at this stage that may be relevant to your project.
Validate Data Transformations & Output
With clean source data and a successful load, it’s time to validate your transformations. This step ensures that your business logic is correctly applied and that the resulting data meets your quality standards.
One-off Data Tests
Sometimes, you may have those scenarios which require a test that is very specific and cannot be easily reused. In dbt, these are called
Singular data tests
. These are simply custom SQL queries that check for specific data quality issues or business rules in your models. However, if you find yourself reusing your singular tests in multiple models, then it may be time to “upgrade” them to
Generic data tests
which we will come to shortly.
Let us create a simple singular test that asserts that there are no future order dates in the order table, as that would be strange.
Data Quality Tests
Create this file and save it in the tests directory. Now run
dbt test —select assert_no_future_order_dates
to run this test.
I included 99 records in the future and thus, my test failed as expected:
DBT run with test fail
Reusable Data Tests
Now, when it comes to checking for nulls, uniqueness etc, these are tests you’d typically apply to several dbt models; and this brings us to Generic data tests. These are tests in dbt that take in certain parameters and can be reused across all models across your project. Think about them like a function in any programming language that expects a parameter and returns a result. This makes generic tests
DRY
and easily reusable.
dbt comes with 4 generic tests out of the box, but you can
build your own
custom generic tests. However, before you go build yours, check out these open-source packages (
dbt-utils
and dbt-expectations which we looked at already) to see if the test you have in mind hasn’t already been created.
In dbt, a good practice is to include test definitions directly in the schema.yml file alongside your model definitions, centralizing your data quality checks, making them easier to manage and maintain.
Below is an example of the 4 built-in generic tests being used for the orders model in a schema.yml file:
In plain English, these data tests translate to:
unique
: the order_id column in the orders model should be unique
not_null
: the order_id column in the orders model should not contain null values
accepted_values
: the status column in the orders table should be one of ‘placed’, ‘shipped’, ‘completed’, or ‘returned’
relationships
: each customer_id in the orders model exists as an id in the customers table (also known as
referential integrity
)
Enforcing Referential Integrity
An important test to point out here is the “
relationships
” generic data test. In Databricks, primary key and foreign key constraints are not enforced as they are
only informational
. If your application requires enforcing referential integrity, including this test in your pipeline will ensure that only datasets that meet this constraint are published into your schema.
Anomaly Detection in Time Series Data
dbt-expectations has another handy test, that I call “
Z-sigma
”. It’s a simple anomaly test based on the assumption that differences between periods in a given time series follow a log-normal distribution. This statistically measures a certain data point’s relationship to the mean of the entire dataset. Try it out, very easy to use.
Constraints & Data Contracts
dbt
constraints
apply additional validation on data as it is being inserted into a new or pre-existing table using
Databricks constraints
. When enforced, a constraint guarantees that you will never see invalid data in the table materialized by your model.
To enable constraints, you need to enable dbt
contracts
, which are configurations that guarantee the shape of a model while it is being built to avoid surprises or breaking changes for downstream queries.
Let’s take a look at an example of some constraints embedded within a model contract:
DBT contracts
You may notice that constraints are very similar to data tests, and in some cases, you can replace data tests with their equivalent constraint. Data tests validate the content of your model after it is built while constraints check this at build time. See
here
and
here
for more details.
At the moment with dbt-databricks, once you implement constraints, you get an error message; however, dbt only validates the data once it has already been inserted into the table, meaning that if the constraints and/or constraint_check fails, the table with the failing data will still exist in Unity Catalog/Databricks SQL. There is an open
issue
to update the implementation.
Unit Tests
Unit tests examine the smallest testable parts of your models. We implement unit tests to validate specific transformations or calculations in our models, especially when there is complex logic. These can be written as singular tests or using dbt’s recently released
unit test
framework.
Let’s consider a scenario where you’re calculating Customer Lifetime Value (CLV) in an e-commerce dataset. This calculation involves multiple steps and business rules, making it an ideal candidate for unit testing.
Testable Code Logic in SQL
To ensure the CLV calculation is correct, you can create unit tests where we pass our sample data (“given”) and then our expected output (“Expect”):
Inputs & Expected outputs in DBT for Unit Testing of pipeline logic
To run the test, run this command in your terminal:
dbt test - select test_type:unit
This unit test verifies that:
The annual value calculation is correct: (1000/4) * 12 =3000
The lifetime value calculation is correct: (3000 *3) =9000
Unit tests like this are invaluable when dealing with complex calculations, ensuring that your dbt models produce accurate and reliable results. Since the inputs of unit tests are static, it is recommended that they be run only in development or CI environments.
Additional Tips
To enhance your testing strategy, consider implementing these additional features:
Test Severity
Another feature that I particularly like is test severity. This allows you to configure your tests to return a warning instead of an error. Generally, if your test query returns at least one row, this will return an error.
I recently tackled a project involving an important model named “opportunity” with a crucial field called “customer_id.” Naturally, it was important for all opportunities to be linked to a customer_id. Unfortunately, the sales team overlooked the customer_id for 33 opportunity records. Despite this, I still wanted dbt to run while ensuring the integrity of this field.
To achieve this, I configured a test to “warn” me if there were any failing rows, and “fail” when the records exceeded 33, as this would suggest the identification of new records that were not initially taken into account. Below is a sample of what the configuration would look like:
Here is a simple breakdown:
Not_null
: A generic not_null test
Where
: in the test configuration, you can include a filter to apply the tests to. In this case, it was only important that “closed won” opportunities had a customer ID assigned to them.
Config
: with severity set to error (which is the default), dbt will check the error_if condition first. If the error condition is met, the test will return and error, if not, it checks the warn_if condition and “warns” if that condition is met, or else, the test passes if neither the “error” nor the “warn” conditions are met.
To run the test in your environment, simply run
dbt test
in your terminal.
We will see how you can add alerts using Databricks Workflows in the Monitoring and Alerting in the next part of this article.
Failing Fast
Sometimes, during development, if there is a failure during your build or test, you may want to exit dbt immediately instead of waiting for all the models to complete. This will help you save some time and money on your warehouse especially when you have a lot of models. dbt has a little-known flag called — fail-fast or -x which immediately exits dbt if any resource fails to build. You can find out more about it
here
.
Conclusion
We looked at many approaches to implementing data quality checks in your data pipeline in Databricks. Remember that data quality is not a one-time effort but an ongoing process. It is essential to regularly review and update your tests as your data models evolve. In summary, you should now be familiar with the following:
How to set up a basic dbt project for Databricks including a rule of thumb for compute sizing,
How to implement unit tests and custom one-off tests on dbt models,
How to validate data and perform freshness checks of sources,
How to implement constraints and data contracts to prevent models from being populated with invalid data,
The importance of testing at each stage of your data lifecycle strategy to isolate errors and identify if they are occurring at source, during ingestion or during transformation, and
The benefits of leveraging
persist_docs
to push metadata from your dbt models directly into Unity Catalog for data discovery.
In the next article, we will look at how to automate monitoring and alerting using Databricks Workflows and Databricks SQL in a CI/CD pipeline.
Databricks Sql
Databricks
Dbt
Data Quality
Data Mesh
--
--
3
Published in
DBSQL SME Engineering
622 Followers
·
Last published
Nov 22, 2024
Publication for content from the DBSQL SME group and the surrounding community on DBSQL best practices, new design patterns, creative solutions, and real world user stories all in one place.
Follow
Written by
Databricks SQL SME
2.8K Followers
·
39 Following
One stop shop for all technical how-tos, demos, and best practices for building on Databricks SQL
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Simple, Easy, and Efficient Data Quality with OpenMetadata | by Teddy Crépineau | OpenMetadata","Simple, Easy, and Efficient Data Quality with OpenMetadata | by Teddy Crépineau | OpenMetadata
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Simple, Easy, and Efficient Data Quality with OpenMetadata
Teddy Crépineau
·
Follow
Published in
OpenMetadata
·
7 min read
·
Oct 11, 2023
--
Listen
Share
Data quality and reliability are two top priorities for organizations. Many organizations have made considerable investments in Data Quality initiatives. With the current crop of tools, success has been limited due to tool disconnect, lack of collaboration, and accessibility to a broader audience.
We have a different take on how data quality should be done. Our approach has been to start with metadata. Gathering metadata based on a
metadata standard
allows us to contextualize data in a centralized place and to develop applications and automation on top of this metadata standard — one of which is our data quality solution.
In this blog, we’ll explore how
OpenMetadata
natively supports data quality that is key to many advantages — lowered cost, improved productivity, better
collaboration
, and many more. Let’s dive deep into some of these advantages to show why the approach taken by OpenMetadata is superior. We will use data validation tools like
Great Expectations
and
Soda
to contrast with
OpenMetadata
.
No need for yet another tool for data quality
With
OpenMetadata
, you don’t need a separate standalone system for data quality. The data quality validations are run as data workflows similar to workflows like ETL. This reduces the cost of buying yet another tool in the already fragmented data architecture and the operational costs of installing it, integrating it with various databases with various access control permissions, and managing another service.
Efficient creation and running of validation
The diagram below shows the steps for creating and running validation:
OpenMetadata
reuses the existing metadata, and data source connections are already set up. For other tools, such as
great expectations
or
Soda
a connection to the data sources must be set up, and a duplicate copy of metadata must be created even before writing tests. Creating a test is simple in
OpenMetadata
; it requires only two steps and a few clicks. You can have a suite of assertions running against your table in no time.
Writing validations for a table can be reduced from hours to minutes.
Rich data context makes writing validations easy
OpenMetadata
provides rich data profile visualization that helps you identify data quality issues quickly and write validations to detect them, as shown below:
If you see a column with many unexpected null values, you can quickly add a test to validate for null values. If you see a column like total orders having unexpected negative values, you can add a test to check for a valid range.
No code tests to democratize data quality
For
great expectations
and
Soda
, writing tests involves installing Python packages, writing tests in YAML or Python, and running them using schedulers. This makes these tools only accessible to technical users.
OpenMetadata
provides simple no code and intuitive UI to allow even business users to write tests easily to democratize data quality. Suppose an analyst or a data scientist is making assumptions about the data (non-null, a range of values, cardinality, etc.); they can contribute a test to ensure those assumptions are validated. This helps build a culture where data quality is not just a responsibility of some technical users but a shared responsibility of the organization.
With the current data quality tools, there is no central place to store the tests and collect the results. You will also need to deploy and maintain a separate user interface to visualize your test results separately per project. Oftentimes, different teams write the same validations in siloes resulting in needless duplication of tests and unnecessary compute costs. All this can be avoided with the central metadata repository for all the tests and results with OpenMetadata.
Faster debugging and resolution of data quality issues
Writing validation is just the first step. These tests often fail for various reasons. Currently, debugging data quality issues requires a user to jump from tool to tool to understand lineage, schema changes, code changes, etc., as the data quality tools lack the complete context. Fixing the issue requires identifying owners and going from team to team, which can take multiple days of firefighting.
Central metadata
provides one place to get the complete picture of your data instead of piecing it together by jumping across the tools. When a validation fails, all the concerned users are alerted about what is failing. The end-to-end lineage is provided to help in impact analysis to identify where the failure originated. All the changes to data and ETL pipelines are versioned, which helps identify how a change might have introduced the issue. Users can easily identify the owner of the data assets to identify who should fix the issue, start a conversation directly inside
OpenMetadata
, and collaborate on resolving the issue.
The complete context of what failed, where it failed, how it failed, and who is responsible for fixing it helps save countless hours in debugging and resolving data quality issues.
Communication is key when there are data quality issues. With
OpenMetadata
, you can also quickly implement a resolution workflow. Test failures will be marked as “New” in the entity data quality dashboard, informing users of ongoing failures. The status can be changed to “Ack” for acknowledgment to let users of the table know the failure is being investigated. Once resolved, users can edit the status to resolved and choose a failure reason that best explains the incident.
Organizational Visibility to Data Quality
In
OpenMetadata
, you will also find a data quality health dashboard, allowing you to understand the overall health of your quality assertions. This view is available at the data asset and the organization level. The dashboard shows how the organization is doing with data quality and where improvements are needed.
Extensibility for power users
The native tests OpenMetadata offers cover a wide range of data quality dimensions. It provides assertions covering
Accuracy
,
Completeness
,
Timeliness
,
Relevance
,
Uniqueness
, and
Validity
. It enables teams from across the organization to collaborate on data quality definition to test the business standard of data sets and the engineering ones.
OpenMetadata
goes one step further by making the data quality tool easily extensible. For users with SQL knowledge, our custom SQL test enables users to implement testing logic that is specific to their use case.
Users with Python knowledge can easily extend the
OpenMetadata
quality framework. You can create and define your own data quality assertions, install them inside
OpenMetadata
, and have your users configure them and run them directly from the OpenMetadata UI — more details here. With its extensive SDKs and APIs, it is easy to integrate
OpenMetadata
tests with your data processing pipeline. You can define your tests in
OpenMetadata
and execute them as a step of your ETL/ELT process — more details
here
.
While data quality programs have become strategic for organizations, few tools have approached it as such. Focused primarily on validation, requiring technical expertise, or treating metadata as a by-product, they have fallen short in solving the multiple challenges of data quality programs.
With its metadata standard first approach and easy-to-use and extensible data quality tool,
OpenMetadata
is the solution that can help organizations scale and foster innovation with reliable data at their fingertips.
Feature Comparison
Data Quality , How to Guides
Please refer to our
documentation about Data Quality , Profiler
to learn more about the features in
OpenMetadata
Road Ahead
We have many amazing features related to Data Quality in our roadmap:
Anomaly Detection
— Automated Anomaly detection of Freshness, Volume, Completeness, and Schema change detection.
Suggest Tests for Tables —
OpenMetadata will suggest tests based on Tables and Columns using Machine Learning.
Data Diff —
Show the differences between tables across different database services.
Data SLAs —
Data SLAs to provide guarantees around data assets.
Data Quality
Data Governance
Data Catalog
Data Engineering
Metadata
--
--
Follow
Published in
OpenMetadata
581 Followers
·
Last published
Oct 15, 2024
OpenMetadata is an open-source project that is driving Open Metadata standards for data. It unifies all the metadata in a single place in a Centralized Metadata store and helps people Discover, Collaborate, and Get their data right.
Follow
Follow
Written by
Teddy Crépineau
160 Followers
·
203 Following
Three o’clock is always too late or too early for anything you want to do — Jean-Paul Sartre.
http://www.teddycrepineau.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Ensuring Data Quality with Snowflake | by Vladimir Pasman | Medium,"Ensuring Data Quality with Snowflake | by Vladimir Pasman | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Ensuring Data Quality with Snowflake
Vladimir Pasman
·
Follow
7 min read
·
Oct 30, 2022
--
Listen
Share
Introduction
Why would we be collecting data if not to make business critical decisions? How accurate would our decisions be if our data were of low quality?
Actually, it depends. Sometimes, we can model from data that is not so good by cleaning up, making assumptions, etc. But generally, the lower the quality of data we deal with, the less accurate insights we are able to produce. So
to maintain proper data quality, we mist understand both our customers
(the tolerances)
and our data
(its inherent structure and quality).
In the application development world, we have our unit tests, integration tests, end-to-end tests, performance tests, and many other varieties.
Data world is a bit more complex and consists of the following:
Raw data sources maybe inaccurate, noisy, have issues with referential integrity, etc.
Transformations maybe pure SQL or code or both are more like our application code.
Data retrieval, when someone mistyped a column name or misinterpreted a data type.
Workflow complexity, where things seem to break as seemingly accurate data propagates through different steps in of a workflow.
Because of this, ensuring data quality is typically a bigger and a harder problem than with a typical application development lifecycle.
We are iteratively building a Data Quality Framework that is based on:
knowing your data
uninterrupted feedback loop based on monitoring and automation
improved tooling
applying
Data Ops
mindset and practices to learning and continuous improvement
Ensuring Initial Data Quality
How well do we know our data?
what meaning/insights does this data convey
how frequently is it updated
what do we consider noise and how it can be cleaned
which data features are we concerned the most
how does each dataset interact with the others
what the expected data types and values in each column are
is referential integrity properly maintained
what are the proper, uniquely identifiable primary keys
It would be impossible to make a decision of whether we could produce meaningful insights until we understand all of the above.
Here are some of the ways we used to to learn about our data and its behavior:
From Business. Documentation, talking to data owners, as well as reading original code.
Visually. Using metadata and representing it in SnowSight. For example, what you can see in the graphic below (apologies for scraping off proprietary detail), are the changes in the ingestion for unique primary keys across a set of tables, where the left side shows absolute changes and the right side showing a percentage change on the day. Here, the cyclical pattern of spikes on the right side lead to investigation of the update patterns for that table which ultimately lead to a discovery of a scheduled job that was running every Saturday. It happened to be an expected behavior, but this investigation resulted in a deeper understanding of our data sources.
DBT
documentation (upcoming)
A general workflow (below) we have implemented can be found in my
prior post
.
Figure 1.
Operational Data Store
What you will find here is an ELT pipeline, where we ingest raw data from various data sources using step functions as well as Change Data Capture events directly into Snowflake and then implement Transformations in Snowflake, thus turning Snowflake into both a Data Lake and a Data Warehouse.
There isn’t a better tool better suited to support data transformations in Snowflake than
DBT
, which is uniquely situated to support SDLC, improve data quality, and simplify incremental updates. Unfortunately, due to the volume of diverse datasets, variable quality of our initial data sources, the intrinsic complexity of data transformations, and time-to-market constraints, bringing a tool like
DBT
day one proved too difficult at the inception of the project. We learned that implementing
DBT
under time constraints works best when there is at least a minimal understanding of the behavior of underlying data and some organizational structure has already been established. Therefore
DBT
implementation has been initially postponed, and is being worked on as we speak.
We have taken the following steps to ensure initial data quality to date:
described and catalogued datasets in Confluence
identified the primary keys to help our transformations
created metadata tables at each step to enable visual insights
identified an initial set of notification thresholds to trigger
PagerDuty
partially automated and continuing to automate regression of the most important artifacts using
Gauge
engineers use seed data in dedicated staging environment to manually test every transformation
as we gained a much deeper understanding of our data, and were able to reduce the magnitude of the original raw data sources to just a few tables in our new
Star Schema
, we are bringing in
DBT
to improve on data recency, automated (unit/seed) testing, automated quality testing (nulls/uniqueness/referential integrity/etc.), engineer productivity, cataloging, documentation, and much more.
executing DevOps-like postmortems on all issues discovered
Testing Transformations
Initially, we chose not to bring in a workflow management tool such as
Apache Airflow
(
Amazon MWAA
). And we did it for a number of reasons:
Most of our data sources today require CDC and flow directly through the Snowflake Kafka Connector
For those few initial sources that would come in as batch, such as files in google drive, logs, etc, we thought that the maintenance overhead of the Airflow-based solution such as
Amazon MWAA
was too much both in terms of both cost and maintenance at project’s inception. We have decided to stick with
AWS Step Functions
, which may not have been perfectly suited for the problem at hand, but were easier to learn and implement by a team of engineers who did not have prior experience with Apache Airflow.
Our transformations today come in 2 flavors:
lambda-driven based on external events
scheduled task hierarchies in Snowflake
Testing transformation took a combination of a typical development testing workflow, such as unit/integration/functional with pytest/
Gauge
and a pure data quality testing techniques which use seeding and isolated development environments to test changes to SQL and stored procedures in isolation, with
DBT
coming soon to help.
Testing Microservices
Our data would have been useless if we kept it all to ourselves. We are currently exposing it through a BI Tool (Domo), SnowSight, REST (through AWS API Gateway and AWS Step Functions), Retool, and single-page web apps will be coming soon. As you maybe able to observe from my
prior post
, we have a number of different applications and micro-services already accessing curated data in a variety of ways and more will be coming soon.
When we deployed our first application, we were comparing results from an existing Django-based system with the results coming from Snowflake. Therefore, in addition to everything I described above, we also had to run a check against a vetted “golden-copy”. We have developed a
Jupyter Notebook
which creates an extensive
delta
report to help engineers verify
every mismatched record
. This notebook is currently being transformed into a full regression test written with
Gauge
, which could be applied to every data point, with the end results delivered to engineer’s mailbox for detailed analysis. I am pleased to say that today results coming from Snowflake have often been more accurate than the “golden-copy”.
Figure 2. an example of a Step Function used to generate a data point.
In order to test python-based step functions we are focusing on a typical development workflow, such as unit/integration/functional testing with pytest/
Gauge
. This approach also comes handy when testing our next-gen architecture which relies heavily on Apache Kafka (
Amazon MSK
) and event-driven micro-services.
Roadmap
As our batch volumes increase, so will the complexity of managing those data sources. At that junction, we will bring-in
Amazon MWAA
or an equivalent fully-managed Airflow distribution to both implement workflows and perform additional testing directly from Airflow DAGs as well as through our upcoming
DBT
pipelines.
We are dealing with a few roadblocks trying to bring data to Snowflake. Performance characteristics of our CDC data prior to it making it into Snowflake made it impossible to perform a meaningful scan of data prior to it flowing into kafka or immediately before it is loaded into Snowflake. One of the most important features that
Monte Carlo
could be able to provide to us today would be its use of Artificial Intelligence to identify patterns in our data, such as the update frequency, value ranges, etc. This ability to detect errors would have been invaluable if we could also apply it to Kafka. Today, we have to rely on Kafka-monitoring and other metadata to make sure that we did not lose anything during initial ingestion. It is somewhat imperfect and required a lot of work to reach a required level of confidence. As
Monte Carlo
is bringing in additional features, it will become more attractive to us. Therefore, we will be revisiting our initial decision in the coming months.
Snowflake is working on native-SQL machine learning capabilities, which we could use to improve our own error detection.
Although regressions have not yet been baked into our automated deployment pipeline which uses
Terraform
/
Github Actions
, that is also on the roadmap.
Conclusion
Our Snowflake journey unravelled at a speed of light, which meant we learning as we moved along and we will have to revisit a number of initial decisions (see Roadmap). Those decisions were made under constraints, which we hope to be incrementally lifting over the coming months.
Perhaps the most important learning of our journey was that The Data Quality Framework established above seems to be working and is constantly improving in an iterative, agile manner. It also seems to help improve team’s performance as the amount of manual, repetitive testing is being automated away and quality improves resulting in fewer bugs and investigations.
Data Quality
Data Quality Management
Snowflake
Testing
Dbt Labs
--
--
Follow
Written by
Vladimir Pasman
314 Followers
·
533 Following
Vlad is a Director of Engineering at Redshelf responsible for Data and Analytics.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Perform Your Data Quality Checks with A Single Line of Code | by Balu Rama Chandra | Medium,"Perform Your Data Quality Checks with A Single Line of Code | by Balu Rama Chandra | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Perform Your Data Quality Checks with A Single Line of Code
Introducing ydata-quality to Data Analyst & Data Scientist
Balu Rama Chandra
·
Follow
3 min read
·
Dec 21, 2022
--
2
Listen
Share
Image from Unsplash by Markus Winkler
Data quality management is an iterative process. It is almost impossible to solve your data quality issues right at the first time. As your product and business scales up new unknown bugs are expected to appear.
Data quality monitoring and data governance are the keys to solve this problem, but in my opinion implementing these systems would require extensive resources and time, which most data analysts and data scientists don’t have.
Data analysts and scientists are expected to provide prompt response to any stakeholders’ request, either ad-hoc or project based, with high accuracy of data and analysis. Hence, they can’t really wait for the Data Engineers to detect and solve all the data quality issues.
In this post, I would like to briefly share a Python library that can help you with data quality assessment. It can speed up your data analysis process and ensure you provide a high quality data or reports to your stakeholders. Moreover, it lets you perform data quality assessments with minimum lines of code, in fact it is only with a single line of code for running the quality checks. It is called
ydata-quality
.
ydata-quality
is a Python open-source library that allows you to inspect your data quality throughout multiple stages of your data pipeline development.
The Github repository has
detailed tutorials
and explanation on how to use the features with real datasets. You can get result with this library with only few lines of code. Here are the steps:
Load example dataset
Instantiate the Data Quality engine
Run the quality checks
Assess the warnings
The library has some off-the-shelf modules that you can directly use to measure the quality of your data. I am going to outline them below.
1.
Bias & Fairness
This module tries to give us an understanding of the inherent biases embedded in the datasets that can cause different treatments based on the sensitive attributes. This module has 4 tests: performance discrimination, proxy identification, sensitive attributes predictability and sensitive attributes representativity.
2.
Data Expectations
This module leverage Great Expectations, a Python library to perform data quality test based on the defined expectations of the data. This module has 3 tests:
overall assessment, coverage fraction and expectation level assessment.
3.
Data Relations
This module will explore the correlation and causality between variables in your dataset. This module also gives you information on the possible confounders, colliders, feature importance and inflated variance.
4.
Drift Analysis
This module is useful if you want to detect the differences in the data observed by your model during training and prediction time. It can help you detect covariate, label drift and concept drift.
5.
Duplicates
Data duplicates is every where. This module will help you to detect entity and column duplicates in your dataset. Entity duplicates exist when we have exactly the same rows after grouping by a given entity. Column duplicates are the any column that contains the exactly same feature values as another column in the same dataset.
6. Labelling (
Categorical
and
Numerical
)
This module allows you to inspect the quality of your variable/feature. For categorical variable this module has
missing labels, few labels,
o
ne vs rest performance, unbalanced Classes and outlier detection
test. For numerical variable, it has
missing labels, outlier detection and normality
test
.
7.
Missings
This module detects null/missing information in your dataset. It has
null count, correlation of missings, prediction of missingness and performance drop
feature.
Hope this short post gives you some valuable information, tools and options in your stack to assess your data quality. If you have any feedback, please let me know in the comment section.
Let’s get connected here
and thank you.
Python
Analytics
Data Analysis
Data Science
Data Quality
--
--
2
Follow
Written by
Balu Rama Chandra
474 Followers
·
20 Following
Analytics | Lifelong Learner | Subscribe here:
https://baluramachandra90.medium.com/subscribe
| LinkedIn:
https://www.linkedin.com/in/baluramachandra/
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How to use PyDeequ for Testing Data Quality at Scale | Medium,"How to use PyDeequ for Testing Data Quality at Scale | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
PyDeequ — Testing Data Quality at Scale
Akashdeep Gupta
·
Follow
11 min read
·
Dec 24, 2023
--
Share
This blog post will cover the different components of PyDeequ and how to use PyDeequ to test data quality in depth.
💡All the code present in this post is present on my GitHub
here
.
⚠️
Currently (Dec’23), PyDeequ
isn’t
compatible with Spark version > 3.3, but the community is working on it. All the details around it can be seen
here
.
What is PyDeequ?
PyDeequ is an open-source Python wrapper around Deequ (an open-source tool developed and used in Amazon). Deequ allows you to calculate data quality metrics on your dataset, define and verify data quality constraints, and be informed about changes in the data distribution. It’s built on top of Apache Spark so it can scale with large datasets (billions of rows).
Why testing Data Quality is important?
The quality of data within a system can make or break your application. Incorrect, missing, or malformed data can have a large impact on production systems. This bad data quality in the system can result in failures in Production, unexpected output from ML models, wrong business decisions, and much more.
Environment and Data Preparation:
--
--
Follow
Written by
Akashdeep Gupta
62 Followers
·
19 Following
I am a Professional Data Engineer with over 10 years of experience. I write Deep Dives on Data Engineering Tech. Newsletter @
https://www.guptaakashdeep.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Exam Diaries: What Happened in Today’s “Big Data” Test? | by Mudit Biala | Never Stop Writing | Oct, 2024 | Medium","Exam Diaries: What Happened in Today’s “Big Data” Test? | by Mudit Biala | Never Stop Writing | Oct, 2024 | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Exam Diaries: What Happened in Today’s “Big Data” Test?
Day 5 — Last Exam
Mudit Biala
·
Follow
Published in
Never Stop Writing
·
2 min read
·
Oct 25, 2024
--
3
Share
Image By Freepik
Hey everyone!
It’s day 5 of my exams, and this will be the last story in the “exam diaries” series — at least for now — because the exams are officially over!
Hurray!!!
I feel a little sad wrapping up this series for the time being.
It was something really different for me to start, and I hope it’s inspired some of you fellow writers to see that even in busy times, we can still find a way to write.
Now, about today’s Big Data exam —
it went surprisingly well! I hadn’t prepared much, but I still managed to answer almost every question… now, fingers crossed for how it all turns out.
Wait… I just got an idea.
I could start a new series —
My Parents’ Reactions to My Exam Marks
😂😂😂😂😂 It’d be hilarious. What do you think? 😂
Anyway, the exams are finally over, and it’s time to get back to business —
THE MEDIUM BUSINESS.
But with the festival season going on in India, I don’t think I’ll be able to give my 100% for the rest of the month.
This is the time of year when everything — the markets, the crowds, the lightings of houses— makes the atmosphere so lively and joyful.
--
--
3
Published in
Never Stop Writing
11.8K Followers
·
Last published
just now
A publication for a variety of topics. Feel free to join!
Follow
Written by
Mudit Biala
4.3K Followers
·
384 Following
An 18-year-old trying to make a life out of writing. You've known me till now as 'The Fitpreneur Network'
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Measuring data quality: bringing theory into practice | by Mikkel Dengsøe | Medium,"Measuring data quality: bringing theory into practice | by Mikkel Dengsøe | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Measuring data quality: bringing theory into practice
Practical steps for measuring and actioning data quality
Mikkel Dengsøe
·
Follow
8 min read
·
Jan 17, 2024
--
2
Listen
Share
dbt
surveyed
thousands of data practitioners and found the area where most companies are looking to increase investment is data quality–the most popular area for future investment.
There are good reasons for this. First, poor data quality leads to loss of trust in data and incorrect decision-making, putting the decision of having
1–5% of the company’s headcount in data roles
into question. Second, companies increasingly invest in business-critical systems such as ML or user-facing analytics directly powered by the data warehouse with no room for errors.
However, most companies don’t have a way of objectively assessing the quality of their data, which prevents them from identifying problematic areas or tracking improvements over time.
In this post, we look into
Considerations when measuring quality
The data asset-centric way
The data product-centric way
Combining quality insights with quantitative metrics
Putting insights into action
Which game are you playing?
If you’re like most people, you don’t want to measure data quality for the fun of it. Instead, you have a clear business need, e.g.,
Getting distributed teams to take ownership of data quality by highlighting problematic areas
Showing confidence to stakeholders after repeated questions about data quality being perceived as low
Showing regulators that you’re on top of data quality
Sharing system uptime with engineers on a data product where you’ve committed to an SLA
Considerations when measuring quality
We recommend setting guidelines around measuring data quality, so everyone approaches it coherently. While these can depend on your business circumstances, we’ve found these guiding principles to work well.
Metrics
— measure both coverage and uptime
Action
— actionable with clear next steps
Segment
— segmentable by key dimensions
Use case
— considered in the context of how the data is used
Trend
— consistent and measurable over time
In most scenarios, you want to specify expectations depending on how data is used. We recommend doing this as you may not want to hold all assets accountable to the same standards. Keeping in mind what data is business-critical, what’s being exposed (e.g., in a dashboard), and what assets should have an SLA (e.g., if data doesn’t arrive on time, there are downstream ramifications) helps you assess which checks you need and thus measure what matters.
Quality specifications and metadata per table
dbt yml files are a great place to define this metadata. This lets you use CI checks, such as
check-model tags
from the pre-commit dbt package, to ensure that each data model has the necessary metadata, such as a criticality or domain owner tag.
Measuring data quality–the data asset-centric way
Monitoring data quality on an asset level is intuitive: you calculate the uptime and coverage on a table level with the option to segment by metadata dimensions such as owner domain, criticality, and over time.
This gives you a bird’s eye view of your data stack, which areas you’re weak and strong in, and how that relates to your expectations.
We recommend that you consider two metrics to get the most complete picture.
Uptime
— what % of controls in place are passing successfully at each run
Coverage
— what % of data assets have the required controls in place
Grouping your quality metrics into semantically similar areas lets you talk about data quality in a way that’s closer aligned to the business use cases.
(1) Timeliness
–is data fresh and up to date per the SLAs agreed with the business,
(2) Correctness
–is all data available,
(3) Completeness
–is data semantically correct and
(4) Consistency
–is the available data consistent across systems. We also suggest clearly defining when something is “poor”, “ok”, or “good”. In our example, any score below 50% has been marked as “poor, ” meaning an action is required.
Data quality summary table–uptime & coverage across quality dimensions and over time
This level of insight lets you start asking questions about your data quality, such as
While we’ve got good coverage of null value checks, we should look into which ones are failing, if they can be fixed, or if we should remove them.
Why has the uptime of our assertion tests significantly declined?
Given the low coverage, does our high uptime on uniqueness tests give us a false sense of security?
Suppose you’ve specified metadata such as domain owner. In that case, you can segment the data to see that the degrading correctness score is largely driven by assertion tests for tables owned by the data warehouse team–guess you know who to go to now.
The team at Airbnb recently shared their
next chapter on measuring data quality
, which is also worth a read.
Measuring data quality–the data product-centric way
While the data asset-centric approach above has upsides–it’s easy to understand, implement, and reason about it has one main drawback.
Data asset-centric quality metrics are not intrinsically tied to the use cases of your data.
In other words, your timeliness score can be 95%. But if the data model containing your ML predictions for a critical ad-spend pipeline is failing right before the weekly run, you’re no better off, and your CTO will come to you angrily, asking you to get your data quality under control.
We propose an alternative way to measure your data quality–the data product-centric way.
Data products are a group of related assets, such as dbt models linked by an exposure, tables used by a CLTV model that powers marketing automation, or a selected collection of dashboards in a BI tool. In other words, data products are closely tied to their business use cases.
Data products are akin to the warning lights in a car dashboard. They don’t immediately tell you anything about the underlying metrics (e.g., is the oil level below 10%) but rather that there’s a problem with the product, its importance, and how you should investigate it.
Data products dashboard from
Synq
–business KPIs are in trouble.
In the example above, you can immediately see an issue with an asset upstream of the Business KPIs data product.
You can track the health of the data product over time, considering issues both on and upstream of the data product. This creates a more intuitive and end-user-centric approach to measuring data health. With this information at hand, you can rightfully tell your CFO that you’ve had reliability issues with your business KPI dashboard with a downtime of 6 days over the past 15 days.
Data product overview in
Synq
Once you’ve established that the reliability of a data product is problematic, you can go deeper into the different data controls to see the underlying root causes.
Combining quantitative metrics with qualitative insights
Despite having a high coverage of data quality checks and few errors, you may still have blindspots. You can address these by combining quality metrics with qualitative insights and metrics about the data user experience.
Survey your stakeholders
Combining quantitative metrics with surveying stakeholders can be a good way to uncover areas that need improvement. Ask them to give candid, anonymous feedback on questions such as
happiness about self-serve
,
the quality of their dashboards
, and
if there are enough data people in their area
.
Example data survey to be shared with stakeholders
Declare and monitor data incidents
Data teams are no strangers to incidents. But there’s often no standard way of acting on data issues, and many teams only sporadically declare data incidents.
Unlike automated data quality metrics, incidents are manually declared and categorized (e.g., P1, P2, P3). Keeping a log of historical incidents is a good way to supplement your data quality metrics and be transparent to your stakeholders.
An example of an incident declared in
Synq
See our
guide to incident management for data teams
for steps to implement this.
Monitor data usability
While not strictly a data quality measure, the usability of data assets is often overlooked. This can lead to bad end-user experiences, such as having multiple definitions of the same metric or a bloated stack with hundreds of unused data models and dashboards. To overcome this, we recommend you complement your quality measures with usability metrics such as
Engagement score
— number of users of a dashboard and number of downstream dependencies and queries on a data model
Usability score
— % of columns and tables with a description
These metrics help you with initiatives such as sunsetting unused data models or dashboards, an essential part of maintaining a healthy stack.
Putting insights into action
You’ll often want to put the insights you uncover from monitoring data quality into action. Whether it’s to improve a particular area, share with stakeholders how you’re improving, or something else.
While there’s no one-fit-all solution, we’ve seen these work well.
Automated accountability with a weekly email digest
— being the person having to slide into other teams’ Slack channels to tell them that their data quality is not great is not always fun (we’ve been there). Scheduling an automated weekly email with the quality score over time and per owner domain is a great way to bring accountability without one person having to point fingers.
It does wonders when people see their team scoring lower than their peers.
Be religious about including metadata
— the most common reason we see for data quality initiatives failing is that everybody owns data quality, and thus, nobody feels responsible. Only by enforcing metadata such as criticality and owner or domain can you hold people accountable for data quality in their area.
Beware of the broken windows theory
— the broken windows theory can be traced back to criminology and suggests that if you leave a window broken in a compound, everything else starts to fall apart. If residents start seeing that things are falling apart, they stop caring about other things. We can draw the same analogy to data quality.
If you’ve got many failing tests, it’s often a symptom that the signal-to-noise ratio is too low or that you don’t implement tests in the right places. Don’t let failing data checks sit around. Instead, set aside dedicated time, such as “fix-it Fridays” every other week, to work on these types of issues and remove data checks that are no longer needed.
Create run books for data quality
— if you’re in a larger team, include clear steps around addressing each data quality dimension so it’s clear for everyone. For example, if the
Timeliness
score is low, you can recommend steps such as adding a dbt source freshness check or an
automated freshness monitor
.
Actioning the data quality score doesn’t have to end here. We already see the best teams taking it further by embedding the quality score on key dashboards to indicate to stakeholders if the data is trustworthy and setting requirements that data assets used in business-critical processes should score “good” before you can expose them.
If you’ve any questions or ideas or are curious about monitoring data quality,
connect with me on LinkedIn
for a chat.
Data Engineering
Data Quality
Data Governance
Data
Analytics
--
--
2
Follow
Written by
Mikkel Dengsøe
4.2K Followers
·
21 Following
Co-founder, Synq (
www.synq.io
). Reach me here:
https://www.linkedin.com/in/mikkeldengsoe/
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"I spent 3 hours learning how Uber manages data quality. | by Vu Trinh | Nov, 2024 | Data Engineer Things","I spent 3 hours learning how Uber manages data quality. | by Vu Trinh | Nov, 2024 | Data Engineer Things
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Mastodon
I spent 3 hours learning how Uber manages data quality.
From the standard to the data quality platform
Vu Trinh
·
Follow
Published in
Data Engineer Things
·
7 min read
·
Nov 9, 2024
--
3
Listen
Share
Image created by the author.
This article was published at my newsletter 14 days ago. Subscribe for free at
https://vutr.substack.com
to get my writing delivered straight to your inbox sooner.
Intro
If you’ve been following my writing for a while, you might have noticed that I’ve spent a lot of time covering the technical aspects of OLAP systems, table formats, and how big companies manage their data infrastructure.
I’ve mainly focused on the “engineering” side and seemed to have overlooked the “data” — the most crucial part for anyone who works with it daily!
Thus, I’ve decided to dedicate more time to learning and writing about the “data” itself: data modeling, data management, data quality, and more.
This article will be the first you’ll read about “data” on my site.
(Hopefully, it won’t be the last!)
Today, you and I will dive into how Uber manages their data quality.
Overview
Uber extensively leverages data to provide efficient and reliable transportation worldwide, supported by hundreds of services, machine learning models, and thousands of datasets.
Being a data-driven company means that poor data can significantly impact operations — something Uber understands better than most.
Image created by the author.
To address this, they built a consolidated data quality platform that monitors, automatically detects, and manages data quality issues. This platform supports over 2,000 datasets, detecting around 90% of data quality incidents.
The following sections explore how Uber established data quality standards and built an integrated workflow to achieve operational excellence.
Challenges
To address data quality issues at Uber’s data scale, they had to overcome the following limitations:
Image created by the author.
Lack of standardized data quality measurements across teams.
Creating tests for datasets involved significant manual effort.
The incident management process required improvement.
Integration with other data platforms was necessary to provide a centralized experience for all of Uber’s internal data users.
A standardized, automated method for creating alerts was needed.
Data Quality Standardization
Here are some common data issues when Uber tried to collect feedback from internal data users and analyze significant data incidents in the past:
Data arriving late after
Data are missing or duplicated entries
Data discrepancies between different data centers
Data values are incorrect.
Following these insights, they define the below test categories, which expect to cover all data quality aspects:
Image created by the author.
Freshness:
the delay after which data is 99.9% complete
Assertion pass if current_timestamp —
latest_timestamp where data is 99.9%
complete
< freshness SLA
Completeness:
the row completeness percentage.
Assertion pass if downstream_row_count / upstream_row_count > completeness SLA
Duplicates:
the percentage of rows that have duplicate primary keys
Assertion pass if (1 — primary_key_count ) / total_row_count < duplicates SLA
Cross-datacenter Consistency:
the percentage of data loss by comparing a dataset copy in the current data center with the copy in the other data center.
Assertion pass if min(row_count, row_count_other_copy) / row_count > consistency SLA
Others:
any test with complicated checks based on business logic
Based on User-defined tests
Data Quality Platform Architecture
From a 10,000-foot view, Uber’s data quality architecture consists of the following components:
Image created by the author.
Test Execution Engine
Test Generator
Alert Generator
Incident Manager
Platform’s success metrics
Consumption Tools
The Test Execution Engine runs onboarded tests on schedules or on-demand using various query engines, with the results stored in databases. The other components leverage this engine to cover the entire data quality lifecycle.
Test Generator
Image created by the author.
This component is designed to automatically generate the standard tests defined in the “Data Quality Standardization” section. The tests are generated using the dataset’s metadata, fetched from Uber’s centralized metadata service. Key fields required during the test auto-generation process include the dataset’s SLAs, partition keys (for large tables that test only the latest data partition), and primary keys.
Uber also supports auto-generating tests for all upstream and downstream tables, leveraging internal lineage service. Uber created a daily Spark job to fetch the latest lineage to support this use case. The job also refreshes all auto-generated test definitions to reflect any metadata change and accordingly updates the logic of the test generation process.
Test Execution Engine
The engine is the Celery-based web service supporting approximately 100,000 daily executions of about 18,000 tests. Tests can be auto-generated (like mentioned above) or defined by users. Each test is characterized by an assertion that must be true for it to pass.
The tests can be reduced to a few logical assertions. The most basic is comparing a computed value with a constant number. Another common pattern is comparing one computed value with another computed value. Uber observed that most data quality tests are defined by one of these two simple assertion patterns.
Uber uses assertion patterns to construct the test expression, which is a string of symbols representing instructions that the execution engine can interpret.
The string is a flattened
Abstract Syntax Tree (AST)
that contains expressions and parameters to control the execution. At execution time, the expression is parsed into a tree and evaluated in a
post-order traversal
. In this approach, every test can be represented as an AST and processed by the execution engine.
Alert Generator
Alerts can also be auto-generated following templates and business rules. The process needs extra parameters which can be retrieved from the metadata service, such as table owners, or alert email. Uber will create alerts per dataset (table A or table B) per test category (freshness or completeness) based on results generated by the test execution engine. Moreover, Uber’s engineers also need to prevent false alerts and provides good user experience.
Uber introduced a sustained period indicating the table SLA allowing test failures. If the test has a sustained period of 3 hours, the platform will set its status as WARN until the test failures violate the sustained period.
Even for real alerts, the unnecessary number of alerts can overwhelm users. For example, when data arrive late, the freshness alert will trigger, and the Completeness and Cross-datacenter Consistency alerts are very likely to be triggered at the same time — three alerts for one issue.
Uber tries to limit the alert count in this case by default setting the Freshness alert as a dependency of other categories, so other alerts will be ignored to avoid duplicate notifications about the same root cause.
Incident Manager
Incident Manager workflow. Figure 5,
How Uber Achieves Operational Excellence in the Data Quality Experience
(2021)
After users had received an alert, investigated the root cause, and mitigated the quality issue, Uber added an internal scheduler to rerun failed tests with exponential backoff automatically. Thanks to this, users can validate whether the incident has been resolved successfully if the same test passes again and resolve the alert automatically without any user manual intervention.
Uber also developed a tool that allows users to annotate an incident and trigger a rerun manually. Users can report any incidents they discover while consuming data, and the data quality platform will check for overlap with any auto-detected incidents. Data producers are notified to acknowledge reported incidents. Uber aggregates both auto-detected and user-reported incidents to ensure that the final data quality status reflects all quality-related factors.
Consumption Tools
Uber also provides a variety of different tools to let users understand their datasets’ quality:
Image created by the author.
Databook
is the centralized dashboard that manages metadata for all Uber datasets. Uber integrated the quality platform with Databook to show data quality results in the UI.
Uber has a Query Runner tool that can access any data storage, such as MySQL, Postgres, or Hive. The data quality platform integrates with this tool to help users query quality status. The query API takes the dataset name and time range and verifies whether the query time range overlaps with any ongoing data incidents.
The ETL Manager serves as the controller for all Uber data pipelines. It can call the data quality platform to trigger new test executions immediately after a pipeline finishes, ensuring a quality check is performed. Additionally, before scheduling a data pipeline, the ETL Manager can consume data quality results for its input datasets. If the quality of any dataset fails to meet the SLA, the ETL Manager will not run the pipeline.
Uber has a metric platform that consolidates business metrics definitions and calculates and serves metrics using raw datasets. The data quality platform is closely integrated with the metric platform by defining specific standard tests for metrics and providing metric-level quality through the metric platform’s query layer.
Outro
Thank you for reading this far.
In this article, we’ve explored how Uber established data quality standards across internal teams and built a platform capable of efficiently testing data quality across Uber’s vast number of datasets.
See you in my next blog.
References
[1] Uber Engineering Blog,
How Uber Achieves Operational Excellence in the Data Quality Experience
(2021)
Data Quality
Data Engineering
Data Analytics
Data Management
Big Data
--
--
3
Follow
Published in
Data Engineer Things
14.4K Followers
·
Last published
7 hours ago
Things learned in our data engineering journey and ideas on data and engineering.
Follow
Follow
Written by
Vu Trinh
23K Followers
·
66 Following
🚀 My newsletter
vutr.substack.com
🚀 Subscribe for weekly writing, mainly about OLAP databases and other data engineering topics.
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Action-Position data quality assessment framework | by Yerachmiel Feltzman | Israeli Tech Radar | Medium,"Action-Position data quality assessment framework | by Yerachmiel Feltzman | Israeli Tech Radar | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Action-Position data quality assessment framework
Where do you place data quality validations? What are the actions you take when they fail?
Yerachmiel Feltzman
·
Follow
Published in
Israeli Tech Radar
·
8 min read
·
Feb 22, 2023
--
1
Share
“What is the business impact of an error on production for this pipeline?”, I asked our senior manager.
“Well” — he said — “it’s ugly”.
“So, we will be better served with a downtime than an error”, I concluded.
We were talking about a data pipeline triggering deletion on a production database, based on a TTL. Therefore, deleting the wrong items could cause a direct impact on client-facing features. The technical implementation of the pipeline itself was straightforward, but the business impact of an error was huge.
At the same time, we had a super complex streaming pipeline running a change-data-capture that powered several analytical workloads. Analysts could handle temporary errors by themselves, but freshness was a key KPI.
How should we approach data quality checks when designing those two pipelines?
I am sure you care about the quality of the outputs of your data pipelines.
You also care about the end user and do your best to ensure they can rely on the data your pipelines release downstream. It is also true that you have done that using one or a mix of tools to validate your job outputs. As a matter of fact, validations can be done both for inputs and outputs and you…
--
--
1
Published in
Israeli Tech Radar
637 Followers
·
Last published
just now
Unleashing tech insights by Tikal’s Experts. Explore the forefront of technology with Tikal, a leading hands-on tech consultancy. Get invaluable insights based on The Israeli Tech Radar, covering advancements, emerging technologies, and industry best practices.
Follow
Written by
Yerachmiel Feltzman
223 Followers
·
69 Following
Senior Big Data Engineer @ Tikal - Home of Tech Experts
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Transforming Data Quality: Automating SQL Testing for Faster, Smarter Analytics | by Akash Mukherjee | Oct, 2024 | Towards Data Science","Transforming Data Quality: Automating SQL Testing for Faster, Smarter Analytics | by Akash Mukherjee | Oct, 2024 | Towards Data Science
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Transforming Data Quality: Automating SQL Testing for Faster, Smarter Analytics
How to test the quality of SQL and resultant dataset against the business question to increase trust with customers
Akash Mukherjee
·
Follow
Published in
Towards Data Science
·
11 min read
·
Oct 26, 2024
--
2
Listen
Share
Photo by
Caspar Camille Rubin
on
Unsplash
When it comes to software development, there are plenty of automated testing tools and frameworks to rely on. But for analytics teams, manual testing and data quality assurance (QA) are still the norm. Too often, it’s the customer or business team who first spots issues with data quality or completeness, rather than the analytics team.
That’s where automation can make a huge difference. By setting up an automated system with scripts to run data quality tests at scale, you can keep things running fast without sacrificing the accuracy or completeness of your data.
Of course, this gets trickier when business questions are vague or open-ended. In those cases, a mix of rule-based logic and large language models (LLMs) can really help — allowing you to generate scenarios and run automated checks. In this tutorial, we’ll walk through how to build an automated testing system that evaluates and scores the quality of your data and SQL queries, even when the business questions are written in plain English.
What You’ll Need Before We Start
To follow along with this tutorial, make sure you have the following:
A solid understanding of databases and SQL
Experience with Python for API calls and handling data
Access to GPT-4 API tokens
A dataset of business questions for testing
Designing the System Architecture
To build an automated QA system for evaluating SQL queries, the architecture must integrate rule-based logic, LLM validation, and automated scoring. This setup is perfect for handling those open-ended business questions, letting you scale your testing beyond manual processes.
Key components include:
Query Ingestion Engine
: Where SQL queries are received and executed.
Evaluation Module
: Combines static rules with LLM-based to validate the results.
Scoring System
: Grades the results based on different user roles like Data Scientists, Business Leaders, and End Users.
The architecture includes a feedback loop that logs issue types–things like missing data, wrong granularity, or slow performance. This information get stored in a centralized database, so you can keep optimizing the system over time. We will use Python for scripting, SQL for tracking backend issues, and OpenAI’s LLM for interpreting natural language inputs. By scheduling these tests to run regularly, you’ll maintain consistent data quality and scalability, while also fine-tuning query performance to align with business goals.
The diagram below shows how data flows through the system — from SQL ingestion to automated testing, scoring, and issue tracking — so you can maintain high data quality at scale.
In the end, this system doesn’t just catch errors — it drives continuous improvement and keeps your technical execution aligned with business objectives.
Image by the author: Diagram created to illustrate technical architecture
Tutorial
Step 1: Prepare Dataset of Test Questions & Answers
To get started, collect real business questions that your internal teams or customers frequently ask the analytics team. Many of these might be ad-hoc data requests, so by having a variety of questions on hand you can make sure your testing is relevant. Here are a few examples to get you going:
Question #1
: “How many of our Pro Plan users are converting from a trial?”
Question #2
: “How many new users did we bring on in June 2024?”
Question #3
: “What products are trending right now?”
Question #4
: “What’s the current sales volume for our top products?”
Step 2: Building Your Evaluation & Scoring Criteria
2a: Define Your Graders
For thorough testing, set up graders from different perspectives to cover all bases:
End User
: Focuses on usability and clarity. Is the result easy to interpret? Does it address the original business question directly?
Data Scientist
: Evaluates technical accuracy and completeness. Are all the necessary datasets included? Is the analysis detailed and reproducible?
Business Leader
: Looks for alignment with strategic goals. Does the output support decision-making in line with business objectives?
2b: Define Scoring Criteria
Each grader should assess queries based on specific factors:
Accuracy
: Does the query provide the right answer? Are any data points missing or misinterpreted?
Relevance
: Does the output contain all the necessary data while excluding irrelevant information?
Logic
: Is the query well-structured? Are joins, filters, and aggregations applied correctly?
Efficiency
: Is the query optimized for performance without extra complexity or delays?
2c: Track and Log Issue Types
To cover all bases, it’s important to log common issues that arise during query execution. This makes it easier to tag and run automated evaluations later on.
Wrong Granularity
: Data is returned at an incorrect level of detail.
Excessive Columns
: The result includes unnecessary fields, creating clutter.
Missing Data
: Critical data is missing from the output.
Incorrect Values
: Calculations or values are wrong.
Performance Issues
: The query runs inefficiently, taking too long to execute.
import openai
import json
# Set your OpenAI API key here
openai.api_key = 'your-openai-api-key'
def evaluate_sql_query(question, query, results):
# Define the prompt with placeholders for question, query, and results
prompt = f""""""
As an external observer, evaluate the SQL query and results against the client's question. Provide an assessment from three perspectives:
1. End User
2. Data Scientist
3. Business Leader
For each role, provide:
1. **Overall Score** (0-10)
2. **Criteria Scores** (0-10):
- Accuracy: How well does it meet the question?
- Relevance: Is all needed data included, and is irrelevant data excluded?
- Logic: Does the query make sense?
- Efficiency: Is it concise and free of unnecessary complexity?
3. **Issue Tags** (2D array: ['tag', 'details']):
- Examples: Wrong Granularity, Excessive Columns, Missing Data, Incorrect Values, Wrong Filters, Performance Issues.
4. **Other Observations** (2D array: ['tag', 'details'])
Client Question:
{question}
SQL Query:
{query}
SQL Results:
{results}
Respond ONLY in this format:
```json
{{
""endUser"": {{""overallScore"": """", ""criteriaScores"": {{""accuracy"": """", ""relevance"": """", ""logic"": """", ""efficiency"": """"}}, ""issueTags"": [], ""otherObservations"": []}},
""dataScientist"": {{""overallScore"": """", ""criteriaScores"": {{""accuracy"": """", ""relevance"": """", ""logic"": """", ""efficiency"": """"}}, ""issueTags"": [], ""otherObservations"": []}},
""businessLeader"": {{""overallScore"": """", ""criteriaScores"": {{""accuracy"": """", ""relevance"": """", ""logic"": """", ""efficiency"": """"}}, ""issueTags"": [], ""otherObservations"": []}}
}}
```
""""""
# Call the OpenAI API with the prompt
response = openai.Completion.create(
engine=""gpt-4"",  # or whichever model you're using
prompt=prompt,
max_tokens=500,  # Adjust token size based on expected response length
temperature=0  # Set temperature to 0 for more deterministic results
)
# Parse and return the result
return json.loads(response['choices'][0]['text'])
# Example usage
question = ""How many Pro Plan users converted from trial?""
query = ""SELECT COUNT(*) FROM users WHERE plan = 'Pro' AND status = 'Converted' AND source = 'Trial';""
results = ""250""
evaluation = evaluate_sql_query(question, query, results)
print(json.dumps(evaluation, indent=4))
Step 3: Automate the Testing
3a: Loop Through the Questions
Once you’ve gathered your business questions, set up a loop to feed each question, its related SQL query, and the results into your evaluation function. This lets you automate the entire evaluation process, making sure that each query is scored consistently.
3b: Schedule Regular Runs
Automate the testing process by scheduling the script to run regularly — ideally after each data refresh or query update. This keeps the testing in sync with your data, catching any issues as soon as they arise.
3c: Log Scores, Tags, and Observations in a Database
For each test run, log all scores, issue tags, and observations in a structured database. Use the Python script to populate a table (e.g.,
issue_catalog
) with the relevant data. This gives you a history of evaluations to track trends, pinpoint frequent issues, and optimize future testing.
Step 4: Reporting Test Outcomes
4a: Pivot & Group by Scores
Leverage SQL queries or BI tools to create pivot tables that group your results by overall scores and specific criteria like accuracy, relevance, logic, and efficiency. This helps you spot trends in performance and figure out which areas need the most attention.
To calculate an overall score for each query across all graders, use a weighted formula:
Overall Score = w1​×Accuracy + w2​×Relevance + w3​×Logic + w4​×Efficiency
Where w1​, w2​, w3​, w4​ are the weights assigned to each scoring criterion. The sum of these weights should equal 1 for normalization.
For example, you might assign higher weight to
Accuracy
for Data Scientists and higher weight to
Relevance
for Business Leaders, depending on their priorities.
4b: Highlight Top Issues
Identify the most frequent and critical issues — things like
missing data
,
wrong granularity
, or
performance inefficiencies
. Provide a detailed report that breaks down how often these issues occur and which types of queries are most affected.
Focus on patterns that could lead to more significant errors if left unaddressed. For example, highlight cases where data quality issues might have skewed decision-making or slowed down business processes.
Prioritize the issues that need immediate action, such as those affecting query performance or accuracy in key datasets, and outline clear next steps to resolve them.
Image by the author: Chart Created using Sample Test Data
4c: Analyze Variance of Graders
Look closely at any discrepancies between scores from different graders (End User, Data Scientist, Business Leader). Large differences can reveal potential misalignments between the technical execution and business objectives.
For example, if a query scores high in technical accuracy but low in relevance to the business question, this signals a gap in translating data insights into actionable outcomes. Similarly, if the End User finds the results hard to interpret, but the Data Scientist finds them technically sound, it may point to communication or presentation issues.
By tracking these differences, you can better align the analytics process with both technical precision and business value, keeping all stakeholders satisfied.
To quantify this variance, you can calculate the variance of the graders’ scores. First, define the individual scores as:
S-EndUser​:
The overall score from the End User.
S-DataScientist​:
The overall score from the Data Scientist.
S-BusinessLeader
​: The overall score from the Business Leader.
The mean score
μ
across the three graders can be calculated as:
μ = (S-EndUser​ + S-DataScientist​ + S-BusinessLeader​​) / 3
Next, calculate the variance
σ²
, which is the average of the squared differences between each grader’s score and the mean score. The formula for variance is:
σ
²
= (S-EndUser − μ)
²
+ (S-DataScientist − μ)
² +
(S-BusinessLeader − μ)
²
/ 3
By calculating this variance, you can objectively measure how much the graders’ scores differ.
Large variances suggest that one or more graders perceive the quality of the query or relevance differently, which may indicate a need for better alignment between technical output and business needs.
Step 5: Create a Feedback Loop
5a: Pinpoint Key Issues
Throughout your testing process, you’ll likely notice certain issues cropping up repeatedly. These might include
missing data
,
incorrect values
,
wrong granularity
, or
performance inefficiencies
. It’s important to not only log these issues but also categorize and prioritize them.
For example, if critical data is missing, that should be addressed immediately, while performance tweaks can be considered as longer-term optimizations. By focusing on the most impactful and recurring problems, you’ll be able to improve data quality and tackle the root causes more effectively.
5b: Refine Your SQL Queries
Now that you’ve identified the recurring issues, it’s time to update your SQL queries to resolve them. This involves refining query logic to achieve accurate joins, filters, and aggregations. For example:
If you encounter
wrong granularity
, adjust the query to aggregate data at the appropriate level.
For
missing data
, make sure all relevant tables are joined correctly.
If there are
performance problems
, simplify the query, add indexes, or use more efficient SQL functions.
The goal here is to translate the feedback you’ve logged into tangible improvements in your SQL code, making your future queries more precise, relevant, and efficient.
5c: Re-Test for Validation
Once your queries have been optimized, re-run the tests to verify the improvements. Automating this step ensures that your updated queries are consistently evaluated against new data or business questions. Running the tests again allows you to confirm that your changes have fixed the issues and improved overall data quality. It also helps confirm that your SQL queries are fully aligned with business needs, which can enable quicker and more accurate insights. If any new issues arise, simply feed them back into the loop for continuous improvement.
Example Code for Automating the Feedback Loop
To automate this feedback loop, here is a Python script that processes multiple test cases (including business questions, SQL queries, and results), evaluates them using OpenAI’s API, and stores the results in a database:
for question, query, results in test_cases:
# Call the OpenAI API to evaluate the SQL query and results
response = openai.Completion.create(
engine=""text-davinci-003"",  # Replace with GPT-4 or relevant engine
prompt=prompt.format(question=question, query=query, results=results),
max_tokens=1000
)
# Process and store the response
process_response(response)
def store_results_in_db(test_run_id, question, role, scores, issue_tags, observations):
# SQL insert query to store evaluation results in the issue catalog
insert_query = """"""
INSERT INTO issue_catalog
(test_run_id, question, role, overall_score, accuracy_score, relevance_score, logic_score, efficiency_score, issue_tags, other_observations)
VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
""""""
db_cursor.execute(insert_query, (
test_run_id, question, role, scores['overall'], scores['accuracy'], scores['relevance'],
scores['logic'], scores['efficiency'], json.dumps(issue_tags), json.dumps(observations)
))
db_conn.commit()
Setting Up the Issue Catalog Table
The
issue_catalog
table serves as the main repository for storing detailed test results, giving you a clear way to track query performance and flag issues over time. By using JSONB format for storing issue tags and observations, you gain flexibility, allowing you to log complex information without needing to update the database schema frequently. Here’s the SQL code for setting it up:
CREATE TABLE issue_catalog (
id SERIAL PRIMARY KEY,
test_run_id INT NOT NULL,
question TEXT NOT NULL,
role TEXT NOT NULL,  -- e.g., endUser, dataScientist, businessLeader
overall_score INT NOT NULL,
accuracy_score INT NOT NULL,
relevance_score INT NOT NULL,
logic_score INT NOT NULL,
efficiency_score INT NOT NULL,
issue_tags JSONB,  -- Storing issue tags as JSON for flexibility
other_observations JSONB,  -- Storing other observations as JSON
created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
What This Feedback Loop Accomplishes
Continuous Improvement
: By keeping track of issues over time, you’ll be able to fine-tune your SQL queries and steadily boost their quality. Each test run delivers actionable insights, and by targeting the most frequent problems, your system becomes more efficient and resilient with every pass.
Data Quality Assurance
: Running tests regularly on updated SQL queries helps you verify that they handle new data and test cases correctly. This ongoing process shows whether your adjustments are truly improving data quality and keeping everything aligned with business needs, lowering the risk of future issues.
Alignment with Business Needs
: Sorting issues based on who raised them — whether it’s an End User, Data Scientist, or Business Leader — lets you zero in on improvements that matter to both technical accuracy and business relevance. Over time, this builds a system where technical efforts directly support meaningful business insights.
Scalable Testing and Optimization
: This approach scales smoothly as you add more test cases. As your issue catalog expands, patterns emerge, making it easier to fine-tune queries that affect a wide range of business questions. With each iteration, your testing framework gets stronger, driving continuous improvements in data quality at scale.
Summary
Automating SQL testing is a game-changer for analytics teams, helping them catch data issues early and resolve them with precision. By setting up a structured feedback loop that combines rule-based logic with LLMs, you can scale testing to handle even the most complex business questions.
This approach not only sharpens data accuracy but also keeps your insights aligned with business goals. The future of analytics depends on this balance between automation and insight — are you ready to make that leap?
Sql
Automation
Testing
AI
QA
--
--
2
Follow
Published in
Towards Data Science
774K Followers
·
Last published
3 hours ago
Your home for data science and AI. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.
Follow
Follow
Written by
Akash Mukherjee
473 Followers
·
214 Following
Follow for spicy takes on product, market and tech
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
A Beginner’s Guide to Big Data Testing | by Nadeesha Gamhewage | Engineering at 99x,"A Beginner’s Guide to Big Data Testing | by Nadeesha Gamhewage | Engineering at 99x
Open in app
Sign up
Sign in
Write
Sign up
Sign in
A Beginner’s Guide to Big Data Testing
Process of validating creation, storage, retrieval, and analysis of massive volumes data
Nadeesha Gamhewage
·
Follow
Published in
Engineering at 99x
·
7 min read
·
Aug 3, 2021
--
1
Listen
Share
Photo by
UX Indonesia
on
Unsplash
What is Big Data?
It is a collection of large amounts of data where the volume is bound to increase with time. Data can be categorized as:
Structured
: File type is known because the format is fixed
Unstructured
: File format is unknown because it can be in the form of images, videos or flat files etc.
Semi-structured
: It can be a combination of both structured or unstructured data
What is Big Data Testing
It is a process of validating functionalities such as creation, storage, retrieval, and analysis of massive volumes of data like images, flat files and audio in a Big Data application. As a result, traditional data testing techniques or strategies may not be able to cater to such a requirement, hence the need of a separate strategy for Big Data Testing
This would result in the testing approach requiring R&D effort due to specialized skills being needed to establish a Sampling strategy which is a challenge for a big data system.
A
data set
is a grouping of related, discrete pieces of data that can be accessed separately or in combination. A data set consists of a large volume of data that can be hard to analyze.
Sampling resolves this challenge by using a subset of data from the data set.
Big Data testing ensures:
Data Security and Authenticity
Testing done at different levels and intervals to overcome any breach of security or vulnerabilities in the system. This would ensure that you maintain the security and confidentiality of data and also enable you to build trust with your clients
Data Accuracy
Dealing with unstructured data would help provide better insights of useful data. It enables better decision making, improves risk analysis and helps build better digital marketing strategies.
Validating Real-time Data
Performance testing should be done on real-time data. We should ensure the validity of data used for this purpose
Reliability and Effectiveness
In big data systems, data is collected and fetched from different sources and channels which may result in the data obtained being ineffective, inaccurate and unreliable. Testing helps ensure that data from top to bottom includes verification of data layers, components and logics.
Big Data testing challenges
The difference between traditional testing and Big Data testing
Factors to be taken into account when building a Big Data testing strategy
Big data testing focuses more on verification of application’s data processing instead of testing the distinct features. As well as testers should consider quality factors like data accuracy, duplications, validity and consistency etc.
Before defining a suitable test strategy, the test team should have a clear understanding of the following:
What kind of a test plan is needed for big data application testing
What kinds of testing approaches that would need to be considered applicable for the big data application
What are the requirements needed for environmental setup?
What approaches that would need to be applied to verify and validate data in the big data application
What tools would be needed for testing
There are three key aspects to be considered in Big data testing that can be summarized as follows:
Types of testing to be considered when building a Big Data testing strategy
How to perform some of the main testing types
Database testing
The data should be loaded to a selected framework —
We can use an open-source software framework like Hadoop. It is used to store data on commodity servers and run the big data application on clusters
Testers should verify correct data is imported and data completely imported to the framework
Testers should validate that data processing occurs correctly according to the business requirements in the selected framework
Then define a subset of data from the data and validate same as above
Then need to compare the result of that, with the processed data from the framework. Here testers need to do scripting for testing in order to extract data and process data
Then, in warehouse those data should be stored
Stored data should be validated once again using CRUD operations. This would ensure the processed data is displayed correctly in the warehouse before and after data loading
Then data should be visualized using business intelligence tools (e.g. Oracle, SAP etc.) for analysis purposes —
Here web services are used to transfer data from Warehouse to business intelligence. So, testers should test the services as well.
Functional Testing
Similar to functional testing performed on other applications:
Frontend (e.g. web pages) testing should be done according to the user requirements.
The results displayed in the frontend should be validated with the expected result.
Performance Testing
The steps outlined below should help you in executing a performance test:
Set up the big data cluster
Identify and plan the jobs need to be run on workloads
Create individual scripts/clients. When creating scrips below parameters can be used
Data Storage
: How data is stored in different nodes of the application
Commit Logs
: How large the commit log is allowed to grow
Concurrency
: How many threads can perform write and read operation
Caching :
Tune the cache setting “row cache” and “key cache.”
Timeout
: Values for connection timeout, query timeout, etc.
4. Execute the performance test and analyze the results. If the results do not meet the performance requirements, then components should be optimized and the tests should be executed again
5. If the performance is stable, then the process is finished
Designing Test Scenarios and Test Cases
Testers need some other documents along with the requirement document to design test scenarios or test cases for a big data application. Here are some of those documents to name a few :
Input document —
it contains the DB structure and its relationships of the tables
Mapping Document —
It contains the mapping data types of source to target tables after extracting the data to warehouse
Design Document —
it contains what are all the input they will take and what kind of output they will produce, etc.
Note:
These things are not mandatory because this may vary due to company standards and complexity or the nature of the project.
Big Data Testing Tools
Here are a few tools you could take into consideration when building your big data testing strategy:
You can refer to the below resources for further details regarding the above mentioned tools
https://www.upgrad.com/blog/big-data-tools/
https://www.testing-whiz.com/big-data-testing#capabilities-tab1
https://towardsdatascience.com/8-open-source-big-data-tools-to-use-in-2018-e35cab47ca1d
Big data testing best practices
Testing should be based on the business logics
Bug fixing should be prioritized
Always keep in touch with the context
Because of massive volume of data always try to automate to save time
The objectives of the testing should be clear
The communication between the team and client is important
Ensure you have good technical skills as you would be working with sampling of data
The tester must be capable of working with both unstructured and semi-structured material. They should be able to work with structured data in the data warehouse or source RDBMS as well.
A software tester must be able to deal with a constantly changing schema.
In order to come up with methodologies and utilities that give enough test coverage while retaining high test productivity, testers will have to be creative.
Conclusion
Big Data testing is a procedure of validating the functional and non-functional testing with the huge amount of structured or unstructured data. So, we have to use special tools and testing techniques to validate the end to end business logics.
For processing of data , a data sampling strategy could be considered
Manipulating an environment for the testing is very critical. The basic requirements to conduct testing would be as follows:
Need space for data storing, processing and validating
The responsiveness of clusters and its corresponding nodes
Having powerful data processing resources
Specified tools should be identified before we initialize the test strategy as many tools are available for different processes
It is always better to identify any challenges associated with automation, performance testing, Data visualization etc. upfront and devise plans to overcome the same during testing
Big Data Testing
Big Data
Data Security
Testing
--
--
1
Follow
Published in
Engineering at 99x
402 Followers
·
Last published
Sep 6, 2024
Headquartered in Sri Lanka, 99x is a technology company co-creating well-engineered, innovative digital products for the Scandinavian market. ​
Follow
Follow
Written by
Nadeesha Gamhewage
10 Followers
·
15 Following
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Perform Data Quality test on your Data Pipelines with Great Expectations! | by Haq Nawaz | Python in Plain English,"Perform Data Quality test on your Data Pipelines with Great Expectations! | by Haq Nawaz | Python in Plain English
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Perform Data Quality test on your Data Pipelines with Great Expectations!
Haq Nawaz
·
Follow
Published in
Python in Plain English
·
6 min read
·
Apr 20, 2023
--
3
Listen
Share
Using Python, Pandas & Great Expectations
Great Expectations
With Great Expectations you can expect more from your data. Great Expectations is one of the leading tools for
validating
,
documenting
, and
profiling
your data to maintain quality. Data Quality is key to making informed decisions. Inaccurate data erodes users’ trust and adaptability of the data platform. Therefore, it is essential to have Data Quality tests in place when developing a data pipeline.
Today we are going to cover the Great Expectations library for Data Quality testing.
Previously
we have used PyTest to carry out data quality tests. With PyTest we wrote our own functions to perform testing. The Great Expectation library has built-in functions to carry out the data quality tests.
If you are a visual learner then I have an accompanying video on
YouTube
with a walk-through of the complete code.
This tutorial will:
Familiarize you with various built-in Great Expectations functions
How to apply these functions to our dataset
Save the the Data Quality Tests to config file and re-use them
The only prerequisite for this tutorial is Great Expectations, make sure it is installed on your system.
pip install great_expectations
# check the install
great_expectations --version
We will use Jupyter Notebook as an IDE. Let’s open our Jupyter Notebooks. As usual the completed notebook is available on
GitHub
. In the Notebook import our libraries; pandas and great expectations. Using Pandas we read the product dataset into a pandas DataFrame.
import great_expectations as ge
import pandas as pd
#
url = ""https://github.com/hnawaz007/pythondataanalysis/blob/main/ETL%20Pipeline/Pytest/Session%20one/Product.xlsx?raw=true""
# read from url
df=pd.read_excel(url)
df.head()
We preview the data in the DataFrame to familiarize ourselves with columns and their data type. In order to run Great Expectations tests on this dataset we need to convert it to Great Expectations DataFrame. We can convert it with the from_pandas
function from the great library. We can check the type to make sure it is the correct type. Otherwise, we won’t be able to run the following tests on it.
my_df = ge.from_pandas(df)
# check the type
type(my_df)
All Great Expectations tests start with expect keywork. Great Expectations offers various built-in test. You can read about them on their
glossary page
. Let’s start with row count in our DataFrame. We can check how many records we have in our DataFrame. Our table row count expectation fails. We can see that the success is false and the actual number of records in this table are six hundred and six. We expected thousand rows in this table.
# check number of rows in the dataset
my_df.expect_table_row_count_to_equal(1000)
{
""exception_info"": {
""raised_exception"": false,
""exception_traceback"": null,
""exception_message"": null
},
""success"": false,
""result"": {
""observed_value"": 606
},
""meta"": {}
}
We will explore some of these tests that make sense for our dataset. If you recall in PyTest we wrote our own functions to test the assumptions about our data. However, Great Expectations provide us with built-in functions. We apply them to our dataset to test the data quality. Let’s start with the primary key column. First of all, we check if the column exists in the dataset with the expected column to exist and provide it the column name, ProductKey. This displays the result of the test and whether it succeeds or fails.
# check if column exists
my_df.expect_column_to_exist('ProductKey')
{
""exception_info"": {
""raised_exception"": false,
""exception_traceback"": null,
""exception_message"": null
},
""success"": true,
""result"": {},
""meta"": {}
}
Next, we find out if our primary key is unique. This returns a little more information about the test. We get the status which is a success. It gives us the total record count and whether we have missing values and what percentage is missing. On both accounts it is zero. Which is a good sign. Our primary key columns look in good shape. We can perform a null test to check if it contains any nulls with our next test. This is a success as well. Our source system is producing some good data. Let’s wrap it up with a data type test. We know this column is of type integer so we put this assumption to test.
# check for uniqueness
my_df.expect_column_values_to_be_unique('ProductKey')
# check for nulls
my_df.expect_column_values_to_not_be_null('ProductKey')
# data type check
my_df.expect_column_values_to_be_in_type_list(""ProductKey"", [""int"", ""int64""])
Let’s move on to other columns in our dataset. We check if certain columns contain values in a set. For example, we have a product line column that groups the values into four categories. We can perform this test on columns with few distinct values such as region or product category that contains 4–10 distinct values. To
expect_column_values_to_be_in_set
function we provide column name and the list of expected values. We are checking if the Product Line only contains these four values. This assumption is correct. However, we can see that a lot of values are missing in this column. 226 to be precise or 37 percent.
my_df.expect_column_values_to_be_in_set(""ProductLine"", ['R ', 'S ', 'M ', 'T '])
{
""exception_info"": {
""raised_exception"": false,
""exception_traceback"": null,
""exception_message"": null
},
""success"": true,
""result"": {
""element_count"": 606,
""missing_count"": 226,
""missing_percent"": 37.29372937293729,
""unexpected_count"": 0,
""unexpected_percent"": 0.0,
""unexpected_percent_total"": 0.0,
""unexpected_percent_nonmissing"": 0.0,
""partial_unexpected_list"": []
},
""meta"": {}
}
Next, we will test if the column values are between a range. We can perform this test on a numeric column to check if they fall in a certain range. For example, the Safety stock level can be between one and thousand. We can apply the same test on the days to manufacture columns. We expected it to be between one and 10. This assumption is true. So, we are producing products in a timely manner.
# check values in range
my_df.expect_column_max_to_be_between(""SafetyStockLevel"", 1, 1000)
# check days to manufacture range
my_df.expect_column_max_to_be_between(""DaysToManufacture"", 1, 10)
We can also check the average of a column to be between a range. For example, we are testing if Standard Costs mean is between 100 to 500 and this test passes so our assumption is true. The actual mean is 434.
my_df.expect_column_mean_to_be_between(""StandardCost"", 100, 500)
{
""exception_info"": {
""raised_exception"": false,
""exception_traceback"": null,
""exception_message"": null
},
""success"": true,
""result"": {
""observed_value"": 434.26582886075965,
""element_count"": 606,
""missing_count"": 211,
""missing_percent"": 34.81848184818482
},
""meta"": {}
}
If some of our columns are sparsely populated but we still want to check if a certain percentage is expected to be populated then we can use the mostly parameter. In this case we are checking if the Color column is populated with values 55 percent. This returns true as 41 percent values are missing from this column.
my_df.expect_column_values_to_not_be_null(""Color"", mostly=0.55)
One of the good features of Great Expectations is that we can save or export all the tests we ran and use them later on. This comes in handy as we run our data pipelines daily and, on each run, we perform these tests using a config file.
# export the assertions to a config file
my_df.save_expectation_suite('product.data.expectations.json')
Let’s run the tests with our config. We load the config file and set this equal to
expectation_suite
parameter inside the validate function. We save the results into a variable. Now we can check the results of all the tests. Using this method, we can inject the Great Expectations test cases into our data pipeline.
# read the dataset into ge DataFrame
df2 = ge.read_excel(url)
# read the assertions from config file
test_results = df2.validate(expectation_suite=""product.data.expectations.json"")
# Take action based on the results
if test_results[""success""]:
print (""Awesome. All Data Quality Tests are green!"")
else:
raise Exception(""You've got issues."")
Awesome. All Data Quality Tests are green!
All tests pass successfully, so our data is matching our expectation and we can be confident that we have set certain data quality standards and our data is matching them. We can further build on this to include more tests to catch any data quality issues.
Conclusion
We showcased how easy it is to use Great Expectations test cases.
We implemented test cases and established data quality standards and prevented inconsistent data flowing into the ETL pipeline.
The complete Great Expectations tutorial code can be found
here
More content at
PlainEnglish.io
. Sign up for our
free weekly newsletter
. Join our
Discord
community and follow us on
Twitter
,
LinkedIn
and
YouTube
.
Learn how to build awareness and adoption for your startup with
Circuit
.
Data
Data Quality
Great Expectations
Etl
Python
--
--
3
Follow
Published in
Python in Plain English
33K Followers
·
Last published
15 hours ago
New Python content every day. Follow to join our 3.5M+ monthly readers.
Follow
Follow
Written by
Haq Nawaz
2.2K Followers
·
5 Following
I am a business intelligence developer and data science enthusiast. In my free time I like to travel and code, and I enjoy landscape photography.
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data quality management in the age of AI | by Barr Moses | Medium,"Data quality management in the age of AI | by Barr Moses | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data quality management in the age of AI
Barr Moses
·
Follow
3 min read
·
Oct 9, 2024
--
2
Listen
Share
Over the last 12 months, data quality has become THE problem to solve for enterprise data teams — and unsurprisingly, AI is driving the charge.
As more enterprise teams look to AI as their strategic differentiator, the risks associated with bad data become exponentially greater. At the speed and scale of modern data environments, data teams need advanced data quality methods that can rise to meet these challenges.
In this week’s edition, I’ll consider three of the most common tactics for managing data quality — monitoring, testing, and observability — and discuss how each can (and will) work themselves out in the age of AI.
Defining our terms — data quality monitoring, data testing, and data observability.
Before we can understand the future of data quality, we need to understand the present. In its simplest terms, you can think of data quality as the problem; testing and monitoring as methods to detect problems; and data observability as a comprehensive approach that combines and extends both methods to actually triage and resolve the problem at scale.
Data testing
Data testing
is a detection method that employs user-defined rules to identify specific known issues within a dataset. Manual data testing can be effective for specific use-cases, but naturally becomes less effective at scale. Moreover, testing can only detect the issues you
expect
to find, and its visibility is limited to the data itself — not the system or code that’s powering it.
Data quality monitoring
Unlike the one-to-one nature of testing,
data quality monitoring
is an ongoing solution that continually monitors and identifies anomalies in your data based on user-defined thresholds or machine learning. Benefits include broader coverage for unknown unknowns and the ability to track metrics and discover patterns over time. However, broad monitors can be expensive to apply effectively across a large environment, and still require monitors to be expressed in SQL. Like testing, monitors are also limited to the data itself and don’t support the root-cause process.
Data Observability
Inspired by software engineering best practices,
data observability
is an end-to-end AI-enabled approach to data quality management that’s designed to answer the what, who, why, and how of data quality issues within a single platform. It compensates for the limitations of traditional data quality methods by leveraging detection, triage, and resolution in a single workflow across your data, systems, and code — the three places data products can break.
The future of data quality management for AI applications and beyond
It isn’t simply the AI that needs better data quality management, though. To maximize scalability, your data quality management will also need to incorporate AI as well.
By leveraging AI into monitor creation,
anomaly detection
, and root-cause analysis, advanced solutions like data observability can enable hyper-scalable data quality management for real-time data streaming,
RAG architectures
, and other
AI use-cases
.
As we move deeper into the AI future, I expect that we’ll see data teams continue to adopt solutions that unify not just tooling but teams and processes as well, leveraging automation and AI in intelligent ways to democratize data quality for the teams that own it.
What do you think? Agree? Disagree? Let me know in the comments.
Stay reliable,
Barr
Data Quality
Generative Ai Use Cases
Data Observability
Data Science
Artificial Intelligence
--
--
2
Follow
Written by
Barr Moses
20K Followers
·
227 Following
Co-Founder and CEO, Monte Carlo (
www.montecarlodata.com
). @BM_DataDowntime #datadowntime
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Test Your Data Until It Hurts. A data testing tale | by Micha Kunze | Towards Data Science,"Test Your Data Until It Hurts. A data testing tale | by Micha Kunze | Towards Data Science
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Making Sense of Big Data
Test Your Data Until It Hurts
A data testing tale
Micha Kunze
·
Follow
Published in
Towards Data Science
·
8 min read
·
Nov 3, 2020
--
Listen
Share
Photo by Jay Heike on Unsplash
If you work in analytics, it is safe to assume that you have complained about data quality issues more than once. We all have, and the pain is real!
As a Data Engineer, data and its quality are close to my heart. I am acutely aware of how paramount good data quality is — and recently this topic has created some buzz with e.g. #
DataOps
. There are a plethora of articles on that topic: simply go to the
DataOps Medium page
from DataKitchen, or if you like the term MLOps better, there is also plenty of content, such as
this blog post about data testing in MLOps
from the
great expectations
blog.
There is much more out there — but from my experience, the adoption of systematic data testing and quality monitoring is lacking in practice. In reality, if there even is data testing it often ends up as some assertions mangled into the data pipeline, polluting the data pipeline code, and not creating any visibility of your data quality issues. In this post, I want to give some insight into our (ongoing) journey of data testing and why it has to hurt!
Data Quality
The first problem with data quality that I see is that it is a very vague term. The actual implications of how one defines data quality or which data quality issues matter are dependent on your use case and your type of data.
One usual suspect is missing data/observations, either due to incomplete records or due to a copying error in your pipeline that went unnoticed. An upstream job in the enterprise data store changed, suddenly you have fewer rows/observations in your data. Then there are wrong entries, i.e. a person entered a number in a field somewhere that was plain wrong or in the wrong format. And the list goes on.
Ok, so if the data coming into your system has issues, what about the data that you put out? And even if you manage to fix those issues, are you making sure that the data you publish has top-notch quality? Yet another data quality issue.
All in all, there are a lot of things to consider mixed with a lot of noise and uncertainty on how to deal with data quality issues.
You should care
If you still think that you have no data quality issues because your code passed all the automated checks and you mocked all the data or even unit tested your sample data, you are wrong! Or rather, you might be correct right now, but eventually when things break in your data (and they will) you might very well be using and possibly putting out some bad data without even knowing.
If you do not test you do not know, simple as that. So start testing now! And once you start testing, you should test until it hurts. If you just collect data metrics that show that the pipeline is running with X rows so you can pat yourself on the back for how great your automation skills are, this will not generate any value for you. Or the other way around, paraphrasing
Daniel Molnar
: “vanity metrics are useless”, meaning don't monitor metrics that make you look good, instead monitor the bad things that make painfully obvious what is wrong and what you need to fix. If the tests do not hurt you, if they do not show you the things that are wrong, they are worthless.
Test your data until it hurts so that you have to fix the issues and constantly improve!
Let’s flip that over to something good-what can we do to start testing? How can we feel the good pain? 😅
The most important thing is to start.
And to start with something simple. From my perspective the most value can be gained from two things when testing our data:
Break automated pipelines if data quality is bad
Observe data quality and unexpected data / failed pipelines
The first point is obvious, we do not want to use or publish bad data, so we fail if the data is bad. The second one is nearly as valuable: When you start testing data quality, you very likely will not know what good data even looks like, i.e. what are the bounds of good data or how does a good data distribution look like? There might be variations on data coming in on weekends and weekdays or seasonal trends that you did not pick up on the initial dataset you worked on. So in order to stay on top of breaking things for the sake of data quality, you need to learn and iterate on what good data actually looks like over time.
This works for us
Until now I have been vague on how we actually test — so let's get to some more tangible examples of how our team continuously tests data.
As I wrote in a
story on Data Engineering practices
: keep it simple! And if you, like me, are working a lot with python and pandas, then
great-expectations
is a fantastic place to start with your data testing journey.
great-expectations works by building
expectation-suites
(basically test suites) which are subsequently used to validate data batches. These suites are saved as
.json
files and live with your code. To build expectations you can use some basic profiler from the package (or write your own — it is rather simple) to start with some suggestions or start from scratch. Let us look at a simple example:
The above example assumes you have installed the package and initialized it (I created
this repo
to get you started quickly). The example shows how to quickly add a data source to great_expectations, add an expectation to the expectation suite and then test data against the expectation suite. Optionally you quickly check the data documentation rendering.
Key features
As stated earlier, the first thing we wanted to aim for is breaking a pipeline if the data is bad and we then wanted to observe data quality over time. Using the data validation feature lets you do the former one easily: just run the validation as part of your pipeline and break if needed. The latter one is also covered as the validation result will tell you what went wrong, i.e. which values you did not expect. So you immediately know what is wrong and you can act.
Other key features include that the expectation suites live with the code, which is perfect for version control. Furthermore, you can easily add comments to your suites and capture data quality issues (and they render nicely in the automated data documentation).
Since all of it lives in your codebase (and of course you use git) it straight-forward to collaborate in the team. The addition of editing expectations in a notebook (with customizable jinja templates) is a feature that we use constantly: just run
great_expectations suite edit <suite_name>
!
Testing your data and data distribution.
great-expectations allows you to easily test data distributions. Simple mean, median, min, max, or quantiles or more advanced things such as Kulback-Leibler divergence. You can use the
mostly
keyword for most expectations to tolerate a certain fraction of outliers. The simple build-in expectations get you very far!
Of course, there is much more: you can test data freshness against pipeline runtime and use evaluation parameters generated and runtime or build your own expectations. There is a ton that I am not covering, such as automated data documentation, data profiling reports all with automatic HTML renders that make it easy to share and publish.
And on top of all that you can easily get involved contributing to the
code base
— the guys are extremely helpful and appreciative!
How we use it
We are testing the input and output data of each pipeline run, independent if the pipeline code. Our pipelines run as kubernetes jobs and so we created a simple wrapper that reads the command line arguments, resolves the dataset names, matches them to an expectation suite, and validates the data.
Data validation (blue diamonds) decoupled from the actual pipeline code, controlling failing, alerting, and publishing of the pipeline.
The above image is a simple diagram of what happens when any of our pipelines run: the input gets validated, only then does the actual job run. After the job ran, the output data gets validated, only then will the data be published. If a validation on the output data fails, we publish into a
failed
destination so we may inspect the output if needed.
Pain
As soon we put that in production the pain began. We had spent significant time profiling the data, running all the pipelines with new data validation checks in our non-production environment. And we were confident. Still, there were so many things we did not know about our data and how it behaved over time.
For several weeks we had failing pipelines due to the data validations. Some we expected, but at some point, I felt really horrible for putting one of my colleagues who was on support rotation through this: Constantly failing runs, many times updating the expectation suites, rinse and repeat. In the beginning, it was hard to figure out when to change thresholds/expectations, and when we were looking at an actual problem.
There was simply pain.
Gain
After the first pain subsided we quickly saw strange data issues. In one instance this led us to actually find bugs in some of our data transformation pipelines that only showed up over time. We fixed it -> profit!
In another instance we actually prevented publishing bad predictions — it had turned out that one of the upstream jobs that was copying data from the enterprise data store had incomplete data 😱. Previously, this bug has caused us to give out some bad predictions in one
known
incident, which had some real 💰 consequences for the business. So, preventing us from doing that is some serious profit!
Conclusion
It is true: no pain no gain!
We all know we have to get our data quality in check. And once you open that can of worms you will quickly feel the pain of dealing with all the data quality issues that you find. But the good news is: it is well worth it. Simple data tests can get you very far and significantly improve the quality of your product.
In the team, I work closely with data scientists, and they now love the data testing. They appreciate the tool and the value it generates for the team and our products. We continuously improve our datasets with it, we collaborate on the testing and the testing is not getting mangled up with the pipeline code.
So, while this journey promises pain, it is well worth it. Start testing your data now! 💯
Data
Data Engineering
Dataops
Software Development
Making Sense Of Big Data
--
--
Follow
Published in
Towards Data Science
774K Followers
·
Last published
3 hours ago
Your home for data science and AI. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.
Follow
Follow
Written by
Micha Kunze
50 Followers
·
51 Following
Data engineer, scientist, and developer. Passionate about driving good outcomes with data and solving complex data problems! 👉
linkedin.com/in/michakunze
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Data Engineering with a ‘Cover and Move’ Approach to Data Quality | by Tim Webster | Art of Data Engineering | Nov, 2024 | Medium","Data Engineering with a ‘Cover and Move’ Approach to Data Quality | by Tim Webster | Art of Data Engineering | Nov, 2024 | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Data Engineering with a ‘Cover and Move’ Approach to Data Quality
A Tactical Approach to Data Quality
Tim Webster
·
Follow
Published in
Art of Data Engineering
·
7 min read
·
Nov 5, 2024
--
Share
Photo by
Ali Pazani
on
Unsplash
If you work in data in any shape or form, you’re going to hear those words thrown around at some point. Most people take a nonchalant attitude to data quality, thinking, “The data should be quality-checked upstream at the source, right?” Wrong.
The data input is always right — alrighty then.
The “not my problem” stance. These are the same folks who like to wave at data quality issues as they fly past.
“Our data is always accurate” (heard this one at a recent conference —
insert audible eye roll
) — yeah, right.
My personal favorite: “I thought it was already quality-checked” — hmm, by who, the fairies?
These are assumptions, and assumptions will bite you harder than you think.
Data quality is a silent killer in the industry. It will sneak up on you when you least expect it or slam your system shut in an instant. This is why data quality should be at the heart of any data system, especially those making data-driven decisions. If people are making decisions based on data, then that data should go through some kind of data quality “sausage machine.”
The even bigger problem brewing now is all that low-quality data floating…
--
--
Published in
Art of Data Engineering
1.8K Followers
·
Last published
3 days ago
Making Sense of Data & Helping Others Grow: Tips, Advice, and Stories from the Front Lines of Data Engineering
Follow
Written by
Tim Webster
2K Followers
·
55 Following
🚀 Lead Data Engineer | Ex DBA | Dad | Technology Enthusiast | Star Wars Addict | I love data, writing and helping people.
https://linktr.ee/artofdataeng
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Ensuring Data Integrity: A Data Engineer’s Guide to Testing | by Andy Sawyer | Medium,"Ensuring Data Integrity: A Data Engineer’s Guide to Testing | by Andy Sawyer | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Ensuring Data Integrity: A Data Engineer’s Guide to Testing
Andy Sawyer
·
Follow
3 min read
·
Apr 1, 2024
--
Listen
Share
Photo by Kristina Alexanderson (Internetstiftelsen)
When it comes to deploying production code ensuring the accuracy and reliability of your data pipelines is paramount. As data underpins critical business decisions, testing becomes an indispensable practice. In this post, I dip my toe into the world of testing from a data engineer’s perspective, spotlighting unit tests and data quality tests. Through Python examples, we can explore how these tests safeguard data integrity, enabling businesses to navigate the data-driven future confidently.
The Cornerstone of Data Reliability: Unit Testing
Unit testing, the foundation of software testing, verifies the smallest testable parts of an application, known as units, to ensure they function as intended. For data engineers, this often means testing individual functions or methods used in data processing scripts.
Python Example: Testing a Data Transformation Function
Consider a function designed to cleanse and standardize phone numbers in a dataset:
def std_phone_number(phone_number):
""""""Remove dashes and parentheses and removes country code if present.""""""
std = phone_number.replace(""("", """").replace("")"", """").replace(""-"", """").replace("" "", """").strip()
if std.startswith('+61'):
std = phone_number.replace('+61', '0', regex=False)
return std
To test this function, we use the
unittest
module in Python:
import unittest
from your_data_module import std_phone_number
class TestPhoneNumberStandard(unittest.TestCase):
def test_std(self):
self.assertEqual(std_phone_number(""(02) 1234-3456""), ""0212343456"")
self.assertEqual(std_phone_number(""02-1234-3456""), ""0212343456"")
self.assertEqual(std_phone_number(""+61 2 1234 3456""), ""0212343456"")
if __name__ == '__main__':
unittest.main()
This simple yet effective test ensures our function behaves as expected across various input scenarios. It allows us to test the code regardless of whether these scenarios exist within the dataset that eventually gets piped through.
Elevating Data Quality: Data Quality Tests
While unit tests validate code logic, data quality tests assess the data itself. These tests verify completeness, consistency, accuracy, and reliability of the data in our pipelines.
Python Example: Testing Data Completeness and Consistency
Using the
pandas
library, we can perform data quality checks on a DataFrame:
import pandas as pd
# Sample DataFrame
data = {'name': ['Alice', 'Bob', None], 'phone': ['0235437535', '0734784924', '0475832456']}
df = pd.DataFrame(data)
# Data Completeness Test
def test_data_completeness(df):
missing_values = df.isnull().sum().sum()
assert missing_values == 0, f""Data is missing. {missing_values} values found.""
# Data Consistency Test
def test_data_consistency(df):
unique_phones = df['phone'].nunique()
assert unique_phones == len(df), ""Duplicate phone numbers found.""
# Running the tests
test_data_completeness(df)
test_data_consistency(df)
These tests ensure our dataset is complete without missing values and consistent with unique phone numbers. Here are tests that can be run on the data in production to let us know if there is anything wrong with the quality of data coming from the upstream systems.
Embracing a Test-Driven Future
Unit and data quality testing are not mere check-boxes in the data engineering process; they are critical practices that ensure the reliability and integrity of data pipelines. By incorporating these tests, data engineers can significantly mitigate the risk of data inaccuracies, fostering trust and confidence in data-driven decisions.
In the spirit of continuous improvement, embracing testing as part of your development process is not just about catching errors; it’s about building a culture of excellence and reliability in your data practices.
Python
Data Engineering
Data Pipeline
Unit Testing
Data Quality
--
--
Follow
Written by
Andy Sawyer
964 Followers
·
10 Following
Bringing software engineering best practices and a product driven mindset to the world of data. Find me at
https://www.linkedin.com/in/andrewdsawyer/
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Implementing Data Quality at Scale: Investigating Validation & Testing for Large Data Sets | by Martin Arroyo | 99P Labs | Medium,"Implementing Data Quality at Scale: Investigating Validation & Testing for Large Data Sets | by Martin Arroyo | 99P Labs | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Implementing Data Quality at Scale: Investigating Validation & Testing for Large Data Sets
Martin Arroyo
·
Follow
Published in
99P Labs
·
6 min read
·
Nov 18, 2022
--
Listen
Share
Validation is a critical step in data processing pipelines. Following ingestion and one or more transformations, it is imperative that we validate our expectations of the data sets that are produced by these pipelines. But how exactly can we test our data to ensure its validity? What kinds of checks should there be? And at what points should we run these checks?
Additionally, what if our data set is both large and continuously streaming new data? How does that impact our data quality monitoring and measurement efforts? These are some of the questions we will investigate and attempt to answer.
This article is the first in a series on implementing data quality checks at scale. Be sure to follow us to stay up-to-date with this series and others from the team at
99P Labs
.
Photo by
Myriam Jessier
on
Unsplash
What exactly are data quality and validation? Why is this important?
Data quality is a measurement of the condition of a given data set, based on factors like accuracy, completeness, consistency, timeliness, and reliability. It measures how well-suited a data set is to serve its intended purpose.
Validation, in this context, refers to testing data outputs to confirm that they accurately represent the constructs we intend to measure. This includes confirming our expectations of the data shape, structure, and completeness.
But why are data quality and validation so important? Well, for one, erroneous data can cost businesses a lot of money. IBM has estimated that
poor data quality costs businesses $9.7 million per year on average
, and
up to $3.1 trillion overall in the US alone
. Other estimates place this figure at roughly 8–12% of total annual revenue.
Organizations typically find data error rates between 1% and 5%, but this
figure can be as high as 30% (or more) for others
. To calculate the data error rate, we take the number of fields where errors were observed and simply divide by the total number of fields under test:
Calculating the total data error rate
Poor data quality is costly. But there is a cost to bad quality that can’t truly be quantified — trust. Bad data can lead to spurious insights, which in turn can have disastrous consequences stemming from decisions that are made based on those insights.
Establishing trust is difficult. It is even harder to win back once it has been damaged. Consider the
average cost of a data breach, which in 2022 is $9.44 million (in the US.)
While this figure includes legal and regulatory fees, technical activities, as well as other factors like customer turnover, it still cannot represent the entire cost of that breach of trust.
Aside from financial and reputational costs, bad data costs time and creates a negative impact on analytic efforts by introducing errors into data models. At a minimum, such errors introduce extra roadblocks that the data team must spend time troubleshooting and correcting. At worst, this can lead to the aforementioned spurious insights and consequently ill-informed decision-making.
Data quality and validation are critical because bad data costs time, money, and trust.
What have we done so far?
In previous posts, we have explored possible data quality frameworks and metrics, including running some experiments using open source tools like Great Expectations. Our last post about data quality frameworks,
Weighing the Value of Data Quality Checks
, surfaced several key findings and recommendations.
Our primary consideration for a data quality framework, based on research done so far, remains that our output should be useful, easy to understand, and easy to access for our stakeholders. This would look like a set of meaningful metrics that users of our data could view to quickly determine quality at both the table and column levels. The ideal output would describe the quality of the given data at a glance.
With respect to tools, the consensus so far is to leverage Trino — a distributed query engine — over our data lake to create queries that would profile our data and perform validation checks. Great Expectations is quickly becoming ubiquitous when it comes to data quality monitoring tools, but the jury is still out as to whether or not it’s the best fit for our needs. We found it to be a good tool for automating data quality checks in general, but we’re still unsure whether dealing with the overhead that it comes with makes sense for us. Since it is still under active development, we may revisit its use in future posts.
While we have yet to implement a data quality framework, our research so far has helped shape the direction that we want to go and has given us an idea of what the final product should include.
What challenges are we currently facing?
From a technical standpoint, our data volume presents the biggest challenge to overcome at this time. This creates difficulties with profiling the data and performing even basic validation checks, such as checking for the presence of null values or duplicate rows.
The velocity with which we ingest new data also presents challenges to implementing an effective data quality framework. Our telematics data streams in near-real time, which means that any potential framework would need to accommodate the rate of ingestion and processing.
Another challenge is understanding the data from a semantic perspective well enough to create reasonable expectations, which could then be validated. Data quality is a team sport, and we will require help from domain experts to establish reasonable and effective data validation checks.
How are we planning to overcome these challenges?
The first step in implementing a data quality framework would be to enable data profiling. Knowing more about the properties and distributions of our data will be critical in establishing baseline measurements, as well as continued monitoring. This is a prerequisite for implementing validation and testing.
To overcome the challenge of profiling a large volume of data, we will use statistical sampling techniques to select a representative subset of our data to profile. As mentioned earlier, Trino will be leveraged to perform the profiling due to its ability to query large amounts of data in a distributed manner. To deal with data velocity issues, we will investigate partitioning methods to include as part of our overall sampling strategy.
Once we have implemented a profiler, we will then work on creating our validation checks. There are general validation checks that can be used across all (or most) of our data sets, particularly those that focus on validating structure and completeness. However, this stage will largely be a collaborative endeavor, as we will need the help of domain experts and other knowledgeable parties to create checks that are specific to a given data set.
What’s next?
Our next step is to enable data profiling which, as mentioned earlier, is a prerequisite for implementing a data quality framework. First, we will explore sampling and partitioning methods for large data sets to help us overcome volume and velocity issues. Then we will specify and implement our data profiler, leveraging Trino and other distributed technologies.
Final Thoughts
Data quality and validation are important because poor data costs time, money, and trust. To test our data and ensure validity requires knowledge of the characteristics of the data (via profiling) as well as its semantic qualities (which can be uncovered with the help from domain experts.) Given both the volume and velocity of our data, it is imperative to leverage both statistical sampling and distributed computing to efficiently profile and validate it.
We hope you have found this informative and that you’ll continue
to follow along
as we work on implementing a data profiler for a large scale data set in the next part of this series!
Data
Data Science
Technology
Data Quality
Dataset
--
--
Published in
99P Labs
118 Followers
·
Last published
8 hours ago
A collection of our shared learnings across Mobility, Data, Energy and Innovation.
Follow
Written by
Martin Arroyo
12 Followers
·
15 Following
Applied AI Research Engineer @ 99P Labs | Data Analytics Instructor @ COOP Careers
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Pipeline Quality Checks. Building a pipeline with data quality… | by Elise Casey | Building Ibotta | Medium,"Data Pipeline Quality Checks. Building a pipeline with data quality… | by Elise Casey | Building Ibotta | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Pipeline Quality Checks
Elise Casey
·
Follow
Published in
Building Ibotta
·
10 min read
·
Jan 9, 2023
--
Listen
Share
Photo by
Quinten de Graaf
on
Unsplash
This is a description of a strategy to implement data quality checks throughout a pipeline. These checks can halt the pipeline upon data quality test failures to prevent bad data from propagating to downstream tables and consumer data pipelines. While this is not the only method, it is one that has proved to be effective in improving data quality for the Analytics Platform Engineering (APE) team at
Ibotta
.
The Problem
Ibotta
is a data-driven company, where we treat data as a product. As with any product that a company produces, data should undergo thorough testing to ensure that it meets quality standards. These tests should help data consumers trust that the data is consistently reliable, accurate, and complete. Data is integral to making business decisions, so bad data can lead to incorrect conclusions, which could be detrimental to the company.
Before the APE team began Project Cerberus (goals: to decouple, optimize, and validate pipelines), there were only a few data validation tests in place for key tables. They were not comprehensive enough, and they did not provide insight into exactly where data bugs might have been introduced. Sometimes source tables would have bad data, which would propagate to downstream tables. Other times, a new logic change could have an unforeseen impact on downstream tables. It tends to be much easier to prevent data issues from entering tables than it is to correct those issues after the fact, so APE implemented a method to validate source tables and halt pipelines upon source data validation failures.
The Solution
Below are the steps that APE’s
Airflow DAG
s take to build a key rollup/fact/aggregate table:
Use sensors to check that source tables are up-to-date with the latest data
Validate source table data
— Conduct pipeline blocking checks on source data for critical quality issues
— Emit non-blocking warnings for non-critical quality issues which can be investigated during business hours
Build table
Validate final table data for known potential quality or integrity issues
Example DAG with data quality checks
After determining that all upstream tables are up-to-date, we conduct data quality checks on those tables. These check for quality issues that might impact the integrity of the final table, so we only test the columns in the source tables that are actually used to build the final table. “Circuit-breaker” validations are upstream of the final table build task, so they halt the process that builds the table if the source table does not pass the checks. These tests are the most important ones because they impact the overall correctness of the build, such as row counts, join keys, and finance-related values. “Warning” validations are not upstream of the final table build task, so they simply emit a warning. These tests are not quite as integral to the final table’s integrity, but it is still important to be warned when columns from the source table differ from expectations.
After the circuit-breaker validations succeed, the final table builds, and then we perform some validation checks on that table. When a validation test on the final output of our build fails, warnings are emitted, but the pipeline does not halt. These checks generally test every column in the table and provide insight into whether any of the most recent data deviates from expected values. These essentially check that the business logic used to build the table is sound.
The subsequent steps of the pipeline follow a similar pattern. The screenshot below shows an example where there are multiple source tables:
Example DAG with data quality checks on multiple source tables
This DAG
Ensures that all source rollup tables are up-to-date
Validates all relevant data from source tables (both circuit-breaker and warning validations)
Updates the fact table
Validates the fact table
This pattern continues for almost all downstream tables in the pipeline. For tables that build in multiple stages, we validate each intermediate table independently. This means that the tables stay decoupled, so they can be safely modified independently. It also makes it much easier to track down precisely where quality issues are coming from in an otherwise complex process.
PyDeequ: the testing method
To perform these data quality validation tests, we use
PyDeequ
, which is a Python API for Deequ. Deequ measures data quality metrics using data quality constraints in a user-defined testing suite. It then generates reports, stating whether a specific check on a column or set of columns passed or failed. Since Deequ is implemented on top of Apache Spark, it is scalable and has the ability to work with large datasets.
We wrote helper functions that accept YAML configuration files that define data quality constraints for each table column. Using those data quality constraints, the functions can run Deequ testing suites and generate reports, determining whether tests passed or failed.
Demo: configuring DAGs with data quality tests
The APE team uses a custom-built Dag Factory library, which dynamically generates Airflow DAGs from YAML configuration files. Let’s take a look at example configuration files that generate these DAGs and data quality tests. Below is a YAML configuration file that generates a DAG using the Dag Factory.
config.yaml
tasks:
- lvl : 1
task:
- name: ""update_fact_table""
execute_via: ""notebook""
new_cluster_config:
is_variable: False
cluster_config: 'dags/fact_table/cluster_config.json'
notebook_path: '/Repos/repo_name/git_repo_name/dags/fact_table/fact_table_DML'
parameters:
variables:
- ""ENV""
partition: ""{{ds}}""
validate_task:
- validate_fact_table
- validate_fact_table_parents
source_data:
- table: ""schema.rollup_table_1""
partition_type: ""none""
delta_table: True
circuit_breaker_validate_task: circuit_breaker_validate_rollup_table_1
warning_validate_task: warning_validate_rollup_table_1
- table: ""schema.rollup_table_2""
partition_type: ""none""
delta_table: True
circuit_breaker_validate_task: circuit_breaker_validate_rollup_table_2
warning_validate_task: warning_validate_rollup_table_2
We can just focus on the lines after
validate_task
, as those are the most relevant to the data quality tests in the DAG. On the lines that follow
source_data
, we define which tables are upstream of the table that we are building. In this example, those tables are
schema.rollup_table_1
and
schema.rollup_table_2
. For each of these tables, we define a
circuit_breaker_validate_task
and a
warning_validate_task
. Those values map to configurations in a circuit-breaker and warning YAML file, respectively. This is where we define what tests we want to perform on each table. Below are example configurations of each.
circuit_breaker.yaml
circuit_breaker_validate_rollup_table_1:
email_on_failure: True
email: FAILURE_EMAIL
schema: schema
table: rollup_table_1
tests:
- type: size
threshold: 15000
# This is join key. Should be 100%
- type: complete
columns: [join_key]
threshold: 1.0
circuit_breaker_validate_rollup_table_2:
email_on_failure: True
email: FAILURE_EMAIL
schema: schema
table: rollup_table_2
tests:
- type: size
threshold: 50000
# Missing this value would impact financial reporting.
- type: complete
columns: [finance_col]
threshold: 1.0
warning.yaml
warning_validate_rollup_table_1:
email_on_failure: True
email: LOW_URGENCY_EMAIL
schema: schema
table: rollup_table_1
tests:
- type: min
column: col_1
min: -50.00
- type: max
column: col_1
max: 20000.00
- type: complete
columns: [ col_2 ]
threshold: 0.99
- type: containment
column: col_3
values: [VAL_1, VAL_2, VAL_3]
warning_validate_rollup_table_2:
email_on_failure: True
email: LOW_URGENCY_EMAIL
schema: schema
table: rollup_table_2
tests:
- type: non_negative
column: col_1
- type: unique
columns: [ col_2 ]
threshold: 1.0
- type: complete
columns: [ col_2 ]
threshold: 0.99
- type: containment
column: col_3
values: [VAL_1, VAL_2, VAL_3]
There are fewer tests in the circuit-breaker config than the warning config. This is by design. We want to limit the circuit-breaker tests to what is absolutely necessary. This reduces noisy high-urgency alerts and unnecessary halting of the pipeline. In these examples, we only check that the source table’s row count meets baseline expectations and that join keys and finance-impacting values are completely populated.
The warning config is a bit more extensive. This is where we check all columns from the source table that are used to build the final table. In the first example (
warning_validate_rollup_table_1
), we are checking that
col_1
fits the minimum and maximum value constraints that we defined in the config. We are also checking that
col_2
is 99% non-null and that
col_3
only contains values
VAL_1
,
VAL_2
, or
VAL_3
. These are set as warnings because perhaps
col_1
can expand beyond the set min and max values over time, or
col_3
could begin to contain another value
VAL_4
and still be valid. As for
col_2
, there could already be some nulls in that column that are still valid data, and we just want to check that the null rate of that column does not increase abruptly.
In the second example (
warning_validate_rollup_table_2
), we are checking that
col_1
only contains values greater than or equal to 0, that
col_2
is 100% unique and 99% non-null, and that
col_3
only contains values
VAL_1
,
VAL_2
, or
VAL_3
. By sending low-urgency warnings, APE team members are alerted to determine if the data that failed test is actually bad data, or if it is something new and expected in the source data, so the tests should be modified.
validate.yaml
validate_fact_table:
email_on_failure: True
email: LOW_URGENCY_EMAIL
schema: schema
table: fact_table
tests:
- type: unique
threshold: 1.0
columns: [col_1]
- type: complete
threshold: 1.0
columns: [col_1]
- type: complete
threshold: 0.9999
columns: [col_2]
- type: complete
threshold: 0.9999
columns: [col_3]
- type: containment
column: col_3
values: [VAL_1, VAL_2]
validate_fact_table_parents:
email_on_failure: True
email: LOW_URGENCY_EMAIL
schema: schema
table:
SELECT COUNT(*) AS no_parent_count
FROM schema.fact_table f
WHERE f.id NOT IN (SELECT id FROM schema.monolith_table);
tests:
- type: max
column: no_parent_count
max: 0
One line of the config.yaml file defines a
validate_task
. This is the task that will validate the final table’s data. The values assigned to
validate_task
(
validate_fact_table
and
validate_fact_table_parents
) map to configurations in a final YAML file called validate.yaml (example shown above). The first config (
validate_fact_table
) has general tests that check for unique and complete columns that only contain a specific set of values. The second config (
validate_fact_table_parents
), is a bit more specific. This one queries from the fact table that was just built and checks if each ID maps to an ID in
schema.monolith_table
. These tests are easily customizable, so we can test just about anything.
What do we do if circuit-breaker tests fail on source data?
Real example of a DAG with a failed circuit-breaker validation task
Output from failed circuit-breaker validation tests
The screenshots above are from a real example of an APE DAG that had a failed circuit-breaker data validation task on 2022–12–25. This validation task failed on a
size
check, stating that “Value: 1312 does not meet the constraint requirement.” We set this test to fail if the daily number of source table rows was ever fewer than
24000
. In this case,
1312
is much lower. So what happened? When the pipeline halted with this test failure, it alerted the person that was on-call. They looked at the process that builds the source table, and confirmed that everything had executed successfully. This low volume of records was actually valid. This failure happened on Christmas day, so there were simply fewer consumer purchases due to store closures for the holiday. After confirming that this was the case, the on-call person marked the task as success to allow the pipeline to continue.
Prior to implementing these circuit-breaker validation tests, there was an incident where a process was running behind that builds the source tables that APE’s pipeline depends on. The upstream process had partially completed, so to APE’s table sensors, it seemed like it was safe to begin the pipeline. Unfortunately, this data was incomplete. If we had had our circuit-breaker validation tests implemented at this time, the pipeline would have halted with a failed
size
(row count) test. Instead, we had to rerun our pipeline when we discovered what had happened, which resulted in an increase in cost and delayed availability of data that day. Circuit-breaker validation tests would have prevented this! (Side note: We have put additional measures in place since then to prevent this from happening again).
What do we do if warning tests fail on source data?
When warning validation tests fail, we investigate the failed columns, and determine if the data seems flawed. If so, we let the upstream data producers know during business hours. We also notify data consumers that the table may need reprocessing pending feedback from producers. If they tell us that this is the expected behavior, we modify our tests so that they pass in the future. If not, we work with the data producers to account for the issue in the existing data and ensure that it meets quality expectations moving forward. This is very rare, as we have a process for validating event data prior to making it available for use in downstream builds. These are set as low-urgency alerts because there is very little we can do in the middle of the night when it is an issue with upstream data from another team.
What do we do if validation tests fail on the final table?
When these tests fail, we check if there was a recent logic change in the script that builds the table. If so, we have to dig in to see how exactly that logic change is affecting any failed column tests, and if that logic is flawed. If so, we work on fixing that logic. If not, we modify the tests to reflect the data’s new behavior. Either way, it is good to be warned about the new behavior before data consumers find potential issues.
Conclusion
It is considered best practice to incorporate data quality tests into pipelines. It is especially beneficial to have data quality tests that can halt the progress of a pipeline upon source data test failures. These tests can prevent bad data from being inserted, which minimizes time and resources spent resolving data issues. Having multiple data quality checkpoints throughout each DAG in a pipeline provides data consumers with the peace of mind that they are making reliable data-driven decisions for the company.
Interested in working at Ibotta? Check out
https://ibotta.com/careers
Data Quality
Deequ
Pipeline
Validation
Data Validation
--
--
Published in
Building Ibotta
511 Followers
·
Last published
Aug 24, 2023
Thoughts and experiences from Ibotta's engineering, analytics and product teams
Follow
Written by
Elise Casey
13 Followers
·
3 Following
Senior Data Engineer at Ibotta, Inc.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Create Data Quality Framework with Great Expectations | by Pallavi Sinha | Medium,"Create Data Quality Framework with Great Expectations | by Pallavi Sinha | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Create Data Quality Framework with Great Expectations
Pallavi Sinha
·
Follow
7 min read
·
Aug 7, 2023
--
4
Listen
Share
Photo by
Markus Spiske
on
Unsplash
In the modern era of data-driven decision-making, data quality has emerged as a crucial factor that significantly impacts an organization’s success. However, with the ever-increasing volume, velocity, and variety of data, ensuring its accuracy, reliability, and consistency has become a formidable challenge. This is where a well-structured data quality framework comes into play, as it provides a systematic approach to address data quality issues and lays the groundwork for reliable and trustworthy data-driven insights.
In this blog, we will try to develop a basic Data Quality Framework and test it on a sample DataFrame. We will be using Great Expectations Library for our project.
A small introduction to Great Expectations Library —
Great Expectations is an open-source Python library that offers a comprehensive suite of tools and functionalities to validate and monitor data across various data sources.
With support for Pandas DataFrames, PySpark DataFrames, SQL databases, and cloud storage, it seamlessly integrates with popular data processing frameworks, making it accessible to a wide range of data pipelines.
In this blog, we will be using it for PySpark DataFrames.
At the core of Great Expectations lie “expectations” — predefined rules or checks that data must adhere to. Users can define expectations based on data types, null values, uniqueness, value ranges, and more.
You can read the
documentation
to know a lot more about this library.
Creating Data Quality Framework —
Step-1: Set up a virtual environment and install the required packages —
Create a project directory and open the terminal in that path.
Create a virtual env —
python3 -m venv ./venv
Activate it —
source ./venv/bin/activate
Install PySpark library —
pip install pyspark
Install Great Expectations Library —
pip install great_expectations
Step-2: Creating expectations —
First, create the
expectation
folder. Inside it let's create a Python file named
Expectation.py
. This file contains an abstract class
Expectation
. This class will be inherited by any new Data Quality check class which in this context we call an expectation.
from abc import ABC, abstractmethod
class Expectation(ABC):
def __init__(self, column, dimension, add_info = {}):
self.column = column
self.dimension = dimension
self.add_info = add_info
@abstractmethod
def test(self, ge_df):
pass
Here the
column
attribute is the name of the column on which the expectation rule will be applied. The
dimension
attribute is the category of the data quality which can be any of the five — Completeness, Uniqueness, Validity, Accuracy, and Consistency.
add_info
is the optional parameter for adding any additional information required for the test.
The
test
method accepts
ge_df
parameter which is
SparkDFDataset
.
SparkDFDataset
is a thin wrapper around PySpark DataFrame which allows us to use Great Expectation methods on Pyspark DataFrame. Now the
test
method can be implemented to check if the column values are passing that particular data quality test.
Now, let's create an expectation class to check if the value is not null. A new file named
NotNullExpectation.py
is created which contains the following code :
from expectation.Expectation import Expectation
class NotNullExpectation(Expectation):
def __init__(self, column, dimension, add_info = {}):
super().__init__(column, dimension, add_info)
def test(self, ge_df):
ge_df.expect_column_values_to_not_be_null(column=self.column,meta = {""dimension"": self.dimension})
In the test method implemented above we directly applied the expectation rule available in Great Expectation Library for checking the null values.
Most of the commonly used tests are available in Great Expectations Library and we can directly use them. We can even create custom expectations. For more info refer to the official doc.
Let's create two more expectation classes in
UniqueExpectation.py
and
ValuesInListExpectation.py
respectively.
from expectation.Expectation import Expectation
class UniqueExpectation(Expectation):
def __init__(self, column, dimension, add_info = {}):
super().__init__(column, dimension, add_info)
def test(self, ge_df):
ge_df.expect_column_values_to_be_unique(column=self.column,meta = {""dimension"": self.dimension})
from expectation.Expectation import Expectation
class ValuesInListExpectation(Expectation):
def __init__(self, column, dimension, add_info = {}):
super().__init__(column, dimension, add_info)
def test(self, ge_df):
ge_df.expect_column_values_to_be_in_set(column=self.column, value_set=self.add_info[""value_set""],meta = {""dimension"": self.dimension})
The first one is for checking if all the values are unique and the second one is for checking if the column contains only those values that are available in a predefined values list. Here we expect that the
add_info
contains
value_set
field for getting the predefined set of values.
Step-3: Create a Data Product config reader
This step is for creating a reader to read the data quality rules mentioned for various columns of a Data Product. In this project, we are going with JSON format config to provide the details. Create a
JSONFileReader.py
and add it inside
reader
folder. The following code goes into it —
import json
class JSONFileReader:
def __init__(self, filename):
self.filename = filename
def read(self):
with open(self.filename) as f:
return json.load(f)
The
read()
method returns the content of the JSON file in dictionary format.
Step-4: Create Data Quality Class which runs tests on PySpark DataFrame-
Now we will create a
DataQuality.py
file inside the
data_quality
folder. The following code is added to it —
from reader.JSONFileReader import JSONFileReader
from great_expectations.dataset.sparkdf_dataset import SparkDFDataset
from expectation.NotNullExpectation import NotNullExpectation
from expectation.UniqueExpectation import UniqueExpectation
from expectation.ValuesInListExpectation import ValuesInListExpectation
class DataQuality:
def __init__(self, pyspark_df, config_path):
self.pyspark_df = pyspark_df
self.config_path = config_path
def rule_mapping(self, dq_rule):
return{""check_if_not_null"" : ""NotNullExpectation"", ""check_if_unique"" : ""UniqueExpectation"", ""check_if_values_in_list"" : ""ValuesInListExpectation""}[dq_rule]
def _get_expectation(self):
class_obj = globals()[self.rule_mapping()]
return class_obj(self.extractor_args)
def convert_to_ge_df(self):
return SparkDFDataset(self.pyspark_df)
def read_config(self):
json_reader = JSONFileReader(self.config_path)
return json_reader.read()
def run_test(self):
ge_df = self.convert_to_ge_df()
config = self.read_config()
for column in config[""columns""]:
if column[""dq_rule(s)""] is None:
continue
for dq_rule in column[""dq_rule(s)""]:
expectation_obj = globals()[self.rule_mapping(dq_rule[""rule_name""])]
expectation_instance = expectation_obj(column[""column_name""], dq_rule[""rule_dimension""], dq_rule[""add_info""])
expectation_instance.test(ge_df)
dq_results = ge_df.validate()
return dq_results
Let's understand this class step-by-step. The
pyspark_df
attribute of this class is the PySpark DataFrame object on which Data Quality tests are to be executed. The
config_path
attribute is the path of the Data Product config which contains details about which test to apply on which column.
The method
rule_mapping
returns the name of the Exception class from a lookup dictionary which maps the rules to the name of the Exception classes implemented. For the scope of this project, the dictionary is limited to 3 items. The
_get_expectation
method internally used this method to get the required Expectation class instance.
As mentioned above we have to convert the PySpark DataFrame into
SparkDFDataset
so that we can apply the expectations test on it. The method
convert_to_ge_df
helps in this task.
As the name suggests
read_config
helps to read the config file by using
JSONFileReader
class implemented in the previous step. The method
run_test
is the function which will run the validations mentioned in the config on the DataFrame. It returns the detailed result in a dictionary format.
Step-5: Implement a helper function to get the Data Quality Test report in DataFrame format
Here we want to get the report in the form of a PySpark DataFrame. Create a
utils.py
file inside the
utils
folder and add the following code to it —
def create_df_from_dq_results(spark, dq_results):
dq_data = []
for result in dq_results[""results""]:
if result[""success""] == True:
status = 'PASSED'
else:
status = 'FAILED'
dq_data.append((
result[""expectation_config""][""kwargs""][""column""],
result[""expectation_config""][""meta""][""dimension""],
status,
result[""expectation_config""][""expectation_type""],
result[""result""][""unexpected_count""],
result[""result""][""element_count""],
result[""result""][""unexpected_percent""],
float(100-result[""result""][""unexpected_percent""])
))
dq_columns = [""column"", ""dimension"", ""status"", ""expectation_type"", ""unexpected_count"", ""element_count"", ""unexpected_percent"", ""percent""]
dq_df = spark.createDataFrame(data=dq_data,schema=dq_columns)
return dq_df
This method accepts
SparkSession
object and
dq_results
. The
dq_results
is the dictionary output after running the validations. It will return Data Quality Report in the form of PySpark DataFrame.
Step-6: Create the main file and add a config file —
Our Framework is complete. Now let’s create a sample config file and main file to check how our Framework is working. Consider a
student
data product that contains columns —
roll_no
,
first_name
,
last_name
and
subject
. Below is the config JSON and place it in
config.json
file inside
config
folder.
{
""data_product_name"": ""student"",
""columns"" : [
{
""column_name"" : ""roll_no"",
""dq_rule(s)"" : [
{
""rule_name"" : ""check_if_not_null"",
""rule_dimension"" : ""Completeness"",
""add_info"" : {}
},
{
""rule_name"" : ""check_if_unique"",
""rule_dimension"" : ""Uniqueness"",
""add_info"" : {}
}
]
},
{
""column_name"" : ""first_name"",
""dq_rule(s)"" : [
{
""rule_name"" : ""check_if_not_null"",
""rule_dimension"" : ""Completeness"",
""add_info"" : {}
}
]
},
{
""column_name"" : ""last_name"",
""dq_rule(s)"" : []
},
{
""column_name"" : ""subject"",
""dq_rule(s)"" : [
{
""rule_name"" : ""check_if_values_in_list"",
""rule_dimension"" : ""Validity"",
""add_info"" : {
""value_set"" : [""Maths"", ""Science"", ""English"", ""Hindi"", ""Sanskrit""]
}
}
]
}
]
}
The above is the JSON structure which contains the
dq_rule(s)
field that has an array value. Inside this array, we can place as many rules as required for that column. If we don’t have any data quality rule for a column we can leave it blank and the code will take care of it.
Now we will create
main.py
file. For now we have created a sample DataFrame inside the main file itself. But its better to implement a reader for reading the Data File and use it in main file. After creating DataFrame we will run the Data Quality Tests on it using the framework.
from pyspark.sql import SparkSession
from data_quality.DataQuality import DataQuality
from utils.utils import create_df_from_dq_results
spark = SparkSession.builder.appName(""DataQuality"").getOrCreate()
student_df = spark.createDataFrame([
(1,""Ram"",""Kumar"", ""Maths""),
(2,""Shyam"",""Kumar"", ""History""),
(3,""Mohan"", None , ""Science""),
(4,""Sohan"",""Singh"", ""Maths""),
(5,""Rohini"",""Kumari"", ""Science""),
(6,""Raj"", ""Kumar"", ""Maths""),
(7,""Meena"", None , ""Hindi""),
(8,""Rani"", ""Kumari"", ""Sanskrit"")], [""roll_no"", ""first_name"", ""last_name"", ""subject""])
dq = DataQuality(student_df, ""config/config.json"")
dq_results = dq.run_test()
dq_df = create_df_from_dq_results(spark, dq_results)
dq_df.show()
Here we intentionally added one wrong subject
History
in the second row of the Data. We can see the above output after running the
main.py
file.
Further, we can save this output DataFrame to storage and publish it however we want for example in dashboards, PowerBI, etc.
Conclusion:
In this blog, we went through a hands-on implementation of the Data Quality Framework. I hope you enjoyed learning this. The code can be found on this link —
Data Quality Framework
. Feel free to add feedback and ask questions. Thank you!
Data Engineering
Data
Data Quality
Pyspark
Great Expectations
--
--
4
Follow
Written by
Pallavi Sinha
253 Followers
·
10 Following
Data and AI enthusiast
Follow
Responses (
4
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality: a lesson from the myth behind Popeye the Sailor | by Vinícius Mello | hurb.labs | Medium,"Data Quality: a lesson from the myth behind Popeye the Sailor | by Vinícius Mello | hurb.labs | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality: a lesson from the myth behind Popeye the Sailor
What a cartoon from the ’30s can teach you?
Vinícius Mello
·
Follow
Published in
hurb.labs
·
8 min read
·
Feb 20, 2021
--
1
Listen
Share
“Data is like garbage. You’d better know what you are going to do with it before you collect it” — Mark Twain. Image by
QuoteFancy
.
Data is being increasingly used by businesses on a decision-making basis. Until a few months ago, I had a different understanding of what data quality is. I wasn’t used to thinking about it. Recently, I started to learn more about the subject; then, I found some interesting, and a bit humorous, stuff I’d like to share.
When I was a kid, I watched
Popeye the Sailor
and other TV cartoons as an ordinary kid from the ’90s. I grew up listening to my mom saying, “You should eat more spinach to become stronger,” and she always gave me the example of this character Popeye, one of the most famous cartoon characters, created by
Elzie Crisler Segar
in 1929.
Image by
Pixy
.
If you’re not familiar with this character, let me introduce him to you. Popeye was a sailor with his odd accent and improbable forearms who instantly possess super-strength after ingesting an always-handy can of spinach, a sort of anti-Kryptonite. It gave him his strength and perhaps his distinctive speaking style. He often takes many daunting challenges, such as battling his brawny nemesis Bluto for the affections of his love interest Olive Oyl, often kidnapped by Bluto. Without any effort, he only needed to take some spinach to become stronger.
The Myth
There is no doubt that eating vegetables is good for your health [7]
(if you don’t eat those, you definitely should!!)
, but recently, I discovered that this character was created based on a Data Quality Problem. I’ll explain it!
The fact behind this starts over fifty years earlier from the first publication of the strip. In 1870, Erich von Wolf, a German chemist, investigated the amount of iron in spinach. Von Wolf unintentionally missed a decimal point in his observations while transcribing data from his notebook, altering the iron content in spinach by magnitude order. Although only 3.5 milligrams of iron are actually in a 100-gram of spinach, the agreed reality was 35 milligrams. To put this in perspective, if the calculation were correct, each 100-gram would be like consuming a tiny piece of a paper clip.
Once this wrong number was installablepublished, the nutritional value of spinach became legendary. When Popeye was created, its misunderstood health properties lead the studio executives to suggest that Popeye should eat spinach to get his strength — In reality, if for iron, he should be eating the cans. It was only in the 1930s [1], more than 50 years after the first publication, that someone rechecked the numbers and finally corrected this mistake.
Nevertheless, Popeye helped improve American spinach consumption by a third [2]! But the harm was done. It spread and spread, and only recently went by the wayside, probably aided today by the relative obscurity of Popeye. But the mistake was so common that an article about this spinach case in 1981 [2] was published by the British Medical Journal, trying its best to finally demystify the problem.
Fake!, a British Medical Journal article about the spread information of the use of spinach.
Despite the increase in vegetable consumption and the strip's success, decisions were taken based on this printed error. Luckily, nothing bad has happened, but imagine if critical decisions have been taken on this. Even this error was corrected years later, it had already spread, and even today, some people still believe in this myth.
The Importance of Data Quality
Now, let’s change the context a little bit. Companies all the time and from various sources are ingesting data. Because of data quality problems, some of them have their own “spinach” false consumption, which is not immediately evident. The ones that haven’t been watching for it might have a long history of data-driven decisions based on poor quality data.
Articles about Data Quality constantly cites one expression:
Garbage In, Garbage Out
Garbage In, Garbage Out effects. Image by the author.
You can create amazing dashboards or even complex machine learning models, but if the quality of the data consumed is not good enough, I’m sorry, but the outcome will not be good either. Your analysis and models are just as good as your data.
If you’re not familiar with this concept, here are a few definitions:
Data quality is a measure of the condition of data based on factors such as accuracy, completeness, consistency, reliability, validity, and whether it’s up to date.
Data Quality is to measure the condition of data throughout different perspectives and different dimensions [3].
Accuracy
: How well does a piece of information reflect reality?
Completeness
: Does it fulfill your expectations of what’s comprehensive?
Consistency
: Does information stored in one place match relevant data stored elsewhere? Is everybody looking at the same data?
Timeliness
: Is your information available when you need it? Is data refreshed on time and the right cadence?
Validity
: Is the information in a specific format, does it follow business rules, or is it in an unusable format?
Uniqueness
: Is this the only instance in which this information appears in the database?
Other authors include more dimensions, I’ll go with these as an example, but I strongly recommend studying more about it.
Data errors can be propagated and, after a few years, can be hard to correct. That is why it needs to be paid attention to by businesses and data teams. We are now living in a data-driven era in which data-based decisions are taken every day. Many organizations are still doing their digital transformation to start gaining from the use of their data. But many who are already in this process want to build complex machine learning models without developing a good policy on data quality. The algorithms by themselves cannot do magic and fix issues in data unless they’re being trained for it.
How can an organization improve its Data Quality?
There is no one-size-fits-all rule. One thing you need to know is that you’re going to have any data issues eventually. You can’t always avoid it, but instead, you can create mechanisms that alert that an error has occurred. Test the data, start thinking about how you can guarantee the dimensions showed above. Is data accurate? Is it valid according to the business rules? Is it refreshed on the right cadence, or are you making decisions based on stale data?
Let the stakeholders know that errors can, and will, occur, but you have measures to identify and fix them. So you can still give your stakeholders more trust and make them feel more secure making decisions based on reports or even accept predictions from machine learning models.
If 10% of your data isn’t accurate, it can change the perception of the overall. If this 10% of dirty, poor quality data is considered in one decision without knowledge, the results can be highly affected.
The pipeline debt
It’s technical debt in data pipelines mostly due to the lack of tests and documentation. Traditional Software Engineering focuses on creating unit tests for the development of applications, but it is much more complex when dealing with data.
A Data Pipeline can extract and transform information from one or more sources (sometimes within the organization or from partners), then load it to another location. The code can be tested many times, but if the source data changes suddenly through an update to an upstream application, the schema of the data can change and break everything. Worst, if a bug is not detected and begins to populate a column with incorrect values, it may mess with downstream activities that depend on it.
So, dealing with Data Pipelines means constantly testing the code and testing the data continuously because it’s never known when something can change. An article from
Great Expectation
folks [4], an open-source project written in python, can help you create expectations and assertions for different datasets. There the authors explain more about pipeline debts and how this tool can help a Data Team improves its datasets quality. Below is an example of it:
The use of Great Expectations to define a column must lie between 60 and 75, at least 95% of the time. Image by
https://greatexpectations.io/
The
Great Expectations
tool is a Python package, installable via pip or conda, that allows data teams to create expectations and validations for every dataset. Expectation, a flexible, declarative syntax for describing the expected form of data, is the core abstraction. Expectations provide an excellent medium for communication, surfacing, and documenting latent knowledge about the shape, format, and quality of data used in exploration and creation. It is a powerful tool for testing when used in production.
Final Considerations
I hope that somehow I may have shown why it’s essential to have a Data Quality culture within an organization. Despite being a decimation error, the example of the myth behind Popeye shows that even minor errors can be hard to fix after their spread. Data Teams need to check its code continually, and most important, check the data.
The utopia would be defining quality checks for all datasets, especially for those on a critical decision-making basis. My recommendation is to determine what datasets are most important within the company and start creating quality checks. This tool Great Expectations can be helpful, but there are other tools out there, open-sources and paid ones.
Sometimes, even these quality checks, these assertions that check if the column values lie within a specific range, won’t be enough to guarantee the trust, and you’ll need to use anomaly/outlier detection techniques [5, 6]. Only the context you’re dealing with can tell you if it’ll be necessary to address these more advanced techniques. I will explain more about it in future posts.
References
The Science News-Letter. Spinach Over-Rated as Source of Iron Vol. 28, №749. Aug. 17, p. 110 (1935)
Hamblin, Terence J. “Fake.”
British Medical Journal (Clinical research ed.)
283.6307 (1981): 1671.
Pipino, Leo L., Yang W. Lee, and Richard Y. Wang. “Data quality assessment.”
Communications of the ACM
45.4 (2002): 211–218.
https://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a
Hodge, V.J., Austin, J. A Survey of Outlier Detection Methodologies.
Artif Intell Rev
22
,
85–126 (2004).
The Nutrition Source. Vegetables and Fruits. [online] Available at: <
https://www.hsph.harvard.edu/nutritionsource/what-should-you-eat/vegetables-and-fruits/
>
Data Science
Data Quality
--
--
1
Published in
hurb.labs
317 Followers
·
Last published
Jun 11, 2024
hurb.com
’s geekiest blog. reshaping how we travel
Follow
Written by
Vinícius Mello
70 Followers
·
196 Following
Passionate about technology, leadership, and martial arts
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
My Team Won the 2024 Big Data Bowl | by Matt Chang | Medium,"My Team Won the 2024 Big Data Bowl | by Matt Chang | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
My Team Won the 2024 Big Data Bowl
Matt Chang
·
Follow
11 min read
·
Mar 7, 2024
--
Listen
Share
Here’s some advice that we would give — both technical and non-technical — to future contestants. This post is
not
about the exact technical details and implementation of our specific model and metric. For those details see our
Kaggle notebook
, our accompanying
Github repo
, and listen to our re-
recorded presentation
.
Intro
I found the
Big Data Bowl
on
Kaggle
in mid-October, while searching for an interesting project to work on. I knew this was a project I could get behind; it entailed both interesting data (player tracking data) as well as a fun topic (I regularly watch football on Sundays). I texted three of my good friends with similar interests and we began our work.
Roughly four months later, we found ourselves at the 2024 NFL Combine to present our work to an audience of over 200 NFL team staff members, operations staff, media members, and vendors. It was a surreal experience getting to talk with club and league analytics professionals about how their teams could use our work. At the end of the day, we were
declared the grand finalist
, with the potential for our work to inspire the next NFL NextGenStats metric.
It was a fun and fascinating foray into the world of sports analytics, and I highly recommend it to those who have a love of data and a love of sports.
You don’t need to be a data scientist or a sports analyst
Before giving concrete advice, I want to mention that you don’t need to be a data scientist or a sports analyst to be competitive. My own background is in hardware engineering (chip design). One of my teammates works in Business Operations. My other two teammates work in machine learning and I know that sounds like an ace in the hole, but it wasn’t their machine learning background that made the difference.
What you do need are some core technical skills, namely the ability to code and the ability to visualize and process data (e.g. numpy, pandas if you work in python). Some knowledge of machine learning models does help, but there was at least one other finalist submission this year that didn’t use machine learning at all. I think the other useful technical skill is the ability to design experiments and have a habit of developing hypotheses and testing them.
As for sports knowledge, the winner of the 2020 Big Data Bowl was a team who knew nothing about American football. So intimate knowledge of the game of football is also not required, although I think it helps with motivation and communication.
OK, now some concrete advice. Here are 7 pieces of advice that can improve your team’s chances of winning the Big Data Bowl. I’ve provided an overview below, which you can use to navigate to different sections.
Overview:
1. Read Previous Submissions
2. Get really familiar with the data
3. Keep it simple
4. Verify your results
5. Save time for the write-up
6. Always be thinking of your audience
7. Be Humble and Professional
1. Read Previous Submissions
Before you dive into the data and starting furiously writing code, it is well worth your time to research previous Big Data Bowl submissions — winners as well as runner-ups. This serves several purposes:
Figure out what types of questions, metrics, and insights that the NFL is interested in and use that to calibrate your own ideas.
Study how teams presented their results and what types of visualization techniques they used. It’s very likely that you can piggy-back off of these ideas, since you will also be working with tracking data.
Calibrate what level of technical depth you should include in your submission. 2000 words is not a lot, so you’ll have to be very selective.
Another suggestion is to browse the discussion forums of previous years’ competitions, as previous participants have left a lot of useful code and kernels on those forums. Don’t reinvent the wheel.
2. Get really familiar with the data
Next, immerse yourself in the data. Being familiar with the data was critical to our success, because we knew what categories of data existed, what data we could use to test a hypothesis, and what data we could use to debug an issue. Knowing how much data you have is also helpful to determine if you have enough for your particular approach. For example, if you’d like to identify trends that lead to interceptions, you better have enough interception plays to work with.
Finally, the NFL gives you a filtered set of plays, based on what they want you to investigate. For example, since our year focused on tackling, we didn’t have any plays that resulted in an incomplete pass or an interception. Also, all of our tracking data on pass plays started a second before the catch — not at the snap. Knowing these caveats will be helpful when interpreting your results.
A few concrete suggestions:
Develop a way to visualize tracking data early. I used
this code
, which I found in the discussion forums of the 2023 Big Data Bowl. Tracking data in table format is incomprehensible, but seeing the dots move around on the field is an intuitive and easy way for you to visualize individual plays. I used this script often in the early days of testing ideas and debugging.
Visualizing tracking data is a must-have. This visualization comes from
this notebook
.
Visualize the remaining data. For example, how many plays are there? How many tackles and missed tackles are there? What’s the breakdown of run and pass plays? How many players are there of each position? How many plays resulted in penalties? Depending on your particular goal, there’s many ways to slice up the data to draw insights.
Find ways to visualize your remaining data. It’ll be useful later when you need to make quick decisions.
Invest in data cleaning. It’s likely that you’ll need to translate/rotate the data so that all offensive plays run in the same direction. Also there is likely erroneous data. We found examples where the ballcarrier’s ID was not one of the 22 players on the field. When digging deeper, I found that whoever had filtered the data confused Robby Anderson with another Robby. So pay attention and be willing roll up your sleeves and dig in. I usually like to say that “the data doesn’t lie”, but sometimes it is a little confused.
3. Keep it simple
Okay, you’ve become one with the data, scoured previous submissions, and now you’re ready to begin building. You’ve brainstormed a list of ideas with your team and have selected the most promising one.
Time to swing for the fences, right? No. This is a mistake that cost us dearly. In our first attempt, we tried to develop a complicated neural network model because we thought it would be cool. We almost went as far as building a neural network that took in a sequence of frames and tried to predict the next frame. Bad. No one in the NFL cares whether you trained a 52B parameter model that’s on the verge of AGI or whether you used logistic regression. What they care about are results and whether your metric/model produces any value for them.
I recommend starting with the simplest possible approach or model that you can think of and test whether it generates results make sense. Then build in complexity one step at a time, but only if that complexity helps improve the validity of your results.
For those who care, our neural nets were almost always overfitting. Part of that is because of a nasty data leakage bug I let slip through. But even after fixing that, reducing the number of parameters, simplifying model architectures, and using large amounts of regularization and data augmentation, I could not find a satisfactory point in the bias-variance tradeoff curve. My last neural net, a simple MLP, was actually OK, but you know what generalized better? XGBoost. Nine weeks of tracking data is simply not a lot of data for neural nets. Keep it simple, don’t reinvent the wheel, and focus on the results.
4. Verify your results
You have a model that produces results! Great, but now you need a way to ground those results and verify that they make sense. Not just to you, who have a vested interest that they make sense, but also to the NFL, who is not intimately familiar with your work.
There’s two categories of verification, both of which you should do:
Technical verification.
If you build a model, then test it on a held out test set to ensure that it generalizes. If you haven’t build a model but have uncovered a clever data trend, you still need to make sure that you haven’t fooled yourself. I recommend manually stepping through your code, at least for a couple examples, and hand-verifying all the math. Catching errors early on will save you a lot of time, and it is so easy for small bugs to sneak by when you’re in the zone and producing hundreds of lines of code.
Football verification.
Make sure that your results make football sense. This is super important for the judges, who will surely be asking the same questions. For example, for our metric, missed tackle opportunities, we made sure that the missed tackle opportunities also caught all the missed tackles, which by definition are a subset of missed tackle opportunities. We would watch plays that our model identified as missed tackle opportunities (both using our tracking data visualizations as well as finding highlight reels on YouTube) to make sure that our human brains agreed with our model’s judgement. We also ranked all defensive and offensive players by their rate of missed tackle opportunities or rate of generating missed tackle opportunities, respectively, and made sure that these matched our intuition. Lamar Jackson is the most elusive QB? Checks out. Fred Warner and Nick Bolton generate a lot of tackle opportunities and don’t miss often? Checks out. Now certainly, there will be surprises — and that’s also the value of data — but if your results are telling you something that doesn’t jive with intuition, you should investigate
why
, rather than just accepting it as truth.
5. Save time for the write-up
It’s tempting to work up to the deadline polishing and tweaking your model, but resist this temptation. Your work will be judged through the lens of your write-up, so even if you have the world’s best metric and model, if you can’t explain it in a way that the judges will understand, you might as well have not submitted.
We set an internal deadline to stop technical work (except simple checks/generating visualizations) two weeks before the deadline and focus on writing. Our team treated the write-up as if we were writing an academic publication (and my teammates are very good at those). Every image was critiqued and iterated on to make sure it was clear. Every sentence needed to have a reason for being there because 2000 words is so short. We also sent our write-up to friends, who had no idea about our work, to make sure that we didn’t fall victim to the
curse of knowledge
.
Some specific advice:
Study the rubric before you begin writing. Again, look at previous years’ winners to see what they did successfully.
Don’t focus on the technical details and implementation. The judges may not be technical.
The scoring rubric in our year only assigned 20% to the data science portion
, so as much as you may be attached to your work, you should only assign it 20% of the value.
I made this mistake and my teammates bailed me out.
Make sure your write-up answers the “so what?” question.
Make sure your images are easy to digest within seconds of looking at it. Make sure that, if someone just looked at your images and nothing else in the notebook, they would still have a sense of your main message.
Keep your images simple and easy to digest. Within seconds of looking at this chart, you can figure out what it’s trying to say, and it is much better than a big table of numbers.
6. Always be thinking of your audience
From the very beginning of the project, put yourself in the shoes of your audience. They may or may not be technical. They may be coaches. Former players. Data Analysts. AWS. NextGenStats. Media members. You have such a wide distribution of audience members to address that you can’t afford to focus on any one of them. The common thread that links them all together is that they love the sport of football and they want to know if you’ve uncovered something that they can use.
We started off our project by asking ourselves the question “if I were a coach, a data analyst, or even a player, what would I want to know? What would help me win games?”
In the end, we found a way to identify defensive mistakes that weren’t labeled as missed tackles, but things that we believed were worth capturing (e.g. defenders getting juked out, making weak/ambiguous arm tackles, or just being lazy). Every time we watched a football game, we’d hear commentators make comments about player X “being out of position” or “tackling with their arms” or the ballcarrier “getting away from X”. Our audience already intuitively understood our metric — we were just putting numbers to it.
Even more important, you need to be thinking about your audience when writing up your results and, if you make it as a finalist, putting together your presentation. Focus on the key things that make your work unique and valuable, and aggressively strip away any unnecessary details. Tell the story and show examples of how your work brings value and sheds insight on the game of football. One good exemplary play is infinitely more valuable than a big table of names and numbers that no one will read.
If you’ve made it as a finalist, then all the advice above applies even more. You’ll have six minutes to deliver your message to the audience, who have possibly sat through four other presentations in the last hour. If you’d like an example of a presentation, we re-recorded (and extended) the
presentation that we gave at the 2024 Combine
. We only had six minutes at the Combine, but this video is 11 minutes because we added an introduction to the Big Data Bowl as well as some additional detail about the use cases of missed tackle opportunities.
This simple slide from our presentation showed that (1) our model captured almost 90% of existing NFL labeled missed tackles (establishing credibility) and (2) identified 3566 new mistakes that weren’t previously known (the so what?)!
7. Be Humble and Professional
You haven’t solved football, and your audience knows football better than you. Make sure you acknowledge the limits of your work and how it can be improved together with the domain expertise that your audience has. Remember that while the Big Data Bowl is a fun side project for you, it is the literal day-to-day job of your audience. Make sure you approach it with the same professionalism as they do.
Have fun! Go Niners!
Big shoutout to my teammates,
Kat Dai
,
Daniel Jiang
,
Harvey Cheng
for all the hours they put in, outside of work. Also a big thank you to
Mina Lee
for her incredibly detailed feedback. Finally, thank you to Michael Lopez, Thompson Bliss, Ally Blake, and all the NFL staffers, who make the Big Data Bowl a reality.
Advice
Nfl Big Data Bowl
Data Science
NFL
Sports Analytics
--
--
Follow
Written by
Matt Chang
17 Followers
·
4 Following
A chip designer who became a software engineer
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Know your data better with Great Expectations | by Thorat Harshal | Globant | Medium,"Know your data better with Great Expectations | by Thorat Harshal | Globant | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Know your data better with Great Expectations
Thorat Harshal
·
Follow
Published in
Globant
·
8 min read
·
Feb 10, 2023
--
Listen
Share
In today's world, data has become a crucial element for organizations and individuals in making informed business decisions. Data retrieval and generation have become common practices. The most crucial part is ensuring that our data is correct and precise, as other business decisions are entirely dependent on the same. Hence, in this blog, we will see how
Great Expectations
will help us get more confidence in our data and identify use cases for the same.
What is Great Expectations?
Great Expectations is an open-source Python library that provides a flexible and powerful framework for data quality checks and tests.
It helps data teams ensure that their data is accurate, consistent, and meets the expected standards.
It is a data quality framework that helps data engineers and scientists build trust in their data.
It provides a simple and intuitive way to automate data quality checks, increase transparency in the data pipeline, and ensure that the data meets the required quality standards.
One of the key features of Great Expectations is its ability to perform automated data quality checks. It allows data engineers to define a set of expectations for their data and automatically check the data against these expectations regularly. This helps to catch any data quality issues early on, allowing data engineers to fix them before they become a problem.
Another advantage is its user-friendly interface. The framework has a simple and intuitive API that makes it easy to define expectations and perform checks. Additionally, it provides clear and detailed reports of the data quality check results, making it easy for data engineers to understand the issues and how to fix them.
One of its unique aspects is its ability to integrate with a wide range of data storage solutions. This includes databases, cloud storage solutions, and file systems. This allows data engineers to perform data quality checks on the data wherever it resides without having to move it to a separate environment.
Why use Great Expectations?
Data quality
is a critical concern for organizations of all sizes. Inconsistent or inaccurate data can lead to incorrect conclusions, ineffective decision-making, and even business failures. Great Expectations helps organizations prevent these issues by automating the data quality process and making it easy for teams to identify and address data quality issues. With Great Expectations, teams can easily create data quality checks and tests, automate them and monitor their performance over time.
Great Expectations is used to ensure data accuracy, consistency, and reliability. It helps organizations prevent data quality issues by automating the data quality process, making it easy to identify and address data quality issues, and ensuring that data meets specified standards. By using Great Expectations, teams can save time, reduce the risk of human error, and make informed decisions based on accurate data. Additionally, it provides a centralized repository of data quality expectations and test results, enabling teams to monitor data quality over time.
How Great Expectations Works
Great Expectations works by defining data quality expectations in terms of a series of tests and checks. These expectations can be defined for individual datasets or entire data pipelines and can be as simple or as complex as needed. When data is loaded into Great Expectations, the library automatically performs the defined checks and tests to determine if the data meets the specified expectations.
Source: great expectations official documentation
As shown in the above diagrams, once data is loaded into Great Expectations, it will provide high-quality data based on defined expectations; in addition to that, it will provide data quality reports with alerting mechanism based on data quality
Key Features
Some key features identified from Great Expectations are the following.
Expectations
""
Expectation
"" is a specific criterion or requirement that data must meet to be considered accurate and reliable. Expectations are defined and stored, and the library uses these Expectations to automatically perform checks and tests on incoming data. If the data does not meet the defined Expectations, alerts and reports are generated, allowing data teams to quickly identify and address any issues. For example, to assert that you want the column
employee_count
to be between 1 and 6, you can say:
expect_column_values_to_be_between(
column=""employee_count"",
min_value=1,
max_value=6
)
Great Expectations then uses this statement to validate whether the column employee_count in a given table is indeed between 1 and 6 and returns a success or failure result. The library currently provides several highly expressive built-in expectations and allows you to write custom Expectations.
Automated data profiling
Automated data profiling
in Great Expectations is the process of automatically analyzing and summarising the key characteristics of data, such as its structure, distribution, and content. The purpose of automated data profiling is to help data teams quickly and easily understand the data they are working with, identify any potential quality issues, and ensure that the data meets the specified standards.
For example, using the profiler on a column
employee_count
that only contains integer values between 1 and 6 with an average value of more than 95% as data quality metrics, Great Expectations automatically generates this Expectation as below:
expect_column_values_to_be_between(
column=""employee_count"",
min_value=1,
max_value=6,
mostly=0.95
)
Great Expectations then uses this statement to validate whether the column employee_count in a given table is indeed between 1 and 6 with average calculation metrics as 95% and more. This allows you to quickly create tests for your data without having to write them from scratch.
Data validation
Data validation
in Great Expectations refers to verifying that data meets a set of specified standards, expectations, or criteria. This process helps to ensure data accuracy, consistency, and reliability, and is a critical step in maintaining data quality. Data validation is performed by defining expectations, or a series of tests and checks, that the data must meet. The expectations can range from simple to complex and can be defined for individual datasets or entire data pipelines.
By automating the data validation process, Great Expectations helps organizations save time, reduce the risk of human error, and make informed decisions based on accurate data. Additionally, it provides a centralized repository of data quality expectations and test results, enabling teams to monitor data quality over time.
Checkpointing
A
Checkpoint
in Great Expectations is a saved snapshot of the library's data quality checks and tests results. Checkpoints are stored in the Great Expectations repository and provide a historical record of data quality over time.
Checkpointing in the phase of data validation to analyze the process of Data Quality in depth for Great Expectations
As shown in diagram we can create custom checkpoints in each phase of deployment and quality check process to get detailed insights of data validation
Data Docs
Data docs
in Great Expectations are documentation that provides detailed information about the data, including its structure, content, and quality. The purpose of Datadocs is to provide a centralized repository of information about data that can be easily accessed and understood by data teams.
Data docs in Great Expectations are generated automatically as part of the data quality checks and tests performed by the library. The Datadocs provide detailed information about each column in a dataset, including data type, distribution, missing values, and more. Additionally, the Data docs summarize the results of the data quality checks and tests, making it easy for data teams to understand the quality of the data.
Bird's eye view for expectations validation results. Source —
Great Expectations
As shown above Data docs in Great Expectations help data teams make informed decisions about the accuracy, consistency, and reliability of their data with statistics for the runs per individual datasets
Example Use Case
A retail company that collects customer purchase data and wants to maintain its accuracy and consistency can make use of Great Expectations by setting expectations based on their key performance indicators (KPIs) and running them as validations within the Great Expectations library. For instance, the following code demonstrates the creation and validation of data through Great Expectations.
import great_expectations as ge
# Load the dataset into Great Expectations
df = ge.read_csv('customer_purchase_data.csv')
# Define expectations for the dataset
expectations = [
ge.expect_column_values_to_be_unique('customer_id'),
ge.expect_column_values_to_be_greater_than('purchase_amount', 0)
]
# Validate the dataset against the expectations
results = ge.validate(df, expectations)
# Print the results of the validation
print(results)
In this scenario, Great Expectations is used to verify the accuracy of customer purchase data. First, the data is loaded into the program using the
read_csv
method. Then, two expectations are established — that customer IDs are distinct and that the purchase amount is higher than zero. Finally, the
validate
method is utilized to evaluate the customer purchase data against these expectations, and the outcome of the validation is displayed in the console.
Evaluating Great Expectations
Let's see now the pros and cons of Great Expectations, as well as some possible alternatives.
Benefits of Using Great Expectations
There are several benefits to Great Expectations:
It provides a flexible and powerful framework for data quality that can be easily adapted to meet the specific needs of any organization.
It automates the data quality process, saving teams time and reducing the risk of human error.
It provides a centralized repository of data quality expectations and test results, making it easy for teams to monitor data quality over time.
Limitations of Using Great Expectations
Complexity: Setting up and using great expectations can be challenging for users without technical backgrounds.
Scalability: The library may struggle with large data sets or rapidly changing data or slow-changing data, impacting its performance.
Integration: Great Expectations has several built-in integrations, but integrating it with certain systems or workflows can be difficult.
Limited visualizations: Users who need more extensive visual analysis may find the limited visualization options offered by great expectations to be restrictive.
There are alternative tools available in the market that provide similar capabilities as Great Expectations, including:
Trifecta
: a tool for raw data preparation and cleaning
Talend
: a data integration and management platform
Datarobot
: an AI and machine learning automation platform
Alteryx
: a self-service analytics platform.
Dataiku
: a collaborative platform for data science, AI, and machine learning.
KNIME
: an open-source platform for data analytics, reporting, and integration.
Conclusion
G
reat Expectations is a valuable tool for data teams looking to ensure the accuracy and consistency of their data. With its flexible framework and powerful capabilities, Great Expectations can help organizations prevent data quality issues and make informed decisions based on accurate data. Whether you're working with large or small datasets, Great Expectations can help you manage data quality effectively and efficiently.
Data Engineering
Data Quality
Great Expectations
Python
Data
--
--
Published in
Globant
1.8K Followers
·
Last published
11 hours ago
Our thoughts as a strategic disruptor in business and cognitive transformation. Visit us at
www.globant.com
Follow
Written by
Thorat Harshal
35 Followers
·
2 Following
Mentor @
topmate.io/harshal_thorat
| Techie | Data Architect | Tech Blog writer | Traveller
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
OMSCS — Big Data for Health Informatics | by Jonathan Lao | Medium,"OMSCS — Big Data for Health Informatics | by Jonathan Lao | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
OMSCS — Big Data for Health Informatics
Jonathan Lao
·
Follow
3 min read
·
Jan 13, 2021
--
Listen
Share
Atlanta is a good TV show.
Overview
This was the first class I’ve taken that left me with an overall negative impression. Other reviewers described this class as a sort of ‘bootcamp’ of a firehose of different technologies, rather than a traditional ‘academic’ course about big data. Ultimately while I agree with that characterization, the class turns out to not be a very good bootcamp.
Practical Tips
It’s true that you will touch a lot of different technologies like Hadoop, Pig, Hive, Spark, etc. But ultimately, I think the key to succeeding in the class is being comfortable with the basics of Python, SQL, machine learning (like understanding how to train and test a regression model), and Scala. If you’re comfortable with at least three of the four concepts, you’re probably in good shape. If not, recognize you’ll be spending more time than average by virtue of getting up to speed on new syntax.
The class Piazza discussion threads for each assignment will ultimately answer any question that you may possibly have about each homework. One tactic can be to wait a bit after an assignment is first released so the early go-getters hash out all the nitty details and clarifications. If instead you want to finish the homework early, know that there will be many other students ready and willing to work with you.
The sunlabs are the single best resource that will help with the homework. Following the corresponding lab step-by-step will often have >50% overlap with each homework. I tried ‘front-loading’ the labs ahead of time, but ultimately I think just doing the corresponding lab alongside the homework is the best use of your time.
Knowing the basics of Docker can be useful. I followed along a LinkedIn Learning class which we get a free subscription to as students.
Individual grading for each homework can be very hit or miss. In general, the local tests are rather simplistic, so passing them is a low bar. Sometimes passing them is sufficient to get a 100 in the end. Other times, they are wholly inadequate. And in one instance, I was passing the local tests despite knowing that I had an incorrect implementation of the algorithm, and I still got a 100.
Group projects can be so hit or miss. A good group can really make your life easier at the end of the semester. I don’t have any good tips as to how to find a good group, but in general all the slackers tend to drop out of the class.
To my knowledge, as long as you hit every deliverable for the project, you will get an A.
The assignments take up the first half of the course and the project the last half. Overall, this is a much more front-heavy course.
Overall grading for the class is very generous. Know that if you stick with the class to the end and at least turn in every deliverable, you’re very likely to get an A.
In light of the lenient class grading, don’t stress too much about the exam. Some of the questions are easy if you have a good grasp of the basic machine learning algorithms. Others felt like obscure trivia questions.
Georgia Tech
Omscs
Computer Science
Big Data
--
--
Follow
Written by
Jonathan Lao
33 Followers
·
3 Following
I sometimes do tech things.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Partnering for data quality. How two groups at Microsoft teamed up… | by Vlad Rișcuția | Data Science at Microsoft | Medium,"Partnering for data quality. How two groups at Microsoft teamed up… | by Vlad Rișcuția | Data Science at Microsoft | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Partnering for data quality
Vlad Rișcuția
·
Follow
Published in
Data Science at Microsoft
·
9 min read
·
Aug 14, 2020
--
1
Share
Author
Vlad Rișcuția
is joined for this article by co-authors
Wayne Yim
and
Ayyappan Balasubramanian
.
Why data quality?
Data quality is a critical aspect of ensuring high quality business decisions. An estimate of the yearly cost of poor data quality is $3.1 trillion per year for the United States alone, equating to approximately 16.5 percent of GDP.¹ For a business such as Microsoft, where data-driven decisions are ingrained within the fabric of the company, ensuring high data quality is paramount. Not only is data used to drive, steer, and grow the Microsoft business from a tactical and strategic perspective, but there are also regulatory obligations to produce accurate data for quarterly financial reporting.
History of DataCop
In the Experiences and Devices (E+D) division at Microsoft, a central data team called IDEAs (Insights Data Engineering and Analytics) generates key business metrics that are used to grow and steer the business. As one of its first undertakings, the team created the Office 365 Commercial Monthly Active User (MAU) measure to track the usage and growth of Office 365. This was a complicated endeavor due to the sheer scale of data, the number of Office products and services involved, and the heterogenous nature of the data pipelines across different products and services. In addition, many other business metrics, tracking the growth and usage of all Office products and services, also needed to be created.
--
--
1
Published in
Data Science at Microsoft
6.6K Followers
·
Last published
2 days ago
Lessons learned in the practice of data science at Microsoft.
Follow
Written by
Vlad Rișcuția
199 Followers
·
4 Following
Engineering Leader @Microsoft and author
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"DEEQU, I mean Data Quality. We say how we can turn the world upside… | by Ajith Shetty | Medium","DEEQU, I mean Data Quality. We say how we can turn the world upside… | by Ajith Shetty | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
DEEQU, I mean Data Quality
Ajith Shetty
·
Follow
5 min read
·
Nov 20, 2021
--
1
Listen
Share
Photo by
Stephen Phillips - Hostreviews.co.uk
on
Unsplash
We say how we can turn the world upside down with the power of DATA. And it is rightly so.
But a small mistake in the data could send the same world for a toss.
A comma in an Amount column would make a lot of difference.
Data quality is something which you can never ignore and it should always be a part of your pipeline FOR THE LIFETIME.
At this moment of time, the use of Spark is enormous. No pipeline would be complete without having Spark in between.
We need a framework which can talk to Spark directly and it can run run on top of Big data.
Enter, Amazon Deequ.
Introduction
Deequ is a library built on top of Apache Spark for defining “unit tests for data”, which measure data quality in large datasets.
Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on
GitHub
,
readthedocs
, and
PyPI
.
Source
What does it do
Amazon Deequ would help you in:
Metrics Computation:
You can use Deequ to get the quality metrics like maximum, minimum, correlation, completeness etc. Once the metrics are calculated you can store the data in S3 to analyse at later point.
Constraint Verification:
You may define the constraint verification and the Deequ will generates the data quality report.
Constraint Suggestion:
Well Deequ is smart enough to generate automated constraints based on the data you define.
source:
https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/
Deequ reads the data directly and runs the Constraints verification directly on top of the Spark and generates the Data Quality report.
User can provide the constraints to verify or the Deequ could suggest it for you.
Once the Metrics or report is generated, the output could be saved in a local file or S3, which can be queried at later point.
Supported list of Analyser
deequ/src/main/scala/com/amazon/deequ/analyzers at master · awslabs/deequ
Deequ is a library built on top of Apache Spark for defining ""unit tests for data"", which measure data quality in large…
github.com
How does it help
Using Deequ, you can create a pipeline to validate the completeness or the missing of your data.
You can provide the constraints to verify or let the Deequ define it for you.
You can use Deequ for Anomaly detection.
Deequ supports the Repository where you can store your metrics results and query later.
Deequ can profile all your columns.
Demo
Setup:
We will be using Databricks community edition for easy setup and demo.
https://community.cloud.databricks.com/
install the library.
2. Import the Classes
3. Setup the Dummy Data
4. Lets create the repository in the local file system.
It can be S3 location as well.
We need to pass the tag, which under which each of your run metrics will be stored.
5. Lets create the Verification Suite.
Where we define all the checks we want to perform.
In our example, we want to check:
a. Completeness on id and name
b. Record count should be greater than 5.
c. citizenship should be either YES or NO
d. The travel_count should not be negative.
And here you can see the result.
Our check has failed for:
record count, We expected it to be greater than 5.
name column contains null
citizenship, contains a value other than YES and NO
Checks have succeeded for:
travel count which is not lesser than 0 for any records
ID column never being null.
Now let’s save the data and query it.
Give the useRepository method and pass the repository Name.
Now either you can query on top of repository for a given column.
Or you can convert the data to Dataframe.
Bonus: You can run the profiling for all the columns by just 1 command.
You may find the above code example here:
GitHub - ajithshetty/DeequDemo
You can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or…
github.com
Reference:
GitHub - awslabs/deequ: Deequ is a library built on top of Apache Spark for defining ""unit tests…
Deequ is a library built on top of Apache Spark for defining ""unit tests for data"", which measure data quality in large…
github.com
deequ/src/main/scala/com/amazon/deequ/examples at master · awslabs/deequ
Deequ is a library built on top of Apache Spark for defining ""unit tests for data"", which measure data quality in large…
github.com
Test data quality at scale with Deequ | Amazon Web Services
In this blog post, we introduce Deequ, an open source tool developed and used at Amazon. Deequ allows you to calculate…
aws.amazon.com
Ajith Shetty
Bigdata Engineer — Bigdata, Analytics, Cloud and Infrastructure.
Subscribe
✉️ ||
More blogs
📝||
LinkedIn
📊||
Profile Page
📚||
Git Repo
👓
Interested in getting the weekly newsletter on the big data analytics around the world, do subscribe to my:
Weekly Newsletter Just Enough Data
Data Quality
Deeque
Data Enginnering
--
--
1
Follow
Written by
Ajith Shetty
345 Followers
·
38 Following
Bigdata Engineer — Love for BigData, Analytics, Cloud and Infrastructure. Want to talk more? Ping me in Linked In:
https://www.linkedin.com/in/ajshetty28/
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"The Test Pyramid, Data Engineering, and You! | by David O'Keeffe | Cognizant Servian","The Test Pyramid, Data Engineering, and You! | by David O'Keeffe | Cognizant Servian
Open in app
Sign up
Sign in
Write
Sign up
Sign in
The Data Engineering Testing Series
The Test Pyramid, Data Engineering, and You!
David O'Keeffe
·
Follow
Published in
Cognizant Servian
·
9 min read
·
Feb 10, 2021
--
1
Listen
Share
Writing your first test can be a daunting moment. Every test comes with a cost and some more than others. You will find yourself asking questions such as:
What do I test?
Do I test the whole system?
Do I test every function?
How many tests do I need?
How many failure scenarios am I satisfied with? If any?
Unfortunately, I can’t explicitly tell you the answer to what is right or wrong for your particular scenario, but hopefully, this post can help you out. I’m going to walk you through the process of creating tests for a simple data pipeline in Julia. Why Julia you ask?
Because it’s cool.
The rest of the series continues as below:
Part 1: Why Great Data Engineering Needs Automated Testing
Part 2: The Keys To Unlock TDD For Data Engineering
Part 3: The Test Pyramid and Data Engineering (with Julia)
Part 4: What is Data Quality Really?
The Scenario
In our fictional scenario, we work for a cryptocurrency platform that helps investors track their trades. As part of our platform we’ve hired an external vendor to deliver the following timestamped Arrow DataFrame to us at some regular interval:
20200114_151100_new_investors.arrow
DataFrame
(
names
= String[],
countries
= String[],
net_worth
= Int64[],
holdings
= Float64[],
y
= Int64[],
z
= Int64[],
id
= String[]
)
The boss upstairs is
chomping at the bit
to know how many people from each country are in this DataFrame so they can focus their marketing efforts. They want to keep track of how the number of new investors per country changes over time.
Our data is luckily being delivered in a nice structured format (being Arrow), the timestamp is in UTC, and it should only contain new investors, thus we should not have to worry about deduplication, time zone issues, or deserialization.
Choosing your test scope
Before we jump in, let’s gather an understanding of the theory behind
test pyramids
. Simply put, there are several levels of testing, and as you move up the levels, the tests become increasingly flaky and expensive.
Take building a table for example
, the legs, the bolts, the brackets, and the tabletop are the individual parts, and you then put them together, eventually to form a whole system.
To test this table, first, you can look at all the parts and make sure they are structurally sound. This would be a unit test, it’s quick and easy to do. You can then start putting the parts together, forming a larger system, and testing that they also work together. Perhaps you pull on the legs to make sure they’re attached to the bolt and the bracket. An integration test, which is a slightly more labour intensive exercise. Finally, you put the tabletop on and make sure you can stand on top of the entire table. The final end-to-end test, and most expensive.
The basic philosophy behind test pyramids is that unit tests are superior to
end-to-end tests
as they are much faster, have obvious failure points, and are reliable. Thus you should invest in having lots of unit tests, a few integration tests, and even fewer end-to-end tests. In our table scenario, if we’re confident that all the parts are structurally sound and they fit well together, we probably don’t need to stand on it much. Likewise, if our end to end test fails, we don’t know whether it’s the bolts, or the bracket, or any other piece that’s the problem until closer inspection.
Pivoting back to data testing, we should look at the levels we can test it in a slightly different way to software. This is because instead of three levels of tests (unit, integration, and end-to-end)
you usually have five
. These levels are highlighted by layers of the dotted lines in the diagram below.
Full system, including client
Pipeline, including service
Multiple jobs
Single job
Unit/component
In this scenario, we have an application that interacts with a microservice, that streams data to a pipeline with several jobs, which eventually populates a database that the service returns a response with. This pattern is typical of a machine learning pipeline for recommendation engines.
One of my favourite resources on this subject is
Lars Albertsson
(it’s also his diagram above). He suggests that with data it’s only worthwhile testing at the job to pipeline level. His argument being that unit tests are too volatile because ‘the data’ in data pipelines is often rapidly changing. This level of testing works because each job in a data pipeline is essentially a black-box function, free from external factors, so it shares many of the same advantages of unit testing.
Pure data pipelines are free from external factors
However, let us consider our scenario again, this is where I prefer to follow the advice of Kent Beck, one of the leaders in TDD thinking.
I get paid for code that works, not for tests, so my philosophy is to test as little as possible to reach a given level of confidence (I suspect this level of confidence is high compared to industry standards, but that could just be hubris). If I don’t typically make a kind of mistake (like setting the wrong variables in a constructor), I don’t test for it. I do tend to make sense of test errors, so I’m extra careful when I have logic with complicated conditionals. When coding on a team, I modify my strategy to carefully test code that we, collectively, tend to get wrong. — Kent Beck on
Stack Overflow
The scope that’s right for me
Immediately upon looking at our task we can see we got three tasks in our pipeline and that it should only consist of one job:
Read in an Arrow file as a DataFrame (unit test)
Parse the timestamp and perform the aggregation (unit test)
Append the new data to an existing DataFrame (unit test)
In a real-life scenario, where the disk could be an object store (S3, ADLS Gen 2, Cloud Storage), our system would also have to address the following issues:
How do we keep track of what Arrow files we have already consumed? (unit/integration test)
How do we manage different versions of our country count history DataFrame? (unit/integration test)
What are we going to use for compute? (integration-test)
How are we going to trigger (orchestrate) the compute to do our job? (system/integration-test)
How are we going to ensure the integrity of the data we’re consuming? (unit test)
What would we do if the data did not arrive when expected? (integration-test)
How do we recognize and recover from failures? (unit/integration test)
How do we deliver the data to the dashboard? (integration-test)
How do we keep our data safe? (system/integration-test)
We don’t have the time to address all these issues, so I’m going to assume that our job will be running inside a container that is triggered manually, and for our MVP, generating the aggregated dataset is enough. I will use storage inside the container as a proxy for the object-store.
This dramatically reduces the scope of what I think I need to test. Remember, we get paid for code that works, producing value at the end of the day is all that matters. With this in mind, I’m going to go with five tests to get started quickly:
Unit
test to aggregate the data to perform the country counts
Unit
test to parse the DateTime from the filename
Unit
test to add that DateTime as a new column to the aggregated dataset
Integration
test to make sure that all three units work together (job level)
Manual
test to make sure the Julia container runs to completion when triggered (system level)
Notice how I am not going to unit test whether I am reading and writing the data correctly. I also skipped unit testing whether the DataFrame append command works. I am going to assume that the libraries that are doing these functions for me already work. Additionally, I will get test coverage of this with the integration test. I opted for a manual test of the container because Medium articles don’t come with CI/CD pipelines.
Setting up the project
The first thing we will have to do is set up the repository structure. In Julia, you must create a folder with the intended name of the package, navigate to that folder, then use
the REPL to activate the environment
. You must then create the following two scripts to enable the package and tests.
<package_name>
/src/
<package_name>
.jl
<package_name>
/test/runtests.jl
Julia requires these files with these specific names for it to work. At the end we have the following directory structure:
To help split up the tests into their own scripts I am employing the use of the
Jive
package. This will allow me in the future to add extra scripts in my test folder that can be ignored when running tests. It also makes the
runtests.jl
file quite succinct.
The tests
Since this is the first run I will only write tests for the happy path, that is, the form of what I expect the data to be. So lets start by with the unit tests:
You will notice a few things here:
Each of the unit tests we identified corresponds to functions in the application.
The tests are extremely simple and exclude unnecessary columns.
Creating DataFrames in Julia is quite a bit more succinct than in other languages. Simply declare the column name as the argument.
Now onto the integration test, which is at the job level. Here, we produce files and save them to disk, testing not only that we read the data correctly, but that we write the right data back. I also threw in a few dummy columns to the input to make sure that they didn’t impact the desired result.
Here I write out the input and the expected output to disk (lines 23–24), call my main function (line 28), make my assertion (line 38), then clean up the results. I couldn’t find a neat way in Julia to implement the usual test setup and teardown functions that are available in suites such as pytest, so if the test failed for whatever reason, the files remain on disk (if you know how to do this please leave a comment below).
The Country Count Application
As we have planned out our tests, we now want to match our functions with the functionality we are seeking to achieve.
This “job” script contains six different functions. Notice how I could have put all this code inside the main function easily enough, but to test the individual pieces we have to split them up into their own functions.
Finishing Touches
Now let’s create our final manual test, which will be just running the test suite to completion. This will test whether we’ve set up our Julia project correctly.
To run simply navigate to the
CountryCountJob
folder in your console then run
docker-compose run julia-data-testing
. Docker will take care of most of the heavy lifting, and we will be left with the following output.
Beautiful!
The devil of structural coupling
For those savvy software architectural experts out there, you would have noticed that we are coupling the structure of our program to our tests. This is generally considered to be an anti-pattern in software development, as now you need to change your tests when you change your program, which of course contributes to test fragility.
However, keep in mind that we are working with data pipelines, and while it shares similarities with software projects, it’s not
necessarily
the same thing. Remember at the job level, we are essentially working with singular black-box functions, thus we are not dealing with an intense amount of coupling. If it is a deal-breaker for you though, you may forgo the unit tests and just have an integration test (Lars’s suggested pattern).
The main contributor to test fragility in data is usually when jobs share the same changing data source. This is where we bump into one of the main sticking points of this approach…
when the real data changes, how do we make the right tests break?
That will be next time.
Thanks for taking the time to get this far and please feel free to leave any comments below or reach out to me on
LinkedIn
.
The Data Engineering Testing Series
Part 1: Why Great Data Engineering Needs Automated Testing
Part 2: The Keys To Unlock TDD For Data Engineering
Part 3: The Test Pyramid and Data Engineering (with Julia)
Part 4: What Is Data Quality Really?
Part 5: Machine Learning Is The Future Of Test Data
Data Science
Data Engineering
Dataops
DevOps
Data
--
--
1
Follow
Published in
Cognizant Servian
1.93K Followers
·
Last published
Aug 14, 2023
Cognizant Servian was formed on the deep expertise of two acquisitions: Servian and Contino. Together, we are experts in innovative data and analytics, digital, customer engagement and cloud solutions to evolve your competitive advantage.
Follow
Follow
Written by
David O'Keeffe
323 Followers
·
61 Following
David is a Databricks Solutions Architect whose expertise is in DataOps and Platform Engineering.
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
dbt: How We Improved Our Data Quality by Cutting 80% of Our Tests | by Noah Kennedy | Better Programming,"dbt: How We Improved Our Data Quality by Cutting 80% of Our Tests | by Noah Kennedy | Better Programming
Open in app
Sign up
Sign in
Write
Sign up
Sign in
dbt: How We Improved Our Data Quality by Cutting 80% of Our Tests
How to cut out all your bad tests and fix your data quality framework
Noah Kennedy
·
Follow
Published in
Better Programming
·
5 min read
·
Jan 25, 2023
--
1
Listen
Share
Photo by
Tolga Ulkan
on
Unsplash
Introduction
Testing the quality of your data is an important aspect of any mature data pipeline. One of the biggest blockers to developing a successful data quality pipeline is aggregating test failures and successes in an informational and actionable way. However, ensuring actionability can be challenging. If ignored, test failures can clog up a pipeline and create unactionable noise, rendering your testing infrastructure ineffective.
Producing a data quality framework that allows stakeholders to take action on test failures is challenging. Without an actionable framework, data quality tests can backfire — one failing test becomes two becomes ten and suddenly you have too many test failures to act on any of them.
Recently, we overhauled our testing framework. We cut down the number of tests by 80% to create a more mature framework that includes metadata and emphasizes actionability. Our system for managing data quality is a three-step process described below:
Leveraging the contextual knowledge of stakeholders, writing specific, high-quality data tests, and perpetuating test failure results into aliased models for easy access.
Aggregating test failure results using Jinja macros and preconfigured metadata to pull together high-level summary tables.
Building views on top of the base table to split tests by owner or severity and creating visualizations using our tool of choice.
It should be noted this framework is for dbt v1.0+ on BigQuery. Small adaptations are likely necessary to make this framework run on a different setup.
Specific, High-Quality Data Tests
When we talk about high-quality data tests, we aren’t just referencing high-quality code and the informational quality of our testing framework and their corresponding error messages. Originally, we theorized that any test that cannot be acted upon is a test that should not be implemented. Later, we realized there is a time and place for tests that should receive attention at a critical mass of failures.
We only needed a higher specificity system: tests should have an explicit severity ranking associated with them, equipped to filter out the noise of common but low-concern failures. Each test should also mesh into established
RACI
guidelines that state which groups tackle what failures and what constitutes a critical mass.
To ensure that tests are always acted upon, we implement tests differently depending on the user groups that must act when a test fails. This led us to have two main classes of tests — data integrity Tests (called
generic tests
in dbt docs) and context-driven tests (called
Singular Tests
in dbt docs), with varying levels of severity across both test classes.
Data integrity tests (generic tests) are simple — they’re tests akin to a uniqueness check or not null constraint. These tests are usually actionable by the data platform team rather than subject matter experts. We define Data Integrity tests in our YAML files, similar to how they are
outlined by dbt’s documentation on generic tests
. They look something like this:
Example data integrity tests in a YAML file — the alias argument is an important piece that will be touched on later
Context-driven tests are more complex and look a lot more like models. Essentially, they’re data models that select bad data or records we don’t want, defined as SQL files that live in the
dbt/tests
directory. An example is shown below:
The above test selects all patients with a birth date before 1900. Due to data rules, we have about maximum patient age
Importantly, we leverage
test aliasing
to ensure our tests all follow a standard and predictable naming convention; our naming convention for data integrity tests is
table_name_
_column_name_
_test_name
, and our naming convention for context-driven tests is
ad_hoc_
_test_name
. Finally, to ensure all of our tests can be aggregated, we modify the
dbt_project.yaml
and
set the
store_failures
tag to
TRUE
, thus persisting test failures into SQL tables.
At this point in development, we have data integrity tests defined in the YAML and context-driven tests defined as SQL files. Tests are specific, actionable, and realistic; each comes with an idea of the severity and a group of users who care when it fails. All of our tests are aliased according to a specific naming convention so that we know the table names they will put data into, and we have modified our dbt project config to set
store_failures
true for all tests.
Test Aggregation Using Metadata
Our next step is to define test metadata for each of our tests. The reason for this is twofold. First, we want to ensure that we can attach a description and a more human-readable name to the test in later visualization steps. Second, having a metadata file allows us to attach all sorts of extra information to tests: who owns the test, how severe it is, and if the test is active or inactive, to name a few.
Our metadata is stored in a
seed file
. The only required field here is the
test_alias
, which acts as a primary key to link the metadata to the name of the test failures table. We also include the test severity, the test owner, a test description, and several other fields that act as filters for future aggregation tables.
After defining our metadata Seed file, we begin aggregating our data. We aggregate our data by defining a base model that joins our test failure results (now stored in a separate schema) with the metadata we defined. Below is an example of what that code looks like:
Example metadata + test failure aggregation base model
Here’s some key components:
We materialize our base model as incremental, set
full_refresh
to
false
within the
dbt_project.yml
, and partition our table by date to ensure that we keep historical data.
We use BigQuery, which allows
wild card selectors
and makes our life much easier. If you’re using a different framework, you most likely need to write a loop using Jinja.
Since we have an expected naming convention, we can split the
test_alias
to get components like table name or column name if we desire.
Now that our base model is developed, we have a central point of truth that aggregates all of our data tests into one location, complete with metadata that gives more insight into the test and who owns it. Our final step is leveraging our base table to gain added insights from our tests.
Finishing Touches and Conclusions
With our finalized data quality base table, many other options exist for cleaning up our framework or creating visualizations. Our team uses the base table in a few main ways.
First, we create views on top of the base table that filter down by test owner. Test noise is the biggest risk to the success of a quality framework. Creating specific views is like giving each team a magnifying glass that lets them zoom into only the tests they care about. We also have a dashboard, currently, in Google Looker Studio, that shows historical test failures with filters to let users magnify high-severity tests and constructs machine-composed example queries for users to select failing records. When a test fails, a business analyst can copy and paste a query from the dashboard and get all the relevant information.
As with any framework, it’s always a work in progress — we still encounter issues with noise in our tests and still struggle to wrangle our users to care when a test fails. However, we’ve found that this data framework works exceptionally well at enabling data users to create and deploy their own tests. All they need to do is submit a pull request with SQL code that flags bad data and writes one line of metadata.
Dbt
Data Engineering
--
--
1
Follow
Published in
Better Programming
220K Followers
·
Last published
Nov 10, 2023
Advice for programmers.
Follow
Follow
Written by
Noah Kennedy
418 Followers
·
522 Following
DE @ Meta. Previously - Tempus AI. All opinions are my own. Easily excited, mainly by topics like dbt, endurance sports, pour-overs, and biotech trends.
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Data quality in Google Cloud BigQuery and
Data Lake using Great Expectations. | by Mariusz Kujawski | Medium","Data quality in Google Cloud BigQuery and
Data Lake using Great Expectations. | by Mariusz Kujawski | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data quality in Google Cloud BigQuery and
Data Lake using Great Expectations.
Mariusz Kujawski
·
Follow
6 min read
·
Aug 30, 2022
--
Listen
Share
In modern data solutions, it’s required to deliver data quality for solutions like AI, ML, data lakes and modern analytics. For many years companies, BI teams and data engineers had to find the best way to provide data with good quality. It’s not a trivial task to do. We know that for business customers and data consumers it’s important to have correct data in their dashboard to draw the right conclusions about the business that they are responsible for. To achieve this, data engineers should implement processes responsible for correct data
processing, data cleansing and data quality metrics at the end. One of the interesting ways to achieve this is the integration of Great Expectations with the GCP environment that I’ll try to explain in this article.
What is it?
Great Expectations is an open-source data quality framework based on Python. The tool is for validating, documenting, and profiling your data to maintain quality. Data Engineers have long known that automated testing is essential for managing complex code bases and huge amounts of data. You can simply integrate it with GCP, AWS or Azure. Great Expectations brings the same discipline, confidence, and acceleration to data science and data engineering teams.
How to start?
You need to install GE and create a project.
pip install great_expectations
great_expectations init
To integrate GE with GCP BiqQuery you need to adjust the configuration file
great_expectations.yml. You need to define a data source in this case BigQuery.
bq_datasource:
module_name: great_expectations.datasource
class_name: Datasource
execution_engine:
connection_string: bigquery://&lt;PROJECT&gt;/&lt;DATASET&gt;
module_name: great_expectations.execution_engine
class_name: SqlAlchemyExecutionEngine
data_connectors:
default_runtime_data_connector_name:
module_name: great_expectations.datasource.data_connector
class_name: RuntimeDataConnector
batch_identifiers:
- default_identifier_name
default_inferred_data_connector_name:
module_name: great_expectations.datasource.data_connector
class_name: InferredAssetSqlDataConnector
include_schema_name: true
The alternative way is to use:
great_expectations datasource new
Create a new test suit
To perform this tutorial I created a table in BigQuery based on a titanic dateset. You can create a table and load data to your table in the GCP console.
To create a new suit/test you need to execute the command below or run a code in python (An example at the end of this article).
great_expectations datasource new
In your web browser in a Jupiter notebook, you can define your validation roles based on profiling data.
Now we can investigate our data and start to define validations roles. GE is very powerful here; we can test if values are in the correct range, in a specific set or fit to a regex expression. You can also validate if a column is not empty or define an acceptable percent of empty values.
expect_column_values_to_not_be_null('Age', 0.90)
Age distribution in our dataset :
Based on the chart we can assume that our mean is between 20 and 40 and create a test:
expect_column_mean_to_be_between('Age', 20,40)
Other examples of validation roles:
validator.expect_column_values_to_be_between('Age', 0,80)
validator.expect_column_values_to_match_regex('Name', '[A-Z][a-z]+(?: \([A-Z][a-z]+\))?, ', mostly
=
.95)
validator
.
expect_column_values_to_be_in_set('Sex', ['male', 'female'])
validator.expect_column_values_to_be_in_set('Survived', [1, 0])
validator
.
expect_column_values_to_be_in_set('PClass', ['1st', '2nd', '3rd'])
More examples can be found
here
.
When you execute all cells you will see the report of our validation.
Now you can create your first checkpoint that will connect your tests with data.
great_expectations checkpoint new tytanic_checkpoint
As a result, a notebook will be created. To run your checkpoint you need to execute:
great_expectations checkpoint run tytanic_checkpoint
In the terminal, you will see information about progress and final status. In the project, you can also find user-friendly reports with detailed information.
The report location in the project:
great_expectations\uncommitted\data_docs\local_site\validations\bq_tytanic\
Integration with GCP bucket
If you use GCP GE allows you to change the configuration to extract reports to the GCP bucket. To do it you need to adjust great_expectations.yml. In the storage section, you can define where you want to store validation and expectation output.
stores:
validations_GCS_store:
class_name: ValidationsStore
store_backend:
class_name: TupleGCSStoreBackend
project: <PROJEKT>
bucket: <BUCKET>
prefix: uncommitted/validations/
expectations_GCS_store:
class_name: ExpectationsStore
store_backend:
class_name: TupleGCSStoreBackend
project: <PROJEKT>
bucket: <BUCKET>
prefix: GE/expectations/
At the end, you need to change the store setting like on the screen below.
expectations_store_name: expectations_GCS_store
validations_store_name: validations_GCS_store
The third step would be to configure a destination for your document reports. You can do this by adding a new gcs site in the data_docs_sites.
data_docs_sites:
# Data Docs make it simple to visualize data quality in your project. These
# include Expectations, Validations & Profiles. The are built for all
# Datasources from JSON artifacts in the local repo including validations &
# profiles from the uncommitted directory. Read more at
https://docs.greatexpectations.io/docs/terms/data_docs
local_site:
class_name: SiteBuilder
# set to false to hide how-to buttons in Data Docs
show_how_to_buttons: true
store_backend:
class_name: TupleFilesystemStoreBackend
base_directory: uncommitted/data_docs/local_site/
site_index_builder:
class_name: DefaultSiteIndexBuilder
gs_site:
class_name: SiteBuilder
store_backend:
class_name: TupleGCSStoreBackend
project: antidote-data-infrastructure
bucket: antidote-commercial-domain-assets
prefix: uncommitted/
site_index_builder:
class_name: DefaultSiteIndexBuilder
Then you can generate a report using the command below.
great_expectations docs build --site-name gs_site
Python script to generate all steps
There is an alternative way to create all steps using the python script. In the below example we use the existing data source of our BigQuery connection that was defined at the beginning of this article. In the next steps, we define a batch and tests. We save it, we define a new checkpoint and at the end, we run it.
import great_expectations as ge
from great_expectations.core.batch import RuntimeBatchRequest
import yaml
context = ge.get_context()
context.get_datasource(""pd_datasource"")
batch_request = RuntimeBatchRequest(
datasource_name=""pd_datasource"",
data_connector_name=""default_runtime_data_connector_name"",
data_asset_name=""tytanic"",  # this can be anything that identifies this data
runtime_parameters={""query"": ""SELECT * from XXXX.tytanic""},
batch_identifiers={""default_identifier_name"": ""default_identifier""},
)
context.create_expectation_suite(
expectation_suite_name=""test_tytanic_suite"", overwrite_existing=True
)
validator = context.get_validator(
batch_request=batch_request, expectation_suite_name=""test_tytanic_suite""
)
# test for dataset
validator.expect_column_values_to_be_in_set(column=""Sex"", value_set=[""male"", ""female""])
validator.expect_column_values_to_be_in_set(column=""Survived"", value_set=[1, 0])
validator.expect_column_values_to_be_in_set(
column=""PClass"", value_set=[""1st"", ""2nd"", ""3rd"", ""*""]
)
validator.save_expectation_suite(discard_failed_expectations=False)
my_checkpoint_name = ""tytanic_checkpoint""
checkpoint_config = f""""""
name: {my_checkpoint_name}
config_version: 1.0
class_name: SimpleCheckpoint
run_name_template: ""%Y%m%d-%H%M%S-my-run-name-template""
validations:
- batch_request:
datasource_name: bq_datasource
data_connector_name: default_runtime_data_connector_name
data_asset_name: tytanic
batch_identifiers:
default_identifier_name: 1
runtime_parameters:
query: SELECT * from XXXX.tytanic
expectation_suite_name: test_tytanic_suite
""""""
context.add_checkpoint(**yaml.safe_load(checkpoint_config))
checkpoint_result = context.run_checkpoint(
checkpoint_name=my_checkpoint_name,
)
context.build_data_docs(site_names=""gs_site"")
Script to run tests in python
When it’s ready we can create an additional python script to call only our test regularly or as a part of our ETL process.
import great_expectations as ge
from great_expectations.core.batch import RuntimeBatchRequest
import yaml
context = ge.get_context()
context.get_datasource(""pd_datasource"")
my_checkpoint_name = ""tytanic_checkpoint""
checkpoint_result = context.run_checkpoint(
checkpoint_name=my_checkpoint_name,
)
context.build_data_docs(site_names=""gs_site"")
if not checkpoint_result[""success""]:
print(""Failed"")
Summary
This article demonstrates how to set up a data quality test on GCP using GE. As you can see there are a lot of different options to configure and use your test using python or a configuration file. There is a possibility to dockerise this application and use it in the Cloud run or integrate it with orchestrators like Airflow or Prefect. Using Pandas you can perform data profiling that will help you to understand your data and perform the best data quality metric for your dataset.
However, if you need help setting up cloud-related solutions, you can contact me.
Data Quality
Gcp
Great Expectations
Data Engineering
--
--
Follow
Written by
Mariusz Kujawski
2.4K Followers
·
2 Following
Cloud Data Architect | Data Engineer
https://www.linkedin.com/in/mariusz-kujawski-812bb1103/
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
The problem with looking at only big data | by Carol Rossi | Medium,"The problem with looking at only big data | by Carol Rossi | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
The problem with looking at only big data
Carol Rossi
·
Follow
4 min read
·
Oct 26, 2016
--
Listen
Share
My co-workers know when they see me pull this image up it means somebody’s violated some basic principle of research. It’s not necessarily that the data is
bad
per se — it could be that people are misreading the data or looking at the wrong data and drawing the wrong conclusions as a result — but hey, the cat is cute and it’s a good way to bring humor into the conversation.
With any research, we’re trying to answer a pertinent business question. In order to do that, the first thing we need to figure out what’s the best research method to answer the question at hand. And we have infinite options: in-person or remote usability studies, surveys, AB tests, clickstream analysis, interviews, diary studies … you get the idea. We need all of these approaches because there’s no one method that’s always best.
Unfortunately, people have become so enamored with big data that they’re starting to forget about the value small data can bring. (Just a quick note: I’m defining big data as anything that comes from analytics (clickstream, AB test results) and small data as anything that comes from qualitative approaches (usability studies, interviews, diary studies, etc.). With tactical questions one approach is often sufficient but for large strategic questions, to get a full, 360 view of whatever it is you’re studying it’s often best to triangulate quantitative
and
qualitative insights, and to pull quantitative insights from not just analytics but also survey results. Otherwise at best you won’t have the full picture, or worse, will draw incorrect conclusions with minimal data.
A lot is being written on blending qualitative and quantitative methods (e.g., by researchers at
Microsoft
and
Google
), which is great and we need more researchers to write on this, but I still hear people in the wild draw conclusions that are inappropriate by looking at data that can’t possibly answer their question.
Here are two of the most common ways I hear people confounding insights by looking at the wrong data.
Trying to infer happiness or other preferences from engagement metrics
Recently I heard a colleague report a much lower bounce rate on a new version of one of our main pages. Yay! Cause for celebration! A lower bounce rate is always great news. But he said: “the bounce rate is 50% lower than the last version so that means our customers like it.” What’s wrong with this conclusion? The bounce rate is lower simply because customers are staying on the page and not bailing immediately. But from just this metric alone we don’t know
WHY
the bounce rate is down. Is it truly because customers like the experience? Because they’re staying on the page and finding what they need but they don’t really like the experience? Because they’re not finding what they need and are spending more time looking for it than they did before? We have no idea by simply seeing that the bounce rate is lower.
The only way to find out if customers like the experience is to either ask them directly (likely through a survey), or watch them use the site and see for yourself if they express joy. Qualitative research is needed to understand WHY the bounce rate is lower.
Trying to predict engagement by asking customers what they’d do
People always want to predict how likely customers are to engage with a site or better yet to convert. No one wants to allot resources to build something that customers don’t want, so it’s totally reasonable that researchers would be asked to help identify the likelihood of conversion.
We were recently asked to help answer just this question. The requester wanted to know how likely customers would be to fill in personal info on a form by showing a few variations of the design and asking users directly “how likely would you be to fill in this form”. This approach is challenging for several reasons, the primary one being there are factors beyond just the design (how much trust the user has with the site, the perceived value provided) when people decide whether to give personal information on a website. But the main reason we balked at the approach is because people are lousy at predicting their own behavior. How many times have you said you were giving up sugar or not going to buy the expensive shoes but then caved? I know I have. People are emotional and we often go with the emotional decision instead of the logical one.
We can’t know how likely people are to pick one option or another unless we run an AB or vaporware test. We can (and frequently do) run usability tests to narrow the options before an AB test by determining which versions are more understandable, but even then we can’t predict which one people will more likely complete.
So what can we do?
As I said earlier, it’s awesome that we have so many research methods available and can use whichever method we need to get the desired answer. If you’re a researcher, start by sitting down with the requester to fully understand what is the objective of the research — what is the person trying to understand? Then carefully pick the method(s) that will get that answer. If you’re a product manager, marketer, designer, then start to think through the outcome you’re trying to get instead of thinking about the research methods you already know. So instead of going to your research team and saying “we need to run a survey” simply tell them “we need to find out why people who gave us their contact info haven’t yet used our service.” Let your research partner choose the appropriate method.
UX
User Research
Big Data
Research
--
--
Follow
Written by
Carol Rossi
200 Followers
·
393 Following
I run a consultancy to help organizations maximize customer insights through advising and training. See more at
carolrossi.com
.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Validation at Scale with Spark/Databricks | by Sandip Roy | Medium,"Data Validation at Scale with Spark/Databricks | by Sandip Roy | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Validation at Scale with Spark/Databricks
Sandip Roy
·
Follow
6 min read
·
Mar 11, 2022
--
Listen
Share
What is Data Quality?
Data quality is the measure of how well suited a data set is to serve its specific purpose. Measures of data quality are based on data quality characteristics such as accuracy, completeness, consistency, validity, uniqueness, and timeliness.
Now while you generally write unit tests for your code, but do you also test your data? Incoming data quality can make or break your application. Incorrect, missing, or malformed data can have a large impact on production systems.
A typical data ingestion pipeline with data quality
Data Quality in Bigdata world
Apache Spark has become a technology by default nowadays for big data ingestion & transformation. This becomes more robust with the managed service provided by Databricks in terms of working with data at scale.
Without spending too much time on the theoretical aspects, let’s jump into few cases/scenarios (I’ll be using Databricks notebooks for demo purpose):
Anomaly Detection
Here’s the result:
What’s there beyond anomaly detection?
Above we can clearly see which records got changed and specifically which are the columns that got changed. Now that we are comfortable with basic anomaly check, how do we address broader aspects like following:
Missing values that can lead to failures in production system that require non-null values (NullPointerException)?
How changes in the distribution of data can lead to unexpected outputs of machine learning models?
How aggregations of incorrect data can lead to wrong business decisions?
Well there are several open source data quality frameworks viz. Apache Griffin, Great Expectations, Deequ as well as Delta Live Tables (DLT) from Databricks to facilitate this.
In our case, we will be using Deequ from AWS. It allows you to calculate data quality metrics on your dataset, define and verify data quality constraints, and be informed about changes in the data distribution. Instead of implementing checks and verification algorithms on your own, you can focus on describing how your data should look. Deequ supports you by suggesting checks for you. Deequ is implemented on top of
Apache Spark
and is designed to scale with large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse.
Firstly let’s configure the required libraries deequ (pydeequ for PySpark) as shown below:
And then let’s start with sample data set below:
And then run quality checks around it as below:
Here’s the result:
Through column constraint_status = “Failure”, you can track all the records that got failed during the validation process.
Though Deequ provides an overall data quality report it doesn’t fetch the individual bad records which failed the constraints. However, we can construct methods to create dynamic queries to identify bad records. We can get ideas from few Deequ constraint implementations that we used against each dataset.
I’ll illustrate two sample implementations (e.g. “CompletenessConstraint” and “MaxLengthConstraint”) as such to give an idea.
Sample I
Sample II
So you can easily derive from above implementations how quickly and efficiently we can build scalable data validation framework with minimum of effort. You can view full set of constraints/checks for your use below:
hasSize
— calculates the data frame size and runs the assertion on it.
isComplete
— asserts on a column completion.
hasCompleteness
— asserts on a column completion.
isUnique
— asserts on a column uniqueness.
isPrimaryKey
— asserts on a column(s) primary key characteristics.
hasUniqueness
— asserts on uniqueness in a single or combined set of key columns.
hasDistinctness
— distinctness in a single or combined set of key columns.
hasUniqueValueRatio
— the unique value ratio in a single or combined set of key columns.
hasNumberOfDistinctValues
— asserts on the number of distinct values a column has.
hasHistogramValues
— asserts on column’s value distribution.
hasEntropy
— asserts on a column entropy.
hasMutualInformation
— asserts on a mutual information between two columns.
hasApproxQuantile
— asserts on an approximated quantile.
hasMinLength
— asserts on the minimum length of the column.
hasMaxLength
— asserts on the maximum length of the column.
hasMin
— asserts on the minimum of the column.
hasMax
— asserts on the maximum of the column.
hasMean
— asserts on the mean of the column.
hasSum
— asserts on the sum of the column.
hasStandardDeviation
— asserts on the standard deviation of the column.
hasApproxCountDistinct
— asserts on the approximate count distinct of the given column.
hasCorrelation
— asserts on the Pearson correlation between two columns.
satisfies
— runs the given condition on the data frame.
hasPattern
— checks for pattern compliance.
containsCreditCardNumber
— verifies against a Credit Card pattern.
containsEmail
— verifies against an e-mail pattern.
containsURL
— verifies against an URL pattern.
containsSocialSecurityNumber
— verifies against the Social security number pattern for the US.
hasDataType
— verifies against the fraction of rows that conform to the given data type.
isNonNegative
— asserts that a column contains no negative values.
isPositive
— asserts that a column contains no negative values.
isLessThan
— asserts that, in each row, the value of columnA < the value of columnB.
isLessThanOrEqualTo
— asserts that, in each row, the value of columnA ≤ the value of columnB.
isGreaterThan
— asserts that, in each row, the value of columnA > the value of columnB.
isGreaterThanOrEqualTo
— asserts that, in each row, the value of columnA ≥ to the value of columnB.
isContainedIn
— asserts that every non-null value in a column is contained in a set of predefined values.
For simplicity and priority perspective, I’ve discussed only the critical and commonly used scenarios in our day to day life but deequ framework also covers advanced aspects like full-fledged metrics calculation (i.e. Deequ computes data quality metrics, that is, statistics such as completeness, maximum, or correlation) and auto suggestion of constraints (i.e. automated constraint suggestion methods that profile the data to infer useful constraints).
References
https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/
https://github.com/awslabs/deequ
https://databricks.com/session_na21/data-quality-with-or-without-apache-spark-and-its-ecosystem
Thanks for reading. In case you want to share your case studies or want to connect, please ping me via
LinkedIn
Spark
Databricks
Data Quality
Data Validation
Deequ
--
--
Follow
Written by
Sandip Roy
145 Followers
·
1 Following
Bigdata and Databricks Practice Lead at Wipro Ltd
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality with Great Expectations | by Charles Verleyen | Astrafy,"Data Quality with Great Expectations | by Charles Verleyen | Astrafy
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality with Great Expectations
Charles Verleyen
·
Follow
Published in
Astrafy
·
14 min read
·
May 9, 2023
--
1
Listen
Share
An automated approach with Airflow
“Data quality is a team effort, and the strength of the team depends on the strength of its weakest link.”
Introduction
Data quality is critical for any data-driven organisation that wants to make informed decisions based on reliable information. Inaccurate or incomplete data can lead to erroneous insights, wrong business decisions and loss of credibility. Many data projects fail to deliver and to gain stakeholders’ buy-in due to a lack of trust in data.
At Astrafy we always select open-source tools that excel in what they do and avoid tools that say they can do it all. For data quality, it was a no-brainer to include Great Expectations (GX) in our data quality stack as it has a huge community, it integrates really well with the main cloud providers and it now has a Cloud version to simplify the rendering of the test results.
As with every open-source project, GX OSSrequires some technical skills to set it up and be able to use it seamlessly. We have been through this learning curve with Great Expectations and we wanted to share our learning in this article. The following topics will be covered:
Some reminders about the two families of data quality (prevention versus detection)
Comparison of data quality tools and why we selected GX
Data Contracts with Great Expectations
Python package to abstract boilerplate to run GX via Airflow
Local development process made simple for analytics engineers to test quickly their Expectations
Keep your Expectations DRY by defining custom Expectations
Warning versus Failing Expectations
Great Expectations Cloud
Prevention versus Detection
There are two primary aspects to Data Quality:
Prevention
and
Detection
. Data teams need both to tackle quality.
Prevention
refers to data producers/controllers making considered changes to production-grade data and eliminating as many data quality issues as possible in the build phase. When you hear the term ‘shift left’, this is where it applies to data quality.
Main categories of prevention include:
Upfront collaboration and code review
Data Contracts (APIs)
Contract Enforcement / DLQ
Data Diffs
Detection
(or Observability) refers to understanding the state of data quality at any given team, identifying errors/regressions when they happen, and root causing them.
Main categories of prevention include:
Pipeline monitoring
Tests & Alerting
Root cause analysis
Assertions and Data Profiling
Data Cataloging
Detection is currently facilitated by great tools which consume metadata from your data platform of choice (Snowflake, Databricks, BigQuery). Just to name a few:
dbt [transformation, testing]
Great Expectations [Assertions, Profiling]
Monte Carlo [Data Reliability, Monitoring, Alerting]
In this article we focus on prevention with data contracts to prevent corrupted data to reach the data consumers.
Choosing the right data quality tool
It can be really daunting to choose amongst all the data quality tools in the market. This huge amount of choice has the drawback that you can easily feel lost and choose the wrong tool based on the best sales pitch you hear. A significant amount of time must be spent selecting the right tools that fits best within your data stack. Some rules to help you guide in this decision process:
Choose tools that excel in what they do
: the Modern Data Stack offers the flexibility of having different tools that can be assembled as lego blocks. Solutions that focus on a specific area often beats the solutions that want to do it all.
Choose tools that integrate well within your data stack
: you need seamless integration with your current data stack, otherwise you will spend time and effort constantly trying to integrate and fix those tools within your current stack.
Don’t reinvent the wheel:
there are plenty of open-source and enterprise-grade data quality tools in the market. You certainly need time to select the right one but you don’t have to build something custom.
Start small
: Follow the principle of “crawl, walk, run”. Start with a simple open-source tool and have your data quality process fine-tuned and well-oiled with this tool before moving to a more complex solution. Buying the latest expensive data quality tool and plugging it on top of messy data with no data management will only make things worse.
With that in mind, we decided to go with the following data quality stack at Astrafy:
Great Expectations
for data contracts within the data warehouse
(inspiration article from Chad Sanderson
here
): Lot of our data comes from external data sources where we don’t have control over the data producers. Therefore we get the raw data within BigQuery and before transforming this data, we check if we haven’t received any corrupted data.
We went with GX for the following reasons:
Laser focus tool on data quality with hundreds of built-in Expectations
Seamless integration with BigQuery
Nice UI to visualise data quality results
Open-source solution with large community
Data Quality stack continuation:
dbt test
to test transformations
: once we start transforming data and that we are within dbt context, dbt tests are the easiest and most efficient way to test your data. Singular test, generic test and packages such as
dbt-utils
and
dbt-expectations
will give you all the flexibility you want to write advanced tests.
As SQL transformations are mainly deterministic, it is a good practice to mock data and test your transformations within a CI tool. This ensures that your transformation outputs match expected results and that wrong SQL code is stopped at the CI stage. We highly recommend checking the unit tests developed within the
dbt-ga4
package to get a good grasp.
Data-diff
to detect semantic changes in the data
: this tool allows you to see how every change to dbt code affects the data produced in the modified model and its downstream dependencies. The SQL code introduced by one of your engineers might be right technically speaking (and therefore not detected by your tests) but is just wrong in terms of logic.
Elementary
for data observability:
Elementary provides a convenient user interface to visualise all your dbt tests and also offers features to detect anomalies in your data. This is convenient to detect outliers for certain fields where you did not set up any tests.
This data quality stack in place allows us to guarantee data quality at all levels and more importantly to have full visibility on our data quality. We have set up slack alerts on all data quality issues directly from those tools with relatively small development. It is of major importance to add results of those tests as metadata to your data catalogue tool; many data catalogue tools offer direct integration with the aforementioned data quality tools. In our case we use Dataplex and did leverage Dataplex API to send data quality results as metadata.
Data Contracts with Great Expectations
At Astrafy we are applying a data mesh paradigm to our data and therefore have different data products that represent our different family of data. Each of those data products corresponds to an Airflow DAG and the first stage in each of those DAGs is to make sure that source data is not corrupted. We want to avoid at all costs the “Garbage in, Garbage out”. Our standard Data Product Dag is as follows:
Without high-quality data, every analytics initiative will be underwhelming at best and actively damaging the business at worst. Data contracts are API-based agreements between producers and consumers designed to solve exactly that problem Data Contracts are not a new concept. They are simply new implementations of a very old idea — that producers and consumers should work together to generate high-quality, semantically valid data from the ground up.
Ideally data contracts should be implemented before your data reaches your data warehouse. This is feasible when the data producer is part of your company and that you can collaborate on setting up a dynamic data contract mechanism with a messaging system, dead letter queue for data not meeting the contract, etc. This perfect implementation is well explained by Chad Sanderson in his article ‘
An engineer’s guide to data contracts
’.
In our case, we have no control over data producers and get the data “AS IS” in our warehouse. We might have received corrupted data and for that reason, we consider this data as not reliable until it has passed through a data contract that consists of a series of tests defined in GX. The rest of this article will focus on implementation of those contracts with GX, how we can easily extend and test those locally and how we automated the run of those tests via
Airflow
.
Great Expectations
Great Expectations
comes with two flavours:
Great Expectations Open Source
(GX OSS). It provides a suite of tools that allows to easily validate data, define Expectations, and monitor data pipelines.
Great Expectations Cloud
(GX Cloud). Recently released in beta, it abstracts away some of the boilerplate you have with the open-source version.
At Astrafy, we use both GX Cloud and GX OSS to store and manage our daily validation results. With GX OSS as the core of everything Cloud offers, we have more control over the customization and deployment options, while GX Cloud provides us with a fully managed data quality management platform where our data quality results are stored and where any stakeholders can go to check the quality of the data.
Workflow Implementation
We wanted to achieve the following features for this data contract initiative to be successful:
Abstract away the boilerplate
for handling authentication, setting up the engine to run the Expectations, etc.
Ease to define Expectations
. Analytics engineers should be able to write and test Expectations from their IDE by just defining JSON Expectations and run those via a notebook.
Possibility to test the expectations locally through a notebook
: Analytics engineers should be able to run GX Expectations seamlessly through a curated notebook. More on this one in the ‘
local development
’ section.
Keep the code DRY
by having custom Expectations defined once and reused anywhere.
Fully automated deployment
within our Airflow Data Product DAG. More on this feature in the ‘
Automation & Airflow scheduling
’ section.
We achieved all those thanks to GX modularity and a few developments on our side. We tackled the first feature by building a python package that handles all the boilerplate of creating data assets, handling authentication, etc. One core component of this package is that it contains a logic to switch between GX OSS in case it is running locally or GX Cloud in case it is running on Airflow.
The second feature was maybe the most important as it would define adoption rate by our analytics engineers. If defining and running tests is too complex then you will get your team frustrated and few tests deployed in the end. We structured our data repository as follows:
We have a top folder ‘data-contracts’ that contains one folder per data product and within each of those subfolders we have an additional folder split for warning or failing tests. This split allows us to go with two kinds of test criticalities and in case of failing tests, we would stop the data pipeline in Airflow.
For analytics engineers, they just have to define their tests as GX Expectations in json files with one json file per table. Within this file, they can put any placeholders they want and those will be automatically replaced by environment variables. We recommend starting small with tests and having those tests driven by the business. Fields that are of high business value should have multiple tests while most fields could not be tested at first. Then a schema validation test should be done for each table to ensure that your downstream dbt models will not break in case of undetected schema changes. Our data contracts fulfil two aspects of the data contract described above:
Semantic checks
through tests at field level
Schema validation
The GX json file contains all the necessary information to define tests (the data source, the batch request, and the specific Expectations that need to be validated). It is worth noting that the data repository where analytics engineers work does not contain any GX boilerplate code. All this code is abstracted away in a python package hosted on Google Cloud Artifact Registry. Next section deeps dive into this package.
GX python package
As aforementioned, we have built a python package that abstracts away all the boilerplate needed to handle authentication to BigQuery, the logic to run either the OSS version or the cloud version and also to store custom Expectations (this meets one of required criteria of keeping the code DRY). By separating this repository from our main data project, we allow non-technical members of our team to define Expectations without having to understand the technicalities of Great Expectations. In our setup we have data engineers maintaining this package and analytics engineers using it in the data repository where the Expectations are defined. It has proven to be a very efficient set up where each team works on what they master at best.
The package selects by default Pandas as
execution engine
as it is the engine that most built-in Expectations support. However, analytics engineers can override this default execution engine directly from the json Expectation file. Pandas can be slow in case of a lot of data and SQLAlchemy engine can be better suited in that case. It is also the responsibility of the analytics engineers to work only with new data and to define this delta logic in the query definition of their Expectations.
The CI pipelines around this python repository generates two artifacts:
A python package that can be installed via “pip” and that is used by analytics engineers on the data repository (more on this in the next section dedicated to ‘
local development
’).
A docker image that is used by Airflow in a self-contained stage to run GX code via a
Kubernetes Pod Operator
.
The CI pipeline is as follows:
The git tag follows
semantic versioning
and this allows users to easily pin a specific version for the python package and docker image to use.
Local Development
One of the key features we wanted to achieve is that Analytics engineers should be able to run GX Expectations seamlessly through a curated notebook. We wanted this experience to be as straightforward and simple as possible and we did this by creating a notebook with placeholders for the data product to run and environment variables to be exported. This notebook will then install the GX package from the artifact registry and run all the Expectations for the specified data product.
This design is depicted in the following architecture:
For tests that could be reused in other tables or data products (for instance check if a product field contains only specific products code), those should be defined by the data engineers within the GX package and then any analytics engineers can use those tests.
Results are displayed directly within the notebook and once the analytics engineer is satisfied with the GX tests he/she wrote, he/she would then push on his/her feature branch. This will trigger a CI job that will upload the json Expectations on a storage bucket. This is the beginning of the fully automated data quality journey explained in the next section.
Automation & Airflow scheduling
We use Airflow as an orchestrator (the following idea would apply the same on other orchestrators) and run GX tests just after the data is ingested. This stage is depicted hereafter:
This data contract stage uses the docker image published from our GX python repository (details in
this section
). It takes as argument the data product for which it has to run the different Expectations and it will then fetch all those expectations from Cloud Storage. Once it has finished running the different Expectations, it will then output the results in GX Cloud where those can be visualised. All this code runs via a
Kubernetes Pod Operator
and the decoupling between the different Expectations and the code that runs it make it very flexible to add/change Expectations without having to redeploy the docker image that will run the code. This docker image will mainly be rebuilt when custom Expectations are added or the GX version is changed.
One important point about this Airflow stage is that it will be marked as failed if one of the “failing” Expectations fails. As aforementioned, we have a split between “warning” and “failing” Expectations and any Expectations that would lead to highly corrupted data being propagated to downstream consumers should be categorised in a “failing” Expectation.
Last but not least, you will certainly have a ‘dev’ and a ‘prd’ environment and this design of pushing Expectations to separate environment buckets and having semantic versioning for the docker image allow for seamlessly running different Expectations and version of the code in ‘dev’ and ‘prd’.
Checking results In Great Expectations Cloud
One of the benefits of using Great Expectations Cloud is the ability to visualise our data validation results through a nice UI. We can store and monitor our data validation results over time and track any changes or anomalies in our data.
The GX Cloud provides a user-friendly interface that allows us to visualise our history of data validation results in a variety of ways, including data quality reports and data profiling. We can easily share these reports with other members of our team and ensure that everyone is on the same page when it comes to the quality of our data.
By visualizing our data validation results in the GX Cloud, we can quickly identify any issues with our data and take action to rectify them. This has been incredibly valuable for us, as it allows us to maintain high-quality data and ensure that our data pipeline is running smoothly.
Conclusion
Data quality is one of the main topics nowadays in the Modern Data Stack and most companies struggle to achieve it. Reasons are multiple and as often arise from a lack of knowledge about how exactly to tackle data quality, what are data quality components, what tools should I use for which use cases, etc. Once you start demystifying and go beyond the buzzword, you gain clarity and are ready to set up a plan of action.
In this article, we have started by reviewing a few concepts of data quality and have then deep dived into implementation of data contracts with GX within a data warehouse. This was a specific use case and subsequent articles will tackle how we tackle data quality for our dbt transformation, how we detect anomalies automatically and last but not least how we feed back our data catalogue with those data quality results.
Regarding Great Expectations specifically, we are convinced it is a great tool to tackle data contracts use cases and its cloud version makes it comfortable to share data quality results with a non-technical audience.
As a final word, we want to emphasise that tools and data engineers alone will not solve your data quality issues. It’s foremost a team effort involving everyone in the chain of data. Data producers, data engineers, analytics engineers and data analysts should all be involved in the data quality process. Data producers should be aware of the data contracts and be incentivized to ship data of good quality, data engineers should build and maintain the data quality frameworks that then allow analytics engineers and data analysts to write tests; it’s a real living system that is not stronger than its weakest link. We have often seen in companies data engineers bearing the entire responsibility of data quality and this has never led to sustainable results.
If you enjoyed reading this article, stay tuned as more articles will come in the coming weeks on data quality.
Follow Astrafy on LinkedIn
to be notified for the next article ;).
If you are looking for support on Modern Data Stack or Google Cloud solutions, feel free to reach out to us at
sales@astrafy.io
.
Data Quality
Airflow
Great Expectations
--
--
1
Follow
Published in
Astrafy
127 Followers
·
Last published
12 hours ago
We are a niche consulting company focusing on delivering custom-made solutions for companies looking to modernize their data stack We are experts in Google Cloud, dbt, Airflow and all the DataOps around putting into production heavy scale data ecosystems. Web:
https://astrafy.io
Follow
Follow
Written by
Charles Verleyen
544 Followers
·
18 Following
Lead Architect @ Astrafy
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How to integrate data quality tests in the Python ETL pipeline? | by Haq Nawaz | Dev Genius,"How to integrate data quality tests in the Python ETL pipeline? | by Haq Nawaz | Dev Genius
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to integrate data quality tests in the Python ETL pipeline?
Haq Nawaz
·
Follow
Published in
Dev Genius
·
4 min read
·
Jan 14, 2023
--
2
Listen
Share
using Python, Pandas, SQL Server, Postgres and Pytest
Test ETL Pipelines
In the previous
article
we covered Pytest and developed data quality tests on an Excel dataset. Today we will integrate these data equality tests into the
Python ETL pipeline
. This ETL pipeline extracts data from a SQL Server database and loads it to a PostgreSQL database. We will modify the base script so we can include test cases to it. Since each table is unique and has different columns, we will have to process each table separately in order to run data quality tests against it. Therefore, we will select only a single table in each script. In this session we will select the
DimProduct
table and process its data.
The complete is available on the
GitHub
and code walk through is available on
YouTube
.
This ETL pipeline tutorial will enable you to:
Read data from a source database, SQL Server
Load data into a DataFrame using Pandas
Run Data Quality tests on the data
Persist Data to destination database, PostgreSQL database.
The Extract
We modify our extract function, borrowed from Python ETL pipeline, to only process a single table. We can remove the initial query that checks for multiple tables. Along with this we can also get ride of the for loop. We save the table name into a tbl underscore name variable. This variable is passed to the query. In addition, we switch the SQL Server connection to
SQLAlchemy
as pandas throws a warning and the future version only support
SQLAlchemy
connection.
SQLAlchemy
doesn’t have a close function so we remove the finally clause after the except. This is how extract function looks now. We are only processing a single table
DimProduct
. We are returning a dataframe and table name from this function.
The Load
Our load function will remain the same. We remove the function call at the end of the script. We will call the extract and load function from the test script.
We save this script as product_pipeline.py in a folder called product. The parent directory of this folder is called Python ETL and this can serve as the base directory. We can create a folder for each table and create a script for a table in each folder. For example, we have created a product’s folder and we have saved the product_pipeline.py script here. Similary, we can create individual scripts for each table and save them in different folders.
The Test Script
We create another file in this folder for testing the product pipeline. If you recall from the previous session the test file must begin with the test keyword. So, we name this file as test_product_pipeline.py. We will import our required libraries along with our product_pipeline.py file.
import pandas as pd
import numpy as np
import pytest
from numpy import nan
from product_pipeline import extract, load
In our test file we will define a fixture in the beginning. We provide the session and auto use to this fixture. It is possible to apply a fixture to all of the tests in a hierarchy, even if the tests don’t explicitly request a fixture, by passing auto use parameter. We define a function under this fixture called df. Here we invoke our extract function. This will pull the data from our database and return it as a DataFrame. The extract function will be called before the first test. We use the yield statement to push the dataframe to our tests. The yield statement returns one value, then it waits, saves the local state, and resumes again. When the calls to the function are complete then we process any subsequent code after the yield statement. In this case, we make a call to our load function once all the tests are complete.
@pytest.fixture(scope='session', autouse=True)
def df():
# Will be executed before the first test
df, tbl = extract()
yield df
# Will be executed after the last test
load(df, tbl)
Now we can start writing our test cases. I will borrow the data quality tests we defined in the
last session
as we process the same table but this time we pull the data from a database. Let’s assume next we process the Product Category table then we will utilize the same extract and load script simply update the table name in the tbl underscore name variable. The main changes will be in the test file. Be sure to update the column names and run appropriate tests on those columns.
The data quality test integration is complete in our ETL pipeline. We can save our work and give it a run. We invoke the tests with a few additional arguments to get more details about our test cases.
python -m pytest -v
Pytest found our seven tests and all the tests ran successfully. Note with dash v flag we receive the test names along with the status. Previously we were seeing a dot for success and F for failure. This gives us a little information on the test cases. We can explore our database to make sure the stg_product table is present and it includes data.
Pytest Test Results
Conclusion:
We have successfully integrated Data Quality tests into our Python ETL pipeline.
We revised our ETL pipeline script and imported Data Quality tests.
We defined test cases and established data quality standards and prevented inconsistent data flowing into the ETL pipeline.
The complete code for tutorial can be found
here
Etl
Etl Pipeline
Python
Data
Testing
--
--
2
Follow
Published in
Dev Genius
24K Followers
·
Last published
8 hours ago
Coding, Tutorials, News, UX, UI and much more related to development
Follow
Follow
Written by
Haq Nawaz
2.2K Followers
·
5 Following
I am a business intelligence developer and data science enthusiast. In my free time I like to travel and code, and I enjoy landscape photography.
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Mastering Data Quality: 10 Essential Checks with Real-World Examples and 7 Best Practices | by Rajanikant Vellaturi | Data Engineer Things,"Mastering Data Quality: 10 Essential Checks with Real-World Examples and 7 Best Practices | by Rajanikant Vellaturi | Data Engineer Things
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Mastering Data Quality: 10 Essential Checks with Real-World Examples and 7 Best Practices
Rajanikant Vellaturi
·
Follow
Published in
Data Engineer Things
·
5 min read
·
Oct 30, 2023
--
3
Listen
Share
Introduction:
In the world of data engineering, data quality should be the fundamental pillar upon which businesses make reliable insights and informed decision-making. Ensuring that your data is accurate, complete, and trustworthy is non-negotiable, and this demands a robust set of data quality checks.
This blog will walk you through the must-do data quality checks every data engineer should incorporate into their data pipelines. But we won’t stop at theory — we’ll back each check with real-world examples to illustrate its practical significance.
Whether you’re handling customer data, financial transactions, or sensor readings, these data quality checks and examples will equip you with the knowledge and tools to enhance the integrity and reliability of your data. Let’s dive in and empower your data engineering efforts with the power of data quality.
Photo by
John Schnobrich
on
Unsplash
Here are ten must-do data quality checks
1. Data Validation:
Check for Missing Values:
Route or exclude records with missing data and mark them as bad records.
Data Type Validation:
Ensure data types match the expected format and perform necessary conversions.
Example: If you’re processing customer data, ensure that critical fields like email addresses or phone numbers are not missing. You might implement a validation rule like: “No customer record should have a missing email address.”
2. Duplicate Detection:
Implement checks to identify and prevent duplicate records.
Example: In a sales database, you can identify duplicate transactions by comparing order IDs. A validation rule might be: “No two transactions should have the same order ID.”
3. Consistency Checks:
Check data consistency across tables or datasets to ensure related data elements match.
Use foreign key constraints to enforce referential integrity in relational databases.
Example: In an e-commerce database, validate that product IDs in order items exist in the product catalog table.
4. Cross-Validation:
Verify data consistency and correctness by comparing data with external sources, if available.
Cross-check data against authoritative sources or reference data.
Example: Cross-validate totals to ensure they match if you’re aggregating sales data (like total sale amount) from multiple sources. For instance, compare the total sales from online and offline channels to the company’s total sales.
6. Data Profiling:
Profile data to understand its distribution, summary statistics, and patterns.
Identify data anomalies or unusual patterns that may require attention.
Example: Profile the distribution of customer ages. You might discover an unusual concentration of ages in a specific range, prompting further investigation.
7. Schema Validation:
Ensure data adheres to the expected schema, including column names, data types, and constraints.
Implement schema evolution checks to handle changes gracefully.
Example: If you expect dates in the “
yyyy-mm-dd
” format, validate that all date values follow this format.
8. Business Rules Validation:
Apply business-specific validation rules to ensure that data aligns with business requirements.
Example: If you’re processing financial data, validate that all transactions have positive values. A validation rule could be: “All transaction amounts should be greater than zero.”
9. Data Cleansing:
Implement data cleansing routines to address issues such as inconsistent capitalization, formatting, or typos.
Standardize data to maintain consistency.
Example: Cleanse text data by converting all text to lowercase to maintain consistent capitalization.
10. Data Lineage and Traceability:
Maintain data lineage documentation to track the source and transformations of data throughout the pipeline.
Enable traceability to identify issues and their sources quickly.
Example: Document the flow of customer data from source systems to downstream applications. Show how data moves through various transformations in the pipeline.
Best Practices for Effective Data Quality Checks
Effective data quality checks are essential for maintaining data integrity and ensuring your data is reliable for analysis and decision-making. Here are some best practices to consider when implementing data quality checks in your data engineering pipelines:
1. Automation:
Why:
Automating data quality checks allows you to perform them consistently and at scale, reducing the risk of human error.
How:
Use scheduling tools or workflow orchestration platforms to automate data quality checks as part of your data pipeline. Ensure that these checks run at regular intervals to catch issues early.
2. Logging and Alerting:
Why:
Logging and alerting mechanisms provide real-time visibility into the health of your data and allow you to take immediate action when issues arise.
How:
Implement robust logging for data quality checks, including detailed information about the checks performed and their outcomes. Set up alerts to notify relevant stakeholders when data quality issues are detected.
3. Documentation:
Why:
Documentation ensures that data quality checks are well-defined and everyone involved understands their purpose and expected results.
How:
Create documentation that describes each data quality check, its validation criteria, and the actions to take if issues are identified. Make this documentation accessible to the data engineering team, data analysts, and business users.
4. Sampling:
Why:
When dealing with large datasets, if it is not feasible to validate every record, Sampling allows you to validate a representative subset of data efficiently.
How:
Develop sampling strategies to select a subset of data for validation. Ensure the sampling process is random and statistically sound to provide meaningful insights.
5. Regression Testing:
Why:
Data pipelines and source systems evolve over time. Regular regression testing ensures that changes do not introduce data quality issues.
How:
Establish a regression testing process that reruns data quality checks on historical data to identify discrepancies. This process should be automated and triggered whenever changes are made to the pipeline.
6. Collaboration:
Why:
Data quality is a collective effort. Involving data stakeholders, including data analysts and domain experts, in defining data quality rules and validation criteria ensures that checks align with business needs.
How:
Hold regular meetings or workshops with data stakeholders to gather input on data quality rules. Collaboratively define thresholds for acceptable data quality and validate these rules with real-world scenarios.
7. Data Profiling and Monitoring:
Why:
Data profiling provides insights into the distribution and characteristics of your data. Ongoing monitoring helps detect trends or shifts in data quality.
How:
Implement data profiling tools to analyze data distribution and quality characteristics. Set up continuous monitoring to track changes in data quality metrics over time.
Conclusion
It’s essential to run data quality checks to ensure that the dashboard or reports used by business users display accurate data. Overlooking these checks can cause significant trouble for both users and you, as it will require you to review everything again, which is challenging.
Incorporating data quality checks into your data engineering practices is crucial as it will enhance the credibility of your data and enable your organization to make informed decisions confidently. Data quality is not a one-time task; it requires a continuous commitment to excellence in data engineering.
By having these critical checks, real-world examples, and best practices in your toolkit, you’ll be well-equipped to master data quality and enhance the value of your data assets.
Please share the data quality checks implemented in your pipelines below. Don't forget to like and comment!
Data Engineering
Data Quality
Data Pipeline
--
--
3
Follow
Published in
Data Engineer Things
14.4K Followers
·
Last published
7 hours ago
Things learned in our data engineering journey and ideas on data and engineering.
Follow
Follow
Written by
Rajanikant Vellaturi
146 Followers
·
18 Following
My experience includes two decades of data and analytics. TPM@ Snowflake | ex-Cloudera. Opinions are my own.
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Big Data Checklist. The Joel Test for big data. | by Dima | Medium,"Big Data Checklist. The Joel Test for big data. | by Dima | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
photo from publicdomainpictures.net
Big Data Checklist
The Joel Test for big data.
Dima
·
Follow
10 min read
·
Oct 16, 2013
--
Listen
Share
I started with Big Data before the term was coined.
Today it is trending. More and more projects understand that “Data is King” and are moving towards it.
Big data requires different framework for your gut feeling and intuition to keep finding the right routes. Good thing is, it’s a skill and it can be mastered.
Below is my “
Joel Test
” for big data projects.
It is not aiming at making everyone a big data expert. But it should help great engineers become great big data engineers. And make it more fun too.
If you are just starting to look into big data, you may think of the questions below as a guide to follow. If you already are into big data, you may find a few areas to sharpen your focus in.
Do you have big data separated from the rest of your product?
Do you have established metrics?
Do you have a live dashboard?
Do you have the fastest possible prototype-evaluate-ship-repeat cycles?
Do you log everything?
Do your designs enforce repeatability of results?
Do you run live experiments?
Do you run regression tests?
Do you have infrastructure ready for big data?
Do you understand the importance of organic data?
Do you know how much headroom you have?
Do you do research outside your main task?
Deep Dive
Having identified and prioritized twelve bullet points above, I am going to elaborate on them in more detail.
Down the text “big data” is used in a broad sense, that includes machine learning, data mining, information retrieval and other scientific disciplines along with infrastructure, evaluation pipelines and other implementation details.
On Metrics
The metrics are to help you track your progress, but their value drops below zero once improving the metric no longer improves the end user experience.
The metrics should allow comparing apples to apples.
If the metric you like does not allow comparing its value today to yesterday, you may want to introduce another metric to track day to day progress.
The metrics should have a rationale behind them.
The metrics should be easy to explain to people who are not into engineering.
Imposing numerical goals on metrics can be a curse.
If you do impose them, base your expectations on headroom analysis and proven big ideas to go live soon — not on past improvements rate or on how well your competitors are doing.
Do reality checks on your metrics often.
Your localized precision or relevance or engagement should correlate well with higher-order metrics like growth rate, retention rate, how often your users comment on or share what you show them, etc.
On Evaluation
Along with data infrastructure, evaluation pipeline is what enables you to run dozen iterations per week instead of barely one.
With big data 10x iterations is what makes a difference between good and great.
The evaluation should be as automated as possible.
The evaluation should be as fast as possible.
Offline evaluation should ideally run in a few minutes.
If it’s all about going through under a gigabyte of labeled validation data, why on Earth should it be taking longer?
Online evaluation, on the other hand, often does take longer.
You may have to settle for something on the scale of 24-hour timeframes. It’s OK — but make sure you can run multiple experiments in parallel within one 24-hour window.
Sometimes it helps to have a model created manually, without involving machine learning techniques, to have the baseline to compare against.
On Dashboard
Put a big screen in the office with the sole purpose of showing off how great are you doing numbers-wise.
Be honest with yourself and don’t just hide it if the numbers don’t look good.
Dashboard is way more useful if it is using post-cooked big data.
This way it serves as the first-order customer of logs processing logic and pipelines.
Key big data metrics should be on the dashboard, along with the basic usage numbers.
The basic usage numbers may come from outside of the big data infrastructure, but most should go through it.
On Organic Data
Organic data is the data that captures the behavior of your users best, without pruning or filtering.
Be well aware of user behavioral patterns and the
80%/20% rule
.
If some type of action accounts for the majority of user actions, it would account for the majority of user actions in organic data.
If some content accounts for the majority of content your users go through, organic data will have the same skew.
Evaluations using the organic data are the most valuable.
Whenever possible, top-level metrics should be based on organic data sets.
Having said that, you will need to sub-sample the data for more accurate metrics on deeper levels. But make sure the top-level metric does improve as well. It will take more work and more time to get noticed, but one should be able to see the improvement.
Have a good idea of your headroom.
It is not the absolute best value for the metric you have crafted: it is where can you get in short-, mid- and long term in a decent, yet realistic, scenario.
Understanding headroom requires manual work. The people doing the job of data scientists should get their hands dirty from time to time.
It also requires creativity, so make it as much fun and as little routine as possible.
In smaller teams a useful habit to have is to dedicate a day or half a day per month trying to pretend you are “the real users” to get a feel of how their lives differs from what you thought it is.
At times, you may want to involve more people, unaware of your current direction, whose only job would be to tell you how and where can you do better.
On Labeled Data
Labeled data is orders of magnitude more expensive.
Don’t hesitate to label some data yourself.
Ten minutes of looking through some side-by-side results of old and new models is a good start of the day.
Reuse your labeled data as much as you can. Don’t invalidate it until you absolutely have to — or until it becomes obsolete on its own.
In particular, keep at least part of your labeled data excluded from training for validation purposes.
Looking at the value of your high-level metric on labeled data is OK.
Manually examining concrete cases in labeled dataset instantly marks this set ineligible for further clean experiments.
Don’t do it and don’t let your teammates to.
Rotating labeled data is a good habit.
If you are willing to have 1'000 labeled examples per week, keep the ones from the most recent weeks “clean” and use the older ones for deep dives.
On Live Experiments
Live experiments help a lot. Unless you have a good reason, don’t hesitate to route 1% or 0.1% of traffic to experimental models.
In fact, in a lively product with big data team at work, multiple live experiments running non-stop is a healthy environment.
Sharding your traffic may be harder than you think.
For a stateless search engine you can afford to shard
by query hash. But the world seems to be pretty much done with building stateless search engines.
Sharding should be designed in a way where splitting off 0.1% of traffic keeps both 0.1% and 99.9% parts organic.
For example, if you are building an app store and some app has significant share of traffic, sharding by app ID does not work since it denies you the opportunity to fan out 0.1% of it.
Shard by user sessions or come up with something smarter.
For most applications it is perfectly fine for the same query from the same user to end up in different shards from time to time. The users would not hate you if sometimes the results they see get altered by a bit — while in return you will get valuable apples-to-apples comparison results to explore.
Once you have established live experiments infrastructure, back tests are a great way to confirm you are going in the right direction.
Tee-ing some traffic to test/canary machines is a good thing too, assuming your data stays immutable.
On Serving Infrastructure
Big data logic should run on dedicated machines.
At the very least this covers logs cooking jobs, modeling processes, evaluation pipelines and serving infrastructure.
Of all components, serving infrastructure is the first one you want to have dedicated environment for. Now.
REST APIs are your friend.
Have your results repeatable. Take it seriously. More seriously.
Two decent ways to ensure repeatability are: 1) put everything into source control (usually git) and make the configuration parametric on commit hash, 2) keep server code and models as VM images.
Spawning a new serving job, production-ready on a fresh machine or locally for testing, should be a matter or running one script.
Top-level logs cooking jobs should also be possible to spawn via one script.
Regression tests are shame to not have.
It only takes gathering some data, anonymizing if necessary, running it through your service, saving the results into a golden results file and diff-ing against that file later on.
A good regression test can also test live experiments and sharding logic.
A good regression test is also a load test.
On Data Infrastructure
Along with evaluation pipeline, data infrastructure is what enables you to run dozen iterations per week instead of barely one.
At risk of repeating myself, with big data 10x iterations is what makes a difference.
“It’s not big data yet when it doesn’t fit into Excel!”
Early on you may well live with CSV/JSON/BIN files and Python/Ruby/Clojure scripts.
There is no need to set up or maintain a Hive/Hadoop cluster until you hit terabytes scale.
Make sure you log all the data you need.
It goes beyond the user clicks on your website. Consider viewport, from where did this particular user land on your service, IP / proxy information, browser / OS / screen resolution, mouse hovering, actions involving external websites, co-installed applications on mobile — and much more.
Mobile is especially important: lots and lots of data is available once you have an actively used mobile app.
You never know where the next insight would come from — but, chances are, it will come from the data.
Log your own work along with user actions.
Which live experiments were running and when, which user requests got routed to which experiments, labels you have obtained, by what means, which portions of data did you send out for labeling and for what reason — all these in-house things count as the data you must log and keep.
Log data cooking is usually harder than serving.
And it is one the few pieces that falls in between the big data and the other part of the product.
KISS is your friend. I’d totally bless something like “the server stores logs in certain directory in the cloud, the big data infrastructure parses those logs as they arrive”.
Normally, most features would be computed on data infrastructure side.
If this is your case, plugin structure works best.
Often times it is more efficient time-wise to first implement the logic that adds a new feature and keep it running for a few days. After the new feature is already stamped along with the existing ones, it is much easier to experiment with.
Therefore, make sure new featurizers are easy to hook up — perhaps, automatically, when the code is checked in.
On Modeling
Make it enjoyable and comfortable to dig into your data — the world of modeling is where most of creative time is being spent.
The efficiency of modeling depends largely on how fast the iterations can be.
Multiple full-stack iterations per day should be your goal.
If viable, make it possible to run modeling on a single machine.
Running stuff locally is way faster and has less or zero external dependencies.
On Prototyping
Do whatever you want and have fun —as long as you are moving forward.
Try any idea you feel is worth trying — but aim at getting headroom estimate soon and don’t hesitate to drop the idea as soon as you believe there may be a lower hanging fruit.
Use any tools you feel like using.
Don’t hesitate to invest into building reusable tools for the future.
At the same time, don’t hesitate to live on dirty hacks if it lets you run a reality check of some idea quickly.
Don’t bother if the implementation looks ugly. It’s one of very few places it’s allowed to.
However, once you have something you can demonstrate business value with, switch from prototyping to productization and clean up the mess before it hits the fan.
On Research
No matter how strong of a team you have, make sure to communicate to the outside world.
Sending data scientists to conferences asking them for trip reports in exchange is a practice that works well.
Dedicate some time to explorations that do not have immediate value.
For example, if your job is to do supervised learning and categorize your users into paying and not paying customers, find time to train a few unsupervised models and look at the clusters you get.
A few insights coming from this may be well worth it very soon.
Give talks, open-source stuff that does not carry immediate business value, write blog posts about how amazing are your data challenges — make sure you establish presence in the community.
Interns are a great way to accomplish all or most of the above.
Bottom Line
The field of big data is different from other software engineering disciplines. The intuition and gut feeling you used to rely on may play a joke on you. And with data-driven projects it often takes more time to realize the wrong route was taken — and sometimes it may be too late.
Getting big data done right should become easier with twelve high-level concepts embodied above.
I have done plenty of machine learning and can say with confidence that changing a “no” into a “yes” for the questions above has been the right thing to do consistently — and would probably keep being the right thing for quite some time.
Big Data
Software Development
Engineering
--
--
Follow
Written by
Dima
1.1K Followers
·
879 Following
http://dima.ai/
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Beginner’s Guide to Data Testing. What is Data Testing? | by Sumit Mudliar | Data Quality & Beyond | Medium,"Beginner’s Guide to Data Testing. What is Data Testing? | by Sumit Mudliar | Data Quality & Beyond | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Beginner’s Guide to Data Testing
Sumit Mudliar
·
Follow
Published in
Data Quality & Beyond
·
8 min read
·
Oct 26, 2023
--
Listen
Share
What is Data Testing?
Data testing is the process of evaluating the quality, accuracy, and completeness of data. It involves verifying that the data meets the expected requirements and is free of errors. Data testing is an important part of any data-driven organization, as it helps to ensure that the data is reliable and can be used to make informed decisions.
Why is Data Testing Important?
Data testing is important for a number of reasons:
To ensure the quality of data: Data testing helps to identify and correct errors in the data, such as missing values, duplicate records, and inconsistent data formats. This ensures that the data is accurate and reliable, which is essential for making informed decisions.
To meet regulatory compliance requirements: Many industries have regulations in place that require organizations to test their data on a regular basis. For example, the financial industry has regulations that require organizations to test their data for accuracy and completeness before it is used to make investment decisions.
To improve the performance of data-driven applications: Data-driven applications rely on accurate and reliable data to function properly. Data testing helps to ensure that the data is of high quality, which can improve the performance and accuracy of data-driven applications.
Types of Data Testing
There are a number of different types of data testing that can be performed, depending on the specific needs of the organization. Some of the most common types of data testing include:
Unit testing: Unit testing involves testing the smallest possible units of data, such as individual fields or records. This is typically done using automated testing tools.
Integration testing: Integration testing involves testing how different data sets work together. For example, integration testing might be performed to ensure that the data in a customer relationship management (CRM) system is consistent with the data in an enterprise resource planning (ERP) system.
System testing: System testing involves testing the complete data system, including all of the data sets and all of the applications that use the data. System testing is typically performed manually to ensure that the data system meets the overall business requirements.
Acceptance testing: Acceptance testing involves testing the data system with users to ensure that it meets their needs and expectations. Acceptance testing is typically performed at the end of the development process before the data system is deployed to production.
How to Get Started with Data Testing
If you are new to data testing, there are a few things you can do to get started:
Learn about the different types of data testing: It is important to understand the different types of data testing and how they can be used to improve the quality of your data.
Identify the data testing needs of your organization: Consider the specific requirements of your organization and the types of data that need to be tested.
Develop a data testing plan: Your data testing plan should outline the specific tests that need to be performed, the data that will be tested, and the expected results.
Select the appropriate data testing tools: There are a number of different data testing tools available, both manual and automated. Select the tools that are most appropriate for your needs and budget.
Execute the data testing plan: Once you have developed a data testing plan and selected the appropriate tools, you can begin executing the plan.
Analyze the results and take corrective action: Once you have executed the data testing plan, analyze the results to identify any errors in the data. Take corrective action to correct the errors and improve the quality of the data.
Tips for Data Testing
Here are a few tips for data testing:
Start early: Data testing should not be an afterthought. It is important to start testing the data early in the development process to identify and correct errors as soon as possible.
Be comprehensive: Data testing should be comprehensive and cover all aspects of the data system, including all of the data sets and all of the applications that use the data.
Use a variety of testing methods: There are a number of different data testing methods available. Use a variety of methods to ensure that the data is tested thoroughly.
Document the results: It is important to document the results of your data testing efforts. This will help you to track your progress and identify areas where improvement is needed.
Data Testing Tools
There are a number of different data testing tools available, both manual and automated. Some of the most popular data testing tools include:
Manual data testing tools: Manual data testing tools typically involve using spreadsheets and other common software applications to test data. Manual data testing can be time-consuming and error-prone, but it is often necessary to test complex data sets and/or data that is not easily automated.
Automated data testing tools: Automated data testing tools use scripts to automate the data testing process. Automated data testing can be very efficient and effective, but it is important to develop and maintain the test scripts carefully.
Some popular manual data testing tools include:
Microsoft Excel
Google Sheets
OpenOffice Calc
SQL Server Management Studio
Some popular automated data testing tools include:
iCEDQ
Selenium
TestComplete
UFT One
DataGrip
Data Testing Process
The data testing process typically involves the following steps:
Data preparation: The first step is to prepare the data for testing. This may involve cleaning the data, removing duplicates, and/or converting the data to a format that is compatible with the data testing tool.
Test case design: The next step is to design the test cases. Test cases are sets of instructions that specify the data to be tested, the expected results, and the steps to be taken if the results are unexpected.
Test execution: Once the test cases have been designed, they can be executed manually or using an automated data testing tool.
Result analysis: The results of the test cases need to be analyzed to identify any errors in the data.
Defect reporting: Any errors that are found need to be reported to the appropriate team for correction.
Retesting: Once the errors have been corrected, the data needs to be retested to ensure that the errors have been fixed.
Data Testing Best Practices
Here are a few data testing best practices:
Use a risk-based approach: Focus your testing efforts on the areas of the data system that are most at risk of errors.
Use a variety of testing methods: Use a variety of testing methods, such as unit testing, integration testing, system testing, and acceptance testing, to ensure that the data is tested thoroughly.
Automate as much as possible: Automate as much of the data testing process as possible to improve efficiency and accuracy.
Involve users: Involve users in the data testing process to ensure that the data system meets their needs and expectations.
Document your work: Document your data testing efforts so that you can track your progress and identify areas where improvement is needed.
Data Testing for Specific Data Types
There are a number of different data types that can be tested, including:
Structured data: Structured data is data that is organized in a specific format, such as a database table or a CSV file. Structured data is relatively easy to test using automated data testing tools.
Unstructured data: Unstructured data is data that is not organized in a specific format, such as images, videos, and text documents. Unstructured data can be more difficult to test, but there are a number of automated data testing tools that can be used to test unstructured data.
Big data: Big data is data that is too large or complex to be processed using traditional data processing tools. Big data testing can be challenging, but there are a number of specialized data testing tools that can be used to test big data.
Data Testing for Specific Applications
Data testing can also be performed for specific applications, such as:
Data warehouses: Data warehouses are used to store and analyze large amounts of data. Data testing is important for ensuring that the data in a data warehouse is accurate and reliable.
Machine learning models: Machine learning models are trained on data to make predictions. Data testing is important for ensuring that the data used to train a machine learning model is accurate and complete.
Real-time streaming data: Real-time streaming data is data that is generated and processed in real time. Data testing is important for ensuring that real-time streaming data is accurate and reliable.
Data Testing for Specific Industries
Different industries have different data testing requirements due to the unique nature of their data and the regulations that they are subject to. For example, the financial industry has strict regulations in place for data testing to ensure that the data is accurate and complete for financial reporting and risk management purposes. The healthcare industry has unique data privacy concerns and must comply with regulations such as HIPAA.
Here are some specific examples of data testing considerations for specific industries:
Financial industry: Data testing in the financial industry typically focuses on ensuring the accuracy and completeness of financial data, such as account balances, transaction history, and risk metrics. Data testers in the financial industry may also need to test for compliance with specific regulations, such as Sarbanes-Oxley and MiFID II.
Healthcare industry: Data testing in the healthcare industry typically focuses on ensuring the privacy and security of patient data. Data testers in the healthcare industry may also need to test for compliance with specific regulations, such as HIPAA and GDPR.
Retail industry: Data testing in the retail industry typically focuses on ensuring the accuracy and completeness of product data, customer data, and sales data. Data testers in the retail industry may also need to test for compliance with specific regulations, such as the General Data Protection Regulation (GDPR).
Data Testing for Cloud-Based Applications
Cloud-based applications present new challenges for data testing due to the scalability and security requirements of cloud computing. Data testers for cloud-based applications need to ensure that the data can be processed and analyzed at scale, and that the data is secure from unauthorized access.
Here are some specific examples of data testing considerations for cloud-based applications:
Scalability: Data testers need to ensure that the data testing process can scale to handle large volumes of data. This may require using cloud-based data testing tools or developing custom data testing scripts.
Security: Data testers need to ensure that the data testing process does not compromise the security of the cloud-based application or the data being tested. This may require using encryption and other security measures.
Data Testing for Artificial Intelligence (AI) and Machine Learning (ML) Systems
AI and ML systems rely on data to learn and make predictions. It is important to ensure that the data used to train and test AI and ML systems is accurate and complete. Data testers for AI and ML systems need to develop tests that can identify errors in the data and ensure that the AI and ML systems are making accurate predictions.
Here are some specific examples of data testing considerations for AI and ML systems:
Accuracy: Data testers need to develop tests that can identify errors in the data that could impact the accuracy of the AI and ML system’s predictions.
Completeness: Data testers need to ensure that the data used to train and test the AI and ML system is complete. This means ensuring that the data set includes all of the possible data points that the AI and ML system could encounter in production.
Bias: Data testers need to ensure that the data used to train and test the AI and ML system is not biased. This means ensuring that the data set represents the real world population that the AI and ML system will be used to interact with.
Conclusion
Data testing is an important part of any data-driven organization. By following the tips above, you can get started with data testing and improve the quality, accuracy, and completeness of your data.
Data Testing
Data Validation
Data Quality
--
--
Published in
Data Quality & Beyond
16 Followers
·
Last published
Nov 23, 2024
Sharing stories, concepts and ideas
Follow
Written by
Sumit Mudliar
457 Followers
·
479 Following
Transforming ideas into reality through code. Driven by purpose, fueled by curiosity. Always learning and growing.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality Dashboard. Informed decision making relies on… | by Arpine K | Open Data Discovery,"Data Quality Dashboard. Informed decision making relies on… | by Arpine K | Open Data Discovery
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality Dashboard
Informed decision making relies on data: a crucial asset for business, however, not all data holds the same value
Arpine K
·
Follow
Published in
Open Data Discovery
·
4 min read
·
Dec 21, 2023
--
Listen
Share
Photo by
Ant Rozetsky
on
Unsplash
The effectiveness of using enterprise data for decision making is heavily influenced by its quality, evaluated based on accuracy, timeliness and relevance to its intended purpose. To ensure data reliability, it is very common to use monitoring tools for data quality assurance. Choosing the right tool for data quality checks and monitoring typically depends on the organization’s specific needs and technologies and while there are a variety of tools available in the market to handle this, here we introduce a way to make a unified view on all the data quality with ODD․
Open Data Discovery
Open Data Discovery
is an open-source Data Governance Platform, which improves productivity, collaboration, and manageability of modern data products and teams.
Open Data Discovery
First open-source data discovery and observability platform - Open Data Discovery
github.com
We do not check, we incorporate check results!
Before we dive into data quality monitoring offering from our team, please note that quality checks aren’t performed directly within ODD Platform. Instead, we provide tools through integrations with leading tools in the field.
Integrated data quality checks
Additional information on these integrations can be found in our previous articles:
1.
Enhancing Data Quality with OpenDataDiscovery and GreatExpectations
and
2.
Trust your data with dbt and OpenDataDiscovery
.
Both recognizing the significance of quality checks and understanding the results of those checks are equally crucial. To facilitate this, we’ve developed a Data Quality Dashboard that builds on integrated quality check results.
But why Dashboard?
The shift from traditional reports to dashboards is gaining popularity due to their dynamic updates, linkage to multiple data views and user interactivity. Let’s have a look at the ODD Data Quality Dashboard:
It might appear simple, but it is both informative and practical
What we observe here are data quality metrics which are dimensions representing diverse characteristics of data that are used to evaluate and measure its overall quality:
Assertion Tests
: these are validations or checks put in place to ensure that specific conditions or assertions about the data are met.
Custom Values Anomalies
: these anomalies involve irregularities or unexpected values in the data that deviate from a predefined set of acceptable or standard values.
While inaccuracies can stem from various factors such as human errors, logical flows in data pipelines and multiple intermediaries between the source and destination systems,
Data Accuracy is a fundamental criterion for any data product to be ensured.
Freshness Anomalies
: these anomalies are related to the timeliness of the data, checking whether the data is up to date and falls within the acceptable time frame.
Schema Changes
refers to the modifications in the structure or organization of the data, with a focus on monitoring whether the data schema remains consistent over time.
Unknown Category
represents a situation where data is placed into a category that was not foreseen or specified in the established data model or schema.
Volume Anomalies
involve checking for unexpected changes in the quantity or volume of data.
For each of these metrics we assign statuses to the checks, distinguished by colors for better visualization. These statuses make it easy to understand which checks succeeded, failed or were skipped across the entire data within the platform:
Additionally, we provide information on what portion of the data was monitored and what part was skipped, noting that this specifically applies to Table type datasets:
While all detailed information about tests and testing history is available in the description of each separate dataset along with alerts, activity and other useful information, on the dashboard we see the overall picture of data quality.
Recap
These metrics form integral components of a comprehensive data quality assessment strategy. By actively monitoring and addressing anomalies in these areas, organizations can ensure the accuracy, timeliness and alignment of their data with expected structures and vales, thereby enhancing overall data quality and reliability.
What’s next?
We believe that the interactivity of dashboards significantly contributes to data discovery. Going beyond visualizations, we are committed to enhancing our dashboards with filters, customizable reports and other useful functionalities.
If this captures your interest, we welcome your thoughts and suggestions. Feel free to follow us on
Medium
|
LinkedIn
or join our
Discussions
|
Community Slack
.
Additional Resources
Features
End-to-end Data Objects Lineage End-to-end Microservices Lineage Data Quality Test Results Import Pipeline Monitoring…
docs.opendatadiscovery.org
ODD is powered by Provectus
Powered by Provectus the ODD stands for a data discovery and observability service developed through a close partnership between Provectus and the Open Source Community.
AI-Consultancy and Solutions Provider | Provectus
Provectus provides Artificial Intelligence & Machine Learning Consulting Services, helping businesses achieve their…
provectus.com
Data Discovery
Open Data Discovery
Data Governance
Data Quality
Dashboard
--
--
Follow
Published in
Open Data Discovery
98 Followers
·
Last published
Jun 21, 2024
First open-source data discovery and observability platform
https://opendatadiscovery.org/
Follow
Follow
Written by
Arpine K
26 Followers
·
133 Following
Writer at Provectus
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Maintain Data Quality with Data Refactoring Tests | by Wayne Yaddow | Medium,"Maintain Data Quality with Data Refactoring Tests | by Wayne Yaddow | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Maintain Data Quality with Data Refactoring Tests
Wayne Yaddow
·
Follow
5 min read
·
Apr 3, 2024
--
Listen
Share
Ensuring data excellence through data model modifications, synchronizing storage, and rigorous testing.
A recent article on
DZone.com
subtly reminds readers of the need to test all types of data refactoring thoroughly.
Data refactoring is a process by which the structure of an existing database or dataset is changed — typically without altering its meaning or content. This process is akin to code refactoring in software development, which aims to improve the code’s structure and readability without changing its external behavior.
Refactoring is designed to make data more organized, accessible, and easier to work with, thus improving the efficiency and reliability of data management and data-driven applications.
Data refactoring is often complex due to the frequent impact of interconnected data structures and processes.
What Can Be Affected When Refactoring Data?
The results of data refactoring can affect various areas of the data architecture, data management practices, and the data itself. Therefore, tests should be considered to ensure that refactoring works as required and that no troublesome effects have emerged.
1) Data architecture
Schema design
: Refactoring can lead to changes in the database schema, such as table structures, relationships, and indexes. This can improve query performance, data integrity, and ease of data access.
Data storage
: Changes in how data is structured may influence the choice of storage technology (e.g., relational databases, NoSQL databases, data lakes) and how data are distributed across these storage solutions.
Data integration
: Refactoring can necessitate updates to data integration pipelines and ETL processes to accommodate new data structures or to optimize data flows.
2) Data management
Refactoring can impact data governance
policies and procedures, particularly data quality, privacy, and regulatory compliance. Adjustments may be needed to ensure that the refactored data meets all regulatory and internal standards.
Data quality management
: Improved data structures can enhance data quality by reducing redundancy, improving consistency, and simplifying data cleansing and validation processes.
Performance management
: Refactoring can significantly impact the performance of data queries and analyses by optimizing the organization and structure of data, potentially requiring updates to performance monitoring and optimization strategies.
3) The data itself
Data accessibility
: Refactoring can enhance data accessibility by making it easier for users to understand, query, and extract insights from the data. This can lead to more effective data-driven decision-making across the organization.
Data security
: Changes to data structures may require updates to security measures and access controls to adequately protect sensitive data.
Data usage
: As the ease of use and performance of the data improves, it may lead to broader or more practical uses of the data, potentially unlocking new opportunities for analysis, insights, and data-driven products and services.
Data refactoring is critical to maintaining an agile, efficient, scalable data environment. By thoughtfully addressing how data is structured, stored, and managed, organizations can ensure their data architecture remains robust and responsive to changing business needs and technological advancements.
Common Challenges and Mitigations When Trying To Test Complex Data Refactoring
Complex data refactoring entails substantial changes that can impact the data’s structure, relationships, or integrity on a broader scale than minor refactoring activities.
Here are two instances of intricate data restructuring and methods to assess each to maintain data quality and integrity.
*** INTEGRATING MULTIPLE DATA SOURCES INTO A SINGLE SYSTEM
Integrating multiple data sources into a single system is a complex refactoring task that involves unifying data from various origins into a coherent database or data warehouse. This process is critical for organizations looking to aggregate and analyze data across different systems for comprehensive insights. It addresses challenges like data format inconsistencies, duplication, and the reconciliation of different data models. The goal is to create a unified view that supports better decision-making and reporting.
Testing such integrations involves ensuring data consistency and integrity and that the merged data provides accurate, actionable insights without losing critical information from the sources.
Testing Challenges:
Heterogeneity of data sources
: The diversity in formats, standards, and data quality across sources complicates validation and transformation processes. Ensuring consistency and integrity across such varied data sets requires sophisticated mapping and transformation logic, which can be challenging to test comprehensively.
Data quality issues
: Inherent inconsistencies, duplicates, and errors in the original data sources can propagate through to the integrated system unless identified and corrected early, requiring extensive data cleaning and quality checks that can be time-consuming and complex.
Testing Scenarios:
Data validation
: Implement comprehensive validation checks to ensure that data from all sources conforms to unified format, type, and scale specifications. This could involve checking that date formats are standardized across all integrated data and that numeric data uses the same scale and precision.
Data profiling
: Perform extensive data profiling to identify and resolve discrepancies in data distribution, such as unexpected nulls or outliers, ensuring that data integration hasn’t introduced anomalies.
Regression testing
: To ensure accuracy and completeness, develop and run tests on common queries and reports spanning newly integrated data sources, comparing the results with those obtained from isolated sources.
*** TRANSFORMING COMPLEX DATA ETLs
Complex ETL transformations for data warehousing involve intricate processes of extracting data from various sources, transforming it to fit operational needs, and loading it into a data warehouse. This process is fundamental in building a data warehouse that consolidates diverse data into a unified format, making it readily available for analysis and decision-making.
Complex transformations might include data cleansing, deduplication, integration, and aggregation to ensure high quality and relevance. Testing these transformations requires a thorough examination of data flow from source to destination, validation of transformation logic, and ensuring that the data loaded into the warehouse accurately reflects the source data while meeting the informational needs of the business.
Testing Challenges:
·
Transformation logic complexity
: The complexity of ETL transformation logic, especially when involving conditional processing, data cleansing, and aggregation from multiple sources, makes it challenging to ensure accuracy. Testing must cover many scenarios and edge cases, requiring extensive test cases and validation rules.
·
End-to-end process reliability
: Ensuring the reliability and efficiency of the entire ETL process requires comprehensive end-to-end testing, which can be challenging to orchestrate, especially in dynamic environments where source data characteristics may change over time.
Testing Scenarios:
Transformation logic testing
: Execute detailed unit tests on each transformation rule or function to ensure it correctly processes the input data and produces the expected output. This might involve testing individual functions for data cleansing (e.g., removing duplicates, standardizing formats) and aggregation (e.g., summing sales data by region).
End-to-end ETL testing
: Conduct comprehensive tests that run the full ETL process on a subset of production data to validate the end-to-end data flow, transformations, and loading. This helps identify any integration issues or bottlenecks in the ETL pipeline.
Data integrity testing
: After ETL execution, integrity checks are performed on the data warehouse to ensure that all expected data has been accurately transferred and transformed and that referential integrity is maintained across tables.
Complex refactoring requires testing to verify correct execution and prevent adverse effects on data quality, performance, and usability. These testing procedures are crucial for reducing risks related to significant changes in data design and management practices.
#DataObservability
#DataPipeline
#DataPipelineQuality
#DataTesting
#DataEngineerTesting
Data Pipeline Test
Big Data
Data Engineer Skills
Etl Testing
Data Testing
--
--
Follow
Written by
Wayne Yaddow
96 Followers
·
232 Following
Senior Data Quality Analyst
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How I used data analytics to pass my exam | by Michał Tajchert | Medium,"How I used data analytics to pass my exam | by Michał Tajchert | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How I used data analytics to pass my exam
Michał Tajchert
·
Follow
5 min read
·
Feb 1, 2017
--
1
Listen
Share
This is a story about how I got accepted for master’s degree studies at Warsaw School of Economics (SGH) — passing its test without hard-learning economics beforehand.
Exams, exams, exams
Small background: SGH is one of top universities for economics in Poland as well in the CEE region. When going for master’s degree studies, you either have to have bachelor’s degree from SGH (then it depends on your grades) or pass a qualifying test (then it depends on your test score).
For me (a computer science graduate), I only had the second option. If you prepare for a qualifying test (knowledge of economics and foreign language ) you can study from:
Existing notes.
On your own — books.
Preparation course run by SGH itself (I recommend it)
A few test exams. Previous ones are not published at all.
I bought a book (read about 30 pages of it — and as you might guess, that’s not even a full chapter), took the weekend course (four months before the exam as I didn't know there were two dates… and took no notes) and most important…
Wikipedia
. However, my biggest issue was the lack of more sample tests, as this is a best way for me to learn when studying alone.
Solution
As that time I’d just graduated with a bachelor’s degree from PJATK in Warsaw, I had an overblown ego and I was sure there should be a better way to prepare! This qualifying exam is run twice a year (once since 2016) and even if the questions are not made public by the university itself, students must share them by word of mouth, friends or on Facebook groups. We live in an era of wide Internet access and if I encounter a hard question, I will most likely google it right away(ok,
maybe not everything
) and if I look for an educational topic, I will use most likely Wikipedia at some point… and Wikipiedia has a visit counter —
Chewie we’re home!
A small code block later (as everything is usually more interesting than just studying), I was getting all articles related to economics from Polish Wikipedia (for example, articles related to
economic policy
). A moment later, I was getting views per day for each article! If you go deeper, you can get
precise hours
after playing with database files a bit. Cool! But what is the purpose?
Views per day for an article about Nash equilibrium. This was a small inconvenience as exams are not held on the same date each year.
Results
Optimistic version —we know what phrases people study before exams (previous years’ notes, etc.), what they are checking just after the exam (most likely it was on the test itself), or in next couple of days (checking correct results, talking with friends etc.). If we observe a sudden spike in popularity of articles that are going to be on the exam (just before it),
most likely exam is compromised and leaked somewhere online
. In my case, I focused on articles that suddenly got popularity after the exam but not before — potentially unexpected topics.
Realistic version — as you might expect, Wikipedia is not only for SGH students and, for example, in 2014, just before the qualifying test itself, there was a nation-wide knowledge competition about economics. As you can guess, results in such case were useless. So any such anomaly is very problematic. Also, as you go back in time, results are less and less significant (due to not much internet access, fewer smartphones and smaller FB groups).
Results from 2014, sorted by home-calculated deviation from expected. The last title is the name of a nation-wide competition.
So, can I stop learning?
For now… no. The above work can be treated as an interesting fact with some other potential uses — detecting leaked exams, or trends in topics over the years for any kind of ‘mass’ exams (qualifying exam for SGH is for 2000+people). In my case, I don’t recall any topics or articles on my exam that I learned thanks to this way of studying. But mixing passion with learning was a perfect solution for me — I was learning economics while doing interesting stuff! Also, you don’t expect to get the same questions each year, so overall trends might be more useful than single articles from Wikipedia.
Last words
I’m very curious if with more and more smartphones (googling questions just after exam) and Facebook groups to share who-remember-what from test, this method will over time be more and more accurate. Maybe even to the point that universities will start to ‘mask‘ important exams with other tests or end-of-term examinations to generate noise in the analysis? Also, it would be very interesting to see trends over the years on what people learn before exams (like
Brand24
, but for education), or just monitor it to detect too many correct searches in the hours before the test itself to watch for potential leaks. For sure the future for Big Data is still very interesting as we uncover more and more interesting patterns, and myself, despite not going for a Big Data specialization, I’m very happy at E-Business.
And now I’m going back to old-school learning for my exams tomorrow. Let me know if you have any other possible applications for such Wikipedia-visit-monitoring strategy in a comments below!
About me
I’m a SGH student with E-Business specialization. Passed the qualifying test with score of 69 by 100, when 69 was a minimum to pass. Doing Android apps as a day-to-day job, such as:
SGH Daily
,
Pola
,
Kanarek
or
12Hours
. Loves startups and I’m currently involved in
Yawn
and
Icelytics
.
Education
University
Economics
Big Data
Wikipedia
Some rights reserved
--
--
1
Follow
Written by
Michał Tajchert
377 Followers
·
329 Following
Mobile Technology Lead @ MODIVO, author of Kanarek app.
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Orchestration with Data Quality: Announcing Data Reconciliation | by Hugo Lu | Orchestra’s Data Release Pipeline Blog | Nov, 2024 | Medium","Orchestration with Data Quality: Announcing Data Reconciliation | by Hugo Lu | Orchestra’s Data Release Pipeline Blog | Nov, 2024 | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Reconcile Data with a single Orchestra Task
Member-only story
Orchestration with Data Quality: Announcing Data Reconciliation
How to easily automate manual reconciliations of datasets in Orchestra
Hugo Lu
·
Follow
Published in
Orchestra’s Data Release Pipeline Blog
·
4 min read
·
Nov 12, 2024
--
1
Share
Not a medium member?
Read free here
.
Introduction
Today we’re excited to announce the latest standalone feature in Orchestra: Data Reconciliation, or “DataRec”, for short.
A problem many data teams face is the need to intermittently reconcile data between what exists in source and target databases. These often come in the form of tedious “can you check these numbers match?”-type requests. Not only are these tasks mundane for data teams, but time-consuming and increase context-switching.
With
DataRec
, Orchestra users can easily reconcile data between source databases like SQL Server and target databases like Snowflake, BigQuery or Databricks.
Because of Orchestra’s flexible integration paradigm, any integration can be used as a source or target. Data teams need only write the queries and specify the connections and Orchestra handles the rest.
Orchestra users can use this additional security to keep building trust with end stakeholders, which ultimately increases the rate of Data and AI adoption in an organisation, while (hopefully!) decreasing the number of those…
--
--
1
Published in
Orchestra’s Data Release Pipeline Blog
88 Followers
·
Last published
Nov 12, 2024
A blog by Orchestra on Data Release Pipelines
Follow
Written by
Hugo Lu
9.5K Followers
·
51 Following
I write on Data engineering and the coolest data stuff. CEO@ Orchestra, the best-in-class unified control plane for dataops.
https://app.getorchestra.io/signup
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality with Databricks Delta Live Tables | by Sandip Roy | Medium,"Data Quality with Databricks Delta Live Tables | by Sandip Roy | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality with Databricks Delta Live Tables
Sandip Roy
·
Follow
5 min read
·
Aug 8, 2022
--
Listen
Share
What is Data Quality?
Data quality is the measure of how well suited a data set is to serve its specific purpose. Measures of data quality are based on data quality characteristics such as accuracy, completeness, consistency, validity, uniqueness, and timeliness.
Now while you generally write unit tests for your code, but do you also test your data? Incoming data quality can make or break your application. Incorrect, missing, or malformed data can have a large impact on production systems.
Data Quality in Databricks
Though Databricks is known for its excellence in data processing, recently Databricks released new frameworks to make data governance easier and more efficient. Without going into much details on how overall governance aspect is handled, in this article, we would discuss how Databrick’s Delta Live Tables (DLT) framework ensures different data quality rules are applied and then records are segregated into cleansed (good) and quarantined (bad) records by calculating data quality metrics on your dataset, defining and verifying data quality constraints etc. The advantage is instead of implementing checks and verification algorithms on your own, you can focus on describing how your data should look.
Without spending too much time on the theoretical aspects, let’s jump into few cases/scenarios. To begin, let’s start with sample data set below:
And then read raw data in a DLT table called person_raw as below:
Once data is read into person_raw table, apply data quality rules and add bad record indicator against each records like below:
Then create workflow using WorksflowsàDelta Live Tables with following sample specification:
Now if we run this workflow, we can see the results how records are marked for good/bad records in tables created under metastore db. Please note if you don’t specify any target db (e.g. sample_db as shown above), tables would be created under default db.
Here’s the result:
So through column
is_quarantined = “true”
, you can track all the records that got failed during the validation process.
Now while this approach is highly performant (as it scans entire records once), the only trade-off is — since this doesn’t use expectations framework, data quality metrics do not appear in the event logs or the pipelines UI.
In order to have rich/enhanced DQ metrices, we would have to go for expectation based approach where we can bifurcate good and bad records into different tables, fail pipeline if there is any bad records, get data validation statistics as metrices etc.
I’ll illustrate sample implementations (how we segregate good and bad records) below as such to give an idea. Please note you can use the expect, expect or drop, and expect or fail expectations with Python or SQL queries to define a single data quality constraint while you have to use one or more data quality constraints in Python pipelines using the @expect_all, @expect_all_or_drop, and @expect_all_or_fail decorators.
Once above is executed, the graph will look like this:
Also when you check the metastore db, you can see following:
Again this approach has got it’s own disadvantage and that is, that it generates the quarantine table by processing the data twice. If you don’t want this performance overhead, you can use the constraints directly within a query to generate a column indicating the validation status of a record (i.e. previous approach as demonstrated above).
But you can easily derive from above implementations how quickly and efficiently we can build scalable data validation framework with minimum of effort. Hope you liked this!
References
https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-quickstart.html
https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html
Thanks for reading. In case you want to share your case studies or want to connect, please ping me via
LinkedIn
Data Quality
Databricks
Delta Live Tables
--
--
Follow
Written by
Sandip Roy
145 Followers
·
1 Following
Bigdata and Databricks Practice Lead at Wipro Ltd
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality Automation With Apache Spark | by Tanya Lutsaievska | People.ai Engineering | Medium,"Data Quality Automation With Apache Spark | by Tanya Lutsaievska | People.ai Engineering | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality Automation With Apache Spark
Tanya Lutsaievska
·
Follow
Published in
People.ai Engineering
·
6 min read
·
Aug 1, 2019
--
Listen
Share
How People.ai built an automated data quality system.
“Data that is loved tends to survive.” — Kurt Bollacker
Data dashboard displays possible root causes of incorrectly computed time spent on daily sales activities. Notice how the main root cause changes over time: you always need fresh data to make relevant decisions.
Monitoring data quality is crucial to understand customers and provide them with a great product experience. Data quality informs decision-making in business and drives product development. For example, one of
People.ai
’s features is capturing all activity from
Sales
and
Marketing
. We analyze activities ingested from a user’s email inbox, calendar, and CRM system and display actionable insights to help sales and marketing teams take the best next action.
As our system rapidly scaled, we began to see abnormal numbers for certain users, such as 70 hours spent in a day. This number seemed unrealistic (unless you time travel with a
Time-Turner
). After we manually investigated the problem, we didn’t find any bugs. The algorithm worked as expected. However, we have identified several edge cases, and these were impacting the reports. Identifying these edge cases helped us improve our model.
The goal around monitoring activity data quality at People.ai is to identify outliers and various edge cases without customer involvement and improve our platform to provide the best experience for every user. We set out on a journey to build a rigorous quality assurance system that verifies data at every stage of a pipeline. Over the last three years, we have iterated our data quality validation flow from manual investigations and ad-hoc queries, to automated tests in
CircleCI
, to a fully automated
Apache Spark
pipeline.
Semi-manual checks.
In the early days, we manually investigated edge cases by running ad-hoc scripts and queries in a
Jupyter notebook
. To illustrate, below is pseudo-code of a test that verifies whether we have filtered out too many emails sent by users during the intake and analysis:
analyzed_emails_count = len([entry for entry in data if entry[‘outbound’]])
total_emails_count = len([entry for entry in data if entry[‘from’] == user_info.user_email])
emails_mismatch = 100 — analyzed_emails_count * 100 / total_emails_count
assert emails_mismatch <= 5, ‘Filtered emails value exceeds allowed threshold of 5 percent’
To define a threshold for the test, we analyzed data derived from emails sent by our users. The analysis showed that, on average, we filter out five percent or less of sent emails. These are usually some non-work related emails. For example, we can use five percent as a threshold to verify if we missed any edge cases that would cause over-filtering. If the number of filtered emails is greater than the expected threshold, we can look for root causes, such as various aliases a user might use that we failed to detect.
Those queries became a part of our data quality analysis routine. But as we started to grow our customer base, the data became too large to manually check. Validation used to take more than 20 hours of work a week.
Next came automation with CircleCI.
In this phase, we wrote down all checks we performed manually as unit tests and built them into the pipeline. We used those tests to verify activity data for every new user upon registration.
Streamlining semi-automated testing flow with CircleCI
Registration of a new user would trigger a “data quality checking job” in CircleCI. If at least one of the data quality tests failed, an internal ticket was created to investigate a root cause before the system marked the user account as ready.
We saw three main challenges with this approach:
This flow involves a manual step to review tests that failed and identify a root cause.
User registration was not complete until the issues were fixed. Whenever a test failed, we only had 48 hours to fix new user registration.
Tests were run for new users upon registration only. We wanted to ensure the data validation tests were in place at every stage of a pipeline every time we ingest activities for existing users as well.
It didn’t take long for us to realize that running data quality tests for each user in CircleCI was not scalable, involved too much time to identify the root causes, and caused production database overload. This led us to Apache Spark.
Fully Automated Data Quality Apache Spark Job Streaming
As our customer base became bigger, we started seeing a dramatic increase in the amount of data we ingested from users’ mailboxes, calendars, and CRM systems. Tests in CircleCI set up to run at the user level were no longer scalable.
To improve the flow automated with CircleCI, we replicated all tests by a scheduled Spark job in Databricks. One of the advantages of Apache Spark is executing code in parallel across many different machines. By introducing Spark jobs, we moved from running CircleCI tests upon user registration only, to setting up a weekly job to evaluate all data at user level at all times.
From a technical standpoint, we chose Databricks mainly because of its job scheduling capability. We also wanted the simplicity of cluster configuration that does not require DevOps engineer involvement. It became possible with Databricks as it provides Apache Spark as a hosted solution.
We also automated investigations that our support engineering team usually performs manually to check for all possible root causes. This helped us eliminate the manual step we had in the previous flow.
As of right now, we have numerous tests in place to validate data quality on user level that scheduled to run weekly. Our goal is to run Spark job daily and eventually embed data validation checks into the pipeline.
Data Quality Pipeline
The Spark Data Quality Pipeline
The Spark pipeline includes three jobs:
ETL layer
Data quality checking layer
The reporting layer
The
ETL layer
involves a Spark job that extracts a snapshot from multiple production databases, checks and corrects data type inconsistencies, and moves the transformed data into a Hive table. This step is important to keep the data clean and avoid production database overload.
The
data quality checking layer
contains the code that includes a set of rules for checking and measuring data quality. We apply these rules to measure data quality metrics in the following dimensions:
Data completeness.
Verify if there was an unexpected loss of data during activity intake or due to over filtering.
Data accuracy and integrity.
A set of rules to identify outliers and edge cases.
Data consistency and stability.
Verify if data is accurate, consistent, and stable for all users and that the numbers we report stay within an expected range.
Lastly,
the reporting layer
includes a data dashboard that displays the state of data quality across the board and email notifications sent to the support engineering team whenever data anomalies or outliers are detected, such as failure to detect email blasts.
Data dashboard that shows the quality of job title data detected by People.ai algorithms for people with which sales reps engage
The chart above shows whether People.ai detected a job title of a person correctly (such as Senior Account Manager), failed to correctly capture a correct job title, or detected an incomplete job title. For example, an incomplete job title might be the title “Director” but without a department or the title “Business Development” without seniority.
Data Quality Validation is Our Responsibility, Not the Customer’s
Data quality is a critical component of our system. At People.ai, data quality is placed at the same level of importance as integration testing is for software development.
We found the best way to validate activity data quality is by asking data itself what does not work and why it does not work. Finding an edge case and addressing it early helps us provide smoother user onboarding process and better customer experience.
Activity data quality validations should be automated as much as possible. Our goal is to embed validations into the data pipeline code, but in a manner that allows it to be effortlessly changed. Depending on the criticality of the data and validation, we want our pipeline to either fail completely or to flag and report the issue, and continue processing.
This has been our journey so far at People.ai. We continue to learn and grow overtime to ensure that our customers can have full faith in the quality of data we deliver.
See All Open Opportunities By Visiting:
https://people.ai/careers/
Software Development
Data Quality
Data Science
Apache Spark
Databricks
--
--
Published in
People.ai Engineering
305 Followers
·
Last published
Aug 24, 2023
Practical Data Science, Engineering, and Product
Follow
Written by
Tanya Lutsaievska
17 Followers
·
8 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Mastering the unittest Module in Python: Best Practices for Big Data Testing with PySpark | by Sachan Pratiksha | Medium,"Mastering the unittest Module in Python: Best Practices for Big Data Testing with PySpark | by Sachan Pratiksha | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Mastering the
unittest
Module in Python: Best Practices for Big Data Testing with PySpark
Sachan Pratiksha
·
Follow
3 min read
·
Sep 2, 2024
--
Listen
Share
Introduction
Testing is a crucial aspect of software development, ensuring that code behaves as expected. In the realm of Big Data, particularly when working with PySpark, effective testing can be challenging due to the scale and complexity of data. However, Python’s
unittest
module provides a robust framework for creating and running tests, making it an excellent choice for validating your PySpark jobs.
This article will guide you through the fundamentals of the
unittest
module, along with strategies for effectively utilizing it in a Big Data environment with PySpark.
Understanding the
unittest
Module
Python’s
unittest
module is a built-in testing framework inspired by Java's JUnit. It supports test automation, aggregation of tests into collections, and sharing of setup and shutdown code for tests.
Key Features:
Test Case
: The smallest unit of testing. It checks for specific responses to a particular set of inputs.
Test Suite
: A collection of test cases, test suites, or both.
Test Runner
: A component that orchestrates the execution of tests and provides the outcome to the user.
Test Fixtures
: Setup and teardown methods for preparing the environment for tests.
Getting Started with
unittest
Here’s a simple example to demonstrate how to create a basic test using the
unittest
module.
import unittest
def add(a, b):
return a + b
class TestMathOperations(unittest.TestCase):
def test_add(self):
self.assertEqual(add(2, 3), 5)
if __name__ == '__main__':
unittest.main()
In this example:
TestMathOperations
is a subclass of
unittest.TestCase
, containing our test case
test_add
.
assertEqual
is an assertion method to verify that the sum of
2
and
3
is indeed
5
.
Utilizing
unittest
with PySpark
Testing PySpark applications involves a few additional considerations, such as setting up a Spark session, handling large datasets, and ensuring that distributed computations yield correct results.
1. Setting Up the Spark Context in Tests
Before you can run any PySpark code, you need to create a Spark session or context. You can do this within the
setUp
method of your
unittest.TestCase
subclass.
from pyspark.sql import SparkSession
class SparkTestCase(unittest.TestCase):
@classmethod
def setUpClass(cls):
cls.spark = SparkSession.builder \
.master(""local[*]"") \
.appName(""PySparkUnitTest"") \
.getOrCreate()
@classmethod
def tearDownClass(cls):
cls.spark.stop()
Here,
setUpClass
is a class method that initializes a Spark session before any tests are run, and
tearDownClass
stops the session after all tests have completed.
2. Testing DataFrames
DataFrames are central to PySpark, so you’ll often need to validate that DataFrames are created and transformed correctly.
from pyspark.sql import Row
class TestDataFrameOperations(SparkTestCase):
def test_dataframe_creation(self):
data = [Row(name=""Alice"", age=29), Row(name=""Bob"", age=31)]
df = self.spark.createDataFrame(data)
result = df.collect()
expected = [Row(name=""Alice"", age=29), Row(name=""Bob"", age=31)]
self.assertEqual(result, expected)
In this example, we create a DataFrame and compare the collected results with an expected list of Rows.
3. Handling Large Datasets
When dealing with large datasets, it’s often impractical to validate entire DataFrames. Instead, you can validate specific aspects, such as schema correctness, specific row values, or the results of transformations.
def test_large_dataframe(self):
df = self.spark.read.csv(""large_dataset.csv"", header=True)
# Example: Check if the DataFrame has the expected schema
expected_schema = [""column1"", ""column2"", ""column3""]
self.assertEqual(df.columns, expected_schema)
# Example: Check if a specific transformation yields the expected result
transformed_df = df.withColumn(""new_column"", df[""column1""] + 1)
self.assertTrue(""new_column"" in transformed_df.columns)
Best Practices for Using
unittest
with PySpark
Use Fixtures Wisely
: Utilize
setUpClass
and
tearDownClass
to manage the Spark session lifecycle, ensuring tests are isolated and do not interfere with one another.
Test Incrementally
: Break down your PySpark jobs into smaller functions and test them individually. This approach allows you to catch errors early and makes debugging easier.
Mock External Dependencies
: When your PySpark job interacts with external systems (e.g., databases, file systems), consider mocking these dependencies to focus on the logic within your Spark job.
Performance Considerations
: Be mindful of the time tests take to run, especially with large datasets. Use smaller, representative samples for testing where possible.
Parallel Testing
: Leverage parallel test execution to speed up your test suite, especially useful when running numerous tests involving Spark operations.
Conclusion
Effective testing in Big Data environments is critical to ensure the reliability and performance of your applications. By leveraging Python’s
unittest
module with PySpark, you can create robust tests that validate your code against a variety of scenarios, from simple DataFrame transformations to complex distributed computations. Following best practices will further enhance your testing strategy, making it both efficient and effective.
By incorporating these techniques into your PySpark development workflow, you’ll be better equipped to deliver high-quality, error-free code in the demanding world of Big Data.
Python
Python Programming
Data Engineering
Pyspark
Unit Testing
--
--
Follow
Written by
Sachan Pratiksha
4 Followers
·
108 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality with Great Expectation in Python and Airflow | by Maik Paixão | Medium,"Data Quality with Great Expectation in Python and Airflow | by Maik Paixão | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Quality with Great Expectation in Python and Airflow
Guide on using Great Expectations, a powerful Python library, to enhance data quality in your pipelines.
Maik Paixão
·
Follow
5 min read
·
Dec 8, 2023
--
Listen
Share
Data quality is the cornerstone of reliable analytics and informed decision-making. It refers to the accuracy, consistency, completeness, reliability, and relevance of data.
High-quality data is crucial for generating valid insights, while poor data quality can lead to misleading analysis and erroneous business decisions. This section delves into the nuances of data quality, highlighting common issues like inaccuracies, duplications, and missing values.
Understanding these challenges is the first step in ensuring your data is trustworthy and fit for purpose. We’ll also discuss the broader impacts of data quality on business outcomes, emphasizing why it’s an essential aspect of any data strategy.
This tutorial provides an in-depth exploration of Great Expectations, guiding you through its installation, basic concepts, and advanced functionalities. By the end, you’ll be equipped to implement robust data quality checks seamlessly within your Python projects, enhancing the integrity and reliability of your data-driven insights.
Great Expectations ?
Great Expectations is an innovative open-source library in Python, designed to enhance data quality and testing. This tool provides a robust framework for validating, documenting, and profiling your data, which is critical for maintaining high data quality standards.
I’ll guide you through the installation process, making it easy to incorporate into your Python environment. By understanding Great Expectations’ fundamental concepts and capabilities, you’ll be well-equipped to start implementing effective data quality checks in your data projects.
Setting Up your Project
Creating on your first project with Great Expectations begins with setting up the necessary environment.
Start by installing the package using pip:
pip install great_expectations
Once installed, create a new project:
great_expectations init
This command sets up the structure of your project, including directories for Expectations, Checkpoints, and Data Docs.
Expectation Suites:
Define specific tests for your data.
Data Context:
Manages the configuration of your project.
Data Docs:
Visual representations of your Expectations and validation results.
By following these steps, you’ll have a foundational setup ready for creating and managing your data quality checks.
Creating Expectations
Expectations are the core of Great Expectations, serving as assertions about your data’s quality.
In this section, we’ll explore how to create and define these expectations.
First, select a data source to work with. For example, if you’re using a CSV file:
from great_expectations.dataset import PandasDataset
import pandas as pd
data = pd.read_csv('your_data_file.csv')
dataset = PandasDataset(data)
Then, apply expectations to your dataset. For instance, to ensure a column contains unique values:
dataset.expect_column_values_to_be_unique(column='your_column_name')
Or to verify the number of rows in a table falls within a specific range:
dataset.expect_table_row_count_to_be_between(min_value=100, max_value=2000)
Great Expectations offers a wide array of built-in expectations, and you can also create custom ones tailored to your specific data needs.
Validating Data with Checkpoints
Checkpoints in Great Expectations are a way to automate the validation of your data against the defined Expectations.
Start by configuring a checkpoint:
from great_expectations.data_context.types.base import CheckpointConfig
checkpoint_config = CheckpointConfig(
name=""sample_checkpoint"",
config_version=1,
class_name=""SimpleCheckpoint"",
validations=[
{
""batch_request"": {
""datasource_name"": ""your_datasource"",
""data_connector_name"": ""default_inferred_data_connector_name"",
""data_asset_name"": ""your_data_asset"",
},
""expectation_suite_name"": ""your_expectation_suite"",
}
],
)
context.add_checkpoint(**checkpoint_config.to_dict())
Now, run the checkpoint to validate your data:
results = context.run_checkpoint(checkpoint_name=""sample_checkpoint"")
This code initiates the validation process using the specified checkpoint, which applies your defined Expectations to the data batch.
Successful validation will confirm that your data meets the quality standards set in your Expectation Suite, while any deviations will be reported, allowing for quick identification and rectification of data issues.
Integrating with Data Pipelines (Airflow)
Integrating Great Expectations with your data pipelines enhances the automation of data quality checks.
For example, integrating with an ETL pipeline in Python might look like this:
# Assuming you have an ETL function
def etl_process():
# Your ETL code here
pass
# After completing ETL, run a Great Expectations checkpoint
def run_data_quality_checks():
context.run_checkpoint(checkpoint_name=""your_checkpoint_name"")
# Main ETL process
def main():
etl_process()
run_data_quality_checks()
if __name__ == ""__main__"":
main()
In this snippet, after the ETL process completes, a Great Expectations checkpoint is triggered to validate the data. This ensures data quality checks are seamlessly integrated into your regular data processing activities.
Similarly, for more complex workflows, you can integrate Great Expectations with orchestration tools like Apache Airflow, using Airflow operators to trigger data quality checks as part of your pipeline:
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
default_args = {
'owner': 'airflow',
'start_date': datetime(2023, 1, 1),
# Other default args
}
dag = DAG('etl_with_data_quality',
default_args=default_args,
schedule_interval='@daily')
def etl_task():
# ETL logic here
pass
def data_quality_task():
context.run_checkpoint(checkpoint_name=""your_checkpoint_name"")
etl = PythonOperator(
task_id='etl',
python_callable=etl_task,
dag=dag)
data_quality = PythonOperator(
task_id='data_quality_check',
python_callable=data_quality_task,
dag=dag)
etl >> data_quality
This integration allows automated execution of data quality checks within your existing data workflows, ensuring continuous monitoring and validation of data quality standards.
Advanced Features
Great Expectations not only provides essential data validation tools but also offers advanced features for sophisticated data quality management. One such feature is data profiling, which automatically generates Expectations based on the characteristics of your dataset.
This is particularly useful for getting a quick start on creating Expectations or understanding new data sources.
from great_expectations.profile.basic_suite_builder_profiler import BasicSuiteBuilderProfiler
suite = BasicSuiteBuilderProfiler().profile(dataset)
context.save_expectation_suite(suite, ""your_new_expectation_suite_name"")
This code snippet illustrates how to use data profiling to create an Expectation Suite. Additionally, managing Expectation Suites becomes crucial as your project grows. Organizing these suites, maintaining version control, and documenting changes are best practices that ensure long-term manageability.
For scaling data quality checks, consider optimizing performance for large datasets. This might involve selectively applying Expectations or using batch processing to manage resource utilization effectively.
Embracing these advanced features and best practices enhances the robustness of your data quality framework, ensuring it remains efficient and scalable as your data environment evolves.
That’s Great Expectations
Remember, the journey to excellent data quality is continuous. As data evolves, so should our strategies to maintain its integrity. Great Expectations offers the flexibility and depth required to meet these evolving challenges. By leveraging its capabilities, we can ensure our data remains a reliable foundation for insights and decisions, ultimately driving success in our data-centric initiatives.
For further exploration, the Great Expectations
documentation
is an invaluable resource. Additionally, the community forums and GitHub repository are great places for support and to stay updated with the latest developments. Keep experimenting, learning, and elevating the quality of your data with Great Expectations.
LinkedIn:
https://www.linkedin.com/in/maikpaixao/
Twitter:
https://twitter.com/maikpaixao
Facebook:
https://www.facebook.com/maikpaixao
Youtube:
https://www.youtube.com/@maikpaixao
Instagram:
https://www.instagram.com/datamaikpaixao/
Github:
https://github.com/maikpaixao
Data Science
Data Engineering
Data Quality
Great Expectations
Python Programming
--
--
Follow
Written by
Maik Paixão
68 Followers
·
57 Following
Data Scientist with expertise in building modern analysis on financial instruments.
http://www.maikpaixao.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Automated Data Quality Checks with Deequ using Spark | by Tiger Analytics | Medium,"Automated Data Quality Checks with Deequ using Spark | by Tiger Analytics | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Automated Data Quality Checks with Deequ using Spark
Tiger Analytics
·
Follow
5 min read
·
Sep 3, 2020
--
Listen
Share
Author:
Joshan Kotni
Introduction
When dealing with data, the main factor to be considered is the quality of data. Especially in the Big data environment, data quality is critical. Having inaccurate or flawed data will produce incorrect results in data analytics. Many developers test the data manually before training their model with the available data. This is time-consuming, and there are possibilities of committing mistakes.
Deequ
Deequ is an open-sourced framework for testing the data quality. It is built on top of Apache Spark and is designed to scale up to large data sets. Deequ is developed and used at Amazon for verifying the quality of many large production datasets. The system computes data quality metrics regularly, verifies constraints defined by dataset producers, and publishes datasets to consumers in case of success.
Deequ allows us to calculate data quality metrics on our data set, and also allows us to define and verify data quality constraints. It also specifies what constraint checks are to be made on your data. There are three significant components in Deequ. These are Constraint Suggestion, Constraint Verification, and Metrics Computation. This is depicted in the image below.
Deequ Components
This blog provides a detailed explanation of these three components with the help of practical examples.
1.
Constraint Verification:
You can provide your own set of data quality constraints which you want to verify on your data. Deequ checks all the provided constraints and gives you the status of each check.
2.
Constraint Suggestion:
If you are not sure of what constraints to test on your data, you can use Deequ’s constraint suggestion. Constraint Suggestion provides a list of possible constraints you can test on your data. You can use these suggested constraints and pass it to Deequ’s Constraint Verification to check your data quality.
3.
Metrics Computation:
You can also compute data quality metrics regularly.
Now, let us implement these with some sample data.
For this example, we have downloaded a sample csv file with 100 records. The code is run using Intellij IDE (you can also use Spark Shell).
Add Deequ library
You can add the below dependency in your pom.xml
<dependency>
<groupId>com.amazon.deequ</groupId>
<artifactId>deequ</artifactId>
<version>1.0.2</version>
</dependency>
If you are using Spark Shell, you can download the Deequ jar as shown below-
wget
http://repo1.maven.org/maven2/com/amazon/deequ/deequ/1.0.1/deequ-1.0.2.jar
Now, let us start the Spark session and load the csv file to a dataframe.
val
spark
= SparkSession.
builder
()
.master(“local”)
.appName(“deequ-Tests”).getOrCreate()
val
data
=
spark
.read.format(“csv”)
.option(“header”,true)
.option(“delimiter”,”,”)
.option(“inferschema”,true)
.load(“C:/Users/Downloads/100SalesRecords.csv”)
data.show(5)
The data has now been loaded into a data frame.
data.printSchema
Schema
Note:
Ensure that there are no spaces in the column names. Deequ will throw an error if there are spaces in the column names.
Constraint Verification
Let us verify our data by defining a set of data quality constraints.
Here, we have given duplicate check(isUnique), count check (hasSize), datatype check(hasDataType), etc. for the columns we want to test.
We have to import Deequ’s verification suite and pass our data to that suite. Only then, we can specify the checks that we want to test on our data.
import com.amazon.deequ.VerificationResult.
checkResultsAsDataFrame
import com.amazon.deequ.checks.{Check, CheckLevel}
import com.amazon.deequ.constraints.ConstrainableDataTypes
import com.amazon.deequ.{VerificationResult, VerificationSuite}//Constraint verification
val
verificationResult
: VerificationResult = {
VerificationSuite
()
.onData(data) //our input data to be tested
//data quality checks
.addCheck(
Check(CheckLevel.
Error
, “Review Check”)
.isUnique(“OrderID”)
.hasSize(_ == 100)
.hasDataType(“UnitPrice”,ConstrainableDataTypes.
String
)
.hasNumberOfDistinctValues(“OrderDate”,_>=5)
.isNonNegative(“UnitCost”))
.run()
}
On successful execution, it displays the below result. It will show each check status and provide a message if any constraint fails.
Using the below code, we can convert the check results to a data frame.
//Converting Check results to a DataFrame val verificationResultDf =
checkResultsAsDataFrame
(
spark
, verificationResult)
verificationResultDf.show(false)
Constraint Verification
Constraint Suggestion
Deequ can provide possible data quality constraint checks to be tested on your data. For this, we need to import ConstraintSuggestionRunner and pass our data to it.
import com.amazon.deequ.checks.{Check, CheckLevel}
import com.amazon.deequ.suggestions.{ConstraintSuggestionRunner, Rules}//Constraint Suggestion
val suggestionResult = {
ConstraintSuggestionRunner
()
.onData(data)
.addConstraintRules(Rules.
DEFAULT
)
.run()
}
We can now investigate the constraints that Deequ has suggested.
import
spark
.implicits._
val suggestionDataFrame = suggestionResult.constraintSuggestions.flatMap {
case (column, suggestions) =>
suggestions.map { constraint =>
(column, constraint.description, constraint.codeForConstraint)
}
}.toSeq.toDS()
On successful execution, we can see the result, as shown below. It provides automated suggestions on your data.
suggestionDataFrame.show(4)
Constraint Suggestion
You can also pass these Deequ suggested constraints to VerificationSuite to perform all the checks provided by SuggestionRunner. This is illustrated in the following code.
val allConstraints = suggestionResult.constraintSuggestions
.flatMap { case (_, suggestions) => suggestions.map { _.constraint }}
.toSeq
val generatedCheck = Check(CheckLevel.
Error
, “generated constraints”, allConstraints)//passing the generated checks to verificationSuite
val verificationResult =
VerificationSuite
()
.onData(data)
.addChecks(
Seq
(generatedCheck))
.run()
val resultDf =
checkResultsAsDataFrame
(
spark
, verificationResult)
Running the above code will check all the constraints that Deequ suggested and provide the status of each constraint check, as shown below.
resultDf.show(4)
Metrics Computation
You can compute metrics using Deequ. For this, you need to import AnalyzerContext.
import com.amazon.deequ.analyzers._
import com.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame
import com.amazon.deequ.analyzers.runners.{AnalysisRunner, AnalyzerContext}//Metrics Computation
val analysisResult: AnalyzerContext = {
AnalysisRunner
// data to run the analysis on
.
onData
(data)
// define analyzers that compute metrics .addAnalyzers(
Seq
(Size(),Completeness(“UnitPrice”),ApproxCountDistinct(“Country”),
DataType
(“ShipDate”),Maximum(“TotalRevenue”)))
.run()
}
val metricsDf =
successMetricsAsDataFrame
(
spark
, analysisResult)
Once the run is successful you can see results as below.
metricsDf.show(false)
Metrics Computation
You can also store the metrics that you computed on your data. For this, you can use Deequ’s metrics repository. To know more about this repository, click
here
.
Conclusion
Overall, Deequ has many advantages. We can calculate data metrics, define, and verify data quality constraints. Even large datasets that consist of billions of rows of data can be easily verified using Deequ. Apart from Data quality checks, Deequ also provides Anamoly Detection and Incremental metrics computation.
Originally published at
https://www.tigeranalytics.com
on September 3, 2020.
Big Data
Data Engineering
Machine Learning
--
--
Follow
Written by
Tiger Analytics
79 Followers
·
1 Following
A global leader in AI and Analytics. We provide certainty to shape a better tomorrow.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Unit tests for better data quality | by Chloe Lubin | Women in Technology | Medium,"Unit tests for better data quality | by Chloe Lubin | Women in Technology | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Unit tests for better data quality
Chloe Lubin
·
Follow
Published in
Women in Technology
·
4 min read
·
Oct 23, 2023
--
Share
How unit tests in software engineering can help inform data quality control in analytics
Photo by
ThisisEngineering RAEng
on
Unsplash
💻 What are unit tests?
In software engineering, unit tests are tests applied to individual modules of the code before the software is tested as a whole during the integration testing phase. Unit tests ensure that the deployment of the change is functional, will not introduce breaking changes, and returns expected values. While it is a time-consuming process, it’s a critical step in the development workflow.
Similarly, the concept of
unit tests
can be applied to analytics, where data is the code we’re looking to integrate into a production environment.
🤔 How to set up unit tests for analytics?
A few solutions exist to automate unit testing, especially for simple tests like duplication, missing values, outliers, etc. Examples of those include
pandas_profiling
and
pydqc
, which are Python libraries.
Some companies offer resources to create data quality rules, data profiling, and rule mappings, such as Ataccama or First Eigen (see
this article
, published by global analytics consulting group Eckerson).
However, when it comes to controlling the quality of the data for more complex scenarios (in cases where…
--
--
Published in
Women in Technology
1.7K Followers
·
Last published
13 hours ago
Women in Tech is a publication to highlight women in STEM, their accomplishments, career lessons, and stories.
Follow
Written by
Chloe Lubin
243 Followers
·
111 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Enhancing Data Quality with OpenDataDiscovery and GreatExpectations | by Pavel Makarichev | Open Data Discovery,"Enhancing Data Quality with OpenDataDiscovery and GreatExpectations | by Pavel Makarichev | Open Data Discovery
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Enhancing Data Quality with OpenDataDiscovery and GreatExpectations
Pavel Makarichev
·
Follow
Published in
Open Data Discovery
·
5 min read
·
Mar 17, 2023
--
Listen
Share
Image by vector_corp
on Freepik
Greetings, everyone! At OpenDataDiscovery, we understand the significance of providing our users with a comprehensive understanding of their data. This encompasses critical information such as data flow lineage, data structure, dataset ownership, and data health. It’s important to note that even a small dataset can have a significant impact on the entire data pipeline. Therefore, having a tool that can swiftly alert you and help you identify problematic areas is essential.
OpenDataDiscovery + GreatExpectations
OpenDataDiscovery Test Reports
Service-level agreement
Overall
Resources
Important notice
Before we start, I want to pause for a moment and clarify that OpenDataDiscovery does not perform any checks by itself. Instead, we offer integrations with the most popular projects for data quality testing.
OpenDataDiscovery + GreatExpectations.
Great Expectations is an excellent tool for testing data quality. It offers useful documentation, a supportive community, and a well-defined API for seamless integration. One of its key features is the ability to create custom actions, which are Python classes derived from the
ValidationAction
class. These custom actions should override
run()
method, which is executed at runtime with
ValidationResults
.
How to start using OpenDataDiscovery Action.
Actions are a powerful tool in Great Expectations, allowing you to perform a wide range of tasks such as filtering data, converting data types, and performing calculations. These actions can be defined within your pipeline using the
action_list
parameter, which takes a list of dictionaries containing information about each action.
To start using the ODDAction in Great Expectations, the first step is to install the
odd-great-expectations
package from PyPI and add it to the
action_list
list in your
<checkpoint_name>.yml
file. Once you've done that, you can start using ODDActions to validate and transform your data as it moves through the pipeline.
Example checkpoint using ODDAction
To start using action we additionally need to specify a few configuration parameters.
First, you’ll need to set the
platform_host
parameter to the location of your OpenDataDiscovery Platform. This can be a URL such as
http://localhost:8080
, depending on where your platform is hosted.
Next, you’ll need to obtain a
platform_token
from the platform itself. This token will allow your data source to communicate securely with the platform. You can follow the instructions on the
OpenDataDiscovery documentation to create a token
.
Once you have your
platform_host
and
platform_token
, you can set them using the
ODD_PLATFORM_HOST
and
ODD_PLATFORM_TOKEN
environment variables, respectively.
Finally, you’ll need to specify a unique
data_source_name
that will be used by the platform to identify the source of your data. This name should be descriptive and unique to your data source, such as
local_gx_test
.
More information about how to start working with an action you can find at
odd-great-expectations
repository.
The next time we run a checkpoint,
ValidationResults
will be automatically handled and ingested into the OpenDataDiscovery Platform.
Great Expectations run example.
We can see that we ran the same validation suite
validate_bank_data
against two datasets, but one of them has failed the validation. To investigate this further, let’s take a look at the OpenDataDiscovery UI.
OpenDataDiscovery Test Reports
The platform provides a user-friendly interface for exploring data quality tests, which can help you quickly identify any issues. By navigating to the relevant dataset within the UI, you can view the details of the failed validation and take appropriate action to address the underlying issue.
List of GX expectations attached that were run on BankChurners_Bad dataset
Simply knowing that something went wrong with a validation run is not sufficient. To get a better understanding of what went wrong and how to address the issue, we can view additional information on the details page for each validation run.
The details page provides a comprehensive view of the validation result. By examining this information, we can gain insights into the underlying issues affecting the data quality and take appropriate action to resolve them.
Service-level agreement
OpenDataDiscovery provides one more interesting feature known as the Service-Level Agreement (SLA) score indicator. This score allows us to determine the severity of a particular test and choose an appropriate course of action based on the results.
Service-level agreement for BankChurners_Bad dataset
By default, each expectation has a
MAJOR
severity level. However, we can easily adjust the severity level of an expectation at any time by visiting the test reports page. This feature is especially useful when certain expectations are more critical than others for maintaining data quality.
By leveraging the SLA score indicator and adjusting severity levels as needed, we can prioritize and address issues in a more effective and efficient manner, ensuring that our data quality is always at its best.
The SLA score indicator is not limited to use within the OpenDataDiscovery platform. In fact, it can be used as an endpoint outside of the platform, such as to display data conditions on a dashboard and show the business whether our data is in good condition or not.
Overall
Actions in Great Expectations are a powerful feature that can help you ensure data quality throughout your data pipeline, and the
ODDAction
package provides additional functionality to help you achieve your data validation goals. That integration is a significant step forward in our data trust efforts, and we're thrilled to share it with the community. Stay tuned for more updates and new integrations!
Have a great idea or query? We’d love to hear it! Stop by our
Slack
channel and join the conversation. Looking for ways to contribute even more? Check out our
Contribution guide
as a starting point — we’re always on the hunt for new integration possibilities, so come make your mark with us today!
Resources
OpenDataDiscovery —
https://opendatadiscovery.org/
Source code —
https://github.com/opendatadiscovery/odd-great-expectations
Demo project —
https://github.com/opendatadiscovery/odd-great-expectations-demo
Video about that integration —
https://youtu.be/NdjsjNwOz_s
Great Expectations —
https://greatexpectations.io
GreatExpectation’s meetup, where we discussed the way everyone can implement custom Action —
https://www.youtube.com/watch?v=gxggGorDAWE
Great Expectations
Data Quality
Data Observability
Open Data Discovery
Data Catalog
--
--
Follow
Published in
Open Data Discovery
98 Followers
·
Last published
Jun 21, 2024
First open-source data discovery and observability platform
https://opendatadiscovery.org/
Follow
Follow
Written by
Pavel Makarichev
24 Followers
·
8 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
List: Automation Testing for Data | Curated by Kovid Rathee | Medium,"List: Automation Testing for Data | Curated by Kovid Rathee | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Kovid Rathee
Follow
Jan 31, 2023
·
3 stories
Automation Testing for Data
Collection of my writings on testing data applications in an automated manner.
Share
This is my first piece on automation testing. I summarized different types of testing methodologies along with their use cases.
In
Cognizant Servian
by
Kovid Rathee
Automation Testing for Data Systems
A short introduction to automation testing for Databases, Data Warehouses, and Data Lakes
Mar 1, 2021
1
Mar 1, 2021
1
This piece summarizes my learnings from implementing automation testing solutions for databases and data warehouses during multiple projects.
In
Cognizant Servian
by
Kovid Rathee
Common Database  Testing Problems
Potential issues while testing databases, warehouses, and lakes
Mar 30, 2021
Mar 30, 2021
In this piece, I talk about the big three of the open-source data quality and testing world - dbt, Deequ, and Great Expectations.
In
Cognizant Servian
by
Kovid Rathee
Data Quality and Testing Frameworks
A short introduction to open-source data quality & testing tools — dbt, Deequ, and Great Expectations
May 11, 2022
3
May 11, 2022
3
Kovid Rathee
3.4K Followers
Friend
of Medium
I write about tech, Indian classical music, literature, and the workplace among other things. 1x engineer on weekdays.
Follow
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Test In Production. Stop pretending non-production… | by Sven Hans Knecht | Medium,"Test In Production. Stop pretending non-production… | by Sven Hans Knecht | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Test In Production
Sven Hans Knecht
·
Follow
6 min read
·
Aug 21, 2023
--
3
Listen
Share
Stop pretending non-production environments are production.
Person repairing a plane while flying in it
At every company I’ve ever worked at in my 10+ year career, a variation on the following statement has been made by developers: The development environment sucks! It doesn’t represent production! We can’t test there!
For a development environment to be successful, the following things need to be true:
The shape of data flowing through it matches production
The volume of data flowing through it matches production
Each of the third party integrations exists or are 100% mocked
These are never true. Ever. Non-prod environments never match production. It may be that your company has solved this; if so, let me know how because I’d love to hear about it. But I have never seen it solved. The closest I’ve ever seen to a perfect prod environment match was at Mission Lane. Some teams there were pushing continuous traffic through a staging environment using
k6
. Even then, it was only a subset of teams, a subset of calls, and not the same volume as production.
I have watched company after company spend entire teams' time and energy on “improving test data quality.” I’ve yet to see them be happy with their test environment at the end of the day.
So, instead, we, as software engineers, should give up the ghost. We should stop pretending that lower environment testing is good enough. And instead, we should focus on production engineering (note: I don’t strictly mean Meta’s definition of
production engineering
). Testing in lower environments mostly serves to provide a smoke test. Any more than that is a false sense of security.
What Is Production Engineering?
Production Engineering means engineering how you release code to production. Production engineering is about building systems and tools that allow teams to safely release software to production without worrying about breaking something. Allowing engineers to get code out without impacting customers easily and without fear of their changes wreaking havoc on your bottom line.
Software Engineering is a tug of war between wanting to release new features to make money and not wanting to change anything because it works and makes you money. Production Engineering stops that from being a choice.
Production Engineering should be a key focus of an internal developer platform. Or your Cloud Platform team. Or your SRE team. Whoever is in charge of writing developer tooling/maintaining your developer platform.
Instead of having a “testing team” or a “test data quality team” or a “testing environment maintenance team,” you should spend the energy and effort of your engineers on building automated release processes that make it safe to release code to production.
Examples Of Production Engineering
It’s easy for me to sit here and tell you to test in production. It’s far harder to make a convincing case to your business leadership. Testing in lower environments
feels
safer. And it makes intuitive sense to leadership. Testing in production? To quote a former software architect: “You don’t engineer the plane while you are in the air en route to your destination.”
Here are some concrete examples of how to test in production:
Canary releases.
A canary release is the process of slowly shifting an increasing amount of your traffic to a new version of your app. This can be done based on raw request traffic percentages, cookies/headers, etc. If you are running in Kubernetes, there is a fantastic tool called
Flagger
which will do automatic canary releases based on metrics. It will route a percentage of traffic to a new version/deployment of your service, steadily increase that percentage until a pre-defined percentage, and then promote your new version to take 100%. An alternative to Flagger is
Argo Rollouts
. If you aren’t using Kubernetes, tools like Spinnaker or
API gateway
can help with canary releases.
Tiered Releases. A tiered release means releasing a new version of your software only to specific instances. Similar to a canary release, but on a broader scale. Think on a per tenant or per region tier. This is particularly helpful for SaaS companies. Being able to roll out your software first to customers in Group A, then Group B, and so on means that you limit the blast radius of a change going wrong. Combining canary releases and tiered releases means that you can protect each individual client with a canary release and protect the entire SaaS environment by not having to respond to failures in all customers at the same time
Feature Flagging
. Feature flags allow you to hide code paths until they are ready. Feature flags, more importantly, allow you to separate the code release from the business release. This means your engineers can release the code, smoke test, test their code against production, and then slowly ramp up the usage. This also means code can be shipped when it is completed and activated when the business side decides it is ready (allowing for decoupling software deadlines from marketing or business deadlines). Feature flags also force you to write backward-compatible code, especially database code. By forcing you not to replace old code but write a new code path, you have to ensure continued support and prevent breaking changes from making it to production.
Automated Rollbacks. When a feature flag is enabled, a tiered release or canary release happens, something should be measuring known SLIs to ensure they don’t go beneath acceptable levels. If they do, the system should automatically fail and roll back the release. A human should also have the option of manually rolling back a release. Still, there should be an automated system measuring how your software is performing and rolling it back if you have performance regressions.
PR Automation. Ensuring that bad code never makes it master is by far the easiest method of preventing issues. Most companies have some form of CI and linting for PRs, mostly focused on code styling or perhaps requiring unit tests. But you can expand this further if you want to. Does your SQL get linted? It should. Use a tool like
Squawk
or
SQLFluff
to ensure your database doesn’t have bad migrations run against it. This can be extended as far as you’d like. Most rules and checks a human does on code, especially in a PR, should be automated. Let humans check for intent rather than a checklist.
Known Good Production Traffic. Use a tool like
k6
to ship known good traffic through production. This has the following effects: it ensures that traffic levels are always acceptable to measure against, forces engineers to know how their code is being used, allows you to write monitors against known good traffic, and forces you to think about API design/dog food the API you wrote.
Outcome-Based Alerting. Unlike outcome-based decision-making (which we shouldn’t do), outcome-based alerting allows you to define general alerts for engineers to respond to when it matters. Business doesn’t care if the number of exceptions increases as long as it doesn’t impact the customer experience. Customers don’t care if you see OOMkills if it doesn’t impact their user experience. Alert on either latency or
apdex
. Alert on failed requests. Use the golden signals. Customers don’t even care if you have rollbacks. Anything but the golden signals, or your SLIs, cause the symptom, and while you should have metrics and dashboards for causes of symptoms, only alert if it is actionable. This makes it so engineers don’t artificially stress about releases.
Contract Tests
. Ensure your APIs are backward compatible without running expensive integration tests. Ensure engineers are notified when they break something before it gets to any environment. Don’t rely on others testing your code in a lower environment to tell you if you broke your contract.
Mirroring Production Traffic To A Lower Environment. Instead of trying to generate your own traffic, mirror the traffic at the network layer to a separate environment that can be used for testing changes. This lets you see how your code performs in the real world without impacting your customers. The downside is cost; however, my experience is that dev/staging costs more than prod anyway, so this shouldn’t be a significant factor.
Conclusion
Test in production. Stop being afraid. There is no environment like production. That’s where your software delivers value. I’m not advocating for the removal of unit tests. I’m not arguing that we should eliminate dev/staging environments. Those can help show UI changes or ensure the software boots properly, perhaps allowing people to comment on design/user experience, etc. There are valuable outcomes from non-production environments. But they aren’t testing if your software is safe for production. That’s only a statement that can be made after the software has been released to production. So instead of trying to improve test data quality, or making your non-prod environments more production-like, spend it on improving your release pipeline and production tooling. Use community tools to automate your releases. It’ll have a far greater reward when you have to scale.
Software Engineering
Programming
Kubernetes
--
--
3
Follow
Written by
Sven Hans Knecht
348 Followers
·
46 Following
SRE/Platform Engineer Professional @Anomalo. Amateur Analytics and Sports Enthusiast
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How to Integrate OpenMetadata Test Suites with Your Data Pipelines | by Teddy Crépineau | OpenMetadata,"How to Integrate OpenMetadata Test Suites with Your Data Pipelines | by Teddy Crépineau | OpenMetadata
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to Integrate OpenMetadata Test Suites with Your Data Pipelines
Teddy Crépineau
·
Follow
Published in
OpenMetadata
·
7 min read
·
Dec 6, 2022
--
Listen
Share
With OpenMetadata v0.12.0 onwards, users can create and execute test suites directly from the user interface for Data Quality. While this approach covers most use cases, certain data teams will want to integrate their test suite executions with their data processing pipelines. Instead of validating data after it has been modeled, integrating your test suite execution with your data processing pipeline allows you to stop any further tasks to run if quality criteria are not met.
In this article, we will go over OpenMetadata test suite concepts, how to create a test suite in OpenMetadata, and how to integrate your test suite execution with your data processing pipelines. We’ll use Airflow as our workflow scheduler for our pipelines, but the concepts apply to any type of workflow.
What are OpenMetadata Test Suites
At their core,
Test suites
are execution containers. They hold
test cases
, which themselves represent the specification of
test definitions
. OpenMetadata provides a growing number of table-level tests [
details
] and column-level tests [
details
], as well as custom SQL tests allowing you to write SQL tests [
details
].
Image 1-0 — Test Suite, Test Case, and Test Definition relationship
Test Suite represents a grouping of test cases for a table. In OpenMetadata 1.1.0 and up, test suites are divided into executable (test suites used to run test cases) and logical (test suites used to group tests together and visualize the assertion results). Tests from different tables can be grouped within the same logical test suite. This allows users to visualize tests from different tables in one place. We can imagine multiple source tables used to model a downstream table. All the tests from the source tables can be grouped together to ensure the accuracy of the newly modeled table.
How to Create a Test Suite?
Make sure you have a running instance of OpenMetadata v1.1.0 or later. If you don’t, check out our
Try OpenMetadata in Docker
. Once OpenMetadata is running, ingest metadata for a Database service. Then, navigate to your table data asset and click “Profiler & Data Quality”. From there, you will be able to create a test. Go ahead and click on “Add Test” — you will need to select your test type “Table” or “Column”. Next, you will have to select the test definition you want to run. Select the definition you want and configure your test according to your needs.
The final step will be to add an ingestion workflow for our test execution. To integrate our test suite execution with our data processing pipeline, we won’t add a schedule. On the “Schedule for Ingestion” page, simply select “none” as a scheduled time.
Image 1.1 — Test Suite Creation
Integrating OpenMetadata in your Data Processing Pipeline
For this example, we’ll create a simple pipeline with 3 tasks (data extraction, OpenMetadata test suite, modelization). We’ll use Los Angeles Sewer data for our example. The data can be fetched at the following API endpoint 👉
https://data.lacity.org/resource/qcn5-9nfy.json
. It also assumes you have:
A running OpenMetadata instance, and
Followed the instructions in “How to Create a Test Suite” to create a test suite.
Image 1.2 — Test Suite before executing our pipeline
Extract Source Data
Example 1–1 shows our Airflow task used to extract Los Angeles sewer data using the API.
example 1–1 — extract and insert task
@task(task_id=""extract_and_insert_data"") ①
def extract_and_insert(**kwargs):
""""""Extract function""""""
resp = requests.get(ENDPOINT)
df = pd.DataFrame(resp.json())
hook = RedshiftSQLHook() ②
engine = hook.get_sqlalchemy_engine() ③
engine.dialect.description_encoding = None
df.iloc[:100,1:].to_sql(
""los_angeles_sewer_system"",
con=engine,
if_exists=""replace"",
index=False,
) ④
①
@task
is an Airflow decorator used to define tasks in the pipeline
② we get a
DbpApiHook
which allows us to access specific SQLAlchemy methods
③ we get the SQLAlchemy engine (our database connection) that we will use to write our data
④ write 100 rows skipping the first column
Now that we have written our source extraction function, the next step is to execute our OpenMetadata tests against our data source. Before moving on to the next step, ensure that you have:
Ingested metadata for the source table, and
Created a test suite with some tests for your data source table (we named our test suite
LASewerTests
).
Execute Tests Against the Source Data
We chose to break down this step into 2 tasks, although it can be combined together. First, we will execute the test suite (i.e. run the test cases associated with our test suite) (
example 1–3
) and then we will check the results of our test suite’s test cases (
example 1–4
). We also defined a
get_ingestion_pipeline()
function (
example 1–2
) used by 1–3 and 1–4.
example 1–2 — get ingestion pipeline util function
def get_ingestion_pipeline(name: str):
""""""get ingestion pipeline by name
Args:
name (str): start or full name of ingestion
""""""
pipelines = OPENMETADATA.list_entities(
IngestionPipeline,
fields=""*"",
).entities ①
test_suite_pipeline: Optional[IngestionPipeline] = next(
(pipeline for pipeline in pipelines if pipeline.fullyQualifiedName.__root__.startswith(""LASewerTests"")),
None
) ②
return test_suite_pipeline
① we use OpenMetadata Python SDK to list all of our ingestion pipelines
② we filter the test suite ingestion we have created when creating our test suite or we return
None
if we don’t find our test suite ingestion pipeline
example 1–3 — execute test suite pipeline
@task(task_id=""run_om_test"")
def run_om_test(**kwargs):
""""""Run Openmetadata tests""""""
test_suite_pipeline: Optional[IngestionPipeline] = get_ingestion_pipeline(""LASewerTests"") ①
if not test_suite_pipeline:
raise RuntimeError(""No pipeline entity found for {test_suite_pipeline}"")
OPENMETADATA.run_pipeline(test_suite_pipeline.id.__root__) ②
timeout = time.time() + TIMEOUT ③
while True:
statuses = OPENMETADATA.get_pipeline_status_between_ts(
test_suite_pipeline.fullyQualifiedName.__root__,
get_beginning_of_day_timestamp_mill(),
get_end_of_day_timestamp_mill(),
) ④
if statuses:
status = max(statuses, key=operator.attrgetter(""timestamp.__root__"")) ⑤
if status.pipelineState in {PipelineState.success, PipelineState.partialSuccess}:
break
if status.pipelineState == PipelineState.failed:
raise RuntimeError(""Execution failed"")
if time.time() > timeout:
raise RuntimeError(""Execution timed out"")
① get our pipeline entity
② run our pipeline entity which will run the tests
③ we’ll be checking the status of our pipeline using a
while
loop so we’ll add a timeout to make sure we don’t get stuck in an infinite loop
④ retrieve the pipeline statuses for today.
get_pipeline_status_between_ts
takes 2 timestamps milliseconds as its second and third argument
⑤ get the latest pipeline status
Once our pipeline finishes without failure we need to check the results of our test cases. We’ll do that in the below task.
@task(task_id=""check_om_test_results"")
def check_om_test_results(**kwargs):
""""""Check test results""""""
test_cases = OPENMETADATA.list_entities(
TestCase,
fields=""*"",
) ①
test_cases = [
entity for entity in test_cases.entities if entity.testSuite.name == ""LASewerTests""
] ②
test_suite_pipeline: Optional[IngestionPipeline] = get_ingestion_pipeline(""LASewerTests"")
pipelines_statuses = OPENMETADATA.get_pipeline_status_between_ts(
test_suite_pipeline.fullyQualifiedName.__root__,
get_beginning_of_day_timestamp_mill(),
get_end_of_day_timestamp_mill(),
)
if not pipelines_statuses:
raise RuntimeError(""Could not find pipeline"")
latest_pipeline_status: PipelineStatus = max(pipelines_statuses, key=operator.attrgetter(""timestamp.__root__""))
for test_case in test_cases:
timeout = time.time() + TIMEOUT
while True:
test_case_results = OPENMETADATA.get_test_case_results(
test_case.fullyQualifiedName.__root__,
get_beginning_of_day_timestamp_mill(),
get_end_of_day_timestamp_mill()
)  ③
if time.time() > timeout:
raise RuntimeError(""Execution timed out"")
if test_case_results:
latest_test_case_result = max(test_case_results, key=operator.attrgetter(""timestamp.__root__""))
if latest_test_case_result.timestamp.__root__ < latest_pipeline_status.startDate.__root__: ④
continue
if latest_test_case_result.testCaseStatus in {TestCaseStatus.Failed, TestCaseStatus.Aborted}: ④
raise RuntimeError(f""Test case {test_case.name.__root__} returned status {latest_test_case_result.testCaseStatus.value} "")
break
① Get our test case entities
② Filter test cases that belong to our test suite. It is also possible to pass a
testSuiteId
parameter in ①, though we prefer this approach in this demo as we have a small number of test cases, and it prevents us from sending another API request
③ We iterate through all of our test cases and retrieve the results
④ If our result timestamp is earlier than our pipeline execution, we keep iterating. This means our results are not yet available.
⑤ If our test result has either failed or was aborted, we fail our task.
Examples 1–3 and 1–4 allow us to stop our data processing if an error is raised in one of our test cases. Unlike traditional data quality tools test results will be centralized back inside OpenMetadata, allowing the whole team to know a test has failed. On top of centralizing data test failure, any user with the right permission can add additional tests to that test suite, which will be executed as part of the data processing pipeline.
It is simple to imagine a flow where a Data Engineer creates a data processing pipeline, where an OpenMetadata test suite is run, and a business user defines test cases directly from OpenMetadata user interface. This ensures that teams with business knowledge can easily define quality rules that a data asset needs to have.
Create our Modeled Data
Our final step is to model our data.
Example 1–5
takes the source data and performs some aggregation.
Example 1–5
@task(task_id=""model"")
def model(**kwargs):
""""""model function""""""
hook = RedshiftSQLHook()
engine = hook.get_sqlalchemy_engine()
engine.dialect.description_encoding = None
df = pd.read_sql(
""SELECT mapsheet, avg(shape_leng) mean_shape_leng FROM los_angeles_sewer_system GROUP BY 1;"",
con=engine
)
df.to_sql(""sewer_mean_len"", con=engine, index=False, if_exists=""replace"")
Conclusion
[
Full DAG Code
]
Now that we have our whole DAG it is time to execute it. As we saw in Image 1.2, OpenMetadata had no test results for our “LA Sewer Tests” test suite. We created our test case to make it fail to show how our modelization task will not run.
Image 1.3 — Airflow data processing pipeline execution
Image 1.3 shows the 4 tasks we previously defined. Task #4 was not executed as task #3
check_om_test_results
reported test case failures and failed the Airflow task. This prevented our modelization task to run.
Image 1.4 — OpenMetadata UI after Airflow DAG execution
After our data processing pipeline execution, we can navigate to OpenMetdata test suite to view that our test result has been updated in our test suite with a failed test. This allows easy discovery of test failure for OpenMetadata users.
This illustrates the strength of OpenMetadata. With the API at the core of its architecture, technical teams, and business teams can easily collaborate on building accurate and reliable data products.
Data Quality
Data Governance
Data Catalog
Data Processing
Data Discovery
--
--
Follow
Published in
OpenMetadata
581 Followers
·
Last published
Oct 15, 2024
OpenMetadata is an open-source project that is driving Open Metadata standards for data. It unifies all the metadata in a single place in a Centralized Metadata store and helps people Discover, Collaborate, and Get their data right.
Follow
Follow
Written by
Teddy Crépineau
160 Followers
·
203 Following
Three o’clock is always too late or too early for anything you want to do — Jean-Paul Sartre.
http://www.teddycrepineau.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
We Built an Open-Source Data Quality Testframework for PySpark | by Tomer Gabay | Towards Data Science,"We Built an Open-Source Data Quality Testframework for PySpark | by Tomer Gabay | Towards Data Science
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Mastodon
Member-only story
We Built an Open-Source Data Quality Testframework for PySpark
Measure and report your data quality with ease
Tomer Gabay
·
Follow
Published in
Towards Data Science
·
7 min read
·
Aug 16, 2024
--
5
Share
[image by author, generated with Dall-E]
Every data scientist knows the classic saying “
garbage in, garbage out
”. Therefore it is essential to measure what the quality of your data is.
At
Woonstad Rotterdam
, a Dutch social housing association, we use PySpark in Databricks for our ETL. Data from our external software suppliers is loaded into our datalake using APIs. However, not every software supplier is testing on data quality. Consequences of faulty data in the social housing sector can be significant, ranging from tenants being unable to apply for allowances to rents being set at prices that are illegal according to the
Affordable Rent Act
. Therefore, we built
a data quality testframework
for PySpark DataFrames to be able to report about data quality to the suppliers and the users of the data.
Consequences of faulty data in the social housing sector can be significant, ranging from tenants being unable to apply for allowances to rents being set at prices that are illegal according to the Affordable Rent Act.
--
--
5
Follow
Published in
Towards Data Science
774K Followers
·
Last published
3 hours ago
Your home for data science and AI. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.
Follow
Follow
Written by
Tomer Gabay
1K Followers
·
32 Following
Data Scientist / Machine Learning Engineer / Python Developer from the Netherlands. Writing articles and publishing open source code on a regular basis.
Follow
Responses (
5
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"DataOps and Data Quality. DataOps is a culture, not a process. To… | by Kevin Kautz | Medium","DataOps and Data Quality. DataOps is a culture, not a process. To… | by Kevin Kautz | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Used by permission of A.K.S.
DataOps and Data Quality
Kevin Kautz
·
Follow
7 min read
·
Sep 7, 2019
--
Listen
Share
DataOps is a culture, not a process. To repeat a quotation from one of my earlier posts,
Building a DataOps Team
,
Culture is knowledge that is transmitted to individuals and across time, that can be taught and learned, and that is distinctive to groups.
—
Nicholas Christakis
When we speak of a culture, we necessarily draw attention to a group of people. In the case of a DataOps culture, the people are those that construct and operate data pipelines and data analytics applications, especially when the data and insights derived from it are the products that your organization sells to others.
If you monetize data and insights, you need to encourage your teams to develop a DataOps culture.
Start with a Broad DataOps Team
Culture is not produced by organizational structure. But organizational structure can kill culture. Be careful at the start. The DataOps culture absolutely requires a view that is broader than design & engineering.
DataOps Can Learn from DevOps
Let’s look at DevOps for a moment. DevOps is an active and valuable movement, and we can learn from its successes and failures. DataOps inherits some cultural values from DevOps, so it’s worth a quick comparison.
Many organizations fail in the first attempts to implement DevOps because they only engage design and engineering. This leads to Test-First and CI/CD and then … generally, it completely stops, right there. That’s a DevOps failure. You’ve used DevOps to improve Dev but not Ops. Test-First and CI/CD are great behaviors, but they’re only engineering behaviors. You’re not successful with DevOps until you also address production operations, monitoring in production, the ability to tune performance and adjust scalability “live” in production, and so much more.
DataOps Must Extend Beyond Engineering
DataOps also needs a broader perspective. If you attempt to introduce a DataOps culture by engaging only design and engineering teams, you’re missing the point. DataOps needs to be a shared culture
across teams
. The whole point is to establish shared knowledge in a larger group with diverse perspectives and needs.
If your engineers do not know when your data is good or bad, expand your team to include those who do. I repeat (because so many miss this point), please do not start working on DataOps with only a group of software engineers, not even if their titles have recently changed to “data engineer”. You need people who deal with production data every day and who handle client inquiries and recover from data quality failures in production.
If possible, also include the wide-view product managers who deal with value propositions and funding and market viability and go-to-market and A-B testing and intentional product obsolescence. Be careful with this one. If your organization uses “product owner” as a role, that’s not what we are talking about. Product owners live within the design & engineering teams as the voice of those outside. Product owners live within the engineering glass house, even if their role encourages them to look out of those glass walls. Wide-view product managers, on the other hand, bring additional perspectives far beyond those of engineering and operations. It is those additional perspectives that we are looking for.
Just as with the introduction of DevOps culture, the shared DataOps culture can span product management, design, engineering, deployment, production monitoring, operations, and direct client support. DataOps is not intended for engineers but for a larger and more diverse group of people that are not dominated or directed by engineering concerns.
First DataOps Goal? Data Quality Automation
It’s not the only place to start, but it’s a good one. You may already have what you need to make an immediate impact. This is a good choice if you want to demonstrate added value with some quick wins.
Notice, please, that I did not say to start with Data Quality. I said to start with the
automation
of it.
And now, we immediately encounter a culture collision. We have to talk about the word
process
and not everyone will understand this in the same way.
What Does the Word “Process” Mean?
Automation, by definition, is the automation of a process that was carried out by people before it was automated. Operations teams love to talk about process measurements, process improvements, and to speak about success & failure in terms of processes that succeed or fail.
Software engineering, teams, on the other hand, have learned to have a disdain for “process”. When they use this word, they don’t mean the same thing as operations teams do. There are good reasons for engineers to push back against unwieldy practices that interfere with design & engineering success. Process is a code word for many engineering teams, and it means “non-value-added activities that prevent me from doing value-added stuff”.
Sigh.
If you’re talking about “value add”, you’re talking about process. That’s a principle from Lean Manufacturing. If you speak of “adding value” as a way to fight against “process”, you’ve lost your way. You’ve forgotten what Lean is.
(We could write a book on the ways in which agile methodologies have lost their way. But that’s an incendiary assertion for another day.)
This culture collision over the meaning of the word “process” is one of the reasons why I emphasize that Lean Manufacturing is one of the foundations for the DataOps culture. “Adding value” and “process waste” have very clear meanings in Lean Manufacturing. Every agile methodology in software engineering teams relies on concepts borrowed from Lean. If we return to that foundation, we can align to a common understanding of terminology. Lean Manufacturing is very process-oriented. The word “process” can become the point of overlap where both engineering and operations share the desire to remove waste from process.
But we will save the conversation about Lean Manufacturing principles for another time.
Add Automation to Human Behaviors
For now, here’s the place to start. We need to learn how data quality is measured in production operations, and focus on how to automate the human behaviors that
define
data quality,
test
for data quality, and
remediate
data quality failures.
It’s not hard to define data quality in a production operations process. You already do this. You may not recognize it for what it is.
Data Quality for Inputs
When data arrives as input to a software application or component, you will already have: (1) an expected schema, (2) an expected format, (3) an expected interface or location, (4) an expected size, (5) an expected time of arrival, and (6) an expected range of values in each column or field. Please feel free to extend “schema” and “format” to include flat-file, relational, JSON, and any number of other varieties.
Tests for each of the (1) through (6) can be automated. It’s not hard to imagine. You’re likely to be already checking each of these, but it may be the operations team who is doing it.
The point is to
automate
the above checks. Define just enough metadata so that you can write the rules for success and failure based on metadata. The automated code that runs the tests (in production) should be generic enough to use the metadata to test for expected conditions.
Did the schema or format change from what was expected? Did data fail to arrive when and where it was expected? Did the size or content of the data surprise you?
Capture the results of your data input tests. Collect these results in a queryable data structure so you can monitor trends. Eventually, you’ll get more sophisticated and you’ll want to write cooler tests such as “number of rows in a weekly incoming file should be the same or greater than prior weeks”, or “the percentage of null values in this column should vary no more than 10% from the trend line of the percentage of null values from the prior three months”. Cool tests require trending of data profiling statistics.
And of course, automate the alerts when tests fails. Automation does not remove the people from the conversation. It simply takes the drudgery out of the process. It runs the set of tests and collects the results and notifies the humans when something does not match expectations.
Data Quality for Outputs
As you begin to recognize that data quality tests can be pushed upstream (at least, within your own organization’s pipelines), you’ll realize that you also need to a good citizen about your downstream data consumers. If your downstream partners are going to push their data quality requirements upstream to you, then you need to test your outputs against their stated expectations. You should test both inputs and outputs.
With outputs, you’ll need to know something more interesting. You need to know what you published last time, and the time before. And some data profiling statistics about data that you published. And trend lines. And … oh … yeah … you can write tests that succeed or fail on the outputs that you produce, so that you know immediately if this is going to set off alarms in the downstream processes. To speak in manufacturing terms, you need to ensure that what you’re producing falls within the specifications and precision measurements that are needed downstream.
This begins to get more interesting than the tests for inputs. We’re now talking about testing for “fitness for use”. You have to know what the “use” is. How will downstream applications use the data that you’re producing? If you get good at this, you’ll find that this concept also applies when data is going to leave your organization and go to your clients and business partners. And that will start to have “teeth” — legal, contractual, regulatory compliance. Data quality measures can demonstrate compliance.
Here are a few common “fitness for use” tests and remediation behaviors: (1) referential integrity in outputs, (2) consistency across time, (3) restatements of prior published data, (4) freshness, and (5) tagging of data that has special regulatory or contractual meaning.
Once again, humans in your operations teams are likely to already have ways to handle each of these tests and remediation behaviors. Your goal is to collect these behaviors and to start to automate them.
Summary
The DataOps culture requires a common understanding, a common language, and shared knowledge between people of different skills and responsibilities. Engaging an extended team (or teams) with your DataOps cultural transition is essential to success. A good first goal is to identify existing human behaviors related to data quality and to automate them.
There’s more to the DataOps culture than the automation of data quality behaviors. But data quality automation is absolutely essential to DataOps because it demonstrates that the extended team shares the cultural value of data quality, and actually does something about it.
DevOps
Dataops
Data Quality
Data Quality Automation
--
--
Follow
Written by
Kevin Kautz
57 Followers
·
15 Following
Professional focus on data engineering, data architecture and data governance wherever data is valued.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How To Use Soda For Data Quality Checks With Apache Airflow | by Ahmed Mokbel | Medium,"How To Use Soda For Data Quality Checks With Apache Airflow | by Ahmed Mokbel | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How To Use Soda For Data Quality Checks With Apache Airflow
Ahmed Mokbel
·
Follow
5 min read
·
Apr 26, 2024
--
Listen
Share
In the world of data, quality is like the superhero cape that keeps your databases flying high above the chaos! Employing data quality tools is crucial, as they ensure the accuracy, consistency, and reliability of data — fundamental for effective decision-making, compliance, and operational efficiency. These tools keep your data pristine and trustworthy, safeguarding against costly errors. By prioritizing high data standards, organizations can avoid costly mistakes and inefficiencies that arise from poor data quality, leading to better business outcomes and improved data-driven strategies.
Soda Data Quality Framework
Soda Core is a powerful open source Framework and Python library that enables you to use the Soda Checks Language to turn user-defined input into aggregated SQL queries. When it runs a scan on a dataset, Soda Core executes the checks to find invalid, missing, or unexpected data. When your Soda Checks fail, they surface the data that you defined as bad quality.
How it works
S
oda works by taking the data quality checks that you prepare and using them to run a scan of datasets in a data source. A scan is a command that instructs Soda to prepare optimized SQL queries that execute data quality checks on your data source to find invalid, missing, or unexpected data. When checks fail, they surface bad-quality data and present check results that help you investigate and address quality issues.
To test your data quality, you choose a flavor of Soda (choose a deployment model) which enables you to configure connections with your data sources and define data quality checks, then run scans that execute your data quality checks.
Connect to your data source.
Connect Soda to a data source such as Snowflake, Amazon Athena, or BigQuery by providing access details for your data source such as host, port, and data source login credentials.
Define checks to surface bad-quality data.
Define data quality checks using Soda Checks Language (SodaCL), a domain-specific language for data quality testing. A Soda Check is a test that Soda performs when it scans a dataset in your data source.
Run a scan to execute your data quality checks.
During a scan, Soda does not ingest your data, it only scans it for quality metrics, and then uses the metadata to prepare scan results. After a scan, each check results in one of three default states:
pass: the values in the dataset match or fall within the thresholds you specified
fail: the values in the dataset do not match or fall within the thresholds you specified
error: the syntax of the check is invalid, or there are runtime or credential errors
A fourth state, warn, is something you can explicitly configure for individual checks.
Review scan results and investigate issues.
You can review the scan output in the command line and in your Soda Cloud account.
How to Integrate Soda with Apache Airflow :
Install Soda SQL:
pip install soda-core-scientific
pip install soda-core-redshift
soda-core-redshift can be any other data source (soda-core- postgres)
Create Custom Operator For Soda:
Airflow doesn't have an operator For the soda Quality Framework So We Need to make our own
create a file named ‘Customoperators.py’ and insert the following code into it :
from soda.scan import Scan
from airflow.hooks.base_hook import BaseHook
import os
class SodaScanOperator(BaseOperator):
@apply_defaults
def __init__(
self,
scan_name: str,
conn_id:str ,
schema_name:str ,
data_source: str = 'redshift_datamart',
project_root: str = 'soda',
*args, **kwargs
):
super().__init__(*args, **kwargs)
self.conn_id=conn_id
self.scan_name = scan_name
self.data_source = data_source
self.project_root = project_root
self.schema_name=schema_name
def define_Variables(self) :
username=BaseHook.get_connection(self.conn_id).login
password = BaseHook.get_connection(self.conn_id).password
host = BaseHook.get_connection(self.conn_id).host
port=BaseHook.get_connection(self.conn_id).port
os.environ['username'] = username
os.environ['password'] = password
os.environ['host'] = host
os.environ['schema_name'] = self.schema_name
os.environ['port'] = str(port)
def execute(self, context):
self.define_Variables()
self.log.info('Running Soda Scan ...')
config_file = f'{self.project_root}/configuration.yml'
checks_path = f'{self.project_root}/schema/{self.schema_name}.yml'
scan = Scan()
scan.set_verbose()
scan.add_configuration_yaml_file(config_file)
scan.set_data_source_name(self.data_source)
scan.add_sodacl_yaml_files(checks_path)
scan.set_scan_definition_name(self.scan_name)
result = scan.execute()
self.log.info(scan.get_logs_text())
if result != 0:
raise ValueError('Soda Scan failed')
return result
1. Initialization (
__init__
method):
This method sets up the operator with necessary parameters such as
scan_name
,
conn_id
,
schema_name
,
data_source
, and
project_root
.
scan_name
: Name of the specific scan to execute.
conn_id
: Airflow connection ID used to fetch database credentials.
schema_name
: Specifies the schema within the database to be scanned.
data_source
: Indicates the type of database (e.g., 'redshift_datamart').
project_root
: The directory path where the Soda project and configuration files are stored.
2. Environment Setup (
define_Variables
method):
Retrieves database credentials using the connection ID from Airflow’s connection management system.
Sets these credentials and connection details as environment variables, which are presumably used by the Soda scan to access the database.
3. Execution (
execute
method):
Calls the
define_Variables
method to set up the environment.
Constructs paths for the Soda configuration file (
configuration.yml
) and the checks file (
schema_name.yml
).
A
Soda scan
executes the checks you write in an agreement, in a checks YAML file, or inline in a programmatic invocation, and returns a result for each check: pass, fail, or error.
Initializes a
Scan
object from Soda SQL and configures it with verbose logging, data source name, scan configuration, and checks file.
Executes the scan and logs the results or errors.
Create Configuration File and Checks File :
For Soda to run quality scans on your data, you must configure it to connect to your data source.
create a folder called soda have file
configuration.yml
data_source redshift_datamart:
type: redshift
host: ${host}
username: ${username}
password: ${password}
database: data_mart
schema: ${schema_name}
create a file named ‘bank_customers.yml’ within the ‘schema’ folder located inside the ‘soda’ directory “soda/schema/bank_customers.yml”. This folder called schemas should contain all the necessary files schemas. We will conduct checks on these schemas
hint
:
Ensure that the file name ‘bank_customers.yml’ matches the schema name. in the database
Soda Checks Language (SodaCL)
is a YAML-based, domain-specific language for data reliability. Used in conjunction with Soda tools, you use SodaCL to write checks for data quality, and then run a scan of the data in your data source to execute those checks.
Example :
Initialize our dag :
In
conclusion, the integration of Soda with Apache Airflow offers a powerful solution for maintaining data quality and ensures their pipelines effectively. By leveraging Soda’s Data Quality Framework, Data Engineers can define and execute comprehensive data quality checks tailored to their specific requirements. Through custom operators, such as the one outlined above.
Thank you for getting this far 🫡
Airflow
Data Quality
Data Engineer
Soda
Redshift
--
--
Follow
Written by
Ahmed Mokbel
21 Followers
·
5 Following
Analytics Engineer
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Testing vs Observability: Which is right for your data quality needs? | by Eric Sayle | Bigeye | Medium,"Testing vs Observability: Which is right for your data quality needs? | by Eric Sayle | Bigeye | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Testing vs Observability: Which is right for your data quality needs?
Eric Sayle
·
Follow
Published in
Bigeye
·
9 min read
·
Jul 7, 2021
--
1
Listen
Share
Image credit
Alexander Popov
To be data-driven, it’s fundamental to verify that the data being acted upon is correct and of a high quality. Depending on the shape of the data, data quality can include everything from ensuring that the data is running on schedule to making sure that row counts are within the expected range and much more.
Pipeline testing is a relatively new technique that is quickly gaining traction with data engineers. Data engineering teams are borrowing from traditional software engineering practices and adopting pipeline testing to catch data quality issues. Tests can be run on the pipeline code that produces the data, and specific tests can be added to the code to validate the quality of the data flowing through the pipeline. In this blog, when I refer to tests, I’m referring to assertions run during pipeline execution — not to be confused with unit tests, which test the correctness of the transformation itself. We will cover the important differences between the tests covered in this blog and unit tests in a future explainer.
While tests are a common approach, observability is increasingly emerging as a scalable alternative for addressing data quality. Data observability takes a page from
the world of SRE
and solutions like AppDynamics, Datadog, and New Relic. Data observability tools repeatedly collect various statistics from tables and columns, creating time series for things like freshness, row count, and a bevy of other issues that can affect the data itself. Some tools even provide built-in anomaly detection on the resulting time series.
In this blog, I will delve into these two of the main approaches to determining whether the data is correct and high quality. I will describe each approach, the potential shortcomings, and also touch on when a combination of the two approaches may make the most sense. From this blog, I hope you’ll leave with a better understanding of pipeline testing and observability and which is right for your data quality needs.
Data pipeline testing
Data pipeline testing requires running SQL queries to test for specific issues that you know might occur with the data. When the test catches an issue, the pipeline is halted, ensuring that the incorrect data doesn’t reach the analysts or line-of-business users downstream.
Whether you are writing your own query or using one of the test harnesses built into a modern data engineering tool, like
dbt
,
Dagster
, and
Prefect
, the process of using tests to ensure the quality of your data is similar. Let’s look at an example.
How long have you been shopping?
Let’s say that you work for an online retailer, and an analyst starts reporting that the max session length looks too long. You dig in to investigate.
Below is a sample of the raw access data. You can find the entire data set
here in GitHub
if you want to reproduce this example yourself.
The access data is used to compute how long a user spent browsing without stopping for at least 30 minutes between requests.
When the data is ordered by session_duration, it’s easy to see that there are some sessions coming from users with empty ids that continue for far too long.
You discover that there are empty user_ids inflating the session length. Maybe this is test data or maybe there is an issue with an upstream job. To prevent this problem in the future, you create a test that stops the pipeline if user_id is blank.
The test is run as part of the pipeline job. If the test fails, the job will stop without writing any new data. Now, you and the analyst can be assured that the issue will be caught and prevented from affecting downstream analytics in the future.
More tests!
Tests can be combined to gain more complete control over the quality of a data set. Let’s say that you want to make sure that your order table is behaving as expected. You may write tests to check the order_id column to make sure that each order_id is unique and another test to ensure that it doesn’t include nulls. You write a test for the status column to ensure that it only contains one of the four expected values. And you write a foreign key check to the customers table using the user_id column. Depending on the shape and complexity of the data, you may write many more tests as well.
By this point, you might start seeing the limitations of this method of testing. While tests are powerful, particularly for stopping data quality issues before they affect users downstream, tests also have some big shortcomings.
You only know what you know
This might sound obvious, but you can only test for issues that you are aware of — and you have to be precise. You can’t ballpark pipeline tests and hope to be successful. If you test for nulls but end up having an issue with empty strings, well, now your dashboard is broken.
If tests are being written to test a particular column, any changes to the behavior in another column won’t be detected. That means creating more tests. Similarly, if the test data has very regular values, it won’t show how the pipeline will behave if production data with large variance is introduced. On top of that, in an environment where each piece has its own set of tests, there can still be unexpected interactions between the various components that result in data issues.
As you can probably tell, the need for tests can snowball quickly. What seems like a manageable number of tests turns into a myriad of tests.
Writing tests takes work
Whether you are writing your own SQL queries or using a framework like dbt, Dagster, or Prefect, each test takes time to put together. One test won’t take you long, but to be thorough, you’re going to need a lot of tests and that is going to take a lot of time.
Let’s say that you are proficient at writing pipeline tests and each one takes you only 60 seconds to write. If you have 200 tables in your data warehouse with 15 columns each, and getting really good coverage requires 3 tests on average per column, you’re looking at over 9,000 minutes (150 hours) to create those tests.
Tests don’t like uncertainty
Tuning the tolerances of the tests effectively is incredibly important. If the tolerances are too narrow, your pipeline might get stopped for a non-issue, but if the tolerances are too broad, you may miss real data quality issues. Tuning tolerances becomes exponentially more challenging when you factor in the natural changes to the data and the business that occur over time.
For example, if the average session length for your online retail store is a bit longer than you expect, you don’t necessarily want to stop your pipeline. Maybe one of your products has gone viral, maybe the marketing team is running a particularly effective promotion, or maybe seasonality in the business model is the culprit. In short, the data engineer really needs to know what is happening with the business and the data to be able to tune tolerances accordingly. That tuning is extra maintenance on top of the work that was done to set up the tests in the first place.
Observability
Observability refers to leveraging monitoring and anomaly detection to detect data quality issues. Unlike testing, which checks for sets of conditions and whether each is passing or failing, observability is based on measuring statistics and changes in the behavior of the data over time — not unlike what
APM tools like Datadog
do for software applications.
And because observability methods are simply collecting lots of statistics about the data, the instrumentation process can be heavily automated. Given a schema and maybe some sample data, an observability tool can more easily decide which statistics to collect, and the configuration of those statistics doesn’t need to change even if the data’s behavior does. As long as there’s a column there to query, the statistic can be collected. This makes it easy to track tons of attributes of the data up front and identify problems later, instead of predicting all the potential failure modes.
Some observability tools (shameless plug, this is what we make at
Bigeye
) also include anomaly detection techniques to detect potential problems, so even the configuration of the alerting criteria can be heavily if not completely automated. Again, this is a page borrowed from the APM playbook, where manual alert configurations break down when you have lots of services that need to be monitored and not enough engineers to throw bodies at the problem.
In the example below, the percentage of values in the client_card_zip column that meet the USA Zip Code format is about 28%. If in the future, the business grows in the US that percentage could rise. After initially flagging an anomaly, the observability system can easily adjust its anomaly detection model to incorporate this change in the business without requiring any manual updates to test configurations.
Bigeye’s observability platform dynamically adjusts to changes in the data and the business
Of course, with a different technique come different limitations:
Observability shines as data scales up, but so does infrastructure load
Observability can quickly add load to your warehouse if not heavily performance optimized, and it can create “alert storms” if the anomaly detection isn’t
designed carefully
. We put a lot of thought into this problem for our product, but it’s a real concern once you’re working with a large warehouse or lake.
Observability platforms observe
Another obvious drawback is right in the name. By default, these types of tools only “observe” the data and won’t stop the flow of data problems downstream unless you integrate them with your orchestrator. This observe-only behavior can be a benefit in some situations — like when the “problem” is actually a valid business logic change that the data team simply didn’t know about. But using pipeline tests to break the data flow can be incredibly powerful for some classes of failure, like null, or duplicate ID’s.
Alert storms
While copious testing can definitely wreak havoc on your Slack channels, it’s even easier to do with observability tools. If the anomaly detection algorithm can’t handle a wide range of seasonalities, trends, and other time series features, it could produce a lot of false positives, drowning your team and creating alert blindness.
Tests + Observability
Often, the best option is a combination of observability and tests. Judicious use of tests can help you stop serious data problems right inside your pipeline. Observability can handle the long tail of unknown unknowns and fuzzy cases, like outliers.
Take our online retail example. In that case, monitoring things like the distribution of session duration is useful to show changes in the underlying data but doesn’t need to stop the pipeline. While null or empty user_ids and session_ids might warrant stopping the pipeline.
In this case, observability provides the breadth of coverage, automation necessary to remove the toil from writing tests and ensures that data engineers are alerted when data issues arise. Tests stop the pipeline when it’s necessary to ensure that critical issues aren’t propagated downstream. Together, the data engineering team gets the best of both techniques.
Taking a page from software engineering
Data pipeline testing and observability are two of several big trends in which practices and principles from software engineering influence data engineering. This is a great sign, no matter which route you go down.
Decades ago software wasn’t as reliable as it is today. In a 2002 article in
MIT Technology Review
, one software engineer laments that good software, “is usable, reliable, defect free, cost effective, and maintainable. And software now is none of those things.” At that time, software was expected to have bugs and to crash regularly. Now, we expect a high degree of reliability from our software.
If all goes well, the way we all approach our data pipelines will follow a similar evolution, bringing the reliability of testing and observability techniques to the data so much of modern society depends on.
Data Pipeline
Testing
Data Quality
Observability
Monitoring
--
--
1
Published in
Bigeye
222 Followers
·
Last published
Mar 21, 2023
Treating data quality like an engineering problem.
Follow
Written by
Eric Sayle
15 Followers
·
2 Following
Staff Software Engineer at Bigeye
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
The Importance of Data Quality & Quantity for Performance and Scale Testing | by Shainesh Baheti | Salesforce Architects | Medium,"The Importance of Data Quality & Quantity for Performance and Scale Testing | by Shainesh Baheti | Salesforce Architects | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
The Importance of Data Quality & Quantity for Performance and Scale Testing
Shainesh Baheti
·
Follow
Published in
Salesforce Architects
·
7 min read
·
Aug 17, 2021
--
2
Listen
Share
If your team is tasked with building scalable applications on the Salesforce platform to meet complex business requirements, you are going to need to ensure your application performs well, particularly as it scales. That means you’ll need
performance testing
to identify, for example, how responsive your application is for a reasonable number of users in terms of efficient SOQL, efficient web pages, and optimized Apex code. And you’ll need
scale testing
to identify concurrency related errors, governance limits, and similar issues that may only become apparent as the application scales to handle more customers, transactions, users, and so on.
When creating and executing performance and scale tests, the test data you use will determine how effective your tests are. Ideally, you want your test data to match your production data as closely as possible. That is, you want to have roughly the same
quantity
of test data as production data, and you want your test data to have the same
qualities
as your production data — with the same variety of data values, access limits, and so on. This post explores the importance of these two dimensions of test data — quantity and quality — in running realistic performance and scale tests that yield valuable insights.
Query plans and query optimization
As you may know, when an SOQL query is issued to retrieve data, it is processed based on a query plan designed to minimize the time needed to complete the query. Like most cost-based query optimizers, the one that Salesforce uses to create query plans relies on statistics gathered about the data. As explained in
this post
:
The database routinely collects statistics about the data in the database. For example, the number of records in each table/object, the cardinality (number of corresponding records) of particular values in an indexed field, and much more.
When you execute a query, the optimizer considers such statistics to calculate costs for various execution plans.
The optimizer then chooses the plan with the lowest cost to execute the query
This is why it’s so important to use test data that matches your actual production data as closely as possible. If it doesn’t match well, it’s likely that the statistics collected for it will also not match, the calculated costs of queries will not match, and the query plan chosen will not match. As a result, the performance of your application will likely look very different when it is run against your test data compared to when it is run against your production data, resulting in lots of false positives and false negatives:
False positives:
Performance and scale issues that are identified in the test environment but not present in production
False negatives:
Performance and scale issues that are not identified in test environment but actually are present in production
False positives can result in wasted time and resources as teams address non-issues; and false negatives, more importantly, can affect the business when potentially serious production issues are undetected by testing.
Why create synthetic test data
Ideally, you would create test data by simply copying your production data to a full copy sandbox and conduct your performance testing there. In many cases, however, this is not feasible and you’ll need to generate synthetic data. The need for synthetic data can be driven by various factors:
Legal.
In some cases you may simply not be allowed to use production data (although you still may be able to copy metadata).
New use cases or features.
You may be working with new custom entities or fields for which there is no data already available in production.
Greenfield implementation.
If you are a new customer on Salesforce, you won’t have production data to work with.
Rapid growth.
When you anticipate rapid growth, you may not have enough production data available to conduct realistic tests that reflect that performance at the expected scale.
Challenges in synthetic data creation
The creation of synthetic test data requires a thorough understanding of the test data needed and then a series of steps to design, generate, and load the test data.
Understanding test data requirements
To gain a better understanding of your test data needs, ask the following questions:
What use cases require data to be populated? Performance tests should be performed only for all commonly used use cases.
Which test data entities are transactional, and which are static or master data that are used for lookup only?
Static or master data entities are used by an application to look up values for completing business transactions. This test data is generally small in terms of volume but could be complex depending on your application. For a loan processing application, for example, this data includes mortgage terms, current mortgage rates, and relevant credit profiles.
Transactional data is created or updated while performing a business transaction. The volume will vary based on the application. For the loan processing application, transactional data might include customer details, loan details, and other values that are inserted or updated throughout loan processing.
How much volume needs to be created?
For static or master data, the volume required depends on the target throughput you want to achieve. For example, if one user can perform five business transactions in a minute and your target throughput is 100K/minute then at least 20K users need to be created.
For transactional data, the volume depends on how much production data you have. You’ll need to create roughly the same amount synthetically to conduct realistic testing.
Data shape design
As you design your test data, you’ll want to define:
A list of objects needed in your test set as well as fields of each those selected objects
The relationship between selected objects
Parent-child data skew
between selected objects
Test data generation
Once you know your synthetic test data will look like, you need to write some code or a script to generate it. Keep in mind that complex data models and requirements will take more time to code.
Test data loading
Finally, you’ll need to load your test data once it’s generated. The
Bulk API
is a good option for this, but make sure you follow best practices to achieve optimal throughput and avoid row locks.
When you’ll need more test data
There are several scenarios in which you’ll need more test data than you currently have.
Scenario 1: Little test data exists
The most obvious case for needing more data is when you have created no — or very little — test data to start with. If you have only a few dozen rows in your test data set, but you have millions of rows in production, then in testing, all of your queries will be efficient and your response times will be short. This, of course, is a false negative because those same queries may indeed yield unacceptable response times when executed in production.
Scenario 2: Visibility mismatches
If you have not defined sharing rules for your test data entities and fields that match the sharing rules in production, then the visibility of and access to those entities will not be the same across the two environments. If in production a user has access to all entities, but in test the user does not, your tests will produce false negatives. On the other side of the coin, if in production a user has limited access, but in test the user has full access, your tests will produce false positives.
Scenario 3: Skewed data within fields
Consider a situation in which you generated test data by populating some fields for a few rows, and then duplicated those rows repeatedly to generate a large test data set. (Examples include using only “true” for Boolean values, selecting the same value for a picklist, or using the same name, city, and phone number over and over again.) This results in poor quality test data that does not well represent the variety of data and distribution of values present in production. In this case, you’ll want to create more test data that is a closer match to your production data.
Scenario 4: Duplicate rows with the same data for indexed fields
Similar to the previous scenario, if you simply duplicate rows with same data for indexed fields to create a large volume of test data, the likely result is an inefficient query plan — since indices will not be used because index selectivity thresholds will be breached — resulting in lots of false positives.
Scenario 5: Parent and child entity skew
If the production data set has a large number of child records associated with the same parent record, you’ll want roughly the same skew in your test data set. For example, if you have an average of 100 Contacts for each Account, then you should have same ratio in your test data. If not, the validity of your performance and scalability tests will suffer.
If you have a mix of parent-child skews in production, then make an effort to create test data skews to at least cover the corner cases and majority of skew cases. For example, if in production your Account:User skews range from
1:1 to 1:1000
but the majority are between
1:10 to 1:100
then create test data with Account:User skews of
1:1, 1:10, 1:100, and 1:1000
to maximize test coverage. The ideas here is to cover the majority of cases without needing to create test data for each and every skew in production.
Scenario 6: No data for displayed fields
Your test data set should have values for any fields displayed on a page. This means that you need to pay close attention to dates in your test data. For example, if one of your tests involves a list view filter that shows only items from the past month and all the dates in your test data are more than a month old, then your tests will result in false negatives.
Counterexample: More data than required
Generally speaking, there is no harm in creating more test data than you need. That said, you still need to be careful. If you have too much data then you may start seeing false positives in the form of Apex CPU time limit errors.
Conclusion
Although generating test data with the same qualities — and in the same volume — as your production data is challenging, doing so is imperative for realistic performance and scale testing. If you do need to create synthetic test data, look for an
App exchange data seeding tool
or here is an
open source data seeding tool
that can simplify the process.
For more on Salesforce performance testing, see
Introduction to Performance Testing on the Developers’ Blog.
Salesforce Architect
Ask An Architect
Scale Academy
Performance Testing
Scale Testing
--
--
2
Published in
Salesforce Architects
6.5K Followers
·
Last published
Oct 14, 2024
A tech publication for architects, by Salesforce Architects
Follow
Written by
Shainesh Baheti
4 Followers
·
4 Following
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Big Data — Testing Strategy. Big Data — Introduction | by LoginRadius | Medium,"Big Data — Testing Strategy. Big Data — Introduction | by LoginRadius | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Big Data — Testing Strategy
LoginRadius
·
Follow
8 min read
·
Oct 5, 2020
--
Listen
Share
Big Data — Introduction
Earlier, we were only dealing with well-structured data, hosted in large data warehouses, and investing a cost in maintaining those data warehouses and hiring expert professional to maintain and secure information hosted in that data warehouse. Data was structured and can be queried anything as per the needs. But now, this exponential growth of data generates a new vision for data science along with some major challenges.
Big Data
is something which is growing exponentially with time, and carry raw but very valuable information inside that can change the future of any enterprise. It is a collection, which represents a large dataset, may be collected from multiple sources, or stored in an organization. Let‟s understand a real-time example, Big companies like Ikea and Amazon are leveraging the benefit of big data by collecting data from customer‟s buying patterns at their stores, their internal stock information, and their inventory demand-supply relations and analyze all, in seconds even in real-time to add value to its customer experience.
So, extracting information from a large dataset somewhat calls a concept of Data mining which is an analytic process originally designed to explore large datasets. The ultimate goal of data mining is to search for consistency in a pattern or systematic relationship between variables, which helps in predicting the next pattern or behavior.
Now, if we take concepts of data mining forward along with large data set, to some extent it becomes a blocker for our existing approach, because big data may contain structured or unstructured data even it may contain data in multiple formats also.
Big Data Testing
Testing is an art of achieving quality in your software product, in terms of perfection of functionality, performance, user experience, or usability. But for big data testing, you need to keep your focus more on the functional and performance aspects of an application. Performance is the key parameter in any big data application which is meant to process terabytes of data. Successful processing of terabytes of data using a commodity cluster with a number of other supportive components needs to be verified. Processing should be faster and accurate which demands a high level of testing.
Processing may be of three types:-
And based on which, we need to integrate different components along with NoSQL data store as per the needs.
Big Data Testing — Test Data
Data plays a vital role in the testing of big data applications. Application is meant to process data and provide an expected output based on implemented logic. The logic needs to be verified before moving to production, as the implementation of logic is completely based on business requirements and data.
1. Test Data Quality
Good quality test data is as important as the test environment. In the big data world, data can have any format or size, it may be in the form of a document, XML, JSON, PDF, etc. at the same time data size may go up to terabytes of petabytes. Hence, test data should also have multiple formats and size should be large enough to ensure the handling of large data processing. In big data testing, it needs data with logical values as per the application requirement and format which is supported by the application.
Along with it, data quality is another aspect of big data testing. Ensuring the quality of data before processing through application ensures the accuracy of the final output. Data quality testing itself is a huge domain and covers a lot of best practices which include — data completeness, conformity, accuracy, validity, duplication, and consistency, etc. It should be included in the big data testing and this ensures the level of accuracy application is supposed to provide.
2. Test Data Generation
The generation of test data is again a challenging job, there are multiple parameters, which have to be taken care of while generating test data. It needs a tool, which can help to generate data and should have functions or logic can also be applied over it. Tools like Talend (an open studio) is the best candidate to fulfill the requirements of data generation.
3. Data Storage
After the generation of test data along with quality, it needs to host on a file system. For testing big data applications, data should be stored in the system similar to the production environment. As we are working in big data space, there should have a different number of nodes, and data must be in a distributed environment.
Big Data Testing — Test Environment
In Big data testing, the test environment should be efficient enough to process a large amount of data as done in the case of a production environment. Real-time production environment clusters generally have 30–40 nodes of cluster and data is distributed on the cluster nodes. There must have some minimum configuration for each node used in the cluster. A cluster may have two modes, in-premise or cloud. For testing in big data, it needs the same kind of environment with some minimum configuration of node.
Scalability is also desired to be there in the test environment of big data testing, it helps to study the performance of application with the increase in the number of resources. That data can be used to define SLA (service level agreement) for that particular application.
Big Data Testing can be categorized into three stages:
Step 1: Data Staging Validation
The first stage of big data testing, also known as a Pre-Hadoop stage, is comprised of process validation.
Validation of data is very important so that the data collected from various source like RDBMS, weblogs, etc are verified and then added to the system.
To ensure data match you should compare source data with the data added to the
Hadoop
system.
Make sure that the right data is taken out and loaded into the accurate HDFS location
Step 2: “Map Reduce” Validation
Validation of “Map Reduce” is the second stage. Business logic validation on every node is performed by the tester. Post that authentication is done by running them against multiple nodes, to make sure that the:
The process of Map Reduce works perfectly.
On the data, the data aggregation or segregation rules are imposed.
Creation of key-value pairs is there.
After the Map-Reduce process, Data validation is done.
Step 3: Output Validation Phase
The output validation process is the final or third stage involved in big data testing. The output data files are created and they are ready to be moved to an
EDW (Enterprise Data Warehouse)
or any other such system as per requirements. The third stage consisted of:
Checking on the transformation rules is accurately applied.
In the target system, it needs to ensure that data is loaded successfully and the integrity of data is maintained.
By comparing the target data with the
HDFS file system data
, it is checked that there is no data corruption.
Big Data — Performance Testing
Big data applications are meant to process a large amount of data, and it is expected that it should take minimum time to process maximum data. Along with it, application jobs should consume a considerable amount of memory and CPU. In big data testing, performance parameter plays an important role and helps to define SLA’s. It covers the performance of the base machine and cluster. Also, for example, In the case of Hadoop, map-reduce jobs should be written with proper coding guidelines, to perform better in the production environment. Profiling can also be done on map-reduce jobs before integration, to ensure their optimized execution.
Tools used in Big Data Scenarios
Big Data Testing — Challenges
In big data testing, certain challenges are involved which needs to be addressed by the big data testing approach.
1. Test Data
Exponential growth had been observed in the growth of data in the last few years. A huge amount of data are being generated daily and stored in large data centers or data marts. So, there is a demand for efficient storage and a way to process it in an optimized way. If we consider the telecom industry, it generates a large number of call logs daily and they need to be processed for better customer experience and compete in the market. The same goes with the test data, test data should be similar to production data and should contain all the logically acceptable fields in it.
This becomes a challenge for testing big data application, generating test data similar to production data is a real challenge. Test data should also be large enough to verify proper working big data application.
2. Environment
The processing of data highly depends on the environment and its performance. An optimized environment setup gives high performance and fast data processing results. Distributed computing is used for the processing of big data which has data hosted in a distributed environment. The testing environment should have multiple numbers of nodes and data should be distributed over the nodes. At the same time, it also needs to monitor those nodes, to ensure the highest performance with minimum CPU and memory utilization. Nodes should be monitored and there should have a graphical presentation of node performance. So, the test environment has two aspects — distributed nodes and their monitoring, which should be covered in the testing approach.
3. Performance
Performance is the key requirement of any big data application, and of course because of which enterprises are moving towards NoSQL technologies, technologies that can handle their big data and process in the minimum time frame. A large dataset should be processed in a minimum considerable time frame. In big data testing, performance testing is a challenge, it requires monitoring of cluster nodes during execution and also time is taken for every iteration of execution.
Conclusion
Big Data is the trend that is revolutionizing society and its organizations due to the capabilities it provides to take advantage of a wide variety of data, in large volumes and with speed. Keeping the challenges in mind we have defined the approach of testing big data applications. This approach of big data testing will make it easy for a test engineer to verify and certify the business requirement implementations and for stack holders, it saves a huge amount of cost, which has to be invested to get the expected business returns.
Originally published at
https://www.loginradius.com
.
Big Data
Testing
Developer
Hadoop
Etl
--
--
Follow
Written by
LoginRadius
761 Followers
·
365 Following
LoginRadius customer Identity management platform serves over 3,000 businesses with a monthly reach of over 1.2 billion users worldwide.
https://loginradius.com
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"""Leverage Dagster & Great Expectations for Data Quality""  | Medium","""Leverage Dagster & Great Expectations for Data Quality""  | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
“Ensuring Data Integrity: Leveraging Dagster and Great Expectations for Automated Data Quality Checks”
kajal kumari
·
Follow
14 min read
·
Sep 25, 2024
--
Share
Introduction
Ensuring data quality is paramount for businesses relying on data-driven decision-making. As data volumes grow and sources diversify, manual quality checks become increasingly impractical and error-prone. This is where automated data quality checks come into play, offering a scalable solution to maintain data integrity and reliability.
At my organization, which collects large volumes of public web data, we’ve developed a robust system for automated data quality checks using two powerful open-source tools: Dagster and Great Expectations. These tools are the cornerstone of our approach to data quality management, allowing us to efficiently validate and monitor our data pipelines at scale.
In this article, I’ll explain how we use Dagster, an open-source data orchestrator, and Great Expectations, a data validation framework, to implement comprehensive automated data quality checks. I’ll also explore the benefits of this approach and provide practical insights into our implementation process, including a Gitlab demo, to help you understand how these tools can enhance your own data quality assurance practices.
Let’s discuss each of them in more detail before moving to practical examples.
Learning Outcomes
Understand the importance of automated data quality checks in data-driven…
--
--
Follow
Written by
kajal kumari
104 Followers
·
52 Following
Data Science Enthusiast|
M.Tech
Computer Science| Machine Learning|Member of Analytics vidhya|https://www.linkedin.com/in/kajal-kumari-6642ba147/
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Automate Data Quality with an LLM | by David Bodie | Medium,"Automate Data Quality with an LLM | by David Bodie | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Automate Data Quality with an LLM
David Bodie
·
Follow
6 min read
·
Apr 18, 2024
--
Listen
Share
I was recently talking with one of my colleagues about using an LLM for automating rules-based data quality validation. The idea is to have the LLM generate deterministic rules which could be applied to a dataset and provide a pass or fail type test. When used in combination with other data quality strategies, such tests could be used to help catch issues and improve overall quality. Automating the development and execution of these tests would also save time for data engineers.
The first step is to outline the process that needs to take place. We are going to start with the premise that the LLM can receive a sample dataset and generate rules based on that sample only. There are other approaches instead of using sample data, such as including DDL or other metadata (descriptions, etc.), but this seemed the most straight forward to start with.
We will utilize a series of prompts (prompt chaining), maintain those prompts and responses (memory), and call a tool (function) to perform an action. Prompt chaining, memory, and tools are concepts utilized by automation frameworks such as LangChain for building sophisticated LLM applications. As with these frameworks, the key is to ensure outputs from the LLM can be successfully parsed. In our case, we want to insure all responses are returned in JSON.
One way of doing this (in Open AI) is to set the response_format parameter to {“type”: “json_object”} as in the code below.
Note
: You still need to declare the output format of JSON in your prompts.
response = openai.chat.completions.create(
model=self.model,
response_format={ ""type"": ""json_object"" },
messages=self.msg,
temperature=self.temperature,
max_tokens=self.max_tokens,
)
We will start with the initial prompt:
You are a data analyst. Your role is to create deterministic rules in order to test the quality of a dataset.
You will be provided sample data below that represents the dataset you need to create rules for.
The sample data provided may not be representative of every value in the data.
You will generally follow this process:
1. Observe the sample data.
2. For each column in the dataset, create a rule based on the observation. Each rule should have a name and a description.
Take into account that the observation is on a sample of data.
[{""rule_name"":""some name"", ""rule_description"": ""some description"",}]
3. Repeat the process to create a rule for each column in the dataset.
Do not return any results until you have at least one rule for each column in the dataset.
Group all of the rules together and output the result into a single JSON object.
We will provide 10 sample records in the following format:
{
""order_date"": ""2023-01-02"",
""order_id"": 1000102,
""customer_name"": ""Bob Johnson"",
""address"": ""789 Oak St"",
""zip_code"": 67890,
""state_abbr"": ""TX"",
""order_amount"": 50.00
}
Combining the sample data with the prompt will return our first output from the model:
{
""rules"": [
{
""rule_name"": ""Valid Order Date Format"",
""rule_description"": ""Order date should be in the format 'YYYY-MM-DD'.""
},
{
""rule_name"": ""Unique Order ID"",
""rule_description"": ""Order ID should be unique for each order.""
},
{
""rule_name"": ""Non-Empty Customer Name"",
""rule_description"": ""Customer name should not be empty.""
},
{
""rule_name"": ""Valid Address Format"",
""rule_description"": ""Address should be in a valid format.""
},
{
""rule_name"": ""Valid Zip Code Format"",
""rule_description"": ""Zip code should be in a valid format.""
},
{
""rule_name"": ""Valid State Abbreviation"",
""rule_description"": ""State abbreviation should be a valid 2-letter abbreviation.""
},
{
""rule_name"": ""Positive Order Amount"",
""rule_description"": ""Order amount should be a positive value.""
}
]
}
This output is added into the message history to set the stage for the next prompt.
Note
: We indicate the type of database (SQLite) in the prompt to get the correct SQL dialect.
1. Create a SQL query for each rule.
2. The output value should be true or false and should return a single aggregate value for the entire column.
Make sure the query is an aggregate query.
3. Be sure to include the ""rule name"" in the sql query.
4. The database is SQLite. Generate SQL compatible with SQLlite.
5. The dataset name is ""Orders"".
6. The output should be in JSON format like this:
output:
""[{""rule_name"":""some name"", ""sql_query"": ""SELECT 'rule name' as Rule, 'true or false' as value FROM TABLE...""},
{""rule_name"":""some name"", ""sql_query"": ""SELECT 'rule name' as Rule, 'true or false' as value FROM TABLE...""},
{""rule_name"":""some name"", ""sql_query"": ""SELECT 'rule name' as Rule, 'true or false' as value FROM TABLE...""},
...]""
7. Make sure the results are valid JSON.
The model has generated deterministic SQL queries we can run against the database table:
{
""output"": [
{
""rule_name"": ""Valid Order Date Format"",
""sql_query"": ""SELECT 'Valid Order Date Format' as Rule, CASE WHEN COUNT(*) = COUNT(CASE WHEN order_date LIKE '____-__-__' THEN 1 END) THEN 'true' ELSE 'false' END as value FROM Orders""
},
{
""rule_name"": ""Unique Order ID"",
""sql_query"": ""SELECT 'Unique Order ID' as Rule, CASE WHEN COUNT(*) = COUNT(DISTINCT order_id) THEN 'true' ELSE 'false' END as value FROM Orders""
},
{
""rule_name"": ""Non-Empty Customer Name"",
""sql_query"": ""SELECT 'Non-Empty Customer Name' as Rule, CASE WHEN COUNT(*) = COUNT(CASE WHEN customer_name <> '' THEN 1 END) THEN 'true' ELSE 'false' END as value FROM Orders""
},
{
""rule_name"": ""Valid Address Format"",
""sql_query"": ""SELECT 'Valid Address Format' as Rule, CASE WHEN COUNT(*) = COUNT(CASE WHEN address <> '' THEN 1 END) THEN 'true' ELSE 'false' END as value FROM Orders""
},
{
""rule_name"": ""Valid Zip Code Format"",
""sql_query"": ""SELECT 'Valid Zip Code Format' as Rule, CASE WHEN COUNT(*) = COUNT(CASE WHEN length(zip_code) = 5 THEN 1 END) THEN 'true' ELSE 'false' END as value FROM Orders""
},
{
""rule_name"": ""Valid State Abbreviation"",
""sql_query"": ""SELECT 'Valid State Abbreviation' as Rule, CASE WHEN COUNT(*) = COUNT(CASE WHEN length(state_abbr) = 2 THEN 1 END) THEN 'true' ELSE 'false' END as value FROM Orders""
},
{
""rule_name"": ""Positive Order Amount"",
""sql_query"": ""SELECT 'Positive Order Amount' as Rule, CASE WHEN COUNT(*) = COUNT(CASE WHEN order_amount > 0 THEN 1 END) THEN 'true' ELSE 'false' END as value FROM Orders""
}
]
}
The SQL is executed by a function that queries the database and returns a result. The results are added to the message history. Below is the final prompt:
Summarize the results.
Include the rule definition for each column, the SQL query used, and the results of the SQL query.
Return the results in a JSON object without any additional notation. Make sure the results are valid JSON.
The final output is returned:
{
""results"": [
{
""rule_definition"": ""Order date should be in the format 'YYYY-MM-DD'."",
""sql_query"": ""SELECT COUNT(*) = COUNT(CASE WHEN strftime('%Y-%m-%d', order_date) IS NOT NULL THEN 1 END) FROM Orders;"",
""result"": true
},
{
""rule_definition"": ""Order ID should be unique for each order."",
""sql_query"": ""SELECT COUNT(DISTINCT order_id) = COUNT(*) FROM Orders;"",
""result"": true
},
{
""rule_definition"": ""Customer name should not be empty."",
""sql_query"": ""SELECT COUNT(*) = COUNT(CASE WHEN customer_name <> '' THEN 1 END) FROM Orders;"",
""result"": false
},
{
""rule_definition"": ""Address should be in a valid format."",
""sql_query"": ""SELECT COUNT(*) = COUNT(CASE WHEN address <> '' THEN 1 END) FROM Orders;"",
""result"": true
},
{
""rule_definition"": ""Zip code should be in a valid format."",
""sql_query"": ""SELECT COUNT(*) = COUNT(CASE WHEN length(zip_code) = 5 THEN 1 END) FROM Orders;"",
""result"": true
},
{
""rule_definition"": ""State abbreviation should be a valid 2-letter abbreviation."",
""sql_query"": ""SELECT COUNT(*) = COUNT(CASE WHEN length(state_abbr) = 2 THEN 1 END) FROM Orders;"",
""result"": true
},
{
""rule_definition"": ""Order amount should be a positive value."",
""sql_query"": ""SELECT COUNT(*) = COUNT(CASE WHEN order_amount > 0 THEN 1 END) FROM Orders;"",
""result"": true
}
]
}
The code for this is relatively simple. You can view it on my
Github
. There are many directions you could take this. A next step might be having the LLM come up with multiple rules and pick the best one. Alternatively, you could give the LLM more context about the types of rules you need it to generate. You could even provide support for loading your own rules. These options would likely involve additional reasoning by the LLM which may be better implemented using a framework like LangChain.
Lastly, there are a couple of caveats you need to take note of. First, if you are going to automate querying a database, you need to take special care and limit permissions to make sure there are no rogue queries getting executed. Second, you do not want to be sending your data to the public Open AI API, so you will want to leverage Open AI’s Enterprise capability or Microsoft’s Azure Open AI service. Both offer ways to securely interact with an LLM and your data.
Generative Ai Use Cases
Data Quality
Generative Ai Tools
--
--
Follow
Written by
David Bodie
8 Followers
·
2 Following
Passionate about all things Data.
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Creating a custom data quality check on DBT (data build tool) | by Hengkywu | tiket.com | Medium,"Creating a custom data quality check on DBT (data build tool) | by Hengkywu | tiket.com | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Creating a custom data quality check on DBT (data build tool)
Hengkywu
·
Follow
Published in
tiket.com
·
3 min read
·
Apr 8, 2022
--
Listen
Share
This article will cover the topic of creating a custom data quality check since there might be various use cases to do quality checks depending on demands/needs for a specific dataset.
Definition
In this context, quality data means that the data ingested into a database/data warehouse would be qualified enough in terms of accuracy, validity, completeness, and cleanliness to be represented in business purpose insight such as reports that support decision-making.
First things first, you should assess the data, define your use cases and decide what’s need to be checked. Nevertheless, this article will demonstrate the general use cases you may encounter, particularly duplicate and null data checks.
Let’s get into how we can create a custom data quality check on DBT.
Disclaimer:
For the data environment, we use Google’s BigQuery.
Write a quality check query:
Given the following dummy data:
The above query will check if there is any duplication or null value on the field that we use as the key.
Utilizing macro on DBT
In this exercise, we will create two important macros, namely the data quality generator and get test values.
Data quality generator
Firstly, we will need a macro that can automatically generate a query based on use cases that we defined earlier and then load the result back into the log table.
Next, we parameterize the values that will be received by this macro:
unique_keys = list of field that will be check
qa_target_table = target table where the result of check will be stored
qa_source_table = a table that will be use as source table for checking
As a result, you can call the macro anywhere and as simple as:
{% do data_quality_generator(unique_keys, qa_target_table, qa_source_table) %}
Sample of log table:
Get Test Values
Remember that in order to know if our data quality is good or not, we need to focus on the objective of our use cases. Hence, we need another macro to get a concrete measurement of data quality. In this exercise, we want to know if data points are either duplicated or null. Therefore, the purpose of this macro is simply to get the results (computed via the previous generator) in the log table by using the following two parameters:
model = refer to field table name on log table
test_type = will be use cases we need to get result from
To get a result, we can input the following command:
{% set duplicate_check_result = get_test_values(tablename, 'use case name') %}
And last but not least, run the validation based on our needs, for example:
{% if duplicate_check_result[0] != ""0"" %}
{% do exceptions.raise_compiler_error('duplicate records found') %}
That’s it.
This might not be the best approach, but it is a comfortable approach personally for me.
Thank you for reading. I am willing to be open to any inputs, discussions or improvement.
Data
Data Engineering
Dbt
Data Quality
--
--
Published in
tiket.com
857 Followers
·
Last published
Jul 17, 2024
Mau ke mana? Semua ada tiketnya
Follow
Written by
Hengkywu
14 Followers
·
4 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How Data Analytics Professionals Can Sleep Better | by DataKitchen | data-ops | Medium,"How Data Analytics Professionals Can Sleep Better | by DataKitchen | data-ops | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How Data Analytics Professionals Can Sleep Better
DataKitchen
·
Follow
Published in
data-ops
·
4 min read
·
Mar 21, 2017
--
Listen
Share
Seven Steps to Implementing DataOps: Step 1 — Add Data and Logic Tests
In a previous blog we introduced
DataOps
, a new approach to data analytics, which can put analytics professionals in the center of the company’s strategy, advancing its most strategic and important objectives. DataOps doesn’t require you to throw away your existing tools and start from scratch. With DataOps, you can keep the tools you currently use and love.
You may be surprised to learn that an analytics team can migrate to DataOps in seven simple steps. Step 1 of 7 is below.
If you make a change to an analytic pipeline, how do you know that you did not break anything? Between the anxiety and the phone calls at odd hours, a data analytics professional might not be sleeping well when changes are being made to a business-critical system. Below we’ll discuss how to prevent IT emergencies and hopefully improve your sleep quality.
Tests applied throughout the data analytic pipeline
Automated testing insures that a feature release is of high quality without requiring time-consuming, manual testing. The idea in DataOps is that every time a data-analytics team member makes a change, he or she adds a test for that change. There are two categories of tests: Logic Tests that cover the code in a data pipe line and Data Test that cover the data as it flows by in production.
Testing is expanded incrementally, with the addition of each feature, so testing gradually improves and quality is literally built in. In a big run, there could be hundreds of tests at each stage in the pipeline. Every time a release is deployed to users, the tests are run to validate the functionality of the release. Each one of those tests is an insurance policy against critical failures. This is bound to improve the mental well-being of the data analytics professional.
Adding tests in data analytics is analogous to the
statistical process controls
that are implemented in a manufacturing operations flow. Tests insure the integrity of the final output by verifying that work-in-progress (the result of intermediate steps in the pipeline) matches expectations. Testing can be applied to data, models and logic. The table below shows examples of tests in the data-analytics pipeline.
Tests may be applied to inputs, business logic and outputs
For every step in the pipeline, there should be at least one test. The philosophy is to start with simple tests and grow over time. Even a simple test will eventually catch an error before it is released to the users. For example, just making sure that row counts are consistent throughout the process can be a very powerful test. One could easily make a mistake on a join, and make a cross product, which fails to execute correctly. A simple row-count test would quickly catch that.
Tests can detect warnings in addition to errors. A warning might be triggered if data exceeds certain boundaries. For example, the number of customer transactions in a week may be OK if it is within 90% of its historical average. If the transaction level exceeds that, then a warning could be flagged. This might not be an error. It could be a seasonal occurrence for example, but the reason would require investigation. Once recognized and understood, the users of the data could be alerted. Warnings can be a powerful business tool which helps the company understand its business better.
DataOps is not about being perfect. In fact, it acknowledges that code is imperfect. It’s natural that a data-analytics team will make a best effort, yet still miss something. If so, they can determine the cause of the issue and add a test so that it never happens again. In a rapid release environment, a fix can quickly propagate out to the users.
With a suite of tests in place, DataOps allows you to move fast because you can make changes and quickly rerun the test suite. If the changes pass the tests, then the data-analytics team member can be confident and release it. The knowledge is built into the system and the process stays under control. Tests catch potential errors and warnings before they are released so the quality remains high. When quality is ensured, the data analytics team can sleep like babies.
In our
next blog
we will cover step 2 in implementing DataOps.
Like this story? Download the
140 page DataOps Cookbook
!
Dataops
Analytics
Agile
DevOps
Big Data
--
--
Published in
data-ops
2.4K Followers
·
Last published
Nov 18, 2022
The DataOps Blog
Follow
Written by
DataKitchen
2.2K Followers
·
47 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
🔧Article 2 : Using Models and Tests with dbt and Databricks: Ensuring Data Quality and Accuracy! ✅ | by Abdelbarre Chafik | Medium,"🔧Article 2 : Using Models and Tests with dbt and Databricks: Ensuring Data Quality and Accuracy! ✅ | by Abdelbarre Chafik | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
🔧Article 2 : Using Models and Tests with dbt and Databricks: Ensuring Data Quality and Accuracy! ✅
Abdelbarre Chafik
·
Follow
4 min read
·
Jul 16, 2023
--
Listen
Share
Now that you’ve mastered the basics of dbt and Databricks integration, let’s take it a step further and explore how to use models and tests to ensure data quality and accuracy in your data transformation workflows. By leveraging dbt’s modeling capabilities and testing framework, you can confidently transform your data with precision. Let’s dive in!
🔧 Defining Models
1️⃣ Create a Model:
In your dbt project’s `models` directory, you can build models on top of other models to create complex transformations. Simply reference the existing models in your new model definition. For example:
In this example, we create a model called `
customers
` that references stg_customers and stg_orders. This allows us to perform any necessary data cleaning or transformations specific to the customer data.
To do this
Create a new SQL file,
models/stg_customers.sql
, with the SQL from the
customers
CTE in our original query.
Create a second new SQL file,
models/stg_orders.sql
, with the SQL from the
orders
CTE in our original query.
select
id as customer_id,
first_name,
last_name
from dbt_achafik.jaffle_shop_customers
select
id as order_id,
user_id as customer_id,
order_date,
status
from dbt_achafik.jaffle_shop_orders
2️⃣
Build Models on Top
: To build a model on top of the `
customer
` model, Edit the SQL in your
models/customers.sql
file as follows and reference the existing model. For instance:
with customers as (
select * from {{ ref('stg_customers') }}
),
orders as (
select * from {{ ref('stg_orders') }}
),
customer_orders as (
select
customer_id,
min(order_date) as first_order_date,
max(order_date) as most_recent_order_date,
count(order_id) as number_of_orders
from orders
group by 1
),
final as (
select
customers.customer_id,
customers.first_name,
customers.last_name,
customer_orders.first_order_date,
customer_orders.most_recent_order_date,
coalesce(customer_orders.number_of_orders, 0) as number_of_orders
from customers
left join customer_orders using (customer_id)
)
select * from final
Here, we’re building the `
customer
` model on top of the `
stg_orders
` and
stg_customers
model. By reusing and extending existing models, you can create a layered and modular approach to your data transformations.
Execute
dbt run
.
This time, when you performed a
dbt run
, separate views/tables were created for
stg_customers
,
stg_orders
and
customers
. dbt inferred the order to run these models. Because
customers
depends on
stg_customers
and
stg_orders
, dbt builds
customers
last. You do not need to explicitly define these dependencies.
🧪 Writing Tests
1️⃣
Define Tests
:
Writing tests for models built on top of other models follow a similar approach. Create separate test files and reference the corresponding models. For example:
Create a new YAML file in the
models
directory, named
models/schema.ym
version: 2
models:
- name: customers
columns:
- name: customer_id
tests:
- unique
- not_null
- name: stg_customers
columns:
- name: customer_id
tests:
- unique
- not_null
- name: stg_orders
columns:
- name: order_id
tests:
- unique
- not_null
- name: status
tests:
- accepted_values:
values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
- name: customer_id
tests:
- not_null
- relationships:
to: ref('stg_customers')
field: customer_id
2️⃣ Run Tests:
Execute the tests using the
Run
dbt test
, and confirm that all your tests passed.
dbt will run the defined tests against the transformed data in Databricks, ensuring the integrity and accuracy of your data transformations.
✅ Data Quality Assurance Achieved!
By building models on top of other models in your dbt project, you can create complex and interconnected data transformations while maintaining a modular and reusable structure. The tests validate the column structure, aggregations, and any other criteria you define, ensuring the quality and accuracy of your data.
💡 Next Steps:
Experiment with additional models, explore different testing scenarios, and fine-tune your data transformation workflows. Stay tuned for more articles where we’ll delve deeper into advanced techniques and best practices for using dbt and Databricks effectively!
#dbt #Databricks #DataTransformation #DataAnalytics #DataEngineering #DataQuality #Testing
--
--
Follow
Written by
Abdelbarre Chafik
81 Followers
·
71 Following
Senior Data Engineer
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
"Avoid bad data completely: Continuous Delivery Architectures in the Modern Data Stack (Part 2) | by Hugo Lu | Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science | Medium","Avoid bad data completely: Continuous Delivery Architectures in the Modern Data Stack (Part 2) | by Hugo Lu | Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
The answer to your data quality woes. Photo by
Tatiana Rodriguez
on
Unsplash
Avoid bad data completely: Continuous Delivery Architectures in the Modern Data Stack (Part 2)
How to use dbt and Snowflake to avoid releasing bad data into production
Hugo Lu
·
Follow
Published in
Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science
·
10 min read
·
Sep 6, 2023
--
Listen
Share
About me
Hi there 👋 I’m Hugo Lu — I started my career working in finance before moving to a scale-up and falling into data engineering. After a brief stint back in finance, I headed up the Data function at London-based Fintech
Codat
. I’m now CEO at
Orchestra
, which is a data release pipeline tool that helps Data Teams release data into production reliably and efficiently.
Introduction
In the
previous article
, I covered how using a clone command is a possible solution to releasing bad data into production. Although the architecture only includes a single “Production” environment, it also includes an “exposed” and a “non-exposed” set of schema and tables. As data is
static
(if data is in a certain state at a given point in time, so long as the data doesn’t change it will continue to be so at any point in time) and
independent
(changes in external systems do not affect the state of the data) we saw how this architecture could be sufficient to reliably and efficiently release data into production using a data release pipeline. This represents a Continuous Delivery (“CD”) process
but for data
which is what Data Teams should strive to achieve.
In this article, we’ll dive into how to use
dbt-core
and
Snowflake
to bring this to life.
dbt-core and Snowflake
Snowflake is a data warehouse. A data warehouse is a place to store data (you might consider this a database or group of databases). Data warehouses typically store data in structured formats (columns and rows, rather than unstructured jsons or NO-SQL databases) for analytical or reporting use-cases.
dbt-core is an open-source package that can be used to reliably and efficiently transform data. Data teams that use dbt-core have a repository of SQL code and .yml configuration files that effectively govern a set of procedures or queries that are run on data in a data warehouse.
In the following example, we use the Snowflake Customer (100m rows) and Snowflake Catalog Sales or “Orders” (c.140bn rows) datasets which can be found at
SNOWFLAKE_SAMPLE_DATA.TPCDS_SF100TCL.
Project setup
The dbt project follows a very basic architecture. Models are divided up into:
Sources: the customer and orders raw datasets
Clean: tables that do basic manipulation of data like converting Julian dates into readable dates and renaming columns
Staging: tables that include data from multiple datasets i.e. tables where joins are required
Aggregated: tables that represent the final structure of data to be exposed to end users
We introduce another model sub-directory which is:
Production: tables for sole-use by end-users
The code in Github can be found
here
.
A basic structure for a dbt project
In this example, the customers and orders datasets are cleaned. We then join them into an
order_staging
table with enriched customer information. We perform group-by’s in the
orders_aggregated
table to yield a table which is one row per customer per day (so we can see aggregated order information over time). This table is eventually replicated to prod.
We can see that access to the various schema is also clearly demarcated; in a real-life scenario, data producers would have
write
access to the raw schema. dbt has access to every section. End-users only have access to the
prod
schema. This means that as long as data quality testing can catch data quality issues before the aggregated table gets materialised, and this step is independent of the copying to prod, data quality issues never appear in prod.
The one exception to this rule is
recency
. If data fails to load for whatever reason (maybe data producers have broken their Kafka streams), a data quality test (to test for recent data) should pick this up in the
raw
schema and prevent tables being materialised in the blue section. Technically, at this point in time, there
will
be a data quality issue in the
prod
schema — the data will not be
up-to-date.
An *incredibly important* feature of this architecture is having adequate tests in place. Every SQL model (including sources!!!) has a corresponding entry in a .yml file that looks a bit like this:
There are typically three must-have tests; primary key, pk not null and recency
Here the tests are commented out as the table is 140bn rows and therefore I couldn’t run them on the whole datasets quickly using an X-SMALL warehouse, but ideally you would always run these tests before kicking off a dbt workflow using
dbt build.
By having this configuration, if the data is expected to be no more than 24 hours old and it
is
older than this, dbt won’t run. You’ll get an error, and you’ll need to fix it.
It’s straightforward to add a test for recency. Here, the Snowflake raw date format is annoyingly in Julian days. Don’t forget to put tests in tests/generic:
{% test recent_julian_days(model, column_name, num_days) %}
WITH max_date AS (
-- Not 100% this is completely correct, would be easier to check if
-- we knew when the most recent date in Snowflake was meant to be
SELECT MAX(cast(dateadd(DAY,{{column_name}} - 2415020, '1920-01-01') as date)) as latest_date
FROM {{model}}
)
SELECT *
FROM max_date
WHERE latest_date < dateadd(day, -{{num_days}}, current_date())
{% endtest %}
You can also use Chat GPT to write tests for you:
It’s not exactly how I would’ve done it and it doesn’t have the proper jinja syntax, but it’s 80% of the way there.
Generally, I would also recommend a row count test like the one in the screenshot above; it’s no use if there
is
recent data. You want that recent data to be close to the volume you expect it to be. For example, if you normally ingest c.100k rows a day, you’d definitely want a flag if you only ingested 10k rows. Maybe not, if you ingested 90k rows. Sure, you can implement some seasonality using Python and implement more complicated thresholds (I have a post on that
here
) but that’s normally overkill.
The advantage of testing sources is that if you’re confident your dbt is correct, you can catch alot by just testing the hell out of your sources ; if there’s recent data in the source, there should be recent data all the way upstream. There is a tradeoff here; cost for certainty. If you don’t care about cost (or speed) implement recency tests everywhere. If you are confident in your dbt, just do it on sources. If in the middle, you could test on the aggregate schema and the source schema.
Error types
All I’ve described here is a very basic dbt architecture using tests. If you test your sources, generally speaking, you’ll catch any “raw data quality issues” before spending cost doing unnecessary transformations.
Upstream tests are there to catch SQL errors. For example, the below should set alarm bells ringing:
select
a.orderid,
a.customerid,
b.customername,
sha2_binary(concat(
ifnull(cast(a.orderid as string), '')
)) as _pk
from orders a
left join customers b
-- Oops; something missing here
Can you see why? This is very bad from an analyst! A unique test on the primary key will catch the analyst’s error, and prevent downstream models running. This is probably a staging model, though, and is a fundamentally different
type
of error to raw data quality errors (which typically make up like, 90% of data quality issues in my experience at not massive enterprise organisations). A well-defined primary key and a unique test should normally be enough to catch most SQL errors. There will be more complicated use-cases, and here you should have tests that catch those types of errors. For example, if you know your company’s revenue is always going to be between $10m and $100m, you could implement a test on the metric to ensure the value lies between the value. Often, when data is ingested at different speeds or different cadences, customer information (for example) can be null as the requisite tables are not updated. Here, you would use a not_null test on a chosen field or a dependency test. Generally, the main error types are:
Tests on Sources
primary key
PK not null
Recency
Rows
Tests on upstream SQL
Primary key
PK not null
Dependencies (e.g. IDs not getting filled in) / not null tests on IDs
Metric expectations (e.g. Revenue being $1bn dollars or 0)
Implementing the Prod Schema
Ok — so you have all your tests and you finally ran
dbt build
and everything worked. What next?
Hooray
Here’s what you don’t do — what you don’t do is run everything
again
on a completely separate clean, staging, and aggregated schema. That’s because that’s literally doubling everything you don’t want (like time, cost) and is a bit of an orchestration headache in dbt. If you had some more cash to burn and slow speeds was no problem, you could do this.
Remember the Prod Schema? Here’s what the prod model SQL file looks like:
{{
config(
materialized='incremental',
unique_key='_pk'
)
}}
select
*
from {{ref('orders_aggregared')}}
{% if is_incremental() %}
-- this filter will only be applied on an incremental run
-- (uses >= to include records arriving later on the same day as the last run of this model)
where ship_date >= dateadd(DAY,-1,current_date())
{% endif %}
Here we insert the new data into the PROD table. It will run automatically as it depends on
snowflake_orders
, so it only happens if
dbt build
runs successfully. This is quite compute-heavy, though. You’ll use the warehouse resources to execute this query. This is a robust way to do it, but it’s not the best way. You can do this, or ignore this file and move on.
Post-hooks
Post-hooks are a bit hacky, but they’re quite nice. A post-hook is basically a way to call some arbitrary SQL after a model finishes. They only run if the model succeeds, and don’t run if the model fails. Annoyingly, the dbt logs don’t show whether it fails or succeeds. Being able to run arbitrary SQL is important, because the clone command is a command, not a select statement:
create table cloned_table clone original_table
-- Cute right
You create a macro like so:
-- Note: I am sure this could have been parameterised better.
{% macro clone() %}
{% set identifier_parts = this.identifier.split('.') %}
{% set table_name = identifier_parts[-1] %}
create or replace table SNOWFLAKE_WORKING.prod.{{table_name}}_clone
clone snowflake_working.aggregated.{{table_name}}
{% endmacro %}
You then add this into the dbt config for the specific model. In this case, it would be the aggregate model:
{{
config(
materialized='table',
transient=false,
post_hook=[
""SELECT 1;
{{clone()}}""
]
)
}}
Note you need the “Select 1;” statement since there needs to be some select-based SQL for post-hooks to work (remember how I said it was hacky). However, you’re using the Snowflake clone command now which is A) Rapid and B) Cheap. You only use cloud_services_credits instead of Compute credits. The clone doesn’t change even if the underlying data changes, so now you
know
that the data in Production IS CORRECT — as long as you are confident your dbt model has been specced properly ;)
You also know that the data in Production is
never
going to get updated unless your dbt project fully completes.
This can also be set up in such a way where the only users that have access to do the clone operations into the prod schema are the users that are used by dbt i.e. ones with programmatic access. So you can really lock it down and make sure no-one changes prod manually (which is exactly how Continuous Delivery works in software).
You could make use of a different hook; the -on-run-end hook, which fires once the whole run has ended. I don’t think this works for this use-case. If you have two models you want to promote to production, one succeeds and one fails, you need a way to promote the successful one and block the failed one. This can only be done by having hooks specific to models.
Conclusion
In Part 2 of “Avoiding Bad Data completely”, we showed how using Snowflake and dbt data engineers, analytics engineers, hell, anyone can ensure bad data never gets into production. Sure, there can still be some data quality issues in production like recency, but the vast majority of errors get caught before data gets released into prod using this architecture.
There are some areas to explore:
How does this look for other data warehouses, is it even possible?
How do company-specific circumstances introduce types of errors that can’t get caught by this architecture?
Is there a simple way to run a dbt model and its dependencies using Continuous Delivery in response to a partially failed run?
How fast can cloned data be queried?
These are all things I’ll dive into in due course. If you enjoyed, feel free to give me a follow or connect on
Linkedin
.
Peace out!
Hugo
Further reading
There is no-one better at understanding Snowflake Metadata than the folk’s over at London-based SELECT. They have a few nice articles,
like this one
on cloning speeds, that are equal parts enlightening and technical.
Snowflake
Data Warehouse
Data Release Pipeline
Data Science
Data Engineering
--
--
Published in
Snowflake Builders Blog: Data Engineers, App Developers, AI/ML, & Data Science
7K Followers
·
Last published
4 hours ago
Best practices, tips & tricks from Snowflake experts and community
Follow
Written by
Hugo Lu
9.5K Followers
·
51 Following
I write on Data engineering and the coolest data stuff. CEO@ Orchestra, the best-in-class unified control plane for dataops.
https://app.getorchestra.io/signup
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Big Data is Dead. All Aboard the AI Hype Train! | by Hyon S Chu | Medium,"Big Data is Dead. All Aboard the AI Hype Train! | by Hyon S Chu | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Big Data is Dead. All Aboard the AI Hype Train!
Hyon S Chu
·
Follow
6 min read
·
Jan 26, 2016
--
11
Listen
Share
It’s 2016, and businesses big and small, far and wide have finally stopped using the term
Big Data.
The consensus seems to be converging on the idea that data alone doesn’t solve problems. It’s true. You still need to understand, analyze, and test test test data using hypotheses to prove intuitions and make solid decisions. Things that should be happening
regardless
the size of your data.
This is not how AI works
But instead of developing creative uses for the data that we have, we’re all now looking to ‘cognitive computing’ and ‘artificial intelligence’ to save us. Companies like Google, Facebook, Microsoft, and IBM are all having an arms race against each other trying to outsmart and out-engineer each other for better accuracy. Meanwhile, marketing teams with lots of money have entranced us all with the possibility of having computers think for us, tell us what our problems are, and auto-magically fix them and improve our business processes.
I mean it’s twenty-goddamn-sixteen up in here. No one wants a flying car anymore unless it’s driving itself.
And we bought into it, freely throwing around terms like ‘artificial neural networks’ and ‘deep learning,’ reinforcing the belief that computers are becoming more like us. (IT
LITERALLY
REFERS TO BRAINS!) We’ve become comfortable with terms like ‘The Singularity’ as
we, as a society, welcome our new robotic overlords
. Indeed, 2016 is shaping up to be the year of AI.
Yes, Kalev H. Leetaru actually said that
In response to the death of Big Data, companies who need to sell more stuff are now telling us
now that you have this data, what your business really needs is analysis done by super-fast, omniscient computer brains
. Which is a nice idea, but ‘artificial intelligence’ isn’t anywhere close what most people consider it to be.
Raise your hand if you’ve seen PowerPoint slides like this
Near the end of last year, analysts were proclaiming that 2016 is the year the algorithms make data useful. Gartner made headlines by proclaiming “
Data is Dumb. Algorithm is Where the Value Lies
.” IBM seems to allude to the notion that it can help Bob Dylan improve songwriting in TV ads. And nearly everyone’s afraid these
AI algorithms will eventually destroy the world
.
Sanity check
: If these algorithms are so smart and therefore valuable, why are Facebook and Google (and
scikit-learn
) giving away their state-of-the-art algorithms for free?
Consider how Google operates. The MapReduce paradigm was so crucial to Google’s core business that its very existence was kept close to the vest. It was a key business driver and led to enormous growth within the company.
When Google decided to reveal and give away MapReduce
, they were so far ahead of the data parallelization game that they didn’t need it anymore.
Following that logic,
Google giving away their “AI engine” TensorFlow
should mean that Google already has something that is so mind-blowing that it should be able to tell what you’ll have for dinner tonight.
Or perhaps the more likely explanation is that
Google has no idea how to extract value from it
. I mean, other than recognize pictures of cats.*
I know it’s a very bold statement to make. But in practice, neither Google nor Facebook have found a way to use their “artificial intelligence” superpowers to improve their core business:
getting me to view or click their ads.
IBM’s “cognitive computing platform”
Watson
is in a similar situation. Sure, it did a great job of retrieving facts and winning Jeopardy in 2008, but quickly faded into relative obscurity. In 2014, IBM
put together a $100MM fund
to help app development for Watson, and all they seem to have have to show is
8 featured apps on their home page
, none of which I completely understand. Not even a giant pile of money couldn’t bring a high-visibility app to Watson. Curious.
In the Bob Dylan ad, Watson claims it can read millions of documents quickly, with the only conclusion being that Dylan’s major themes are
time passes
and
love fades
. Dylan then suggests they write a song together, to which,
Watson evades the suggestion in its sole stroke of brilliance by saying, “I can sing.”
https://en.wikipedia.org/wiki/List_of_burn_centers_in_the_United_States
While it is an exciting research field, AI in its current state is nothing more than just algorithms—math instructions. Algorithms are fast. Algorithms are often elegant. But algorithms are still dumb. Even when they’re “self-correcting,” they still need an immense leveraging of human intelligence and input to do something simple. It is currently nowhere near the levels promised in Wired, TechCrunch, or Gartner reports.
“By 2030, 90% of jobs as we know them today will be replaced by smart machines,”
again, Gartner
. LOL.
If algorithms are truly the key to business success, why do the largest companies who have spent the most money developing and and investing in clever algorithm platforms just give them away? Why has
Yahoo chosen to give away 13.5 terabytes of real consumer data
for research and recruiting? Could it be possible that the greatest minds in Silicon Valley are giving away algorithms and data, effectively saying, “We don’t know what to do with this, either!”?
¯\_(ツ)_/¯
— Silicon Valley
There is an inherent belief ‘round these parts that all problems (ie.
world hunger
) can be engineered away if you code enough lines, and that intelligence is just a matter of sufficiently-programmed algorithms. Pump enough Big Data™ into this Artificial Intelligence Engine™ and all your problems will be solved by intelligent computers. But I believe intelligence requires much more than just engineering and processing power. Intelligence has overtones of curiosity, problem-solving (and problem-creation), and a touch of insanity, none of which have been replicated in any AI lab.
“At the risk of overgeneralizing, the CS majors have convinced each other that the best way to save the world is to do computer science research.” —
Dylan Matthews, Vox
Maybe we’ll get there some day. But for now and the foreseeable future, the best way to attack your business problems is still done the old fashioned way: creative, smart, and curious people who can ask the right questions and know how to get them answered. Big, dumb algorithms and warehouses of data are useless without them. After all, they are still very much missing the critical portion of the puzzle: actual intelligence.
Edit: Added March 16, 2016
PS. Feel free to ping me on
twitter
or
LinkedIn
with your thoughts.
* Update:
Go.
They taught it to play Go
.
Artificial Intelligence
Machine Learning
Big Data
--
--
11
Follow
Written by
Hyon S Chu
243 Followers
·
192 Following
I write about logic and reason, data and people.
Follow
Responses (
11
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Role of Big Data Analytics in Semiconductor Testing Software | Medium,"Role of Big Data Analytics in Semiconductor Testing Software | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Role of Big Data Analytics in Semiconductor Testing Software
Leonardojaxson
·
Follow
2 min read
·
Dec 2, 2022
--
Listen
Share
As the semiconductor industry looks to big data analytics to solve some of its most pressing challenges, testing software is playing an increasingly important role. By analyzing large volumes of data generated by semiconductor manufacturing processes,
Semiconductor
yield management software
can help identify process anomalies and yield improvements.
In particular, big data analytics can help semiconductor testing software in three key ways:
1. Improving test program quality and effectiveness
2. Reducing test time and cost
3. Enhancing test coverage and productivity
Each of these benefits can have a significant impact on the bottom line for semiconductor manufacturers. Let’s take a closer look at how big data analytics can help deliver each one:
1. Improving test program quality and effectiveness: By analyzing vast amounts of data generated during the testing process, big data analytics can help identify areas where test programs are ineffective or could be improved. This knowledge can then be used to make changes that result in better-quality test programs overall. Additionally, by understanding which tests are most effective at detecting certain types of defects, manufacturers can prioritize those tests to further improve program quality.
2. Reducing test time and cost: The ability to analyze large volumes of data generated by semiconductor manufacturing processes can also help identify areas where test time could be reduced without compromising quality. In many cases, this might involve redesigning tests to target specific types of defects rather than running more general tests that take longer to complete.
3. The role of big data analytics in semiconductor testing software is becoming increasingly important. By analyzing large amounts of data, semiconductor manufacturers can improve the quality of their products and reduce costs. Big data analytics can also help identify problems early on in the manufacturing process, which can save time and money. As the semiconductor industry continues to grow, we expect that big data analytics will play an even more important role in helping manufacturers produce high-quality products.
Semiconductors
--
--
Follow
Written by
Leonardojaxson
4 Followers
·
96 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Best Practices for DataOps Testing: How to ensure data quality and accuracy through effective testing. | by AI & Insights | Medium,"Best Practices for DataOps Testing: How to ensure data quality and accuracy through effective testing. | by AI & Insights | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Best Practices for DataOps Testing: How to ensure data quality and accuracy through effective testing.
AI & Insights
·
Follow
2 min read
·
Mar 13, 2023
--
Listen
Share
Testing is an essential component of DataOps, and it plays a critical role in ensuring data quality and accuracy. Let’s discuss best practices for DataOps testing to help you improve the quality and accuracy of your data analytics.
Define your testing strategy:
Before you begin testing, it’s important to define your testing strategy. This should include defining the scope of your testing, identifying the types of tests you need to run, and determining the tools and technologies you will use.
Test data quality:
Data quality is one of the most critical aspects of DataOps testing. You need to test your data for completeness, accuracy, consistency, and validity. This can be achieved through data profiling, data validation, and data cleansing.
Test data integration:
Data integration is another key aspect of DataOps testing. You need to test your data integration processes to ensure that data is moved from source systems to target systems correctly, and that it is transformed into the right format. This can be achieved through integration testing, regression testing, and end-to-end testing.
Test data security:
Data security is a critical consideration in DataOps testing. You need to ensure that sensitive data is protected throughout the data lifecycle, from data creation to data disposal. This can be achieved through access controls, data encryption, and data masking.
Automate your testing:
DataOps requires a high degree of automation to ensure that testing is fast, repeatable, and reliable. By automating your testing, you can reduce the risk of human error, speed up the testing process, and increase the accuracy of your results.
Use test data management tools:
Test data management tools can help you to manage your test data more effectively. These tools enable you to create, manipulate, and maintain test data, and ensure that it is consistent and accurate across all testing environments.
Monitor your testing:
Finally, it’s important to monitor your testing to ensure that it is delivering the desired outcomes. This might include monitoring testing performance, tracking testing metrics, and measuring the impact of testing on data quality and accuracy.
DataOps testing is essential for ensuring data quality and accuracy. By defining your testing strategy, testing data quality, testing data integration, testing data security, automating your testing, using test data management tools, and monitoring your testing, you can improve the quality and accuracy of your data analytics and ensure that your DataOps processes are delivering the desired outcomes.
Dataops
--
--
Follow
Written by
AI & Insights
683 Followers
·
475 Following
Journey into the Future: Exploring the Intersection of Tech and Society
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Why data quality is hard. Delivering high-quality data is hard… | by Lucas de Vries | Medium,"Why data quality is hard. Delivering high-quality data is hard… | by Lucas de Vries | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Why data quality is hard
Lucas de Vries
·
Follow
6 min read
·
Nov 30, 2023
--
Listen
Share
Delivering high-quality data is hard. Data teams know this. They experience it everyday. Yet, it’s also difficult to explain specifically the reasons that make it so to non-data people.
In this article, we deep dive into why data quality is so hard (and what to do about it). A lot of the ideas expand from
this great article
from
Ari
published on the
Airbyte
blog. It explores why quality is harder (and less mature) in the data world than in the software world. If you’re interested in data quality, make sure you read it as well.
(or if you do not like to read, I actually made a talk about this topic. It’s 15min long and
accessible here
)
What is data quality?
Data quality is over-used across the data community (and among vendors). Yet, it remains loosely defined. If you ask around, you’ll soon notice that people mean different things behind the generic “data quality” term.
Data quality is the combination of code quality and the underlying data quality.
Code quality is the quality of transformations and modelisation applied to raw data. Those transformations can be done in SQL (using dbt for example) or in Python. As any code written by humans (or chatGPT nowadays), it can be subject errors. And those errors introduce bugs. For example poorly joined tables can introduce duplicates.
Underlying data quality is the quality of the actual raw data ingested in your warehouse. This data is created by external systems like a production database or a CRM. And in those system the data can be of poor quality. By poor quality we mean either of poor ‘technical’ quality (corrupted format, duplicates) or not matching business expectations (a payment should be a positive dollar amount or a person’s age should be a number expected around 1 and 99).
Hence, the generic expression “data quality” hides two different and individually complex quality components. And the fun part is that they are not 100% isolated. Their interaction adds to the complexity.
Let’s deep dive into 4 reasons why data quality is hard.
What makes data quality hard?
#1 — Detecting data issues is hard
The first step in achieving data quality involves detecting data issues. This often starts with static tests.
Below is an example of static tests implemented in dbt.
However, static testing has a limitation — it can only identify issues that are already known. How can you write a test for something you do not yet expect or know? You can’t.
This is where monitoring and anomaly detection come into play as a potential solution to uncover unforeseen issues. Yet, implementing effective monitoring is no easy task; it can quickly lead to a flood of alerts, leaving you to grapple with the challenge of deciphering and addressing issues with little clarity on where to start. Or which one is really important.
#2 — Understanding data issues is hard
The second step is actually understanding the issue the detection system caught.
Understanding data issues is a challenging task, given that data quality is intertwined with both code quality and the inherent characteristics of the data. The investigative process requires a thorough examination of both the data and the code.
Below is an illustrative workflow when a dbt test is failing.
What adds an interesting twist to the challenge is that if the issue stems from the data itself, there’s a possibility that the data has changed since the error occurred. This situation will prompt you to embark on a time-travel journey across tables, checking and comprehending the state of the data before the error manifested.
Below is a sample SQL query to use the time-travel feature on BigQuery. However, it only works for the past seven days.
Alternatively, you might find yourself yearning for a versioned approach to data, akin to the version control offered by Git. It is starting to get real for data lake with
LakeFS
or
Project Nessie
. (or with
Datadrift
for your data warehouse)
Furthermore, delving into the root cause of a data issue entails drilling down through your data lineage to scrutinize the sequence of transformations and how the data flows across your tables.
Below is a lineage illustration from a dbt project using the dbt doc UI.
#3 — Fixing data issues is hard
The third component of data quality is fixing the actual issue. As you’ll probably have understood by now the issue can either come from the data itself or the code (or both).
When the root of the issue originates from upstream systems, systems not under the purview of the data team, the fix cannot be done immediately. Rectifying such issues necessitates reaching out to the system owner for resolution. This increases the Median Time to Resolution (MTTR).
Addressing the problem involves manual intervention to effect the necessary fixes. Subsequently, the data must be reprocessed across the entire pipeline to ensure the propagation of corrections. This adds an additional layer of complexity, especially when updates are normally incremental (this is called backfilling — a topic that requires an article on its own)
#4 — Reducing data issues is hard
The final aspect of ensuring data quality involves minimizing the frequency of issues. That’s the holy grail for data teams — but rest assured it will never 0 (only rookie developers expect their app to be bug-free).
One approach is to analyze patterns of errors, enabling a deeper understanding of how to address them. For instance, if a significant percentage of issues originate from a specific system, such as Salesforce, advocating for direct implementation changes with the system owners becomes a targeted solution. However, this process is inherently time-consuming.
Below is an example of how you can store your dbt tests’ failures in your warehouse.
Another significant challenge in reducing data issues lies in the intricacies of data modeling. Simplifying the SQL complexities — often resembling spaghetti code — requires dedicated engineering time and practices, both of which are valuable and not easily acquired resources.
What to do about it?
Hard problems also need solving. So, here are some tips to get started tackling your data quality issues.
Tip #1 — not all data is created equal
Not all data is of equal significance. Focus on monitoring data assets that hold the highest business-critical impact. Kickstart your efforts with a metric-first and business-critical data quality approach.
Tip #2 — focus on actionable testing & monitoring
Ensure that your testing and observability practices are not just theoretical but highly actionable. When a test fails or a anomaly occurs, a person should be accountable for treating it and should know exactly what to do.
Tip #3 — invest in continuous improvement
Continuous improvement is essential for reducing data quality issues over time. To achieve this, learn from past incidents by logging them systematically. Automation in pattern detection is key to driving continuous improvement, you should not have to deal with the same problems over and over again.. By understanding the root causes and recurring patterns, you are empowered to prevent similar issues in the future.
Get started with open source data quality
Static tests: Complement dbt tests with Great Expectations
https://www.getdbt.com/coalesce-2020/building-a-robust-data-pipeline-with-dbt-airflow-and-great-expectations
Alerting: Integrating Slack alert in Airflow for dbt-core
https://medium.com/datareply/integrating-slack-alerts-in-airflow-c9dcd155105
Observability: Monitor dbt run and test results with Elementary
https://www.elementary-data.com/post/dbt-observability-101-how-to-monitor-dbt-run-and-test-results
(and of course
Datadrift
)
If you enjoyed this article,
you can check out a talk I made about this topic
.
Data
Data Quality
Analytics
Data Engineering
Dbt
--
--
Follow
Written by
Lucas de Vries
17 Followers
·
4 Following
ex-VP Data turned freelance. Love building data tools & product. UCL grad. Paris 🇫🇷
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Advanced Data Quality Constraints using Databricks Delta Live Tables | by Shreya Sharma | Medium,"Advanced Data Quality Constraints using Databricks Delta Live Tables | by Shreya Sharma | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Advanced Data Quality Constraints using Databricks Delta Live Tables
Shreya Sharma
·
Follow
7 min read
·
Jun 19, 2023
--
1
Listen
Share
A Recap of Delta Live Tables and Medallion Architecture
Use Delta Live Tables to create your pipeline :
Delta Live Tables (DLT) are an easy-to-use framework that utilises Spark SQL or pyspark to create data processing pipelines. The transformations in these pipelines are defined by you but Databricks will handle everything else — from task orchestration and cluster management to error handling.
The Data in the Pipeline:
Medallion architecture is the recommended Databricks way to structure and process data and goes hand-in-hand with DLT — it enables the processing of data through multiple stages, providing clean, curated, and valuable insights for decision-making.
Bronze: The step where data is standardised, audit columns are added and the data is transformed into a structured format for further analysis.
Silver: The step where curated and enriched data is stored, incorporating business rules and additional context.
Gold: The step where data is aggregated, summarised, and optimised for business intelligence and reporting purposes.
The Base Pipeline
Let’s start with a simple medallion architecture created for this demonstration. We’ll focus on integrating data quality checks for only the bronze layer, but these principles can easily be applied to the silver and gold layers as well.
Bronze Layer
The bronze layer consists of the customer, product and sales tables with the relationships defined as below. To each table, we have added in the audit columns — __ingestion_time and __row_uuid.
Enforcing Data Quality
Now that our data pipeline is set, let’s discuss how to enforce the data quality within that pipeline. Integrating data quality is crucial for organisations to leverage the full potential of their data assets, make informed decisions, maintain trust, reduce costs, comply with regulations, and drive business success.
We’ll discuss two ways to enforce data quality using DLT’s CONSTRAINT functionality. Please note that to integrate any of these conditions into your DLTs, you must be using the Advanced product edition of DLT.
Integrating Constraints into DLT materialised views
Integrating Data Quality Checks within the DLT Pipeline
While the demonstrations in this article focused on using views and materialised views, a number of these tests can also be applied to streaming tables. Keep in mind though, streaming tables are assumed append-only and generally handle (and process) micro-batches at a time. This means that you should avoid any tests such as unique key id, or checking whether a certain key exists in a streaming table — these tests can be done, but on materialised views that come in your pipeline, after your streaming ingestion is done.
A final note on choosing between SQL and Python when writing your DLT pipelines — while SQL provides an easier readability and lower barrier to entry, there are currently more tools available when creating the pipelines in Python — notably the expect_all conditions to allow for multiple constraints and the ability to localise all your tests in one table and dynamically reference those constraints from your DLT tables which improves portability.
Integrating Constraints into DLT Materialised Views
For each constraint that we add in, we need to also define the name of the constraint, a boolean statement and the actions that we will take if it is violated.
Actions:
Expect: If the constraint is violated, the record is logged as a data quality issue, but it is still added to the target table and the pipeline continues to function as normal.
Expect or on Violation Drop Row : The record is not added to the target table, a data quality issue is recorded by DLT and pipeline continues to function as normal.
Expect or on Violation Fail Update: The pipeline fails when it encounters this record — it is not added to the target table.
Let’s introduce a basic data quality rule into our bronze layer:
valid-customer-ids: CustomerID IS NOT NULL — if this record fails, let’s drop the row
The bronze code to generate the bronze_customer table will be updated as follows:
CREATE OR REFRESH LIVE TABLE bronze_customer (
CONSTRAINT valid_customer_ids EXPECT (CustomerID IS NOT NULL) ON VIOLATION DROP ROW
)
COMMENT ""Bronze layer customer data""
AS
SELECT *, current_timestamp() as __ingestion_time, uuid() as __row_uuid
FROM live.raw_customer
The resulting DLT pipeline shows that one row was dropped when processing the bronze-customer table:
Alright but what about in the case when we want a table to have multiple constraints — in pyspark you would use expect all condition, but in SQL you can leverage the AND condition to ensure that all of your conditions are being met.
In the following example, we expect that for the bronze_product table, it must have both a valid ProductID and ProductDescription:
CREATE OR REFRESH LIVE TABLE bronze_product (
CONSTRAINT valid_product_ids EXPECT (ProductID IS NOT NULL AND ProductDescription IS NOT NULL) ON VIOLATION DROP ROW
)
COMMENT ""Bronze layer product data""
AS
SELECT *, current_timestamp() as __ingestion_time, uuid() as __row_uuid
FROM live.raw_product
This will generate the following DLT pipeline, where we can see three records were dropped from bronze-product:
Here is the resulting bronze_product table — we can see that neither of the ProductID or ProductDescriptions have NULL values in them:
An AND condition is generally straightforward — let’s increase the complexity of our constraints by creating a final constraint for the bronze_sales table called valid_sales_data that will flag — but not drop rows and needs to ensure the following:
Must have a valid SalesId
The SalesDate must not be in the future unless the CustomerId is 1
If the ProductId is 1 then the SalesAmount should be $100.50
The resulting code to generate bronze_product is:
CREATE OR REFRESH LIVE TABLE bronze_sales (
CONSTRAINT valid_sales_data EXPECT (((SalesID IS NOT NULL) AND ((SaleDate <= current_timestamp()) OR (CustomerId = 1)) AND (CASE WHEN (ProductID = 1) THEN CAST(100.5 AS DECIMAL(12,2)) ELSE CAST(Amount AS DECIMAL(12,2)) END IS NOT NULL)))
)
COMMENT ""Bronze layer sales data""
AS
SELECT *, current_timestamp() as __ingestion_time, uuid() as __row_uuid
FROM live.raw_sales
We can see in the DLT pipeline that although no records were dropped, there were issues with two rows in our silver table:
We can write a quick query to identify the rows that flagged against the data quality constraints:
Integrating Data Quality Checks within the DLT Pipeline
We have introduced a lot of data quality constraints that are integrated in tables, but there are certain tests that we can integrate into pipelines instead.
Check uniqueness of all primary keys in a table:
Create a materialised view to check the uniqueness of the CustomerId field in bronze_customer:
CREATE OR REFRESH LIVE TABLE unique_customer_bronze_pk(
CONSTRAINT unique_pk EXPECT (record_count = 1) ON VIOLATION FAIL UPDATE
)
AS
SELECT CustomerID, count(*) as record_count
FROM live.bronze_customer
GROUP BY CustomerId
Since there is a duplicate CustomerId in bronze_customer, the constraint will fail.
Since we added the ON VIOLATION FAIL UPDATE condition to the constraint, and the constraint fails, this means the whole DLT fails.
2. Verify the existence of primary keys across multiple tables: Create a materialised view to verify that a CustomerId from the bronze_sales table exists in the bronze_customer table
CREATE OR REFRESH LIVE TABLE valid_customer_id_for_sales(
CONSTRAINT valid_customer_id_for_sales EXPECT (customer.CustomerId IS NOT NULL) ON VIOLATION FAIL UPDATE
)
AS
SELECT sales.*, customer.CustomerId as Validated_CustomerId
FROM live.bronze_sales sales
LEFT JOIN live.bronze_customer customer
ON sales.CustomerId = customer.CustomerId
This can also be integrated into the DLT pipeline:
Final Thoughts
Integrating data quality into pipelines is essential to have meaningful, quality data that can contribute to business outcomes. I would recommend ensuring your data quality by:
Integrating constraints within tables
Integrating further “testing” materialised views throughout the pipelines
Helpful Links
Manage data quality with Delta Live Tables:
https://docs.databricks.com/delta-live-tables/expectations.html#expect-all
Databricks
Delta Live Tables
Data Quality
Data Engineering
Data Pipeline
--
--
1
Follow
Written by
Shreya Sharma
77 Followers
·
8 Following
Data Engineering @ Deloitte | AWS | Databricks Champion |
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
TEST DRIVEN DEVELOPMENT(TDD) FOR BIG DATA PROJECT | by Shakti garg | Medium,"TEST DRIVEN DEVELOPMENT(TDD) FOR BIG DATA PROJECT | by Shakti garg | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Shakti garg
·
Follow
8 min read
·
Feb 7, 2018
--
1
Listen
Share
TEST DRIVEN DEVELOPMENT(TDD) FOR BIG DATA PROJECT
TDD is the mantra today for developers to deliver Agile projects with short development cycles, frequent releases, complex systems, APIs and ecosystems. Delivering “Clean Code That Works” is the minimal expectation from a developer. This translates to breaking complex solution into multiple simple steps and making small workable commits every day so that CI/CD tools can automate tests and deploy it any time. The goal is to fail-fast and as early as possible in production cycle to provide early feedback to the developer.
As Big Data projects have matured in the past few years from silo modules to enterprise softwares and due to increasing complexity of algorithmic and data scale(3 Vs[Volume, Velocity and Veracity]), a need for short development cycles and frequent releases was felt in this domain also. So, this article is about sharing the fruits of the journey in implementing TDD principles as a developer in multiple big data projects.
Regarding the scope of discussion below, we are assuming that readers are well aware of the nuances of Big Data and its technologies like Mapreduce, Hive and Spark at minimum. Regarding TDD fundamentals, we have explained them briefly in next section. Readers can skip it if they feel confident about them.
“What is TDD? ”
Test Driven Development(TDD) is a proven set of development techniques and principles, which enable us to achieve golden goal of “Clean Code That Works”.
For Developers,
It encourages adoption of simple designs and automated test suites, in order to achieve multiple code commits of tested code that are production-ready, instead of big, buggy and untested commits.
It is a predictable way to develop. You know when you are finished, without having to worry about a long bug trail.
TDD provides two simple rules to achieve the above:
“Write new code only if an automated test has failed”
“Eliminate duplication”
These rules, in effect imply an order to the tasks of programming.
Red
— Write a little test that doesn’t work, and perhaps doesn’t even compile at first.
Green
— Make the test work quickly, committing whatever coding sins necessary in the process.
Refactor
— Eliminate all of the duplication created in merely getting the test to work.
Red/green/refactor — the TDD mantra.
This is achieved using Divide and Conquer. First, we solved the “that works” part of the problem. Then, we solved “clean code” part. This is diametrically opposite to architecture-driven development, where you solve “clean code” first, then scramble around trying to integrate into the design the things you learn as you solve the “that works” problem.
The ripples of TDD mantra reaches to the shores of client and manager also. As the solution has been broken into sequence of implementable simple steps and code frequently released, feedback is coming early and fast, which opens interactive channels for the client, managers and developers for better and clear communication.
“Challenges in using known TDD tools in Big Data Projects”
TDD for developers has been synonymous with writing xUnit test cases and suites. Then, CI/CD tools are configured to run this test suite before deployment to QA/prod environments.
But attempts to use same approach of xUnit Test cases for Big Data project came with its own set of challenges. We have tried to generalise those challenges into two broader categories.
TDD of Big Data Algorithm
The business logic of Big Data Projects is mostly data-driven. For example, one of the scenarios could be of Target Marketing. It includes creating customer segments which is a very data and computation intensive step. Once customer segments are created, we have to apply business rules to each segment and create marketing campaigns for the same. One of the business rule could be “if the total value of customer purchase in the last financial year is more than $10,000 and he has written reviews for at least 60% purchases, offer him an additional 20% discount. Otherwise if reviews are written for more than 20% purchases, offer him an additional 10% discount”
From a developer’s perspective, it has two implementable entities: first, to aggregate customer’s purchase data and social media data over multiple dimensions of time and reviews, second, to pass the aggregated data to the business rules of calculating discount. Second entity can be implemented in any general programming language like Java, Scala, etc and can be test-driven easily using suite of xUnit Tests. But for the first entity, as data volume is huge, we will have to take help of some distributed processing framework like mapreduce/hive to do analytics. If we write xUnitTest for them, it will include setting up mock data in HDFS, then executing first-entity implementation in mapreduce/hive and collecting output. Moreover for complete coverage, we have to create a suite of such xUnit Tests with all possible permutations and combinations of data points. For unit testing, if we run all those scenarios on a dev cluster, the feedback time will be huge.
For example, if we have 200 scenarios and each mapreduce/hive job takes best time of 4 minutes(which is an expected thing taking into account significant yarn container launch time), it will take around 13 hours to run unit testing suite. Phew!! If we start taking into account cluster outage and non-local mapreduce tasks, it is a developer’s nightmare.
As mapreduce/hive code is a third-party framework, we can mock the execution of those components in our test suites. But it can result in nasty surprises at QA/production side, as we are deploying untested aggregation(Analytics) logic.
2. Logging of Big Data Components
The second aspect is the debugging efforts in case of error unit testing scenario. Big Data projects have complex technology stacks and a huge number of complementary technologies and components in their ecosystem. This makes testing and debugging in a cluster a huge pain.
For instance, if we have an oozie workflow that is orchestrating a spark job and hive job, if spark job fails, the oozie job will also fail. To debug the failure, we have to first go to oozie logs, then to the oozie step log, then to launcher application logs, then to spark application log and then to the corresponding executor logs. This complexity comes due to multiple cogs into whole ecosystem facilitating distributed computing at best. Also, as it is distributed, all cogs are running in isolation and thus maintaining their own states and logs.
As a solution, we can use Log Aggregation Strategy via ELK or Splunk. But it needs lots of investment and efforts in development. It is more suited in DevOps stage. Also, it is more of a medicinal approach of debugging in cluster. Prevention (minimise production bugs and catch bugs at the earliest stage in development cycle) deserves to be paid attention to reduce medicinal cost.
“Remedy”
To enable TDD effectively in Big Data Project, we have two implement two traits in our unit-testing suites: “Short Feedback cycle” and “Fail-Fast”.
Mapreduce and most of the components of Hadoop Ecosystem are JVM based. If not, they have a Java/Scala API for sure. So, we wrote their clients and integration code in Java/Scala. This helped us in leveraging xUnit frameworks like jUnit and ScalaTest to write unit test suites and follow mantra of “
Red/green/refactor”
. This covered the TDD of code logic at client and integration level.
For all in between(analytical jobs being triggered and executed in cluster), we will be covering the approach below for each of the major tools of Big Data tech stack.
TDD For MapReduce : MRUnit
MapReduce(MR) jobs consists of Framework code + user logic(mapper and reducer). Even if we assume that there is no need to test Framework code, doing TDD on user defined mapper and reducer is difficult as they extends frameworks API classes. Although input is parameterized, output is buffered into OutputCommitter.
MRUnit is one xUnit framework which can TDD your mapreduce code. It executes your mapper and reducer and captures the output. It thus helps in assertion on data as well as verification of code logic.
For example, we can take the example of the classic wordcount problem. There is an additional requirement that we want total count of stop words whose definition will be provided from a service class.
Below is the Mapper:
, Reducer:
And service class, WordCountUtil:
Then, we can write xUnit-like test cases using MRUnit like below: (Each test-case name aptly describes its purpose)
The last unit-test(“testMapperReducerWithPowermock”) shows that we can also use powermock and other mocking frameworks in unison with MRunit to mock business layer or other utilities.
Limitation:
Cannot test custom input formats, RecordReaders and Partitioners, if implemented and integrated in MapReduce job.
TDD For MapReduce(Custom input formats and Partitioner), Hive, Oozie, Hbase : hadoop-mini-clusters
Hadoop-mini-clusters is an open source tool (
https://github.com/sakserv/hadoop-mini-clusters
) which simulate hadoop cluster in your IDE, without the need for a full blown development cluster or container orchestration.
Just add the below dependencies in your POM for project setup:
<dependency>
<groupId>com.github.sakserv</groupId>
<artifactId>hadoop-mini-clusters</artifactId>
<version>0.1.14</version>
</dependency>
<dependency>
<groupId>com.github.sakserv</groupId>
<artifactId>hadoop-mini-clusters-common</artifactId>
<version>0.1.14</version>
</dependency>
There are mini-cluster module for each of the hadoop ecosystem component.
hadoop-mini-clusters-hdfs — Mini HDFS Cluster
hadoop-mini-clusters-yarn — Mini YARN Cluster (no MR)
hadoop-mini-clusters-mapreduce — Mini MapReduce Cluster
hadoop-mini-clusters-hbase — Mini HBase Cluster
hadoop-mini-clusters-zookeeper — Curator based Local Cluster
hadoop-mini-clusters-hiveserver2 — Local HiveServer2 instance
hadoop-mini-clusters-hivemetastore — Derby backed HiveMetaStore
hadoop-mini-clusters-storm — Storm LocalCluster
hadoop-mini-clusters-kafka — Local Kafka Broker
hadoop-mini-clusters-oozie — Local Oozie Server
Below are the steps to set up your TDD environment:
Define a name for your cluster
Instantiate a new Configuration object for your cluster
Create a data directory for your cluster to use
Instantiate a new mini-cluster object
For example, for hive:
HiveLocalServer2 hiveLocalServer2 = new HiveLocalServer2.Builder()
.setHiveServer2Hostname(“localhost”)
.setHiveServer2Port(12348)
.setHiveMetastoreHostname(“localhost”)
.setHiveMetastorePort(12347)
.setHiveMetastoreDerbyDbDir(“metastore_db”)
.setHiveScratchDir(“hive_scratch_dir”)
.setHiveWarehouseDir(“warehouse_dir”)
.setHiveConf(new HiveConf())
.setZookeeperConnectionString(“localhost:12345”)
.build();
Now, to TDD a test scenario,
1. start the MiniCluster
hiveLocalServer2.start();
2. stage the data needed for the test case (set up in hdfs/hive table using same commands/process as you will do on data cluster)
3. setup the MapReduce/Hive/Oozie/Hbase job and run it
4. verify the results (Assertions)
5. stop the MiniCluster
hiveLocalServer2.stop();
TDD For Spark : Spark’s “local” deployment mode
Doing TDD in spark is very easy as it has built-in “local” deploy-mode. In this mode, it simulates the spark job execution on a cluster in a single JVM.
Spark job can be unit tested using following “local” deploy modes.
As first deploy mode, “local” run with one worker thread, there will be 1 executor only. It can be used to do functional unit testing.
After doing functional testing, we can run the code with second deploy mode, “local[K]”. For example, if we run with local[2], meaning two worker threads — which represents “minimal” parallelism. It can help in detecting bugs that only exist when we run in a distributed context.
Another major usability factor is the ease of testing the code with these deploy modes. In contrast to MRUnit which is a third party dependency and needs writing lots of boilerplate code, spark deploy modes are sweet and simple.
In Junit testcase for that spark job, you can just use “setMaster” method of sparkConf object to set deploy mode like below:
sparkConf.setMaster(“local[2]”);
This will ensure that your spark code will now run on local JVM with your test-data.
For illustration, taking again the word count example in spark.
Then the junit test for this will look like this:
Conclusion
The above mentioned tools/techniques have proved to be sufficient in covering the needs of TDD in many of my projects using Hadoop Ecosystem. This is the bare minimum which can set your TDD goals running in a Big Data Project.
Though one observation is that sometimes, the test suite becomes heavy and takes considerable time for execution. If such situation comes, we can run test classes in parallel using fork option in maven-surefire plugin (similar option is also available in other build tools).
(Thanks to Noah, Nisha, Chandu, Saurabh and Jitendra from my Thoughtworks project team for their feedback and help. )
Big Data
Tdd
Mapreduce
Hive
Spark
--
--
1
Follow
Written by
Shakti garg
8 Followers
·
8 Following
Data Architect at Expedia | Ex-Thoughtworks
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
From Application Developer to Big Data Engineer | by Chandu Kavar | Medium,"From Application Developer to Big Data Engineer | by Chandu Kavar | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Chandu Kavar
·
Follow
6 min read
·
Sep 20, 2017
--
3
Listen
Share
From Application Developer to Big Data Engineer
I started my career as an application developer mostly having worked on OOP languages, databases, REST APIs, a little client-side programming, etc. It was around that time that I heard about “Big Data” and “Hadoop” but didn’t know much in detail. My journey into Big Data started when, in the project that I was working on, we started migrating to Hadoop in order to support the large volume of data for our client. In my first month, I did a few things that most people do when they start learning Big Data —
read blogs and books about Hadoop and Map Reduce
manually setup a small standalone cluster with the help of Google (Mostly copy/paste all the commands and config)
copy/paste word count problem from the internet, solve all the errors and ran it successfully on the cluster, and be happy.
tried to understand the syntax and working of map-reduce
tried to find other similar problems like word count, but there are not many. I have tried to explain “H
ow to find top most-viewed products from large datasets
” , “T
ranspose of matrix for large data
” and
“Cumulative sum of one billion numbers”
.
playing with different configs in core-site.xml and few other XMLs. If the solution is giving some exception after config change, searched on the internet and fixed it.
The above things help me to understand Big Data to some extent, but I didn’t get the whole picture. Here are a few reasons —
continued using the earlier programming approaches in big data problems, forgetting that they mostly work for silo-computing(single machine/data in MBs).
hard to understand how Hadoop can solve the same problem that we solved using common language once data size increased. (I was wondering how Hadoop can solve most of the problem by just giving key/value pair as input.)
not able to come up with the solution of any problem when data is partitioned and stored on different machines.
ignored the downstream impact of each API and how exactly it will affect the performance and memory usage.
not able to decide which tool or framework would solve my problem efficiently. Because there are many tools and frameworks available in Hadoop ecosystem and day by day it’s increasing.
While working on big data projects, I observed a couple of things that we need to focus —
Mindset change
While solving a problem for silo-computing, we mostly focus on performant APIs, immutability, heap memory and few other things. But, for distributed computing, you also have to consider in-memory vs disk I/O, CPU usage and network bottleneck. At every stage from analysis to production deployment, we need to test and verify all above parameters for each and every small change.
Problem solving approach
Let’s say we want to do an average of 1000 numbers, we can implement it using any language and run it on a single machine. But, what if the numbers are 100 billion? We cannot efficiently solve this using the earlier approach. There are mainly two parameters that we need to consider — the volume of data and parallel execution. We need to split the data and store it on multiple machines and want to run each split in parallel to get the answer in minimum time.
The Average Problem using map reduce —
Find the average of 1 to 10 numbers. Numbers are partitioned into three machines.
As you may know, MapReduce is a two-phase process: Map phase and Reduce phase. In map phase, the mapper will take any Hadoop directory as input, one line at a time as a key-value pair. The goal of the mapper is to take key-value as the input, process it and emit the other key-value pair. Between the map and reduce phase, all the values emitted by mapper are grouped together according to the key. Reduce phase will take one key and all associated values at a time and process it and emit the other key-value pair.
Data flow diagram to calculate average of 1 to 10 numbers
In the above diagram, all mappers are run in parallel and generate key as a number of elements and value as a sum of all the elements. Then, all the grouped values will be consumed by the reduce task and emits key as zero and value as a tuple of a number of element and its total respectively. The emitted pairs will pass to one more mapper and it will give the average of one to ten number.
Some points to remember:
Unlearn app dev programming approach
While implementing the solution of any problem for a large volume of data, you have to make it run in parallel.
Don’t think about the logic that will directly give you the final result. Break the problems into parts and finalize the intermediate outputs ( and make sure all the splits run in parallel). In the average problem, in step 1, we do a sum of numbers for all the split in parallel and the result will go for further processing in step 2 and 3.
Minimum network I/O
Try thinking how one can solve problems like cumulative sum, the transpose of a matrix, top N most frequently used words etc. in a distributed fashion.
Do the impact analysis of the API that you are going to use
The framework like Spark provides higher level abstraction such as RDD (Resilient Distributed Datasets). When you do any operation on RDD, behind the scene it will execute on each of the split available on a different machine. That’s why it is more important to understand the internal working of each API that you are going use for any transformation. For example, word count can be computed using either reduceByKey or groupByKey. Both will give the correct answer, but reduceByKey is much better on a large volume of data.
Try new tools or frameworks on an environment that is close to a copy of the production
When I started exploring tools in Big Data space, I was confused because you will find many tools or frameworks to fulfill your single requirement. Each of them has its own set of pros and cons. Here is a snapshot of how the big data landscape looks like —
Some frameworks give more flexibility but don’t perform well on very large volume of data. Some tools are good for iterative processing and some tools are not. Some frameworks are good in writing the data and some frameworks are good in querying the data. Understand your use cases, a volume of data that you want to process by that framework and the performance. Before finalizing any tool or framework, I would suggest to spike out the framework on a test environment and measure the performance on the production-like environment to get a better understanding of the framework on a large data set. If everything goes right, use it in production!
Thanks for reading. If you found this blog helpful, please recommend it and share it. Follow me for more articles on Big Data.
Thanks to Surabhi Seth, Shakti Garg, Himanshu Khantwal and Noah Pereira who reviewed this blog. I truly appreciate it.
Big Data
Hadoop
Mapreduce
Problem Solving
Become A Data Engineer
--
--
3
Follow
Written by
Chandu Kavar
399 Followers
·
28 Following
Data Engineer at Grab | Ex-ThoughtWorker
Follow
Responses (
3
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality in Snowflake Using Soda | by Iamjaswanth | Medium,"Data Quality in Snowflake Using Soda | by Iamjaswanth | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Data Quality in Snowflake Using Soda
Iamjaswanth
·
Follow
3 min read
·
Sep 18, 2024
--
Share
Data quality testing is essential to ensure that the data in your Snowflake environment is accurate, reliable, and suitable for analysis. This post will guide you through how to use
Soda
— an open-source tool for data quality monitoring and testing — specifically with Snowflake.
What is Soda?
Soda is a platform designed for data observability and monitoring, ensuring that data is consistent, complete, and compliant with defined rules. It provides data engineers and analysts with insights about the health and quality of their data across various databases, including Snowflake.
Why Use Soda for Data Quality Testing in Snowflake?
Automated Testing
: Soda allows you to automatically detect data issues such as missing values, schema changes, duplicates, and more.
Flexible Data Quality Rules
: With Soda, you can define your own data quality rules that are tailored to the needs of your business.
Seamless Snowflake Integration
: Soda integrates easily with Snowflake, allowing you to query and monitor Snowflake tables directly from Soda.
Setting Up Soda with Snowflake
Step 1: Install Soda Core
First, you need to install Soda Core to start running data quality tests. You can install Soda Core using Python’s package manager,
pip
:
mkdir soda_sip
cd soda_sip…
--
--
Follow
Written by
Iamjaswanth
51 Followers
·
142 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Lake And Quality Assurance. What is Data Lake? | by Usman Zahid | Analytics Vidhya | Medium,"Data Lake And Quality Assurance. What is Data Lake? | by Usman Zahid | Analytics Vidhya | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Data Lake And Quality Assurance
Usman Zahid
·
Follow
Published in
Analytics Vidhya
·
4 min read
·
Mar 16, 2020
--
Listen
Share
What is Data Lake?
A data lake is a centralized repository of data that allows you to store all your structured and unstructured data at any scale. You can store your data as-is, without having to first structure the data, and run different types of analytics from dashboards and visualizations to big data processing, real-time analytics, and machine learning to guide better decisions.
Why Data Lake is different from the Data Warehousing?
Based on some characteristics, we can differentiate the DL and DW as follow:
Data
Data Warehouse:
Relational from transactional systems, operational databases, and line of business applications
Data Lake:
Non-relational and relational from IoT devices, web sites, mobile apps, social media, and corporate applications
Schema
Data Warehouse:
Designed prior to the DW implementation (schema-on-write)
Data Lake:
Written at the time of analysis (schema-on-read)
Price/Performance
Data Warehouse:
Fastest query results using higher cost storage
Data Lake:
Query results getting faster using low-cost storage
Data Quality
Data Warehouse:
Highly curated data that serves as the central version of the truth
Data Lake:
Any data that may or may not be curated (ie. raw data)
Users
Data Warehouse:
Business analysts
Data Lake:
Data scientists, Data developers, and Business analysts (using curated data)
Analytics
Data Warehouse:
Batch reporting, BI and visualizations
Data Lake:
Machine Learning, Predictive analytics, data discovery and profiling
Benefits to building Data Lake:
Enables “productionizing” advanced analytics
Cost-effective scalability and flexibility
Derives value from unlimited data types (including raw data)
Reduces long-term cost of ownership across the entire spectrum of data use
Risk upon building Data Lake:
Loss of trust
Loss of relevance and momentum
Increased risk
Long-term excessive cost
What could be the reference architecture of any Data Lake solutions?
Today’s data lake is comprised of:
Landing Zone
Standardization Zone
Curated Zone/Analytics Sandbox
Any Additional Zone (If required)
Quality Assurance in Data Lake
In Big Data projects, the quality of data is more concerned. QA’s role in Data Lake is radically different than that of other streams of work. QA engineers have to unlike in other streams build counter systems that validate if raw data and any aggregated data are being folded correctly.
Simple count checks and mismatches.
Any other calculations/computes being done correctly — if you run against raw vs aggregated data for example.
Missing data — if the source of ingestion has some new timestamped data available is it available in the application’s data source.
Data Lake ad-hoc querying and web applications are radically different than regular queries, e.g. most big data queries often gather data from columnar storage because they attempt to do projections on a few columns and get massive datasets on a date range with some aggregations.
A corollary on the above point — often a lot of DL queries are interactive so testing has to be on WebSockets, and so it needs a lot of involved tooling around the use case that is flexible (again engineering skills here).
Knowledge of how histograms and some basic statistics work.
Simulation of the dataset — knowledge of perhaps how to do monte Carlo simulation, to produce an artificial dataset for smoke testing.
Streaming — if data is streaming data, knowing what is a sample (know reservoir sampling).
So we are as a QA Engineer need to make sure the accuracy of the following:
Data
Processes
Performance
Infra
Data Validation
Data quality tests include the validity of completeness, duplication, and accuracy after executing the ETL process.
Validation of Catalog and Data Lineage
What to test for Data accuracy?
Count Validation
Aggregations if required
Apple to Apple comparison
Data Transformation
CDC
Meta-Data Validation
Technical Metadata
Operational Metadata
Process Validation
The integrity of the End to End Process
Type Conversion(Parquet, Avro)
Cataloging
Transformation
Process Orchestration Validation
Hop to Hop job validation
ETL job-status via Data lineage
Retry Mechanism
Audit Trail
Performance Testing
Volume testing is must require to analyze the ETL capacity with a large number of data.
Volume test should be run as per given SLAs
Consolidated performance reports for different data sets
Infra Validation
DevOps Validation
Infrastructure Stack Constraints
Service Spin-Up
Service Level Configuration Change
What tests could be automated?
The integrity of the end to end process
Perform hop to hop validations
Count Validation
Data Validation
Catalog Validation
Validation of Transformation Logics in the Curated/Analytics sandbox zone
Infrastructure Validation
Audit, logs, and monitoring
Conclusion
So being a QA engineer in Data Lake space is not a test engineer, she/he is a software development engineer who is capable of understanding functional programming constructs, know technologies currently available at hand like Spark, HDFS, parquet, Kafka, Pandas for example; or at the very least capable of knowing and writing code in “a” particular language (could be scala, python or whatever is the language of the current day) and write automated testing templates for above scenarios easily.
Data Lake
Qa Testing
Data Science
QA
Big Data
--
--
Published in
Analytics Vidhya
71K Followers
·
Last published
Oct 15, 2024
Analytics Vidhya is a community of Generative AI and Data Science professionals. We are building the next-gen data science ecosystem
https://www.analyticsvidhya.com
Follow
Written by
Usman Zahid
23 Followers
·
14 Following
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Data Quality checks in ETL Processes | by Arun Karunakaran | Data Quality and Automation | Medium,"Data Quality checks in ETL Processes | by Arun Karunakaran | Data Quality and Automation | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Member-only story
Data Quality checks in ETL Processes
Arun Karunakaran
·
Follow
Published in
Data Quality and Automation
·
11 min read
·
Aug 3, 2020
--
1
Share
There is always an exponentially increase in cost is associated with finding software defects in the later stages of the development lifecycle. In data warehousing, this is compounded because of the additional business costs of using incorrect data to make critical business decisions. So the early detection of these system defects is prioritized as critical checks along with which a regular process of doing continuous testing using a CI pipeline. And can this be implemented?
Maintaining Quality in ETL projects is a very complex process because of many reasons. It depends on the complexity of the system which is in development and in use. Typically huge amounts of data are handled in an ETL pipeline or in a data orchestration systems and data often comes from multiple sources which need to be combined, transformed and then loaded to target schemas. During the initial setup of client systems in production, it will be loaded as a full load and then the scheduled load runs on an ongoing basis (daily, nightly, weekly/monthly loads) is treated as incremental/delta loads.
ETL work-flows involves all kinds of complex calculations and transformations on the data based on client needs. And how often these calculations/transformation methods remain to be the same and do they make similar patterns? some can have similarity and often it changes based on the client needs. If we can trace out all the minor to major patterns, similarities and occurring in this entire process of loading data to target schemas from the extracts, then it is…
--
--
1
Published in
Data Quality and Automation
3 Followers
·
Last published
Aug 3, 2020
This publication contains collections of topics relating to Testing, Tools, Processes, Standards, Policies, Governance and Automation in a Data Quality Engineering Platform
Follow
Written by
Arun Karunakaran
80 Followers
·
203 Following
Arun Karunakaran is a passionate CI/CD/CM, SDET & Data professional. He enjoys writing articles on Automation, DevOps, Data QA, DataEngineering, MachineLearning
Follow
Responses (
1
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
Explain Big Data testing?. To ensure that all the features of a… | by MultiTech | Medium,"Explain Big Data testing?. To ensure that all the features of a… | by MultiTech | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
Explain Big Data testing?
MultiTech
·
Follow
5 min read
·
Nov 6, 2020
--
Listen
Share
To ensure that all the features of a big data application work as intended, Big Data Testing is a testing method for a big data application. Big data testing aims to ensure that, while retaining efficiency and security, the big data system runs smoothly and error-free.
Big data is a series of massive databases that conventional computing methods can not be used to process. The testing of these datasets requires different processing instruments, methods, and frameworks. Big data refers to the creation, storage, retrieval, and analysis of information, which is remarkable in terms of volume, variety, and speed,More info go through
Big Data Hadoop Course
Tutorials Blog
Strategy for Big Data Testing
Instead of testing the individual features of the software product, Big Data testing application involves more verification of its data processing. Performance and functional testing are the keys when it comes to big data testing.
The QA engineers process terabytes of data and verify using commodity clusters. Then other supporting components in Big Data testing. As processing is very fast, it requires a high level of testing skills. Moreover, there are three types of data processing.
Testing for Big Data: Functional & Performance
Data quality is also an important factor in Hadoop testing alongside this. It is necessary to check data quality before testing the application and should be considered as part of database testing. It involves checking different features, such as conformity, accuracy, duplication, consistency, validity, completeness of data, etc.
How to test Applications for Hadoop?
A high-level overview of the stages of testing big data applications is given in the points below.
Testing for Big Data: Functional & Performance
Big Data Testing can be divided broadly into three stages.
Step 1: Validation of Data Staging
Process validation is the first stage of big-data testing, also referred to as the pre-Hadoop stage.
To ensure that correct data is pulled into the system, data from various sources such as RDBMS, weblogs, social media, etc. should be validated.
Comparison of the source data with the information pushed into the Hadoop system to ensure that it matches.
Verify that the correct data is extracted and loaded into the correct location for HDFS.
For data staging validation, tools such as Talend, Datameer, can be used.
“Step 2: Validation” MapReduce
The validation of “MapReduce” is the second step. At this stage, the tester verifies the validation of business logic on each node and then validates it after running against multiple nodes, ensuring that the code validates.
Map Process Reduction works correctly.
The rules on data aggregation or segregation are enforced on data.
The generation of key-value pairs.
Validating the data after the process of Map-Reduce.
Step 3: Phase of Output Validation
The process of output validation is the final or third stage of large data testing. Based on the requirement, the output data files are generated and ready to be moved to an EDW (Enterprise Data Warehouse) or any other system.
Third-stage activities include
The transformation rules are correctly implemented to check
Checking the integrity of the data and the successful load of data into the target system.
Compare the target data with the HDFS file system data system to check that there is no data corruption.
Testing Architecture
Hadoop processes very large data volumes and is extremely resource-intensive. To ensure the success of your large Data project, architectural testing is therefore crucial. A poorly or improperly designed system may lead to degradation of performance, and the system may fail to meet the requirements. In a Hadoop environment, at least Performance and Failover test services should perform.
Performance testing involves job completion time testing, memory utilization, data throughput, and similar metrics of the system. While the purpose of the Failover test service is to verify that data processing happens seamlessly in the event of a data node failure.
Testing for Performance
Two key actions for Big Data performance testing are as follows.
Data intake and throughout:
The tester verifies in this stage how data from different data sources can be consumed by the fast system. Testing involves identifying a different message that in a given time frame can be processed by the queue. For example, the insertion rate in a Mongo and Cassandra database includes how quickly data can be inserted into the underlying data store.
Data Processing:
It involves verifying the speed at which jobs are executed by queries or map reduction. When the underlying data store is populated within the data sets, it also involves testing the data processing in isolation. For instance, on the underlying HDFSS, running Map-Reduce jobs
Sub-Component Performance:
These systems consist of multiple components and each of these components needs to be tested in isolation. For instance, how fast the message is indexed and consumed, MapReduce jobs, the performance of queries, search, etc.
Performance Approach Testing
Big data application performance testing involves testing huge volumes of structured and unstructured data, and a specific testing approach is required to test such massive data.
Testing for Big Data: Practical & Efficiency
In this order, Performance Testing carries out as follows.
The method starts with the setting of the Big Data cluster to be tested for performance.
Identify and design matching workloads
(Custom scripts are created) Prepare individual clients
Execute the test and analyze the result (tune the component and re-execute if targets are not met)
Configuring Optimum
The Performance Testing Parameters
For performance testing, various parameters to verify are as follows.
Storage of Data:
How data is stored in different nodes
Commit logs:
How large can the commit log grow?
Concurrency:
How many threads can operate in writing and reading
Caching:
You can tune row cache settings.
Timeouts:
Connection timeout, query timeout, etc. values.
JVM Parameters:
Heap size, algorithms for GC collection, etc.
Performance reduction map:
sort, merge, etc.
The queue for messages:
Message rate, size, etc.
Needs for Test Environments
The test environment must rely on the type of application that you are testing. The test environment for large data testing should include the test environment.
A large amount of data should have enough space for storage and processing.
A cluster with distributed nodes and data should be present.
To maintain high performance, it should have minimal CPU and memory use.
Large Data Research Issues
The automation
Big data automation research needs someone who has professional knowledge. Automated tools are often not designed to deal with unforeseen issues that occur during research.
Virtualization
It is one of the main stages of research. In real-time, large data testing, virtual machine latency causes timing issues. Managing images in this technology is also a concern.
Broad Dataset
You need to check more data and do so more quickly.
Need to automate the effort for testing
The need to be able to test through multiple platforms
Challenges with Performance Testing
Diverse technology set:
Each sub-component is part of various technologies and needs isolation testing.
Unavailability of specialized tools:
End-to-end testing can not be done by a single instrument. NoSQL would not work for message queues, for instance,
Test Scripting:
The creation of test scenarios and test cases involves a high degree of scripting.
Test environment:
Because of the huge data size, it requires a specific test environment.
Monitoring Solution:
Few solutions can monitor the whole environment.
Diagnostic Approach:
To drill down the output bottleneck areas, a personalized solution needs to create.
Conclusion
I hope you conclude testing using big data. You can learn more about big data testing from
Big data online training
.
Big Data Testing
Big Data
Hadoop
Programming
Technology
--
--
Follow
Written by
MultiTech
71 Followers
·
2 Following
Big Data,ios,android,Spark
Follow
No responses yet
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
How to measure data quality. Practical guidelines for how to measure… | by Mikkel Dengsøe | Medium,"How to measure data quality. Practical guidelines for how to measure… | by Mikkel Dengsøe | Medium
Open in app
Sign up
Sign in
Write
Sign up
Sign in
How to measure data quality
Mikkel Dengsøe
·
Follow
5 min read
·
May 15, 2022
--
2
Listen
Share
Data about data quality. It doesn’t get more meta than this.
Measuring
data
is likely a better title than measuring data
quality
and I suggest combining quality with productivity and engagement metrics
.
At its core, measuring data can be boiled down to:
Key data metrics = data quality + productivity + engagement
Why measure data quality
What you can’t measure you can’t improve. Or that’s how the old saying goes. I’m not convinced it’s actually true and being overly obsessed with metrics is likely to lead you astray.
However, there are plenty of good reasons to measure data key metrics. Here are a few
Because some data is really important.
If you’re a fintech company you may be required to submit data to regulators or you use data to decide who should be allowed to sign up for your product. If you’re a SaaS company you may have some data that decides which bill end-users get at the end of the month. This is data you want to get right and if you don’t it can get really ugly (one such example is
Facebook charging advertisers too much due to overcounting
).
Because measuring data quality helps set high standards for your data team
. A Looker dashboard not being updated before 9 am when the C-level team looks at the KPIs or frequently being told about data issues by stakeholders before you identify them are both examples that reduce trust in data. Measuring data quality can help you be scientific about this and be proactive about where to improve controls.
So you know where to place your bets
. If you have a team of analytics engineers you want them to work on the most important things. Having a clear understanding of data quality can both help you point to areas that are doing well but also highlight areas where you may want to improve.
Which metrics should you use?
There are many ideas for metrics around
reducing data downtime
or
minimising errors caught by end-users
but these are not practical as metrics you can track every week. Instead, you should think about specific, measurable metrics that you can track and objectively evaluate yourself and your team against. I recommend grouping them into three categories.
Quality
: These metrics help you understand the quality and timeliness of your data and how well you’re set up for catching issues when something goes wrong.
Productivity
: Time spent improving data quality is a double edged sword and has to be balanced with also doing other strategic work. You should track how much time your team is spending on work related to data quality.
Engagement
: Often dashboards and data models get thrown over the fence without much consideration for who uses them. Engagement metrics help keep everyone responsible that what’s being created is also being used.
Image by Author
A few of my favourite ones are:
% of days data models are updated within SLA
: I like this one because it gives a clear idea of when data is ready to use. If you know your execs look at the KPI dashboard each morning at 9 am, hold yourself and your team accountable for having data ready by then by setting an SLA. There’s no hiding from this.
# data test failures per week:
I’ve previously written
about how leaving data tests failing is akin to the broken windows theory. By putting test failures in front of your team they’ll be more likely to address them and not accept dozens or hundreds of failing tests sitting around unaddressed in a Slack channel.
Weekly active users of a dashboard
:
Data people should align themselves with the value they create
. One of the best ways to do this is by keeping an eye on who uses a data product which in many cases is a dashboard. Not only does this give you visibility into if people use your work but you can also share the success with team members such as analytics engineers or product squads to show them that the upstream work they put into data is paying off.
What’s really exciting is when you start building workflows around these metrics.
Want to improve data test coverage? Make a rule that every time someone is made aware of a data issue that was not caught by a test, they should add a new test.
Want to make your data models run faster? Make a rule that every time someone pushes code to Github, you automatically check if it impacts the run time of the data model and raise an alert if it does.
You could also schedule weekly send-out of the metrics to make it competitive and hold people accountable and celebrate success by showing how much you’ve improved.
Segmenting is key
If you look at data metrics across different segments it becomes much more interesting and actionable. The following segments are particularly interesting:
Image by Author
Team/squad:
If you’re part of a larger data team it’s important to break data quality down by team to understand how each one is doing. Similarly, if you have a decentralised setup with many product squads, you should try getting the data producers to take joint ownership of the data quality by sharing key metrics with them.
Criticality:
Not all data should be treated the same. An error on a data model that’s only used by you and a few close colleagues may have a very different impact than an error in your top level KPI dashboard or in a data service that powers a production level ML system. Most teams have a way of knowing about this, for example by tagging data models as “tier 1”, “critical” or “gold standard”.
Time:
You want to be able to understand if you’re improving or deteriorating data quality over time and how this looks across teams and squads.
End-users:
In the same way that you segment data models by criticality, you can segment end-users. For example, you could have a filter on C-suite so you can see which dashboards they use and if they’ve been using wrong data. Although everyone should expect great data quality, it’s often more important to pay extra attention to data that has a lot of senior people using it.
How to get started
Start with a few metrics and stick with them. Optimise for data that’s accessible and avoid creating too much manual work by e.g. having people input time spent on solving data issues in a given week. This will be too subjective and your team will get tired of it.
Be consistent and have a regular schedule of looking at the metrics. Maybe you review them in your bi-weekly data team meeting or schedule a send-out on Slack every Friday. Whichever way you go, get it in front of people regularly so it’s not just another shining object that doesn’t get used.
If you have some stories or ideas around how to measure data quality from your company, I would love to hear from you. You can reach me
here
.
Data
Data Engineering
Analytics Engineering
Data Science
Data Leadership
--
--
2
Follow
Written by
Mikkel Dengsøe
4.2K Followers
·
21 Following
Co-founder, Synq (
www.synq.io
). Reach me here:
https://www.linkedin.com/in/mikkeldengsoe/
Follow
Responses (
2
)
See all responses
Help
Status
About
Careers
Press
Blog
Privacy
Terms
Text to speech
Teams"
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
"Testing Tableau - ""I like Big Data and I Cannot Lie""","Testing Tableau - ""I like Big Data and I Cannot Lie""
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Testing Tableau - ""I like Big Data and I Cannot Lie""
Report this article
Sarah Richey
Sarah Richey
Senior Technical Program Manager - same project great new vendor!
Published Jan 22, 2016
+ Follow
Is it just me, or do all testers apply ""testing skills"" to daily living?
How I do anything is how I do everything
. While sitting next to a software developer one day (Agile Testing approach) I asked him ""Why do you think I'm still single?"" With his back to me, head tilted down, both hands tapping away on the keyboard, he said ""Perhaps you take your work home with you"". He didn't even stop typing as he spoke, it just rolled off his subconscious developer’s brain as if he was tweaking the parameters of my code. It took me a minute to understand what he was saying. After a short pause, the developer realized what he'd just said, he froze, popped his head up quickly and turned to me with a big smile ""You know, raising CRITICAL issues? Nobody likes that! ""
I've taken a break from the software development and test industry for 2 years. I wanted to experience a right brain job for a while. Actually, I just wanted to ride my Harley around all day and not work, however; epic road tripping is expensive. So, I choose Uber driving. UBER driving is like neighborhood ride sharing, I treat everyone like they are my neighbor, all over Seattle. People pay Uber and I to transport them around Seattle. Well, guess what, I started collecting data on every person in my car. (A tester’s soul through and through) ""Where do you work?"", ""What do you do there?"", ""Do you like your job?"", ""How do you like working for them""?  Especially the people in the technology industry. Seattle's a big tech town. I drive around all night long 7 days a week. I drive people from Amazon, Google, Microsoft, Adobe, Tableau, F5, Bigfish, and startup after startup after startup. And we chat. I query them just like I'm testing their software. What's even better, I drive at night (remember, day time is for Harley riding) and a lot of them are slightly tipsy from a good night out. Slightly more un-inhibited than when dead sober. Here's an example of un-inhibited techie conversation. Last night I had two of the funniest Microsoftee's ever, listening into their seriously inebriated conversations after midnight was hysterical, diabolical and a mind conundrum to follow.
""You're fired!""
said the guy behind me. ""
You can't fire me, I'm your boss!""
said the other guy laughing and laughing and laughing.
""I was demoted, wasn't I.""
said curly mop-top guy (I give them all nicknames).
""Yup, they didn't even  skip a breath or take a beat
(he meant to say, ""skip a beat"", ""take a breath"") ,
not even half a blink when they noticed you weren't in the meeting
"" said Fuzz-face.
""Well, he can't live without you, and just keep saying NO!""
replied Mop-top. Fuzz-face says
""He takes me to every meeting, and all I do is turn to him and say no, no don't do that. No it’s not o.k. No, no, no. I'm going to just keep saying no, that's what I'm paid for, to stop him from doing what he's paid to do.""
They Laugh and laugh and laugh more. I sure enjoyed that ride. I just sat quietly as I drove, didn't say a word.
Some of them ask me questions too, and I tell them my saga of Testing for Microsoft in the 90's, my one claim to fame in Visual Basic 1.0 and Access 1.0, how I had the idea of stripping all the hard coding from the 10,000 Altar Basic automated test cases and replacing them with consonants. We would then only have to localize resource files into French, German and Italian to automatically test all the different languages of the software.  That effort launched me into leadership at Microsoft with both Visual Basic 1.0 and Access 1.0, a star was born.
A side note. It's dark at night, I never see the faces of my Uber passengers in the back seat, but I hear them, or I see a shadow of a head pop up in the rear view mirror when they are intrigued, or I note an inflection in their reply. Uber driving has greatly improved my listening skills because I'm watching the road and not the person's body language. I've learned to ""listen"" for tone inflections or pauses to determine their interest, instead of facial expressions.
My ""saga"" as a pioneer of automated testing in the 90's tends to get their attention. I can almost hear the wheels of their mind ready to ask that next question ""How do you go from top of your industry CEO/ Managing Director of Test to Uber driving?"". There is always a lonnnnnng pause before they ask this, I know they are attempting to formulate the question tastefully without sounding judgmental. (Another blog, for another day)
With the mutual respect of each other established, our conversations escalate back into the current roles at where they work. I love hearing about what they are doing in their roles, their saga and claim to fame. Surprising to me, is most people have no ""claim to fame"" in their jobs. I find that sad, shouldn't everyone feel like a rock star at work?
By now you are thinking, ""What does this have to do with Testing Tableau?"" let's time warp back to the present with Tableau. People that work at Tableau that ride in my car, love working there. Ok, one young girl said it was ""o.k."" but she qualified it with, ""my boyfriend LOVES it there"".
I even had a few people from Microsoft say ""Several of my friends have moved to Tableau and they love it there!""
So I started evaluating Tableau as a possible ""next step"" for me since I love big data and metrics - based decision making was my secret weapon on projects.
I downloaded the trial version. Put my software tester hat on and got to work. I found it was easy to use, easy to understand, easy to link to data source, in fact it was too easy for me. I took the Advanced Server Live course online. Wow, that was easy too. Why is ""advanced"" so easy? Where's the challenge? Then it dawned on me. The TOOLS are easy, the challenge is figuring out my story for the data to tell. Aaah, this is where thinking comes in. What story is my data telling?
I like big data, I love that everything out there has a growing data source. I started simple. I used Tableau to re-think my resume. I'm going to dump my old fashioned word document. And use Tableau and their story board from now on. I'm going to create a website with all my stories like this one. Chart my results from Uber driving on the website (Google, Microsoft, Tableau, F5, Amazon, etc.) I'm going to USE the software of the company I hope to work for one day. (Because all my Uber driving data says they are a great company to work for) And here’s my first effort. It was so easy to do, so simple, my next effort will be to embed a tableau story onto a website and continue to update the results of my Uber driving queries. Who is the best tech company to work for, and why, in Seattle. I think I'll put a poll on the website that updates in a Tableau embedded storyboard. I'll also note if I think the candidates are slightly inebriated or high, clearly that skews the reality of the data.
P.S. I'm still single, I think I'll start collecting data on that too, that's big data, decades of data! I want to see what story unfolds with this topic!
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
22
7 Comments
jos vermeulen
Computer & Network Security Professional
8y
Report this comment
searched ""i hate not open stuff""    ps: this is pretty closed / won't get much exposure
Like
Reply
1 Reaction
Steve Fawthrop
New Client Sales l Sales Mngt.l Acct. Mngt. l Blogger-Editor
8y
Report this comment
Sarah, nicely done from the heart.
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Interviewing with Camera's Off?
Mar 5, 2021
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Design Data Quality Test Cases: A Guide,"How to Design Data Quality Test Cases: A Guide
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Last updated on
      Sep 19, 2024
All
Data Quality
How can you design data quality test cases?
Powered by AI and the LinkedIn community
1
Define the data quality dimensions
2
Identify the data quality requirements
3
Design the data quality test cases
4
Execute the data quality test cases
5
Analyze the data quality test results
6
Review and refine the data quality test cases
7
Here’s what else to consider
Data quality is the degree to which data meets the expectations and requirements of its users and consumers. Poor data quality can lead to inaccurate, unreliable, and inconsistent results, affecting decision making, business performance, and customer satisfaction. Therefore, data quality testing and validation are essential steps in any data project or pipeline, to ensure that the data is fit for its intended purpose and conforms to the defined standards and rules. In this article, you will learn how to design data quality test cases, which are specific scenarios or checks that verify and measure the quality of data.
Top experts in this article
Selected by the community from 27 contributions.
Learn more
Amit Sharma
QA Manager / Data quality Architect
View contribution
19
Jean-Georges Perrin
Architect @ Expedia | Technologist & Innovator | Author | Co-founder AIDA User Group | Lifetime IBM Champion | Chair…
View contribution
5
Nidhi Rai
Senior QA Engineer @Workhuman | Managing team | Selenium | Python| Web UI Testing | API testing | Data warehouse…
View contribution
3
See what others are saying
1
Define the data quality dimensions
The first step in designing data quality test cases is to define the data quality dimensions that are relevant and important for your data. Data quality dimensions are the aspects or characteristics of data that determine its quality, such as accuracy, completeness, consistency, timeliness, validity, and uniqueness. Depending on the nature and context of your data, you may need to prioritize some dimensions over others, or add custom dimensions that suit your specific needs. For example, if you are dealing with customer data, you may want to focus on accuracy, completeness, and uniqueness, while if you are dealing with transactional data, you may want to focus on timeliness, validity, and consistency.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Nidhi Rai
Senior QA Engineer @Workhuman | Managing team | Selenium | Python| Web UI Testing | API testing | Data warehouse Testing | Business Intelligence Expert
Copy link to contribution
Report contribution
Among all these checks, it’s essential to validate the presence of documentation. This not only facilitates the sharing of domain expertise between teams but also ensures the maintainability of documents with each change. Prioritizing document maintainability will make future references more reliable.
…see more
Like
3
Gerald Tesha
Data Officer @ HETA | Data Visualization | Monitoring and Evaluation
Copy link to contribution
Report contribution
Picture it like setting criteria for a painting—identify dimensions like accuracy, completeness, consistency, reliability, and timeliness. These are the standards against which data quality is measured.
…see more
Like
2
Alexander Kell
Co-Founder rapiddweller | DATAMIMIC Champion & Test Data Lover | Entrepreneur, Explorer & Mechanic
Copy link to contribution
Report contribution
Designing effective data quality test cases is essential for maintaining high standards and ensuring your data is reliable. To start, it’s important to define key data quality dimensions such as accuracy, completeness, and consistency. Next, identify clear and measurable requirements for each of these dimensions. 

Once you have the requirements in place, design test cases that are repeatable and scalable, ensuring they can validate the data thoroughly. Tools like DATAMIMIC can help execute these test cases using real or anonymized data. After execution, analyze the results to identify any issues and understand their root causes. Finally, regularly review and refine your test cases to adapt to changing data needs.
…see more
Like
2
Nick B.
Test Automation Architect
Copy link to contribution
Report contribution
The most commonly used DQ dimensions are completeness (is there any missing data?) accuracy (are there any human error problems like misspelling, empty fields, wrong selections?) consistency (we have age but date of birth was entered) validity (does it use the right schema and version?), uniqueness (are there duplicates anywhere?) integrity (does one record reference another in the right way?)
…see more
Like
1
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Defining data quality dimensions is a crucial step in developing effective data quality test cases. It is essential to understand that these dimensions are not one-size-fits-all, but should be tailored to the specific context and requirements of the data at hand. In regulatory or compliance-driven environments, dimensions such as validity and accuracy may take precedence, while in dynamic, customer-facing applications, timeliness, and completeness may be more crucial. Furthermore, it is important to consider the interplay between these dimensions - how improving one can impact another. For example, enhancing completeness could be detrimental to consistency. Understanding these dynamics helps in creating a more holistic data quality strategy
…see more
Like
Wessam Abu Regeila
Architecture& Strategy |  solution Architecture| Data Management | Data Security | Sustainability| Data Governance| DAMA
Copy link to contribution
Report contribution
Data quality dimensions are like picky houseguests:
 Never misspells your name...but rearranges all the furniture.
 Brings the perfect outfit...lacks shoes.
Shows up for dinner on time...every year of your life.
…see more
Like
2
Identify the data quality requirements
The next step in designing data quality test cases is to identify the data quality requirements that specify the expected or desired level of quality for each dimension. Data quality requirements are the criteria or rules that define how the data should look like, behave, or perform, based on the business objectives, user expectations, and data standards. For example, a data quality requirement for accuracy could be that the customer name and address match the records in the source system, or a data quality requirement for timeliness could be that the data is updated within 24 hours of the transaction. Data quality requirements should be clear, measurable, and testable, and should be aligned with the data quality dimensions.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Jean-Georges Perrin
Architect @ Expedia | Technologist & Innovator | Author | Co-founder AIDA User Group | Lifetime IBM Champion | Chair Bitol | Opinions are mine
Copy link to contribution
Report contribution
Remember that the data quality requirements come from both the technical (engineering) world and the business world, which understands the data.
…see more
Like
2
Wessam Abu Regeila
Architecture& Strategy |  solution Architecture| Data Management | Data Security | Sustainability| Data Governance| DAMA
Copy link to contribution
Report contribution
Think of data quality requirements as a client with unreasonable standards ""This data better be accurate, like it reads my mind and fixes my typos accurate.""
""No missing pieces! I need the whole picture, down to the tiniest pixel.""
""This data needs to be so up-to-date I smell the future...and it smells profit-scented.""
…see more
Like
2
Gerald Tesha
Data Officer @ HETA | Data Visualization | Monitoring and Evaluation
Copy link to contribution
Report contribution
It's akin to specifying the colors needed for your artwork. Outline specific requirements for each dimension. For accuracy, define how close the data should be to reality; for completeness, specify the expected coverage.
…see more
Like
1
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Identifying specific, achievable data quality requirements is a crucial step in ensuring that your data meets both business and user expectations. It's like establishing the GPS for your data journey - you need to know the destination and the route. These requirements are as benchmarks against which data quality can be evaluated and improved. It is crucial to involve stakeholders from different sectors (such as IT, business users, and data governance teams) in this process to ensure that these requirements are comprehensive and aligned with the overall business objectives. Remember that clarity and measurability are your allies here - vague requirements lead to ambiguous outcomes.
…see more
Like
1
3
Design the data quality test cases
The final step in designing data quality test cases is to design the specific scenarios or checks that will verify and measure the data quality requirements. Data quality test cases are the executable units of data quality testing and validation, that compare the actual data with the expected data, and report any deviations, errors, or issues. Data quality test cases should cover all the data quality dimensions and requirements, and should be designed in a way that is repeatable, scalable, and automated. For example, a data quality test case for completeness could be to count the number of null or missing values in each column, or a data quality test case for validity could be to check if the data values conform to the predefined format or range.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Gerald Tesha
Data Officer @ HETA | Data Visualization | Monitoring and Evaluation
Copy link to contribution
Report contribution
Craft your tests like an artist choosing brush strokes. Develop scenarios that check if your data meets the defined requirements. If completeness is a dimension, a test case might involve ensuring all necessary fields are filled.
…see more
Like
2
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Designing specific and effective data quality test cases is the key component of a robust data quality strategy. It's like setting up checkpoints in a race to ensure that everyone is on the right track. These test cases are practical tools that enhance your data quality and requirements. They must be comprehensive, covering all aspects of your data, and efficiently designed for efficiency and accuracy. Automation is essential here; it ensures consistency and saves valuable resources. Remember, the goal is not only to discover errors but to understand their root causes for long-term improvement.
…see more
Like
2
Nikhil Wanpal
Founder & CEO Upshot Ideas/Dragonfly Data, Software Craftsperson
Copy link to contribution
Report contribution
The test cases can generally be categorised into:
1. schema-related checks: data types, nulls, units, lengths.
2. accuracy checks: thresholds/bounds of the data, trend deviation, anomaly detection.
3. recency checks: the data lands at the required frequency.
4. Integrity checks: gaps, data reconciles properly, relationships are maintained etc.

And finally, the business-specific or rules checks:  You need SMEs to define these, and the rules tend to change as the behavior of the system changes.
…see more
Like
1
Wessam Abu Regeila
Architecture& Strategy |  solution Architecture| Data Management | Data Security | Sustainability| Data Governance| DAMA
Copy link to contribution
Report contribution
Data quality test cases are like putting your data on the witness stand ""Are you accurate, or just trying to make me look bad?""
""Spill the beans! Where are you hiding those missing values?""
""Your story changes every time I look at you – what's your consistency alibi?""
…see more
Like
1
Olaoluwakiitan Olabiyi replied:
An interesting analogy
4
Execute the data quality test cases
Once you have designed the data quality test cases, you need to execute them on the data set or source that you want to test and validate. Depending on the size and complexity of your data, you may need to use different tools or methods to execute the data quality test cases, such as SQL queries, scripts, frameworks, or software. The execution of the data quality test cases should produce a report or a dashboard that summarizes the results and outcomes of the testing and validation process, such as the number of passed or failed tests, the data quality score or metric, and the data quality issues or defects.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Wessam Abu Regeila
Architecture& Strategy |  solution Architecture| Data Management | Data Security | Sustainability| Data Governance| DAMA
Copy link to contribution
Report contribution
Think of test case execution like a surprise pop quiz for your data ""Thought it could pull a fast one, huh data? Surprise test! No bluffing on the accuracy answers.""
""Data didn't see those completeness drills coming – bet it's sweating on those open-ended questions.""
""The grading rubric is harsh. Errors might scrape by, but good data gets those quality gold stars.""
…see more
Like
2
Gerald Tesha
Data Officer @ HETA | Data Visualization | Monitoring and Evaluation
Copy link to contribution
Report contribution
Think of it like applying paint to a canvas. Run your test cases against your data, just as an artist applies different techniques to see if the colors blend harmoniously.
…see more
Like
1
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Execution of data quality test cases is where the rubber meets the road. It's crucial to use the right tools and methods that match the complexity and volume of your data. Whether it's SQL queries for database checks or specialized software for more intricate analyses, the key is precision and thoroughness. The outcome, ideally presented in a clear and actionable report or dashboard, not only highlights the areas of success and concern but also serves as a roadmap for continuous improvement. This step is not a one-time event but an ongoing process to ensure sustained data integrity.
…see more
Like
1
5
Analyze the data quality test results
The last step in the data quality testing and validation process is to analyze the data quality test results and identify the root causes and impacts of the data quality issues or defects. The analysis of the data quality test results should help you understand the current state and level of your data quality, and the areas or aspects that need improvement or correction. You should also evaluate the effectiveness and efficiency of your data quality test cases, and determine if they are sufficient, relevant, and accurate. The analysis of the data quality test results should also provide you with actionable insights and recommendations on how to resolve, prevent, or mitigate the data quality issues or defects, and how to improve or maintain the data quality in the future.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Analyzing data quality tests results is a crucial step in transforming data into actionable insights. It's not just about identifying what's wrong, but about identifying why it's wrong and how it affects your business. This analysis should be thorough, examining the underlying causes of data issues. It is also a time for reflection - examining the effectiveness of your test cases and their alignment with your data quality objectives. The key here is to transform findings into actionable steps to improve, ensuring that each test cycle brings you closer to higher data quality standards. Remember, this is an iterative process, a cycle of continuous refinement and enhancement.
…see more
Like
2
Merrill Albert
Enterprise Data Leader, Data Governance Officer, Data Thought Leader, Chief Data Officer, Fractional Governance, Data Evangelist, LinkedIn Top Data Governance Voice, creator of #CrimesAgainstData
Copy link to contribution
Report contribution
Identifying the root cause of the data quality issue is essential so you can resolve the issue and prevent it from happening again.  However, that root cause is not always discovered immediately, so you need to inform people using the data that there is a data quality issue.  Depending on the significance, people may have to stop their work until the problem is solved.  If it's not significant, they may choose to continue their work, but monitor to see if the data quality issue worsens or expands.  If you involve the right people, they might also be able to tell you things they've seen with the data that can help in the detective work of finding the root cause.
…see more
Like
2
Gerald Tesha
Data Officer @ HETA | Data Visualization | Monitoring and Evaluation
Copy link to contribution
Report contribution
Like evaluating a finished piece, analyze the results. Identify areas where the data meets expectations and where it falls short. This step is crucial for understanding the overall quality of your data.
…see more
Like
1
Sitraka Forler
Senior Data Scientist | NLP | Digital Transformation & Machine Learning
Copy link to contribution
Report contribution
Go beyond identifying data quality issues to understanding their underlying causes. Use Root Cause Analysis (RCA) methods like the 5 Whys or fishbone diagrams to systematically trace back each issue to its origin, providing a clearer pathway to effective solutions.
…see more
Like
6
Review and refine the data quality test cases
As a final note, you should remember that data quality testing and validation is not a one-time activity, but a continuous and iterative process. As your data changes, evolves, or grows, you may need to review and refine your data quality test cases, to ensure that they are still valid, relevant, and effective. You may also need to add new data quality test cases, to address new data quality dimensions, requirements, or scenarios. By reviewing and refining your data quality test cases regularly, you can ensure that your data quality testing and validation process is always up to date, reliable, and comprehensive.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Jean-Georges Perrin
Architect @ Expedia | Technologist & Innovator | Author | Co-founder AIDA User Group | Lifetime IBM Champion | Chair Bitol | Opinions are mine
Copy link to contribution
Report contribution
Have you heard of PDCA (Plan-Do-Check-Act)? It is at the heart of all the Agile methodologies and there is no reason it cannot be used for data engineering (and data quality).
…see more
Like
5
Gerald Tesha
Data Officer @ HETA | Data Visualization | Monitoring and Evaluation
Copy link to contribution
Report contribution
Similar to an artist refining their technique, review your test cases regularly. If you discover new requirements or patterns in data issues, refine your test cases to capture these nuances.
…see more
Like
1
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Amit Sharma
QA Manager / Data quality Architect
Copy link to contribution
Report contribution
While designing test cases break them into two buckets: 
Operational Test Cases: These test cases cover the following scenarios :
1) Data Freshness
2) Null Values
3) Duplicates
4) Data Validity 
5) Data Anomaly checks like 
The second bucket of test cases will be for:

1) Business Rules: Essentially the requirements for building the data product or the data asset
2) Referential integrity checks.

Different stages of pipelines should have DQ checks. 
For examples, checks should be at
Data Source Level
Staging Layer
Reporting Layer
…see more
Like
19
Jean-Georges Perrin
Architect @ Expedia | Technologist & Innovator | Author | Co-founder AIDA User Group | Lifetime IBM Champion | Chair Bitol | Opinions are mine
Copy link to contribution
Report contribution
Let's all put that in a data contract: data quality rules can be defined there. The tools can execute the rules. The open standard ODCS from the Linux Foundation project Bitol is here to help.
…see more
Like
2
Nick B.
Test Automation Architect
Copy link to contribution
Report contribution
Outcomes might include development of an improvement plan based on test results so that continuous improvement cycles are in effect
…see more
Like
1
Data Quality
Data Quality
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Quality
No more previous content
How can you identify and fill gaps in your data quality strategy?
58 contributions
How can you improve data quality maturity across domains and industries?
53 contributions
How do you tell stakeholders if your data is bad for machine learning?
52 contributions
How do you report data quality results to your audience?
62 contributions
How do you prioritize data quality when resources are limited?
69 contributions
How can you improve data quality documentation and metadata management?
61 contributions
How do you set data quality policies?
54 contributions
How can you establish and enforce data quality standards?
56 contributions
How can you compare data quality improvement techniques?
35 contributions
How can you build a data quality culture using a framework?
51 contributions
How do you automate data quality metrics for different platforms?
49 contributions
How are you improving your data quality?
45 contributions
How can you improve data quality when integrating multiple sources?
12 contributions
What are the best ways to transform data for different audiences?
33 contributions
How do you prepare for emerging data quality trends and challenges?
11 contributions
No more next content
See all
More relevant reading
Data Quality
How do you set data quality benchmarks?
Data Strategies
How do you identify and prioritize data quality issues and improvement opportunities?
Business Reporting
How do you explain your data quality to others?
Data Management
How do you align data quality assessment with your business goals and strategies?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
3
28 Contributions"
"Data quality testing without monitoring pipelines is like running a kitchen without inspecting ingredients. | Kevin Hu, PhD posted on the topic | LinkedIn","Data quality testing without monitoring pipelines is like running a kitchen without inspecting ingredients. | Kevin Hu, PhD posted on the topic | LinkedIn
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Kevin Hu, PhD’s Post
Kevin Hu, PhD
Chief SQL Wrangler & CEO at Metaplane | MIT research | Data-informed posts about data
5mo
Report this post
Data quality testing without monitoring pipelines is like running a kitchen without inspecting ingredients.

No matter how skilled the chef, bad ingredients ruin the dish.
#dataengineering
#dataquality
#analytics
336
22 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Eric Gonzalez
Husband & Father | Data Executive | Creator | Advising Executives on Leveraging Data for Strategic Decisions | Bridging the Gap Between Boardrooms and Tech Teams
5mo
Report this comment
+ there's only one person who understands the ingestion pipeline and won't allow others to make changes because the associated tech debt acts as job security.
Like
Reply
15 Reactions
16 Reactions
Merrill Albert
Enterprise Data Leader, Data Governance Officer, Data Thought Leader, Chief Data Officer, Fractional Governance, Data Evangelist, LinkedIn Top Data Governance Voice, creator of #CrimesAgainstData
5mo
Report this comment
That's far too clean.  You're missing all the silos and the people trying to shoot holes in the infrastructure.
Like
Reply
3 Reactions
4 Reactions
Yavin Owens
In humble beginnings and meticulous details, I strive to master data quality, transforming raw bits into powerful insights. Enthusiastic Data Quality Expert | EY |
5mo
Report this comment
Also too many cooks spoil the broth
Like
Reply
1 Reaction
2 Reactions
Ann Pickett
Director of eCommerce Fulfillment | Analytics Consulting
5mo
Report this comment
xkcd being accurate once again!
Like
Reply
1 Reaction
2 Reactions
Ronan TREILLET
Data Architect | Certified Solution Architect AWS (SAA-C03) | Freelance
5mo
Report this comment
Yes checks within your pipeline as monitoring are important, but if you have data quality issues it not you pipeline’s problem. And associating pipelines to data quality often leads to mix both concepts. Yes you should test your ingredients but if you need to put some time (and money) somewhere it usually better to spend some effort to check your provider’s process rather than checking their product once they arrive in the kitchen even if, ideally, you need to do both. Cause if have to cook for thousands people and you realize all of you meat is in fact made of plastic it’s will be a little bit late to order new ingredients.

The reality is that we often ask data engineers to solve issues they didn’t created, understand undocumented dynamics within data sources, force them to imagine complex solutions to work arounds terrible decisions people made just because it’s too late to change the provider’s process.

Data quality is the responsibility of the data source, the same way people(hopefully) monitoring code quality in their application, they should monitor data quality of data they produce. Data quality is part of the product exposed to your colleagues or to your business partners. PO should be incentivized on this metric too
Like
Reply
1 Reaction
2 Reactions
Satya Choudhury
Director, Personalization & AI/ML Architecture at Fidelity Investments
5mo
Report this comment
Not just it ruins the dish, it ruins the pan too. Applying certain data quality checks at source greatly reduces the impact on the dish as well as the pan
Like
Reply
1 Reaction
2 Reactions
Fares Hasan Alfadhli
AI & Data Science Lead
5mo
Report this comment
The funny part is that most product and top management are oblivious to the detrimental effects of that ingestion pipeline. I would say it's sad also when they are taken off guard by how a small change in the data source schema blows the engine.
Like
Reply
1 Reaction
Trygvi Zachariassen Laksafoss
Senior Manager AI&GenAI at Lundbeck
5mo
Report this comment
One huge python script pulling data from flat files and excel sheets
Like
Reply
1 Reaction
2 Reactions
Ashwini Sharma
Engineering Leader
5mo
Report this comment
Constant demand from business to build dashboards and do analytics makes it difficult to invest more time in data quality and observability. Data quality and observability should have been more popular than what it is now.
Like
Reply
1 Reaction
Sibbir Ahmmed Sihan
A Microsoft Certified Data Engineer solved problems with Azure, Data Factory, Databricks, Azure DevOps, SQL, Airlfow, Git, DAX,  Python, dbt and so on. Got an MSc in Data.
5mo
Report this comment
I don't like this visual representation.
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More Relevant Posts
Tokhirjon Orifjonov
Data Analyst at Oqtepa Lavash 🍟
2mo
Report this post
🔎 Data Storytelling in Action: Cooking Time Analysis 🍽

In my role as a Data Analyst, I recently analyzed cooking times across all our restaurants to improve service efficiency. The chart below shows the distribution of cooking times, segmented into three categories:
🍳 Less than 5 minutes (Green): These fast-prep dishes account for 63.02% of all orders.
🍔 5 to 9 minutes (Blue): Moderate-prep meals make up 24.9% of the total.
🍕 More than 9 minutes (Red): Dishes that take over 9 minutes to prepare represent 9.73% of all orders.
📈 Insights:

1. Efficiency Boost: With over 60% of meals prepared in under 5 minutes, our kitchens are performing efficiently in high-demand situations.
2. Focus Area: The remaining 10% of meals taking longer than 9 minutes presents an opportunity to optimize processes, reduce wait times, and improve customer satisfaction.
Data-Driven Decision: Based on these insights, we can explore techniques like better meal prep strategies or adjusting menus to shift more orders into the faster time brackets.
#DataAnalysis
#DataStorytelling
#RestaurantManagement
#Efficiency
#DataDriven
16
4 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Abdishakour Hersi
Head of procument and logistics at SQCC | data analyst | advanced power PI , Tablue | policy analyst
4mo
Report this post
Hi linked fam. paradise friday
---

🧑💻✨

Cooking data📊 is like preparing a meal🍛,It starts with cleaning, where you remove any mistakes or unwanted parts from the data, just like washing vegetables. Next, you mix different pieces of data together, similar to combining ingredients. After that, you shape the data into a format that’s easy to work with, like chopping and arranging your ingredients. Finally, you analyze the data to find useful information, much like tasting and serving a delicious dish. Just as a good chef makes a meal great, a skilled data analyst makes data useful and insightful.
#data_analysist
#skill_generator
31
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Vibhor Goyal
Data Engineering Lead - Consultant @ Innowave | Ex - AXAXL | IIM Rohtak
9mo
Report this post
""Imagine being a chef of a bustling restaurant. Each dish you prepare represents different aspects of your business like sales, marketing, customer service, or HR. Keeping track of the popularity of each dish, perfecting recipes, and maintaining inventory can seem like trying to stir a soup without a spoon! 🍲👨🍳📊 

As data engineers, we're your kitchen staff, turning the heaps of raw ingredients (data) into a savory broth of insights.

We create the 'Chef's Dashboard' - a real-time data monitoring tool. It allows you to efficiently manage your 'menu' of services, check which 'dish' is bringing in more customers, who needs more 'seasoning,' and ensuring your business kitchen runs smoothly.

Ready to cook up some business success? Join us in our webinar 'Restaurant of Business: Cooking Success with Data Engineering', this Wednesday at 7 PM EST.

Season your thoughts, simmer your queries, and let's get
#DataEngineering
sizzling on LinkedIn!

In the restaurant of business, data is your recipe, guiding you towards a Michelin star. Ready to dish up some success? 🍽️📈🌟
#DataCuisine
#RecipeForSuccess
#BusinessGourmet
""
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Subhosmita Ghosh
Cerifited Data Analyst | Power-BI Analyst |
5mo
Edited
Report this post
🎉 Excited to Announce the Completion of My Latest SQL Project! 🎉

I am thrilled to share that I have successfully completed a comprehensive SQL project focused on analyzing restaurant operations. 🍽️📊

In this project, I delved deep into the data to uncover valuable insights that can help restaurants optimize their operations and enhance customer experiences. Here are some key highlights of what I achieved:

🔍 Data Extraction and Cleaning: Extracted data from various sources and ensured its accuracy and consistency.

📈 Operational Analysis: Analyzed key metrics such as order processing times, peak hours, and table turnover rates.

💡 Customer Insights: Identified trends in customer preferences, spending patterns, and feedback to drive improvements.



This project has not only honed my SQL skills but also deepened my understanding of the intricacies of the restaurant industry. I am grateful for the opportunity to work on such an impactful project and look forward to applying these insights to drive operational excellence.
A big thank you to my mentors and peers for their support and guidance throughout this journey. 🙌
Shashank Singh 🇮🇳
Thoufiq Mohammed
Munna Das
Satyajit Pattnaik
project was given by
Maven Analytics
Project Link -
https://lnkd.in/dB2GesVA
#SQL
#DataAnalysis
#RestaurantIndustry
#OperationalExcellence
#DataScience
#ProjectCompletion
#Analytics
#CustomerInsights
Feel free to reach out if you’d like to discuss the project in more detail or explore potential collaborations!
GitHub - Subhosmita/Restaurant-Operation-Analysis
github.com
11
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Harsha B
Frontend Developer Trainee at Amogha Foundation, An Oorjja Ecosystem partner
9mo
Report this post
The restaurant data dashboard is created with Power BI. I transformed raw data into valuable insights to aid decision-making. Using a restaurant dataset, I carefully cleaned the data and made interactive Power BI dashboards. This data covers various aspects such as the number of restaurants in a city, their customer base, staffing levels, average order values, and delivery times. Additionally, we have information on restaurant categories, order volumes, customer ratings, and food preparation durations based on cuisine types.

Total number of orders placed 1898. The average order cost was $31.31, with a minimum of $4.47 and a maximum of $35.41.The most popular restaurants were Shake Shark (400 orders), The Hecta (200 orders), and 12 Curs (200 orders).The cuisine types with the most orders were Thai (350 orders), American (250 orders), and South American (200 orders).The restaurants with the highest average rating were The Rasta (5 stars), Song (4.8 stars), and French (4.7 stars).The restaurants with the fastest average food preparation time were Song (15 minutes), French (20 minutes), and South American (20 minutes).The most orders were placed on weekends (600 orders), followed by Fridays (300 orders) and Saturdays (250 orders).
Amogha Foundation
Oorjja
Daksha Foundation
Jomon Joseph
Ajay Thomas
Sheetal Anna Sabu
#powerbi
#dashboard
#data
#datavisualization
#restaurantdata
17
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
WP Food Manager
1,090 followers
4mo
Report this post
👉 Data Analytics can be a game changer for your restaurant. Find out the role of data analytics in restaurants in this article.

👉  To know more, Do visit:
https://zurl.co/iLQI
#data
#dataanalytics
#restaurant
#restaurantbusiness
#WPfoodmanager
#Wordpress
#tips
9
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Sean Evers
Sales @ IndyKite
7mo
Report this post
Personal Chef, or Cafeteria?

Many self service analytics tools claim to be the first, when they are the latter.

They offer a pre prepared selection of insights to choose from, even drill downs are restricted to parameters and pre built logic. 

Most people get tired of eating the same cafeteria food day after day, leading them to stop getting food there.

We experience the same effect with analytics adoption, people use the self service analytics tools for a few weeks, but ultimately end up back at their go to dashboard after the novelty has worn off
Cimba.ai
is a bit different, we are the personal chef of analytics - want something that's not on the menu? all you have to do is ask.
12
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Isaac Oresanya
Data Analyst @ Data Career Jumpstart | Web Scraper | Python, Tableau, SQL, Excel | Open to freelance gigs
7mo
Report this post
No matter how advanced your analytical techniques or models are, if the data is inaccurate, inconsistent, or incomplete, the results will be flawed.

It's like trying to cook a gourmet meal with rotten ingredients - the final product will be a disaster, no matter your cooking skills.

Remember, the effort you put into data cleaning will pay off in the long run, ensuring your analysis and decision-making are built on high-quality data.
12
4 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ruby T
SQL Developer Trainee at Oorjja
9mo
Report this post
I created this restaurant data dashboard using Power BI! It shows things like customer numbers, order numbers, cuisine type, rating, food preparation and delivery time. It's easy to use and lets you see the data coolly. I turned data into helpful information about selling restaurant data
Jomon Joseph
Ajay Thomas
Sheetal Anna Sabu
Oorjja
Daksha Foundation
Amogha Foundation
#DataAnalytics
#Powerbi
#restaurantdata
#visualization
23
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Christopher Penn
Co-Founder and Chief Data Scientist at TrustInsights.ai
3mo
Report this post
Brooke B. Sellas
asked why marketers are data rich but insights poor.

First, what are insights? Break it down as the actual work. Insight = sight within, looking within.

What are we looking within? OUr data.

What are we looking for? Connections within that data. Patterns. Conclusions.

Think of data as an ingredient. We have no shortage of data.

Are the ingredients fresh or spoiled? Bad data is worse than no data.

Do we know what we're trying to accomplish with the data? Say our ingredient is flour. Are we making a cake? Pasta? Sourdough bread? Salt dough for a salmon on the grill? The outcome matters a whole lot.

Do we have a recipe that can get us to the outcome? Process matters.
Katie Robbert
taught me that. No process = no reliable, repeatable outcomes.

Do we have a chef with the skills to execute the recipe? You can give a dog a recipe, but good luck getting the outcome you want.

Do we have the appropriate appliances? We can have a skilled chef, a great recipe, and solid ingredients, but if we have no source of heat, we're not baking bread, period.

Asking why marketers are data rich but insights poor is a great question. The answer is the same as asking why a restaurant is ingredients rich but has no finished dishes to serve.
13
11 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
21,101 followers
545 Posts
View Profile
Connect
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more industry insights
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
David Kaplan on LinkedIn: Data Warehouse Testing Strategies for Better Data Quality,"David Kaplan on LinkedIn: Data Warehouse Testing Strategies for Better Data Quality
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
David Kaplan’s Post
David Kaplan
CTO at Alma
1y
Report this post
Check out
Lily Chang
's latest post on how to effectively test your data warehouse for improved data quality.
Lily Chang
Data Engineering Manager at Justworks
1y
Our data team has been long battling with data quality issues. These issues are exhausting and undermine stakeholders' trust and their ability to make informed decisions. While testing methods are well-established in the field of Software Engineering, they are yet to mature and gain traction within the data community. In this new post, I reflected on the journey of improving data warehouse testing strategies and tools, as well as the investment made in addressing data quality issues at Policygenius.
#dataengineering
#dataquality
#datawarehouse
https://lnkd.in/ey2XMnsF
Data Warehouse Testing Strategies for Better Data Quality
medium.com
24
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
1,485 followers
103 Posts
View Profile
Follow
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Sign in to view more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
How to Create a Data Quality Test Plan in Six Steps,"How to Create a Data Quality Test Plan in Six Steps
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Last updated on
      Jul 31, 2024
All
Data Quality
How do you create a data quality test plan?
Powered by AI and the LinkedIn community
1
Define data quality dimensions
2
Identify data sources and stakeholders
3
Establish data quality requirements and rules
4
Select data quality methods and tools
5
Design data quality test cases and scenarios
6
Define data quality metrics and reports
7
Here’s what else to consider
Data quality is the degree to which data meets the expectations and requirements of its users and stakeholders. Data quality testing and validation are the processes of checking and verifying that data is accurate, complete, consistent, reliable, and fit for its intended purpose. A data quality test plan is a document that outlines the scope, objectives, methods, tools, criteria, and metrics for assessing data quality in a specific project or context. In this article, you will learn how to create a data quality test plan in six steps.
Top experts in this article
Selected by the community from 27 contributions.
Learn more
Jonathan Agee
Co-Founder & CEO @ Validatar | Holistic Data QA Pioneer | I help data teams automate QA
View contribution
11
Swapnil Dixit 🇮🇳
Data Analyst  | SEO Specialist  | Solving Complex Ranking Challenges  | 400+ Keywords Ranked | Technical SEO |SERP…
View contribution
9
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
View contribution
8
See what others are saying
1
Define data quality dimensions
The first step is to define the data quality dimensions that are relevant and important for your data and its use cases. Data quality dimensions are the aspects or characteristics of data that measure its quality, such as accuracy, completeness, consistency, timeliness, validity, and uniqueness. You can use existing frameworks or standards, such as ISO 8000 or DAMA DMBOK, or customize your own dimensions based on your business needs and goals. You should also define the sub-dimensions and indicators for each dimension, such as error rate, coverage, conformity, currency, and duplication.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Jonathan Agee
Co-Founder & CEO @ Validatar | Holistic Data QA Pioneer | I help data teams automate QA
Copy link to contribution
Report contribution
There are many more dimensions that impact data quality and reliability than many people initially think of. Here's a list that is a starting point for our implementations:
- Completeness
- Uniqueness
- Validity
- Consistency
- Accuracy
- Timeliness
- Standards
- Relationships
- Security
- Precision
- Performance
- Processing
- Compliance
…see more
Like
11
Jose Almeida
Data Consultant/Advisor 🇵🇹 🇳🇬 🇰🇪 🇦🇪 🇩🇰 🇿🇦 🇩🇪 👉 Data Strategy 👉 Data Governance 👉 
 Data Quality 👉 Master Data Management ✈️ Remote/Onsite Consulting Services in EMEA
Copy link to contribution
Report contribution
Creating a data quality test plan involves:
-	Defining objectives
-	Identifying data sources
-	Selecting metrics
-	Developing test cases and scenarios
-	Setting acceptance criteria
-	Executing tests
-	Analyzing results
-	Reporting findings
-	Iterating for improvement
…see more
Like
5
Prachi Pandey
Data Consultant | Business Engagement, Data Quality And Governance | Indian Institute of Foreign Trade, New Delhi
Copy link to contribution
Report contribution
Creating a data quality test plan begins with defining data quality dimensions. Data quality dimensions are specific aspects or characteristics of data that are used to assess its quality. These dimensions provide a framework for evaluating the completeness, accuracy, consistency, timeliness, and other attributes of the data.

Once you have defined the data quality dimensions, you can use them to develop a data quality test plan. The test plan should outline the objectives, scope, methodologies, and metrics for assessing data quality across each dimension. It should also specify the tools, resources, and responsibilities for conducting the tests and analyzing the results.
…see more
Like
4
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Start by identifying the key dimensions of data quality that are relevant to your project, such as accuracy, completeness, consistency, timeliness and validity. These dimensions will be the basis of your test plan, guiding the identification of specific areas to focus on during testing. Clarifying these dimensions ensures that all aspects of data quality are examined and addressed.
…see more
Like
3
Nourhan Radwan
CDMP Master Level | Data Strategy | Data Management | Data Modeling | Data Integration | Data Quality | Data Governance | Google Cloud
Copy link to contribution
Report contribution
A data quality test plan is a roadmap for assessing and improving the quality of your data. It outlines the specific steps you'll take to identify and address data issues. Here's a breakdown of the key steps involved in creating a data quality test plan:

1. Define Data Quality Goals and Standards
2. Identify Data Sources and Scope
3. Choose Data Quality Testing Methods:
-Data Profiling
-Data Auditing
-Data Cleansing
-Data Validation

4. Define Test Procedures and Responsibilities:
-Detailed Steps
-Tools and Techniques
-Roles and Responsibilities
…see more
Like
3
Yugandhara Saste
Data Engineer
Copy link to contribution
Report contribution
Creating a data quality test plan involves several key steps: Start by defining the scope and objectives, including the specific data elements and quality dimensions (accuracy, completeness, consistency, etc.) to be tested. Develop a detailed test strategy outlining the methodologies, tools, and resources needed. Create test cases and scenarios based on data requirements and business rules. Establish metrics for measuring data quality and set up a schedule for regular testing and validation. Finally, document the test plan thoroughly and ensure it includes a process for reporting and addressing any identified issues. Regular reviews and updates will help maintain data quality over time.
…see more
Like
1
Martina Dippel 🙌🏻
🌟 I breathe data to the fullest! 💫  Datenversteherin aus Leidenschaft! 🔥
Copy link to contribution
Report contribution
In our approach to crafting data quality test plans, we prioritize aligning data quality dimensions with our clients' strategic objectives. For instance, when working with a retail client, we emphasized 'timeliness' and 'completeness' to enhance their inventory management system. By applying the DAMA DMBOK framework, we customize dimensions to fit specific industry needs, ensuring a targeted improvement in data quality that supports business outcomes.
…see more
Like
Conner Eagleton
Scaling Data Quality Capabilities
Copy link to contribution
Report contribution
Clearly articulate the goals of the data quality testing, such as improving decision-making or enhancing data-driven processes. Specify the scope by identifying the specific data sets, systems, and stakeholders involved. This clarity ensures alignment with business objectives and focuses efforts on areas with the most significant impact on data quality.
…see more
Like
2
Identify data sources and stakeholders
The second step is to identify the data sources and stakeholders that are involved or affected by your data quality testing and validation. Data sources are the origins or locations of your data, such as databases, files, APIs, or external providers. Stakeholders are the people or groups that have an interest or role in your data, such as data owners, producers, consumers, managers, or regulators. You should document the characteristics, relationships, and dependencies of your data sources and stakeholders, such as data formats, schemas, flows, access rights, expectations, and requirements.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Identify all data sources that will be tested, including databases, data lakes, and external data feeds. Additionally, identify individuals who have an interest in the data quality, such as data owners, users, and business analysts. Identifying the origin of your data and the expectations of stakeholders is crucial for creating a test plan that meets the organization's needs.
…see more
Like
3
Olaoluwakiitan Olabiyi
Data Quality Analyst @Raven Housing Trust 
|| Data Governance || Data Literacy || Metadata Management 
|| Reporting & Analytics
Copy link to contribution
Report contribution
Gaining senior stakeholder buy-in is crucial for the success of any data quality test plan. Their support can provide the necessary resources, authority, and visibility to ensure the plan's effectiveness. 

Also, collaborating with data stewards and custodians is also essential as they possess valuable insights into the organisation's data landscape and business processes.
…see more
Like
2
Olaoluwakiitan Olabiyi replied:
Developing a data lineage is also relevant for understanding the journey of data elements through various systems and transformations. 

It helps in identifying potential points of data quality issues and assessing the trustworthiness of the systems involved in handling the data. This information is invaluable for designing effective data quality tests and prioritising efforts to improve data quality.
Martina Dippel 🙌🏻
🌟 I breathe data to the fullest! 💫  Datenversteherin aus Leidenschaft! 🔥
Copy link to contribution
Report contribution
Understanding the ecosystem of data sources and stakeholders is crucial in our projects. We engage in detailed mapping exercises, identifying how data flows between systems and stakeholders. For a financial services project, this meant cataloging data origin points and establishing clear communication channels with data producers and consumers. This holistic view ensures that our data quality testing is both comprehensive and aligned with stakeholder expectations.
…see more
Like
3
Establish data quality requirements and rules
The third step is to establish the data quality requirements and rules that specify the expected level and criteria of data quality for your data sources and stakeholders. Data quality requirements are the statements that define the desired or acceptable quality of data, such as thresholds, ranges, formats, or values. Data quality rules are the logical expressions that test or validate the data quality requirements, such as constraints, checks, or conditions. You should align your data quality requirements and rules with your data quality dimensions and indicators, and prioritize them based on their importance and impact.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
For each data quality dimension, define specific requirements and rules that data must meet. This might involve setting thresholds for acceptable levels of data completeness or criteria for data accuracy. These requirements and rules form the basis for what will be tested and how data quality will be measured.
…see more
Like
5
Jonathan Agee
Co-Founder & CEO @ Validatar | Holistic Data QA Pioneer | I help data teams automate QA
Copy link to contribution
Report contribution
When establishing data quality requirements and rules it is critical to define them in consistent ways so that they can be implemented across your entire environment as quickly and efficiently as possible. 

Ideally you would use an active metadata approach were data quality rules and tests can be automated based on documentation that is captured in a data catalog or centralized repository. 

Rule templates are important to create a standard for how metadata will be used to generate specific checks and validation rules.
…see more
Like
1
Martina Dippel 🙌🏻
🌟 I breathe data to the fullest! 💫  Datenversteherin aus Leidenschaft! 🔥
Copy link to contribution
Report contribution
Setting clear, actionable data quality requirements is a cornerstone of our methodology. By leveraging industry-specific benchmarks, we define precise quality thresholds that drive value. In a healthcare data management project, we established rules around data 'validity' to ensure compliance with regulatory standards, demonstrating how tailored requirements can mitigate risk and enhance operational efficiency.
…see more
Like
Conner Eagleton
Scaling Data Quality Capabilities
(edited)
Copy link to contribution
Report contribution
Involves defining clear standards and criteria to ensure data accuracy, consistency, and reliability. This includes identifying key data elements, setting validation rules, and implementing monitoring processes to maintain data integrity. Effective data quality management enhances decision-making, regulatory compliance, and operational efficiency.
…see more
Like
4
Select data quality methods and tools
The fourth step is to select the data quality methods and tools that will help you perform and automate your data quality testing and validation. Data quality methods are the techniques or approaches that you use to assess or improve your data quality, such as profiling, cleansing, standardizing, enriching, or monitoring. Data quality tools are the software or applications that you use to implement or support your data quality methods, such as ETL tools, data quality platforms, or data validation libraries. You should choose your data quality methods and tools based on your data characteristics, requirements, rules, and resources.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Select appropriate methods and methods for conducting data quality testing. This could include automated data quality software, SQL scripts, or manual checks for smaller datasets, as well. The selection should be based on the complexity of your data environment and the specific tests you plan to perform.
…see more
Like
3
Martina Dippel 🙌🏻
🌟 I breathe data to the fullest! 💫  Datenversteherin aus Leidenschaft! 🔥
Copy link to contribution
Report contribution
Choosing the right tools is pivotal to the success of our data quality initiatives. We assess each client's unique environment and data landscape to recommend tools that offer the best fit. For a client in the e-commerce sector, this involved integrating a data quality platform that excels in real-time data monitoring, directly addressing the dynamic nature of their online transactions.
…see more
Like
1
Jonathan Agee
Co-Founder & CEO @ Validatar | Holistic Data QA Pioneer | I help data teams automate QA
Copy link to contribution
Report contribution
Some really important factors that should be considered when selecting a tool to support data quality processes:
1. What data can it connect to? - You will eventually need a tool that can work with data in databases, files, apis, etc. 
2. How easy to use is it? - Your users need to be comfortable using the tool in order to foster adoption.
3. How flexible is it for solving complex problems? - Only the most superficial data quality checks are always the same. A tool that will actually solve your complex data quality tests needs to be highly flexible.
…see more
Like
1
5
Design data quality test cases and scenarios
The fifth step is to design the data quality test cases and scenarios that will execute and evaluate your data quality rules and requirements. Data quality test cases are the sets of inputs, outputs, and expected results that verify the functionality and performance of your data quality rules and requirements. Data quality scenarios are the collections of test cases that simulate the real-world situations or use cases of your data quality testing and validation. You should design your data quality test cases and scenarios based on your data sources, stakeholders, methods, and tools.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Helmi Tatanaki - Data Management Consultant
Data Management Consultant specializing in Data Governance and Quality
Copy link to contribution
Report contribution
Develop detailed test cases and scenarios that demonstrate real-world use of data. This involves determining the data to be tested, the tests to be applied, and the expected outcomes based on your data quality requirements. Test cases provide comprehensive coverage of all critical data quality issues.
…see more
Like
8
Merrill Albert
Enterprise Data Leader, Data Governance Officer, Data Thought Leader, Chief Data Officer, Fractional Governance, Data Evangelist, LinkedIn Top Data Governance Voice, creator of #CrimesAgainstData
Copy link to contribution
Report contribution
Testing, including data quality testing, is often done wrong or isn't thorough enough.  Test cases with inputs and expected outputs are critical to test appropriately.  Too often, people just look at what is currently happening.  The number is 10 now, you test and get 10, and you're good, right?  Not necessarily.  You're looking at data quality because you've had a problem.  The value might be 10 now, but it's maybe supposed to be 8.  Unless you properly craft your test cases with the data going through it and know what you're supposed to get out of it, you won't be able to tell if the test passed.
…see more
Like
4
Martina Dippel 🙌🏻
🌟 I breathe data to the fullest! 💫  Datenversteherin aus Leidenschaft! 🔥
Copy link to contribution
Report contribution
Our test case designs are informed by real-world scenarios that mirror our clients' operational challenges. By constructing test cases that reflect actual data issues encountered in past projects, like duplicate records in a customer database, we ensure our testing process is robust and relevant. This practice has proven effective in preemptively identifying potential data quality issues before they impact business operations.
…see more
Like
3
6
Define data quality metrics and reports
The sixth step is to define the data quality metrics and reports that will measure and communicate your data quality testing and validation results. Data quality metrics are the quantitative or qualitative measures that indicate the level or degree of data quality, such as error rate, completeness percentage, consistency ratio, or validity score. Data quality reports are the documents or dashboards that display or summarize your data quality metrics and findings, such as issues, trends, gaps, or recommendations. You should define your data quality metrics and reports based on your data quality dimensions, indicators, requirements, and rules.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Sheron Philip Koshy
Technical Director at ECCMA
Copy link to contribution
Report contribution
Generally data quality (DQ) metrics are set when you define your DQ dimensions. Also define the metric values at which the dimension indicates the data is good/usable/poor or it can acceptable/unacceptable depending on the use case. For example, in a specific use case the metric for the ""validity"" dimension needs to be 100% for the data to be acceptable.

DQ reports can go beyond just summarized displays of DQ metrics, it can include information on SLA metrics for DQ fixes and ROI's over a period of time.
…see more
Like
1
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Swapnil Dixit 🇮🇳
Data Analyst  | SEO Specialist  | Solving Complex Ranking Challenges  | 400+ Keywords Ranked | Technical SEO |SERP Ranking | SaaS & Ecommerce SEO | Core Web Vital Optimization | GreyHat SEO | SEO Reporting
Copy link to contribution
Report contribution
Here's a step-by-step guide to creating a data quality test plan:
☀️Define Objectives and Scope
☀️Identify Stakeholders
☀️Define Data Quality Dimensions
☀️Select Testing Techniques
…see more
Like
9
Kah Weng Lim
Manager - Data Management Governance | Analytics | Principal Consultant
Copy link to contribution
Report contribution
And also make sure the goal is can get support by management/company

If not, they will treat as less value and keep questions abt the contribution
…see more
Like
3
Dan Blake
Data and Technology | Financial Crime, Sanctions, AML & KYC SME | Harvard Business Review Advisory Council member
Copy link to contribution
Report contribution
It’s advisable to record the specifications, aspects, and involved parties (such as data owners and stewards) in a data contract. This documentation is crucial for maintaining clear communication among all participants and can be revised to include additional aspects and informational reports as needed.

These data contracts can subsequently serve as a foundation for ongoing quality assessments using various tools, reports, and metrics.

Knowing the data owners and stewards is essential so they can address issues identified through continuous testing, alerting and reporting.
…see more
Like
2
Conner Eagleton
Scaling Data Quality Capabilities
Copy link to contribution
Report contribution
Incorporate thorough assessments to ensure data quality testing adheres to pertinent regulations and standards such as GDPR, HIPAA, or industry-specific mandates. Verify that the testing methodologies and outcomes comply with legal requirements concerning data collection, processing, storage, and dissemination. Addressing regulatory compliance not only mitigates legal risks but also fosters trust with customers and stakeholders by demonstrating a commitment to ethical and responsible data management practices.
…see more
Like
1
Sheron Philip Koshy
Technical Director at ECCMA
Copy link to contribution
Report contribution
Additionally, 
1) Initially define your goals/objectives of the DQ test plan
2) validate the test plan to make sure your objectives/goals are met.
…see more
Like
Data Quality
Data Quality
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Quality
No more previous content
How can you identify and fill gaps in your data quality strategy?
58 contributions
How can you improve data quality maturity across domains and industries?
53 contributions
How do you tell stakeholders if your data is bad for machine learning?
52 contributions
How do you report data quality results to your audience?
62 contributions
How do you prioritize data quality when resources are limited?
69 contributions
How can you improve data quality documentation and metadata management?
61 contributions
How do you set data quality policies?
54 contributions
How can you establish and enforce data quality standards?
56 contributions
How can you compare data quality improvement techniques?
35 contributions
How can you build a data quality culture using a framework?
51 contributions
How do you automate data quality metrics for different platforms?
49 contributions
How are you improving your data quality?
45 contributions
How can you improve data quality when integrating multiple sources?
12 contributions
What are the best ways to transform data for different audiences?
33 contributions
How do you prepare for emerging data quality trends and challenges?
11 contributions
No more next content
See all
More relevant reading
Data Analysis
How do you enforce data quality standards using data quality rules?
Data Quality
How do you match data quality goals with stakeholder needs?
Data Management
What are the most effective data quality improvement initiatives?
Data Engineering
What do you do if your real-time data has quality issues?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
2
27 Contributions"
"Nicole Janeway Bills on LinkedIn: Exam debrief on the Data Quality Specialist Exam 📘 I scored an 84%, which… | 33 comments","Nicole Janeway Bills on LinkedIn: Exam debrief on the Data Quality Specialist Exam 📘 I scored an 84%, which… | 33 comments
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Nicole Janeway Bills’ Post
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Edited
Report this post
Exam debrief on the Data Quality Specialist Exam 📘 I scored an 84%, which gets me a step closer to my Master level CDMP credential from
DAMA-I
. 

Test preparation tips:
-  It certainly doesn't hurt to re-read DMBOK ch. 13, though at the same time, I didn't feel like this was particularly helpful.  I did use the DMBOK as my one book since I already have the ebook.
-  Review statistics fundamentals (links below 🔽)
-  Read ""Measuring Data Quality for Ongoing Improvement"" by
Laura Sebastian-Coleman
(thanks
Kathryn Lesh
for this tip)
-  As with any CDMP exam, make sure you use an ebook as your one book and practice looking up answers using your PDF viewer

Test taking strategies:
-  Make sure to center yourself before sitting for the exam; there's A LOT of reading, so you'll want to make sure you're well rested and focused (
Data Strategy Professionals
Career Coach Saif Sheik recommends going for a brisk walk; I think meditation would have been helpful for me)
-  Prepare for scenario-based questions, basic stats questions, and a few easy definition questions
-  If you're really stuck, the longest answer choice often seems be the correct one  
- Be sure to work until the last minute! BrightSpace will automatically submit your exam. You want to use all the time to double check your responses.

Other comments:  I found this test much more difficult than the Fundamentals Exam.  I ended up with only 10 minutes at the end to double check my answers (compared to ~40 minutes in the Fundamentals Exam), and I felt relieved when I saw my score.

If you've taken this test or you're preparing to take it, curious what you're doing to prepare?  Share any helpful links in the comments.
231
33 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Report this comment
Stats 101:
https://openstax.org/books/introductory-statistics-2e/pages/1-introduction
Like
Reply
5 Reactions
6 Reactions
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Report this comment
""Measuring Data Quality for Ongoing Improvement""
https://amzn.to/2NskNPF
Like
Reply
5 Reactions
6 Reactions
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Report this comment
Data Strategy Professionals
article about the Specialist Exams:
https://www.datastrategypros.com/resources/cdmp-specialist-exam#dq
Like
Reply
4 Reactions
5 Reactions
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Report this comment
Statistics fundamentals from
StatQuest
:
https://www.youtube.com/playlist?list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9
Like
Reply
3 Reactions
4 Reactions
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Report this comment
cc:
Louai Haj Hussein
,
Harikumar Sasidharan
,
Adetola O. Kolawole
,
Suchandra S.
,
Ádám Süki
,
Israel Cosío Arce
Like
Reply
3 Reactions
4 Reactions
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
2mo
Report this comment
Thank you so much for reposting,
kranthi reddy
😊
Like
Reply
1 Reaction
Bjarte Tolleshaug
Teamlead | Power BI | CDMP | Data Governance Specialist | DAMA Norway |
2mo
Report this comment
Thanks for sharing
Nicole Janeway Bills
It’s on my to do list and I agree that the specialist exam is thight on time. 

On my Data Governace exam I didn’t have much time to review the answers, and it’s also a lot of words that is hard to understand for a simple Norwegian 😅

I learned a lot from
Robert Hawker
book on practical data quality, I will for sure read that again, but it’s good to know what other chapters than 13 is relevant from DMBook. 

Link to the book:
https://www.amazon.co.uk/Practical-Data-Quality-real-world-organization/dp/180461078X
Like
Reply
4 Reactions
5 Reactions
Darryl Everett, CDMP Master
DAMA CDMP Master, DCAM with PwC Canada
2mo
Report this comment
You are almost there!
Like
Reply
2 Reactions
3 Reactions
Aimen Aslam
DataAnalyst | DataAnalytics | DataViz | Client & Stakeholder Management| Excel |Power Query- Data Integration & Transformation| Powerpivot| PowerBi| SQL
2mo
Report this comment
Congratulations 🎉🎉
Like
Reply
1 Reaction
2 Reactions
Ivana BIRTIC
Data Management - Programmes Data - Coaching Agile
2mo
Report this comment
Congratulations 🎉
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More Relevant Posts
Nicole Janeway Bills
Founder & CEO at Data Strategy Professionals
6mo
Report this post
Preparing for the Certified Data Management Professional (CDMP) Fundamentals Exam can be a daunting task!  With over 500 pages in the DMBOK that span 14 Data Management knowledge areas, there's a lot to remember...

As you're getting ready for the test, you'll need to intake, then process, then practice applying information from the DMBOK, so you can eventually ace the exam.   Here are some recommendations for each of these stages.

Intake:
-  Don't just read the DMBOK
-  Find a study aid that makes complex concepts feel clear and manageable
-  Make sure you're incorporating auxiliary study materials (books, articles, and videos) to ensure you have a full understanding of the material and get a new perspective on DMBOK content
-  Keep study sessions short and focused on key concepts; consider using the pomodoro technique

Process Information:
-  Build a study and review schedule to learn new concepts while you revisit the material you've already studied
-  Challenge yourself with active recall methods such as flashcards to reinforce your memory of key concepts
-  Make sure you can summarize key concepts using straightforward, decomplexified vocabulary and try teaching what you're learning to another person to avoid falling victim to the illusion of explanatory depth
-  Utilize study groups and discussion sessions to support other learners; you may be interested in joining CDMP Study Group on LinkedIn

Test Your Knowledge:
-  Utilize practice questions in the style of the target exam
-  Create a log of questions that you missed or were confused by
-  Seek out advice from others about common mistakes

So hopefully these recommendations can help you ace the exam!   

Just reading the DMBOK may not be enough to help you really internalize the concepts and get the best score on the test.  (As a reminder, passing is 60%+, but you'll need a 70%+ to continue to the Practitioner level, and 80%+ for the Master level).  Given that 20% of questions cannot be found in the DMBOK, you should definitely add some auxiliary materials to help you get ready.  Reviewing books, articles, and videos, especially from
DAMA-I
associated thought leaders, can help you get a new perspective and better understand the DMBOK.
31
3 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Data Strategy Professionals
5,352 followers
6mo
Report this post
Preparing for the Certified Data Management Professional (CDMP) Fundamentals Exam can be a daunting task! With over 500 pages in the DMBOK that span 14 Data Management knowledge areas, there's a lot to remember...

As you're getting ready for the test, you'll need to intake, then process, then practice applying information from the DMBOK, so you can eventually ace the exam. Here are some recommendations for each of these stages.

Intake:
- Don't just read the DMBOK
- Find a study aid that makes complex concepts feel clear and manageable
- Make sure you're incorporating auxiliary study materials (books, articles, and videos) to ensure you have a full understanding of the material and get a new perspective on DMBOK content
- Keep study sessions short and focused on key concepts; consider using the pomodoro technique

Process Information:
- Build a study and review schedule to learn new concepts while you revisit the material you've already studied
- Challenge yourself with active recall methods such as flashcards to reinforce your memory of key concepts
- Make sure you can summarize key concepts using straightforward, decomplexified vocabulary and try teaching what you're learning to another person to avoid falling victim to the illusion of explanatory depth
- Utilize study groups and discussion sessions to support other learners; you may be interested in joining CDMP Study Group on LinkedIn

Test Your Knowledge:
- Utilize practice questions in the style of the target exam
- Create a log of questions that you missed or were confused by
- Seek out advice from others about common mistakes

So hopefully these recommendations can help you ace the exam! 

Just reading the DMBOK may not be enough to help you really internalize the concepts and get the best score on the test. (As a reminder, passing is 60%+, but you'll need a 70%+ to continue to the Practitioner level, and 80%+ for the Master level). Given that 20% of questions cannot be found in the DMBOK, you should definitely add some auxiliary materials to help you get ready. Reviewing books, articles, and videos, especially from DAMA-I associated thought leaders, can help you get a new perspective and better understand the DMBOK.
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
rana karame
Micro entrepreneur
5mo
Report this post
I successfully finished the Coursera course ""Process Data from Dirty to Clean."" During the course, I learnt how to deal with incomplete, inaccurate, and dirty data that has no bearing on the issue I'm trying to address. 

categories of contaminated data and the appropriate syntax to employ in order to clean it up and recover accurate information.

This course is part of a series on data analysis.
Process Data from Dirty to Clean
coursera.org
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ashwini Patil
Data Analytics Expertise | Proficient in Tableau, SQL, and Power BI
9mo
Report this post
""Excited to share that I've successfully completed the 'Process Data from Dirty to Clean' certification! 🎉 This comprehensive course equipped me with essential skills in data cleaning and transformation, ensuring accuracy and reliability in analytical processes. Looking forward to applying these techniques to real-world data challenges!
#DataCleaning
#DataTransformation
#Certification
""
Completion Certificate for Process Data from Dirty to Clean
coursera.org
11
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Paul Rakké
Sr. Data Consultant | CDMP | Arcitura Certified Trainer at  Longo-Data-Management
6mo
Edited
Report this post
Again an enjoyable moment to share: my first book is released last week.

The objective of this book is to help data professionals who are studying for the CDMP-certificate (Certified Data Management Professional) with their preparation for this exam. In this DAMA DMBOK-based book sample exam-questions with answers and relevant comments are written down. In the first part of the book the questions are categorized in the knowledge areas as described in DMBOK and the second part consists of a complete exam with a set of 100 exam-like questions conform the weights per knowledge area as stated for the formal exam. So you can get not only a good idea about your knowledge-level but also about the pace needed for a successful result of the CDMP-exam.
This book is published in the same series as ‘Data Management courseware based on CDMP Fundamentals’ by Bas van Gils et al. and published by van Haren Publishing.
It is available now via Amazon (
https://amzn.eu/d/5uNrGOW
),
ManagementBoek.nl
and of course also via other channels.
I hope that this book contributes to a successful result for the data professional in obtaining this very valuable certificate.
Success
Data Management Fundamentals (DMF) - CDMP exam preparation
amazon.nl
37
22 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Vardan Aroustamian
8mo
Report this post
Looking back at DW/BI projects I worked on…. Some small, some large. Some pure Kimball, some Inmon, some very complex, some incomplete, some ugly, and some beautiful too 😊

Considering what I knew then, what was trendy, new, right at the time, and what I know today, I think the better way is to use Data Vault methodology.

Not a revelation for those who know and practice it 😊 But for those who don’t, I suggest to consider > study > use DV 😊 For study part, I highly recommend Certified Data Vault 2 Practitioner class taught by
Cindi Meyersohn
from
DataRebels®
. She has knowledge, experience, and teaching talent.

For those who built DV already. If there are warnings, issues to watch, or suggestions – I’m very interested to learn that too and will appreciate sharing.
25
9 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Agnieszka Kucharska
Academic Teacher / Researcher in WUT; Managing Editor in International Journal - Foundations of Management
4mo
Edited
Report this post
I followed the advice and started exploring the
SAS
Educator Portal (
learn.sas.com
), a new learning platform from
#SAS
. Advantages that I see? 
💡 24/7 learning experience,  
💡 online access to software, 
💡 instructional materials, 
💡 digital learning, 
💡 certification exam prep materials. 
It's a nice place to find out what to learn and teach to build analytics skills.
Data Literacy Essentials was issued by SAS to Agnieszka Kucharska.
credly.com
20
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
CPE Flow - Expert Accounting & Finance Online Courses
6,048 followers
1mo
Report this post
Data Analytics Mini-Course

The Four V’s You Must Know 👇

Ever feel like we're swimming in an ocean of data?

That's the world of Big Data for you

It’s immense, fast, and a bit overwhelming

What exactly is Big Data & why does it matter? 

Let’s break it down into plain English

Imagine walking into the world’s largest library

Instead of books, it’s filled with zettabytes of data 

Big Data is precisely like that library!

Now, let’s go even deeper into this topic

Presenting the four V’s of Big Data:

1️⃣ Volume: 
We’re talking about data of gigantic proportions. For finance pros, this means dealing with numbers not just in spreadsheets but in data centers that store information from countless transactions and interactions.

2️⃣ Velocity: 
This data doesn’t just sit there; it streams at us in real-time. Think stock market feeds, real-time analytics, and instant transaction records. The speed at which data comes in can be as dizzying as it is exciting.

3️⃣ Variety: 
It’s not all numbers and text. Data comes in emails, images, videos, and more. For example, analyzing customer sentiment from social media or video testimonials requires us to be fluent in more than just Excel.

4️⃣ Veracity: 
Ever play the game of telephone? Information can get twisted quickly. Similarly, Big Data comes with uncertainty. It’s on us to sift through this data, ensuring its accuracy and reliability for making informed decisions.
—-------------------

👉How are you navigating Big Data in your role? 

What opportunities or hurdles have you seen?

 Drop your thoughts in the comments! 

—---------------

Hi! I’m Nathan Liao, Founder & CEO of:

🚀 CMA Exam Academy dot com
- Pass the CMA exam on your first attempt!
- 16-week Accelerator program (link in bio)
- Students in 120 countries. 92% exam pass rate

🚀 CPE Flow dot com
- Are you a certified accountant?
- Earn your annual CPE credits (link in bio)

➕ Follow me for accounting & finance insights
24
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Viacheslav Baranovskyi
Producer - Project Manager - Scrum Master - Gamer
5mo
Report this post
Ultimately, the quality of a manager is determined by the quality of his (or hers) decisions.

Thus, when it comes to Project Management and Production roles in general, being able to make informed and data-driven decisions is a must.

This is why I'm taking my time to gain knowledge and practical skills of a certified Data Analyst.

For in the world of uncertainty and ambiguity the difference between a good and bad decision is sometimes the difference between failure and success.

And Data is the answer ;)

1 in, 7 more to come.
Completion Certificate for Foundations: Data, Data, Everywhere
coursera.org
3
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ibrahim Alghamdi
Senior Customer Experience Analyst @ GOSI | Project & Product Management | Business Development & Data-Driven CX | CERT.APM, PMP, CXAC
6mo
Report this post
Enhancing my Data Analysis skills 📊 to make informed decisions based on accurate data. Certification in data cleaning reinforces our commitment to staying updated in today's dynamic environment. Investing in our skills ensures we're equipped to tackle any data challenge with confidence. Strengthening our competitive edge for growth and achievement! 🚀
#DataAnalysis
#DataCleaning
#Certification
🌟
Completion Certificate for Process Data from Dirty to Clean
coursera.org
81
14 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
12,278 followers
2,715 Posts
1 Article
View Profile
Follow
More from this author
CIPP/E Update — what’s changing in October 2023
Nicole Janeway Bills
1y
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Sign in to view more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
The Perils of Big Data: The Gary Lineker Test,"The Perils of Big Data: The Gary Lineker Test
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
The Perils of Big Data: The Gary Lineker Test
Report this article
Dr. Chris Donegan
Dr. Chris Donegan
Sceptical Empiricist.
Published Oct 20, 2015
+ Follow
Gary Lineker once said
""Football is a simple game. Twenty-two men chase a ball for 90 minutes and at the end, the Germans always win.""
Given the evident truth in this quotation I was shocked to read in the Financial Times today that FIFA have announced Belgium as the world's best soccer team.
If there are any non-Belgians out there that think this makes sense I would love to hear from them.
Of course there are some very talented Belgian players.  Eden Hazard for one.  But the best team in the world?
I wonder what the odds would be from a straw poll of supporters for the outcome of a Belgium versus Brazil game.  Or Germany.  Or Argentina. Or Spain.
So, how did this happen?
Statistically speaking it seems that FIFA is correct. Their model undoubtedly runs beautifully in a spreadsheet.  Of course assumptions drive the model and one surprising one is responsible for the red devils ascent into greatness.
The FIFA ranking model weights results according to the quality of opposition - an approach that seems wholly rational.  To save splitting hairs the model rates all teams below the 150 spot as equal, which again seems sensible.  After all, who can separate Andorra and San Marino?
The thing is that these fine tuning decisions at the margins that seem irrelevant actually do matter in a butterfly effect sort of way.  When you take the trouble to apply the rankings to all teams in the FIFA universe and not just the top 150 the world starts feeling right again.  Germany comes out on top.
My take home from this story is that we should beware of statistics even when they come from a plausible source (OK so FIFA falls at the first hurdle).  The business world is full of this stuff and the popularity of ""Big Data"" is grist to the mill.
A simple reality check can quickly puncture 90% of the nonsense in 15 seconds.  Use your common sense (lets call it the Gary Lineker test) and you will save yourself a lot of grief.
Postscript for Belgians, your team really is pretty good in global terms (top 15) and frankly that is accolade enough for a country with 11 million people.  It would be great if England could punch at the same weight!
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by Dr. Chris Donegan
Decision making in a time of uncertainty: part 2
Sep 7, 2020
Decision making in a time of uncertainty: part 2
Statistics, Probability, Newton and The Old Bailey Newton's laws are wonderful heuristics for our brains to understand…
25
10 Comments
Decision making in a time of uncertainty
Aug 17, 2020
Decision making in a time of uncertainty
We live in a time of dramatic and unfamiliar change. This is nothing new, but it sure feels like it.
28
24 Comments
Coffee Time.
Mar 25, 2020
Coffee Time.
Over the past three months I have worked on the restructuring of two firms (an MNC spin-out and an SME pre-pack) and…
12
3 Comments
The Clerical Error that Created a Christmas Legend
Dec 30, 2019
The Clerical Error that Created a Christmas Legend
It’s a Wonderful Life regularly polls as the #1 Christmas movie of all time. I must have seen it 30 times.
13
3 Comments
Hardboiled IP.
Nov 15, 2019
Hardboiled IP.
I am a devotee of film noir and hardboiled detective novels. Bogart in the Maltese Falcon, Elliot Gould in The Long…
6
1 Comment
Investing in IP.  Size matters.
Aug 27, 2018
Investing in IP.  Size matters.
In the early 2000's I was asked to value a catalogue of songwriters rights as collateral for a bank loan. The catalogue…
22
7 Comments
LTW: The future is here
Jun 12, 2018
LTW: The future is here
I am frequently asked by my children what I think the future holds and I truthfully answer that I have no idea. This…
5
4 Comments
七転び八起き: Stay in the Game
May 23, 2018
七転び八起き: Stay in the Game
The Japanese proverb ""Nana korobi ya oki"" has been translated for me by my oldest son as ""fall seven times, get up…
9
5 Comments
Investing in Deep Value: Tokenisation
May 14, 2018
Investing in Deep Value: Tokenisation
Traditional accounting and the Wall Street financial analysis that derives from it is not fit for purpose in the modern…
5
4 Comments
#Linky Brain Superhero
Apr 2, 2018
#Linky Brain Superhero
I am a comic fan. Its nice to be able to say that in 2018 and be mainstream.
7
3 Comments
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data Testing,"Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing
Report this article
QA Touch
QA Touch
We help companies deliver quality projects on time with a Test Management tool built for growth.
Published Sep 9, 2022
+ Follow
The never-ending demand for the creation, storage, retrieval, and analysis of massive amounts of data sparked the inspiration for the development of
Big Data Testing
. Testing such a huge amount of data requires the use of precise tools, outstanding structures, and smart tactics. Here is a brief overview of Big Data Testing that you need to know.
What is Big Data Testing?
Big Data Testing is a method that involves evaluating and testing the performance of Big Data Applications. Big Data is a massive data collection that typical storage systems cannot manage.
There are several areas in it, where its testing strategy is essential. Testing in Big Data projects can take many different forms, including database testing, infrastructure, performance testing, and functional testing. It is described as a huge volume of data structured or unstructured. Data may occur in any format such as flat files, images, video files, etc.
Strategies behind Big Data Testing
Testing an application that manages terabytes of data would require new proficiency levels and original thoughts. The most important tests that the quality assurance team focuses on are based on the three scenarios.
Batch Data Processing Test
When an application is handled in batch mode using batch processing storage devices like
HDFS
, the batch data processing test requires test processes that execute the data. The bulk of the batch process testing consists of:
Testing the application with faulty inputs
Varying the data volume
Real-Time Data Processing Test
When the program is in Real-Time Data Processing mode, the Real-Time Data Processing Test deals with the data. The tools like
Spark
are used to run the application In real-time testing.
The application is examined for stability while being evaluated in a real-time environment.
Interactive Data Processing Test
The Interactive Data Processing Test incorporates real-life testing protocols that interact with the application from the perspective of a real-life user.
HiveSQL
and other interactive processing tools are used in interactive data processing mode.
Types of Big Data Testing
Architecture Testing:
This type of testing ensures that the data processing is accurate and complies with the needs of the business. Additionally, if the architecture is incorrect, it could lead to performance issues, which could cause data loss and interruptions in processing. So, to guarantee the success of your Big Data project, architectural testing is essential.
Database Testing:
As the name implies, this testing often involves the verification of data obtained from numerous databases. It confirms that the data gathered from local databases or cloud sources is accurate and correct.
Performance Testing:
It analyzes loading and processing speed to ensure big data applications run consistently. This testing method assists in determining the rate of the data output as IOPS (Input Output Per Second) from various databases and data warehouses. By performing multiple test scenarios, it also checks the primary functionality of the big data application under demand.
Functional Testing:
Deep functional testing at the API level is required for big data systems that include operational and analytical components. All of the scripts, programs, and tools used for storing, loading, and processing applications are included in the tests.
Conclusion
To provide reliable results and stay within the given timeline and budget, comprehensive testing on big data requires vast and expert understanding. You may discover the top approaches for testing big data applications from a dedicated team of QA professionals.
QA Community Newsletters
QA Community Newsletters
2,511 followers
+ Subscribe
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
8
1 Comment
Gaurav Saini
SEO Executive | SEO Analyst | Open for Link Exchange Opportunities | Online Entrepreneur
9mo
Report this comment
Are you seeking the best data software testing company for your industry? Based on my research across multiple companies, I recommend considering AppSierra for big data software testing.
AppSierra
specializes in comprehensive big data software testing, ensuring the reliability, scalability, and performance of data-driven applications. With advanced tools and methodologies, they meticulously validate data processing, storage, and analytics systems. Their tailored approach mitigates risks, enhances data quality, and optimizes the functionality of big data solutions for seamless operations.

You can contact them directly through their website:
https://www.appsierra.com/services/big-data-testing
Like
Reply
1 Reaction
To view or add a comment,
sign in
More articles by this author
No more previous content
From Automobiles to Software Testing: Mohammad’s Inspiring Career Transition
Dec 6, 2024
40,000 Reasons to Be Grateful
Nov 29, 2024
HTTPS Status Code!
Nov 22, 2024
🎙️ 3rd Episode of Coffee With Tester is Here!
Nov 15, 2024
🚀 Exciting News! We just launched a new episode on our YouTube channel: Coffee with Tester! ☕️🎉
Nov 8, 2024
How to Inspect Element on Mac: A Step-by-Step Guide
Nov 1, 2024
Benchmark Testing: A Guide to Measuring and Optimizing System Performance
Oct 25, 2024
Coffee With Testers: Your Daily Dose of Testing Insights
Oct 18, 2024
What Is Fuzz Testing, And How Does It Work?
Oct 11, 2024
15 Best React Testing Libraries for Developers in 2024
Oct 4, 2024
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
A tale of Data Engineer - Small Data to Big Data  - Enterprise Strategy - Application Performance Testing,"A tale of Data Engineer - Small Data to Big Data  - Enterprise Strategy - Application Performance Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
https://www.linkedin.com/in/osamamunir/
A tale of Data Engineer - Small Data to Big Data  - Enterprise Strategy - Application Performance Testing
Report this article
Osama Munir
Osama Munir
Transformational Sr. Tech Exec | Trusted Cloud & AI Strategist | 11x AWS Certified | FinOps Expert | Data Storyteller
Published Aug 21, 2020
+ Follow
Big Data analytics is a pure example of ""Necessity is a mother of invention"". Although for me it's not an invention rather a long journey to find the right solution.
This article is very close to me as it takes me to 14 years of my career in B2B2C domain. I worked in different countries, continents, and face to face roles with different customers. Every customer project is unique. In this scenario where the customer is sold a solution built out of 'n' number of products from different companies and then making sure that you achieve dimensioning numbers agreed with the customer. It is an observation that not one T-Shirt fits all customers. There is reuse but that is minimal. You need to do a lot of protocol adaptation to make the solutions integrated to the customer environment.
While these all solutions are so custom that either you follow waterfall or agile delivery, the panic button is around performance tuning. Always a slogan ""I used best in class technology"" and still there are non-functional issues. My article specifically deals with the non-functional aspect of my journey from creating a Unix script to get data metrics to the usage of data pipes, lakes, batch, stream processing, and visualization.
Taking to 2008 and so, whenever a code is delivered from development as an integrator, we had to write an extensive script on the logs being written by per application. This script used to parse each line of the logs and get the information needed for the collection of matrices. Now that within a single data center there are 'n' machines / VM's / Containers and each generates a huge amount of logs. By then, you had to use your eyeballs to make some judgments and collect all data.
Later, I learned about consolidating all the data back to a single machine and let that machine perform all the parsing of logs. That resulted in me having to see the performance in one place. Now logs are centralized for applications but what about the system performance. The time I am talking about the only way was to use top and other commands of Unix and get that data with snapshots. No real-time analysis and plots are available. I must have spent a good amount of time playing around with those environments and certifying that we are good to go in production.
With time, we got new tools in the market. I would like to say about Prometheus which greatly changed life for people who are responsible for application performance. It helped to integrate all applications based out of Java-like Kafka, Springboot, Jboss, Hazelcast, and so on for the JVM performance and parameters then. Also with node exporter version of Prometheus, it is an excellent tooling for systems real-time performance. That solves the mystery of monitoring applications and systems. Visualization can be done over Grafana as it has a lot of reuse for dashboards available.
Similarly, for the application logs still, the tale is not solved. One way we solved the problem is to parse the logs in the Unix scripts, convert them to useable format, push them to (TSDB) time-series database and then visualize them over Grafana. This is a very time taking process for the first time to have logs in the right format and then Unix script has to be done every time there are new logs. Once the logs integration is done then the solution works fine. But with rapidly changing world and applications this solution is not agile enough.
To solve it, I found ELK as a charm that works out of the box. Beats component is an agent that runs on the remote machine and sends the raw logs to Logstash. Logstash cleans the raw logs to a refined log and with the usage of filters like grok. No development is needed and you can format logs as needed. Logstash pushes formatted logs to Elastic which converts the formatted logs to JSON and indexes them for visualization. The display for the indexed data is done on the Kibana which is a very nice tool. Kibana comes with observability as well as Application performance monitoring. With these tools, you can also perform the tracing in application to see how much application took time for different endpoints and classes.
So, in reality, it looks like my life is now wonderful. No, not yet. Data centers are generating TB of data every day and we can not have infinite storage in private data centers. Also, is this Penta bytes of data useful? How I can determine which data is important to keep and which to delete. Here comes the twist that why I had to write so many paragraphs above to explain. Big data with stream and batch processing solves the mystery here. In my view, after cleansing the logs they must be sent to a data lake where each source is a data pipe to reside. I need to be able to make a correlation between all data pipes which are sources of data like applications, DB, systems, servers, switches, storages, etc. and process them as needed. I can also imagine that I will need multiple levels of processing like Map/Reduce i.e. Apache Spark/Hadoop/Beam/Databricks first data lake to have the cleansed data. Then the second level of the data lake to get processed with aggregated data. And then I can present the same data to AI for further analysis or to data scientists. Similarly, either we use Kibana, Grafana, Power BI, Data Studio, or any other best in the class tool it needs the data to be aggregated.
I am currently, working on the last stage and the idea is to include machine learning to predict the anomalies between applications, systems, and DB pipelines. Per release to customer once ran with performance test a report to be generated that can pinpoint the vulnerabilities and give the analysis that where the stacks are taking more than normal time.
Again all the above must have been done by many. My two cents that it took me 14 years to reach to a point where I see that no cloud-native / digital disruption solution can run without analytics for data. Today, 90% of the data is unprocessed and the main reason is that organizations don't have the energy and resources to have the right set of competencies to get the real value out of it. Indeed Machine Learning and AI will play major roles but in the end, humans will be still needed to find the monetization layers around the new domains.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
21
To view or add a comment,
sign in
More articles by this author
No more previous content
12-Factor Apps: Is it really an App or something else we are taking about ?
Jul 6, 2020
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
"John Steiner, EMBA on LinkedIn: Putting fleet data quality to test for fleet risk | Geotab","John Steiner, EMBA on LinkedIn: Putting fleet data quality to test for fleet risk | Geotab
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
John Steiner, EMBA’s Post
John Steiner, EMBA
Building an incredible team of experts in accident reconstruction, vehicle event data recorders, forensic media analysis, and expert testimony
1mo
Report this post
Since the late 1990s, I've been deeply involved in the world of vehicle data recorders, from passenger EDRs to heavy vehicle HVEDRs. I've participated in early Crash Data Retrieval training and served on SAE committees, and now, I'm proud to highlight our decade-long collaboration with
Geotab
, a global leader in GPS telematics.

The team at Mecanica are world-class experts in accident reconstruction and in the complex task of accessing, preserving, and analyzing data from a wide range of vehicle recorders.  The amount of data available on vehicles today is staggering, and our team is driven by the challenge of  helping our clients understand exactly how and why a crash or other incident occurred.

We're honored to collaborate with the talented team at
Geotab
, who are making significant strides in fleet efficiency, sustainability, and most importantly, highway safety.
#Geotab
#FleetManagement
#Telematics
#DataAnalysis
#AccidentReconstruction
#EDR
#HVEDR
Mecanica Scientific Services Corporation
747 followers
1mo
For over ten years, Mecanica Scientific Services Corporation has collaborated with
Geotab
, a leader in GPS fleet telematics. We're proud to contribute to Geotab's commitment to data quality, a cornerstone of effective fleet management.

In 2022, Geotab published a white paper, ""Putting Fleet Data Quality to the Test,"" showcasing our testing of their curve-based algorithm. The results? Impressive data quality and reduced server load.

This partnership underscores our dedication to innovation and excellence in telematics and highway safety.
https://lnkd.in/gV6PibfP
#Geotab
#FleetManagement
#Telematics
#DataQuality
#Innovation
Putting fleet data quality to test for fleet risk | Geotab
geotab.com
35
6 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Francisco Javier Carrión Viramontes
Coordinador de Ingeniería Vehicular e Integridad Estructural del Instituto Mexicano del Transporte
1mo
Report this comment
Buen trabajo
Like
Reply
1 Reaction
2 Reactions
Kurt Weiss
Collision Reconstructionist and Forensic Engineer at Case Study Collision Science, LLC
1mo
Report this comment
Excellent work John. Great news!
Like
Reply
2 Reactions
3 Reactions
Yessika Acosta Anguiano
Jefe de Desarrollo Comercial en Femsa | Mecánica Tek
1mo
Report this comment
¡Congrats John! 🙌🏻
Like
Reply
1 Reaction
2 Reactions
Malena Kinsman
Litigation Management Counsel - Selective Insurance
1mo
Report this comment
Excellent work John and to the team at Geotab!
Like
Reply
1 Reaction
2 Reactions
Andy Nickerson
Director, Strategic Accounts @ Netradyne | Top Producing Sales Executive - Commercial Fleet Risk - AI and Edge Computing
1mo
Report this comment
Congratulations John!
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More Relevant Posts
Kestrel Insights
1,330 followers
1mo
Report this post
Ready to boost your fleet efficiency with smarter location tracking? Our
Motive
integration makes geofencing effortless, precise, and scalable.

With our automated polygon geofencing embedded in Motive, users can:

- Eliminate manual geofence setup processes — automate and save hours.
- Get precise tracking for arrivals and departures.
- Maximize the telematics experience with accurate, data-driven location intelligence
- Scale as your operations expand, adapting to new sites without hassle.

Whether you’re managing a single fleet or a nationwide operation, this integration brings you the power and precision of automated geofencing. With
Kestrel Insights
, Motive users can handle location needs confidently and efficiently — no matter the scale.

Explore the full capabilities and see how this integration can refine your fleet strategy:
https://lnkd.in/gb3Q8kbF
#motive
#geofencing
#data
#supplychain
#logistics
#shipping
#fleet
#fleetmanagement
#trucking
#transportation
Unleashing Telematics Synergy: Announcing Our Newest Integration | Motive
kestrelinsights.com
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Kestrel Insights
1,330 followers
2mo
Edited
Report this post
Kestrel Insights
automated polygon geofencing is now integrated into the
Motive
platform! Through this integration, Motive users can achieve greater precision and efficiency with geofencing.

Here’s how our geofencing solution benefits Motive users:

- No more manually drawn geofences — automate the process to save valuable time.
- Get pinpoint accuracy for arrivals and departures, minimizing errors and improving decision-making.
- Unlock the full potential of your telematics system with advanced geofencing capabilities.
- Scale to meet growing location needs with precision and simplicity.

At Kestrel Insights, we’re constantly pushing the boundaries to deliver the tools you need for success in freight and fleet management. This integration with Motive is just one way we pursue our mission to bring you cutting-edge geofencing technology.

Curious to learn more? Get all the details here:
https://lnkd.in/gb3Q8kbF
#motive
#geofencing
#data
#supplychain
#logistics
#shipping
#fleet
#fleetmanagement
#trucking
#transportation
Unleashing Telematics Synergy: Announcing Our Newest Integration | Motive
kestrelinsights.com
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Trimble Transportation
18,057 followers
2mo
Report this post
Today we announced an agreement with
Platform Science
to transform the commercial driver experience across the global transportation industry. As part of the agreement, Trimble and Platform Science will bring together our telematics businesses to create a safer, more efficient and more compliant transportation ecosystem.

“We believe combining our global transportation telematics portfolio with Platform Science’s will further advance fleet mobility and provide our customers with a broader portfolio of solutions to solve industry problems,” says our CEO
Rob Painter
. “Increased collaboration between the new Platform Science business and Trimble’s remaining transportation businesses will enhance our ability to provide positive outcomes for our global customers of commercial mapping, transportation management, freight procurement and visibility solutions.”

Read the full release to learn more about Trimble and Platform Science’s shared vision to transform transportation:
https://lnkd.in/gPsWhX44
Platform Science to Acquire Trimble's Global Transportation Telematics Business Units to Drive the Future of Transportation In-Cab Technology
investor.trimble.com
264
6 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Argos Connected Solutions
1,060 followers
9mo
Report this post
Coming out of
GEOTAB
Connect, there are a number of big changes that will be rolled out in the coming months.

One that you don't have to wait for is access to GO Anywhere, the latest asset-tracking device from Geotab.

Whether you are tracking trailers, construction equipment, or your service fleet, these devices can help you improve asset visibility, utilization, and routing.

Get the details on all of the changes and be sure to connect with our team if you are interested in learning more about GO Anywhere.
#fleetmanagement
#telematics
#assettracking
What’s new in MyGeotab — February 2024 | Geotab
geotab.com
8
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ron Heijman
2mo
Edited
Report this post
Transporeon
Trimble Inc.
Platform Science
Today we announced an agreement with Platform Science to transform the commercial driver experience across the global transportation industry.

An other big step to bring logistics in Sync with a sustainable world !!!  

Read the full article below !!!
Trimble Transportation
18,057 followers
2mo
Today we announced an agreement with
Platform Science
to transform the commercial driver experience across the global transportation industry. As part of the agreement, Trimble and Platform Science will bring together our telematics businesses to create a safer, more efficient and more compliant transportation ecosystem.

“We believe combining our global transportation telematics portfolio with Platform Science’s will further advance fleet mobility and provide our customers with a broader portfolio of solutions to solve industry problems,” says our CEO
Rob Painter
. “Increased collaboration between the new Platform Science business and Trimble’s remaining transportation businesses will enhance our ability to provide positive outcomes for our global customers of commercial mapping, transportation management, freight procurement and visibility solutions.”

Read the full release to learn more about Trimble and Platform Science’s shared vision to transform transportation:
https://lnkd.in/gPsWhX44
Platform Science to Acquire Trimble's Global Transportation Telematics Business Units to Drive the Future of Transportation In-Cab Technology
investor.trimble.com
7
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Lytx, Inc.
33,801 followers
6mo
Report this post
One powerful
#videotelematics
solution! We are excited to see this new plan with
GEOTAB
come to life for Latin America. A revolutionary all-in-one video telematics solution now has Zero Upfront Cost. 
Learn more from the link below, you won't want to miss out on this opportunity!
#ArtificialIntelligence
#OperationalEfficiency
#FleetManagment
Geotab
73,698 followers
6mo
Edited
🚚 Did you know that you can now implement the
#videotelematics
solution, with zero upfront cost? 💰

In an industry where movement is constant and innovation is the key to progress, a revolutionary solution emerges: Geotab and
Lytx, Inc.
Two leaders, one powerful video telematics solution! Together, we offer a video telematics solution that provides valuable video-based information and advanced risk detection.

Don't miss this opportunity to improve the safety and efficiency of your fleet with Geotab! Learn more information at:
https://lnkd.in/ggtjsMZG
Sean Killen
#VideoTelematics
#ZeroUpfrontCostPlan
#Geotab
#FleetManagement
#ArtificialIntelligence
#OperativeEfficiency
Geotab® Introduces All in One Video Telematics Plan with Zero Upfront Cost | Geotab
geotab.com
14
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Bianca Barbu
Marketing Professional | Driving Revenue Growth and Customer-Centric Solutions | Expert in IT & Software Marketing | Transforming Business Development & Innovation |  Communication Professional | Wine Communicator
6mo
Report this post
Tip 7: Prioritize Data Accuracy – Better No Data Than Fake Data

Ever faced a situation where the data from your telematics solution didn't match reality? Inaccurate data can lead to costly mistakes and operational inefficiencies. Discover why prioritizing data accuracy is crucial and how it impacts your fleet management decisions. 
Read my latest article for insights and a real-life story from the field (no real names provided).
#Telematics
#DataAccuracy
#FleetManagement
#BusinessSuccess
Tips and Tricks: Prioritize Data Accuracy – Better No Data Than Fake Data
https://biancabarbu.com
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Scott VanMeter
Trusted Advisor | GEOTAB Fleet Telematics Partner | Efficiency and Safety Expert
9mo
Report this post
GEOTAB
is unrivaled in the public sector space. Why?

I could list a dozen reasons. 

But it is their commitment to creating an ecosystem that truly stands out.

The ability to connect different components allows you to address each area of your fleet management strategy, and not get bogged down.

And with the new tools unveiled at Connect last week, the platform is only going to get better.
#fleetmanagement
#government
#publicworks
#telematics
Geotab Wins US General Services Administration Contract for Fleet Management
government-fleet.com
48
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Geotab
73,698 followers
9mo
Report this post
Are you already aware of the benefits that
#Videotelematics
can bring to your fleet? At Geotab, we are dedicated to the safety and efficiency of your assets, which is why we’ve compiled a list of some of the advantages our technology provides:
✔ Enhanced Safety
✔ Cost Reduction
✔ Increased Productivity
✔ Regulatory Compliance
✔ Reputation Protection
It's more than just a tool; it's a strategic partner for fleet managers aiming to optimise efficiency and safety. Are you prepared to make the jump?
https://lnkd.in/gjSJ_VMN
#FleetManagement
#Efficiency
#RoadSafety
Master fleet safety and efficiency with video telematics: Download our eBook today. | Geotab
geotab.com
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Track My Truck
255 followers
9mo
Report this post
GPS technology plays a pivotal role in revolutionizing vehicle maintenance, and Track My Truck is at the forefront of this paradigm shift. By leveraging real-time tracking and data analytics, the platform enables businesses to proactively manage and streamline their fleet maintenance processes. 
This not only minimizes downtime by predicting maintenance needs but also enhances overall operational efficiency, making Track My Truck a key player in optimizing vehicle maintenance from a GPS perspective.
.
.
.
http://www.trackmytruck.us
#fleetmanagement
#gps
#routeoptimization
#fieldservice
#maintenancetracking
#telematics
#safetydriving
#realtimemonitoring
#assetmanagement
#cameratechnology
#insuranceclaims
#smartrouteplanning
#fuelsavings
#transparency
#accuratebilling
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
2,040 followers
161 Posts
View Profile
Connect
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Sign in to view more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Software Testing Challenges with Big Data,"Software Testing Challenges with Big Data
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Software Testing Challenges with Big Data
Report this article
Rahul Malhotra
Rahul Malhotra
Web Specialist For Netwrix (Netwrix PolicyPak Product)
Published Jan 9, 2017
+ Follow
Businesses love having access to big data, but that environment presents many software testing challenges. I work with more volume today than I have over the past decade combined, and my job duties require vastly different solutions and skill sets than ever before, too. The biggest big data software testing struggles that I see come from organizations that undervalue testing, are seemingly daunted by the changed testing landscape into activity, and perhaps lack the right expertise on their testing teams.
Importance of Big Data Testing
EMC predicts a
40 percent data growth rate
for the near future, reaching 44 trillion gigs by 2020. Everywhere I look, big data is already playing a key role in enterprise operation across multiple levels.
Analytics solutions leveraging big data can help organizations discover everything from the best types of teams to put together for a project, to the likelihood of someone calling out sick.
As new technology develops around big data, companies will become more dependent on the information generated by these systems.
The quality of this data makes a substantial difference in software effectiveness. If I start with the incorrect information, someone in the organization is going to run into problems down the line. Everyone is still finding their way around the big data world, making this the perfect time to establish the place of testing as a vital piece of the puzzle.
Challenges of Big Data Testing
Some companies expect big data testing to use similar tools and processes as other types, but I find this approach runs right into big challenges. The decision-makers in charge of creating workflows and procuring solutions may look at structured databases as a model. Big data mixes structured and unstructured together, so it's an entirely new ballgame.
My QA team also doesn't have the luxury of testing the complete data set, since that might be billions of gigs. We need ways to identify the correct segments to test, verifying that the data meets the company's requirements, dealing with unstructured data and working in a specialized test environment.
The most important information in a particular data set depends on the teams, departments and external partners accessing it. Sales is far more interested in the revenue generated per client, while accounting wants to know how quickly they pay invoices. I have to coordinate with each stakeholder to run the appropriate tests for a given situation.
If I don't have a standardized process for requesting these details around the organization, I have to re-invent the wheel with each project. Big data offers a lot of potential for companies, but only if they know how to avoid this type of inefficiency.
Skill Sets Required for Big Data Testing
The technical expertise necessary to effectively test big data is also different. I'm working with data that's handled much differently than a typically structured database, with new priorities, technology and workflows.
I've noticed that one of the most important skills for effectively testing big data is a willingness to change one's mindset.
I know it's hard going through years of working with structured data in a relational database, and then learning to undo the habits I formed. My go-to solutions no longer applied to the situation, and there was a lot of thinking outside the box involved.
Understanding how to work with Hadoop and the
ways it interacts with large data
sets provides relevant knowledge that I use on the job regularly. The framework plays an essential role in allowing businesses to gain insights into the collected data, so I run into it a lot.
Quality assurance teams face many challenges when it comes to software testing in the big data world. Organizations will have to take time and effort to adjust to the new testing reality, but they will survive in the long run only if they work with quality data.
About the author
With 8+ years of experience in the field of software quality assurance, I have worked on QA projects for domains such as insurance, ERP, and e-commerce. Having worked on both functional and nonfunctional testing types, I have experience in creating and executing test plans, as also in defect documentation and test result reporting. I have been an avid cricket fan since my school days, and I still make the time to play on my days off, when I am not hunting bugs at
SQA WORX Solutions Private Limited
.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
7
1 Comment
kulwinder singh
Business Owner at Self Employed
7y
Report this comment
Great work and good efforts
Rahul Malhotra
Like
Reply
1 Reaction
To view or add a comment,
sign in
More articles by this author
No more previous content
Learning Docker : A tester's take - II
Sep 20, 2020
Learning Docker : A tester's take- I
Aug 22, 2020
Maybe we should stop testing...right about now
Apr 3, 2017
Sure, I can think in pictures, but how do I use this to map my test cases?
Mar 13, 2017
7 reasons for why you need to work on testing hardware-dependent software
Feb 13, 2017
Here’s why you should factor in Accessibility Testing in your test plan
Dec 5, 2016
Here's what to keep in mind during Functional testing
Oct 31, 2016
It is the (user) journey that counts… - How human factors affect testing
Sep 26, 2016
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data Test Environment Setup,"Big Data Test Environment Setup
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Test Environment Setup
Report this article
Rohini Gopal
Rohini Gopal
Driving Automation Excellence | Quality Engineering Pro | Advocate for AI & Cloud Technologies
Published Jun 30, 2018
+ Follow
Setting up a test environment is biggest challenge as Big Data involves handling huge volume and processing across multiple nodes. Setting up environment on cloud will help in optimizing infrastructure. Key points to be considered while setting up test environment on cloud would be
1.      Assessment of required Infrastructure
We need to evaluate the requirements for data processing, number of nodes required and software inventory. We also need to understand the Data privacy to decide on whether to opt for Private or Public cloud.
2.      Design of Test infrastructure
Firstly, document the high level test strategy, release cycles, types of testing and what is the volume of the data going to be tested. Accordingly document the requirement for RAM, disk space for each node in the cluster. Finally document the plan for environment refresh, maintenance and communication.
3.      Implementation & Maintenance  of Infrastructure
Create the instance, install the Hadoop components as per the design. Perform smoke test on the environment by running small map reduce, pig/hive jobs. Finally deploy the code to perform testing.
Its equally very important to establish the data quality requirements for different data sources so that we can spend time on verifying the transformation logic . An Automated regression suite should be built to use it after each release as database will be updated multiple times during the testing.
It is very important for organization to invest in building skill set in both development and testing. Big data testing is specialized stream and testing team should be built with diverse skill set which should include coding skills, white box testing skills and data analysis skills for them to perform better in identifying quality issues in data.
Other related articles by me
Overview of Big Data Testing
Exciting Era for Testing
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
2
To view or add a comment,
sign in
More articles by this author
No more previous content
Head-to-Head: The Best Features of Selenium 4 vs. Playwright for Modern Test Automation
Nov 8, 2024
Becoming a Full Stack QA Engineer: A Comprehensive Guide to Mastery
Nov 1, 2024
Techniques for Inspecting Toaster Messages in DevTools
Oct 23, 2024
Maximizing Efficiency and Accuracy: The Essential Role of Automation in Software Quality Assurance
Oct 17, 2024
How to ensure effective test coverage across manual, automation, and performance testing,
Oct 16, 2024
AI-Powered Test Case Generation in New Systems: A Different Perspective Beyond Historical Data
Oct 14, 2024
The Future of Quality Engineering with Generative AI (GenAI)
Oct 4, 2024
Dockerfile for Running N Tests from a Repository
Jun 14, 2023
Mastering Test Case Design: Techniques, Code Examples, and Real-world Applications
Jun 1, 2023
Revolutionizing Test Automation with Artificial Intelligence and Machine Learning
May 29, 2023
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
The Power of DNA Self-Testing and Big Data,"The Power of DNA Self-Testing and Big Data
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
lagrangianpoints.com
The Power of DNA Self-Testing and Big Data
Report this article
Sven Jensen
Sven Jensen
Kommando Cyber- und Informationsstand Analyst @ Bundeswehr Wir. Dienen. Deutschland | Enterprise Analytics Deployment
Published Aug 22, 2016
+ Follow
These are interesting times. We already know about the Retail and CPG market and the power of BIG Data technology. Just recently I commented on a post about a health care billing transparency application and its real power
www.linkedin.com/hp/update/6170954503441899520
.
Over the weekend we had an interesting debate about the fun of self-DNA testing. After we figured out how mixed we are we came to the realization that there is REALLY no reason to identify with each other by race or other classifications as we are all somehow in some way related. Also, some serious opportunities came to mind. It is extremely interesting to see where we all came from. But take a step back and start thinking about all this data and how easily it can be used with today’s technologies. As the referenced article discusses
http://www.fastcompany.com/3062731/most-innovative-companies/23andmes-consumer-dna-data-gold-mine-is-starting-to-pay-off
, some companies are sitting on goldmines, if organizations, researches and universities broaden their horizon. For example, we now easily can link information from our Fitbit
https://www.fitbit.com/
or similar tools to our DNA. We could use this information for proactive disease management. Now, if researchers combine this Big Data and potentially link this to individual depersonalized electronic health care records then we can start thinking about personalized proactive medical treatment regimens. Similar to Retailers who are starting to think about the “Brand of One” (person) we can also start thinking about the “Patient of One” (person). Now, let’s go a bit further. Once we know where we genetically are coming from we can also learn to link the DNA to historical events or geographical human traits and design proactive health treatment plans or products that cater to, for instance, ethnical preference. The Big Data version of “Anthony Bourdain Parts Unknown”
http://www.cnn.com/shows/anthony-bourdain-parts-unknown
so to speak.
You may ask: What’s the point. This is too far in the future or this is old news for us. I am not sure that “Main Street” has fully taken the advantage of the power (and risk) of Big Data and the INTEGRATION of data across domains as well as  the use of information for more than marketing campaigns. “Main Street” companies have huge data assets “in-house” for free(!) that can be business accelerators if combined with other external data. I realize many companies are still wrestling with implementing BI profitably. While this is tremendously important it is even more important to think about Big Data Analytics as a business engine NOW because the competitive advantage (and implied risks) as enterprises that become a Big Data grounded company will leave everyone else behind. Maybe there is a second or a third but the forth will be way behind. I would argue that companies may want to look at their organizations, consider the scope and who should lead this new Data Function. New organizational structures and processes
www.linkedin.com/hp/update/6165182854675324929
may have to be put in place today to not be left behind as early as tomorrow.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
24
To view or add a comment,
sign in
More articles by Sven Jensen
Advanced Analytics & Big Data are the Catalysts for the Digitization for Main Street Enterprise
Jul 30, 2016
Advanced Analytics & Big Data are the Catalysts for the Digitization for Main Street Enterprise
One of the biggest challenges for Analytics has been the Business System Analyst (BSA), the Business Analyst (BA) and…
30
3 Comments
Big Data and Advanced Analytics is Powering CNN’s Magic Wall
Apr 7, 2016
Big Data and Advanced Analytics is Powering CNN’s Magic Wall
Big Data is everywhere. I am fascinated about the speed at which election estimates, predictions and results are…
20
1 Comment
Big Data and IoT is transforming the Health Care Market
Mar 10, 2016
Big Data and IoT is transforming the Health Care Market
If anyone doubted that the Health Care market is not technology driven outside the OR was proven wrong by the planned…
16
1 Comment
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Datafold on LinkedIn: What is proactive data quality testing? 3 data quality principles for the…,"Datafold on LinkedIn: What is proactive data quality testing? 3 data quality principles for the…
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Datafold’s Post
Datafold
5,805 followers
6mo
Report this post
What does it actually look like in practice for data teams to *proactively* prevent data quality issues?

A proactive data quality strategy is governed by 3 key principles:

1. Shifting data testing to the left: Test early and often during the development cycle.
2. An essential data checklist before shipping code and data changes: Are your data changes acceptable and expected? What’s the downstream impact of your code change? How will it affect your infrastructure?
3. Leveraging automation: Do away with manual QA tests by automating checks during your CI process.

With testing workflows that make sense, checklists against errors, and an automated data quality system orchestrating everything, sit back and watch your developer velocity increase without sacrificing data quality.

For more proactive data quality testing information, check out our new Data Quality Guide 👇
https://lnkd.in/gBEH-eAU
What is proactive data quality testing? 3 data quality principles for the modern data team
datafold.com
17
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
Curiosity Software
4,983 followers
4mo
Report this post
💡Discover 8 criteria you should consider when looking for a modern test data solution:
https://hubs.ly/Q02JfB2P0
This article explores several test data management trends, deriving 8 criteria for designing an effective, modern test data solution.

These criteria point to one broad trend: The need to shift focus from Test Data “Management” and copying data, to Enterprise Test Data that streams data on-the-fly!

✅Learn more by reading the full blog!
#softwaredevelopment
#tdm
#testdatamanagement
#testdata
#qa
#qualityassurance
8 Criteria for a Modern Test Data Solution
curiositysoftware.ie
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Curiosity Software
4,983 followers
7mo
Report this post
💡Discover 8 criteria you should consider when looking for a modern test data solution:
https://hubs.li/Q02s-ggH0
This article explores several test data management trends, deriving 8 criteria for designing an effective, modern test data solution.

These criteria point to one broad trend: The need to shift focus from Test Data “Management” and copying data, to Enterprise Test Data that streams data on-the-fly!

✅Learn more by reading the full blog!
#testing
#softwaretesting
#softwaredevelopment
#tdm
#testdatamanagement
#testdata
#qa
#qualityassurance
8 Criteria for a Modern Test Data Solution
curiositysoftware.ie
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
PixelQA - Software Testing Company
527 followers
2mo
Report this post
Struggling with managing test data effectively? Check out our latest blog, ""Implementing a Test Data Management Strategy: A Comprehensive Guide,"" written by Rahul Patel. Learn how to streamline your testing process and improve data accuracy!
#TestDataManagement
#SoftwareTesting
#QualityAssurance
#TechGuide
#PixelQA
Implementing a Test Data Management Strategy
pixelqa.com
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Y42
7,849 followers
9mo
Report this post
Let's talk about a way to guarantee bad data never enters production - the Write-Audit-Publish (WAP) pattern.

We always recommend data practitioners to borrow more concepts from software engineering to build data products that are trusted by the broader business and, ultimately, save the cost of leaky data pipelines.

In our most recent article, we show how implementing the WAP pattern can ensure you're:
🤲 Always working on production data in an isolated environment (dev/staging/prod environments)
🤝 Collaborating securely with custom approval flows (GitOps)
✋ Preventing faulty builds from going into production (CI/CD)

Read on in our most recent GitOps for Data piece:
https://lnkd.in/exirNEsv
GitOps for Data - Enabling the Write-Audit-Publish pattern by default - Part 2 | Y42
y42.com
14
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Nicole Waters
Redgate | End-to-End Database DevOps
9mo
Report this post
During our most recent advocate office hours session, there was discussion around how Test Data Management is a relatively newer concept to the database world. I was surprised as it feels like a subject that comes up in almost all our conversations with customers as of late.
 
It's importance and adoption has been exponential - I found this article which I thought had a good outline for anyone like me looking to understand more about TDM, the importance of protecting sensitive data during the SDLC, and how to go about it.
#testdatamanagement
#TDM
#dataprivacy
#redgate
How to develop a test data management strategy | TechTarget
techtarget.com
19
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Curiosity Software
4,983 followers
3mo
Report this post
Are you aiming to modernise your test data strategy?💡Here's 12 reasons why Curiosity's Enterprise Test Data is the right tool for you:
https://hubs.li/Q02MGS0z0
Our team have been creating test data solutions since 1995, and our enterprise test data platform is the culmination of this decades-long history. 

We believe that our tools and services offer you a range of differentiating capabilities. In this blog, we summarise just 12.

✅Talk to a Curiosity expert to start your Enterprise Test Data journey today:
https://hubs.li/Q02MH1W00
#devops
#qa
#qualityassurance
#softwaredevelopment
#tdm
#testdata
#testdatamanagement
#data
12 differentiators of Enterprise Test Data
curiositysoftware.ie
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Kira F.
2mo
Report this post
What does it mean for something like data monitoring, which is inherently reactive in nature, to be proactive?

At Datafold, we're turning data monitoring upside down by shifting it left—starting with monitoring as early as in your upstream OLTP database.

Paired with data diffing and automated testing in CI/CD, Datafold now solves data quality for all core data engineering workflows. 

No more vendor overhead. Just one tool to maintain data integrity at scale.

Learn more about why we're solving for this, and how Datafold's monitors work:
https://lnkd.in/gm3D4HtV
From testing to monitoring: Datafold’s unified data quality platform | Datafold
datafold.com
15
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Bdix Live Tv
350 followers
8mo
Report this post
Automating data provisioning and cleansing
https://lnkd.in/gCzpBaef
#adviceguru
#automating
#automationtesting
#automationtestingcompany
#cleansing
#data
#provisioning
#softwaretestingcompany
#technologyblog
#testautomation
Automating data provisioning and cleansing
adviceguru.site
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
5,805 followers
View Profile
Connect
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Unlock more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Unit Testing and Integration Testing for Big Data Projects,"Unit Testing and Integration Testing for Big Data Projects
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Last updated on
      Sep 21, 2024
All
Big Data
How do you use unit testing and integration testing for Big Data projects?
Powered by AI and the LinkedIn community
1
Unit testing basics
2
Integration testing basics
3
Challenges of testing Big Data projects
4
Best practices for unit testing Big Data projects
5
Best practices for integration testing Big Data projects
6
Tips and tools for testing Big Data projects
7
Here’s what else to consider
Unit testing and integration testing are essential for ensuring the quality and reliability of any software project, but they can be especially challenging for Big Data projects. Big Data projects involve processing large volumes of complex and diverse data sources, often in distributed and parallel environments, using various frameworks and tools. How do you use unit testing and integration testing for Big Data projects effectively and efficiently? In this article, we will explore some of the best practices and common challenges of testing Big Data applications, and how to overcome them.
Top experts in this article
Selected by the community from 43 contributions.
Learn more
Nebojsha Antic 🌟
🌟 Business Intelligence Developer | 🌐 Certified Google Professional Cloud Architect and Data Engineer | Microsoft 📊…
View contribution
26
Iain White
Tech Consultant | IT Leader | Mentor | Virtual CTO | Leadership Coach | Project Manager | Scrum Master | IT Strategy |…
View contribution
24
Eduardo Brandao
Data Engineer | M.Sc. Big Data Analytics | Certified by Azure, AWS, GCP, Databricks, Airflow | KMP®| Lifetime Learner
View contribution
22
See what others are saying
1
Unit testing basics
Unit testing is the process of testing individual components or functions of a software project, in isolation from other dependencies and external factors. Unit testing helps to verify the correctness and functionality of the code, as well as to detect and fix bugs early in the development cycle. Unit testing can be done manually or using automated tools and frameworks, such as JUnit, TestNG, PyTest, or ScalaTest. Unit testing can also be integrated with continuous integration (CI) and continuous delivery (CD) pipelines, to ensure that the code is always tested and ready for deployment.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Nebojsha Antic 🌟
🌟 Business Intelligence Developer | 🌐 Certified Google Professional Cloud Architect and Data Engineer | Microsoft 📊 AI Engineer, Fabric Analytics Engineer, Azure Administrator, Data Scientist
Copy link to contribution
Report contribution
- 🔍 Unit testing verifies individual components in isolation, ensuring correctness and functionality early in development. Use frameworks like JUnit, TestNG, PyTest, or ScalaTest for automation.
- 🔄 Integration testing checks how components work together. It ensures data flow and interactions across modules are accurate and efficient.
- 🌐 In Big Data projects, integrate unit and integration testing with CI/CD pipelines. This continuous approach catches errors early, improves code quality, and maintains deployment consistency.
- 📈 Regularly update tests to reflect changes in data structures and algorithms, ensuring robust and reliable Big Data solutions.
…see more
Like
26
Eduardo Brandao
Data Engineer | M.Sc. Big Data Analytics | Certified by Azure, AWS, GCP, Databricks, Airflow | KMP®| Lifetime Learner
Copy link to contribution
Report contribution
Unit testing and integration testing are like quality control for your big data projects. 

Unit testing checks small parts of your code to make sure they work correctly. Integration testing checks how all the parts work together. 

Remember to use realistic data, automate your tests, and test your system under different loads.
…see more
Like
22
Mehdi TAZI
Chief Technical Officer | Data & Cloud Architect | BigData & NoSQL Expert | Author of 'The Definitive Guide to Data Integration' | Founder
Copy link to contribution
Report contribution
Unit Testing in the Big Data frameworks can be quite challenging, especially for those that are distributed in nature. As instance, when using technologies like Apache Spark, tests must be isolated to prevent one test from influencing the outcome of another. 
Addressing this challenge, require dedicated Spark Sessions, this is way it's important to use libraries like SparkTestingBase. The library provides essential base classes such as SharedSparkContext, which offers a unique SparkContext for every test scenario, and RDDComparisons, RDDGenerator, DataFrameGenerator, and DataSetGenerator, which aid in comparing and generating RDDs, DataFrames, and DataSets.
…see more
Like
8
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
Unit testing involves testing the smallest parts of an application independently to ensure they work as expected. For Big Data projects, this often means testing individual functions or modules that process data, ensuring they handle various data inputs correctly and return the right outputs. Frameworks like JUnit for Java or PyTest for Python are typically used.
…see more
Like
7
Aditya S
Data Scientist at Mu Sigma Inc | Machine Learning & Python Specialist | Turning Data into Powerful Insights for Strategic Success
Copy link to contribution
Report contribution
𝐔𝐧𝐢𝐭 𝐓𝐞𝐬𝐭𝐢𝐧𝐠 𝐢𝐧 𝐁𝐢𝐠 𝐃𝐚𝐭𝐚:

𝐏𝐫𝐞𝐜𝐢𝐬𝐢𝐨𝐧 𝐓𝐞𝐬𝐭𝐢𝐧𝐠: Isolate and test small code units for accuracy.
𝐌𝐨𝐜𝐤 𝐃𝐚𝐭𝐚 𝐕𝐚𝐥𝐢𝐝𝐚𝐭𝐢𝐨𝐧: Simulate varied inputs to ensure robust handling.

𝐈𝐧𝐭𝐞𝐠𝐫𝐚𝐭𝐢𝐨𝐧 𝐓𝐞𝐬𝐭𝐢𝐧𝐠:

𝐄𝐧𝐝-𝐭𝐨-𝐄𝐧𝐝 𝐕𝐚𝐥𝐢𝐝𝐚𝐭𝐢𝐨𝐧: Assess entire data flow for cohesion.
𝐒𝐜𝐚𝐥𝐚𝐛𝐢𝐥𝐢𝐭𝐲 𝐀𝐬𝐬𝐞𝐬𝐬𝐦𝐞𝐧𝐭: Verify system performance with substantial data.
Benefits:

𝐄𝐚𝐫𝐥𝐲 𝐈𝐬𝐬𝐮𝐞 𝐃𝐞𝐭𝐞𝐜𝐭𝐢𝐨𝐧: Detect and rectify bugs during development.
𝐑𝐞𝐥𝐢𝐚𝐛𝐢𝐥𝐢𝐭𝐲 𝐀𝐬𝐬𝐮𝐫𝐚𝐧𝐜𝐞: Build confidence in data processing reliability.
…see more
Like
6
Ujjwal Sontakke Jain
LWD - 13th January 2025 (Immediate Joiner) | AWS Data Engineer @HCLTech | LinkedIn Top Voice'2024 🏅| PySpark | SparkSQL | Python | SQL | HDFS | AWS | Databricks | 130K+ Post Impressions
Copy link to contribution
Report contribution
Unit testing for Big Data involves testing individual components like functions and methods, using frameworks like JUnit or PyTest. Integration testing checks the entire data pipeline's functionality from ingestion to storage, employing realistic datasets and simulating various conditions. Tools like MRUnit, Docker, and data quality frameworks aid in testing. Incorporating tests into CI/CD pipelines ensures reliability and quality in Big Data projects.
…see more
Like
3
Rob Boeyink ✪
LinkedIn 💡 Top Voice AI | Building HumanSwitch | Human Centric AI platform for SMEs | 25 Yrs AI Experience | Strategy | Entrepreneur | Innovator | Digital Transformation
Copy link to contribution
Report contribution
For Big Data projects, unit testing involves checking the smallest testable parts of your application, like functions or methods. It's crucial for ensuring each component works correctly independently, helping pinpoint errors without the complexity of the entire system. Automate these tests with frameworks like JUnit for Java, PyTest for Python, or ScalaTest for Scala, integrating them into your CI/CD pipeline. This guarantees your code remains testable and deployable, streamlining bug detection and fixing early on. Embracing unit testing not only boosts code quality but also enhances project maintainability and scalability.
…see more
Like
3
Qandil Tariq
Mobile Developer (Kotlin, Java, Swift) | Aspiring Data Analyst (Python, SQL, Machine Learning) | Passionate about Turning Data into Insights
Copy link to contribution
Report contribution
n Big Data projects, I use unit testing to validate individual components like data transformation logic and UDFs, ensuring accuracy on a small scale. For end-to-end workflows, integration testing ensures data flows seamlessly across systems like Hadoop or Spark, maintaining data integrity and performance. Combining both ensures a robust and reliable pipeline
…see more
Like
3
Andres D.
Data Platform Architect at PwC
Copy link to contribution
Report contribution
In some cases we can use pytest for unit testing in Databricks notebooks. We can schedule these to run however often we need them to do we can get alerted on data quality. The pipelines can run regularly or irregularly. Databricks has great features to tie certain jobs together so we can chain pipelines and workflows with certain types of tests.
…see more
Like
2
Gughapriyaa Elango
Big Data engineer
Copy link to contribution
Report contribution
Testing Data Processing Functions: In Big Data projects, functions responsible for data processing, such as filtering, aggregation, and transformation, are subjected to unit tests. For instance, a function designed to calculate average sales per customer would undergo unit testing to ensure accurate computation.

Validation of Algorithms: Algorithms utilized for data analysis and machine learning models are meticulously tested through unit testing to verify their correctness and robustness. This ensures that algorithms generate accurate insights and predictions when applied to large datasets.
…see more
Like
2
2
Integration testing basics
Integration testing is the process of testing how different components or modules of a software project work together, as a whole or in subsets. Integration testing helps to verify the compatibility and interoperability of the code, as well as to identify and resolve any issues that arise from the interaction of different parts of the system. Integration testing can be done manually or using automated tools and frameworks, such as Selenium, Cucumber, or Robot Framework. Integration testing can also be integrated with CI and CD pipelines, to ensure that the system is always tested and ready for delivery.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Constantine Shulyak
Author of $100M+ social project | Featured on Forbes | CEO at BLCKMGC
Copy link to contribution
Report contribution
In Big Data projects, integration testing verifies the compatibility and interaction of system components, ensuring seamless data flow.

Automated frameworks like Selenium and Cucumber streamline this process, validating data transformations and interactions across distributed systems.

Testing also extends to external integrations like databases and APIs, promoting reliable data exchange.

Incorporating integration testing into CI/CD pipelines facilitates early issue detection, accelerating the delivery of robust solutions.
…see more
Like
11
JL Suarez, MBA
🏆 50+ LinkedIn Top Voice: 🚀Manager of Enterprise Data Services at Holiday Inn Club Vacations🌐: Inspiring Innovation & Leadership in Business Intelligence📊
Copy link to contribution
Report contribution
Integration testing is crucial in Big Data projects, ensuring seamless interaction between various components, such as data ingestion, processing, and storage systems. Given the complexity and scale of environments, automated tools like Selenium and Robot Framework can significantly enhance efficiency and accuracy. Integrating these tests into CI/CD pipelines ensures continuous validation, making your system robust and delivery-ready. 🚀
Insight 1: Automate for Efficiency - Use tools like Selenium to streamline repetitive tasks.
Insight 2: CI/CD Integration - Continuous testing ensures your Big Data system remains reliable and scalable.
Insight 3: Focus on Interoperability - Verify that all components work harmoniously to avoid bottlenecks.
…see more
Like
11
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
Integration testing checks that different modules or services work together as expected. In Big Data projects, this can involve testing interactions between data pipelines, databases, and processing applications to ensure that data flows correctly through the system and triggers the appropriate actions at each step.
…see more
Like
6
Gughapriyaa Elango
Big Data engineer
Copy link to contribution
Report contribution
Pipeline Validation: Big Data systems typically consist of complex data pipelines involving multiple stages such as data ingestion, preprocessing, analysis, and storage. Integration testing validates the end-to-end functionality of these pipelines, ensuring data integrity and consistency throughout the process.

Compatibility Testing: Integration testing also addresses compatibility issues arising from the interaction between diverse technologies and platforms utilized in Big Data projects. It verifies interoperability between components deployed on different frameworks (e.g., Hadoop, Spark) and environments to prevent potential conflicts.
…see more
Like
2
Rahul Chaturvedi
Distributed Systems | Kafka | Streaming | Big Data | Cloud | Analytics
Copy link to contribution
Report contribution
Similar to other disciplines, in Big Data projects, unit testing validates individual components' functionality, while integration testing ensures seamless interactions between them. Containers provide an efficient means to conduct integration testing by encapsulating different components and their dependencies in isolated environments. This allows for realistic testing of data flows and interactions across the entire system, simulating production-like conditions without impacting other environments. By leveraging containers for integration testing, Big Data projects can verify the interoperability and reliability of their components within a controlled environment, ensuring the robustness and scalability of the overall solution.
…see more
Like
2
3
Challenges of testing Big Data projects
Testing Big Data projects can be a challenge due to the volume, variety, and velocity of data sources, making it difficult to create and maintain test data sets and handle data quality and consistency issues. Additionally, the distributed and parallel nature of Big Data processing can introduce concurrency, scalability, and performance issues, as well as increase the complexity and dependency of system components. Furthermore, the diversity and heterogeneity of Big Data frameworks and tools make it hard to standardize and automate the testing process, as well as ensure compatibility and interoperability of system components.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
Big Data projects present unique challenges for testing due to the volume, velocity, and variety of data. Ensuring test environments replicate the complexity of production systems and handling large datasets efficiently are major hurdles. Additionally, the dynamic nature of data flow and transformation in Big Data environments complicates test planning and execution.
…see more
Like
6
Rob Boeyink ✪
LinkedIn 💡 Top Voice AI | Building HumanSwitch | Human Centric AI platform for SMEs | 25 Yrs AI Experience | Strategy | Entrepreneur | Innovator | Digital Transformation
Copy link to contribution
Report contribution
Testing Big Data projects is challenging due to the sheer volume and diversity of data, making test data set creation and maintenance tough. Data quality assurance across vast datasets adds complexity. The distributed nature of Big Data introduces scalability and performance issues. Moreover, the wide range of Big Data tools complicates standardization and automation of tests, affecting system compatibility. Innovative strategies like synthetic data generation, extensive automation, and specialized tools are essential for effectively addressing these challenges and ensuring successful Big Data deployments.
…see more
Like
5
Anandaganesh Balakrishnan
AI and Data Engineering Leader | Innovator | Author | Carnegie Mellon Alum
Copy link to contribution
Report contribution
The key challenges of testing Big data projects are
1) Some of the errors related to resource utilization are harder to replicate as the root cause can be any of the reasons like skewed job schedule, Improper resource configuration, Not enough data for the stress testing, etc 
2) If the data lineage is not defined properly, it is hard to find the root cause of issues.
3) A huge volume of data needs to be tested.
4) Lack of constraints which can lead to bad data.
…see more
Like
3
Rahul Chaturvedi
Distributed Systems | Kafka | Streaming | Big Data | Cloud | Analytics
Copy link to contribution
Report contribution
Testing Big Data projects presents several challenges due to the scale and complexity of the data involved. Firstly, generating representative test data sets that mimic real-world scenarios can be difficult, as Big Data systems often deal with vast amounts of heterogeneous data. Additionally, orchestrating test environments that accurately simulate production configurations and data processing workflows can be challenging. Furthermore, ensuring comprehensive test coverage across various data processing stages, including ingestion, transformation, and analysis, requires careful planning and execution. Moreover, Validating results from distributed frameworks like Hadoop or Spark poses unique challenges, as traditional testing may not suffice.
…see more
Like
2
Gughapriyaa Elango
Big Data engineer
Copy link to contribution
Report contribution
Ensuring the quality and integrity of data throughout its lifecycle presents a significant challenge in Big Data testing. Data may undergo multiple transformations, aggregations, and analyses across distributed systems, increasing the risk of errors, inconsistencies, and data loss. Validating data accuracy, completeness, and consistency becomes crucial yet challenging in such dynamic environments.
…see more
Like
1
Rahul Chaturvedi
Distributed Systems | Kafka | Streaming | Big Data | Cloud | Analytics
Copy link to contribution
Report contribution
Testing Big Data projects presents several challenges due to the sheer volume, velocity, and variety of data involved. Ensuring the accuracy and reliability of data processing algorithms across massive datasets is complex, requiring specialized testing approaches and tools. Moreover, the distributed nature of Big Data systems introduces challenges in testing data integration, consistency, and performance across multiple nodes and clusters. Scalability testing becomes crucial to assess the system's ability to handle increasing data loads efficiently. And, in my experience, testing in environments that closely resemble production settings is essential to uncover issues related to data skew, resource contention, and fault tolerance.
…see more
Like
1
4
Best practices for unit testing Big Data projects
To address some of the difficulties with unit testing Big Data projects, it is recommended to use mocking and stubbing techniques to isolate and simulate external dependencies. Additionally, test-driven development (TDD) or behavior-driven development (BDD) methodologies should be implemented to design and write tests before or while writing code. Furthermore, code coverage tools and metrics like JaCoCo, Cobertura, or SonarQube should be used to measure and improve the quality of unit tests. Lastly, code review and code analysis tools like Checkstyle, PMD, or FindBugs should be used to ensure the readability, maintainability, and compliance of the code.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Mehdi TAZI
Chief Technical Officer | Data & Cloud Architect | BigData & NoSQL Expert | Author of 'The Definitive Guide to Data Integration' | Founder
Copy link to contribution
Report contribution
Please note that many data engineers thinks developing unit tests by running a spark job on a selected dataset or a generated one instead of mocking. This is a confusing between unit testing and the integration or end-to-end testing.
…see more
Like
7
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
For unit testing in Big Data, isolate the data processing logic from data sources. Use mocking frameworks to simulate data inputs and outputs. Ensure that tests are comprehensive, covering various data formats and edge cases. Keep tests fast and independent to support frequent execution and rapid development cycles.
…see more
Like
6
Gughapriyaa Elango
Big Data engineer
Copy link to contribution
Report contribution
Unit testing in Big Data projects involves breaking down code into smaller units, like functions or methods, and testing them in isolation to ensure they work as expected. It's crucial to generate diverse test data and use frameworks like JUnit to automate the testing process. By designing modular and testable code, teams can catch bugs early and integrate testing seamlessly into their development workflow, fostering collaboration and maintaining code quality throughout the project lifecycle.
…see more
Like
2
Rahul Chaturvedi
Distributed Systems | Kafka | Streaming | Big Data | Cloud | Analytics
Copy link to contribution
Report contribution
Firstly, focus on testing individual components or units in isolation, ensuring that each unit performs its intended functionality correctly. Mocking frameworks can be utilized to simulate external dependencies and isolate the unit under test. Secondly, prioritize testing critical business logic and data transformations to validate their accuracy and integrity for those individual methods/classes. Thirdly, automate unit tests to facilitate continuous integration and deployment pipelines, enabling frequent and reliable testing throughout the development lifecycle. Additionally, maintain a comprehensive suite of unit tests to cover edge cases, error handling, and boundary conditions, ensuring thorough test coverage.
…see more
Like
2
5
Best practices for integration testing Big Data projects
Integration testing Big Data projects can be challenging, but there are some best practices that can help. To ensure the validity and reliability of the test results, use test environments and test data sets that closely mimic the production environment and data sets. End-to-end testing and acceptance testing techniques should be used to verify the functionality and usability of the system from the user's perspective, as well as to validate the business logic and outcomes. Load testing and stress testing techniques can evaluate the performance and scalability of the system under different workloads and scenarios and identify any bottlenecks or failures. Regression testing and smoke testing techniques should be employed to make sure that the system works as expected after any changes or updates, as well as to detect and prevent any defects or errors.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
Ensure the test environment closely mirrors the production environment to capture any discrepancies early. Use controlled datasets that include scenarios for all possible interactions and data transformations. Automate integration tests to run regularly, allowing for early detection of integration issues, especially when components are updated.
…see more
Like
7
Gughapriyaa Elango
Big Data engineer
Copy link to contribution
Report contribution
A key practice is to simulate real-world scenarios by testing data pipelines, processing stages, and interactions between various systems. Additionally, setting up test environments that closely resemble production environments enables comprehensive testing of scalability, fault tolerance, and performance across distributed systems. By automating integration tests and collaborating closely between development, testing, and operations teams, organizations can detect and address issues early, ensuring the reliability and efficiency of their Big Data solutions.
…see more
Like
6
Cmdr (Dr.⁹) Reji Kurien Thomas , FRSA, MLE℠
I Empower Sectors as a Global Tech & Business Transformation Leader| Stephen Hawking Award 2024| Harvard Leader | UK House of Lord's Awardee | Fellow Royal Society | CyberSec | CCISO CISM CCNP-S CEH
Copy link to contribution
Report contribution
Employ data validation frameworks like Apache Griffin or Deequ to automate the validation of data quality & integrity throughout the pipeline. In a financial analytics project, we used Apache Griffin to set up data quality rules that validated the accuracy, completeness, and consistency of incoming transaction data. This framework automated the validation process & generated detailed reports, helping us quickly identify and correct data quality issues.

Perform incremental load testing to verify that the system can handle data updates and additions without disrupting ongoing operations. In a retail inventory management system, we conducted incremental load tests by simulating daily data loads of new and updated inventory records.
…see more
Like
5
Rob Boeyink ✪
LinkedIn 💡 Top Voice AI | Building HumanSwitch | Human Centric AI platform for SMEs | 25 Yrs AI Experience | Strategy | Entrepreneur | Innovator | Digital Transformation
Copy link to contribution
Report contribution
Integration testing for Big Data projects requires meticulous planning and execution. It's vital to replicate production environments accurately in test environments for reliable results. Techniques like end-to-end and acceptance testing validate functionality and usability from the user's perspective. Load testing and stress testing assess system performance and scalability under various scenarios. Regression and smoke testing detect and prevent defects post-updates. Implementing these practices diligently enhances testing reliability for Big Data projects.
…see more
Like
3
6
Tips and tools for testing Big Data projects
To make the testing process easier and more efficient for Big Data projects, there are some tips and tools one can use. Modular and decoupled designs for the system architecture can reduce complexity and dependency, as well as facilitate the testing process. Additionally, a common and consistent coding style and convention can improve readability and maintainability of the code, while also simplifying testing. Automation frameworks and tools should be used that support the Big Data frameworks and tools used for development, such as MRUnit, Spark Testing Base, or HiveRunner. Finally, a test management tool and platform should be used that enables collaboration and communication of the testing team, as well as tracking and reporting of the testing progress and results. Examples of such tools include TestRail, Zephyr, or Jira.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Iain White
Tech Consultant | IT Leader | Mentor | Virtual CTO | Leadership Coach | Project Manager | Scrum Master | IT Strategy | Digital Transformation | IT Governance | Agile | Lean | Theory Of Constraints | SaaS | Brisbane.
Copy link to contribution
Report contribution
In Big Data projects at White Internet, we emphasize modular designs and consistent coding practices to streamline both unit and integration testing. Using automation tools like MRUnit for Hadoop or Spark Testing Base for Spark significantly enhances our testing efficiency. These tools are tailored to Big Data frameworks, enabling thorough testing environments. We also integrate test management platforms like Jira to facilitate collaboration and ensure comprehensive tracking of test progress and outcomes. This approach not only improves code quality and system reliability but also accelerates the development cycle by allowing for early detection and resolution of potential issues.
…see more
Like
24
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
Utilize specialized tools designed for Big Data testing like Apache JMeter for performance testing or Apache MRUnit for testing MapReduce programs. Implement continuous integration to run tests automatically on every code commit. Consider using data anonymization or synthetic data generation to address privacy concerns while testing with real-world scale data.
…see more
Like
7
Rob Boeyink ✪
LinkedIn 💡 Top Voice AI | Building HumanSwitch | Human Centric AI platform for SMEs | 25 Yrs AI Experience | Strategy | Entrepreneur | Innovator | Digital Transformation
Copy link to contribution
Report contribution
To streamline testing for Big Data projects, adopting modular and decoupled system architectures is paramount. This approach minimizes complexities and dependencies, facilitating smoother testing processes. Consistent coding styles enhance code readability and maintenance, simplifying testing efforts. Utilizing automation frameworks like MRUnit, Spark Testing Base, or HiveRunner tailored to Big Data tools expedites testing procedures. Employing test management platforms such as TestRail, Zephyr, or Jira facilitates team collaboration, progress tracking, and result reporting, optimizing overall testing efficiency.
…see more
Like
3
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Mohammed Bahageel
Artificial Intelligence Developer |Data Scientist / Data Analyst | Machine Learning | Deep Learning | Data Analytics |Reinforcement Learning | Data Visualization | Python | R | Julia | JavaScript | Front-End Development
Copy link to contribution
Report contribution
Unit testing and integration testing are crucial for ensuring the correctness and reliability of big data projects. Unit testing involves testing individual units of code, such as data transformations and algorithms, to verify their behavior and correctness. Integration testing focuses on testing the interactions and compatibility between different components and systems, ensuring data flows correctly and components work together as expected. In big data projects, integration testing involves testing data pipelines, ETL processes, and distributed systems. These testing practices help identify issues like data inconsistencies, communication failures, and performance issues
…see more
Like
13
Ashish Joshi
Data Engineering Director at UBS | Cloud, Big Data & Analytics Leader | Agile & DevOps Transformation | Building Scalable Systems for High-Impact Results | Software Architecture Visionary
Copy link to contribution
Report contribution
Don't overlook the importance of non-functional testing aspects such as performance, scalability, and security. Performance testing should validate that the system meets the required response times, throughput, and can handle the load of real-world data volumes. Scalability testing ensures the system performs well under increasing data loads. Security testing identifies vulnerabilities in data handling and access controls.
…see more
Like
7
Cmdr (Dr.⁹) Reji Kurien Thomas , FRSA, MLE℠
I Empower Sectors as a Global Tech & Business Transformation Leader| Stephen Hawking Award 2024| Harvard Leader | UK House of Lord's Awardee | Fellow Royal Society | CyberSec | CCISO CISM CCNP-S CEH
Copy link to contribution
Report contribution
Break down the data pipeline into stages and test each stage independently. In a complex ETL pipeline using Apache Nifi, I created unit tests for each individual processor, such as data ingestion, transformation, and loading. This modular testing approach allowed us to identify and fix issues at specific stages of the pipeline, improving overall reliability.

Use integration tests to validate data consistency and accuracy across different stages of the pipeline. In a data warehousing project with Redshift, I wrote integration tests that compared input data with the final output. By validating that transformations preserved data integrity, we ensured that the entire data pipeline produced accurate and consistent results.
…see more
Like
7
Rahul Chaturvedi
Distributed Systems | Kafka | Streaming | Big Data | Cloud | Analytics
Copy link to contribution
Report contribution
We can leverage tools like Testcontainers for integration testing.  By integrating Testcontainers into the testing framework, developers can easily spin up lightweight, disposable containers that encapsulate the various components of their system, including databases, message brokers, and other dependencies. Testcontainers' flexibility enables developers to simulate real-world scenarios, such as network failures or resource constraints, ensuring the robustness and reliability of their Big Data solutions. Additionally, Testcontainers' ability to programmatically manage container lifecycles simplifies testing setup and teardown, streamlining the testing process and accelerating development cycles.
…see more
Like
4
Big Data
Big Data
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Big Data
No more previous content
How do you leverage Big Data frameworks and tools for debugging purposes?
25 contributions
How do you evaluate and compare different big data metadata catalog solutions and vendors?
5 contributions
How do you monitor and troubleshoot spark streaming jobs and pipelines?
24 contributions
How can big data and IoT support decision making and problem solving in complex and uncertain situations?
22 contributions
How do you optimize spark streaming performance and reliability in a distributed environment?
27 contributions
How do you measure the impact and value of metadata and catalog analytics for your organization?
44 contributions
How do you tune Kafka and Flume for high-throughput data ingestion?
27 contributions
How do you ensure data quality and consistency with spark streaming?
37 contributions
What are the pros and cons of using batch vs. stream processing for big data analysis?
4 contributions
How can Big Data and Machine Learning improve environmental sustainability and climate change mitigation?
2 contributions
No more next content
See all
More relevant reading
Web Applications
What GraphQL API testing strategies work best in a CI/CD pipeline?
Quality Assurance
What are the best practices for testing SOAP web services using Robot Framework?
Statistical Data Analysis
What are some best practices and tools for automating and streamlining regression testing in data analysis?
Functional Training
What are the advanced functional testing techniques to master?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
10
43 Contributions"
Big Data Testing: Complete Guide,"Big Data Testing: Complete Guide
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing: Complete Guide
Report this article
testRigor
testRigor
testRigor is the #1 generative AI-based codeless test automation tool for manual testers and product managers.
Published Sep 1, 2024
+ Follow
With the rise of the internet, social media, and digital devices, there’s a boom in data generation around us. Data has started coming from beyond traditional sources like spreadsheets. Think sensor data from IoT devices, social media feeds, and customer clickstreams.
Businesses need more than basic summaries. They crave deeper insights to understand customer behavior, optimize operations, and identify new opportunities. Traditional data tools were designed for smaller, structured datasets. Big data necessitated new technologies and approaches to capture, store, process, and analyze this vast and varied information. Big data fills the gap, allowing us to harness the power of this data deluge.
What is big data?
“
Big Data
” refers to extremely large datasets that cannot be effectively processed with traditional data processing techniques due to their
volume
,
velocity
, and
variety
. It’s different from regular data, primarily in scale and complexity.
3 V’s of big data
Types of Big Data
What is big data testing?
Big data testing primarily focuses on how well the application handles the big data. It ensures the application can correctly:
Ingest
the big data from various sources (like social media feeds and sensor data) despite its volume, variety, and velocity.
Process
the data efficiently, extracting meaningful insights without getting bogged down by the massive size.
Store
the data reliably and securely, ensuring its accuracy and accessibility for future analysis.
Output
the results in a usable format, providing valuable insights to users without errors or inconsistencies.
The goal is not only to ensure that the data is of high quality and integrity but also that the applications processing the data can do so efficiently and accurately.
Simple analogy
Imagine a chef preparing a giant pot of soup (
big data
). Testing the soup itself might involve checking for the freshness of ingredients (
data quality
). But big data testing is more like ensuring the chef has the right tools (
application
) to handle the large volume, maintain consistent flavors (
data processing
), and serve the soup efficiently (
application performance
).
So, you can say that big data testing is about making sure the application is a skilled chef who can work effectively in a big kitchen (
big data environment
).
How is big data testing different from data-driven testing?
The following table shows how these forms of testing vary.
Parameter: Focus
Big Data Testing:
Focuses on ensuring big data applications function correctly and efficiently when dealing with massive, varied, and fast-flowing data (volume, variety, velocity). It tests how the application interacts with big data throughout its lifecycle (ingestion, processing, storage, output).
Data-driven Testing:
Focuses on automating test cases using external data sources (spreadsheets, databases) to improve test coverage and efficiency. It tests the application’s functionality with various sets of input data to identify edge cases and ensure the application works as expected under different conditions.
Parameter: Data characteristics
Big Data Testing:
Deals with massive datasets that
traditional testing tools
can’t handle. It considers the unique challenges of big data like volume, variety, and velocity.
Data-driven Testing:
Typically uses smaller, structured data sets to automate test cases. The data itself is not the main focus; it’s a means to efficiently execute numerous test cases.
Parameter: Tools and techniques
Big Data Testing:
Often utilizes specialized tools and techniques designed to handle the scale and complexity of big data. These tools can handle data subsetting (using a smaller portion for testing) or data anonymization (removing sensitive information) to manage the data volume.
Data-driven Testing:
Leverages existing testing frameworks and tools with external data sources. The focus is on reusability and automating repetitive test cases with different data sets.
Parameter: Example
Big Data Testing:
Imagine testing a retail application that analyzes customer purchase data from millions of transactions daily. Big data testing would ensure the application can efficiently ingest, process, and analyze this massive data stream to identify buying trends.
Data-driven Testing:
Imagine testing a login functionality.
Data-driven testing
could use a spreadsheet with various usernames and passwords (valid, invalid, empty) to automate login tests and ensure the application handles different scenarios correctly.
Components of big data testing
Big data testing goes beyond just the data itself and focuses on how well the application interacts with the big data. The primary components are:
Test Data:
Representative sample data mimicking real-world big data is used for testing scenarios. This data should reflect the actual data’s volume, variety, and velocity to ensure realistic testing. Techniques like data subsetting or anonymization can be used to manage the scale of test data. Read:
Optimizing Software Testing with Effective Test Data Management Tools
.
Test Environment:
A dedicated environment replicating the production setup is crucial. This environment should have sufficient resources like storage, processing power, and network bandwidth to handle the demands of testing big data applications. Read:
What is a Test Environment? A Quick-Start Guide
.
Stages in big data testing
Types of big data testing
You’ll see the following types of testing commonly being done in big data testing:
Functional testing
You can consider this form of testing as the cornerstone of big data testing because it verifies if the application performs its intended tasks as expected across the entire big data lifecycle. It includes:
Data ingestion testing:
Checks if data is correctly extracted from various sources and loaded into the system.
Data processing testing:
Validates if data transformation and processing happen according to defined rules and logic.
Data storage testing:
Ensures data is stored reliably and securely in the big data storage system.
Data output testing:
Verifies if processed data is delivered in the desired format (reports, dashboards) to users. Read more about
Test Reports
and
Test Logs
.
Performance testing
Crucial for big data applications due to massive datasets, performance testing focuses on how the application behaves under various loads. This is assessed through:
Load testing:
Simulates real-world data volumes to assess processing speed, response times, and scalability.
Stress testing:
Pushes the application beyond normal loads to identify bottlenecks and ensure it can handle peak usage scenarios.
Volume testing:
Evaluates how the application performs with increasing data sizes to ensure it can keep up with data growth.
Data quality testing
Given big data often comes from diverse sources and can be messy, data quality testing focuses on the integrity of the data throughout the pipeline. This assessment happens through:
Accuracy testing:
Verifies if data is free from errors and reflects real-world values.
Completeness testing:
Ensures all required data elements are present and accounted for.
Consistency testing:
Checks if data adheres to defined formats and standards across different sources and processing stages.
Security testing
Big data security is critical as these systems often handle sensitive information. Security testing identifies vulnerabilities to prevent unauthorized access, data breaches, or manipulation. You can test data security through:
Vulnerability scanning:
Identifies weaknesses in the application and data storage infrastructure.
Penetration testing:
Simulates cyberattacks to assess the application’s ability to withstand real-world security threats.
Access control testing:
Verifies that only authorized users have access to sensitive data within the big data system.
How does big data testing help?
Big data testing can help covert your endeavors of harnessing the insights hidden in big data to transform your business.
Enhanced data quality:
Traditional testing methods might struggle with the sheer volume and variety of big data. Big data testing specifically addresses these challenges, ensuring data accuracy, completeness, and consistency throughout the processing pipeline. This prevents errors from creeping in and allows for more reliable insights based on clean data.
Improved application performance:
Big data applications deal with massive amounts of data flowing in at high velocity. Big data testing helps identify bottlenecks and inefficiencies within the application. By optimizing performance, you can ensure the application can handle these data demands efficiently, leading to faster processing times and quicker results.
Trustworthy results and insights:
Testing verifies the application’s functionality and the underlying data quality. This builds confidence in the results and insights generated from big data analysis. You can make crucial decisions based on reliable data, reducing the risk of basing strategies on misleading or inaccurate information.
Reduced risks and costs:
Big data testing acts as a proactive measure. By identifying potential issues early in the development process, it helps prevent problems from surfacing later in production. This reduces the risk of errors, system crashes, and costly downtime associated with fixing issues in a deployed application. Read more about
Risk-based Testing
.
Increased ROI from big data investments:
Big data initiatives can be significant investments. Through big data testing, you can ensure that these investments are worthwhile. If big data applications function correctly and deliver valuable insights, organizations can maximize the return on investment (ROI) from their big data endeavors. Read:
How to Get The Best ROI in Test Automation
.
Challenges in big data testing
Working with so much data can be a challenge, not just due to the sheer volume of data. Here is a list of these challenges.
Volume of data:
One of the most significant challenges is the sheer volume of data. Testing systems need to handle extremely large datasets, making traditional testing methods inadequate. This requires specialized tools and techniques to efficiently manage and manipulate vast amounts of data.
Variety of data:
Big data comes in various forms—structured, unstructured, and semi-structured. Each type of data may require different tools and approaches for effective testing. Handling diverse data types and ensuring their quality and integrity can be complex and resource-intensive.
Velocity of data:
The high speed at which data flows into systems, often in real-time, adds to the complexity of testing. Ensuring the system can process this incoming data quickly and accurately, without bottlenecks, is a significant challenge.
Complexity of data transformations:
Big data applications often involve complex transformations and processing logic. Testing these transformations for accuracy and efficiency, especially when they involve multiple nested operations, can be difficult.
Scalability and infrastructure:
Testing needs to ensure that the system can scale up or down based on the demand. This scalability testing is crucial to handle growth in data volume and concurrent users, which often requires robust infrastructure and can be costly to simulate and test. Read in detail about
Test Scalability
.
Integration testing:
Big data systems frequently interact with numerous other systems and components. Ensuring that all integrations work seamlessly and that data flows correctly between these components without loss or corruption is a formidable task. Read:
Integration Testing vs End-to-End Testing
.
Data privacy and security:
With stringent data protection laws like GDPR and HIPAA, testing must also ensure that the system adheres to legal compliance regarding data security and privacy. This involves testing for data breaches, unauthorized access, and ensuring that all data handling complies with relevant regulations.
Lack of expertise:
There is often a shortage of skilled professionals who understand both big data technologies and testing methodologies. This lack of expertise can hinder the development and implementation of effective big data testing strategies.
Tool and environment availability:
Finding the right tools that can handle big data testing requirements is often a challenge. Moreover, setting up a test environment that mirrors the production scale for accurate testing can be resource-intensive and expensive.
Ensuring data quality:
Maintaining high data quality standards across different datasets and ensuring consistency, accuracy, and completeness of data during and after transformations are critical and challenging.
Strategies for big data testing
The following strategies will help you be thorough with big data testing.
Planning and design strategies
Define testing goals:
Clearly define what you want to achieve with big data testing. Are you focusing on data quality, application functionality, or performance? Having clear goals helps create targeted test plans.
Understand the data:
Gain a thorough understanding of the data you’re working with, including its source, format, and business meaning. This knowledge is essential for designing relevant test scenarios.
Prioritize testing:
Given the vast amount of data, prioritize what needs to be tested first. Focus on critical data flows and functionalities that have the biggest impact on business goals. Read:
Which Tests Should I Automate First?
Choose the right tools:
Select big data testing tools that cater to the specific needs of your project and data environment. Consider tools for data subsetting (using a smaller portion for testing), data anonymization (removing sensitive information), and test automation. Read:
Top 7 Automation Testing Tools to Consider
.
Test execution strategies
Leverage test automation:
Big data volumes necessitate automation. Invest in automating repetitive test cases to improve efficiency and
test coverage
.
Focus on data quality:
Ensure the accuracy, completeness, and consistency of data throughout the big data pipeline. Utilize data quality testing tools to identify and rectify data issues early on.
Performance testing is key:
Big data applications need to handle massive data loads efficiently. Conduct performance testing to identify bottlenecks and ensure the application scales effectively with increasing data volumes.
Security testing is crucial:
Big data often contains sensitive information. Implement security testing measures to safeguard test data environments and prevent unauthorized access or breaches.
Collaboration and communication strategies
Bridge the gap:
Foster good communication and collaboration between development, testing, and data science teams. Everyone involved should understand the testing goals and big data environment.
Seek expertise:
Consider involving big data specialists in the testing process. Their knowledge of the data and big data technologies can be invaluable for designing effective test cases.
Continuous improvement:
Big data testing is an ongoing process. Regularly review and refine your testing strategy based on project progress and evolving data landscapes. Read more about
What is Continuous Testing?
Here are some more strategies that will help you manage big data testing:
Utilize data subsetting and anonymization:
To manage the scale of testing, consider using a smaller representative sample of the data (subsetting) or removing sensitive information (anonymization).
Test in a staged approach:
Break down testing into smaller stages, focusing on specific functionalities or data flows at a time. This can help identify and address issues early in the development process.
Stay updated on big data testing trends:
The big data testing landscape is constantly evolving. Stay updated on new tools, techniques, and best practices to ensure your testing approach remains effective.
How testRigor can help?
testRigor is an intelligent,
generative AI
-powered codeless test automation tool that lets you write test cases in plain English or any other natural language. It enables you to test web,
mobile (hybrid, native)
,
API
, and
desktop apps
with minimum effort and maintenance.
Here are some features and benefits of testRigor that might help:
Quick Test Creation
: Create tests using
testRigor’s generative AI
feature; just provide the test case title/description, and testRigor’s generative AI engine will automatically generate most of the test steps. Tweak a bit, and the plain English (or any other natural language) automated test cases will be ready to run.
Almost Zero Test Maintenance:
There is no maintenance nightmare because there is no reliance on implementation details. This lack of XPath and CSS dependency ensures ultra-stable tests that are easy to maintain.
Test LLMs and AI:
You can use testRigor to analyze and test real-time user sentiments and act based on that information immediately, test your
LLMs (Large Language Models) and AI
using testRigor’s intelligence.
Database Testing
: Execute
database queries
and validate the results fetched.
Global Variables and Data Sets
: You can import data from external files or create your own global variables and data sets in testRigor to use them in
data-driven testing
.
Shift Left Testing:
Leverage the power and advantages of
shift left testing with testRigor
. Create test cases early, even before engineers start working on code using
Specification Driven Development (SDD)
.
Conclusion
Big data testing is not just about validating functionality; it’s about unlocking the true potential of big data. It empowers organizations to harness the power of information and make data-driven decisions that lead to success. As your big data journey continues, remember that big data testing is an ongoing process. By continuously refining your testing strategy and embracing new tools and techniques, you can ensure your big data initiatives deliver long-term value.
Additional resources
Different Software Testing Types
Functional Testing Types: An In-Depth Look
Continuous Integration and Testing: Best Practices
How to do End-to-end Testing with testRigor
More Efficient Way to Do QA
Top 10 Challenges in Automation Testing and their Solutions
Frequently Asked Questions (FAQs)
Why is big data testing important?
Big data testing is crucial because it ensures the integrity and quality of data, which is fundamental for making accurate business decisions. It also helps identify system bottlenecks, ensure high performance and scalability, and maintain data security.
What are the key components of big data testing?
The main components of big data testing include data staging validation, business logic validation, output validation, performance testing, and integration testing.
How does big data testing differ from traditional data testing?
Big data testing deals with larger volumes of data, more complex data structures, and the need to process data at a higher velocity. Traditional data testing typically handles smaller, structured datasets with less emphasis on real-time processing.
How is data security handled in big data testing?
Data security in big data testing involves implementing robust security protocols, conducting security vulnerability tests, and ensuring compliance with data protection regulations. Security testing tools and methods are used to detect potential security flaws and prevent data breaches.
--
Source:
https://testrigor.com/blog/big-data-testing/
--
Scale QA with Generative AI tools.
A testRigor specialist will walk you through our platform with a custom demo.
Request a Demo
-OR-
Start testRigor Free
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
15
To view or add a comment,
sign in
More articles by testRigor
Dec 12, 2024
Test Automation for Beginners: Where to Start?
As software projects continue to grow in complexity, automation becomes a core component of software testing…
Dec 11, 2024
The Role of QA in DevSecOps
In the world of software development, QA is the quality inspector. They ensure that the software is not just functional…
9
1 Comment
Dec 10, 2024
Why No-Code / Low-Code Test Automation Tools Don’t Work
A tester’s ultimate dream is to say ‘Hakuna Matata’ during the testing cycle. You might have even dreamt of this utopia…
10
4 Comments
Dec 9, 2024
Maps or GIS Testing – How To Guide
Maps and Geographic Information Systems (GIS) have evolved into essential components of modern websites and…
9
4 Comments
Dec 8, 2024
How to do email testing using testRigor?
Emails are a part of many workflows and might come up frequently when performing end-to-end testing. Here’s a detailed…
1
Dec 7, 2024
Desktop Testing: Build Your Testing Strategy
What’s Desktop Testing? Desktop testing is a comprehensive evaluation of desktop applications to ensure they meet the…
5
Dec 6, 2024
Test Data Generation Automation
Test data is the most crucial part of software testing because it is the good quality of test data that results in…
3
Dec 5, 2024
Top Challenges in AI-Driven Quality Assurance
Artificial Intelligence (AI) enables higher efficiency, accuracy, and scalability in software testing than traditional…
4
2 Comments
Dec 4, 2024
Manual Testing: A Beginner’s Guide
The global software testing market exceeded $40 billion in 2020 and is expected to grow between 7% to 12% CAGR between…
12
3 Comments
Dec 3, 2024
Test Automation Maturity Model
In today’s competitive market, organizations must deliver high-quality software products at lightning speed. Test…
18
3 Comments
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Data Quality Validations with AI-Generated Data - Part II of the Gen AI with Test Data Series,"Data Quality Validations with AI-Generated Data - Part II of the Gen AI with Test Data Series
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Photo by Lukas from Pexels.com
Data Quality Validations with AI-Generated Data - Part II of the Gen AI with Test Data Series
Report this article
Bharath Hemachandran
Bharath Hemachandran
India Data & AI SL Ops Lead | Data Academy Program Manager | India QA Lead | Principal Consultant | Innovative Technologist & Thought Leader | Generative AI Enthusiast | People, Product & Project Management
Published Apr 7, 2023
+ Follow
I introduced this series last week by defining synthetic test data and discussing its role in the creation of software. In the
previous article
, I ran through an example of creating synthetic data for a discovery session of a new product. This article will take a look at another use case for synthetic data - validating data quality in data pipelines in the ETL process. Ensuring data quality in data pipelines is vital, as errors can lead to cascading issues that impact the accuracy and reliability of insights derived from the data.
First, let us discuss the challenges associated with testing data quality in ETL pipelines, explain how synthetic test data generated based on business rules with the help of a Generative AIs can be used for validation purposes. Using a healthcare scenario, I will demonstrate the process of integrating this technique into existing data testing workflows, considering both the purpose of the application and the developer's point of view.
Challenges in ETL Data Quality Testing
Testing data quality in ETL pipelines can be challenging due to:
Complexity: ETL pipelines often involve multiple data sources, transformations, and destinations. This complexity can make it difficult to identify and validate all possible data issues. Furthermore, the transformations may introduce errors, making it essential to verify the correctness of the data at each stage of the pipeline.
Data sensitivity: Real data (even anonymized) may expose sensitive information or violate privacy regulations (e.g., GDPR, HIPAA). As a result, using real data for testing purposes can lead to ethical and legal concerns. It is crucial to develop solutions that can generate test data while preserving privacy and adhering to regulations.
Test coverage: Manually creating test data can be time-consuming and may not cover all possible edge cases and data variations. Ensuring comprehensive test coverage is vital, as it helps identify potential data quality issues that might arise in real-world scenarios.
Data variety and volume: ETL pipelines often handle diverse data types, formats, and structures. Ensuring data quality for such heterogeneous data can be challenging. Additionally, the sheer volume of data being processed can complicate the validation process, as it becomes increasingly difficult to manage, process, and validate large datasets.
Evolving data requirements: Business requirements and data structures may change over time, which can necessitate modifications to the ETL pipeline. Ensuring data quality in such a dynamic environment requires constant monitoring, validation, and adjustment of the pipeline to meet the changing requirements.
Data dependencies: ETL pipelines may have dependencies on external data sources or other parts of an organization's data infrastructure. If these dependencies are not properly managed and tested, they can introduce errors into the pipeline, affecting data quality.
Limited resources: Ensuring data quality often requires significant resources, including skilled personnel, computing power, and time. Balancing the need for rigorous data quality testing with resource constraints can be a challenge for many organizations.
Lack of standardized testing methodologies: While there are numerous data quality testing tools and frameworks available, there is often a lack of standardized methodologies for validating data quality in ETL pipelines. This lack of standardization can lead to inconsistent testing practices and results, further complicating the validation process.
Addressing ETL Data Quality Testing Challenges with Synthetic Test Data
Synthetic test data, generated based on business rules and Generative AI assistance, offers an effective solution to address the challenges associated with testing data quality in ETL pipelines:
Managing complexity: By generating synthetic test data that closely mimics real data in structure and format, you can effectively validate the correctness of data at each stage of the pipeline. The use of business rules ensures that test data adheres to specific requirements, making it easier to identify and resolve issues.
Preserving data sensitivity: Synthetic test data eliminates the need to use real data, thereby avoiding potential privacy violations and regulatory issues. This approach ensures that sensitive information remains protected while still providing a realistic dataset for testing purposes.
Improving test coverage: Generating synthetic test data using business rules and Gen AI assistance allows you to create diverse data samples that cover various edge cases and data variations. This comprehensive test coverage helps identify potential data quality issues in real-world scenarios more effectively.
Handling data variety and volume: Synthetic test data can be generated for different data types, formats, and structures, enabling the validation of diverse datasets in ETL pipelines. Additionally, by generating test data on-demand, you can control the volume of data being tested, making it easier to manage and validate large datasets.
Adapting to evolving data requirements: As business requirements and data structures change, you can update the business rules and Gen AI prompts to generate new synthetic test data that aligns with the latest requirements. This flexibility allows you to maintain data quality in a dynamic environment.
Managing data dependencies: Synthetic test data can be generated to simulate external data sources or other parts of the data infrastructure, enabling you to test and validate data dependencies in the ETL pipeline more effectively.
Optimizing resource utilization: Generating synthetic test data with Gen AI assistance reduces the time and effort required to create test data manually, freeing up resources for other tasks. This approach can help organizations balance the need for rigorous data quality testing with resource constraints.
Standardizing testing methodologies: By integrating synthetic test data generation with data quality testing frameworks like Great Expectations, you can establish a standardized methodology for validating data quality in ETL pipelines. This standardization leads to more consistent testing practices and results, simplifying the validation process.
Generating Synthetic Test Data Using Business Rules and Gen AI Assistance
For our healthcare scenario, let's consider an ETL pipeline that processes Electronic Health Records (EHR) data for a hospital analytics application. The pipeline extracts EHR data from various sources, transforms it (cleaning, aggregating, and anonymizing), and loads it into a data warehouse for analysis. The analytics application aims to identify patterns and trends in patient diagnoses, treatments, and outcomes.
Real Life like use-case: Generating and validating EHR data
First we need to define business rules by collaborating with domain experts, developers, and stakeholders to determine the types of data required and any specific constraints that should be applied to the test data. For example, in the EHR context, we might need patient demographics, diagnoses, treatments, and test results, with constraints on age range, valid medical codes, and permissible treatments. Let's take the columns below as examples of data that gets populated in our data platform from our pipelines.
Let's say we get the following for a few patients based on the prompt we use to specify age ranges, medical codes, permissible treatments etc.
Note:
It is important that the prompt used contains as many rules as possible to ensure that the data we create is as realistic as possible.
Next, we create the prompt that will generate the list for us. This prompt can be used programmatically or through the chatbot. While the programatic method is the best way to do this in a repeatable fashion, I am using the ChatGPT chatbot to get the data (as it is currently free to use).
At the end of this series I will upload a repository on Github which will demonstrate how to do all of this through code if there is enough interest. Please leave me a comment if you want the code for this.
An example prompt for creating such data would be so (I am not a medical expert so please excuse the oversimplified nature of the rules regarding the prompt). (Tips for
creating good prompts
)
Create a sample dataset of {number} fictional patients with the following constraints

Include patient demographics (patient ID, first name, last name, gender, date of birth, age, race, ethnicity, address, city, state, zip code).
Patient ages should be between {min_age} and {max_age}
Diagnoses should be represented with valid medical codes (e.g., ICD-10) and include a brief description.
Treatments should be represented with valid treatment codes and include a brief description.
Test results should include test codes, a brief description of the test, and a corresponding result.
Ensure that the diagnoses, treatments, and test results are relevant and permissible for each patient.

Generate a CSV dataset that adheres to the above constraints for {number} fictional patients.
number=5
min_age=25
max_age=50
This may give rise to data that looks like the following:
patient_id,first_name,last_name,gender,date_of_birth,age,race,ethnicity,address,city,state,zip_code,diagnosis_code,diagnosis_description,treatment_code,treatment_description,test_code,test_results
1001,John,Smith,Male,1986-08-12,36,Caucasian,Non-Hispanic,123 Main St,Anytown,CA,12345,I10,Hypertension,A01,Antihypertensive medication,T01,Blood Pressure,130/85
1002,Jane,Doe,Female,1990-03-25,33,Asian,Non-Hispanic,456 Oak St,Anytown,CA,12345,E11.9,Type 2 Diabetes,A02,Insulin therapy,T02,Fasting Blood Glucose,145
1003,James,Brown,Male,1982-11-10,40,African American,Non-Hispanic,789 Pine St,Anytown,CA,12345,M16.5,Osteoarthritis of the hip,A03,Physical therapy,T03,X-ray,Hip joint degeneration
1004,Emily,Johnson,Female,1978-02-28,45,Caucasian,Hispanic,321 Elm St,Anytown,CA,12345,J45.909,Unspecified asthma,A04,Inhaled corticosteroids,T04,Spirometry,Reduced FEV1/FVC
1005,Michael,Garcia,Male,1995-05-30,27,Hispanic,Hispanic,654 Maple St,Anytown,CA,12345,K21.9,Gastroesophageal reflux disease,A05,Proton pump inhibitors,T05,Endoscopy,Esophagitis
The data above would be example of good quality data that meets all our rules. Similarly, we can create a prompt that will also give us a known proportion of bad data that we can use to validate our data pipelines. We can break a couple of rules - such as having no age limit or changing the age limit, allowing duplication of data, failing the constraints for realistic diagnoses etc. I will create a separate part on this and reporting on data quality validations next.
Finally, we need to integrate with data testing workflows using tools like Great Expectations to validate data quality. Great Expectations is a Python-based data testing framework that enables you to create and execute data quality tests efficiently. Integrating the synthetic test data generated by the chatbot with Great Expectations can be done as follows (I am saving the data into a csv file called sample_data in this example):
This updated script extracts the success and failure counts for each validation and prints a report with the results. With this report, you can see how many records pass or fail each constraint. Here's what the output of a simple execution looks like:
This simple example should demonstrate how using business rules and Gen AI assistance to generate synthetic test data provides a solution to address the challenges associated with testing data quality in ETL pipelines. By integrating this technique into existing data testing workflows, we can ensure data quality while considering the purpose of the application and the developer's point of view. This approach can significantly improve the efficiency and reliability of data quality validation processes, ultimately leading to better decision-making and improved outcomes in various industries, such as healthcare.
Weekly musings on Tech
Weekly musings on Tech
646 followers
+ Subscribe
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
19
4 Comments
Rosy Roy
Senior Software Engineer | EY |ETL Testing| SQL| Teradata| Report &Dashboard Validation in Tableau,QlikView
1y
Report this comment
This is interesting.. Can u share the github code as well
Like
Reply
1 Reaction
Venugopal Reddy
Director Delivery at Accion Labs
1y
Report this comment
Hi Bharath, Great article. Very interesting and exciting. Can you please share the sample code?
Like
Reply
1 Reaction
Curtis Raymond, MMA
Enterprise Data, Analytics & AI @ Priceline | Master of Management Analytics
1y
Report this comment
Bharath Hemachandran
this is awesome!! Thanks for sharing
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More articles by Bharath Hemachandran
Overcoming the Fear of AI Implementation: Embracing Innovation with Confidence
Feb 19, 2024
Overcoming the Fear of AI Implementation: Embracing Innovation with Confidence
Embarking on the journey of implementing AI can be intimidating for many organizations. The fear of the unknown…
18
A step by step guide to create a Generative AI Application
Feb 12, 2024
A step by step guide to create a Generative AI Application
In today's dynamic technological landscape, I have witnessed firsthand the increasing prevalence of Artificial…
24
From Model T to AI: The Democratization of Revolutionary Technologies.
Aug 7, 2023
From Model T to AI: The Democratization of Revolutionary Technologies.
In the annals of history, few revolutions have been as transformative as the advent of the automobile. A luxury once…
16
5 Comments
Making a Data-Driven Career Leap: Understanding Roles and Pathways
Jun 17, 2023
Making a Data-Driven Career Leap: Understanding Roles and Pathways
Data and AI have swiftly become the driving forces behind the success of many businesses in today's digital era…
12
3 Comments
Part 6 - Enabling Performance Testing with Generative AI: The Power of Design and Execution in Action
Jun 2, 2023
Part 6 - Enabling Performance Testing with Generative AI: The Power of Design and Execution in Action
In my previous article (Part 5), I had talked about Harnessing Synthetic Data Creation with Gen AI for Business-Centric…
13
Navigating the 5 Pillars of Data & AI Quality: What a Quality Dashboard could look like
May 9, 2023
Navigating the 5 Pillars of Data & AI Quality: What a Quality Dashboard could look like
Ensuring success and responsible deployment of AI systems through effective tracking and decision-making. In my…
9
What Quality means in the World of Data and AI: The Five Pillars of Quality
May 4, 2023
What Quality means in the World of Data and AI: The Five Pillars of Quality
This week I am publishing an article on my talk at Yottabyte India at Thoughtworks Bangalore & Gurgaon (Simulcast) on…
20
1 Comment
The Gen AI Series
Apr 28, 2023
The Gen AI Series
I have written a number of articles on the use of Gen AI for various use cases. Collating a list of them on this page…
17
5 Comments
Part 5 - Harnessing Synthetic Data Creation with Gen AI for Business-Centric Performance Workloads - Synthetic Data with Gen AI Series
Apr 28, 2023
Part 5 - Harnessing Synthetic Data Creation with Gen AI for Business-Centric Performance Workloads - Synthetic Data with Gen AI Series
This is the fifth article in my series on creating test data with Gen AI so far. First, we explored various use cases…
6
Part 4 - Enhancing Complex Boundary Value Analysis and Edge Case Testing with Generative AI - Gen AI with Test Data Series
Apr 14, 2023
Part 4 - Enhancing Complex Boundary Value Analysis and Edge Case Testing with Generative AI - Gen AI with Test Data Series
This is the fourth article in my series on creating test data with Gen AI so far. First, we explored various use cases…
9
1 Comment
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Data Quality Does Not Equal Documentation Quality: 5 Super Standards to Test Your Program,"Data Quality Does Not Equal Documentation Quality: 5 Super Standards to Test Your Program
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Data Quality Does Not Equal Documentation Quality: 5 Super Standards to Test Your Program
Report this article
Adrienne Bellehumeur
Adrienne Bellehumeur
Expert on Documentation, Productivity, and Governance, Risk and Compliance | Owner of Risk Oversight
Published Mar 22, 2023
+ Follow
Thank you for catching the latest edition of my Leverage Your Knowledge newsletter. If you haven’t subscribed already, please do so at the top.
And if you'd like more from me, I also publish a monthly newsletter to my email subscribers covering topics including personal productivity, documentation, and the new knowledge workforce.
You can opt-in here.
*******
In recent articles, I’ve talked about new ways to assess, improve, and leverage your information management and documentation processes. I’ve offered ideas on the
5 Stages of Documentation
, developing a
documentation philosophy
, and
the importance of a documentation “skill stack.”
Now, let’s dive into another key benchmark (or “litmus test,” if you will) — the
5 Super Standards
that can give you insight into what is working or not when it comes to your documentation and how to focus your energy on the right areas.
Too many Information Management and Knowledge Management programs put the emphasis on data quality to achieve better results. But that’s not enough. Information—no matter how good—is useless without driving action, changing behaviors, and engaging with its users.
What Is Data Quality? And Why We Need a Broader Metric
In Information Management circles, we talk a lot about data quality. Your organization has strong data quality if you can depend on your data. While in practice this looks different depending on how your organization uses and accesses its data, typical metrics include:
Accuracy
Completeness
Consistency
Integrity
Timeliness
Duplication
Accessibility and availability
But organizations today need more than just clean data. They need a method to assess the overall effectiveness of how users are interacting with their documentation, information, repositories, reports, and systems.
The 5 Super Standards for Evaluating Your Documentation
One of the challenges that organizations have in developing and maintaining excellent documentation is the lack of a robust standard to measure their documentation against. I developed 5 Super Standards of Dynamic Documentation —
Re-Performance, Clarity, Findability, Use, and Engagement
— to fill that gap.
These standards draw from the best practices of myriad professionals and professions that I have worked in — including audit, internal controls, business analysis, information management, organizational design, and corporate communications. The standards are designed for everyone, no matter your profession, industry, level, or role in your organization.
Re-Performance Standard
: The ability of a stand-alone document or system to allow a user to perform the related task or process again. Can the user perform his or her job using the documentation? Is it easy for a new employee to get up to speed?
Clarity Standard
: The ability of the stand-alone document to clearly explain the intended use of the document to the intended audience. Does the reader understand the document without someone explaining it?
Findability Standard
: The ability to easily and quickly find a document or other piece of information using the stand-alone system, setup, or process. Can a user find documentation without asking others?
Use Standard
: The frequency of use of your documents or document systems.  Did anyone use the document or documentation system after it was developed?
Engagement Standard
: A measurement of whether the user was able to grasp the key concepts quickly and efficiently, and their ability to recall the messages and information in the document. Does your documentation or systems have recollection, impact, reaction, thinking, or emotion?
The Operative Word: Stand-Alone
The important criteria to underline in these standards is
“
stand-alone
.”
Your documents, processes, and guidelines need to work in a “stand-alone,” self-explanatory way. You can’t assume that your audience will have someone to help them find the information they need or interpret the content. Retirement at 65 with a gold watch is a thing of the past. Job hoppers, the gig economy, outsourcing, and the Great Resignation are the new reality. In today’s work world, we need information and content that can stand on its own in the absence of the people who developed them or who manage them day to day.
So, ask yourself,
Can your documentation and documentation systems stand on their own without the person who built the documents or the system or repository?
The 5 Super Standards will help you assess your documents to see if they add value, drive change, support your next project, and contribute to the ongoing operations of the company. Ask yourself,
Are people actually using and engaging with your training materials, reports, dashboards, and new SharePoint site?
When you complement your assessment of data quality with the 5 Super Standards, you can watch your Information Management, Knowledge Management, and Content Management programs and strategy truly contribute to your goals and momentum.
How do we change our focus from what we KNOW to what we DO with what we know?
If you liked this article (and I’ll assume you did if you’ve made it to the end!) I think you’ll love my new book,
The 24-Hour Rule and Other Secrets for Smarter Organizations
that’s now available for preorder! You can think of it as the first “mass market” book on documentation – a resource that will help you discover the hidden power of “documentation” to turbocharge your effectiveness and convert ideas, plans, meetings, and proprietary knowledge into action. Learn more and preorder your copy for limited-time bonuses at
The24HourRule.net
.
Leverage Your Knowledge
Leverage Your Knowledge
961 followers
+ Subscribe
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
9
6 Comments
ProcedureFlow
1y
Report this comment
Really love the documentation SUPER standards outlined in your article
Adrienne Bellehumeur
👍!

Specifically we think that when you add visual components (or even take it as far as visualizing an entire process 🤯), that it really speaks to the Clarity standard that you explained when creating effective documented processes.

Sometimes a picture is worth a thousand words, and a #CX agent's boost in confidence is worth it 😊, 

#KnowledgeManagement
Like
Reply
1 Reaction
2 Reactions
Steve Wheelhouse
FD/CFO |  Technology, FinTech, SME and scale up businesses | MBA ACMA CGMA
1y
Report this comment
Really like the dynamic document standards approach. Game changer!
Like
Reply
1 Reaction
2 Reactions
Kim St. Pierre
Executive Assistant
1y
Report this comment
Love this, it's going to help us as we grow our organization.
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Putting “Intimacy” Into Your Professional Plan: How to Grow and Deepen Connections in 2025
Dec 11, 2024
Embracing Rejection and Failure: My Hard-Won Lessons (and Strategies We All Need)
Nov 27, 2024
The Business Analysis (“BA”) Profession of Today – and the Future: Interview with Vanessa Khan
Nov 21, 2024
Do You Want to 10X (or 2X) Your Growth? Applying the Marshmallow Principle
Nov 13, 2024
“Do More with Less”: 4 Ways Internal Control Programs Can Add Value in Lean Times
Nov 5, 2024
Your Documentation Audit: A 5-Stage Benchmark to Assess and Improve Your Documentation
Oct 29, 2024
The Art of Selling for Knowledge Workers: 3 Lessons from the Frontlines
Oct 23, 2024
Worker Smarter, Faster, and Better: Learn the 4 Productivity “Codes” of Human Workflow
Oct 21, 2024
4 Basic Questions to Up-Level Your Information Management Practices
Oct 15, 2024
The Myth of Self-Promotion for Professionals: Why It's a New Requirement for Success
Oct 9, 2024
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
"Predictive Analytics, Big Data=>The Test Book","Predictive Analytics, Big Data=>The Test Book
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Predictive Analytics, Big Data=>The Test Book
Report this article
William K. Furlow
William K. Furlow
Published Dec 15, 2015
+ Follow
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1 Comment
Jeffrey Maskell
Exploration Technical Advisor to Senior Management. Identification of appropriate ONIRIK Exploration Electromagnetic - Passive - Satellite other technologies to be deployed for specific projects and programmes.
9y
Report this comment
“Data Mining is an analytic process designed to explore data (usually large amounts of data - typically business or market related - also known as ""big data"") in search of consistent patterns and/or systematic relationships between variables, and then to validate the findings by applying the detected patterns to new subsets of data.”

I would suggest almost the opposite of this, data mining, Big data and data Analytics can be effectively used to decipher the variables in the data sets, not consistency. Variables, changes can be an effective tool to guide the geoscientists to understand the changes that may occur over time.

Variables, anomalies, spikes and changes can inform the user of key changes within an otherwise stable, long term data stream, to issues that may give cause to change operational parameters and management of a Injection Well or contribute to the long term management of a producing reservoir through the analysis of 4 D time Lapse seismic data. 

Most Big data and Data Mining are used to observe some consistent behaviors, trends that denote a common response to perhaps a specific product, in our case the observation is almost the reverse, we seek the change, the inconsistent, the seismic anomalies, that alert and warn of possible poor optimization or worse indications of cap rock integrity failures.

William, yours is a great read and excellent analysis for the commercial sector, perhaps not so for the more specifics of O&G operational needs and or where operational exceptions need to be trapped by the ""big data"". It requires as much or more ingenuity to identify the inconsistencies that can forewarn and indeed empower the geoscientists to take appropriate remedial actions.
Like
Reply
1 Reaction
To view or add a comment,
sign in
More articles by this author
No more previous content
Saudis lose US clout over oil price war By STEVE AUSTIN for OIL-PRICE.NET, 2016/06/20
Jun 20, 2016
Oil Prices Continue to Slide on Market Uncertainty
Jun 16, 2016
How OPEC lost its iron grip on oil prices
Jun 14, 2016
OPEC Head Calls for $65 Oil
May 25, 2016
Oil Price Spike Is Not As Far Away As Many Think
May 19, 2016
What Does The Next OPEC Meeting Have In Store?
May 19, 2016
The Saudi King Just Fired the Guy Who's Been in Charge of the Country's Oil for 20 Years
May 18, 2016
U.S. Shale Or Saudi Arabia- Who Is Winning The Oil Price War?
May 16, 2016
Massive coral reef discovered (Amazon River)
Apr 25, 2016
Jury seated in trial for ex-BP offshore oil rig supervisor
Feb 17, 2016
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Streamlining Data Complexities: Big Data Testing Case Study,"Streamlining Data Complexities: Big Data Testing Case Study
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing To Eliminate Data Complexities
Streamlining Data Complexities: Big Data Testing Case Study
Report this article
AppTestify
AppTestify
On-demand Digital Testing and Digital Engineering Services
Published Feb 16, 2024
+ Follow
At AppTestify, we encountered a challenge with a client grappling with the intricacies of big data. Their data sets, while abundant, posed significant challenges in terms of reliability, accuracy, and performance. Here's how we navigated through the complexities and delivered robust solutions:
Understanding Data Dynamics
: We began by comprehensively analyzing the client's data architecture, identifying potential bottlenecks, and understanding the intricacies of their big data ecosystem.
Tailored Testing Strategies
: Leveraging our expertise in big data testing, we devised tailored testing strategies that encompass both functional and non-functional aspects. This included validating data quality, ensuring data consistency, and assessing system scalability under varying loads.
Automation for Efficiency
: Recognizing the need for efficiency in testing large data sets, we employed automation tools and frameworks to streamline the testing process. This not only reduced manual effort but also enhanced test coverage and accuracy.
Performance Tuning and Optimization
: We meticulously fine-tuned the performance of the client's big data systems, identifying and addressing performance bottlenecks to ensure optimal system performance even under heavy workloads.
Continuous Monitoring and Maintenance
: Our engagement didn't end with testing and optimization. We established robust monitoring mechanisms to continuously track system performance and address any emerging issues proactively.
The result?
Our client experienced a significant reduction in data complexities, improved system reliability, and enhanced performance, enabling them to leverage their big data assets more effectively for strategic decision-making.
At AppTestify, we're committed to empowering organizations to harness the full potential of their data assets through comprehensive testing solutions. Reach out to us to explore how we can streamline your big data testing challenges and drive meaningful business outcomes.
[ 8999008321 /
contact@apptestify.com
]
#BigDataTesting #DataQuality #PerformanceOptimization #TestingSolutions #AppTestify #DataAnalytics
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
To view or add a comment,
sign in
More articles by this author
No more previous content
AppTestify: Your All-in-One Testing Solution – On-Demand, Automated, and Crowdsourced
Dec 10, 2024
How API Testing Drives Modern Software Reliability
Dec 9, 2024
""Expand your team, not your workload—discover seamless staffing solutions.""
Nov 25, 2024
Revolutionizing Game Testing with Automation and AI: The Future is Now!
Nov 21, 2024
E-commerce Growth: The Untold Story of Quality Assurance
Nov 18, 2024
Why AppTestify is Your Ideal Offshore Testing Partner
Oct 3, 2024
QA for Indie Developers: Delivering High-Quality Testing Without Breaking the Bank
Sep 2, 2024
The Importance of Continuous Testing in Software Development
Aug 5, 2024
The Importance of Continuous Testing in Software Development
Aug 5, 2024
Elevating PWA Testing with AppTestify: Best Practices for Seamless Performance and Compatibility
Mar 18, 2024
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Test your Big Idea without Big Data,"How to Test your Big Idea without Big Data
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Photo by jayninelessons - http://flic.kr/p/oFr9aU
How to Test your Big Idea without Big Data
Report this article
Rick Harris
Rick Harris
Published Apr 15, 2015
+ Follow
Any organization with a problem can brainstorm ideas to fix it. And there are lots of techniques out there to help create ideas (the subject of another blog but not this one!).
Yet the key to successful initiatives lies not just in creating ideas but knowing how to test them. Ideally, you would make your test ‘scientific’ such as having control groups and large sample sizes, backed up with so-called
Big Data
to analyze many related variables.
But what if you only have one store, a handful of customers, or even just one product? Does this condemn you to simply ‘
having a go and seeing what happens
’?
Absolutely not. Many of our smaller clients have taken our advice to balance a simple format for piloting initiatives alongside some tried-and-tested principles adapted from more scientific research.
Here are 4 steps you can take to make testing your business initiatives more effective, whatever the scale or size of your organization:
Make It Specific
– what exactly do you want to learn? A vague ambition such as “we need to find out how to improve our customer referral rates” won’t cut it. Instead, let your idea drive the learning. A better question to target would be “What impact does a follow-up satisfaction call to our new car buyers in the 1st week of ownership have on referrals?”
Look for Possible Bias
– let’s say your big idea is to support your range of ice-creams with a new ad campaign to grow sales. Any resultant year-on-year sales analysis might have more to do with the weather than any change of marketing, pack design or even price. So start with the things you know impact the target variable and make an effort to mitigate their influence on analysis. In this example, you might index-link temperature with sales across a few years to help factor out the impact of a warm or cool summer.
If you don’t have a control set, create one
– whilst large sample sizes help with finding control sets, there are ways to be inventive even if your data is ‘small’. Imagine you run a health gym (just one) and your research shows you that those who come more often stay as members for longer. You already provide an induction for new members to show them how to use the equipment safely, so your idea is to include a personal goal-setting session to help members get better results and so keep coming back. From your possible bias examination, you identify that new members peak in January and taper off during the year. One way to handle this could be to create a control group from the same month in the previous year as your test will happen this year. And to try and improve the sample size, you might pick a busy month, but avoiding January itself as that may not be representative or indeed replicable for future rollout.
Finesse Your Learning
– narrowing the focus of your learning question need not force you into a single-design test. In the gym example above, you may be concerned that a lengthy induction might be unpopular with members keen to get started. So you could test a shorter version of 15 min alongside a 30 min one. By randomly testing the two versions in what’s called an A/B split test, you can assess the impact. Of course, there are lots of other ‘Big Data’ variables that may be important here, such as age or gender, or perhaps how far away members live from the gym. A larger sample size would mitigate such factors, but with a small sample, these can be manually assessed after the test, even if it means capturing some data retrospectively.
As a rule-of-thumb, the bigger the expected change, the smaller your sample size and control group need to be. If you expect only small changes, the risk of other factors causing it will require a larger sample.
Big Data has an important role – in identifying trends and patterns of behavior, especially from business models that collect large sets of information. But this doesn’t mean that smaller organizations, often those that still rely on tacit knowledge about customers derived outside of databases, can’t develop well thought out initiatives with evidence to back up their effectiveness.
-----------------------------------------------------------------------------------------------------
For help and support with customer problems, idea-generation and implementation, contact
rick@customerfaithful.com
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by this author
No more previous content
Where do customer journey maps come from?
Jan 10, 2023
What makes good clinical trials even better? The case for inclusivity.
Dec 21, 2022
Will the cost-of-living crisis drive down customer care for providers seeking cost leadership?
Sep 27, 2022
‘Person vs. Patient’ – the undervalued role of self-identity in cancer survivorship
Jul 13, 2022
Why Employee Engagement needs a ‘Triple Bottom Line’ (3BL) of its own
Apr 12, 2022
What NHS retention struggles can teach us about the Great Resignation
Mar 1, 2022
Rewarding Behaviour - How To Get What We (all) Want
Mar 8, 2021
What all retailers can learn from the evolution of Niketown
Jan 20, 2021
What will motivate 2021 consumers in the aftermath of Covid?
Dec 22, 2020
Planning for the flip side: How is COVID-19 changing what your customers need?
Apr 15, 2020
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data Testing,"Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing
Report this article
Smriti Saini
Smriti Saini
Talent Acquisition Specialist || Let's grow together!!
Published Jun 4, 2021
+ Follow
Consider the internet like Pandora's Box. The internet has become so powerful because of data and data about data.
For example, when you search the internet for something, you search for data about data. And a search proves to be useful only because someone preserved this information you were seeking somewhere over the internet. As technology advanced to support all this data, so too did the demand for big data applications.
In earlier times, data was preserved by using simple data files. As the complexity of data grew, database management systems came into existence. Soon both structured and unstructured data were being generated at a giant level, gaining the title ""BIG DATA.""
Just how big is big data? Check out this big data testing case study on global data growth:
Source:
IDC’s Digital Universe Study
This big data testing tutorial can guide you through the process of creating a big data testing strategy, discusses the best tools for big data testing and answers to your most pressing QA testing questions, including:
What is big data software testing?
What are big data testing challenges?
What big data testing best practices should you follow?
What big data testing techniques should you include in your strategy?
What big data automation testing tools should your tech stack include?
What Is Big Data Software Testing?
Big data testing is the process of data QA testing big data applications. Since big data is a collection of large datasets that cannot be processed using traditional computing techniques, traditional data testing methods do not apply to big data. This means your big data testing strategy should include big data testing techniques, big data testing methods and big data automation tools, such as Apache’s Hadoop. Get the most out of this big data tools tutorial by first exploring how to QA data of gigantic size.
Include These Tests in Your Big Data Testing Strategy
After reviewing big data case study after case study, you can expect to find that successful teams include the same types of big data testing methods.
Is your team ready to explore how to QA data? This big data tools tutorial advocates the inclusion of the following tests within your data QA strategy.
Functional Testing:
Front-end application testing provides advantages to data validation, such as being able to compare actual results produced by the front-end application against expected results as well as gaining insight into the application framework and its various components.
Performance Testing:
Automation in big data allows you to test performance under different conditions, such as testing the application with different varieties and volumes of data. Performance testing big data applications is one of the most important big data testing techniques because it ensures that the components involved provide efficient storage, processing and retrieval capabilities for large data sets.
Data Ingestion Testing:
Include this type of testing within your data testing methods so that you verify that all data is extracted and loaded correctly within the big data application.
Data Processing Testing:
Your big data testing strategy should include tests where your data automation tools focus on how ingested data is processed as well as validate whether or not the business logic is implemented correctly by comparing output files with input files.
Data Storage Testing
: With the help of big data automation testing tools, QA testers can verify the output data is correctly loaded into the warehouse by comparing output data with the warehouse data.
Data Migration Testing
: This type of big data software testing follows data testing best practices whenever an application moves to a different server or with any technology change. Data migration testing validates that the migration of data from the old system to the new system experiences minimal downtime with no data loss.
Big Data Testing Challenges
Challenges faced when testing unstructured data are expected, especially when new to implementing tools used in big data scenarios. This big data testing tutorial uncovers both
big data testing challenges and solutions
so that you always follow data testing best practices.
Heterogeneity and Incompleteness of Data
Problem
: Many businesses today are storing exabytes of data in order to conduct daily business. Testers must audit this voluminous data to confirm its accuracy and relevance for the business. Manual testing this level of data, even with hundreds of QA testers, is impossible.
Solution
: Automation in big data is essential to your big data testing strategy. In fact, data automation tools are designed to review the validity of this volume of data. Make sure to assign QA engineers skilled in creating and executing automated tests for big data applications.
High Scalability
Problem
: A significant increase in workload volume can drastically impact database accessibility, processing and networking for the big data application. Even though big data applications are designed to handle enormous amounts of data, it may not be able to handle immense workload demands.
Solution
: Your data testing methods should include the following testing approaches:
Clustering Techniques
: Distribute large amounts of data equally among all nodes of a cluster. These large data files can then be easily split into different chunks and stored in different nodes of a cluster. By replicating file chunks and storing within different nodes, machine dependency is reduced.
Data Partitioning
: This automation in big data approach is less complex and is easier to execute. Your QA testers can conduct parallelism at the CPU level through data partitioning.
Test Data Management
Problem:
It’s not easy to manage test data when it’s not understood by your QA testers. Tools used in big data scenarios can only carry your team so far when it comes to migrating, processing and storing test data-that is, if your QA team doesn’t understand the components within the big data system.
Solution:
First, your QA team should coordinate with both your marketing and development teams in order to understand data extraction from different resources and data filtering as well as pre and post-processing algorithms. Provide proper training to your QA engineers designated to run test cases through your big data automation tools so that test data is always properly managed.
Best Big Data Testing Tools
Your QA testers can only enjoy the advantages of big data validation when strong testing tools are in place. This big data tools tutorial recommends reviewing these highly rated big data testing tools when developing your big data testing strategy:
Hadoop
Most expert data scientists would argue that a tech stack is incomplete without this open-source framework. Hadoop can store massive amounts of various data types as well as handle innumerable tasks with top-of-class processing power. Make sure your QA engineers executing
Hadoop performance testing for big data
have knowledge of Java.
HPCC
Standing for
H
igh-
P
erformance
C
omputing
C
luster, this free tool is a complete big data application solution. HPCC features a highly scalable supercomputing platform with an architecture that provides high performance in testing by supporting data parallelism, pipeline parallelism and system parallelism. Ensure your QA engineers understand C++ and ECL programming language.
Cloudera
Often referred to as CDH (Cloudera Distribution for Hadoop), Cloudera is an ideal testing tool for enterprise-level deployments of technology. This open source tool offers a free platform distribution that includes Apache Hadoop, Apache Impala, Apache Spark. Cloudera is easy to implement, offers high security and governance, and allows teams to gather, process, administer, manage and distribute limitless amounts of data.
Cassandra
Big industry players choose Cassandra for its big data testing. This free, open source tool features a high-performing, distributed database designed to handle massive amounts of data on commodity servers. Cassandra offers automation replication, linear scalability and no single point of failure, making it one of the most reliable tools for big data testing.
Storm
This free, open source testing tool supports real-time processing of unstructured data sets and is compatible with any programming language. Storm is reliable at scale, fault-proof and guarantees the processing of any level of data. This cross-platform tool offers multiple use cases, including log processing, real-time analytics, machine learning and continuous computation.
Benefits of Big Data Testing:
From one big data testing case study to the next, many companies can boast of the benefits from developing a big data testing strategy. That’s because big data testing is designed to locate qualitative, accurate and intact data. And the application can only improve once confirming the data collected from different sources and channels functions as expected.
What additional benefits of big data testing are in store for your team? Here are some advantages that come to mind:
Data Accuracy:
Every organization strives for accurate data for business planning, forecasting and decision-making. This data needs to be validated for its correctness in any big data application. This validation process should confirm that:the data injection process is error-free
complete and correct data is loaded to the big data framework
the data process validation is working properly based on the designed logic
the data output in the data access tools is accurate as per the requirement
Cost-Effective Storage:
Behind every big data application, there are multiple machines that are used to store the data injected from different servers into the big data framework. Every data requires storage-and storage doesn't come cheap. That’s why it’s important to thoroughly validate if the injected data is properly stored in different nodes based on the configuration, such as data replication factor and data block size.
Keep in mind that any data that is not well structured or in bad shape requires more storage. Once that data is tested and is structured, the less storage it consumes, thus ultimately becoming more cost-effective.
Effective Decision-Making and Business Strategy:
Accurate data is the pillar for crucial business decisions. When the right data goes in the hands of genuine people, it becomes a positive feature. It helps in analyzing all kinds of risks and only the data that contribute to the decision-making process comes into the picture, and ultimately becomes a great aid to make sound decisions.
Right Data at the Right Time:
Big data framework consists of multiple components. Any component can lead to bad performance in data loading or processing. No matter how accurate the data may be, it is of no use if it is not available at the right time. Applications that undergo load testing with different volumes and varieties of data can quickly process a large amount of data and make the information available when required.
Reduces Deficit and Boosts Profits:
Indigent big data becomes a major loophole for the business as it is difficult to determine the cause and location of errors. On the other hand, accurate data improves the overall business, including the decision-making process. Testing such data isolates the useful data from the unstructured or bad data, which will enhance customer services and boost business revenue.
Next Steps for Your Big Data Testing Strategy
Performing comprehensive testing on big data requires expert knowledge in order to achieve robust results within the defined timeline and budget. You can only access the best practices for testing big data applications by making use of a dedicated team of QA experts with extensive experience in testing big data, be it an in-house team or outsourced resources.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
3
To view or add a comment,
sign in
More articles by this author
No more previous content
What Is Portfolio Analytics?
Oct 4, 2021
Annuity
Oct 1, 2021
What is Actuarial Modeling?
Sep 30, 2021
Supervised vs. Unsupervised Learning: What’s the Difference?
Sep 29, 2021
APACHE HIVE
Sep 28, 2021
Acceptance testing
Sep 27, 2021
SAP HANA
Sep 25, 2021
Machine Learning Architecture
Sep 23, 2021
AZURE DEVOPS
Sep 22, 2021
Report Building
Sep 21, 2021
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Christopher Diggins on LinkedIn: 🔬 Here is a test where I am sharing a VERY big data set between two…,"Christopher Diggins on LinkedIn: 🔬 Here is a test where I am sharing a VERY big data set between two…
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Christopher Diggins’ Post
Christopher Diggins
Creating the Next Generation of Parametric 3D Design Tools
1mo
Report this post
🔬 Here is a test where I am sharing a VERY big data set between two processes. The data is written by a C# process to a memory mapped file asynchronously, and read by a Unity rendering process on each frame. 1 million cubes + 1 million tetrahedrons. 

So why would I do this? 

Well let's say I was building a very powerful 3D design tool, and wanted to use Unity as the viewport, this would be an effective way to do it. 😉
…more
40
4 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Raphael Riad
Engineer
1mo
Report this comment
Instanced?
Like
Reply
1 Reaction
2 Reactions
Mike Paraska
Computational Designer| AEC Software Development | Digital Transformation | Musician
1mo
Report this comment
I love simulating these hypotheticals. What if... The building was made out of cheese?
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More Relevant Posts
Arkadiusz Więcław
Unity Game Developer | C# Programmer | Software Developer | .NET Developer
1mo
Report this post
Water surface simulation with polygons, wave simulation of the surface using C++ and OpenGL.

Models were created in Blender then were loaded as obj files into the scene using OBJ loader. Shaders were used to light the scene. Waves are created by changing the height of points in the grid.
The project consists of 2 header files and 1 source file :

IncludeHeaders.h - for better convenience, most of the needed
#include
notations and the used namespaces used are written in this header file.

Model.h - the file contains a class whose methods are used to : initialize shaders, upload the 3D model, drawing the model, and a number of universal functions for passing matrices from the program to the shader.
Main.cpp
- the most important file of the application, which contains the main loop of the program. It is responsible for initialization of shaders and models and their placement on the scene, provides support for controlling camera movements for the user, for testing purposes it calculates the initialization times of shaders and models uploaded to the scene.

Link:
https://lnkd.in/djkwGCC5
…more
10
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Timucin Ozger
Head of CG & Simulation @ MPC Toronto // VES Member // VFX Workflow Architect
5mo
Report this post
This is the second part of Elite Fallen that shows how to add the simulations.
https://lnkd.in/gEbc8kSV
You will need to complete the main videos to be able to continue this tutorial. Follow the first link below.
https://lnkd.in/gAVV5xmY
PROJECT FILES
https://lnkd.in/gtEghVHr
TOPICS COVERED
Redshift Look Dev and Lighting Workflows
POP Network 
PYRO Simulations using PDG Workflow
SOLID Solver Simulations for Metal Bending

01 Gatling Laser -
https://lnkd.in/gRX5H6N9
02 Impact PYRO -
https://lnkd.in/gZwhZuzY
03 Thrusters POPNET -
https://lnkd.in/g-u4Tkva
04 Missiles PYRO -
https://lnkd.in/g86CPU7m
05 BusCrash FEM 01 -
https://lnkd.in/g6z2cmPe
06 BusCrash PYRO 01 -
https://lnkd.in/gra63C_W
07 Rendering -
https://lnkd.in/gJB8QBRN
Requirements :
1080P Monitor
Minimum 8GB ram ,32GB Recommended
Houdini 17.5 Apprentice Minimum
Redshift Trial Edition Minimum
VLC Media Player or Media Player Classic

Basic level of Houdini Knowledge
#houdini
#python
#vfx
#simulation
#redshift3d
#mantra
#cg
#render
#shading
#cg
#sidefx
#procedural
Studio Oriented - Simulations
youtube.com
91
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Mohamed jozef
Motion Graphic Team Lead at SOCIALEYEZ
1mo
Edited
Report this post
Just wrapped up an exciting study on SideFX Houdini, showcasing a dynamic shallow river simulation.meticulously rendered in Redshift and polished in Adobe After Effects.  🚀
#Houdini
#Redshift
#AfterEffects
#VFX
#Simulation
""
…more
30
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
David Robustelli
Creative Director / Founder Beyond.Studio - Reshaping the future of digital experiences for Fashion & Luxury working with Immersive Technologies and Spatial Computing.
1mo
Report this post
First time for the
Beyond Studio
team testing with the
8th Wall
Gaussian Splatting capability. Pretty cool to see how the environment scan is translated into a 3D mesh. This allows developers to add 3D assets to the scene, dynamically interacting with the scanned environment. The whole scene runs pretty smoothly in a mobile browser.

I can really see this work for virtual (shopping) environments which are to represent a real world environment, combined with 3D elements for the user to interact with.
…more
55
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Bruno Conte
Production Assistant | Post Production | Video Editor | VFX Editor | FX Artist | Compositor | Motion Designer and Effects Animator
3mo
Report this post
🚨 Stuck in Houdini? Are Slow Simulations Holding Back Your Projects? 🚨

If you’ve ever worked in Houdini and found yourself waiting forever for a simulation to finish, you know how frustrating that can be! 😫

Simulations in Houdini, especially when dealing with complex effects like fluids, particles, or destruction, can eat up a lot of time and processing power. But there are some ways to speed up the process and optimize your workflow!

🎯 Here are a few quick tips to optimize your simulations in Houdini:

1️⃣ Optimize your geometry – Reduce the number of polygons before running a simulation. Fewer polygons mean less data to process. 💡

2️⃣ Simulate in separate layers – Break your scene into layers (like fluids, smoke, or particles) and simulate each one individually. You can combine them all later in post-production.

3️⃣ Use Simulation Caching – Caching your simulations is key to avoid recalculating everything every time you make adjustments. This saves you a lot of time!

4️⃣ Start with a lower resolution – Begin by simulating in a lower resolution for faster tweaks and only increase the quality at the final stage.

⚙️ Extra tip: If your hardware allows, set up your simulation to run on GPU instead of CPU. In some cases, this can significantly improve processing time.

Time saved in simulation is time invested in creativity! 😉
#VFX
#Houdini
#SimulationTips
#3DSimulation
#VisualEffects
---

🔄 How do you handle slow simulations? Share your strategies in the comments!
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Christopher Diggins
Creating the Next Generation of Parametric 3D Design Tools
8mo
Edited
Report this post
I've been in the woodshed quietly working on the Ara3D.Geometry library (
https://lnkd.in/eQ_yisXs
). 🤫 Here is a little sneak peak of the parametric surfaces being rendered in Unity.   

Ara3D.Geometry is a cross-platform geometry library written in C#. Unlike other geometry libraries it is based on functional programming and immutable data-structures. It has deep support for parametric surfaces, curves, textures, and solids alongside traditional discrete geometry representations (e.g., triangular and quadrilateral meshes or point clouds).
…more
54
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Sayan Bhattacharjee
Developing an Aerospace CFD startup
7mo
Report this post
Progress: rust winit + wgpu is working.

Key press event handler is also working.

If I can get the OpenGL rendering pipeline correctly setup, I would want to render the CAD models in shaded, and wireframe mode.
#aerospace
9
5 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Guillaume Jobst
🎮 Senior pipeline / procedural technical artist. 🎮
2w
Report this post
Little quick blueprints doodling of the day 😀
https://lnkd.in/eqUBtKCr
A simple proof of concept about how we could use Scriptable BP tools to have custom 3D handles, instead of the regular xform gizmos, to drive various systems ( here, a simple PCG graph ).

That would allow the users to have dynamic handles ( using geometry script ), to have direct feedback on manipulations.
I'm thinking about having an easy-to-use BP framework to build handles like this easily, if you're interested by that, keep an eye on
https://lnkd.in/e5MBnMmB
! 👀
#unrealengine
#blueprints
#tools
…more
Unreal 3D Custom handles POC
https://www.youtube.com/
21
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
David Martinez
Unreal Engine Education Advisor | UE Generalist | Virtual Production | Founder Splitmind Studios | ADG 800 | Designer |
8mo
Edited
Report this post
PCG Test 02 - Conducted a quick test in Unreal Engine 5.4, utilizing mesh sampler and tags to determine which objects in the scene are affected by scatter techniques.

Returning to the task at hand, I will soon share insights on how this methodology is integrated into layout design and biome creation. The primary goal is achieving speed and efficiency.

Mushrooms are mesh sampled - Trees are sampled from floor  - 💡 lighting default with Rule of 16 and ISO 💯. 

☕
…more
48
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Jakub Sławiński
Multimedia Software Developer
6mo
Report this post
I you still using Canvas system in Unity here is a grate tool for making everything sooooo much easier. Flexible grid. It can be modified and used for lot more than grids.
https://lnkd.in/djRqrmeV
…more
How To Get A Better Grid Layout in Unity
https://www.youtube.com/
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
2,632 followers
197 Posts
20 Articles
View Profile
Connect
More from this author
Using AI to Help Design a Language
Christopher Diggins
2mo
IFC Property Databases
Christopher Diggins
4mo
The World’s Fastest IFC Loader/Parser?
Christopher Diggins
4mo
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
What I learned from ‘Executing Data Quality Projects’,"What I learned from ‘Executing Data Quality Projects’
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
What I learned from ‘Executing Data Quality Projects’
Report this article
David Finlay
David Finlay
𝗚𝗲𝘁𝘁𝗶𝗻𝗴 𝗣𝗿𝗼𝗷𝗲𝗰𝘁𝘀 𝗗𝗼𝗻𝗲 𝘄𝗶𝘁𝗵 𝗗𝗮𝘁𝗮: Medical Device & Pharma Supply Chain & Regulatory Change (MBA with PhD in progress)
Published Mar 11, 2022
+ Follow
Introduction:
With this article, I set out to bring insights to life from my own experience of (enterprise master) data quality projects, and then compare and contrast these with one of the most influential books written on the subject. Writing this piece has helped me recognize where I’ve been doing well and where I needed to take a step back and consider additional perspectives and methods to focus data cleansing and remediation to bring greater business value while better engaging data creators and consumers. I hope that some of these observations will resonate with fellow business analysts, data stewards, business function leaders, ERP systems program managers, MDM-managers, internal and external business and technology consultants, and anybody else intrigued by the subject matter.
A note about certain copyrighted content used in this article:
5 of the 28 graphics used in this article are from
Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information™,
2nd Ed. (Elsevier/Academic Press, 2021) by
Danette McGilvray
and are used with permission from the author. Additional quotes from the book and are identified with a subsequent
[Executing DQ Projects + page number]
Main article
In late 2016, I brought my then team to Delray Beach, Florida, for the
Data Governance Winter Conference
. Some of us attended a Data Quality training course led by Danette Mc Gilvray. Due to positive response, I purchased licenses to her
Data Quality Learning Plan
, a video-on-demand course on the
Dataversity
online training platform. This enabled the rest of our team to further benefit from the training. Since then, I’ve been formally applying Danette’s ‘Ten Steps to Quality and Trusted Information’ in a deliberate yet non-dogmatic way to various master data-enabled projects ranging from company acquisitions and spinoffs to digitalization initiatives requiring firm data management foundations such as prep for SAP implementations, platform upgrades, and other kinds of systems consolidations.
While, I have been using her 2008 Executing Data Quality Projects book as a framework for more than five years, I’ve always found myself dipping in and out of it. When a particular challenge arises, I return to specific sections for inspiration. Though many readers will end up consuming the book in a similar practical fashion, I decided to read it through cover-to-cover when the second edition was published in 2021. I’ve found the update a worthy one. Enhancements include a greater focus on human interactions, callout boxes with over a decade of real-world examples, and extensive testimonials for the method in action. Templates have also been overhauled, and there are now more than twenty-five of them. My three favorites are: 'Data Specification Evaluation Template,' 'Information Anecdotes Template,' and the 'Price Tag of Poor-Quality Data Template.'
Here below is a flavor of what to expect with a worked example:
Template 4.4.3 - Price Tag of Poor-Quality Data. From Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information™, 2nd Ed. (Elsevier/Academic Press, 2021) by Danette McGilvray. Pg. 175. Used with permission. The template contains links to additional sources from articles by
Tom Redman
,
Tadhg Nagle
and
David Sammon
published in Harvard Business Review and MIT Sloan Management Review.
I agree with Danette’s recommendation to be somewhat more conservative with the numbers when documenting the cost of poor-quality data. However, let’s face it, that in the unlikely event (lucky you!) that the numbers for your organization would even be as low as one-tenth of what is suggested in the above example – then you need to be paying attention to these problems! It’s fascinating to see management reactions the first time you pull out these kinds of estimates. Initial incredulity quickly turns into something all too believable once combined with some relatable anecdotes about how long it takes to get an ‘accurate’ business performance report, understanding just how many people are running around wrangling data, or how product launches or operations have been affected by not having the right data available at the right time.
There are plenty of common challenges for people working with ERP (Enterprise Resource Planning) master data and the business processes that require it. Hopefully, some of the observations will be familiar and may help you decide if the book is worth purchasing. If you reach that conclusion, Danette offers a discount code on her website (please see the link in the comments section). I recommend the paperback book and PDF eBook bundle. This is for several reasons. One is that mailing books tends to take longer than usual in these COVID times (time of writing March 2022). Another reason is that it will be much easier for you to search the book in electronic format using [CTRL+F] when you inevitably lookup or return to specific topics. It’s also easier to cart around in your laptop bag!
I will start with a few paragraphs introducing the book and then provide my observations based on the following:
What I already knew, already practice, and am glad to see it (re)emphasized in this book
What I partially knew but will now integrate more into current and future projects
What I don’t know much about, but intend to learn
Further split into bite-sized chunks we have:
1. What I already knew, already practice, and am glad to see it (re)emphasized in this book
a.      More carrot, less stick.
b.      We are all Data Stewards.
c.      Data Governance (DG) and Data Quality (DQ) go hand-in-hand.
d.      Turn Data Quality knowledge into action.
e.      WIIFT (What’s In It For Them)
f.       Data Quality Specifications.
g.      Perfection is a roadblock to progress.
h.      How can we better control our test data?
i.        Business Process Workarounds.
j.        Cost-Benefit Analysis.
2.  What I partially knew but will now integrate more into current and future projects
a.      The ultimate goal is for Data Quality to become part of the business process fabric.
b.      Constraints can be your friend.
c.      It takes more than ‘sign here, and you are done’ training.
3. What I don’t know much about, but intend to learn
a.      Data Stewardship vs. Ownership.
b.      Data Lineage.
c.      ISO 8000-61 Data Quality Standard.
d.      Information Quality Post-Graduate Programs
‘Executing Data Quality Projects’ is first and foremost a book for ‘doers’ targeting the practical needs of someone or some team with the responsibility (and / or the accountability) to get data clean and keep it clean by implementing appropriate controls backed up by education and awareness (literacy) of how business processes interact. In my opinion, the book does a good job at filling some of the data quality subject-matter gaps between the necessary generality of the
DAMA DMBoK2
and less accessible academic literature. And for the ‘Lean’ practitioners out there, Danette provides an overlay of how her ‘Ten Steps’ complement rather than contradict how DMAIC (Define, Measure, Analyze, Improve, and Control) is used with Six Sigma. Finally, I would also agree with
Paul Grobler
that the methods outlined in the book underscore ‘the importance of linking data quality with problems within the business.’
In this book, Danette covers the data quality dimensions one typically needs to use when profiling data, i.e., accuracy, consistency, coverage, frequency, referential integrity, validity, uniqueness, etc. She also references additional resources such as
the Conformed Dimensions of Data Quality
from
Dan Myers
.
Danette delivers plenty of invaluable practical tips, which have clearly been picked up from many years of practice and observation in projects. She also provides a decent introduction to record matching, linking, and various forms of entity resolution. However, don’t expect this book to offer a technical primer in statistics or advanced themes in data quality science. Instead, its strength is in helping us become better project coordinators of data quality initiatives, spending ‘just enough’ time in each step to ‘get things done’ and minimize expensive rework and waste of time by the SMEs you’ll need to engage with to make progress. ‘Executing Data Quality Projects’ is then pretty much what it says on the tin.
Given its reliance on human behavior, I firmly believe that there is an art as well as a science to ‘doing Data Quality.’ I don’t advocate that one can learn it all from books since data quality must be practiced and iterated. However, one can (and in my opinion, should) take the time to learn a project methodology to ensure that you are considering all relevant aspects. Without a method, you risk disappearing down the rabbit hole of reasoning that nothing less than 100% perfect data is acceptable. Such thinking will not help you achieve tangible business results fast enough and frustrate all involved. The absence of a methodology could also be why you are not getting heard and necessary resources allocated to your data quality initiatives. Although the book is quite a substantial read at over 250,000 words (hats off to you if you can read it in a single weekend), following the authors ’Ten Steps’ cookbook is not a daunting affair and is sure to give you a solid foundation in data quality. I also give it a DDA Highlight Factor of 16.7%. The DDA HF is the % of the total word count which I highlighted in yellow while reading the book. Typically, a book has been very worthwhile (to me) once the DDA HF is > 6%
Before we get into my personal observations, if you only internalize one key takeaway from the book, for me, it would be that you are very unlikely to be successful if you jump ahead and focus too narrowly. In other words, while it’s very tempting to get straight into data profiling and making corrections, you would then only be executing two of the ten steps. The other steps are there for a reason. The fact that Step # 10 runs in parallel to and not in sequence to the other steps is summed up by Danette as: ‘
Communication is part of the work, not something that gets in the way of your work!
’
[Executing DQ Projects Pg. 242]
1. What I already knew, already practice, and am glad to see it (re)emphasized in this book
More carrot, less stick.
It’s time for many organizations to move beyond compliance as the traditional beating stick to encourage people to take data quality seriously. Or at least it should not be the only concern. Instead, it should be just one of several offensive and defensive tactics used to generate enthusiasm when striving for high-quality data.
The above slide image is courtesy Data Driven Associates LLC but was partly inspired by the discussion about defensive and offensive (data) plays in the June 2017 Harvard Business Review article:
What's your Data Strategy? The key is to balance offense and defense.
By
Leandro Dallemule
and
Tom Davenport
.
Danette extends this thinking with:
‘Sadly, many organizations see regulations only as a burden. The threat of bad publicity and high fines, along with the risk of a CEO going to jail, motivate the minimum level of compliance. What a missed opportunity! How much better to jump in with enthusiasm because you understand that compliance can improve processes, decrease risk, and increase opportunities to keep customers happy.’
[Executing DQ Projects Pg. 9]
We are all Data Stewards.
Danette provides a tidy definition of Data Stewardship
:
‘The idea of stewardship means that whoever touches the data knows to care for it, not only for their own immediate needs but also on behalf of others in the organization who also use the data.’
[Executing DQ Projects Pg. 34]
The tightly integrated business system landscape we find ourselves in today requires managing people to take care of data just like any other company asset. Here, the author implores us to think in terms of Data Literacy:
‘Every person who creates data, updates data, deletes data, or uses data in the course of their job affects the data. Yet how many of them understand the impact, they have on this important asset called information? Are we really managing our data and information assets if people do not understand how they affect them?
’
[Executing DQ Projects Pg. 34]
Data Governance (DG) and Data Quality (DQ) go hand-in-hand.
In Chapter 2, Danette acknowledges the level of interdependence and puts it that:
‘Data quality can be started without formalized data governance, but it cannot be sustained without data governance. No matter how a data quality program is structured or placed in your organization, it is still important to have it as a recognized program. There are detailed skills and knowledge needed to manage and sustain data quality.’
[Executing DQ Projects Pg. 19]
We should not assume these skills are common or are readily available. I once learned the hard way by thinking that hiring someone great at data migration would make them a perfect candidate to lead data cleansing projects. I now believe that there is a certain kind of person who excels at data quality analysis and has the drive to see the required changes through. As Brad Bird from Pixar said (albeit in a different context):
‘… give me … the people with a restless, probing nature… If you had thermal glasses, you could see the heat coming off them.’
Turn Data Quality knowledge into action.
Another thing I’ve learned over the years is to invest in myself and to take personal control of improving my data quality-related skills as well as soft skills needed to manage stakeholders and run projects. Danette concurs:
‘Read books, sign up for courses, in person or online, attend conferences and industry chapter meetings. Research opportunities to learn, proactively approach management to help pay for courses and/or time off work and show how this benefits your organization. If you still don’t get support, don’t let it stop you from improving your skills on your own.’
[Executing DQ Projects Pg. 23]
I recommend seeking out training opportunities consequently and always with the intent of putting into practice what you have learned. If you are looking for ideas, let me know or ask the many helpful people out there participating in data forums on LinkedIn or ask around at some of the conferences such as
EDW
(Enterprise Data World),
DGIQ
(Data Governance and Information Quality Conference), or some of the more specialty online events such as the
Master Data Marathon
.  If you are a manager, I recommend you take a regular skills inventory of your team to figure out development needs. Some team members will be glad you did, while others will need nudging. In these times of the so-called ‘great resignation,’ putting your money where your mouth is in terms of training budgets will help retain and attract staff who value lifelong learning.
I am using
WIIFT (What’s In It For Them)
here as a play on the more common term WIIFM (What’s In It For Me). Someone is footing the bill for the effort to improve data quality. They need to know that they are getting a decent Return on Investment (ROI) for the money they are pumping into salaries, consulting fees, and potentially software to enable the initiative. Most senior management doesn’t need to know all about your impressive counts of data records cleansed or merged, but by all means, produce these stats when appropriate (or asked). Instead, focus front-and-center on the improved business outcomes made tangible by achieving greater levels of data quality. Closing Chapter 2, Mc Gilvray encourages us to
‘Right-size the message to emphasize the topics that are most important to them. What they want to hear is that the improvement is working and that it is going to positively impact their work.’
[Executing DQ Projects Pg. 26]
Data Quality Specifications.
Obtaining current and relevant data specifications can be a time-consuming affair. Yet we need to know what ‘good’ is supposed to look like. Where they don’t exist (or can’t be found), we may have to re(write) specs before continuing with any planned data profiling exercises. The task will be much easier if the organization has already had some level of data governance implementation – whereby these artifacts may be both available and actively curated. If Enterprise Architecture is a well-established discipline in your organization– there is a good chance your EA people will know where to find what you need as prerequisites. Befriend them! And whatever you - don’t discount legacy system DBAs (Database Administrators) who usually have a wealth of knowledge to mine. Danette provides sage advice in that a likely scenario is that data specs around business rules are
‘in the minds of SMEs, but not written down
.
’
[Executing DQ Projects Pg. 102]
Or, if they are written down, the information may be
‘orphaned in files on network drives’
[Executing DQ Projects Pg. 102]
or potentially worse by being only on the desktop of a single individual.
Consider also the bi-directional relationship between business rules and data quality specs to better understand how the data under scrutiny contributes to the business processes in scope. This observation is outlined as follows in the book:
‘Data quality rule specifications explain, at the physical datastore level, how to check the quality of the data, which is an output of the adherence (or non-adherence) to the business rules and business actions. Data is the output of a business process, and violations of data quality rules can mean that the process is not working properly. It could also mean that the rule was incorrectly captured or misunderstood.’
[Executing DQ Projects Pg. 35]
While my personal style is that I will anyway be asking to interview current business and IT SMEs, it’s imperative early on to gauge the level of available (and valuable) documentation around data specifications. This advice goes for internal teams who need to work out the actual level of work that lies ahead and then set management expectations, but it equally applies to any data consultants who would be wise to get a feel for the state of this topic in any potential client organization before they commit to assisting with a cleanup project. Ideally, they should be asking for an initial (paid) ‘phase 1’ project to cover steps 1 and 2 so that you have a clear line in the sand to reconvene and be able to state what specs are available (or not) before committing further resources or promising results.
Perfection is a roadblock to progress.
People enthusiastic about data quality tend to be perfectionists, yet great can be the enemy of the good (enough). While you should be glad to have such folk, we need to be mindful that quality has a cost. There is a point (for most data sets) at which incremental gains in data quality outweigh the costs to maintain that level of perfection. Taking the material master in an SAP system just as an example, several hundred fields exist. They are not all equally ‘important’ and depend significantly on your business and process configuration. Danette suggests that
‘With few exceptions, data quality should not be viewed in terms of obtaining a state of “zero defect” or perfect data. More cost-efficient is a balanced, risk-based approach that defines data quality needs and investments in improvements based on business impacts and risks.’
[Executing DQ Projects Pg. 119]
One of the ways to focus data quality improvement efforts is to spend time defining CDEs (Critical Data Elements), socializing them among stakeholders, and ultimately deciding what quality level is acceptable for each and finally which should be tackled first for improvement.
How can we better control our test data?
I found myself in violent agreement with Danette. She says:
When application functionality testing fails, too often, too much time is spent investigating causes that end up being problems with the test data, not the software functionality.’
[Executing DQ Projects Pg. 131]
One sure-fire way to improve test performance is to have dedicated business-savvy data quality resources involved early on in software development and validation projects, and then throughout as necessary. I have participated in several ERP projects where a small band of data quality warriors supplied top quality material master test data. To get this right, you need at least some SMEs who not only know the data inside out but are well versed in the product and service supply chain. I can’t stress highly enough how often this is overlooked. I’m not suggesting that we handpick perfect data merely to ensure tests can pass; however, we need to know how the data ought to look and fit the new business processes. This can only be secured by involvement in process design and stage-gate reviews. Some project managers appear to think that ‘data people’ should stay in their lane and leave business process design to functional consultants and their assigned Business Process Owners. I beg to differ. It’s a partnership because knowledge acquired about to-be process can inform any data cleansing that needs to take place. I can recall several situations where we discovered that a field that had less significance in a legacy application, would now become a primary driver for Revenue Recognition processes in the new global ERP system. What previously only caused minor reporting errors when incorrect could now impact quarterly financial results. Such collaboration across teams is the lifeblood of well-run ERP projects.
Briefly remaining on the subject of testing and my experience of ERP projects, Danette underlines the importance of ensuring program management understands that sufficient time is set aside to fix source data and adjust migration programs in between test cycles.
‘Use your best persuasion skills to avoid this and ensure there is time to fix the data itself and/or the requirements. If not, all the trouble the testing team had will be multiplied for every user once the application has gone live’
. [Executing DQ Projects Pg. 165]
The reality in ERP projects is that you will never get a whole lot of time between cycles however making sure there is visibility to data quality activities will enable better discussions to be had around securing SME availability and prioritization around critical time periods. It’s better to get this horse-trading done early on because parachuting them in for extended periods of hypercare is not going to be cheaper or easier on the organization.
Business Process Workarounds
. We should not be entirely inflexible to business process workarounds. After all, the alternative – can’t be to do nothing and sit on our hands like spoiled children until we get what we want. Workarounds are effective when they are a temporary Band-Aid, such as a short-term solution for a few weeks after a system GoLive. It should have a defined expected time for a more permanent solution, and the follow-up to make it happen. In terms of master data management, an example could be that a new field was introduced into the material master of the new system. Data migration took care of filling it pre-GoLive. But when the first new materials are created, we quickly find that the field is left blank yet is critical to a business process. The root cause is that during the build phase, no group was assigned responsibility to maintain the field going forward. Somehow a discussion never happened between the functional team who added the field and the master data governance team who should have known about it and have already agreed to a RACI with an appropriate business function and embedded the new duties into existing procedures. The workaround, in this case, might be that someone on the project team is temporarily assigned the duty to maintain the field for up to six weeks, pending clarification as to who will ultimately be responsible operationally in the business.
Although this kind of temporary workaround can cause quite a lot of friction, it is nothing compared to the insidious nature of when workarounds become the begrudgingly accepted new normal. Danette offers some indications of what to expect from this in terms of impact:
‘Workarounds hide poor-quality data. Once workarounds become a normal part of business processes, people don’t realize that change is possible. They don’t see that poor-quality data causes costly problems and unnecessary distractions. By showing the effect of poor-quality data on the processes and the resulting costs, the business can make informed decisions about addressing issues that were previously unclear. Addressing process issues not only results in better quality data, it also increases efficiency.’
[Executing DQ Projects Pg. 182]
It can be all too easy to keep our heads in the sand. After all, there is a workaround in place, and the business is not falling apart. But long-term workarounds breed mistrust, i.e., prompting thinking along the lines of ‘management don’t listen to me, so I will keep my ideas to myself’ Left unaddressed, it can also be that ‘workaround fiefdoms’ are created with whole departments dedicated to executing them. It’s not difficult to see why there might not be much incentive to fix workarounds. I uncovered such a situation where a master data procedure stipulated that weights and dimensions for finished good materials should be entered by the Engineering department long before product ever showed up at a warehouse. Nice on paper, but when I visited distribution centers in two different countries (in ‘go to the Gemba’ fashion), I found them both entering this information and often overwriting each other. Not alone were procedures not being followed - but they were unfollowable as written. Multiple hands were inefficiently involved in constantly correcting it (all unbeknownst to one other). These are the types of workarounds that require determination to bring disparate groups together to make cost-effective decisions for the company and avoid duplicate work.
Cost-Benefit Analysis.
One time,
I created a very elaborate Cost-Benefit Analysis using a custom form that I developed specifically for a data quality project I was trying to get funded. It was very comprehensive and looked very nice but took way too long. I needn’t have spent that time had I just spoken sooner to our department controller. Not only did a form exist, but it was mandatory to use it. Any requests sent in without it – would get rejected at the first hurdle, no matter how elaborately compelling they might otherwise have been. In the end, all was not lost in that I could reuse and convert or transcribe much of what I had already done into the official form. Danette recognizes this in her book and says:
‘Make it easy for approvers to understand your request and see it as equally valid as other requests by using a form with which they are familiar.’
[Executing DQ Projects Pg. 203]
The reality is that most firms of any size will have such a form. You might have to ask a few questions to get to the right form because it might be called something else. Some examples I have seen over the years are:
Business Improvement Request (BIR) Form
CapEx (Capital Expenditure) Form
Continuous Improvement Idea Form
Business Case Identification Form
Customer Experience Improvement Form
I have also seen the financial analysis aspect separated from the idea. In these cases, a company might have a very simplistic form for capturing the essence of the idea. After initial vetting, business analysts experienced in business case development would reach out to the submitter. These intake funnels would typically have catchy names such as:
Be the Customer
Fix the Essentials
Time to Fix IT
As we can see, the approach and names for these forms differ widely. However, the main message should be that you likely won’t need to (and often should not) create your own forms.
Another tip I received from a friend who is CEO of a small manufacturing business is that you should be very careful not to overburden otherwise good employees to fill out your forms only to justify that, which is obviously a good idea. However, if you create hurdles like that – you may well find you no longer get good ideas, either because the people with the good ideas don’t have time to complete the form, or they are not trained in MBA finance. As regards data quality, my takeaway is that if you have a small team dedicated to data quality, you can get them trained on such forms and then have them work with data users who have good ideas to get the forms filled out to the required standard. This gives that data quality team experience and gets them known to be enablers as opposed to the police just out to find what else they did wrong that week. Please don’t let red tape get in the way of hearing a good idea.
2. What I partially knew but will now integrate more into current and future projects
The ultimate goal is for Data Quality to become part of the business process fabric.
To avoid ‘data debt’ stacking up, we need to do a better job of operationalizing data quality in much the same way as ‘lean’ concepts have largely been integrated into business processes rather than being the job of ‘someone else.’ Danette states that
‘… without a healthy ongoing program, within two years, the data quality work will start to wither and will eventually disappear. Later someone else wakes up to the importance of data quality and reconstructs what had been in place a few years before. This inefficient and costly way of managing data quality can be avoided by ensuring a foundational program is put into place.’
[Executing DQ Projects Pg. 19]
Constraints can be your friend.
It’s time well spent to articulate constraints as requirements in business processes and enabling software applications. However, for a book dedicated to data quality, Danette still manages to interject humor and wordplay at times! In one instance, she refers to handling constraints with the tongue twister:
‘that what we must do to ensure we do not do what we are not supposed to do.’
[Executing DQ Projects Pg. 96]
Some applications of constraints might be to enforce an entry in a data field, minimize the selectable choices, or implement if/then logic within hierarchy-based fields. While the idea is generally to reduce mistakes, in the future, I am going to spend more time documenting the reasons for constraints to ensure those maintaining the data are more aware of causes and consequences. It’s a very eminently debatable point, but I sometimes found that in a rush to make things simple to operate, we at times go too far and dumb things down more than they need to be.
It takes more than ‘sign here, and you are done’ training
to embed a data quality ethos. There is an immense potential to reassess the way we deliver and assess training for those who need to maintain accurate data. It is no longer appropriate or reasonable to assume that tracking evidence of reading procedures is sufficient to assume someone is trained or even deemed minimally capable. A quiz with a few trick questions also doesn’t prove proficiency; it merely proves that someone has learned to use the ‘back’ button in their web browser. Don’t get me wrong, I have worked most of my career to date in GMP environments and I understand that audit trail is important and non-negotiable.
A better approach may require a higher investment in permanent coach-like resources, who act as guides rather than auditors. These same resources can add further value by being close to the operational action to identify better if job aids and simulated scenario-based training meet ongoing needs. Danette refers to this subject generally when she writes about Life Cycle Thinking and states:
‘A potential data quality problem can be seen when asking the question, “Do all teams that maintain the data receive the same data entry training and do they have the same set of data entry standards?” If the answer is no, you can be sure you have a data quality problem – you just don’t yet know how big it is, or which pieces of data are most affected’
.
[Executing DQ Projects Pg. 40]
Along similar lines, we have to be more careful when writing and enforcing data maintenance procedures. While it is important to focus on accurate, up-to-date, and polished step-by-step work aids, we can’t hide behind them if the overall processes are not understood as to why and how we maintain the data. I’ll admit to having a blind spot on occasion in the past for defending procedures or not moving fast enough to implement feedback. While consideration needs to be made for the organizational burden, which will come with frequent updates, I believe we can be more creative in how we communicate ‘what’s new’ and do so in ways that best produce the desired outcome.
Selecting the right combination of business impact techniques
to make the case for Data Quality improvements
I had previously practiced most of the described ‘Business Impact Techniques’, but I now have a much greater situational awareness of when to use one over the other, or in combination with each other for greater effect.
Below (are the names of the different techniques described in detail in the book. Danette has also placed them on a time/complexity continuum which can help you gauge how much relative effort to expect.
Excerpt from Figure A.8 – Process flow for Step 4 – Assess Business Impact. From Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information™, 2nd Ed. (Elsevier/Academic Press, 2021) by Danette McGilvray. Pg. 317. Used with permission.
In the past, I had only seldom used the ‘Ranking and Prioritization’, which I now see can be a very effective group activity and a good way to instill team accountability for decisions around where to spend data quality time and energy. Danette also asks us to consider additional advantages:
‘One of the biggest benefits from this technique is the conversation between those who utilize or affect the quality of the same information, yet who may not usually interact with each other. A successful session will result in increased understanding and cooperation between those who are responsible for the quality of the information and those who depend on the information.
’
[Executing DQ Projects Pg. 197]
Five Whys’ is another Business Impact Technique that I find helpful when trying to get to the root cause of just about any business process issue, including data quality. It is by no means a new technique and should be familiar to those involved in ‘Lean’ or [Enter your company name here] Production System initiatives which brought lean outside the domain of manufacturing and into office processes during the past few years. The method requires high levels of trust and maturity. The goal is to be objective and fact-based, uncovering personnel issues and bringing to light less than stellar management decisions contributing to the problems. It’s all good if taken in the spirit of continuous improvement but getting people to be open and minimize defensiveness requires tact and careful preparation. Next time I use it, I am going to try out a variation of a roleplay that Danette recommends:
“We need to know more about why this issue matters to the company. I understand you have experience in this area, and I hope you can help. I’m going to ask you some questions. I will be probing a bit – asking why, who, when, where, etc. But I don’t want you to feel defensive. There are no right or wrong answers. It is all about helping us understand the business impact of this particular data quality issue. If you don’t know the answer, that is all right, too, and we can dig into the issue together. Is that all right? Do you have any questions before we begin?”
[Executing DQ Projects Pg. 181]
Finally, given my passionate interest in master data, I found it very helpful that Danette uses the Item Master (aka Material or Product Master) to illustrate how one might use a Fishbone (Ishikawa) diagram when attempting to get to the root cause of why certain data updates were not done in time to allow a product to ship. Please see Figure 4.5.6 (below) to understand how this was created and Table 4.5.5 (not reproduced here) to get an idea of how to extract recommendations from the identified root causes.
Figure 4.5.6 – Item master example using a fishbone diagram. From Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information™, 2nd Ed. (Elsevier/Academic Press, 2021) by Danette McGilvray. Pg. 217. Used with permission
.
The Data Governance as a House analogy.
A picture (of what data governance is about) is worth a thousand words. We’ve already stated that data quality and governance are two peas in the same pod. Yet, despite being involved with Data Governance for several years, I’ve struggled at times to describe it succinctly to stakeholders hearing of it for the first time or who balk at any utterance of the term ‘governance’. I’ve found that metaphors can be handy to get the concept across. Mc Gilvray provides us with the ‘shared house’ analogy, which I will now steal shamelessly and find a use for in the future:
‘A different level of coordination and cooperation is required to live together productively and peacefully in the same house than is needed to live as neighbors in separate dwellings. Any time a company integrates information, it is as though all the source systems with their associated people, business processes, and data – are moving in together. In [Danette’s] example, each family has its own room in one large home. Occupants of a particular room have the right to put down new flooring and decorate the way they want. However, none of them can change the plumbing or redecorate the living room (a common area for all) without the agreement of the others who live in the building. In some cases, the occupants can bestow authority on someone to make the plumbing and common-area decisions. They trust that person to make decisions for the benefit of everyone who lives in the building. However, they expect to be informed of changes and to be able to raise any issues that need attention. There need to be roles, responsibilities, rules, and processes in place for managing the house. In other words, governance is required.’
[Executing DQ Projects Pg. 65]
CSI (Crime Scene Investigation): Data Quality.
Thanks to the analogy Danette provides in her book; I have also learned to be more patient at the scene of ‘data crimes’. While there is a certain satisfaction in making things right as quickly as possible via expert data cleansing, there is more to be gained by taking the time to understand how the data got to be wrong in the first place. For sure, there are occasions when immediate correction is justifiable, e.g., the CEO wants the newly launched product to be shipped and billable. It would likely be career-limiting to hold up the show until you are finished dusting for prints! But in many cases, the data has been wrong for a while, and it can remain wrong for another few days or weeks until you are finished doing your forensics, interviewing the ‘suspects’, and have a theory about cause and motives. I keep a growing OneNote notebook full of ‘data disasters’, which one day might be enough to fill a book where I’ll need to change names and dates to protect the guilty.
Some all too typical master data disasters include:
Obsolete products available in online catalogues
Product descriptions not at all related with the actual item
Downright inappropriate product descriptions! (printing out on customer invoices)
Products blocked for sale authorized markets
Products not blocked for sale in unauthorized markets
Batch management not configured for batch managed materials (and then a recall)
Data not yet known.
When looking at some of the causes of master data quality issues, Danette makes an all too familiar and painful observation of something that plagued me for several years, i.e.,
‘Data quality issues are often found when the technology requires data, but the business does not have the information available when the record needs to be created or updated’.
[Executing DQ Projects Pg. 102]
Many ERP systems require that mandatory product master data fields are filled in order to progress to the next stage of the setup or material use. However, depending on PLM (Product Lifecycle Management) stage gates, we might need to assign and partially set up a material number today for a finished good that may not see the light of day commercially for many months, even years later. In such scenarios, it can well be that information needed later is simply not known at this time. Examples could be operational things like product storage and handling requirements or report-enabling data such as product hierarchy assignments. My experience is that pushing back on the business and expecting that they should know the information sooner to feed the system is unrealistic and sucks you into all kinds of stressful arguments that are not good for the business or your career. A better approach is recognizing that certain data can and will become better defined later in the setup / extension cycle. With this realization, you should build workflows and ‘catch’ mechanisms to make the necessary corrections closer to when they matter most. For example, latest before you need to ship a product for the first time – you had better by then know how it should be stored or handled in transit. It might also be that latest before the first financial month-end close after product launch, that the product hierarchy assignment should be final. Otherwise, the revenue may get reported in the wrong product line bucket.
In a somewhat related manner, it’s frequently the case that an organization has decided to utilize custom master data fields to drive unique business processes for Order to Cash or Logistics. If the processes won’t work without them, we need to require IT to make the field mandatory for saving the record. Danette reminds us here that:
‘If the application cannot be modified, document data entry requirements and train those who enter the data as to what should be entered and why.’
[Executing DQ Projects Pg. 132]
I would contend that such documentation and training is needed anyway and probably more so for custom fields than for any other data fields. My argument is that if the company is say a medium to large player in medical devices or pharma, they will likely already have SAP and that many experienced new hires will have used SAP before. You can make a reasonable assumption that these users are aware of standard functionality in everyday use at most firms. What you absolutely cannot expect is that they will understand features that were explicitly coded for your enterprise. If you want to get a competitive advantage from custom functionality, you should anticipate more training to ensure sufficient data quality for the inputs to such processes.
Who is responsible for the data?
A perennial question for companies making organizational decisions about enterprise data management is to conclude as to who should be responsible for creating and maintaining master data in the ERP systems, or as Danette puts it, the activity to
‘look at the organization to determine if it is appropriate that create and update abilities are distributed across departments or if these should be centralized in one place’.
[Executing DQ Projects Pg. 113]
While the ultimate decision is one for data governance and MDM strategy teams to concern themselves with, I have learned that a data quality assessment can provide valuable input because you need to investigate how the current organization setup contributes to the level of data quality issues being experienced. One of the tools Danette suggests is
a ‘Role-to-Data Interaction Matrix’ as outlined with an example in table 4.2.9 of the book.
The idea is that you map out the data creators and data consumers and what levels of access they have to the information in question. Sometimes you’ll find that data is maintained in the same department that uses it; other times that those who create it are far removed from those who need to make use of it. Although I know it’s a consulting cliché to say this, without a detailed understanding of company structure and culture, there is no single magic answer. However, one thing for sure is that anytime data maintainers are not properly aware of the end-use or are not incentivized, you can expect data quality problems. In addition, adverse knock-on effects will be clearly visible in the business processes relying on the data.
Credible sources for verifying data.
On the subject of navigating discussions about ‘book of record’ and which sources are most valid, Danette asks us to ensure that
‘… there is agreement among team members and stakeholders as to the appropriate authoritative source of reference or you risk distrust in the results.’
[Executing DQ Projects Pg. 136]
My experience has been that this is by no means a simple topic, especially in large companies used to operating in regional system silos and now all of a sudden have to come on board to the same platform, e.g., a single ERP instance. It can well be that different systems in a company all have pockets of data quality excellence but that it can be dangerous (or at least limiting) to pronounce that any one of them is the overall ‘book of record’ or ‘golden record’. The reality can be complex. For example, in one case, I discovered significant discrepancies between the finished good product descriptions used in the European regional sales and service ERP systems vs. what I found in the ‘headquarter’ global manufacturing system. After many weeks of analysis, we learned that the data admins of the regional system had changed the product descriptions after complaints over several years that the words used in over 900 product descriptions were not sufficiently similar to what was on the actual physical product box label. Ultimately, we used the finding to update all systems. Yet the fix wasn’t as simple as copy and paste. Since the original ‘corrections’ had been done based on direct complaints about certain product lines and country kits, only those were updated, leaving many related records (even within the same product families) inconsistent. I don’t want to use this example to show that good intentions can make things worse, but rather to highlight that there need to be companywide forums to raise data quality issues and mechanisms to make decisions for holistic corrections to occur, as well as to integrate findings back into the data maintenance processes to ensure data created going forward can be better.
Downstream effects … of mass corrections.
I learned a lesson once that just because the data cleansing updates need to be done, there may be good reasons why they should not be done immediately and instead be held and done on an agreed date. My example pertained to mass updates to several hundred material masters to get products reassigned to the ‘correct’ product line. The updates were carried out in mid-December to wrap up a data quality improvement mini-project for a business unit rearranging its product line structure. Luckily, I had a good relationship with the relevant Business Performance Controller because she tipped me off on December 23rd that something looked very wrong in her numbers and that she was wondering how a whole product line could disappear overnight. It turned out that financial reporting for the product line was supposed to stay as-is until the close of the 4th Quarter. While she had a workaround to move values from left to right in the reporting, it would be better if the sources were correct to avoid audit questions. All I can say is that it was busy Christmas Eve. I had to log in and manually revert the changes because there was no time to request access to the mass update tool. My key learning was to ensure you get buy-in from key stakeholders when doing mass changes on master data. This advice applies not only to the content of the data (which in this case, had been done) but also to the timing of said updates. It might be perfectly reasonable (essential even) to make immediate updates to operational master data, e.g., weights, storage conditions, etc. but for finance-data where consistency is required within a given time period, there may be reasons why updates need to be scheduled. Although I haven’t experienced much of this in the past few years, I have heard it come up in best practice sharing webinars as a common oversight. It seems to have a special place for Danette too, as she dedicates a ‘Caution’ box to it on her book:
Caution callout box. From Executing Data Quality Projects: Ten Steps to Quality Data and Trusted Information™, 2nd Ed. (Elsevier/Academic Press, 2021) by Danette McGilvray. Pg. 234. Used with permission
3. What I don’t know much about, but intend to learn
Data Stewardship vs. Ownership.
The debate around the use of the term ‘owner’ as in ‘Data Owner’ appears to be vigorous and ongoing. I must admit that I didn’t have much problem with it until recently. My reason for not opposing it was based on the old business adage from Peter Drucker: ‘what gets measured gets managed’ and my (somewhat skewed) reinterpretation of that being ‘what gets owned, gets done’. One has only to make a cursory search of LinkedIn Jobs to find quite a few open ‘Data Owner’ roles in reputable companies. A deeper reading of these job descriptions tells me that there’s no common agreement on the meaning of ‘owner’. In some cases, they are looking for someone who ‘calls the shots’ on a particular data domain … or is accountable for maintaining the data (a throat to choke). In other job descriptions, I see the term more used in the Agile sense of ‘Product Owner’ or as an enabling project manager. Either way, it seems like it’s probably a good idea to minimize or avoid such ambiguity. Danette describes her stance as:
‘I promote governance and the idea of stewardship as it relates to data and information, but I do not generally promote the use of “ownership.” Why? A steward is someone who manages something on behalf of someone else. Too often people act as if they “own” the data as in the first definition. “This data is mine. You cannot have it. I get to decide what to do with it.” This attitude is counterproductive to the well-being of the organization.’
[Executing DQ Projects Pg. 65]
I think I will investigate
Data Lineage
or ‘Track and Trace’ as a root cause analysis technique. Danette describes the method as
‘Identifying the specific location of a problem by tracking the data through the information life cycle, comparing the data at entry and exit points of processing, and identifying where the problem first appears.’
I see that
Irina Steenbeek
recently published a book called
Data Lineage from a Business Perspective
which I will probably pick up later this year (as I have a few others bought and need to finish first).
After getting a taste of the
ISO 8000-61 Data Quality Standard
in Danette’s book, I will definitely be looking into a related
online training by Data & Process Advantage
along with the associated book
Managing Data Quality: A Practical Guide
by
Tim King
and
Julian Schwarzenbach
For this year, I may yet attend an in-person conference if time permits, as I was supposed to participate in
EDW
in March before it was canceled. Failing that, I will at the very least participate in Scott Taylors’
Master Data Marathon
in April and the
MIT The Chief Data Officer and Information Quality Symposium
in July.
Longer-term, I am considering the
Information Quality Post-Graduate Programs
at UALR (University of Arkansas at Little Rock). They appear to have a very well-regarded M.S. in Information Science and even a Ph.D. in Information Science for the bold and the brave. I learned in a recent webinar hosted by
Dr. John Talburt
that UALR uses
Executing Data Quality Projects
as an approved textbook in their programs.
Final remarks
Getting to good data quality need not be a blood sport! But you need a method to the madness, and you need to be persistent and involve people to gain long last improvements.
I am starting to get the feeling that almost any one of these observations could be expanded into its own unique article. I’ll see if there is enough interest depending on how many views this article gets.
In the meantime:
What words of wisdom have you learned from reading Danette’s book or any other book on data quality?
Have you experienced any master of disaster data quality issues that you can share without naming names?
I’d really appreciate your thoughts in the comments section.
Thank you.
David Finlay
https://www.linkedin.com/in/davidjfinlay/
davidfinlay@datadrivenassociates.com
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
81
29 Comments
Dan Rimkus
CEO at Dynamic 3PL
9mo
Report this comment
David, thanks for sharing! Not sure if you could help... Trying to reach the right person... Who would I want to speak with at Data Driven Associates about logistics opportunities?
Like
Reply
1 Reaction
Nélio Mourato
Process Engineering Manager | Mechanical Engineer | Cold Drawn | Stainless Steel | Continuous Improvement | Lean | Six Sigma |
2y
Report this comment
Very nice article. This article is Top.
Like
Reply
1 Reaction
Shauna McGuigan
Maxsold,  Auction Lead
2y
Report this comment
Thank you David. Great article. Great insights.
Like
Reply
1 Reaction
Tim McLain
Chief Strategy Officer (CSO) @ Lexin Solutions & LinkedWares
2y
Report this comment
Thank you, David.  I work primarily in heavy industry and this passage struck a chord.  Easier said than done, but so true!

‘Workarounds hide poor-quality data. Once workarounds become a normal part of business processes, people don’t realize that change is possible. They don’t see that poor-quality data causes costly problems and unnecessary distractions. By showing the effect of poor-quality data on the processes and the resulting costs, the business can make informed decisions about addressing issues that were previously unclear. Addressing process issues not only results in better quality data, it also increases efficiency.’ [Executing DQ Projects Pg. 182]
Like
Reply
2 Reactions
3 Reactions
Anke van Amelsfoort
Master Data Lead at Organon
2y
Report this comment
Your review is almost a book on its own! Inspirational. I can relate that it is easy to focus on correcting the errors instead of looking on the data design to prevent these errors in the first place. Nice review.
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Becoming a data-driven Organisation
Jan 5, 2022
Master Data Management: a 2-for-1 MDM book review
May 1, 2021
Book Review for: Telling Your Data Story - Data Storytelling for Data Management
Feb 13, 2021
Book Review: Data Stewardship (2nd Ed) by David Plotkin
Jan 24, 2021
SAP Activate Project Management: How to get certified for less
Jul 6, 2020
The SAP ‘Cold chain Decision Cockpit’ for customer orders - realized?
Jun 12, 2020
'Official' (e)Books on SAP topics?
Jun 8, 2020
My Learning Journey with SAP and my Top 15 course recommendations
Jun 1, 2020
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Test Data Quality Across Formats: A Guide for Data Engineers,"How to Test Data Quality Across Formats: A Guide for Data Engineers
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Engineering
How do you test data quality across formats?
Powered by AI and the LinkedIn community
1
Data Quality Dimensions
Be the first to add your personal experience
2
Data Quality Framework
Be the first to add your personal experience
3
Data Quality Tools
Be the first to add your personal experience
4
Data Quality Testing Methods
Be the first to add your personal experience
5
Data Quality Testing Examples
Be the first to add your personal experience
6
Data Quality Best Practices
Be the first to add your personal experience
7
Here’s what else to consider
Be the first to add your personal experience
Data quality is a crucial aspect of data engineering, especially when dealing with different data formats. Data formats can vary in structure, syntax, encoding, and semantics, which can affect how data is processed, analyzed, and consumed. In this article, you will learn how to test data quality across formats using some common methods and tools.
Find expert answers in this collaborative article
Experts who add quality contributions will have a chance to be featured.
Learn more
See what others are saying
1
Data Quality Dimensions
Before testing data quality, you need to define what data quality means for your specific use case and data sources. Data quality can be measured by various dimensions, such as accuracy, completeness, consistency, timeliness, validity, and uniqueness. Each dimension can have different criteria and metrics depending on the data format and the expected output. For example, accuracy can be checked by comparing data values with a reference source, completeness can be checked by counting missing or null values, and consistency can be checked by ensuring data formats follow the same standards and rules.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
2
Data Quality Framework
To test data quality across formats, you need to establish a data quality framework that covers the entire data lifecycle, from ingestion to consumption. A data quality framework consists of four main components: data quality rules, data quality checks, data quality reports, and data quality actions. Data quality rules are the specifications and expectations for each data quality dimension and metric. Data quality checks are the processes and methods to apply the data quality rules to the data sources and outputs. Data quality reports are the outputs and feedbacks of the data quality checks, such as dashboards, alerts, or logs. Data quality actions are the corrective and preventive measures to resolve or prevent data quality issues, such as data cleansing, data validation, or data governance.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Data Quality Tools
To implement the data quality framework, you need to use appropriate tools and technologies that can handle different data formats and perform data quality checks and actions. There are various types of data quality tools available, such as ETL tools, data validation tools, data profiling tools, data cleansing tools, and data monitoring tools. Each tool has its own features and capabilities, and you need to choose the ones that suit your data engineering needs and goals. For example, ETL tools can help you transform and load data from different formats into a common format, data validation tools can help you verify and enforce data quality rules and standards, data profiling tools can help you discover and analyze the characteristics and patterns of your data sources, data cleansing tools can help you identify and fix data errors and anomalies, and data monitoring tools can help you track and report data quality metrics and issues.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Data Quality Testing Methods
To perform data quality testing across formats, you need to apply different methods and techniques depending on the type and level of testing required. Data quality testing methods can be classified into two categories: static testing and dynamic testing. Static testing is the testing of data sources and outputs without executing any code or query, such as inspecting the data format, schema, metadata, or documentation. Dynamic testing is the testing of data sources and outputs by executing code or query, such as querying the data values, aggregations, or calculations. Static testing can help you detect data quality issues at an early stage, while dynamic testing can help you validate and verify data quality at a later stage.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Data Quality Testing Examples
To demonstrate how data quality testing can be conducted across formats, consider a CSV file, a JSON file, and a SQL database as the data sources. Static testing can utilize tools such as CSVLint and JSONLint to examine the syntax and structure of the files, and DataCleaner or Trifacta Wrangler to analyze the metadata and statistics of the files and database tables. Additionally, Dataedo or SchemaSpy can generate and review the data schema and relationships of the sources. Dynamic testing utilizes an ETL tool like Talend or Pentaho to transform and load the CSV and JSON files into a staging database. Great Expectations or Deequ then run SQL queries and assertions to check the data values, calculations, and aggregations of the sources and staging tables. OpenRefine or Data Ladder can identify and fix any data errors or inconsistencies in the sources or staging tables. Finally, DataDog or Grafana create and monitor data quality dashboards and alerts, while Collibra or Alation manage and document the data quality rules, standards, and actions.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Data Quality Best Practices
To ensure data quality across formats, you should adhere to certain best practices and principles throughout your data engineering process. Firstly, it's important to define your data quality requirements and objectives clearly and explicitly, and make sure they align with your business goals and user expectations. Secondly, document your data sources, formats, schemas, rules, standards, and processes and keep them updated and accessible. Automating data quality checks, reports, and actions is also recommended to streamline the process. Additionally, testing your data quality at different stages and levels is essential for ensuring accuracy. Finally, you should monitor and measure your data quality metrics and issues regularly to get feedback and insights that can help you improve your data quality strategy and performance.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Engineering
Data Engineering
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Engineering
No more previous content
Clients demand both real-time and batch processing. How do you prioritize their needs?
2 contributions
You're faced with data accuracy and tight ETL deadlines. How do you strike a balance between the two?
3 contributions
You're facing a surge in data volume. How can you fine-tune ETL pipelines to manage it effectively?
4 contributions
You're facing a complex data migration. How do you prevent data loss?
20 contributions
You're facing conflicting opinions on data infrastructure upgrades. How do you prioritize tasks effectively?
3 contributions
Your organization is hesitant about cloud migration. How can you convince them of the benefits?
4 contributions
Struggling to balance data quality and processing speed in ETL workflows?
13 contributions
You've faced a data anomaly in your analysis. How do you prevent it from happening again?
4 contributions
You're facing data pipeline performance issues. How can you optimize them without causing disruption?
6 contributions
Clients are frustrated with delays in real-time data delivery. How do you handle their complaints?
7 contributions
You need to onboard new team members on data governance. How do you make it effective and engaging?
11 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Engineering
How do you track data lineage?
Data Architecture
How can you maintain data integrity during data transformation?
Software Development
How can you maintain data quality and governance in a dimensional model?
Data Management
How can you effectively handle duplicated data during transformation?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share"
Data Quality and Testing Strategies for Data Engineering,"Data Quality and Testing Strategies for Data Engineering
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Engineering
How do you reduce data quality and testing risks in your engineering projects?
Powered by AI and the LinkedIn community
1
Define data quality requirements
Be the first to add your personal experience
2
Implement data quality checks
3
Design data tests
4
Apply data governance
Be the first to add your personal experience
5
Review data quality and testing results
Be the first to add your personal experience
6
Learn from data quality and testing incidents
Be the first to add your personal experience
7
Here’s what else to consider
Be the first to add your personal experience
Data quality and testing are crucial aspects of any data engineering project, as they ensure the reliability, accuracy, and usability of the data products and pipelines. However, data quality and testing can also pose significant challenges and risks, such as data inconsistencies, errors, delays, and failures. In this article, you will learn some practical strategies to reduce data quality and testing risks in your data engineering projects, and how to apply them in different stages of the data lifecycle.
Top experts in this article
Selected by the community from 2 contributions.
Learn more
Nicholas Leong
View contribution
1
See what others are saying
1
Define data quality requirements
The first step to reduce data quality and testing risks is to define the data quality requirements for your project, such as the expected data sources, formats, schemas, standards, rules, and metrics. This will help you to identify the data quality dimensions that are relevant for your project, such as completeness, validity, accuracy, timeliness, consistency, and integrity. You should also document the data quality requirements and communicate them to all the stakeholders involved in the project, such as the data owners, consumers, and analysts.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
2
Implement data quality checks
The second step is to implement data quality checks throughout your data pipelines and processes, using tools and techniques such as data profiling, validation, cleansing, enrichment, and monitoring. Data quality checks can help you to detect and correct data quality issues early, before they propagate downstream and affect the data products and outcomes. You should also automate the data quality checks as much as possible, using frameworks and platforms such as Apache Airflow, Apache Beam, or AWS Glue DataBrew.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Nicholas Leong
Copy link to contribution
Report contribution
Don't underestimate the significance of Data Quality checks and validation. If none of them are being implemented, it's a red flag for a faulty data pipeline. Go back to the drawing board and create a visual diagram representing the flow of data within your business.

Brainstorm with the team. Examine each stage of the data pipeline and identify where data validation should occur. Typically, validation steps take place after data is loaded or transformed in a system.

These validation checks should be integrated into the pipeline in such a way that if they fail, appropriate measures are taken to ensure that downstream tasks are impacted as minimally as possible.

We don't want stakeholders to be looking at inaccurate data now, do we?
…see more
Like
1
3
Design data tests
The third step is to design data tests that verify the functionality, performance, and reliability of your data pipelines and products, using tools and techniques such as unit testing, integration testing, end-to-end testing, and regression testing. Data tests can help you to ensure that your data pipelines and products meet the specifications and expectations of the project, and that they can handle different scenarios and conditions. You should also follow the best practices of data testing, such as using test data sets, test cases, test scripts, and test reports.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Nicholas Leong
Copy link to contribution
Report contribution
Develop data validation tests for critical tables aligned with your business operations. If you have a dataset related to your business's revenue, it makes sense to include a test case that focuses on the revenue field.

For instance, you can create a test by calculating the sum of revenue and comparing it to a predefined benchmark or 'source of truth.'

There are various tools available to assist in crafting effective data tests, and one of them that we've been leveraging extensively is DBT. It has significantly streamlined our data validation processes and enhanced our efficiency.
…see more
Like
1
4
Apply data governance
The fourth step is to apply data governance principles and policies to your data engineering project, such as data ownership, stewardship, lineage, security, privacy, and compliance. Data governance can help you to manage the data quality and testing risks across the entire data lifecycle, from data acquisition to data consumption. You should also establish a data governance framework and team that oversees and coordinates the data quality and testing activities and standards, and that resolves any data quality and testing issues and conflicts.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Review data quality and testing results
The fifth step is to review the data quality and testing results regularly, using tools and techniques such as dashboards, reports, alerts, and feedback. Reviewing the data quality and testing results can help you to evaluate the effectiveness and efficiency of your data quality and testing strategies, and to identify any gaps, errors, or improvements. You should also share the data quality and testing results with the relevant stakeholders, and solicit their feedback and suggestions for continuous improvement.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Learn from data quality and testing incidents
The sixth step is to learn from the data quality and testing incidents that occur in your data engineering project, such as data anomalies, errors, failures, or breaches. Learning from the data quality and testing incidents can help you to prevent or mitigate them in the future, and to improve your data quality and testing capabilities and maturity. You should also document and analyze the root causes, impacts, and solutions of the data quality and testing incidents, and implement the necessary actions and changes to avoid recurrence.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Engineering
Data Engineering
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Engineering
No more previous content
Clients demand both real-time and batch processing. How do you prioritize their needs?
2 contributions
You're faced with data accuracy and tight ETL deadlines. How do you strike a balance between the two?
3 contributions
You're facing a surge in data volume. How can you fine-tune ETL pipelines to manage it effectively?
4 contributions
You're facing a complex data migration. How do you prevent data loss?
20 contributions
You're facing conflicting opinions on data infrastructure upgrades. How do you prioritize tasks effectively?
3 contributions
Your organization is hesitant about cloud migration. How can you convince them of the benefits?
4 contributions
Struggling to balance data quality and processing speed in ETL workflows?
13 contributions
You've faced a data anomaly in your analysis. How do you prevent it from happening again?
4 contributions
You're facing data pipeline performance issues. How can you optimize them without causing disruption?
6 contributions
Clients are frustrated with delays in real-time data delivery. How do you handle their complaints?
7 contributions
You need to onboard new team members on data governance. How do you make it effective and engaging?
11 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Engineering
You're facing project delays from unexpected data quality issues. How can you get back on track efficiently?
Data Engineering
How can you maintain data quality across different engineering tools?
Data Engineering
How can you align data quality monitoring with data loading processes?
Data Engineering
How can you ensure consistent data quality across multiple sources in a large-scale project?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
3
2 Contributions"
Big Data Testing: Bugs are not Allowed,"Big Data Testing: Bugs are not Allowed
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing: Bugs are not Allowed
Report this article
Alex Protasov
Alex Protasov
Delivering Perfection through QA Leadership | Co-Founder @ Cherish DEV
Published Sep 16, 2022
+ Follow
The global market for Big Data tools and services was valued at $162.6 billion in 2019, according to research company MarketsandMarkets. This figure is expected to grow significantly in the coming years as more organizations invest in Big Data technology to gain insights from their data. As Big Data is increasingly used by companies and governments for various purposes such as fraud detection and risk management, it is evident that the Big Data world is dynamic.
While it's easy to understand what Big Data is by simply looking at the size of the data stored, it might be a bit difficult to wrap your head around the actual meaning of the term. And what is more important and relevant is Quality Assurance and testing of Big Data. Relevance, accuracy, duplication, consistency, reliability and completeness of data are important criteria. If they are not met, many companies can’t exist. Let's dive into the world of Big Data together!
1. What is Big Data?
To learn Big Data testing, it’s necessary to have a clear understanding of the concept.
Big Data is a data set constantly growing over time. Big Data is the term for data sets that can’t be processed by conventional database management systems like SQL. This is because they are too large for a single database to handle. They are typically distributed across multiple data centers around the world.
When it comes to Big Data, most often you can find a chart with the letters V. There can be from 3 to 7 V’s (and sometimes even more). But the main principles lie in three of them: Variety, Velocity and Volume.
Your data may vary in structure (Variety):
Structured.
The file type is known because the format is set.
Unstructured
. The file format is unknown as it can be in the form of images, videos, flat files, etc.
Semi-structured.
A combination of structured and unstructured data.
Velocity means significant speed of data receipt and processing when working with Big Data. There are two approaches to handling Big Data:
Batch data processing
. The approach assumes that the data is finite, and its processing can take as long as necessary. It can also be suspended.
Streaming data processing
. Data processing must be carried out with a guarantee at the rate of data arrival and once.
Volume is one of the main characteristics, since we are talking about a large amount of data. The volume can be defined in petabytes, exabytes and zettabytes.
2. What is Big Data testing?
Big Data testing is a process that helps ensure the quality of data by performing various functions such as creating, storing, extracting, and analyzing it. This is imperative for businesses as Big Data has become increasingly important for making decisions. Traditional testing methods are not always able to handle the requirements of Big Data, so a separate strategy is needed.
When testing a Big Data application, it is more important to focus on performance and functionality testing rather than individual features of the software product. This will ensure that the data processing is up to par and meets all expectations.
Since processing is fast, a high level of testing skills is required. Big Data testing also puts emphasis on data quality. Therefore, it is crucial to assess the quality of data before testing the application.
Traditional testing uses structured data and doesn’t require a special environment, as file size is limited. When testing, Excel macros or UI automation tools are available.
At the same time, Big Data is not always structured; the size is not limited, so a special environment is required. The tools vary from ones in traditional testing: MapReduce and HIVEQL are often used.
3. What to consider when testing Big Data?
When choosing a testing strategy, you need to understand that the tester has only input information, which is almost impossible to manage.
Consider the following three aspects:
1. Data verification
Data must be loaded into a Big Data system from a source using extraction tools. It is crucial to confirm that the right data is entered into the system.
2. Data processing
After running on multiple nodes, testers must again check the business logic and data validation. This helps ensure that the results are as expected.
3. Checking the output
The output validation process is critical to ensuring the accuracy of data. This process involves migrating the generated output data files to the Enterprise Data Warehouse and then checking the transformation logic to ensure that the data is intact.
When developing your Big Data testing approach, you will have to choose between mocked data and real data.
Data Mocking allows you to test the correctness of the transformation, the proper unloading of information from mocked data. The tester will be able to cover all data flow functions with tests, but will not be able to be completely sure that everything functions smoothly online.
When testing on real data, special tests are created for real information, but the tester doesn’t fully understand how many of them will be in total and in what form they will be displayed. Tests that can be based on data that exists in real use are created.
Approaches can be combined. This makes it possible to fully verify that everything is functioning correctly online.
3. Conclusion:
Big Data is a term used to describe a huge amount of data that is difficult to store and process. Big Data has become a big deal in recent years, and it has a lot of potential to help businesses who want to use it. However, Big Data also has a lot of potential to cause big problems, so it's important to know how to test Big Data software to ensure that it's up to snuff.
Big Data testing differs from the traditional one as it considers data verification, processing, and the output. Testing Big Data applications is unusual. It poses many challenges for QA. But the growing interest in the field will soon make this type of testing habitual.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
8
To view or add a comment,
sign in
More articles by Alex Protasov
Advanced Code Coverage Techniques for Java Applications Using JaCoCo
Aug 21, 2024
Advanced Code Coverage Techniques for Java Applications Using JaCoCo
Code coverage is a metric used to measure the effectiveness of tests. Among the tools at the disposal of Java…
9
Optimizing Selenium WebDriver for Large-Scale Test Automation: Technical Review
Apr 17, 2024
Optimizing Selenium WebDriver for Large-Scale Test Automation: Technical Review
Selenium WebDriver is an open-source tool for automating web browsers, widely used for end-to-end testing of web…
11
1 Comment
Risk-Based Testing Strategy
Apr 3, 2024
Risk-Based Testing Strategy
Agile methodology is valued for its flexibility, customer-centric approach, and emphasis on rapid delivery. With each…
11
IT Compliance and Governance in the Age of Regulation
Mar 20, 2024
IT Compliance and Governance in the Age of Regulation
Technology threads through every aspect, binding together processes, people, and potential. Yet, as our reliance on…
9
Chatbot Testing Strategies: Ensuring Your AI Conversations Make Sense
Mar 12, 2024
Chatbot Testing Strategies: Ensuring Your AI Conversations Make Sense
Chatbots are tools for providing efficient customer service, engaging user experiences, and streamlining business…
21
Manage IT Assets Effectively
Feb 21, 2024
Manage IT Assets Effectively
IT asset management (ITAM) is a key component of efficiency and savings in business. With global IT spending reaching…
10
Strange myths about remote work
Feb 15, 2024
Strange myths about remote work
Many myths and misconceptions have formed around remote work, creating incorrect expectations and distorted ideas. Some…
13
Test-Driven Development in JavaScript: Practices and Errors
Feb 2, 2024
Test-Driven Development in JavaScript: Practices and Errors
Test-Driven Development in JavaScript: Practices and Errors It all started with a trivial bug. A bug that bypassed our…
12
Testing Asynchronous Operations in Microservices
Jan 5, 2024
Testing Asynchronous Operations in Microservices
Microservices are like a team of people, where everyone specializes in their task. Together they solve a big problem…
9
Stress Testing in Multi-cluster Kubernetes Environments
Dec 1, 2023
Stress Testing in Multi-cluster Kubernetes Environments
Stress testing is a type of testing performed to evaluate the reliability and stability of a system under high load…
15
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Discover thousands of collaborative articles on 2500+ skills,"Discover thousands of collaborative articles on 2500+ skills
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Sign in to view more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
We can’t find the page you’re looking for.
The page you’re looking for may have been moved, or may no longer exist. Try going back to the previous page or check out more articles in this collaborative article page.
Learn more about Collaborative Articles
We’re unlocking community knowledge in an all new way. It starts with an article on a professional topic or skill, written with the help of AI — but it’s not complete without insights and advice from people with real-life experiences. We invited experts to contribute.
Learn more
What are some effective strategies for aligning workforce development with organizational goals?
1 contribution
4 hours ago
Learn some effective strategies for designing and implementing a strategic workforce development plan that fosters employee engagement and performance.
Performance Management
HR Management
Your team is stuck in a problem-solving deadlock. How can you break the impasse and move forward effectively?
2 contributions
3 minutes ago
<end output>
Creative Problem Solving
Soft Skills
You’re leading a long-term project with no end in sight. How do you keep your team productive and motivated?
1 contribution
4 hours ago
<end output>
Project Leadership
Soft Skills
Your team members are feeling undervalued during organizational change. How can you show them their worth?
1 contribution
4 hours ago
Show appreciation and maintain morale during organizational change with gratitude, support, and inclusion. Involve your team to help them feel valued.
Change Management
Business Administration
You're dealing with conflicting team dynamics. How can you cultivate resilience to navigate through it all?
1 contribution
4 hours ago
Dealing with conflicting team dynamics can be tough. Learn how to build resilience and navigate through the challenges with these effective strategies.
Personal Development
Education
Your ERP system just crashed unexpectedly. How can you quickly find and fix the root cause?
1 contribution
4 hours ago
When your ERP system crashes, quickly find and fix the root cause using system logs, reviewing recent changes, and consulting with IT support.
Enterprise Resource Planning (ERP)
Business Administration
You need reliable data for your warehouse integration. How do you ensure its validity?
1 contribution
4 hours ago
<end output>
Data Warehousing
Engineering
You're investigating employee misconduct. How do you ensure confidentiality every step of the way?
3 contributions
5 hours ago
<end output>
HR Consulting
HR Management
Dealing with unexpected technical challenges in an agile sprint: How can you stay on track?
1 contribution
5 hours ago
Stay on track during an agile sprint by quickly assessing technical issues, reprioritizing tasks, and communicating changes effectively.
Software Design
Engineering
You're facing a client demanding access to sensitive data. How do you navigate this risky request?
3 contributions
5 hours ago
Addressing a client's demand for sensitive data? Keep your relationship strong with these strategies for a secure and respectful response.
Data Architecture
Engineering
Your client doubts your direct sales delivery process. How can you convince them of its reliability?
1 contribution
5 hours ago
<end output>
Direct Sales
Sales
You're juggling quotas and client demands in direct sales. How do you maintain work-life balance?
1 contribution
5 hours ago
<end output>
Direct Sales
Sales
Balancing client demands and work-life balance: How do you keep everyone satisfied?
1 contribution
5 hours ago
<end output>
Staffing Services
HR Management
You're faced with unexpected projects and urgent tasks. How do you effectively adjust workload distribution?
1 contribution
5 hours ago
Handle unexpected projects and urgent tasks with ease. Follow these strategies to efficiently adjust your workload distribution and maintain productivity.
Staffing Services
HR Management
You have a candidate with a thin resume. How do you assess their true potential in an interview?
3 contributions
5 hours ago
<end output>
Interviewing
HR Management
You're at a networking event to make a lasting impression. How can you showcase your expertise effectively?
1 contribution
5 hours ago
Highlight your skills effectively at networking events with these strategies. Prepare an elevator pitch, ask insightful questions, and share success stories.
Business Networking
Business Administration
You've just finished a code review. How do you handle clients' sudden change requests?
1 contribution
5 hours ago
Adapt to client change requests post-code review without losing your cool. Set boundaries, communicate clearly, and offer smart alternatives.
Computer Science
Engineering
Your team has conflicting coding standards preferences. How do you navigate towards a unified approach?
1 contribution
5 hours ago
<end output>
Programming
Engineering
You're a personal coach with limited time. How do you guarantee each client gets personalized attention?
1 contribution
5 hours ago
Balance a busy coaching schedule while providing personalized attention. Use smart scheduling, tailored communication, and focused sessions to succeed.
Personal Coaching
HR Management
Your coachee is overwhelmed by conflicting feedback. How do you help them navigate through it?
1 contribution
5 hours ago
Assist your coachee in navigating through conflicting feedback by aligning advice with core values, seeking consensus, and setting boundaries for irrelevant…
Personal Coaching
HR Management
Your coaching clients are resistant to change. How can mindfulness help you connect with them?
1 contribution
6 hours ago
<end output>
Life Coaching
Soft Skills
Your FPGA project hits unexpected roadblocks. How do you adapt your plans?
1 contribution
6 hours ago
When facing FPGA project roadblocks, reassess your timeline, seek expert advice, and focus on modular testing to stay on track.
Electronic Engineering
Engineering
You're struggling to track your social media prospecting efforts. How can you measure success effectively?
3 contributions
6 hours ago
Improve your social media prospecting with these measurement tips. Track engagement, monitor conversions, and evaluate ROI to gauge success.
Sales
You're drowning in emails and digital files. How will you efficiently organize everything?
1 contribution
6 hours ago
Tackle email and digital file overload with these practical tips. Sort with folders, automate with filters, and schedule clean-ups to stay organized.
Administrative Assistance
Market trends are defying your clients' expectations. How do you manage their concerns?
1 contribution
6 hours ago
<end output>
Real Estate
You're faced with a client accusing you of favoritism. How do you navigate this delicate situation?
1 contribution
6 hours ago
<end output>
Case Management
Healthcare
You're facing unexpected design hurdles with remote team members. How can you ensure seamless collaboration?
3 contributions
6 hours ago
Ensure seamless collaboration in remote team design projects. Use shared platforms, schedule check-ins, and employ real-time tools for overcoming unexpected hurdles.
Engineering Design
Engineering
You're tasked with moving from paper to digital case files. How will you handle the transition?
1 contribution
6 hours ago
<end output>
Case Management
Healthcare
Your office is undergoing a major reorganization. How will you keep everyone informed?
1 contribution
6 hours ago
<end output>
Administrative Assistance
You want to give feedback that uplifts your team. How can you ensure it's constructive and not demoralizing?
1 contribution
6 hours ago
<end output>
Servant Leadership
Business Administration
A senior executive insists on a conflicting meeting time. How do you manage this scheduling dilemma?
3 contributions
6 hours ago
<end output>
Administrative Assistance
You're facing a tight deadline and need files instantly. What system ensures you find them fast?
3 contributions
6 hours ago
<end output>
Administrative Assistance
You're overwhelmed by administrative tasks in Higher Education. How can you streamline them effectively?
1 contribution
6 hours ago
Overwhelmed by administrative tasks in higher education? Find out how to streamline your workload with automation, delegation, and workflow reviews.
Higher Education
Education
Stakeholders are doubting your QA results. How do you handle their concerns?
3 contributions
6 hours ago
Handle stakeholder doubts about QA results with transparency, inviting feedback, and showcasing improvements to build trust.
Quality Assurance
Manufacturing
You're facing a client pushing for cost-cutting measures. How do you maintain design quality in your project?
2 contributions
5 hours ago
Facing client pressure for cost-cutting? Discover strategies to maintain design quality without breaking the bank in your projects.
Real Estate Development
Real Estate
Retail clients have fluctuating seasonal demands. How can you sell them IT solutions effectively?
1 contribution
7 hours ago
Sell IT solutions effectively by understanding retail clients' seasonal cycles, offering scalable services, and highlighting cost-saving benefits.
IT Sales
Sales
Your team member is struggling with new processes. How can you provide effective feedback to help them adapt?
4 contributions
1 hour ago
Support your team member's adaptation to new processes with clear feedback, open dialogue, and ongoing support.
Team Leadership
Soft Skills
What are the pros and cons of FPGA vs ASIC in terms of performance, cost, and flexibility?
4 contributions
6 hours ago
Compare the pros and cons of FPGA and ASIC for system design in terms of performance, cost, and flexibility. Learn how to select the best option for your needs.
Electrical Design
Engineering
Your network teams are clashing over new tech integration. How can you resolve these conflicts?
3 contributions
7 hours ago
<end output>
Network Engineering
Engineering
Developers and designers clash over app features. How do you mediate effectively?
1 contribution
7 hours ago
<end output>
Mobile Applications
Engineering
You're struggling with software integration delays. How can you meet stakeholder expectations effectively?
3 contributions
7 hours ago
Facing delays in software integration? Manage stakeholder expectations effectively by communicating proactively, setting realistic milestones, and involving…
Business Analysis
Business Administration
How do you design and implement BPR solutions using agile and lean methodologies?
1 contribution
7 hours ago
Learn how to use agile and lean principles and practices to design and implement BPR solutions that are flexible, adaptive, and customer-centric.
Business Process Re-engineering
Different authorities are giving conflicting messages during an emergency. How do you handle it?
1 contribution
7 hours ago
Conflicting information in emergencies can be confusing. Use these strategies to verify sources, centralize communication, and stay adaptable.
Emergency Management
Public Administration
Your presentation accuracy is questioned by a colleague. How will you respond to maintain credibility?
4 contributions
7 hours ago
When colleagues question your presentation, respond confidently. Acknowledge feedback, verify data, and offer collaboration to maintain credibility.
Presentations
Soft Skills
You're struggling to align priorities with marketing teams. How can you effectively analyze customer data?
3 contributions
7 hours ago
Bridge the gap between sales and marketing by leveraging customer data. Use insights to tailor messages, identify upsell opportunities, and track engagement.
Data Analytics
Engineering
You're expanding operations to a new country. How will you navigate the logistics effectively?
3 contributions
7 hours ago
Expand your business internationally with ease by understanding local regulations, partnering with experts, and streamlining your supply chain.
Business Strategy
Business Administration
You're coaching a high-ranking client with significant influence. How do you handle power imbalances?
1 contribution
7 hours ago
<end output>
Coaching & Mentoring
HR Management
You're proposing innovative ideas at work. How do you manage peer resistance?
3 contributions
1 day ago
<end output>
Career Development
HR Management
Your client is emotionally overwhelmed during vocal coaching. How can you effectively address their needs?
1 contribution
7 hours ago
Help your vocal coaching clients manage emotional overwhelm with these compassionate strategies. Create a safe space, use calming techniques, and adjust lesson…
Vocal Coaching
Leisure Industry
You're building your startup with big dreams. How do you balance ambition with realistic goals?
3 contributions
7 hours ago
<end output>
Entrepreneurship
Business Administration
You're sharing your stunning photos on social media. How can you ensure they're protected?
3 contributions
8 hours ago
<end output>
Photography
Art
A team member is stifling brainstorming. How can you ensure everyone's input is heard and valued?
4 contributions
1 hour ago
Create an inclusive brainstorming environment with these key strategies: set ground rules, use structured techniques, and engage all team members.
Creative Problem Solving
Soft Skills
Event organizers want more than you can deliver. How do you manage their expectations?
1 contribution
8 hours ago
<end output>
Motivational Speaking
Soft Skills
You're managing a flight crew's performance and fatigue. What tools can help you succeed?
1 contribution
8 hours ago
</begin output>
Civil Aviation
Managing a diverse healthcare team with varied schedules. How can you ensure effective communication?
1 contribution
8 hours ago
Manage a healthcare team with varying schedules effectively. Use digital tools, regular meetings, and openness to improve communication and care.
Healthcare Management
Healthcare
You’re looking for ways to build your professional network. How can you connect with industry professionals?
1 contribution
8 hours ago
Learn how to build your professional network in the music industry with these tips on attending events, joining online communities, creating content, and more.
Music Industry
You need to share resource allocation decisions with stakeholders. How can you do it effectively?
1 contribution
8 hours ago
<end output>
Business Management
Business Administration
Your executive team is facing unresolved conflicts. Can data analytics be the key to harmony?
1 contribution
8 hours ago
Use data analytics to resolve executive team conflicts. Identify patterns, make unbiased decisions, and monitor progress for lasting harmony.
Executive Management
Business Administration
Team members are at odds due to role ambiguity. How do you navigate conflicts and restore harmony?
4 contributions
1 hour ago
Resolve internal team conflicts caused by role ambiguity with clear job descriptions, open communication, and prompt mediation. Create a collaborative environment…
Conflict Resolution
Soft Skills
You're navigating diverse prospects in sales. How do you tailor your negotiation style for maximum success?
1 contribution
8 hours ago
Tailor your sales negotiation tactics to suit diverse clients with these strategic tips. Build rapport, listen actively, and offer flexible solutions for success.
Sales Management
Sales
How can you clearly define team roles and responsibilities?
1 contribution
8 hours ago
Learn how to define team roles and responsibilities for effective team facilitation using frameworks, tools, communication, and feedback.
Team Facilitation
Business Administration
Your sales team is struggling to adapt to a new market. How can you guide them to success?
1 contribution
8 hours ago
Guide your sales team to success in a new market with tailored training, clear goals, and fostering collaboration.
Sales Management
Sales
You're rolling out diversity and inclusion programs. How do you measure their success?
3 contributions
8 hours ago
</begin output>
HR Operations
HR Management
You're striving for fresh content and long-term SEO success. How do you find the perfect balance?
1 contribution
8 hours ago
<end output>
Content Development
Content Management
Stakeholders are rushing content creation timelines. How will you ensure high-quality deliverables?
1 contribution
8 hours ago
Keep content quality high even with rushed timelines by streamlining processes, setting realistic goals, and focusing on core tasks.
Content Development
Content Management
You're striving for team cohesion. How do you ensure your ideas align with the team's vision?
5 contributions
3 hours ago
Ensure your ideas contribute positively to your team's vision. Engage in open dialogue, offer enhancements, and build consensus with these strategies.
Teamwork
Soft Skills
Your marketing team faces accusations of false advertising. How do you respond to the public platform user?
5 contributions
1 hour ago
Respond to false advertising accusations with transparency and prompt action. Review the situation internally and communicate openly with customers.
Marketing
You're faced with conflicting opinions on new technology adoption. How do you make the right strategic move?
1 contribution
8 hours ago
Choose the right technology for your business by analyzing data, seeking broad feedback, and running pilot tests. Make strategic decisions confidently amidst…
Decision-Making
Soft Skills
Market instability is hitting commercial real estate hard. How do you calm tenant concerns?
3 contributions
8 hours ago
<end output>
Commercial Real Estate
Real Estate
Stakeholders have underestimated BI project resources. Are you prepared to navigate the challenges ahead?
9 contributions
3 minutes ago
Manage underestimated BI project resources by conducting needs assessments, communicating transparently, and using agile management.
Business Intelligence (BI)
Business Administration
You're negotiating with vendors on pricing. How can you ensure they understand your quality expectations?
1 contribution
9 hours ago
Conveying clear quality expectations is essential in vendor pricing negotiations. Use these tips to maintain high standards during discussions.
Vendor Negotiation
Soft Skills
You've optimized your ERP software for performance. How do you maintain data integrity in the process?
1 contribution
9 hours ago
<end output>
Enterprise Resource Planning (ERP)
Business Administration
Your office is facing double-booked meeting rooms. How can you ensure smooth scheduling and avoid conflicts?
1 contribution
9 hours ago
Double-booked meeting rooms can cause chaos. Use these strategies to ensure smooth scheduling and avoid conflicts in your office.
Office Administration
Administrative Assistance
Your ERP system needs both flexibility and growth potential. Can you achieve both seamlessly?
2 contributions
8 hours ago
<end output>
Enterprise Resource Planning (ERP)
Business Administration
Dealing with diverse international vendors, how do you navigate conflicting negotiation styles effectively?
1 contribution
9 hours ago
</begin output>
Vendor Negotiation
Soft Skills
You're aiming to enhance drone capabilities. How can you do it within your budget constraints?
1 contribution
9 hours ago
Elevate your drone's capabilities without overspending. Implement modular upgrades and open-source software to enhance performance cost-effectively.
Drones
Engineering
Juggling feedback from various sources in your CAD project. How can you stay on track with time constraints?
1 contribution
9 hours ago
Stay on track with your CAD project despite multiple sources of feedback. Prioritize, communicate effectively, and use version control for best results.
Computer-Aided Design (CAD)
Engineering
Your colleague breaches a client's privacy in a team meeting. How do you handle the situation effectively?
1 contribution
9 hours ago
Deal with a colleague's breach of client privacy effectively. Speak up, remind them of policies, and report if needed to maintain professionalism.
Career Counseling
HR Management
Your team values speed in CAD design. How can you convince them to prioritize sustainability?
1 contribution
9 hours ago
<end output>
Computer-Aided Design (CAD)
Engineering
You're facing client demands for quick CAD revisions. How can you effectively manage their time expectations?
1 contribution
9 hours ago
Meet client demands for fast CAD revisions without compromising on quality with these strategies for effective time management and clear communication.
Computer-Aided Design (CAD)
Engineering
You're leveraging analytics in your ERP system. How do you tackle data security concerns?
1 contribution
9 hours ago
<end output>
Enterprise Resource Planning (ERP)
Business Administration
How do you test and validate your state space design using simulation or experiments?
5 contributions
9 hours ago
Learn how to use simulation or experiments to test and validate your state space design for control systems. Follow these practical steps and tools to improve your…
Control Systems Design
You're driving your team to hit ambitious targets. How do you avoid overwhelming them?
1 contribution
9 hours ago
<end output>
Team Building
Soft Skills
You're tasked with creating loyalty rewards for passengers. How do you balance cost and quality?
3 contributions
9 hours ago
<end output>
Airline Management
Transportation
You notice a client is emotionally distressed in a meeting. How can you effectively address their emotions?
1 contribution
9 hours ago
When clients are distressed in meetings, it's vital to acknowledge their feelings and offer support. Learn how to handle these situations with care.
Emotional Intelligence
Soft Skills
How do you clarify team roles?
1 contribution
9 hours ago
Learn how to clarify team roles in your team project or activity. Follow these tips to define, communicate, and appreciate team roles in a clear, fair, and flexible…
Team Building
Soft Skills
You're struggling to balance grading and teaching. How can technology help you focus more on teaching?
9 contributions
4 minutes ago
Discover how technology can assist teachers with grading, allowing them to focus more on teaching. Automated tools and interactive platforms can make a difference.
Teaching
Education
You're introducing new technology to physicians. How can you ensure effective communication?
3 contributions
10 hours ago
<end output>
Working with Physicians
Healthcare
How do you combine Toulmin's model with other communication strategies and tools for better results?
1 contribution
10 hours ago
Learn how to combine Toulmin's model of argumentation with other communication strategies and tools for persuasive and logical messages.
Internal and External Communications
Production demand is fluctuating unpredictably. How do you manage your inventory effectively?
1 contribution
10 hours ago
<end output>
Plant Operations
Manufacturing
You're facing scope changes from a key decision-maker. How do you navigate contract terms that don't align?
1 contribution
10 hours ago
When key decision-makers change the project scope, review your contract, communicate to negotiate, and document everything for clarity.
Contract Negotiation
Soft Skills
You're passionate about copywriting. How do you blend creativity with SEO mastery for maximum impact?
1 contribution
10 hours ago
Merge creativity and SEO in your copywriting with these actionable strategies. Enhance your content’s impact and search visibility effectively.
Copywriting
Content Management
Balancing personalized client interactions and tight deadlines: How can you manage both effectively?
36 contributions
21 minutes ago
Balance personalized client interactions with tight deadlines using these strategies: regular check-ins, CRM tools, and clear boundaries. Maintain quality service…
Client Relations
Sales
You’re launching a new product and need a solid messaging framework. How do you leverage customer feedback?
21 contributions
4 minutes ago
<end output>
Strategic Communications
Business Administration
You're navigating changes in consumer behavior. How do you ensure your team stays informed?
19 contributions
23 minutes ago
Stay ahead of changing consumer habits with these team strategies. Equip your team for adaptability with market research and open communication.
Small Business
Business Administration
You're knee-deep in research for your project. How do you know which online sources to trust?
13 contributions
27 minutes ago
Ensure your research is accurate by evaluating online sources. Check author credentials, look for peer-reviewed articles, and verify publication dates.
Research Skills
Soft Skills
You're juggling various sales platforms with different pricing. How can you keep everything consistent?
3 contributions
22 minutes ago
<end output>
Retail Sales
Sales
Quality control mistakes are causing tension in your team. How do you resolve the conflicts?
9 contributions
26 minutes ago
</begin output>
Plant Operations
Manufacturing
Your lead generation efforts seem to be falling short. How do you measure their true effectiveness?
10 contributions
25 minutes ago
Assess your lead generation efforts by tracking conversion rates, analyzing lead sources, and monitoring engagement metrics to ensure success.
Sales Operations
Sales
Quality control issues are affecting plant operations. How can you meet client expectations effectively?
6 contributions
43 minutes ago
Address quality control issues in plant operations while keeping clients content. Implement a QMS, enhance transparency, and train staff effectively.
Plant Operations
Manufacturing
More to explore
Marketing
Public Administration
Healthcare
Engineering
IT Services
Sustainability
Business Administration
Telecommunications
HR Management
Content Management
Financial Management
Organic Search
User Interface Design
Web Content Writing
Leisure Industry
Food and Beverage Management
Sales
Education
Administrative Assistance
Real Estate
Construction
Art
Manufacturing
Social Media Optimization (SMO)
Soft Skills
See all topics
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Yuzheng Sun on LinkedIn: Data quality is a challenge for most companies. I was hired at Tencent to…,"Yuzheng Sun on LinkedIn: Data quality is a challenge for most companies. I was hired at Tencent to…
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Yuzheng Sun’s Post
Yuzheng Sun
Experimentation Evangelist @ Statsig | Prev. Meta, Amazon, Tencent
4mo
Report this post
Data quality is a challenge for most companies. I was hired at Tencent to do data science, but I spent the better part of my first year on data engineering and ensuring data quality.

When I joined
Statsig
, I was surprised to find that data quality is not a bottleneck for our customers adopting A/B testing. I believe this is because many data quality best practices are built-in, such as:
1. A universal and well-templated event-logging API
2. Tracking exposure events (which test am I in) using the same format as tracking user behavior events (which button did I click)
3. Pipelines behind this API that optimize for availability, reliability, and minimizing event loss
4. A centralized metrics catalog to aggregate events into metrics through a user interface instead of queries
5. Automatic data checks such as Sample Ratio Mismatch (SRM), failed joins, etc.

However, my experience is limited. What does your company do to ensure data quality is both effective and scalable?
48
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Kevin (Tianchi) Zhang
Data products | Data Scientist | Machine learning
4mo
Report this comment
Thanks for sharing, very insightful! One quick question: which of the 5 is more critical from your perspective?
Like
Reply
1 Reaction
2 Reactions
To view or add a comment,
sign in
More Relevant Posts
Yuzheng Sun
Experimentation Evangelist @ Statsig | Prev. Meta, Amazon, Tencent
4mo
Report this post
What does poor data quality typically look like?
1. Conflicting sources of truth
2. Lack of centralized data definitions or metadata
3. Data scattered across multiple locations even multiple clouds
4. Logic and pipelines that are difficult to trace and understand, making them nearly impossible to modify without unexpected breakage
5. A desire to start your own pipeline instead of working with existing lineage
6. But, lack of confidence in the quality of your pipeline due to issues like duplication or event loss
7. Unoptimized costs

What else shall I add to this list?
Yuzheng Sun
Experimentation Evangelist @ Statsig | Prev. Meta, Amazon, Tencent
4mo
Data quality is a challenge for most companies. I was hired at Tencent to do data science, but I spent the better part of my first year on data engineering and ensuring data quality.

When I joined
Statsig
, I was surprised to find that data quality is not a bottleneck for our customers adopting A/B testing. I believe this is because many data quality best practices are built-in, such as:
1. A universal and well-templated event-logging API
2. Tracking exposure events (which test am I in) using the same format as tracking user behavior events (which button did I click)
3. Pipelines behind this API that optimize for availability, reliability, and minimizing event loss
4. A centralized metrics catalog to aggregate events into metrics through a user interface instead of queries
5. Automatic data checks such as Sample Ratio Mismatch (SRM), failed joins, etc.

However, my experience is limited. What does your company do to ensure data quality is both effective and scalable?
20
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Matthew Kelliher-Gibson, MBA
Product Manager | AI/ML & Data Strategy | Cynical Data Guy @ The Data Stack Show
11mo
Edited
Report this post
In Data Science there are two types of problems: solved problems and unsolved problems. Most ML problems data teams encounter day to day in businesses are solved problems. Churn models, predictive LTV, forecast demand, etc.

It is not about creating novel models or algorithms. It is about feeding clean data into models that are known to reliably produce good results.

Those problems don't need PhD Data Scientists or require complex MLOps infrastructure but also shouldn't be relegated to SaaS black boxes. 

There is an opening for Analytic Engineers to fill this gap. They have data skills and business knowledge necessary for solved problems. Empowering Analytic Engineers will speed up activating data and allow Data Science/AI teams to focus on the hard R&D work of unsolved problems.
20
5 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Mayank Gawali
Data Engineer @Deloitte || SQL || Python || Snowflake || DBT || AWS || Spark
2mo
Report this post
As data engineers, our job isn’t just about moving data from point A to point B. Ensuring data quality throughout the pipeline is critical to delivering accurate insights and driving better decision making. Here’s why it matters:

1. Trust in Data: If stakeholders can’t trust the data, any insights or decisions based on it could be flawed.

2. Efficiency: Clean data reduces rework and prevents time spent on troubleshooting downstream issues.

3. Compliance: High-quality data ensures you stay compliant with regulations, avoiding costly penalties.

4. Better Analytics: Data scientists and analysts can deliver more accurate models and reports when working with reliable data.

In my experience, investing time in building robust data quality checks whether it's validation rules, error handling, or monitoring pipelines pays off significantly in the long run.
#dataengineering
#dataquality
#bigdata
#ETL
#datapipeline
#techcommunity
#cloudcomputing
#AI
#datascience
9
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Fahim ul Haq
Co-Founder & CEO at Educative | Software Engineer
2mo
Report this post
If my 19+ years in tech have taught me one thing, it's that you never know what's coming next — but you better be ready for it.

A great example of that? The role of data engineer.

Before the 2010s, data engineering roles didn't really exist. Data was managed manually or with basic ETL tools by IT teams.

But when the internet exploded, the sheer volume, speed, and complexity of Big Data became overwhelming, and traditional methods couldn't keep up.

That's when FAANG giants like Facebook and Airbnb coined the term ""data engineer"" to describe specialists who could rethink data storage, processing, and security at an unprecedented scale. 

Data was no longer just an IT concern — it flowed through every part of the business, from sales to marketing.

Today, data engineering is one of tech's most important jobs. It's essential to managing the lifeblood of modern companies – their data. And it all started because new technology demanded new solutions.

This is a great reminder that new tech creates new roles. Just like we couldn't predict data engineering's rise, we can't predict what's next with blockchain, AI, or whatever else is on the horizon.

My advice? Keep learning. Stay adaptable. The next evolution is always around the corner.
#BigData
#DataEngineering
#TechCareers
#AI
#CloudComputing
128
10 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Yash Kothari
Data Engineer Consultant @Deloitte | Ex-LTIMindtree | Azure | Databricks | Problem Solver | Medium - @ykothari99
2mo
Edited
Report this post
****  Internal Working - Broadcast Hash Join ****

You can checkout a detailed explanation here -
https://lnkd.in/dp5RRK_E
A broadcast hash join is one of the most efficient join strategies in Spark, designed to optimize joins when one of the datasets is much smaller than the other. Instead of shuffling both datasets across the cluster (which can be expensive), the smaller dataset is broadcasted to all executors, making it available in-memory for fast lookups during the join operation.

Here’s a detailed explanation of how a broadcast hash join works, its mechanics, and why it is effective.

1. Preconditions for a Broadcast Hash Join
One dataset is significantly smaller: Typically, Spark will broadcast the smaller dataset if it fits within a size threshold (defined by
spark.sql.autoBroadcastJoinThreshold
).
Threshold: By default, Spark will broadcast a table if it is smaller than 10MB (this value can be adjusted).

2. How Broadcast Hash Join Works

2.1 Broadcasting the Smaller Dataset
The smaller dataset (let's call it B) is broadcasted from the driver node to all worker nodes (executors) across the cluster.
This dataset is then stored in memory on each executor for the duration of the join. Each executor now holds a local copy of the smaller dataset.

2.2 Hash Table Construction
After the dataset B is broadcasted to all executors, Spark builds a hash table using the join key from B.
The hash table is organized as key-value pairs:
Key: The join column (or columns) from B.
Value: The remaining fields (or the entire row) from B.
This hash table is stored in-memory on each executor, so that it can be used for fast lookups when processing the other dataset (let’s call it A).

2.3 Processing the Larger Dataset
Spark then iterates through the larger dataset A (which is already partitioned across the cluster).
For each row in A, Spark calculates the hash of the join key and uses it to perform a lookup in the broadcasted hash table (from B).

2.4 Join Operation
For each row in A:
Spark calculates the hash value of the join key and checks if it exists in the hash table built from B.
If a match is found, the corresponding row from B is retrieved and joined with the row from A.
In case of hash collisions (where multiple rows from B may share the same hash), Spark compares each potential match to ensure the keys are indeed the same (as discussed earlier).

2.5 Result Partitioning
The results of the join are partitioned based on the larger dataset A's partitioning.
The join is done locally on each executor without requiring shuffling, which makes it highly efficient.

If you found this post helpful, feel free to share it so others can benefit too! 😊
#spark
#broadcast
#join
#dataengineer
#data
#bigdata
#pyspark
…more
11
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Nikhil Padman
Power Platform Developer | Machine Learning, Data Science, Gen AI and Data Analysis Enthusiast | DataCamp Certified Data Analyst, Data Scientist, and Data Engineer | DataCamp Certified SQL Associate | Python | Power BI
1mo
Report this post
Ever feel like machine learning roles are just a bunch of characters from a sitcom? Let’s break it down:

🔍 Data Scientist: The dreamer with a whiteboard, always cooking up fancy models and shouting, ""What if we try neural nets?""

🔧 ML Engineer: The practical one, turning those dreams into reality and whispering, ""But does it scale?""

🕵️ Data Analyst: The detective, sifting through data for insights and replying, ""Actually, here’s what the data really says.""

🦸 MLOps Engineer: The superhero, swooping in to keep models alive, updating, and behaving like good citizens in production.

Every role has its quirks, but together, they keep the ML universe spinning. What role do you relate to the most? Or are you juggling all these hats at once
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Dominique Mary
Founder & Managing Director at Darwin X
6mo
Report this post
👩💼👨💼 Data & AI Human Capital is pivotal for banks looking to stay competitive in the evolving financial landscape. Disparities in the workforce footprints of top banks highlight the importance of these talents, ranging from 0.4% to 3.2% of the total workforce, with an average of 1.4%.

Developing a strong Data & AI human capital base involves expanding four key talent pools: Data Analysts, Data Engineers, Data Scientists, and AI Specialists. Each role is critical in transforming banking operations:

📊 Data Analysts drive decision-making through performance dashboards and data analysis.
🛠 Data Engineers build and manage data infrastructures for internal and external data.
🔬 Data Scientists extract insights using advanced analytics.
🤖 AI Specialists focus on Artificial Intelligence solutions.

These roles are vital for innovation and growth, bridging the gap between tech-savvy entities and laggards. Learn more in our latest Data & AI human capital report - BANKING edition by
Darwin X
, a research & consulting firm focused on corporate transformation.
#banking
#talent
#Data
#AI
24
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
William W Collins
Innovative Transformational Leader | Multi-Industry Experience | AI & SaaS Expert | Generative AI | DevOps, AIOps, SRE & Cloud Technologies | Experienced Writer | Essayist | Digital Content Creator | Author
3mo
Report this post
Data Science Product Manager @ Mastercard

by  via all AI news ([Global] Security Breach)

URL:
https://ift.tt/IYpDnWJ
Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, financial institutions, governments and businesses realize their greatest potential. Our decency quotient, or DQ, drives our culture and everything we do inside and outside of our company. We cultivate a culture of inclusion for all employees that respects their individual strengths, views, and experiences. We believe that our differences enable us to be a better team – one that makes better decisions, drives innovation and delivers better business results.

Title and Summary

Data Science Product Manager

Services within Mastercard is responsible for acquiring, engaging, and retaining customers by managing fraud and risk, enhancing cybersecurity, and improving the digital payments experience. We provide value-added services and leverage expertise, data-driven insights, and execution.

Overview: 

As the Data Science Privacy Product Manager, you will drive the product development of machine learning (ML) solutions that power digital risk assessments around the world. The role is focused on inventing the next generation of ML-derived risk scores with Responsible AI at the forefront. Your work will ensure that Mastercard’s Privacy by Design principles are applied to the signals we deliver to our customers. As the product manager, you will lead efforts to research, design, and take to market innovative new solutions powered by data science. If you are intellectually curious, analytical, highly motivated, and want to make cybersecurity products with real-world impact – we want to hear from you! 

Role: 

In this position, you will: 

Work deeply with data science and engineering to research, develop, and deploy machine-learning solutions. 

Define the product strategy and roadmap for the proprietary Machine Learning Platform and related tooling. 

Meet regularly with end customers to understand opportunities for new product capabilities and improvements. 

Partner with sales teams, product marketing, and other product managers to identify business opportunities and deliver solutions to the market. 

Drive improvements to the training data asset by building scalable tools and processes. Partner with legal, sales, and end customers drive data collection and quality improvements. 

Monitor performance, collect customer feedback, and continuously improve the product offering. 

All About You: 

The ideal candidate for this position should have: 

Product management experience; ideally in SaaS products. 

Critical thinking and analytical skills to solve complex problems. 

Technical experience working with APIs, databases, and data-centric applications. Abili...
Data Science Product Manager @ Mastercard

by  via all AI news ([Global] Security Breach)

URL: https://ift.tt/IYpDnWJ

Our Purpose

We work to connect and power an inclusive, digital economy that benefits everyone, everywhere by making transactions safe, simple, smart and accessible. Using secure data and networks, partnerships and passion, our innovations and solutions help individuals, fin...
aijobs.net
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
H. Ezgi Karakaş Schüller
Data Engineer at Der Standard | 
WOMENinICT Ambassador |
Cancer Researcher, PhD
1mo
Report this post
🌟 Very nice Sunday read about
#dataengineering
and its fundemental shift with
#GenAI
🌟 

In my current role, we are already in this futuristic level of merging data engineering with data analysis, in this article I learned that it can be called “analytics engineer”. It is quite powerful to provide end-to-end solutions and to increase ownerships and to be more involved in the business side. 

Please also check “What should we be doing today as data engineers to be ready for the future” It gives a nice check-list and it can be surely extended with
#datagovernance
Nice Sunday ✨
Charlotte Ledoux
Charlotte Ledoux is an Influencer
Data & AI Governance Expert 🔎 | Data Governance coaching | Women in AI | Speaker | LinkedIn™️ Top Voice in AI 🇫🇷
1mo
🔎 No more burnouts for Data Engineers thanks to GenAI.
They will get more done with less time. 
Companies can get the same amount of work done with fewer engineers. 

“AI-enabled” engineers will crush the ones who refuse to learn it.

Data Engineers will embrace either :

1️⃣ Software engineering : being able to own the system “end-to-end” from online system to logging to pipeline to data sets to metrics will be a huge thing in the future.

2️⃣ Product management : think of this as the merging of the data engineer and the data analyst by picking up work in experimentation, visualization and predictive modeling.

Thanks
Zach Morris Wilson
for this awesome article !
How GenAI will impact data engineering
blog.dataengineer.io
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Adnan Abid
Co-Founder: Techtics.ai ; Hyphen.co| Professor | ex EPFL | ex PoliMi | ex GMU | Senior Member IEEE
2mo
Report this post
Pertinence of Data Engineering and Big Data needs to be appreciated
Fahim ul Haq
Co-Founder & CEO at Educative | Software Engineer
2mo
If my 19+ years in tech have taught me one thing, it's that you never know what's coming next — but you better be ready for it.

A great example of that? The role of data engineer.

Before the 2010s, data engineering roles didn't really exist. Data was managed manually or with basic ETL tools by IT teams.

But when the internet exploded, the sheer volume, speed, and complexity of Big Data became overwhelming, and traditional methods couldn't keep up.

That's when FAANG giants like Facebook and Airbnb coined the term ""data engineer"" to describe specialists who could rethink data storage, processing, and security at an unprecedented scale. 

Data was no longer just an IT concern — it flowed through every part of the business, from sales to marketing.

Today, data engineering is one of tech's most important jobs. It's essential to managing the lifeblood of modern companies – their data. And it all started because new technology demanded new solutions.

This is a great reminder that new tech creates new roles. Just like we couldn't predict data engineering's rise, we can't predict what's next with blockchain, AI, or whatever else is on the horizon.

My advice? Keep learning. Stay adaptable. The next evolution is always around the corner.
#BigData
#DataEngineering
#TechCareers
#AI
#CloudComputing
9
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
17,427 followers
492 Posts
1 Article
View Profile
Connect
More from this author
Your products fail because you underestimate users’ switching cost
Yuzheng Sun
3y
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Sign in to view more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Big Data Hadoop Testing,"Big Data Hadoop Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Hadoop Testing
Report this article
Pooja Chougule
Pooja Chougule
Senior Internal Auditor
Published Jun 14, 2016
+ Follow
Big Data Testing Hadoop online training course is designed to test and rectify errors from Hadoop Project's Performance.
It comprises a comprehensive study of Hadoop Software, Hadoop Architecture, HDFS, MapReduce Jobs, Hive, PIG, Sqoop, Oozie, Flume, Yarn, Hadoop Testing.
Learn
1.Understand What is Hadoop, its main components and Hadoop Ecosystem
2.Deep dive into Hadoop Distributed File System (HDFS)
3.Understand MapReduce abstraction and its working
4.Learn to develop a MapReduce application and writing Unit Tests
5.Perform basic data analysis using Pig and Relational data analysis with Hive
6.Understand concepts of advanced platforms like Sqoop, Oozie, Flume and Yarn
7.Know the importance of Hadoop Testing and Hadoop Testing Workflow Process.
For more details -
www.apps2fusion.com
you can write us @
training@apps2fusion.com
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
2
To view or add a comment,
sign in
More articles by this author
No more previous content
ORACLE IDENTITY MANAGER - IDENTITY MANAGEMENT SYSTEM - AN OVERVIEW
May 5, 2016
BIG DATA HADOOP TESTING
Apr 29, 2016
All R12 Training together
Apr 22, 2016
Fusion Financials Modules Package
Apr 12, 2016
INTRODUCTION TO JAVA
Mar 9, 2016
Oracle Fusion Financials Training
Mar 3, 2016
Apps2fusion - Oracle Data Integrator Training
Feb 18, 2016
ORACLE DATABASE 12C :CORE ADMINISTRATION
Dec 30, 2015
ORACLE FUSION MIDDLEWARE 12C ADMINISTRATION
Dec 10, 2015
ORACLE FUSION HCM EMPLOYMENT MODELS
Nov 27, 2015
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
6 Tips to Prepare for Data Quality and Testing Challenges,"6 Tips to Prepare for Data Quality and Testing Challenges
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Engineering
How can you prepare for future data quality and testing challenges?
Powered by AI and the LinkedIn community
1
Understand your data
2
Automate your data quality and testing processes
3
Implement data quality and testing standards and best practices
Be the first to add your personal experience
4
Learn from your data quality and testing feedback and failures
Be the first to add your personal experience
5
Keep up with data quality and testing trends and innovations
Be the first to add your personal experience
6
Experiment with data quality and testing solutions and alternatives
Be the first to add your personal experience
7
Here’s what else to consider
Be the first to add your personal experience
Data quality and testing are essential aspects of data engineering, as they ensure the reliability, accuracy, and usability of data pipelines and products. However, data quality and testing challenges are constantly evolving, as data sources, formats, volumes, and requirements change over time. How can you prepare for future data quality and testing challenges as a data engineer? Here are some tips and best practices to help you stay ahead of the curve.
Top experts in this article
Selected by the community from 2 contributions.
Learn more
Raphaël MANSUY
Data Engineering | DataScience | AI & Innovation  | Author | Follow me for deep dives on AI & data-engineering
View contribution
1
See what others are saying
1
Understand your data
The first step to prepare for future data quality and testing challenges is to understand your data sources, formats, schemas, and characteristics. You need to know where your data comes from, how it is structured, what it contains, and how it changes over time. This will help you define your data quality dimensions, such as completeness, validity, accuracy, consistency, and timeliness. You also need to understand your data consumers, such as analysts, scientists, or applications, and their data requirements, such as frequency, granularity, and format. This will help you design your data pipelines and products to meet their expectations and needs.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Raphaël MANSUY
Data Engineering | DataScience | AI & Innovation  | Author | Follow me for deep dives on AI & data-engineering
Copy link to contribution
Report contribution
Invest in data catalog and observability: Data catalog and observability tools can help you better understand your data sources, formats, schemas, and characteristics

By investing in these tools, you can gain insights into your data's quality dimensions, such as completeness, validity, accuracy, consistency, and timeliness.

Continuously monitor and improve data quality: Regularly assess your data quality dimensions, such as accuracy, completeness, consistency, timeliness, validity, and uniqueness

Implement continuous monitoring and improvement processes to ensure that your data remains of high quality and meets the needs of your data consumers.
…see more
Like
1
2
Automate your data quality and testing processes
The second step to prepare for future data quality and testing challenges is to automate your data quality and testing processes as much as possible. Automation can help you save time, reduce errors, and increase efficiency. You can use tools and frameworks, such as Apache Airflow, dbt, Great Expectations, or Data Quality Monitor, to automate your data pipeline orchestration, transformation, validation, and monitoring. You can also use tools and frameworks, such as Pytest, Unittest, or DataTest, to automate your data testing at different stages of your data pipeline, such as unit testing, integration testing, or end-to-end testing. Automation can help you detect and resolve data quality and testing issues faster and more effectively.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
.Deepak K.
Database Test Engineer at Universal Electronics
Copy link to contribution
Report contribution
Generally Data Quality, data is of high quality when it satisfies the requirements of its intended use for clients, decision-makers, downstream applications and processes. You need to understand your data consumers, such as analysts, scientists, or applications, and their data requirements, such as frequency, granularity, and format. This will help you design your data pipelines and products to meet their expectations and needs.
…see more
Like
3
Implement data quality and testing standards and best practices
The third step to prepare for future data quality and testing challenges is to implement data quality and testing standards and best practices across your data engineering team and organization. Standards and best practices can help you ensure consistency, quality, and reliability of your data pipelines and products. You can use guidelines and templates, such as DataOps Manifesto, Data Quality Framework, or Test-Driven Development, to define and follow your data quality and testing principles, processes, and metrics. You can also use tools and platforms, such as GitHub, GitLab, or Data Catalog, to document and share your data quality and testing code, results, and insights. Standards and best practices can help you improve your data quality and testing culture and collaboration.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Learn from your data quality and testing feedback and failures
The fourth step to prepare for future data quality and testing challenges is to learn from your data quality and testing feedback and failures. Feedback and failures can help you identify your data quality and testing gaps, weaknesses, and opportunities. You can use tools and methods, such as dashboards, alerts, logs, or root cause analysis, to collect and analyze your data quality and testing feedback and failures. You can also use tools and methods, such as surveys, interviews, or user testing, to collect and analyze your data consumer feedback and satisfaction. Feedback and failures can help you improve your data quality and testing performance and value.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Keep up with data quality and testing trends and innovations
The fifth step to prepare for future data quality and testing challenges is to keep up with data quality and testing trends and innovations. Trends and innovations can help you anticipate and adapt to the changing data landscape and demands. You can use sources and resources, such as blogs, podcasts, newsletters, or courses, to learn about the latest data quality and testing tools, techniques, and best practices. You can also use sources and resources, such as conferences, webinars, or communities, to network and exchange with other data quality and testing experts and enthusiasts. Trends and innovations can help you enhance your data quality and testing skills and knowledge.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Experiment with data quality and testing solutions and alternatives
The sixth step to prepare for future data quality and testing challenges is to experiment with data quality and testing solutions and alternatives. Experimentation can help you discover and evaluate new data quality and testing possibilities and outcomes. You can use tools and methods, such as prototyping, A/B testing, or hypothesis testing, to experiment with different data quality and testing solutions and alternatives. You can also use tools and methods, such as metrics, reports, or feedback, to measure and compare the results and impacts of your experiments. Experimentation can help you innovate and optimize your data quality and testing solutions and alternatives.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Engineering
Data Engineering
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Engineering
No more previous content
Clients demand both real-time and batch processing. How do you prioritize their needs?
2 contributions
You're faced with data accuracy and tight ETL deadlines. How do you strike a balance between the two?
3 contributions
You're facing a surge in data volume. How can you fine-tune ETL pipelines to manage it effectively?
4 contributions
You're facing a complex data migration. How do you prevent data loss?
20 contributions
You're facing conflicting opinions on data infrastructure upgrades. How do you prioritize tasks effectively?
3 contributions
Your organization is hesitant about cloud migration. How can you convince them of the benefits?
4 contributions
Struggling to balance data quality and processing speed in ETL workflows?
13 contributions
You've faced a data anomaly in your analysis. How do you prevent it from happening again?
4 contributions
You're facing data pipeline performance issues. How can you optimize them without causing disruption?
6 contributions
Clients are frustrated with delays in real-time data delivery. How do you handle their complaints?
7 contributions
You need to onboard new team members on data governance. How do you make it effective and engaging?
11 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Engineering
What challenges do you face when testing data and how can you overcome them?
Data Science
How can you ensure data provenance in your workflow?
Data Engineering
How can you ensure that your team meets the data needs of the business?
Process Automation
What are the most effective ways to measure data manipulation impact on automation outcomes?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
2
2 Contributions"
"week 19 - TDD in a Big Data Domain, The role of unit testing in training and Effects of TDD on Internal Quality, External Quality and Productivity","week 19 - TDD in a Big Data Domain, The role of unit testing in training and Effects of TDD on Internal Quality, External Quality and Productivity
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
https://unsplash.com/photos/ti0UfqeGcnw
week 19 - TDD in a Big Data Domain, The role of unit testing in training and Effects of TDD on Internal Quality, External Quality and Productivity
Report this article
Marabesi Matheus 🚀
Marabesi Matheus 🚀
MSc, MBA, Software Craftsperson at Codurance
Published Aug 13, 2023
+ Follow
Exploring the Applicability of Test Driven Development in the Big Data Domain
Big data analytics and the according applications have gained huge importance in daily life. This results on the one hand from their versatility and on the other hand from their capability to greatly improve an organization’s performance when utilized appropriately. However, despite their prevalence and the corresponding attention through practitioners as well as the scientific world, the actual implementation still remains a challenging task. Therefore, without the adequate testing, the reliability of the systems and thus the obtained outputs is uncertain. This might reduce their utilization, or even worse, lead to a diminished decision-making quality. The publication at hand explores the adoption of test driven development as a potential approach for addressing this issue. Subsequently, using the design science research methodology, a microservice-based test driven development concept for big data (MBTDD-BD) is proposed. In the end, possible avenues for future research endeavours are indicated.
The role of unit testing in training
The main priority of any modern software company is to improve the quality of the software. This can be achieved by preventing software defects, i.e. Software Testing (ST) applied by well-trained programmers. This puts before each university the task of professional training of students to master theoretical and practical aspects related to various techniques, strategies and methods in the field of ST. The ultimate goal is the creation, implementation, analysis and subsequent maintenance of the software.
Factors limiting industrial adoption of test driven development: A systematic review
Test driven development (TDD) is one of the basic practices of agile software development and both academia and practitioners claim that TDD, to a certain extent, improves the quality of the code produced by developers. However, recent results suggest that this practice is not followed to the extent preferred by industry. In order to pinpoint specific obstacles limiting its industrial adoption we have conducted a systematic literature review on empirical studies explicitly focusing on TDD as well as indirectly addressing TDD. Our review has identified seven limiting factors viz., increased development time, insufficient TDD experience/knowledge, lack of upfront design, domain and tool specific issues, lack of developer skill in writing test cases, insufficient adherence to TDD protocol, and legacy code. The results of this study is of special importance to the testing community, since it outlines the direction for further detailed scientific investigations as well as highlights the requirement of guidelines to overcome these limiting factors for successful industrial adoption of TDD.
The Effects of Test Driven Development on Internal Quality, External Quality and Productivity: A systematic review
Context: Test Driven Development (TDD) is an agile practice that has gained popularity when it was defined as a fundamental part in eXtreme Programming (XP). Objective: This study analyzed the conclusions of previously published articles on the effects of TDD on internal and external software quality and productivity, comparing TDD with Test Last Development (TLD). Method: In this study, a systematic literature review has been conducted considering articles published between 1999 and 2014. Results: In about 57% of the analyzed studies, the results were validated through experiments and in 32% of them, validation was performed through a case study. The results of this analysis show that 76% of the studies have identified a significant increase in internal software quality while 88% of the studies identified a meaningful increase in external software quality. There was an increase in productivity in the academic environment, while in the industrial scenario there was a decrease in productivity. Overall, about 44% of the studies indicated lower productivity when using TDD compared to TLD. Conclusion: According to our findings, TDD yields more benefits than TLD for internal and external software quality, but it results in lower developer productivity than TLD.
Papers of the week
Papers of the week
799 followers
+ Subscribe
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
7
To view or add a comment,
sign in
More articles by this author
No more previous content
week 51 - why developers implement OS-specific tests, does Treatment Adherence Impact in TDD and A framework for compliance rules for TDD
Nov 23, 2024
week 50 - Agile vs Waterfall, Matching Production and Test Files and Test smells in LLM-Generated Unit Tests
Oct 20, 2024
week49 - Monitoring Continuous Integration Practices, Test Smells + Gamification and Unit Test Generation
Sep 30, 2024
week48 - Technical debt can damage moral responsibility, LLM for Understandability of Generated Unit Tests and Mimicking Production Behavior with Mock
Sep 14, 2024
week47 - Which Combination of Test Can Predict Success? and the relationship between unit test coverage and maintainability of production code
Sep 1, 2024
week46 - Evaluating Large Language Models in Detecting Test Smells and Code Reviews Patterns and Anti-patterns
Aug 12, 2024
week45 - How Do Developers Structure Unit Test Cases and The role of slicing in test-driven development
Jul 22, 2024
week44 - Highlights in Evidence-Based Software Engineering  and Ethnographically Study in the Context of Test Driven Development
Jul 7, 2024
week43 - Most Common Mistakes in TDD Practice and Reflections on the REST architectural style
Jun 16, 2024
week42 - Chatbot for Exploratory Testing,  code smells detection using machine learning and Does It Really Matter to Test-First or to Test-Last?
Jun 11, 2024
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Journey From Big Data to Smart Data: How Big Data Testing Can Help You?,"Journey From Big Data to Smart Data: How Big Data Testing Can Help You?
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Journey From Big Data to Smart Data: How Big Data Testing Can Help You?
Report this article
Debjani Goswami
Debjani Goswami
Marketing Manager | Marketing Strategy, Sales Enablement
Published Jul 25, 2022
+ Follow
The idea of big data can be traced decades back to the 20th century. However, it was only after the 21st century’s information explosion on the internet that businesses started discussing the real-life application of big data.
Companies today are privy to large volumes of data, called big data. So, businesses need to keep this big data at their disposal.
Nonetheless, the real deal with big data is its analysis. This is also where the journey from
big data to smart data
begins.
The analysis of big data involves uncovering hidden patterns, establishing a correlation between different data sets, and gathering relevant insights. This form of analysis is also quite distinct and advanced from the usual data processing that most companies execute on a regular basis.
Due to such high levels of sophistication, businesses encounter several challenges while transforming big data to smart data. If these challenges are not tackled early in the big data application development life cycle, they possess even a bigger threat in the data assurance process as well.
Most of these challenges are common across industry. Some of the crucial ones are mentioned below:
Understanding the data
Big data is not a monolith; it is characterized by four V's – volume, variety, velocity and veracity. Enterprises can further sub-characterize big data depending on their business requirement. Based on these characterizations, businesses can actually move ahead with the data analysis process.
In the digital world, everything comes in the form of data. For businesses, one of the significant challenges here is to deal with those scenarios when emotions and sentiments are presented as data. So, it's up to the business to decide when to perform a qualitative analysis and when to evaluate the data quantitatively.
Real-time scalability & performance
Data can be in any of the three formats – structured, semi-structured, and unstructured. Businesses can efficiently process structured data. But, when it comes to semi-structured and unstructured data, enterprises face challenges related to scalability and performance.
To remain ahead of the curve in this competitive domain of big data, businesses need to perform functions on a real-time basis. So, working on such large volumes of differentiated data sets and getting real-time results is the most significant part of the challenge.
Data integrity & security
Dealing with such a massive amount of complex data sets also opens Pandora's box of cybersecurity implications associated with it. Most of this data also includes confidential and financial information of people. So, processing these data sets on multiple nodes involves a high degree of
security risk for businesses
.
Moreover, some security threats are so stealth that even the popular database software systems cannot detect them. In addition, most software systems are open source, making them prone to cybersecurity threats.
How big data testing helps businesses overcome these challenges?
Traditional software testing frameworks are not fully equipped to handle the challenges pertaining to big data. However, big data testing uses unique evaluation strategies to suit the various characterizations and complexities of big data.
It starts with proper data validation.
The validation stage is classified into three key major components:
Data validation: It involves verifying the apparently accurate and uncorrupted data from various sources such as scanners, logs, sensors, etc.
Process validation: Here, the data is checked for its accuracy. QA testers validate the business logic for various nodes at every node point while also verifying the key-value pair generation.
Outcome validation: Here, software testers verify the data stored in the EDW (Enterprise Data Warehouse) for any corruption or distortion.
A large chunk of the testing efforts goes into data validation due to such a thorough validation process. This elaborative data validation testing process also helps QA testers overcome the challenge of discerning complex data sets.
Next comes parallelism and performance testing
While parallelism takes care of the scalability issues, performance testing can validate the actual performance of big data applications.
It is recommended that QA testers conduct parallelism right at the CPU level through data partitioning. This would ensure that different processes are not independent of each other are executed simultaneously.  Moreover, projects must also be segregated into independent and inter-dependent projects for the seamless functioning of simultaneous operations.
As parallelism helps QA testers in scalability issues, big data performance testing helps them tackle issues pertaining to memory utilization, CPU optimization, throughput, etc.
Big data performance testing is quite different from the usual
performance testing
. Right from setting up a customized test environment to creating tailored test scripts, there are numerous stages of complexities involved in performance testing big data applications.
Therefore, businesses need to make sure if they can undertake it themselves or whether they need to outsource it to experienced and professional quality engineering experts who are proficient in quality and technology assurance.
Security testing big data applications
Penetration testing or security testing helps businesses in gauging how well-protected is their data security infrastructure. However, security testing of big data applications can get quite complicated because of multilevel authentications and encryptions. Therefore, QA testers working on security testing need to be experienced and adept with the process.
To ensure a seamless penetration testing process, QA testers validate the configuration efficiency of role-based access control (RBAC) rules while also checking the big data software application’s architectural integrity. Moreover, checking the network's security scanning efficiency also comes under the pen testing process, thereby ensuring a robust security architecture.
Final thoughts
Businesses who want to complete this journey of big data to smart data successfully cannot just rely on a set of data experts or professional testers. Instead, they need cross-functional teams comprising
data scientists, quality engineers, ethical hackers
, etc., along with a bunch of advanced testing tools.
Having all these resources in one place can be a costly affair. There are a few organizations that can undertake this pursuit, but not all of them. So, in all those cases, businesses seek help and guidance from quality engineering and technology assurance firms.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
12
To view or add a comment,
sign in
More articles by this author
No more previous content
Is Your Telehealth Application Good Enough for Patients/Doctors to Trust You?
Aug 19, 2022
5g Rollout: How Quality Engineering Helps Telecom Businesses Adapt to this New Phenomenon?
Aug 2, 2022
Is your Digital Billing System Tested for Flawless Performance?
Jul 8, 2022
Why is Big Data Testing Important for the Healthcare Industry?
Jun 21, 2022
Why does Your Video Game Need Intelligent Quality Assurance Services?
Jun 13, 2022
Are you Quality-proofing your Video based Learning Platform?
Jun 3, 2022
The Right Performance Testing Approach for Virtual Assistants
May 27, 2022
All about Mobile Application Testing - Know the Fundamentals
May 13, 2022
The Need for Microservices Testing in Integrated IoT Systems
May 6, 2022
How to Solve the Test Data Bottleneck?
Apr 29, 2022
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Measure Data Quality and Testing Maturity,"How to Measure Data Quality and Testing Maturity
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Engineering
What are the best methods for measuring data quality and testing maturity in your organization?
Powered by AI and the LinkedIn community
1
Data Quality Dimensions
2
Data Quality Metrics
Be the first to add your personal experience
3
Data Quality Dashboards
4
Testing Maturity Levels
Be the first to add your personal experience
5
Testing Maturity Assessment
Be the first to add your personal experience
6
Testing Maturity Dashboards
Be the first to add your personal experience
7
Here’s what else to consider
Be the first to add your personal experience
Data quality and testing are essential aspects of data engineering, as they ensure the reliability, accuracy, and usability of the data products and pipelines that support business decisions and analytics. However, measuring data quality and testing maturity can be challenging, as there are no universal standards or metrics that apply to every organization or project. In this article, we will explore some of the best methods for assessing data quality and testing maturity in your organization, based on the goals, needs, and capabilities of your data engineering team and stakeholders.
Top experts in this article
Selected by the community from 3 contributions.
Learn more
Nicholas Leong
View contribution
5
Elena Alikhachkina, PhD
Digital-First Operating Tech & AI Executive | Fortune 100 Global businesses | CDO, CIDO, CDAi, CIO
| Non-Exec Board…
View contribution
3
Tim Nargassans
𝐒𝐭𝐫𝐚𝐭𝐞𝐠𝐲 & 𝐓𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐋𝐞𝐚𝐝𝐞𝐫 | Executive Consultant | FinTech & Financial Services SME…
View contribution
2
See what others are saying
1
Data Quality Dimensions
In order to measure data quality, it's important to define the relevant and important dimensions or criteria for your data sources, processes, and outputs. Common data quality dimensions include accuracy, completeness, consistency, timeliness, validity, and uniqueness. Depending on your project, you may also consider other dimensions such as availability, accessibility, security, traceability, or usability. Identifying the dimensions that matter most to your data quality objectives and expectations is essential for success.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Elena Alikhachkina, PhD
Digital-First Operating Tech & AI Executive | Fortune 100 Global businesses | CDO, CIDO, CDAi, CIO
| Non-Exec Board Director
Copy link to contribution
Report contribution
While data quality is essential, it's often perceived as a cost center within many organizations. As a data leader, your role is to align data quality initiatives with business priorities, thereby transforming it into a value driver. The impact of high-quality data should be measurable through business outcomes, not just data-centric metrics. Therefore, it's crucial to establish business-relevant KPIs alongside data metrics to fully realize the potential of your data initiatives.
…see more
Like
3
Tim Nargassans
𝐒𝐭𝐫𝐚𝐭𝐞𝐠𝐲 & 𝐓𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐋𝐞𝐚𝐝𝐞𝐫 | Executive Consultant | FinTech & Financial Services SME | Influencer | Product Visionary | Data & A.I. Enabler | Experience Advocate | Agile Evangelist
Copy link to contribution
Report contribution
Although conceptually the goal is to reach perfect data quality, it's the journey that matters and moving the needle with each use case. Many are dealing with availability aspects either because of use based on-premise scenarios that required right sizing hardware stacks, or because it hasn't been migrated to the cloud where bringing the complete raw feed is ideal to at least set the stage for future use cases. A ""good enough"" for use case is minimum and necessary in some regards to avoid business fatigue. Quality is incremental to improve decisions as far as possible. As close to real-time with meta data, orchestration and observability are the base building blocks for technically trusted data that enables greater business trust/use.
…see more
Like
2
2
Data Quality Metrics
Once you have identified your data quality dimensions, you need to establish metrics or indicators to measure and monitor them. Data quality metrics are quantitative or qualitative measures that capture the level or degree of data quality for a given dimension, source, process, or output. Examples of data quality metrics include error rate, completeness rate, consistency rate, timeliness rate, validity rate, and uniqueness rate. To calculate these metrics, you must collect and analyze data quality information from various sources and methods such as metadata, data profiling, data auditing, data cleansing, data validation, or data reconciliation. Additionally, you must define thresholds, targets, or benchmarks that indicate the acceptable or desired levels of data quality for each metric, dimension, source, process, or output.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Data Quality Dashboards
Creating data quality dashboards is a great way to communicate and visualize your data quality metrics. These dashboards provide a comprehensive and interactive overview of the data quality status and trends for your project. With them, you can track and compare the data quality performance and progress for different data sources, processes, or outputs over time or across dimensions. They can also help you identify and prioritize data quality issues that need to be addressed, evaluate the impact of your data quality initiatives, and engage and align your team and stakeholders on the data quality goals. To create effective data quality dashboards, you need to select the appropriate metrics, dimensions, sources, processes, or outputs to display and analyze. Additionally, you should choose suitable visualization tools, techniques, or platforms that can support your dashboard design, implementation, and maintenance.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Nicholas Leong
Copy link to contribution
Report contribution
Data Quality / Validation Dashboards are a great way to uphold data accuracy.

The dashboard should clearly indicate 
- What data are you testing for?
- The methods used for testing this data.
- Past test results.
- Sources and destinations of the data.

Share the dashboard with concerned stakeholders, allowing them 24/7 access to verify data integrity. This also enables them to suggest if any other metrics or dimensions need validation.

Go the extra mile by setting up alerting practices to notify relevant stakeholders when something goes wrong. For instance, send an email or Slack to the appropriate parties if a data quality test fails. This proactive step helps in swiftly addressing issues, avoiding any panic if downstream tasks break.
…see more
Like
5
4
Testing Maturity Levels
Assessing the maturity level of your data engineering testing processes and practices is another way to measure data quality and testing maturity. Testing maturity refers to how well planned, executed, automated, documented, and integrated your data engineering testing activities are with the data engineering lifecycle and standards. This can help you evaluate the effectiveness and efficiency of your data engineering testing methods and tools, identify your strengths and weaknesses in testing capabilities and skills, determine gaps and opportunities to enhance testing quality and coverage, and align your testing strategy with your data engineering goals. To measure the testing maturity level, you can use frameworks or models such as the Testing Maturity Model (TMM), the Test Process Improvement (TPI) model, or the Test Maturity Model Integration (TMMi). These typically describe characteristics, criteria, or best practices that correspond to each testing maturity level - from initial ad hoc testing to optimized continuous testing.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Testing Maturity Assessment
Once you have selected a suitable framework or model for measuring your testing maturity level, you need to conduct a testing maturity assessment that will help you evaluate and benchmark your current state of testing maturity against the desired or expected state. This assessment can collect and analyze data related to your data engineering testing processes and practices, such as test plans, cases, results, reports, automation, metrics, or feedback. Additionally, it can score and rate your data engineering testing performance and compliance for each testing maturity level, dimension, or criterion. This process also involves identifying and prioritizing gaps, issues, or challenges that need to be addressed or resolved. Finally, it can define and implement the testing maturity improvement actions that will help you achieve or maintain your target testing maturity level. To conduct a successful assessment, it is important to involve data engineering team and stakeholders such as testers and analysts. Moreover, you should use tools such as surveys, interviews, audits, reviews or workshops.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Testing Maturity Dashboards
Creating testing maturity dashboards is an effective way to communicate and visualize your testing maturity assessment results and improvement plans. These dashboards can help you track and compare the performance of different data engineering testing processes, identify and prioritize testing maturity issues, evaluate the impact of your initiatives, and engage stakeholders on the goals. To create effective testing maturity dashboards, you need to select the appropriate levels, dimensions, criteria, or outcomes to display and analyze. Additionally, choose data visualization tools that can support your dashboard design, implementation, and maintenance.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Engineering
Data Engineering
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Engineering
No more previous content
Clients demand both real-time and batch processing. How do you prioritize their needs?
2 contributions
You're faced with data accuracy and tight ETL deadlines. How do you strike a balance between the two?
3 contributions
You're facing a surge in data volume. How can you fine-tune ETL pipelines to manage it effectively?
4 contributions
You're facing a complex data migration. How do you prevent data loss?
20 contributions
You're facing conflicting opinions on data infrastructure upgrades. How do you prioritize tasks effectively?
3 contributions
Your organization is hesitant about cloud migration. How can you convince them of the benefits?
4 contributions
Struggling to balance data quality and processing speed in ETL workflows?
13 contributions
You've faced a data anomaly in your analysis. How do you prevent it from happening again?
4 contributions
You're facing data pipeline performance issues. How can you optimize them without causing disruption?
6 contributions
Clients are frustrated with delays in real-time data delivery. How do you handle their complaints?
7 contributions
You need to onboard new team members on data governance. How do you make it effective and engaging?
11 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
IT Services
How can you identify data quality issues in IT services using data modeling?
Application Development
How can you effectively manage data in your application?
Data Governance
What is the importance of data lineage analysis for data quality?
Data Analytics
What are the steps to ensure data quality during data processing?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
1
3 Contributions"
Data Quality Gates: Testing Our Data As We Create It!,"Data Quality Gates: Testing Our Data As We Create It!
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Data Quality Gates: Testing Our Data As We Create It!
Report this article
Next Insurance Engineering
Next Insurance Engineering
We transform small business insurance with simple, digital and affordable coverage tailored to the self-employed.
Published Jul 18, 2021
+ Follow
Anyone involved with data processing knows that if the one incident that should never happen, in fact, occurs and data errors are discovered, then it pays to act as soon as possible.
That’s exactly why we added the “magical” quality gate  components to our systems as of last July. Quality gates create barriers for alerting engineers that the quality of data they received is invalid.
Since integrating the concept into our ongoing work at Next, we’ve noticed that our data quality has risen, the data our data engineers are delivering is complete and more accurate (enabling them to sleep much better at night!), and our internal customers, analysts and senior management are happier than ever.
Hold on. Now don’t think that everything was rosy from the get-go. The need to add quality gates arose after we found ourselves delivering, time and again, data that we discovered contained errors (nothing that would bring the company down tomorrow, but enough to bother and motivate us to improve the process).
You know how it goes. You build models and suddenly something in the data that was inputted doesn’t work, or a developer changed part of the code and some of the work gets messed up. Be it one way or the other, all of us, yes even those of you reading this post, have had that experience. And it’s okay. To err is human (and to forgive is heavenly, but that’s for another post). The real problem, however, begins when we don’t actually know that something got messed up on the way. The smart approach is that as soon as there’s any type of error, data engineers can be the first to identify it and stop supplying invalid data.
So how do quality gates work for us? During our ETL (Extract Transform Load) process, we define various “gate” types that we won’t cross unless we have tested the generated data up to that point and everything passed muster. What’s significant is that we execute various sanity checks on source data, while simultaneously processing the data.
For example, if today is March 11, we should have data until the end of March 10. Elementary, Watson. Another test is business rules. Every business has rules by which it must operate, and if the rules don’t occur in the data, then something isn’t logical. At Next, for example, potential customers ask us for a quote for purchasing a digital insurance policy. What’s significant is that the number of quotes requests has to be greater than the number of policies actually generated. Another example is that the number of requests that we didn’t process successfully should be smaller, and if the numbers suddenly jumps, then there’s a problem.
Quality gates, by definition, must enable the implementation of various parameters such as different time frames, geographical segments, and small business types. Currently, we have dozens of quality gates, while we are always improving ourselves and adding new gates based on new developments. For example, we recently improved the process whereby only the developer of the data model and its successor receives a notification that quality gates have identified a problem (previously, all developers received notifications, and that was, how can we say, not very fun).
We developed the quality gates system internally at Next. It’s true that we didn’t invent the concept, but from what we know, there aren’t many companies in Israel that use it. The bottom line is that from a technological perspective, they’re not very complex. But from a conceptual perspective, there’s a real fundamental change. Instead of checking data at the end of the road (who said waterfall and didn’t get one?), we check them frequently and agilely, all the time.
So, what’s our next step? To work in a more organized fashion. If today we have one long script with all of the queries, everything becomes more complex and cumbersome as more tests are added. And we, ladies and gentlemen, hate that more than waking up in the middle of the night! So, we are going to manage things in a more organized and orderly way, and on the way, also think about the possibility that not only data engineers, but also data analysts, will be added to the tests.
And if we’re already talking about working in a more organized and quality fashion, the best indicator that our quality gates do the job is that over time, we have noticed that a number of bugs in Jira and the number of data failures are dropping. And you already know how the Jira ticket saying goes: “The less the merrier .”
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
18
To view or add a comment,
sign in
More articles by this author
No more previous content
Everything You Always Wanted to Know About Graph DB
Aug 3, 2021
Scaling Engineering Using K8s
Jul 28, 2021
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Effective Big Data Test Strategy,"Effective Big Data Test Strategy
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Effective Big Data Test Strategy
Report this article
Ritesh Garg
Ritesh Garg
Thinker, Believer, Achiever !!
Published Dec 12, 2014
+ Follow
Quality Assurance (QA) and testing teams need effective assurance and testing strategies to make their dive into Big Data rewarding. In this webinar, Nielsen and TCS share insights on Big Data in today's digital world, the role played by QA professionals and how to make your foray into Big Data.
Join us on Dec 18, 2014 at 12 hrs EST.
Register here:
http://lnkd.in/eVYwEnT
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
7
To view or add a comment,
sign in
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data Testing Overview,"Big Data Testing Overview
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing Overview
Report this article
Rohini Gopal
Rohini Gopal
Driving Automation Excellence | Quality Engineering Pro | Advocate for AI & Cloud Technologies
Published Jun 7, 2018
+ Follow
Big Data is one of the most trending terms used these days, as most of the organizations deal with a large number of datasets, which is quite complex to handle. Testing Big Data application is more a verification of its data processing rather than testing the individual features of the software product. When it comes to Big data testing,
performance and functional testing
are the keys.
It demands a high level of testing skills as the processing is very fast. Processing may be of
three types
1.      Batch
2.     Real Time
3.     Interactive
Along with this, data quality is also an important factor in big data testing as it involves checking various characteristics like conformity, accuracy, duplication, consistency, validity, data completeness, etc.
Big Data Testing can be broadly divided into three steps:
Step 1: Data Staging Validation
The first step of big data testing also referred as Pre-Hadoop stage involves process validation.
1.      Data from various source like RDBMS, weblogs etc. should be validated to make sure that correct data is pulled into the system.
2.     Comparing source data with the data pushed into the Hadoop system to make sure they match.
3.     Verify the right data is extracted and loaded into the correct HDFS location
Step 2: ""Map Reduce""Validation
The second step is a validation of ""Map Reduce"". In this stage, the tester verifies the business logic validation on every node and then validating them after running against multiple nodes, ensuring that
1.      Map Reduce process works correctly
2.     Data aggregation or segregation rules are implemented on the data
3.     Key value pairs are generated
4.     Validating the data after Map Reduce process
Step 3: Output Validation Phase
The final stage of Big Data testing is the output validation process. The output data files are generated and ready to be moved to an EDW (Enterprise Data Warehouse ) or any other system based on the requirement.
1.      To check the transformation rules are correctly applied
2.     To check the data integrity and successful data load into the target system
3.     To check that there is no data corruption by comparing the target data with the HDFS file system data
Performance Testing & Failover Test
Performance testing includes testing of job completion time, memory utilization, data throughput and similar system metrics. While the motive of Failover test service is to verify that data processing occurs seamlessly in case of failure of the data node.
What can be the common data challenges?
·        Heterogeneous and unstructured data spread across different layers
·        Continuous explosion of data resulting in bad data
·        Difficult business processes due to complicated business logic
·        Ineffective decision making bad data
·        Increased cost of handling variety, volume and velocity of large data sets
·        Performance issues due to heightened data volumes
So by adopting to Test Automation for Big Data Testing can help us in
·        Ensuring the large data sets across multiple sources are integrated accurately to provide real-time information
·        Certifying the quality of frequent data deployments to avoid incorrect decisions and subsequent actions
·        Aligning the data with changing dynamics to take predictive actions
·        Enabling leveraging the right insight from the minutest data sources
·        Ensuring scalability and processing of data across different layers and touch-points
Tools for Big Data testing:
·        NoSQL(Non relational): CouchDB, DatabasesMongoDB, Cassandra, Redis, ZooKeeper, HBase
·        MapReduce Technique: Hadoop, Hive, Pig, Cascading, Oozie, Kafka, S4, MapR, Flume
·        Storage: S3, HDFS( Hadoop Distributed File System)
·         Servers: Elastic, Heroku, Elastic, Google App Engine, EC2
·         Processing: R, Yahoo! Pipes, Mechanical Turk, BigSheets, Datameer, Talend
Thanks for reading. Stay tuned for more articles on Big Data testing techniques, tips, and tricks.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
39
2 Comments
Amit Tiwari
Lead Software Engineer
6y
Report this comment
+
Like
Reply
1 Reaction
Paul Haycock
Commercial Cleaning | Delivering Cleaning & Hygiene solutions in a changing Covid world
6y
Report this comment
I was just talking about big data with a business owner the other day - great perspective here.
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Head-to-Head: The Best Features of Selenium 4 vs. Playwright for Modern Test Automation
Nov 8, 2024
Becoming a Full Stack QA Engineer: A Comprehensive Guide to Mastery
Nov 1, 2024
Techniques for Inspecting Toaster Messages in DevTools
Oct 23, 2024
Maximizing Efficiency and Accuracy: The Essential Role of Automation in Software Quality Assurance
Oct 17, 2024
How to ensure effective test coverage across manual, automation, and performance testing,
Oct 16, 2024
AI-Powered Test Case Generation in New Systems: A Different Perspective Beyond Historical Data
Oct 14, 2024
The Future of Quality Engineering with Generative AI (GenAI)
Oct 4, 2024
Dockerfile for Running N Tests from a Repository
Jun 14, 2023
Mastering Test Case Design: Techniques, Code Examples, and Real-world Applications
Jun 1, 2023
Revolutionizing Test Automation with Artificial Intelligence and Machine Learning
May 29, 2023
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Audra Diers-Lawson on LinkedIn: Hallucination or vision? Establishing reliability and validity of AI-based… | 11 comments,"Audra Diers-Lawson on LinkedIn: Hallucination or vision? Establishing reliability and validity of AI-based… | 11 comments
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Audra Diers-Lawson’s Post
Audra Diers-Lawson
Professor of Risk & Crisis Communication
1mo
Report this post
Big data research and AI has created competing narratives with one side often lamenting the 'rise of the machines' and another side extolling the amazing things that we can learn with big data research. I never bought into the Terminator view, but have been increasingly frustrated with big data research because it's largely descriptive and once we get beyond the pretty pictures, it just wasn't as satisfying because I wanted to get beyond sentiment... I wanted to test and/or build theory not just look at very very (did I mention very) cool visualizations and trends. 

Then earlier this year my husband came across a RAG/LLM/NLP tool that didn't talk pish (i.e., addressed the well-known problem of hallucinations in tools like ChatGPT and other AIs) AND didn't require a PhD in computer science to use. LlamaParsing was developed to help organizations sort through their jillions of pages of documentation logically and usefully. But in academic terms, what it allows us to do is big data qualitative research and theory building/fit testing. 

So, I am really excited to share a piece of research that was just posted online today in Corporate Communications: An International Journal where Stuart and I tested the validity and reliability of this LlamaParsing RAG procedure in theory building and testing big data against a chunky data set related to Scottish independence conversations on Twitter (always Twitter, never X). Spoiler alert... yeah... it worked amazingly well. 😉 

Apparently, if you can use R for big data analysis, you do can this (that's what Stuart says... that wasn't my contribution ;) ).
https://lnkd.in/d-Wpfwhj
Hallucination or vision? Establishing reliability and validity of AI-based qualitative analysis of the unified model of activism in the Scottish independence Twitter debate
emerald.com
40
11 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Florian Meißner
Professor for Media Management & Journalism at Macromedia University of Applied Sciences
1mo
Report this comment
This sounds great! I'd appreciate a copy, too. 😬
Like
Reply
1 Reaction
2 Reactions
Roy Aulie J.
PhD Student | Political Communication | Social Media | Data-Driven Insights
1mo
Report this comment
That is very cool! Love to read it. I am unable to access the article due to the paywall- would it be possible to get a copy?
Like
Reply
1 Reaction
2 Reactions
Elisabeth Hasselström
PhD Fellow Kristiania University College
1mo
Report this comment
So cool! 🤩 Please send me a copy too!
Like
Reply
1 Reaction
2 Reactions
Thanas Goga
Driving positive change through strategic and risk communication
1mo
Report this comment
Quite interesting! Many thanks
Audra Diers-Lawson
for sharing! 🙏
Like
Reply
1 Reaction
2 Reactions
Cecilie Taran Skjerdal
Antropolog & journalist som danser teksttango med forskere
1mo
Report this comment
That's so cool!! Could I please have a copy, too?😃
Like
Reply
1 Reaction
2 Reactions
Shalewa Babatayo
Communication Enthusiast
1mo
Report this comment
Amazing! 
Definitely reading this. Thanks for sharing!
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More Relevant Posts
Niiara Aliieva
Aspiring Business Analyst | MSc in Business Analytics | Actively Seeking Internships and Graduate Opportunities | Expertise in Quantitative & Qualitative Market Research
3mo
Report this post
𝗥𝗲𝘁𝗵𝗶𝗻𝗸𝗶𝗻𝗴 𝗔𝗜 𝗘𝘁𝗵𝗶𝗰𝘀: 𝗔 𝗛𝗶𝗽𝗽𝗼𝗰𝗿𝗮𝘁𝗶𝗰 𝗢𝗮𝘁𝗵 𝗳𝗼𝗿 𝗗𝗮𝘁𝗮 𝗦𝗰𝗶𝗲𝗻𝘁𝗶𝘀𝘁𝘀? 🤖💡

🔍 Did you know that some police departments use predictive policing algorithms to determine where crime is likely to occur? While the intention is to enhance public safety, these algorithms can perpetuate biases, leading to disproportionate targeting of certain communities. This raises serious ethical questions about fairness and accountability in the deployment of AI technologies.

The need for an ethical framework in data science is more vital than ever. As future data analyst, I believe we must advocate for transparency, fairness, and accountability in every model we build and deploy.

I really encourage you to read the full article to learn more about this important topic.
https://lnkd.in/dHDaVTqi
To work for society, data scientists need a hippocratic oath with teeth
wired.com
7
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Massimiliano Marchesiello
AI & Machine Learning Specialist | Data Scientist
2w
Report this post
Driving Relevant GenAI / LLM Outcomes with Contextual Continuity
https://ift.tt/EuWlB4a
As GenAI tools like ChatGPT become increasingly popular for problem-solving and decision-making, users often face a critical challenge: navigating the vast expanse of knowledge within Large Language Models (LLMs) to find only the most relevant and accurate information. This task is akin to searching through the 40 million books in the Library of Congress to… Read More »Driving Relevant GenAI / LLM Outcomes with Contextual Continuity

The post Driving Relevant GenAI / LLM Outcomes with Contextual Continuity appeared first on Data Science Central.

via Data Science Central
https://ift.tt/HyueAI9
November 27, 2024 at 08:57PM
Driving Relevant GenAI / LLM Outcomes with Contextual Continuity

https://ift.tt/EuWlB4a

As GenAI tools like ChatGPT become increasingly popular for problem-solving and decision-making, users often face a critical challenge: navigating the vast expanse of knowledge within Large Language Models \(LLMs\) to find only the most relevant and accurate information. This task is akin to searching...
https://www.datasciencecentral.com
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Adjei Ameyaw
--
6mo
Report this post
How to Talk to Your Data

With the advancement of Large Language Models (LLMs) and Graph Databases, virtually anything can be turned into data—from traditional text to images. The challenge then becomes how to effectively store this data and easily access it. For instance, if a Company has a dataset containing customers information, they could pass a simple query like, ""Give me the details of customers who bought this or that,"" and retrieve the relevant information effortlessly. This is where Retrieval Augmented Generation (RAG) comes into play.

RAG enables efficient querying and retrieval of information from stored data. Below are the simplified steps for implementing RAG, adapted from Mariya Mansurova's article in Towards Data Science. For a detailed step-by-step guide, you can refer to the original articles:

- [Towards Data Science: RAG - How to Talk to Your Data](
https://lnkd.in/d-GqXf3k
)
- [Stackademic Blog: Using Neo4j and LangChain for Knowledge Graph Creation - A Detailed Guide](
https://lnkd.in/dqVmRQJX
)

Steps to Implement RAG

1. Loading Documents: Load documents from various data sources.
2. Splitting Documents: Split documents into manageable chunks.
3. Storage: Use vector stores to process and store data efficiently.
4. Retrieval: Retrieve documents relevant to the query.
5. Generation: Pass the query and relevant documents to the LLM to generate the final answer.

By following these steps, you are basically creating a mini ChatGPT with your data as the knowledge foundation. Below is a video demonstration of a simple RAG system I built, tested on the thesis of one of my seniors.
#LLMs
#DataScience
#RAG
#Data
#AI
#DataAnalytics
…more
8
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Jason Simmons
9mo
Report this post
Knowledge graphs continue to look like they will lead the way in producing far superior chat/agent AI performance. This has a nice breakdown of their approach, using an LLM to first create a knowledge graph (pre-processing a large document repository), and then integrating that into the RAG pipeline for chat.
GraphRAG: A new approach for discovery using complex information
https://www.microsoft.com/en-us/research
4
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ruslan Akst
Product Owner | Expert in AI and Digital Solutions | Driving Innovation at Localeats.ca
5mo
Edited
Report this post
Synthetic Data: A Step Towards a New Era of AI Training

🌐 At Hugging Face, an extensive dataset composed entirely of synthetic data has emerged. This approach, trending now, is agent-based. The LLM (in this case, GPT-4o + VLLM) doesn't generate answers randomly but adopts different personas each time, such as a chemist or a musician.

Synthetic data holds great promise, yet it is still viewed with caution. The concerns? It’s often not realistic or diverse enough, and potential hallucinations lurk within. The question remains whether we will ever fully embrace ""synthetics,"" but as you can see, strides are being made.

Explore  more here :
https://lnkd.in/gJ3mwQE6
PersonaHub#ArtificialIntelligence
#MachineLearning
#SyntheticData
#HuggingFace
#AI
#DataScience
#Innovation
#TechNews
#LLM
#GPT4
proj-persona/PersonaHub · Datasets at Hugging Face
huggingface.co
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Multiplatform.AI
1,700 followers
12mo
Report this post
Data.World
Report Unveils Groundbreaking Method for Tripling LLM Accuracy in Business Query Responses
#AI
#AIaccuracy
#AItechnology
#artificialintelligence
#benchmarkreport
#BrettHurt
#businessqueries
#dataworld
#dataworldAILab
#dbtLabs
#DrJuanSequeda
#GenAItechnology
#GenerativeAI
#Governance
#Hallucinations
#Knowledgegraphs
#Largelanguagemodels
#llm
#LLMs
#machinelearning
#McKinsey
#naturallanguagequeries
#organizationalAIreadiness
#responseaccuracy
#semanticknowledge
#SQLdatabases
#Stanford
#Transparency
#UCBerkeley
Data.World Report Unveils Groundbreaking Method for Tripling LLM Accuracy in Business Query Responses
https://multiplatform.ai
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Zhen Chen
7mo
Report this post
People often ask me how to deploy large language models in production and avoid hallucinations. The answer is simple.

You need a solid data structure and you need to perform quality assurance. The thing is... hallucinations will increase exponentially if you try to build on shaky foundations.

I was talking to someone the other day in charge of AI strategy at their company and I took a look at their data. They had two PDFs talking about the same thing but showing conflicting information.

You feed that kind of data to your LLM... you bet it will hallucinate. 

We already see the negative impacts of hallucinations in real life with examples like the lawyer who presented a made-up (hallucinated) case to the judge in court.

Does that mean LLMs are a technology you shouldn't use? Definitely not. It's like fire. You can burn yourself with it or you can enjoy some amazing grilled sausages over campfire if you know how to use it.

So to recap. First, make sure your data foundation is solid. Second, perform quality assurance. For that second step, there are innovative people like my friends
JS Patenaude
and
Philippe Maisonneuve
who are working on automating QA for LLMs.
#AI
#strategy
#innovation
…more
19
6 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Terezija Semenski
LinkedIn [in]structor  | I help people master Machine Learning fundamentals 50% faster | Founder @ Stack labs d.o.o.  ♦ Corporate Trainer |
1w
Report this post
What is 𝐑𝐞𝐭𝐫𝐢𝐞𝐯𝐚𝐥-𝐀𝐮𝐠𝐦𝐞𝐧𝐭𝐞𝐝 𝐆𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧 (𝐑𝐀𝐆)?

Imagine this: you’re asking the model something complex, and instead of just digging through what it learned months (or even years!) ago,
it goes out, finds the freshest info, and brings it right back to you in its answer. 
That’s Retrieval-Augmented Generation (RAG) in action.

RAG is like an AI with a search engine built in. 
Instead of winging it with just its trained data, it actively pulls in real-time facts from external sources and combines them with its own insights. 

The result? You get a response that’s not only coherent but packed with relevant, up-to-date information.

How it works?

1. Query encoding: 
When a user inputs a question, it’s encoded into a format that a search engine or database can process. 
The encoding turns the question into a vector or ""embedding"".

2. Retrieval phase: 
The retriever component then “searches” within an external database or document repository for relevant information. This step is critical as it brings in fresh, factual data, unlike traditional models that rely solely on pre-trained knowledge. 
The retrieved documents, often ranked by relevance, provide context for the response.

3. Generation phase: 
The embedding model takes both the initial query and the retrieved information. 
It compares these numeric values to vectors in a machine-readable index of an available knowledge base. Then it finds a match or multiple matches and retrieves the related data, converts it to words, and passes it back to the LLm. 

4. Response generation: 
With retrieved data, LLM combines the retrieved words and crafts response as an answer to the user.

Pros and Cons
➕ Pros: real-time access, improved accuracy, reduced hallucination, transparency
➖ Cons: complex implementation, increased latency, resource-intensive, dependency on data quality

———

♻️ Repost to inspire your network
🖱️ Follow me,
Terezija
for daily posts
#ai
#machinelearning
#ml
#llm
#rag
#techwithterezija
11
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Nancy Mertzel
Lawyer focused on technology, copyright, trademark and entertainment disputes, trademark clearance and prosecution, and IP counseling and agreements.
8mo
Report this post
Did you know AI Chatbots have political leanings? Check out this NY Times article to learn about the political viewpoints of various chatbots.
 
Read the article here (paywall applies):
https://lnkd.in/ei8PQr5n
#ainews
#aichatbots
#aiandpolitics
Opinion | How A.I. Chatbots Become Political
nytimes.com
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Virtualitics
8,968 followers
8mo
Report this post
Large language models (LLMs) can and should be used to accelerate appropriate tasks, but when it comes to its use in data analytics, the trained responses can also introduce a lot of risk. Despite the current limitations of LLMs, there is huge potential for this technology to benefit the data analytics space with the proper oversight. AI-guided
#IntelligentExploration
can help companies derive value from their data and take strategic action.
https://lnkd.in/ggJrV56t
Aakash Indurkhya
Sarthak Sahu
#LLM
#DataAnalysis
#ExplainableAI
Why A Bad LLM Is Worse Than No LLM At All
datanami.com
21
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
1,689 followers
159 Posts
2 Articles
View Profile
Follow
More from this author
Call for Abstracts Crisis2021
Audra Diers-Lawson
4y
Crisis 7 postponed to 2023, Announcing Crisis2021 Live Virtual Series
Audra Diers-Lawson
4y
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more industry insights
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
"Colin Manko on LinkedIn: We need to talk about data quality, data tests and assumptions.

By far…","Colin Manko on LinkedIn: We need to talk about data quality, data tests and assumptions.

By far…
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Colin Manko’s Post
Colin Manko
Data Engineer | Python Developer | Data & Software Design
2mo
Edited
Report this post
We need to talk about data quality, data tests and assumptions.

By far the number 1 thing to get right in data movement is testing your assumtptions of the data. A good place to start is practicing understanding what assumptions you are making. It turns out most Engineers don’t even know! In an end to end data warehouse the following tests matter:

Quality Checks
—
Quality checks assert the behavior of the data. Any assumption you make of the data can be checked. Combined with predictable transformations on different layers, you can assert the data is not affecting the assumptions you are making for creating your business logic and analytics interface. Examples include uniqueness of identifiers, data freshness, and even mutual exclusive conditions.

Validation of Analytics Metrics
—
Being able to reproduce your analytics metrics off of raw data means you can refactor relentlessly in the data model and assert you change no business definitions.

Certification of Raw Data / Regression of Source
—
You can programmatically assert your raw data is matching the source. Now you know your custom ETLs aren’t producing bugs.

Unit Testing
—-
For code that produces your data movement, make sure to factor out functions that don’t require the environment (eg; pure python) and run unit tests on the code. Any hard rule you introduce in your warehouse should be tested in this way to ensure backwards compatibility.

Integration Testing
—-
For code that acts on an environment (eg; alters a table), having ability to test the end to end behavior in small units means making sure your code doesn’t produce incorrect results.

Final thoughts
—-
Virtually all data issues I’ve seen could have been by 1) testing assumptions and 2) being able to recreate state (another topic). It is not hard to produce tests if you have 1) ability to know your assumptions 2) decent testing framework. If your data gets large, you can always use randomness for your tests! Make lighter assumptions earlier in your layers, and any knit picks later. Finally and most importantly, good tests and being able to recreate state means you can refactor and remodel at lightning speed.

Cheers!
3
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
Dongyang Jin
7mo
Edited
Report this post
Many teams I work with constantly mistook technical debt with data debt, and that proves to be fatal when your team is blinded by the wrong problem statement. 

This article said it well: 
""Treating data like a mere resource sets companies up for failure, as data is viewed as a means to an end for the organization. Just numbers for rarely viewed dashboards, just requirements for CRUD operations, or just a cost of running a business. Yet this perspective misses three significant attributes of data:
1. Data is an asset rather than a resource.
2. Not all data is of the same value.
3. The value of a data asset changes over time.""

To work well with data in our current day and age compels us to not simply rely on off-the-shelf statistical methods, but take time to understand the hidden heuristics and consistently pre-validate ETL pipelines before any modelling or predictive analytics take place.
The True Cost of Data Debt
dataproducts.substack.com
37
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Rosario D.
Accelerating Sales and Partner Growth in Four Languages | Championing Co-Creation for Lasting Success | Building Sustainable, High-Value Relationships | Passionate About Innovation and Blockchain Investment.
9mo
Report this post
Data Analysis for the Curious:  No Coding Required, Just a Dash of Data Know-How

In a world where data is the new gold, being able to mine and refine this resource without becoming a coding ninja is not just convenient; it's revolutionary. Enter the era of no-code data analysis, where the only prerequisite is a bit of data curiosity and some basic data management skills. It's here that platforms like AnalyticsCreator are making waves, catering to those who understand the value of data but don't speak fluent Python or SQL.

Imagine having a powerful tool at your disposal that feels less like rocket science and more like using your favorite smartphone app. This is the heart of no-code data analysis—intuitive, accessible, and, most importantly, inclusive. You don't need to be a data scientist; you just need to know your way around data basics and have the eagerness to explore.

This democratization of data analysis means that more people across an organization can play a role in uncovering insights. Whether you're in finance, marketing, or operations, as long as you've got a foundational understanding of data management, you're good to go. These no-code platforms are like having a data analyst at your fingertips, ready to help you navigate through datasets with ease and precision.

But here's where it gets interesting: while no-code tools are user-friendly, a sprinkle of data management know-how elevates your game. With platforms like AnalyticsCreator, that sprinkle ensures you're not just playing in the data sandbox but also building castles. It's about leveraging your existing knowledge to ask smarter questions, create more relevant models, and, ultimately, drive meaningful insights.

The beauty of this approach is that it accelerates the data analysis process without compromising on depth or quality. Decisions that once hinged on the availability of specialized data personnel can now be expedited, empowering teams to act swiftly and confidently. It's a win-win: the data teams can focus on more complex challenges while everyone else gets to unlock the power of data in their daily decisions.

So, what's the takeaway? No-code data warehouse automation is here, and it's opening doors for those ready to step up their data game with just a dash of know-how. It's an invitation to dive into the data pool without fear, armed with your understanding of data management and a tool designed to simplify the complex.

In the landscape shaped by AnalyticsCreator and similar platforms, the message is clear: the future of data analysis is not just for the tech elite but for anyone with curiosity and the right tools. Let's embrace this shift and explore the possibilities it brings. The data revolution is not just coming; it's already here, and it's more accessible than ever.

No-Code Data Pipeline Solution
https://hubs.ly/Q02nGFYG0
No-Code Data Pipeline Solution
analyticscreator.com
8
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
SHIVAM KUMAR
Full Stack Web Developer (MERN )
6mo
Report this post
What is a Data Structure?
A data structure is a way of organizing data in a computer so that it can be used effectively. It defines the relationship between the data and the operations that can be performed on the data. Choosing the right data structure can drastically improve the performance of an application.
Common Types of Data Structures
Arrays
Definition: A collection of elements identified by index or key.
Use Case: When storing multiple items of the same type.
Example: An array of integers to store marks of students.
Linked Lists
Definition: A linear data structure where each element points to the next, forming a sequence.
Use Case: When you need a dynamic data structure with efficient insertion and deletion.
Example: A linked list to implement a queue or stack.
Stacks
Definition: A linear data structure that follows the Last In, First Out (LIFO) principle.
Use Case: When managing data in a specific order, such as in function calls.
Example: A stack to reverse a string or check for balanced parentheses in an expression.
Queues
Definition: A linear data structure that follows the First In, First Out (FIFO) principle.
Use Case: When you need to handle data sequentially.
Example: A queue to manage tasks in a printer queue.
Trees
Definition: A hierarchical data structure with nodes, where each node has zero or more child nodes.
Use Case: When you need a structure to represent hierarchical relationships.
Example: A binary search tree for efficient searching and sorting.
Graphs
Definition: A collection of nodes connected by edges.
Use Case: When you need to represent networks, such as social networks or web pages.
Example: A graph to model and find the shortest path in a transportation network.
Hash Tables
Definition: A data structure that maps keys to values using a hash
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Seattle Data Guy
49,328 followers
2mo
Report this post
If you've had to build any data pipelines for analytics, then you're likely very familiar with the extract phase of an ELT or ETL.

As the name suggests the extract phase is when you connect to a data source and ""extract"" data from it. The most common data sources you'll be interacting with being databases, APIs, and file servers(via FTP or SFTP).

With my recent focus on going back to the basics, it occurred to me that I have never written about APIs and how we interact with them as data engineers.
Now, there are plenty of APIs that have caused me a lot of heartburn in my career and there are others that have been a piece of cake to handle.

But it all comes down to how the API is set up and the design choices made when it was built.

So, in this article, I wanted to talk about, first, the basics of an API, followed by reviewing some of the issues you'll deal with as a data engineer.
https://lnkd.in/gk7jSyTT
From Basics to Challenges: A Data Engineer's Journey with APIs
seattledataguy.substack.com
146
8 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Siddhesh Palande
Cloud Engineering | Full Stack Web | Kubernetes | Go Lang | JavaScript | TypeScript | Angular | React | NodeJs | Python | Elixir Lang
1mo
Report this post
Data structures are intimidating…

How do i choose a right data structure? asked one of the team member during a code review.

Question was genuine, yet harder to explain however tried to attempt an answer as simple as it could be…

Here is a simplest example of how to choose a data structure and why ?

Before that let us set the context right…

Assume you are to implement two functions for string manipulation all they do is to split the given string and provide appropriate response

And you have two data structures at your disposal 
1. Lists
2. Tuples

Lists are variable length and size do not have to be defined at initialisation.

Tuples are fixed length and size is required while initialisation.

With the context set right,

Function One - split
String.split
(“hello world”)
>> [“hello”, ”world”]

Function Two - split_at

String.split_at(“hello world”, 5)
>> {“hello”, “ world”}

As we can see in above examples Function One simply splits the string based on space delimeter and Function Two splits the string on the index / position into two separate parts.

The most interesting part here is the output of both examples as First Function returns a list and the second one returns a tuple

It’s quite interesting to understand the selection of data structure based on the use case as we can see the first function recieves a string input of arbitrary length which may or may not contain multiple of spaces so the parts the string will be split into are not known ahead of time hence lists fit to be appropriate.

While the second function knows beforehand it has to split the string in two parts no matter what length the input string is or how many spaces it contains hence it returns a tuple.

This way consciously analysing the requirement and usecase you will be empowered to choose the right data structure.
20
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Putta Ramakrishna
Linux Device Driver | C and C++ | Embedded C | Camera driver | UART | I2C | SPI | Linux and QNX | BSP | Bootloaders |TDA4|ISP Camera Tuning|ADAS
5mo
Report this post
Q: Data Structures:
What is Data Structure?

·      The data structure name indicates itself that organizing the data in memory. 
·      There are many ways of organizing the data in the memory as we have already seen one of the data structures, i.e., array in C language. 
·      Array is a collection of memory elements in which data is stored sequentially, i.e., one after another. 
·      In other words, we can say that array stores the elements in a continuous manner. 
·      This organization of data is done with the help of an array of data structures. 
·      There are also other ways to organize the data in memory. 
·      Let's see the different types of data structures.

There are two types of data structures:

1.Primitive data structure
2.Non-primitive data structure

Primitive Data structure:
> The primitive data structures are primitive data types. 
> The int, char, float, double, and pointer are the primitive data structures that can hold a single value.

Non-Primitive Data structure:

> The non-primitive data structure is divided into two types:
               1.Linear data structure
               2.Non-linear data structure

Linear Data Structure:
> The arrangement of data in a sequential manner is known as a linear data structure. 
> The data structures used for this purpose are Arrays, Linked list, Stacks, and Queues. 
> In these data structures, one element is connected to only one another element in a linear form.

Non-linear data structure:

 > When one element is connected to the 'n' number of elements known as a non-linear data structure. 
> The best example is trees and graphs. 
> In this case, the elements are arranged in a random manner.
68
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Monty Mourya
Business Intelligence Engineer
6mo
Report this post
""Data scientists spend 80% of their time cleaning data""? Yeah, it's not a joke. Most data cleaning boils down to these three tasks:
-> Removing Duplicates
-> Correcting Formats
-> Applying Business Rules (While working on a reference data import project, It's a requirement for me to add an auto-incrementing integer ID column for all tables before importing them into the production environment.)

To kick things off, I've got data cleaning challenge for you.

SQL Challenge: 

You're working with a table called customers which has the following schema:
   customer_id INT PRIMARY KEY,
   first_name VARCHAR(50),
   last_name VARCHAR(50),
   email VARCHAR(100),
   phone VARCHAR(20),
   address VARCHAR(255)

The customers table contains duplicate customer records due to data entry errors and inconsistencies. Some duplicates are exact matches, while others have minor variations like:

-> Different Cases: ""john Smith"" vs. ""John Smith""
-> Different Name Formats: ""Robert Downey Jr."" vs. ""R. Downey Jr.""
-> Missing or Extra Spaces: "" Jane Doe"" vs. ""Jane Doe""

Task: 
-> Identify Duplicates: Write a SQL query that identifies all duplicate customer records, taking into account potential variations in names.
-> Remove Duplicates: Write a SQL query that removes all duplicate records, keeping only one copy of each unique customer record.

Additional Considerations:
-> You cannot modify the existing customers table.
-> You can create temporary tables or views as needed.
-> Consider using string functions (e.g., LOWER, TRIM, REPLACE) to normalize data and improve duplicate detection.

Now, can you replicate your solution using Pandas?

Task remains the same:
-> Load the Data: Read the same CSV file containing customer data into a Pandas DataFrame. (Remember, it has the same structure as the SQL table.)
-> Identify Duplicates: Find all duplicate customer records, keeping in mind those pesky variations in names (cases, different formats, etc.).
-> Remove Duplicates: Eliminate the duplicates, leaving only one unique record for each customer.

Create table statements and csv can be found in the GitHub Repository.
Link to Repository:
https://lnkd.in/gA4iERuv
17
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Niko Korvenlaita
Dad | Husband | Co-founder & CTO @ reconfigured
3mo
Report this post
Data modeling tip of the day: eventify everything. Trust me, it's going to make your life so much nicer.

Data warehouse needs to be time variant (among other things) and be able to answer not only questions on current state, but also how things were. And that time travel capability is what will unlock so many valuable cases.

I know, time travel is not the first thing in many peoples mind when they start building their first iterations of some sort of data warehouse. But trust me, we all will run into the moment where we first time go like: ""Oh snap, I wish we'd recorded the change history"".

For that reason ensure everything when it comes in event telling that something happened, be it some sort of activity or change in some state. If the source system and/or ETL tool of your choosing does not support this out of the box make sure you implement it your self.

Learn to work with this kind of event streams, do AS OF joins, reconstruct snapshots of the whole system in different times. This stuff will pay off! And these funnily are the type of queries that ChattyG struggles to generate correctly!

And as
Robert Harmon
so often reminds us: the platforms we have today are insanely fast and cost efficient. For that reason working with this kind of data is super easy today.

🤠 now let's head back to future 🤠
69
32 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Dunith Danushka
Dunith Danushka is an Influencer
Senior Developer Advocate at Redpanda Data | LinkedIn Top Voice | Writer | Data Educator
9mo
Report this post
This week’s sketch note is about four patterns for in-broker transformations.

Last week, we discussed Redpanda’s in-broker data transforms with Wasm. 
I thought of extending it to discuss a few transform patterns. So, here we go.

For context, please refer to the last week’s sketch note.
https://lnkd.in/eU4ThaEm
1️⃣ Message translator

This pattern translates/converts the format of the input message to another format.
The pattern enables communication across different systems, allowing them to communicate seamlessly despite their differences.
The pattern is similar to the map() function in stream processing.

Examples include:
- A large XML message received from a legacy system is converted to a mobile-friendly JSON message.
- Sensitive fields (PI/PII) in a message payload are redacted before sending it to the destination. E.g. password redaction

2️⃣ Message filter
This pattern eliminates undesired messages from a topic based on a set of criteria.
Sometimes, it is necessary to filter out unnecessary messages directed towards a destination to save bandwidth as well as processing capacity.

The Message Filter has only a single output topic. If the message content matches the criteria specified by the Message Filter, the message is routed to the output topic. If the message content does not match the criteria, the message is discarded.

This pattern is similar to the filter() function in stream processing.

Examples include:
- Filtering customer orders based on the region.
- Filtering malformed messages or potential poison pills.

3️⃣ Message splitter

This pattern breaks out a composite message into a series of individual messages, each containing data related to one item.
Similar to the flatMapt() function, this pattern emits one or more individual output messages based on the structure of the input message.

Examples include:
- Extract order items from an order object
- Extract comments from a blog post
- Extract individual Likes from a social post

4️⃣ Content enricher
This pattern enriches an input message with missing information.
For example, look up by productId to fetch the product name, and add it to the payload.

Currently, Redpanda transforms have no external access to disk or network resources. So, you can’t do lookups with external resources. However, you can use this pattern to replace missing fields with sensible defaults or anything meaningful.

Examples include:
- Replace a missing field with a default static value.

Well, these are just four patterns for you to try out. As we improve over the next iterations, we will add support for a few other patterns, like routing messages among multiple topics, etc.

Read more about Redpanda Wasm transformations from here
https://lnkd.in/et4hqwzy
47
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
iheb gabteni
Ingénieur systèmes et production informatique chez BIAT
Project Manager 
Java | Python developer🖥️♉
8mo
Report this post
The concept map includes the following data structures:
Stack: A LIFO (Last In, First Out) data structure. The last element added to the stack is the first element to be removed. Stacks are commonly used to implement undo/redo functionality, function calls, and balancing parentheses.
Queue: A FIFO (First In, First Out) data structure. The first element added to the queue is the first element to be removed. Queues are commonly used to implement task scheduling, bread-first search algorithms, and managing printer jobs.
Linked List: A linear data structure where each element is a separate object. Each element (or node) contains data and a reference to the next node in the list. Linked lists are commonly used to implement dynamic data structures, such as queues and stacks.
Array: A collection of items stored at contiguous memory locations. Arrays are commonly used to store collections of similar data types, such as numbers or strings.
Tree: A hierarchical data structure that simulates a tree with a set of connected nodes. Each node contains data and may contain references to child nodes. Trees are commonly used to implement hierarchical relationships, such as file systems, and sorting algorithms.
Binary Search Tree: A type of tree where each node contains data and references to a left child (lesser value) and a right child (greater value). Binary search trees are commonly used for efficient searching and sorting.
Binary Heap: A specialized tree that satisfies the heap property: the value of a parent node is either greater than or less than (depending on the type of heap) the values of its children. Binary heaps are commonly used to implement priority queues.
The concept map also includes the following algorithms:
Search: A technique for finding an item within a data structure. There are many different search algorithms, including linear search, binary search, and depth-first search.
Traversal: A technique for visiting each element in a data structure. There are many different traversal algorithms, including in-order traversal, pre-order traversal, and post-order traversal.
Sorting: A technique for arranging the elements of a data structure in a particular order. There are many different sorting algorithms, including bubble sort, insertion sort, selection sort, merge sort, and quick sort.
3
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
1,284 followers
57 Posts
View Profile
Follow
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
"2016 Predictions: Security, devops, big data and mobile testing","2016 Predictions: Security, devops, big data and mobile testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
2016 Predictions: Security, devops, big data and mobile testing
Report this article
Ulf Mattsson
Ulf Mattsson
Chief Security Strategist @ Protegrity | Information Security Policy
Published Jan 20, 2016
+ Follow
Source:
http://www.rcrwireless.com/20160120/opinion/2016-predictions-security-devops-big-data-and-mobile-testing-tag10?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+rcrwireless%2FsLmV+%28RCR+Wireless+News%29
I agree that “in 2016, IoT will rapidly expand the attack surface and present a very real threat to individuals, businesses and governments.”
Many of these devices are so small that they lack the computing power needed for data security or remote patching once vulnerabilities are found. And with billions of connected devices entering the world every year, consumer protection and privacy advocates face an uphill battle.
We cannot even wait for a new generation of hopefully more secure devices to be developed.
I see a major issue when IoT data from different sources is collected and stored centrally.  That will require implementing robust and layered risk management controls as well as encrypting or tokenizing that data while it’s stored on the company’s servers, being used for analytics in Big Data environments, or shared with other cloud-based services.
Ulf Mattsson, CTO Protegrity
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
2
To view or add a comment,
sign in
More articles by this author
No more previous content
Questions and slides from my ""Tokenization in Blockchain"" webinar
Nov 20, 2019
What is Practical Risk Management for the Multi-Cloud?
Sep 25, 2019
Please tell me what you think about my ""What I Learned at Gartner Summit 2019""
Jul 5, 2019
Is Tokenization effective for GDPR compliance?
Aug 17, 2018
Will you consider Encryption or Tokenization as effective GDPR Best Practices?
Aug 10, 2018
Do You Have a Roadmap for EU GDPR Compliance?
Oct 10, 2017
Do You Have a Roadmap for EU GDPR Compliance?
Aug 19, 2017
Please check my webinars about Ransomware including WannaCry
Jul 14, 2017
Ulf Mattsson is inventor of these 45+ issued US Patents
Apr 22, 2017
Are you ready for the new requirements of PCI-DSS V3.2?
Jun 23, 2016
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Testing Big Data,"Testing Big Data
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
https://s-media-cache-ak0.pinimg.com/736x/8a/cd/9d/8acd9d53917a390f163cbbea2515f070.jpg
Testing Big Data
Report this article
Gagan Mehra
Gagan Mehra
Managing Director, Commerce & Marketplaces
Published Jul 21, 2015
+ Follow
Data volumes, variety and velocity make testing big data applications a challenge. Several applications go-live without fully testing the exact production setup and if they are lucky, they have a smooth run in production. Unfortunately, leaving success of the business to luck is not considered a best practice. So, here's what should be done to test big data applications before taking them live.
1.
Define Success Criteria
: This is the most important step in testing anything. Without well-defined success criteria nothing can be successful. It is also important to have this criteria static as a lot of preparation and setup goes into making a big data application meet a success criteria. I have been in situations where the customer changed the success criteria when the initial criterion was met and that required significant changes to the setup. So, discuss and finalize the success criteria for testing big data applications.
2.
Test Individual Components
: If multiple vendors or components are being integrated to build a big data application then it is recommended to test each individual component to identify scalability and performance challenges. Once the individual components have met the requirements, then the complete application should be tested together. This means allocating more time for testing but that is better than having issues in the production environment.
3.
Use Data Simulators
: If real data is not easily available or it has limitations like the volume available for testing, then data simulators can be used to create data. Several simulation tools are available and a straightforward simulator can be built by doing some basic scripting. The simulators should be used to test production data loads and beyond, to account for growth.
4.
Scale Horizontally
: Several big data applications do not scale linearly and hence it is a good practice to test horizontal scalability before going live. This can be done with a fairly basic setup  - a large box with multiple VMs. Adding more nodes of the application in different VMs will demonstrate how horizontal scalability works. Key areas to watch out for are severe degradation in performance as the data volumes cross a certain threshold and increased chatter as the number of nodes grow, impacting the application performance.
5.
Use Tools
: Most big data vendors ship their products with guidelines and tools for testing them. If these tools are not available then request the vendor(s) for them. Vendors use a variety of internal testing tools to verify if the product is ready for big data use cases and the same tools can be used by an enterprise after requesting the vendor. Additionally, review existing benchmarks to determine if they align with the scalability and latency requirements of your environment. If not, then collaborate with the vendor(s) to establish new benchmarks and make the purchase of the product contingent on achieving these benchmarks.
Please share your experiences and insights from testing big data applications.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
11
1 Comment
Kumar Chinnakali
A roaming hands-on architect who meets clients, developers, executives in their context.
9y
Report this comment
Helped a lot. Thanks for sharing Gagan !
Like
Reply
1 Reaction
2 Reactions
To view or add a comment,
sign in
More articles by Gagan Mehra
Ten Suggestions for a Better LinkedIn
Oct 25, 2015
Ten Suggestions for a Better LinkedIn
LinkedIn is an amazing tool for maintaining professional connections. But I wish it could do more.
31
6 Steps for Retailers to Deploy an IoT-Based Solution
Oct 23, 2015
6 Steps for Retailers to Deploy an IoT-Based Solution
Though smart logistics and supply chain have emerged as the top use cases for IoT, retailers continue to struggle with…
22
3 Comments
Upcoming Liability Shift for Credit Cards
Sep 29, 2015
Upcoming Liability Shift for Credit Cards
Starting Oct 1 there will be a significant shift in how fraud is handled by credit card companies. Any merchant that…
7
Should Reference Checks be Retired?
Sep 26, 2015
Should Reference Checks be Retired?
I was recently asked by an ex-colleague to be a reference for his next job. The recruiter called me and asked me the…
3
1 Comment
Solution Selling - A Must Have for Big Data
Sep 13, 2015
Solution Selling - A Must Have for Big Data
Big Data has more believers now than ever before. Most people know that they need to utilize big data as part of their…
7
1 Comment
The World of Drones
Sep 7, 2015
The World of Drones
Drones are the new hot item in 2015. Drone startups have already raised over $150M in 2015 and are all set to have a…
21
2 Comments
No Privacy is the New Privacy
Aug 29, 2015
No Privacy is the New Privacy
Hardly a few weeks go by before we hear the news of another hack. A lot of times these are hacks that are made public…
4
2 Comments
Building a Great Product
Jul 20, 2015
Building a Great Product
Great products are rare. These products solve a real challenge or create an opportunity.
4
2 Comments
A Few Amazing Facts About New Horizons Spacecraft
Jul 18, 2015
A Few Amazing Facts About New Horizons Spacecraft
New Horizons has successfully flown past Pluto and in the process has made history as the first space probe to do so…
10
Open Source is Not Free
Jul 16, 2015
Open Source is Not Free
Open source software is heavily used in the enterprise space. There are several instances where open source has become…
5
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
"Data quality testing - Data Science Foundations: Data Engineering Video Tutorial | LinkedIn Learning, formerly Lynda.com","Data quality testing - Data Science Foundations: Data Engineering Video Tutorial | LinkedIn Learning, formerly Lynda.com
Skip to main content
Learning
LinkedIn Learning
Search skills, subjects, or software
Expand search
This button displays the currently selected search type. When expanded it provides a list of search options that will switch the search inputs to match the current selection.
Jobs
People
Learning
Clear text
Clear text
Clear text
Clear text
Clear text
Start free trial
Sign in
Solutions for:
Business
Higher Education
Government
Buy for my team
From the course:
Data Science Foundations: Data Engineering
Unlock the full course today
Join today to access over 24,200 courses taught by industry experts.
Data quality testing
From the course:
Data Science Foundations: Data Engineering
Start my 1-month free trial
Buy for my team
Transcripts
Exercise Files
View Offline
Data quality testing
“
- Alright, now that we have some data in our environment let's start looking at actually performing data quality testing on it. First we're going to do a check for blank values and we'll look at some key fields. Then we'll find missing locations, so in our sales data obviously the location is important. And then we'll look for any invalid dates. Often this is something where the timestamp that was generated from whatever system you're getting the value from is invalid for some reason. It's either way into the future or way into the past before it could possibly have happened. So we'll check for all these three things in our environment. Here in my virtual machine I've loaded the 2_2.sql exercise file. And I'm just going to copy these in one by one and go through them. So first we're going to do a count where orderid is blank. So I'm going to copy that into my Hue Editor. Let me maximize this just a little bit so you can see the full query there. And what we're going to do is basically…
Practice while you learn with exercise files
Download the files the instructor uses to teach the course. Follow along and learn by watching, listening and practicing.
Exercise File: Subscribe to access.
Ex_Files_Data_Eng_EssT.zip
Download the exercise files for this course.
Get started with a free trial today.
Download courses and learn on the go
Watch courses on your mobile device without an internet connection. Download courses using your iOS or Android LinkedIn Learning app.
Watch this course anytime, anywhere.
Get started with a free trial today.
Contents
Introduction
Introduction
Welcome
55s
What you should know before watching this course
23s
Using the exercise files
35s
1. Ecosystem Overview
1. Ecosystem Overview
(Locked)
Data science system overview
3m 20s
(Locked)
Star schema design overview
3m 16s
(Locked)
Where does data engineering fit?
1m 25s
Components of a good data pipeline
2m 54s
(Locked)
Environment setup
4m 40s
2. Staging Data
2. Staging Data
(Locked)
Loading and profiling data
5m 20s
(Locked)
Data quality testing
6m 4s
3. Cleansing Data
3. Cleansing Data
(Locked)
Adding data types
3m 51s
Handling missing values
3m 3s
(Locked)
Verifying addresses
4m 23s
4. Conforming Data
4. Conforming Data
(Locked)
Performing master data lookups
3m 8s
(Locked)
Handling inferred members
1m 17s
5. Delivering Analytical Data Sets
5. Delivering Analytical Data Sets
(Locked)
Loading the star schema
58s
(Locked)
Loading dimension tables
1m 15s
(Locked)
Loading fact tables
1m 49s
(Locked)
Creating views
3m 55s
Conclusion
Conclusion
(Locked)
Next steps
53s
*
Price may change based on profile and billing country information entered during Sign In or Registration
Explore Business Topics
Business Analysis and Strategy
Business Software and Tools
Career Development
Customer Service
Diversity, Equity, and Inclusion (DEI)
Finance and Accounting
Human Resources
Leadership and Management
Marketing
Professional Development
Project Management
Sales
Small Business and Entrepreneurship
Training and Education
See all
Explore Creative Topics
AEC
Animation and Illustration
Audio and Music
Graphic Design
Motion Graphics and VFX
Photography
Product and Manufacturing
User Experience
Video
Visualization and Real-Time
Web Design
See all
Explore Technology Topics
Artificial Intelligence (AI)
Cloud Computing
Cybersecurity
Data Science
Database Management
DevOps
Hardware
IT Help Desk
Mobile Development
Network and System Administration
Software Development
Web Development
See all
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Data Quality and Testing Alignment with Governance Policies,"Data Quality and Testing Alignment with Governance Policies
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Engineering
How can you ensure data quality and testing alignment with your governance policies?
Powered by AI and the LinkedIn community
1
Define your governance policies
Be the first to add your personal experience
2
Implement data quality checks
Be the first to add your personal experience
3
Design data testing frameworks
Be the first to add your personal experience
4
Monitor data quality and testing outcomes
Be the first to add your personal experience
5
Review and improve your data quality and testing processes
Be the first to add your personal experience
6
Here’s what else to consider
Be the first to add your personal experience
Data quality and testing are essential aspects of data engineering, as they ensure the reliability, accuracy, and usability of your data assets. However, data quality and testing are not static or isolated processes, but rather dynamic and interrelated ones that depend on your governance policies and standards. In this article, you will learn how to align your data quality and testing strategies with your governance policies, and what tools and techniques you can use to achieve this goal.
Find expert answers in this collaborative article
Experts who add quality contributions will have a chance to be featured.
Learn more
See what others are saying
1
Define your governance policies
Your governance policies are the rules and guidelines that specify how your data should be collected, stored, processed, accessed, and used within your organization. They also define the roles and responsibilities of the data stakeholders, the data quality dimensions and metrics, the data security and privacy requirements, and the data compliance and audit procedures. To align your data quality and testing with your governance policies, you need to first define them clearly and communicate them effectively to your data team and users.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
2
Implement data quality checks
Data quality checks are the methods and tools that you use to verify, validate, and monitor the quality of your data against your governance policies. They can be performed at different stages of your data lifecycle, such as during data ingestion, transformation, integration, or delivery. Data quality checks can include data profiling, data cleansing, data deduplication, data enrichment, data verification, and data anomaly detection. You can use various data quality tools, such as Apache Airflow, Data Quality Monitor, or Data Quality Tester, to automate and orchestrate your data quality checks.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Design data testing frameworks
Data testing frameworks are the structures and processes that you use to plan, execute, and evaluate your data tests against your governance policies. They can be applied at different levels of your data architecture, such as unit testing, integration testing, end-to-end testing, or user acceptance testing. Data testing frameworks can include data test cases, data test scripts, data test environments, data test data, data test results, and data test reports. You can use various data testing tools, such as PyTest, DBT, or Great Expectations, to facilitate and standardize your data testing frameworks.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Monitor data quality and testing outcomes
Data quality and testing outcomes are the measures and indicators that you use to assess the performance and effectiveness of your data quality and testing strategies against your governance policies. They can be reported and analyzed at different frequencies and granularities, such as daily, weekly, monthly, or quarterly, and by data source, data pipeline, data product, or data user. Data quality and testing outcomes can include data quality scores, data quality issues, data test coverage, data test failures, and data test feedback. You can use various data quality and testing dashboards, such as Grafana, Tableau, or Data Quality Dashboard, to visualize and monitor your data quality and testing outcomes.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Review and improve your data quality and testing processes
Data quality and testing processes are the activities and workflows that you use to implement, execute, and monitor your data quality and testing strategies against your governance policies. They are not static or fixed, but rather dynamic and iterative, as they need to adapt to the changing needs and expectations of your data users and stakeholders. Data quality and testing processes can include data quality and testing planning, data quality and testing execution, data quality and testing evaluation, and data quality and testing improvement. You can use various data quality and testing feedback loops, such as data quality and testing reviews, data quality and testing surveys, or data quality and testing retrospectives, to review and improve your data quality and testing processes.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Engineering
Data Engineering
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Engineering
No more previous content
Clients demand both real-time and batch processing. How do you prioritize their needs?
2 contributions
You're faced with data accuracy and tight ETL deadlines. How do you strike a balance between the two?
3 contributions
You're facing a surge in data volume. How can you fine-tune ETL pipelines to manage it effectively?
4 contributions
You're facing a complex data migration. How do you prevent data loss?
20 contributions
You're facing conflicting opinions on data infrastructure upgrades. How do you prioritize tasks effectively?
3 contributions
Your organization is hesitant about cloud migration. How can you convince them of the benefits?
4 contributions
Struggling to balance data quality and processing speed in ETL workflows?
13 contributions
You've faced a data anomaly in your analysis. How do you prevent it from happening again?
4 contributions
You're facing data pipeline performance issues. How can you optimize them without causing disruption?
6 contributions
Clients are frustrated with delays in real-time data delivery. How do you handle their complaints?
7 contributions
You need to onboard new team members on data governance. How do you make it effective and engaging?
11 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Analytics
How can you ensure consistent data quality across platforms?
Data Engineering
How do you prevent common data quality issues in data pipelines?
Data Engineering
What are the latest trends in data validation?
Data Management
What is the most effective way to establish a data quality framework?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share"
How to Assess and Improve Your Data Quality,"How to Assess and Improve Your Data Quality
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Last updated on
      Oct 23, 2024
All
Digital Trends
What are the best tools and methods for data quality assessment and improvement?
Powered by AI and the LinkedIn community
1
Data quality dimensions
2
Data quality assessment tools
3
Data quality improvement methods
4
Data quality monitoring and evaluation
Be the first to add your personal experience
5
Data quality best practices
6
Data quality benefits and challenges
7
Here’s what else to consider
Be the first to add your personal experience
Data quality is a crucial aspect of any data-driven organization, as it affects the reliability, accuracy, and usability of the data. Poor data quality can lead to inaccurate insights, wasted resources, and missed opportunities. Therefore, data quality assessment and improvement are essential processes to ensure that the data meets the standards and expectations of the stakeholders. In this article, we will explore some of the best tools and methods for data quality assessment and improvement, and how they can help you achieve your data goals.
Top experts in this article
Selected by the community from 8 contributions.
Learn more
Vito Casalinuovo
Digital Experience professional
View contribution
4
Pia Kiviranta
Tech and Business meets Modern Digital Marketing | Building Relevant Brand Awareness & Visibility | Development…
View contribution
2
Marcel Meyer
Geschäftsführer graubünden digital 📲 Projektleiter Fachstelle Tourismus 🏔️ Dozent 👨🏫 Mentor 🚀 Blogger 👨💻…
View contribution
1
See what others are saying
1
Data quality dimensions
The first step in data quality assessment and improvement is to define what data quality means for your organization and your specific use cases. Data quality can be measured along several dimensions, such as completeness, validity, accuracy, consistency, timeliness, and relevance. Each dimension has its own criteria and metrics to evaluate the data quality. For example, completeness refers to the extent to which the data has all the required attributes and values, while validity refers to the extent to which the data conforms to the rules and formats of the data domain.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Vito Casalinuovo
Digital Experience professional
Copy link to contribution
Report contribution
Before organizations embark on the journey of assessing and improving their data, it is crucial to grasp the depth and breadth of the current state of their data. A misconception prevails in many organizations that artificial intelligence (AI) can easily remedy their challenges, yet unstructured data and content often pose the most substantial and underestimated hurdles.
Recognize that data, sometimes decades old and still residing in Mainframe systems, was not originally structured or intended for today's purposes. 
AI capabilities surpass the existing data quality and governance required to unlock the potential that AI promises. Bridging this gap is imperative for organizations to fully capitalize on the benefits that AI can bring.
…see more
Like
4
Pia Kiviranta
Tech and Business meets Modern Digital Marketing | Building Relevant Brand Awareness & Visibility | Development Engineer rockin' Marketing, Branding and Comms | Verkostoitumisrinki-Suomi
Copy link to contribution
Report contribution
""You get what you measure"" Make sure that gathered data reflects 
 your company's target's and KPIs and is relevant in that point of view. After this step you should focus on data quality level to meet the requirements.
…see more
Like
2
Marcel Meyer
Geschäftsführer graubünden digital 📲 Projektleiter Fachstelle Tourismus 🏔️ Dozent 👨🏫 Mentor 🚀 Blogger 👨💻 Speaker 🎤 MSc BA New Business 👨🏻🎓 Familienvater 👼
Copy link to contribution
Report contribution
Datenqualität bezieht sich auf die Gesamtheit von Merkmalen und Eigenschaften von Daten, die ihre Eignung für einen bestimmten Verwendungszweck bestimmen. Qualitativ hochwertige Daten sind präzise, konsistent, aktuell und relevant. Sie erfüllen die Anforderungen der Benutzer und ermöglichen zuverlässige Analysen, fundierte Entscheidungen und effektive Geschäftsprozesse. Aspekte der Datenqualität umfassen Genauigkeit, Vollständigkeit, Konsistenz, Aktualität und Zuverlässigkeit der Daten. Eine hohe Datenqualität ist entscheidend für den Erfolg von Unternehmen, da sie die Grundlage für verlässliche Informationen und Analysen bildet.
…see more
Translated
Show translation
Show original
Like
1
Swati Paliwal
Head of marketing - Sprouts.ai | Ex Disney+ | B2B | Digital Strategy & Consulting | Content | LinkedIn Expert - ICKP
Copy link to contribution
Report contribution
For assessing and improving data quality, several tools and methods can be employed:

Data Profiling Tools: Utilize tools like Talend Data Quality, Informatica Data Quality, or Trifacta to analyze data sets and identify inconsistencies, anomalies, and missing values.

Data Quality Metrics: Define key data quality metrics such as completeness, accuracy, consistency, and timeliness to measure and monitor the quality of your data over time.

Data Cleansing Tools: Employ tools like OpenRefine, DataCleaner, or Microsoft Excel's data cleaning functionalities to standardize formats, remove duplicates, and correct errors in your data.
…see more
Like
1
2
Data quality assessment tools
Once you have defined your data quality dimensions and metrics, you need to use the appropriate tools to assess the current state of your data quality. There are many tools available in the market that can help you perform data quality assessment, such as Data Quality Analyzer, DataCleaner, and OpenRefine. These tools can help you identify, analyze, and report the data quality issues, such as missing values, duplicates, outliers, and inconsistencies. They can also help you visualize and explore the data quality using dashboards, charts, and graphs.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Marcel Meyer
Geschäftsführer graubünden digital 📲 Projektleiter Fachstelle Tourismus 🏔️ Dozent 👨🏫 Mentor 🚀 Blogger 👨💻 Speaker 🎤 MSc BA New Business 👨🏻🎓 Familienvater 👼
Copy link to contribution
Report contribution
Hier einige Tools: 

DataCleaner 🛠️:
Funktion: Open-Source-Tool für Bereinigung und Standardisierung.
Vorteile: Unterstützt verschiedene Datenquellen.

IBM InfoSphere Information Analyzer 🖥️:
Funktion: Analysiert Datenqualität und identifiziert Unstimmigkeiten.
Vorteile: Tiefgreifende Analysen, benutzerfreundliches Interface.

Microsoft DQS 🌐:
Funktion: Integration in SQL Server für Profilierung und Bereinigung.
Vorteile: Nahtlose Integration in Microsoft-Umgebungen.

Google Refine (OpenRefine) 🌐:
Funktion: Open-Source-Tool für Datenbereinigung.
Vorteile: Benutzerfreundlich, umfassende Funktionen für Datenqualitätsverbesserung.
…see more
Translated
Show translation
Show original
Like
3
Data quality improvement methods
After you have assessed your data quality, you need to implement the necessary methods to improve it. Data quality improvement methods can be classified into two categories: preventive and corrective. Preventive methods aim to prevent the occurrence of data quality issues by enforcing data quality rules, standards, and policies at the source of data generation or collection. For example, you can use data validation, data profiling, and data governance techniques to ensure that the data is accurate and consistent from the beginning. Corrective methods aim to fix the existing data quality issues by applying data cleaning, data transformation, and data enrichment techniques. For example, you can use data cleansing tools, such as Trifacta, Talend, and Pentaho, to remove, replace, or modify the erroneous or incomplete data.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Marcel Meyer
Geschäftsführer graubünden digital 📲 Projektleiter Fachstelle Tourismus 🏔️ Dozent 👨🏫 Mentor 🚀 Blogger 👨💻 Speaker 🎤 MSc BA New Business 👨🏻🎓 Familienvater 👼
Copy link to contribution
Report contribution
Einige Möglichkeiten zur Verbesserung der Datenqualität: 

🔴 Datenbereinigung: Identifikation und Korrektur von fehlerhaften oder inkonsistenten Daten.
🔴 Dublettenprüfung: Erkennung und Entfernung doppelter Datensätze.
🔴 Validierungsregeln: Implementierung von Regeln zur Überprüfung und Validierung von Daten.
🔴 Automatisierte Überwachung: Kontinuierliche Überwachung und Aktualisierung von Daten.
🔴 Schulungen: Sensibilisierung und Schulungen für Mitarbeiter zur bewussten Dateneingabe und -pflege.
…see more
Translated
Show translation
Show original
Like
4
Data quality monitoring and evaluation
Data quality improvement is not a one-time activity, but a continuous process that requires regular monitoring and evaluation. Data quality monitoring and evaluation are the processes of measuring and tracking the changes in data quality over time, and assessing the impact and effectiveness of the data quality improvement methods. You can use data quality monitoring and evaluation tools, such as Data Quality Monitor, Data Quality Dashboard, and Data Quality Scorecard, to automate and simplify these processes. These tools can help you set up data quality indicators, thresholds, and alerts, and generate data quality reports and feedback.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Data quality best practices
To ensure and sustain high data quality, it is essential to adhere to best practices that can direct your data quality assessment and improvement efforts. These practices include clearly defining data quality goals and metrics that are in line with business objectives and user needs, establishing a data quality culture among stakeholders and data owners, assigning roles and responsibilities for data quality management, using a combination of preventive and corrective methods to address root causes and symptoms of data quality issues, leveraging tools and technologies to automate and streamline data quality assessment and improvement processes, and continuously monitoring and evaluating data quality performance.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Marcel Meyer
Geschäftsführer graubünden digital 📲 Projektleiter Fachstelle Tourismus 🏔️ Dozent 👨🏫 Mentor 🚀 Blogger 👨💻 Speaker 🎤 MSc BA New Business 👨🏻🎓 Familienvater 👼
Copy link to contribution
Report contribution
Best Practices: 
McDonald's und die McCafé App:
Best Practice: Implementierung von Echtzeit-Datenvalidierung in der McCafé App.
Ergebnis: Vermeidung fehlerhafter Bestellungen, verbesserte Kundenzufriedenheit.

Salesforce bei Unilever:
Best Practice: Einsatz von Salesforce zur kontinuierlichen Datenüberwachung.
Ergebnis: Reduzierung von Datenfehlern, präzisere Kundeninformationen.

Amazon und Produktbewertungen:
Best Practice: Nutzung von Algorithmen zur Erkennung gefälschter Produktbewertungen.
Ergebnis: Vertrauenswürdige Bewertungen, gestärktes Kundenvertrauen.

Deutsche Bank und Data Governance:
Best Practice: Etablierung einer umfassenden Data-Governance-Struktur.
Ergebnis: Konsistente, hochwertige Daten für bessere Entscheidungsfindung.
…see more
Translated
Show translation
Show original
Like
6
Data quality benefits and challenges
Data quality assessment and improvement can bring many benefits to your organization, such as increased reliability, accuracy, and usability of data, as well as improved operational efficiency and customer satisfaction. However, this process can also require a significant amount of time, effort, and resources to implement and maintain. It involves complex data sources, formats, and domains that may change frequently or unpredictably. Additionally, data quality assessment and improvement depend on the collaboration and coordination of multiple stakeholders and data owners who may have different perspectives and expectations on data quality. Lastly, technical and organizational barriers and constraints may limit or hinder the data quality assessment and improvement processes.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Susan Coelius Keplinger
CEO at Force of Nature | Performance Marketing at Scale
Copy link to contribution
Report contribution
High-quality data leads to better decision-making, but it can be resource-intensive to maintain. Balancing the costs of continuous monitoring with the benefits of accurate insights is essential.
…see more
Like
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Digital Trends
Digital Trends
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Digital Trends
No more previous content
How do you incorporate AI and machine learning into your data visualization and dashboarding?
12 contributions
How do you design and develop an accessible and inclusive ecommerce site for diverse audiences and needs?
6 contributions
What are the best tools and methods for collecting and analyzing customer feedback data?
11 contributions
What are the key metrics and tools to measure mobile e-commerce performance and conversion?
8 contributions
How do you choose the right platforms and niches for your personal brand?
23 contributions
How will edge computing affect the future of mobile networks and data consumption?
12 contributions
How can data scientists keep up with the latest digital trends and innovations in their field?
15 contributions
How do you keep up with the changing consumer preferences and behaviors in the digital landscape?
4 contributions
How can mobile e-commerce integrate with social media platforms to boost traffic and sales?
7 contributions
How do you test and optimize your data visualization and dashboarding for different audiences and contexts?
1 contribution
How do you integrate environmental sensors with other smart city solutions and platforms?
8 contributions
No more next content
See all
More relevant reading
Data Quality
How can you improve data quality when human or organizational factors cause data issues?
Data Architecture
You’re asked about your experience with data quality. What’s the best way to answer?
Data Science
What techniques can you use to ensure high-quality data meets stakeholder needs?
Data Architecture
What are the most effective data quality audit practices?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
7
8 Contributions"
Big Data Warehouse Testing,"Big Data Warehouse Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Warehouse Testing
Report this article
Nigel Shaw
Nigel Shaw
Creating A Shared Language Of Data
Published Sep 17, 2017
+ Follow
As the volume increases for your datasets then the challenges increase for QA testing of your ETL processes. Techniques that work well on small sets of data might not work so well with huge data volumes. You should also look at reusing the instrumentation that you build for QA testing so that you can use it to implement data controls to support ongoing processing. Remember your data warehouse is the end of the line for your corporate data - there are many, many upstreams systems that provide you with data - quality comes from continuing to verify your processes, not just testing them the first time you implement them.
I like to test by matching data signatures between my two sets of data. The signatures are calculated by identifying multiple subsets of attributes from the data sets I am comparing and then calculating the distribution for each set. The subsets can be a unique but you need to ensure coverage for all attributes (e.g. one set could be premium_amount, customer_type, age, state_code and another could be customer_type, shoe_size, closest_store_location). As we are grouping attributes any incorrect value will cause a mismatch between our signature data. We can implement this as control by translating the data signature into a single value. We can then compare the two sets of values by just matching the single control value.
For QA we take this one step further and take the median set of data and perform a Hash8 compare on the entire record. It is just a fail safe, but if the sets don't match then the records selected as ""median"" could well be different records.
The most important point is that QA testing does not need to be a lot of effort for just a one time task. You can build for QA and for controls at the same time.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by this author
No more previous content
Integer Bitmaps - Data Modeling
Aug 18, 2024
Adventures In Data
Jul 11, 2024
Being A Great IT Manager
Jul 8, 2024
Data Quality - The Elephant In The Room
Jun 21, 2024
No Room For Heroes
Jun 19, 2024
Data Undressed
Jun 14, 2024
Noor Inayat Khan
Jun 11, 2024
The Eternal Battle - Data vs App
Jun 11, 2024
Dashboard Trust Transparency
Jun 8, 2024
Domains, Lists And Data
Jun 3, 2024
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Test and Evaluate Data Quality Variations,"How to Test and Evaluate Data Quality Variations
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Business Administration
Data Analysis
How do you test and evaluate data quality variations?
Powered by AI and the LinkedIn community
1
Identify data quality dimensions
2
Perform data profiling
3
Apply data quality rules
4
Implement data quality actions
5
Evaluate data quality outcomes
6
Repeat data quality cycle
7
Here’s what else to consider
Data quality variations are inevitable in any data analysis project, but they can affect the validity and reliability of your results and insights. How do you test and evaluate data quality variations to ensure that your data is accurate, consistent, and complete? In this article, you will learn some practical steps and techniques to assess and improve your data quality.
Top experts in this article
Selected by the community from 73 contributions.
Learn more
Himanshu Sharma
Setting up and fixing website tracking to unlock insights. And offering expert courses & books on Digital Analytics…
View contribution
8
CA HENCY SHAH 🇮🇳
🎓FCA |
🎓M.Com (F&T) |
💡16x LinkedIn Top Voice |
🖥️Information System Auditor |
🔍Certified Forensic Accountant…
View contribution
5
See what others are saying
1
Identify data quality dimensions
The first step is to define what data quality means for your specific context and objectives. Data quality dimensions are the characteristics or attributes that measure how well your data meets your expectations and requirements. Some common data quality dimensions are accuracy, completeness, consistency, timeliness, validity, and uniqueness. You should identify the data quality dimensions that are relevant and important for your analysis, and set the criteria and standards for each dimension.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Himanshu Sharma
Setting up and fixing website tracking to unlock insights. And offering expert courses & books on Digital Analytics, GA4 and BigQuery.
Copy link to contribution
Report contribution
To effectively test and evaluate data quality variations, implement a comprehensive framework that involves defining relevant quality metrics (accuracy, completeness, consistency, timeliness, uniqueness), conducting data profiling to understand data attributes, utilizing automated tools for ongoing quality checks, establishing validation rules for data entry, and performing manual data reviews for complex issues. It's crucial to analyze source systems to identify origin issues, maintain a log for tracking quality problems, engage stakeholders in improvement efforts, and adopt a continuous improvement process to update standards and practices as needed.
…see more
Like
8
CA HENCY SHAH 🇮🇳
🎓FCA |
🎓M.Com (F&T) |
💡16x LinkedIn Top Voice |
🖥️Information System Auditor |
🔍Certified Forensic Accountant |
📊Certified Concurrent Auditor |
💼Virtual CFO |
📚Researcher |
👥16K+ Family |
📈800K+ Impressions
Copy link to contribution
Report contribution
When assessing data quality variations, it's crucial to employ robust testing and evaluation methods. One effective approach involves utilizing a combination of statistical analysis, data profiling, and thorough validation processes. By systematically examining data sets for inconsistencies, anomalies, and inaccuracies, organizations can ensure the reliability and accuracy of their information assets. Additionally, implementing data governance frameworks can help establish standards and protocols for maintaining data quality over time. Overall, proactive monitoring and continuous improvement are key to mitigating risks associated with data variability and enhancing decision-making capabilities.
…see more
Like
5
Gunjika Vishwanath Misra (She/ Her) 🚀
On a mission to help every woman & girl become financially literate, secure & independentI National President-Home & Security Council, WICCI| Board Member, 7Even Consultancy| 20X LI TOP VOICE| TOP Person Ambassador
Copy link to contribution
Report contribution
Identifying data quality dimensions for testing and evaluation involves a multi-faceted approach. Firstly, comprehensively understanding the data's purpose and context is crucial. Next, consider dimensions like accuracy, completeness, consistency, timeliness, and relevancy. Incorporate stakeholders' perspectives and industry standards for a holistic assessment. Employ advanced analytics and visualization techniques to unearth subtle variations. Lastly, iteratively refine the evaluation framework to adapt to evolving data landscapes, ensuring continuous improvement in data quality assurance processes.
…see more
Like
3
Muhammed Atef
Head of Digital at AlAin News
Copy link to contribution
Report contribution
In my experience, data quality is foundational for reliable analytics. Key dimensions include:

Accuracy: Data must accurately represent reality. Regular audits ensure alignment with real-world values.
Consistency: Uniformity across datasets prevents conflicts, achieved through standardization protocols.
Completeness: Missing data undermines analysis. Implementing completeness checks identifies gaps early.
Reliability: Data should be dependable over time. Establishing a version control system helps track changes and maintain integrity.
Timeliness: Current data is crucial for relevance. Setting up automated refresh cycles ensures data remains up-to-date.
…see more
Like
2
Abiy Dema
Data Engineer at Decathlon │ Sharing Insights on Data & Software Engineering
Copy link to contribution
Report contribution
Testing and evaluating data quality involves several steps: identify quality aspects, study the data, apply checks, take actions to improve, review outcomes, and repeat for ongoing improvement.
…see more
Like
2
Ayesha Shafique
Senior Data Scientist ( AI & ML ) | NLP & LLM Expert
Copy link to contribution
Report contribution
Profile your data: Use tools to analyze statistics, data types, missing values, outliers, and lineage across sources and systems.
Apply data quality rules: Define standards for validity, accuracy, completeness, and consistency, then identify and measure variations against these rules.
Evaluate outcomes: Track data quality scores, reports, and dashboards to assess the impact of your actions and compare data quality before and after improvement efforts.
…see more
Like
1
Sreetama Das
Data Science and Digital Intelligence | Business Intelligence and Consumer Marketing Insights | NielsenIQ | Ex-ITC | Ex-Nestlé
Copy link to contribution
Report contribution
Testing and evaluating data quality variations involve several steps. Firstly, assess completeness by checking for missing values. Then, examine accuracy through validation against trusted sources or expert judgment. Consistency is tested by ensuring uniform formatting and consistent values across datasets. Timeliness involves verifying data currency and relevance to the analysis timeframe. Finally, relevancy is gauged by assessing the data's alignment with project objectives and user needs. Regular audits and feedback loops help maintain and improve data quality over time.
…see more
Like
1
Sule Ojochide, BIDA™, FMVA®
Data Analyst || Business Intelligence Analyst || LinkedIn Top Voice || McKinsey Forward Alumni
Copy link to contribution
Report contribution
Analyzing data quality variations requires using quantitative metrics, qualitative assessments, and domain expertise to identify inconsistencies and inaccuracies within datasets
…see more
Like
1
Aislan G.
Manager of Data-Oriented Management Initiatives and Artificial Intelligence at Petrobras | Data Analysis
Copy link to contribution
Report contribution
Para assegurar a qualidade dos dados em sua análise, é essencial identificar as dimensões de qualidade relevantes e estabelecer critérios e padrões para cada dimensão. Além disso, é necessário coletar dados de alta qualidade, realizar limpeza, implementar um monitoramento contínuo, documentar todas as ações, envolver a equipe, buscar melhorias contínuas e manter comunicação transparente com todas as partes interessadas. Seguir essas etapas garantirá que os dados atendam às expectativas e requisitos estabelecidos, promovendo uma análise de qualidade.
…see more
Translated
Show translation
Show original
Like
2
Perform data profiling
The next step is to perform data profiling, which is the process of examining your data sources and collecting metadata and statistics about your data. Data profiling helps you understand the structure, content, and quality of your data, and identify any potential issues or anomalies. You can use various tools and methods to perform data profiling, such as descriptive statistics, frequency analysis, data type analysis, null analysis, outlier analysis, and data lineage analysis. Data profiling can help you discover data quality variations across different data sources, formats, and systems.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Erick Alexander Torres Prado
14K | IT Management | DBA | ERP | BI | AI | Python | Clear Prompts | BPM | Dev
Copy link to contribution
Report contribution
Performing data profiling is a crucial step in prioritizing data quality, involving a detailed examination of data sources to gather metadata and statistics. This process aids in understanding the data's structure, content, and quality, while identifying potential issues or anomalies. Techniques such as descriptive statistics, frequency analysis, and outlier analysis are employed, alongside data type and null analysis, to uncover variations in data quality across diverse sources, formats, and systems. Data profiling provides a foundational understanding necessary for addressing data quality effectively, ensuring informed decision-making and efficient data management.
…see more
Like
2
Alina Koryak
Data Analyst, Engineering Support Specialist at Honeywell Aerospace
Copy link to contribution
Report contribution
Performing data profiling is crucial for understanding data quality variations. By examining metadata and statistics, you gain insights into data structure, content, and anomalies. Tools like descriptive statistics and frequency analysis aid in detecting variations across sources and formats.
During a recent project, we conducted data profiling on a dataset from multiple sources. Through descriptive statistics and frequency analysis, we discovered inconsistencies in data formats and missing values. This revealed data quality variations across sources, prompting us to implement cleansing processes and standardization techniques. By addressing these variations, we ensured the accuracy and reliability of our analysis results.
…see more
Like
2
Aislan G.
Manager of Data-Oriented Management Initiatives and Artificial Intelligence at Petrobras | Data Analysis
Copy link to contribution
Report contribution
Para a execução eficiente da criação de perfil de dados, é recomendável utilizar uma abordagem sistêmica, combinando ferramentas estatísticas, análise de frequência, análise de tipos de dados, análise nula, detecção de outliers e análise de linhagem de dados. Essa abordagem abrangente permitirá uma compreensão detalhada da estrutura, conteúdo e qualidade dos dados, identificando potenciais problemas e anomalias em diferentes fontes, formatos e sistemas de dados. A criação de perfil de dados se torna uma ferramenta essencial para uma análise de dados sólida e confiável, proporcionando insights valiosos para tomada de decisões informadas.
…see more
Translated
Show translation
Show original
Like
1
Hazal Sami - Ghazal AlAksah
Public Health Information Automation Officer @ WHO EMRO | Information Management Systems | MBA
Copy link to contribution
Report contribution
The most crucial step in data profiling is conducting quality checks. These checks include identifying missing or null values in critical fields such as customer email addresses, which are crucial for any marketing campaign. Additionally, it's important to check for and quantify duplicate records, which may indicate multiple entries for the same customer. Finally, outliers in numerical fields such as unusually high or low ages or purchase amounts should be detected to identify any potential data entry errors.
…see more
Like
1
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Now that you know what you're looking for, it's time to get your hands dirty (figuratively, of course!). Data profiling involves analyzing your data to understand its characteristics and potential issues. This is where you gather statistics like missing values, outliers, and data types.

Imagine data profiling like taking an X-ray of your data. You see the internal structure, identify any fractures (missing values), and even spot hidden gems (valuable insights).
…see more
Like
Erick Alexander Torres Prado
14K | IT Management | DBA | ERP | BI | AI | Python | Clear Prompts | BPM | Dev
Copy link to contribution
Report contribution
Data profiling is a critical step in assessing data quality, where you examine your data sources to collect metadata and statistical information. This process reveals your data's structure, content, and current quality, highlighting any issues or anomalies that may exist. Techniques include descriptive statistics to summarize data, frequency analysis to identify common values, data type analysis to ensure data is stored in appropriate formats, null analysis to find missing values, outlier analysis to detect data points that deviate significantly from the norm, and data lineage analysis to track the source and evolution of data.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Imagine your data as a detective story – you need to know every detail to crack the case! Data profiling is your Sherlock Holmes, meticulously examining every nook and cranny of your data to understand its characteristics, anomalies, and patterns.

According to recent studies, companies that actively perform data profiling experience a 20% increase in data accuracy, leading to more reliable insights and better decision-making.
…see more
Like
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Data profiling is a critical step in ensuring data quality, which is paramount for any data analyst or BI developer. By thoroughly examining data sources, we can uncover inconsistencies and anomalies that might otherwise skew analysis and dashboards. As a data storyteller, it's essential to ensure the integrity of the data before crafting a narrative. This process not only helps in maintaining the accuracy of BI outputs but also in building trust with stakeholders who rely on this data for strategic decisions.
…see more
Like
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Data profiling is a critical step in ensuring data quality, which is the cornerstone of reliable analytics. As a Data Analyst and BI Developer, I've found that thorough profiling not only uncovers inconsistencies and errors but also provides a deeper understanding of data patterns and relationships. This understanding is vital for crafting accurate data stories and making strategic business decisions. Employing tools like R for statistical analysis and Power BI for visualizations enhances the profiling process, allowing for a more nuanced exploration of data quality dimensions.
…see more
Like
3
Apply data quality rules
The third step is to apply data quality rules, which are the rules or conditions that specify how your data should conform to your data quality dimensions. Data quality rules can be defined based on your business rules, data standards, data policies, or data validation rules. You can use data quality rules to check, monitor, and report on your data quality variations, and identify any data quality issues or errors. You can also use data quality rules to automate data quality checks and alerts, and integrate them with your data pipelines or workflows.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Erick Alexander Torres Prado
14K | IT Management | DBA | ERP | BI | AI | Python | Clear Prompts | BPM | Dev
Copy link to contribution
Report contribution
Applying data quality rules involves establishing specific criteria that your data must meet to align with your identified data quality dimensions, such as accuracy, completeness, and consistency. These rules are grounded in your organization's business requirements, industry standards, and validation protocols. By implementing these rules, you can systematically evaluate your data for discrepancies, inaccuracies, or deviations from expected norms. Data quality rules enable continuous monitoring and reporting on data quality, facilitating the identification and rectification of issues.
…see more
Like
3
Aislan G.
Manager of Data-Oriented Management Initiatives and Artificial Intelligence at Petrobras | Data Analysis
Copy link to contribution
Report contribution
A aplicação das regras de qualidade de dados envolve estabelecer regras específicas para cada dimensão de qualidade e implementar um processo contínuo de verificação e monitoramento. Essas regras podem ser baseadas em regras de negócios, padrões de dados ou políticas de dados e são essenciais para garantir a conformidade dos dados com as expectativas e requisitos estabelecidos. Além disso, a automação de verificações e alertas de qualidade de dados é importante, assim como a documentação e a comunicação regular dos resultados. Isso contribui para a manutenção de dados confiáveis e de alta qualidade em todo o processo de análise e tomada de decisões.
…see more
Translated
Show translation
Show original
Like
1
Erick Alexander Torres Prado
14K | IT Management | DBA | ERP | BI | AI | Python | Clear Prompts | BPM | Dev
Copy link to contribution
Report contribution
Applying data quality rules is crucial for maintaining high data standards. These rules, rooted in business requirements and data policies, dictate the conditions that data must satisfy across dimensions like accuracy and completeness. By implementing these rules, organizations can automate the monitoring and validation of data quality, identifying and addressing variations or errors efficiently. This integration into data pipelines and workflows ensures continuous oversight of data integrity, significantly enhancing data reliability and supporting more informed decision-making processes.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
With your data profile in hand, it's time to establish clear expectations. Define data quality rules that act like guidelines for your data. These rules could specify acceptable ranges for values, required formats, or even expected update frequencies.

Think of data quality rules like traffic laws for your data. They ensure everyone's on the same page, preventing data chaos and ensuring smooth analysis journeys.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Let's talk rules – not the boring kind, but the game-changers for your data! Implementing data quality rules is like having a trusted referee on the field, ensuring fair play.

Statistics reveal that organizations enforcing robust data quality rules witness a 25% reduction in errors, ultimately saving time and resources. It's not just about having rules; it's about having the right rules!
…see more
Like
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Data quality rules are the linchpin in ensuring that the insights derived from data analysis are reliable and actionable. As a Data Analyst and BI Developer, I've seen firsthand how applying these rules can prevent costly mistakes and enhance the credibility of data-driven decisions. By automating checks and integrating them into workflows, we not only streamline the process but also enable real-time monitoring and correction, which is crucial in dynamic business environments. This proactive approach to data management underpins robust business analysis and development strategies, ensuring that data storytelling is based on a foundation of quality and trust.
…see more
Like
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
As a seasoned data analyst and BI developer, I recognize the critical importance of applying robust data quality rules. These rules act as the gatekeepers of data integrity, ensuring that the data we analyze is accurate and reliable. By automating quality checks, we not only streamline the validation process but also embed a culture of quality within the data lifecycle. This proactive approach is key to maintaining high data standards, which in turn supports the extraction of meaningful insights that can drive business development and inform strategic decisions. Integrating these rules within data workflows is essential for real-time quality monitoring, which is crucial in today's fast-paced digital environment.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Time to play the referee for your data! ⚽ Implement data quality rules to keep things in check. Set up constraints, validations, and transformations. The Harvard Business Review (source: hbr.org) highlights that organizations with well-defined data quality rules experience 40% more success in their analytics initiatives. That's a winning strategy, don't you think?
…see more
Like
Víctor Silva
Financeiro | Comercial | Auditoria | Análise de Dados | ALLOS | HELLOO
Copy link to contribution
Report contribution
Applying data quality rules is the third step, involving implementing conditions or regulations to ensure data compliance with predefined quality dimensions. These rules, based on business requirements, data standards, or validation rules, enable verification, monitoring, and reporting of data quality variations, as well as automation of checks and alerts integrated into data pipelines or workflows for efficient data quality management.
…see more
Like
4
Implement data quality actions
The fourth step is to implement data quality actions, which are the actions or processes that address and resolve your data quality issues or errors. Data quality actions can include data cleansing, data transformation, data enrichment, data standardization, data deduplication, or data reconciliation. You can use various tools and techniques to implement data quality actions, such as data quality software, data wrangling tools, scripting languages, or SQL queries. Data quality actions can help you improve your data quality variations, and enhance your data usability and reliability.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Abiy Dema
Data Engineer at Decathlon │ Sharing Insights on Data & Software Engineering
Copy link to contribution
Report contribution
For instance, if dealing with inconsistent date formats in a dataset, a data quality action could involve standardizing all date values into a consistent format.
…see more
Like
3
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Data Profiling: Dive deep with descriptive statistics, frequency analysis, and outlier detection. Uncover hidden patterns and inconsistencies across sources, formats, and systems. Think of it as X-raying your data to reveal any cracks or imbalances.

Data Quality Rules: Establish clear guidelines for accuracy, completeness, and consistency. Use these rules like a measuring tape to assess if your data meets the mark.

Data Lineage: Trace the journey of your data from source to destination. This helps pinpoint where variations might creep in, like a detective tracking down the culprit behind a wobbly seesaw.
…see more
Like
1
Erick Alexander Torres Prado
14K | IT Management | DBA | ERP | BI | AI | Python | Clear Prompts | BPM | Dev
Copy link to contribution
Report contribution
Implementing data quality actions involves processes such as data cleansing, transformation, enrichment, standardization, deduplication, and reconciliation to address and resolve data quality issues. Utilizing tools like data quality software, data wrangling tools, scripting languages, and SQL queries, these actions aim to correct inaccuracies and inconsistencies in data. By improving data quality, these measures enhance the usability and reliability of data for decision-making, analysis, and reporting, ensuring that data meets established quality standards.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Now, let's turn those insights into action! Implementing data quality actions is like upgrading your data from a sedan to a sports car – faster, sleeker, and ready to conquer the analytics race.

Research suggests that companies embracing proactive data quality actions enjoy a 30% boost in overall operational efficiency. So, don't just watch your data, empower it to drive your success!
…see more
Like
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Data Quality is Essential: Ensures reliable insights for accurate decision-making.
Data Cleansing and Enrichment: Removes errors, and inconsistencies, and fills data gaps, enhancing the value of the dataset.
Technical Tools: R and SQL allow for efficient data manipulation, streamlining the preparation process.
Cross-Functional Collaboration: The blend of technical expertise (Data Engineer) and business understanding (Business Developer) is crucial for establishing a robust data foundation.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Take control like a superhero! 🦸♂️ When anomalies pop up, execute predefined actions. Fix errors, update records, or quarantine questionable data. According to the Data Governance Institute (source: datagovernance.com), organizations that implement proactive data quality actions reduce errors by a whopping 45%. Talk about swooping in to save the day!
…see more
Like
Bhargava Krishna Sreepathi, PhD, MBA
Director Data Science @ Syneos Health | Global Executive MBA | 34x LinkedIn Top Voice
Copy link to contribution
Report contribution
Implement Thresholds for Rules:
Tolerance Levels: For some rules, especially those related to data cleanliness and completeness, define tolerance levels or thresholds. This acknowledges that while perfection is ideal, it may not always be practical, and some level of imperfection can be acceptable depending on the context.

Severity Levels: Assign severity levels to the rules based on their importance to the data's integrity and the potential impact of violations. This helps in prioritizing issues that need urgent attention.

Regularly Review and Update Rules:
Adapt to Changes: Regularly review and update data quality rules to adapt to changes in data sources, structures, and usage requirements, based on changes in business needs.
…see more
Like
5
Evaluate data quality outcomes
The fifth step is to evaluate data quality outcomes, which are the outcomes or results of your data quality actions. Data quality outcomes can be measured and evaluated using various metrics and indicators, such as data quality scores, data quality reports, data quality dashboards, or data quality feedback. You can use data quality outcomes to assess the effectiveness and impact of your data quality actions, and compare your data quality before and after your actions. Data quality outcomes can help you communicate and demonstrate your data quality improvements, and support your data analysis and decision making.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Badia Alfathi, (M.Sc.)
Data/Business Analysis Engineer @Orange Telecom🍊|  Founder of Innovative Mental Health startup🧠 | IT&Data Consultant @Madaniya | DA Teaching Assistant @C1 | 🏫 MSc in ML & Data | Computer & Data science @MIT 🎓
Copy link to contribution
Report contribution
Data quality variations can be assessed through rigorous testing and evaluation methodologies. This involves comparing the expected output with the actual output derived from the data. By scrutinizing inconsistencies and discrepancies, we measure the extent of data quality variations. Techniques such as data profiling, completeness checks, and accuracy assessments help identify anomalies. Moreover, continuous monitoring and feedback loops ensure ongoing data quality improvement. Ultimately, by quantifying the loss incurred due to data quality variations, we can refine processes and enhance decision-making accuracy.
…see more
Like
2
Luis Monsalve
Logistics - DC Manager @ LBB Specialties | Google Certified Data Analyst | Lean Six Sigma
Copy link to contribution
Report contribution
Meticulously measure the outcomes!  Use data quality scores and reports as your compass. It's like carefully examining the cleared path, comparing the ""before"" and ""after"" to see how far I've come. These outcomes help me communicate the improvements I've made and ensure my analysis is built on solid ground.
…see more
Like
1
Erick Alexander Torres Prado
14K | IT Management | DBA | ERP | BI | AI | Python | Clear Prompts | BPM | Dev
Copy link to contribution
Report contribution
Evaluating data quality outcomes involves measuring the results of data quality improvements using metrics like scores, reports, dashboards, or feedback. These outcomes assess the effectiveness of actions taken to enhance data quality, allowing for a before-and-after comparison. This step is crucial for demonstrating data quality enhancements to stakeholders and supports informed decision-making and analysis.
…see more
Like
1
Anushtha Kinkar
Rank Higher, Grow Faster 📈 | AI-Powered SEO for Smarter Traffic Strategies & Real Results! | Follow for Proven Insights!
Copy link to contribution
Report contribution
Evaluating data quality outcomes is the pivotal stage in meticulous data management, involving scrutiny of profiling, cleansing, and validation results to meet predefined standards and align with analytical objectives. By assessing completeness, accuracy, consistency, and timeliness, analysts gain insights into dataset reliability and the impact on subsequent analyses. Continuous monitoring ensures data quality maintenance over time, guiding the refinement of processes for trustworthy and effective analytical endeavors.
…see more
Like
1
Maya Salameh, M.A.
J.D. Candidate at UCLA Law
Copy link to contribution
Report contribution
These outcomes, the direct results of your data quality efforts, can be quantified and evaluated through diverse metrics like data quality scores, reports, dashboards, or even user feedback. These metrics serve as a yardstick to gauge the effectiveness and impact of your data quality actions, allowing you to make comparisons before and after implementation. The insights gleaned from data quality outcomes not only bolster your ability to communicate and showcase improvements but also fortify the foundation for more robust data analysis and informed decision-making.
…see more
Like
1
Sachin M
Analytics Engineer at Deloitte 🟢 - AI & Data | Power BI | DAX | SQL | Python | PySpark | ADB | ADF | ADLS
Copy link to contribution
Report contribution
Analyze data quality results with measures such as dashboards, reports, comments, and data quality scores. To evaluate efficacy and facilitate decision-making, compare data quality steps taken before and after.
…see more
Like
1
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Data Cleaning: Address missing values, outliers, and inconsistencies. Imagine patching up any holes in your seesaw to ensure a smooth ride.

Data Standardization: Ensure consistent formats and definitions across your data. Think of it as using the same paintbrush and color on both sides of the seesaw for a uniform look.

Data Enrichment: Add valuable information from external sources to enhance your data's weight and accuracy. Imagine adding weights to one side of the seesaw to balance it out.
…see more
Like
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Evaluating Data Quality for Reliable Insights

Quantitative Metrics: Employ numerical measures to objectively track data quality improvements (e.g., completeness, accuracy, consistency).
Qualitative Feedback: Gather insights from users to gauge the practical relevance of data quality enhancements in the business context.
Data Visualization Tools: Utilize Power BI or Tableau to create dynamic dashboards that showcase data quality transformations.
Stakeholder Clarity: Visualizations clearly demonstrate the value of data quality initiatives for informed decision-making.
…see more
Like
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
It's showtime! 🎬 Evaluate the impact of your data quality efforts. Monitor metrics, track improvements, and celebrate successes. A survey by Forbes Insights (source: forbes.com) found that companies that consistently evaluate data quality outcomes achieve a 25% increase in revenue. Now that's a data-driven ROI worth celebrating!
…see more
Like
6
Repeat data quality cycle
The final step is to repeat the data quality cycle, which is the cycle of testing and evaluating your data quality variations on a regular basis. Data quality is not a one-time activity, but a continuous process that requires constant monitoring and maintenance. You should repeat the data quality cycle periodically or whenever your data sources, formats, systems, or objectives change. Repeating the data quality cycle can help you ensure that your data quality remains high and consistent, and adapt to any changes or challenges in your data environment.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Aalok Rathod, MS, MBA
LinkedIn Top Voice | FP&A Manager | Ex- Amazon | Ex-JP Morgan | Cornell MBA
Copy link to contribution
Report contribution
Data Quality Scores: Develop metrics to track progress and assess the effectiveness of your adjustments. These scores act like a level on your seesaw, showing you if you're getting closer to perfect balance.

Data Quality Reports: Regularly generate reports to visualize trends and identify areas for improvement. Think of these reports as a snapshot of your seesaw's performance, highlighting areas that need a nudge.

Data Quality Dashboards: Create real-time dashboards to monitor data quality continuously. Imagine having a live indicator on your seesaw, constantly showing you if it's balanced or needs a quick fix.
…see more
Like
1
James Bourque
Microsoft Certified / Data Engineering / Data Integration / SQL Database Analyst / Database Administrator / Applications Manager / Solutions Architect
Copy link to contribution
Report contribution
Data quality should factor in timeliness as a dimension to ensure information is available when needed for actionable insights.
…see more
Like
1
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Data quality cycle key in the dynamic data landscape.
• Continuous adaptation to new sources and updates is crucial.
• Regularly refine the process for accurate and actionable insights.
• Quality data drives strategic decision-making.
• Effective communication of changes ensures data tells a clear story.
• A clear story is vital for business development.
…see more
Like
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Favour Gilbert-Ugwu (nèe Ekperi)
Credit Risk Analyst | Chartered Accountant | Finance Expert | Senior Academic Researcher
Copy link to contribution
Report contribution
To evaluate data quality, a data quality assessment should be conducted. Here, you clearly state the business goals. Then, assess relevant data which have been collated to check for completeness, accuracy and compliance. 

The result of this assessment should be documented and strategies for improvement implemented and monitored.
…see more
Like
1
Zia Ibn Hasan
Digital Marketer | Data Analyst | R | Content Creator | #Opentowork
Copy link to contribution
Report contribution
Data quality is an ongoing commitment, not a single event.
Iterative improvement is essential. Analyze data, implement quality rules, assess the results, and refine your approach based on what you learn.
A cyclical process ensures data remains reliable. This ongoing approach helps you maintain trustworthy data for decision-making.
Adapt to changing data landscapes. New data sources and evolving technologies can impact data quality, requiring constant vigilance and adaptation.
…see more
Like
Data Analysis
Data Analysis
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Analysis
No more previous content
What do you do if stakeholders and investors aren't grasping the insights from your data analysis?
78 contributions
Here's how you can boost your problem-solving skills in data analysis through continuous learning.
38 contributions
Here's how you can leverage data-driven insights to drive innovation in your organization.
49 contributions
Struggling to enhance your data analysis techniques?
79 contributions
You're struggling with failure in data analysis. How can self-reflection help you bounce back?
20 contributions
Here's how you can expand your data analysis network through online forums and communities.
29 contributions
You're facing complex data challenges. How can you apply advanced problem solving strategies like a pro?
86 contributions
Here's how you can successfully scale a data analysis project from a small team to a larger team.
41 contributions
What do you do if work-life balance is elusive while advancing your data analysis career?
177 contributions
Here's how you can enhance your problem-solving skills as a data analyst for better decision-making.
33 contributions
Struggling with complex data sets in Data Analysis?
17 contributions
Here's how you can discuss salary negotiation with your current employer as a data analyst.
40 contributions
You're aiming to excel in data analysis. What strategies will set you apart as a thought leader?
17 contributions
You're struggling to connect with stakeholders. How can you use emotional intelligence to bridge the gap?
12 contributions
Here's how you can smoothly transition from data analysis to freelance or consulting roles.
99 contributions
No more next content
See all
Explore Other Skills
Business Strategy
Executive Management
Business Management
Product Management
Business Development
Business Intelligence (BI)
Project Management
Consulting
Business Analysis
Entrepreneurship
Show more
Show less
More relevant reading
Data Science
How do you choose the best data quality metrics?
Data Analytics
You’re starting a data profiling project. What software should you use to ensure quality?
Data Management
How can you conduct a data quality audit and report the results?
Statistics
How can you transform raw data into a usable format?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
2
73 Contributions"
Navigating the Complexities: Challenges in Big Data Testing,"Navigating the Complexities: Challenges in Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Dive into the intricacies of Big Data testing and discover strategies to conquer its challenges.
Navigating the Complexities: Challenges in Big Data Testing
Report this article
Muhammad Usman - ISTQB®CTFL
Muhammad Usman - ISTQB®CTFL
ISTQB Certified Tester | SQA Architect | Web, Mobile & API | Automation | Manual | Load | Security | CI/CD | AWS | Microsoft Azure
Published Mar 7, 2024
+ Follow
Introduction
In the digital age, harnessing the power of Big Data has become paramount for organizations striving to gain insights, drive innovation, and stay competitive in today's dynamic marketplace. However, with the unprecedented growth in data volume, velocity, and variety, testing Big Data systems presents a unique set of challenges that demand careful consideration and strategic approaches. In this article, we'll delve into the specific challenges encountered when testing Big Data systems, including scalability, data variety, data quality, and complex data processing algorithms.
Scalability
Perhaps one of the most significant challenges in Big Data testing is ensuring scalability—the ability of a system to handle increasing volumes of data and user requests without sacrificing performance or reliability. Traditional testing methodologies may fall short when it comes to simulating the massive data loads and complex processing requirements of Big Data systems. Test environments must accurately replicate production conditions to assess system scalability accurately.
Data Variety
Big Data systems often deal with diverse types of data, including structured, semi-structured, and unstructured data from various sources such as social media, IoT devices, and sensor networks. Testing the compatibility, interoperability, and integrity of different data formats poses significant challenges for testers. Ensuring that data ingestion, processing, and analytics functions seamlessly handle diverse data types is essential for the success of Big Data projects.
Data Quality
The quality of data is paramount for deriving meaningful insights and making informed business decisions. However, maintaining data quality in Big Data systems presents a formidable challenge due to the sheer volume, velocity, and variability of data. Testing data quality involves identifying and addressing issues such as missing values, duplicates, inconsistencies, and inaccuracies that can compromise the reliability and trustworthiness of analytics results.
Complex Data Processing Algorithms
Big Data systems often employ sophisticated data processing algorithms, including machine learning, artificial intelligence, and predictive analytics, to extract actionable insights from large datasets. Testing these complex algorithms requires specialized expertise and tools to validate their accuracy, robustness, and performance under various scenarios. Ensuring that algorithms produce reliable results and mitigate biases or errors is essential for building trust in Big Data analytics.
Solution
Addressing these challenges requires a strategic and comprehensive approach to Big Data testing. Organizations must invest in the right tools, technologies, and methodologies to overcome scalability limitations, handle data variety effectively, ensure data quality, and validate the accuracy of complex data processing algorithms. Collaboration between data engineers, data scientists, testers, and domain experts is critical for identifying potential issues early in the development lifecycle and implementing robust testing strategies.
Furthermore, adopting agile and iterative testing practices can help organizations adapt to evolving requirements and mitigate risks associated with Big Data projects. Continuous testing, automation, and DevOps principles can streamline testing workflows, accelerate time-to-market, and ensure the reliability and performance of Big Data systems in production environments.
Conclusion
While testing Big Data systems presents inherent challenges, organizations that prioritize scalability, data variety, data quality, and complex data processing algorithms can unlock the full potential of their data assets and gain a competitive edge in today's data-driven world. By embracing innovation, collaboration, and best practices in Big Data testing, organizations can navigate the complexities and harness the transformative power of Big Data to drive business success.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
11
To view or add a comment,
sign in
More articles by this author
No more previous content
Optimal Methods and Metrics for LLM Testing
Dec 10, 2024
QA Outsourcing: The Secret Ingredient to Scaling Your Business Successfully
Dec 5, 2024
The Impact of Quantum Computing on Software Testing
Nov 27, 2024
Mastering Remote QA Leadership: Strategies for Leading Distributed Testing Teams
Nov 18, 2024
Shift-Right Testing: How Testing in Production Improves Software Reliability and User Experience
Oct 23, 2024
How Machine Learning Algorithms Can Optimize Test Coverage
Oct 21, 2024
Addressing the Challenge of Flaky Tests in Software Development
Oct 1, 2024
The Future of Mobile Testing in 2025: Trends, Challenges, and Opportunities
Sep 19, 2024
Overcoming Common Challenges of Using Appium for iOS Testing: A Comprehensive Guide
Sep 10, 2024
Chaos Engineering for QA: Testing Beyond the Ordinary
Sep 3, 2024
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Take Tom Davenport’s Big Data Challenge,"Take Tom Davenport’s Big Data Challenge
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Take Tom Davenport’s Big Data Challenge
Report this article
Tom Davenport
Tom Davenport
Published Sep 15, 2016
+ Follow
The
following quiz
will test your knowledge about big data. It consists of 10 questions and should take you less than 5 minutes to complete.
The term “big data” burst upon the scene around 2012 and has been popular ever since. The term is about not only a new type of information, but also the technologies that support it and the people who work with it.
The precise meaning of the term is somewhat difficult to pin down and is always evolving. But anyone who has worked extensively with big data should be able to answer the following questions, which seek to determine your level of expertise with a subject that is in a perpetual state of change.
This is a challenging quiz and most participants will not have experience in every area covered. Answer the questions you know, and guess at those you don’t.
Take this test now to find out how you stack up
.
At the end of the quiz on the screen where you’ve provided personal information, scroll down to see the answers.
* This quiz was posted by
DataInformed
on June 27, 2016.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
72
2 Comments
Swaminathan Aiyer (Swami)
Director - Data & AI - India & South Asia @ Microsoft
8y
Report this comment
B has uv toxic hb bttbmf Khyber?5)
Xxxxvvy!? Her. Bn very t Ruth h g g
Like
Reply
1 Reaction
Stephane Hamel
Strategist in Data Governance, Privacy, Ethics, shaping the future of Digital Marketing & Analytics. Consultant, Educator and Speaker.
8y
Report this comment
Funny, interesting and instructive!
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Was Your Information Swimming Naked During The Pandemic?
Apr 7, 2022
How HR Leaders Are Preparing for the AI-Enabled Workforce
May 1, 2021
Deployment as a Critical Business Data Science Discipline
Mar 14, 2021
What is a minimum viable AI product?
Jan 26, 2021
BizOps--Aligning Business and IT in Automated Decision-making
Jun 16, 2020
The Future Of Work Now: Morgan Stanley’s Financial Advisors And The Next Best Action System
May 21, 2020
Finally, AI for Hiring AI Talent
Mar 3, 2020
State of AI in the Enterprise, 2nd Edition
Nov 13, 2018
It’s time to modernize your big data management techniques
Sep 25, 2018
AI-Driven Leadership
Sep 20, 2018
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Why is Big Data Testing Important for the Healthcare Industry?,"Why is Big Data Testing Important for the Healthcare Industry?
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Why is Big Data Testing Important for the Healthcare Industry?
Report this article
Debjani Goswami
Debjani Goswami
Marketing Manager | Marketing Strategy, Sales Enablement
Published Jun 21, 2022
+ Follow
Big data analytics is enabling healthcare providers and insurers to deliver greater patient satisfaction through life-saving care. The healthcare industry produces massive amounts of data for patient care, record keeping and compliance & regulatory requirements. Thus, big data can support multiple medical and healthcare functions including health management, disease monitoring and clinical decision support, among others.
However, with such massive amounts of data to process, big data systems may be prone to defects that can be identified only through
advanced big data testing
. Here's why big data service providers in the healthcare industries, need to employ quality engineering experts to ensure their systems are flawless as lives depend on the same.
Data-driven healthcare
Healthcare data analysis opportunities are increasing with growing healthcare data. It can help solve several raging healthcare issues including healthcare prediction, disease prevention and enable personalized healthcare support.
It can also help healthcare organizations become operationally efficient, manage expenses better and deliver services in a targeted manner. It can help patients maintain their health data through easily accessible applications and wearables. For instance, connected glucose monitors track blood sugar levels and suggest required medication based on the collected data.
Patients get better treatment results when they are more engaged in the treatment. Smart use of sensors also support healthcare organizations deliver more efficient digital therapeutic solutions.
Challenges for healthcare data sharing
Data related to a person’s physical or mental health is protected under the General Data Protection Regulation (GDPR). To avoid any data breach and illegal data use, healthcare organizations need to comply strictly with data sharing rules. Sometimes, it affects the data flow within the organization.
End-of-life (EOL) software and infrastructure provides a healthcare data security risk as vendors discontinue support for your IT systems, including vital security patches. While facing the situation and purchasing a brand-new server can be tough on the budget, it’s easier than dealing with the fallout of a data breach.
Across the nation, healthcare providers are grappling with how to incorporate state-of-the-art technologies into their practices without violating HIPAA or putting patients at risk. One solution  against the growing threat is data encryption.
Big data testing in healthcare
Avoiding big data testing can become counterproductive for the healthcare business. Robust big data testing can ensure legitimate data usage under HIPAA compliance and keep healthcare data accessible and secure from hackers.
Big data testing of healthcare applications is complex. It is executed at three levels including data collection, integration and deployment. Test engineers need to keep a keen eye on the variety, volume, and velocity of big data to ensure flawless and secure user experience.
Big data scalability
The amount of healthcare data stored in databases is increasing at a rapid pace. Quality assurance (QA) teams need to validate the data on a regular basis to ensure it is accurate and relevant to the business objective of the healthcare organization.
Test engineers can run automated scripts to identify and fix any flaws in big applications.
Abrupt increase in workload can also hamper accessibility, processing and networking capability. By leveraging Cluster and data partitioning techniques, performance bottlenecks of big data can be identified and fixed. It will help you achieve real-time scalability without any major performance bottlenecks.
Efficient health monitoring
Big data testing can enable healthcare professionals to easily track vital information of multiple patients and users. It also helps patients to easily track common stats including glucose level, heart rate and sleep rate. It also alerts the patient if any medical attention is required immediately.
Optimized operational costs
Big data can leverage predictive analysis to help you forecast future medical fees, operational resources’ requirements and expected number of patients. Healthcare organizations can leverage this information to become future-ready by planning the resources cost-efficiently and managing the surge in patients in a better manner.
Telehealth
As an aftermath of COVID, Telehealth is more personalized and convenient for many patients as it is almost synonymous with doctor’s home visits. Big data can make the interaction between patients and telehealth providers more effective.
Big data can enable healthcare providers to link up healthcare apps to monitor and track patient health data. Data points can be transmitted directly through wearable apps to track multiple vitals including breath rate, heart rate, diet and more.
Types of big data testing
Every healthcare software enabled must undergo
proper big data testing
. Big data systems are also vulnerable to defects and flaws, which need to be monitored and inspected to rule out chances of errors. Here are the major big data testing types:
Architecture testing:
This type of testing ensures that the processing of data is proper and meets the business requirements. And, if the architecture is improper then it might result in performance degradation due to which the processing of data may be interrupted, and loss of data may occur. Hence, architectural testing is vital to ensure the success of your big data project.
Database testing:
As the name suggests, this testing process typically involves the validation of data gathered from various databases. It verifies the data extracted from cloud sources or local databases that are correct and proper.
Performance testing:
It is for checking the load time and processing speed to ensure stable performance of big data applications. This testing type helps check the velocity of the data coming from various databases and data warehouses as an output known as IOPS (Input Output Per Second). Further, it validates the core big data application functionality under heavy load by running different test scenarios.
Functional testing:
Big data applications encompassing operational and analytical parts involve thorough functional testing at the API level. It includes tests for all the sub-components, scripts, programs, & tools used for storing or loading and processing applications.
Wrapping Up
As healthcare organizations expedite the adoption of digital transformation across the world, the need to become data compliant and data-driven is becoming business-critical. Big data testing can help healthcare organizations make sense of heaps of data generated on a regular basis efficiently. It can also help healthcare providers become more customer-centric and future-ready through predictive analytics and forecasting. Finally, big data testing can keep healthcare organizations compliant and immune from expensive lawsuits.
Talk to experts at Qualitest
to make the first move towards robust big data testing.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
15
2 Comments
Arijit Bhaduri
Chief Strategy Officer @LakeB2B
2y
Report this comment
Good read! Keep posting Debjani
Like
Reply
1 Reaction
2 Reactions
Rakesh Dwivedi
--
2y
Report this comment
Helpful! This will
Like
Reply
2 Reactions
3 Reactions
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Is Your Telehealth Application Good Enough for Patients/Doctors to Trust You?
Aug 19, 2022
5g Rollout: How Quality Engineering Helps Telecom Businesses Adapt to this New Phenomenon?
Aug 2, 2022
Journey From Big Data to Smart Data: How Big Data Testing Can Help You?
Jul 25, 2022
Is your Digital Billing System Tested for Flawless Performance?
Jul 8, 2022
Why does Your Video Game Need Intelligent Quality Assurance Services?
Jun 13, 2022
Are you Quality-proofing your Video based Learning Platform?
Jun 3, 2022
The Right Performance Testing Approach for Virtual Assistants
May 27, 2022
All about Mobile Application Testing - Know the Fundamentals
May 13, 2022
The Need for Microservices Testing in Integrated IoT Systems
May 6, 2022
How to Solve the Test Data Bottleneck?
Apr 29, 2022
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data Testing,"Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing
Report this article
Bukola Olawoyin
Bukola Olawoyin
FinTech, Machine Learning, AI, Robotics Process Automation & Cybersecurity | Doctoral Candidate in Business Administration (DBA) with expertise in Financial and Operations Management
Published Jul 24, 2015
+ Follow
Hello friends,
Is anyone currently testing or have recently tested Big Data...?
Especially in the SAP area.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
3
2 Comments
Alexander W. Higgins
Managing Director at Broadlight Global - enhancing product development, improving user experience, scaling high-performing teams🚀
9y
Report this comment
Hi, I am holding an event down in Brighton soon and looking for a speaker who has experience of testing with Big Data testing. Anyone interested please ping me on LinkedIn
Like
Reply
1 Reaction
Olatunji (Ola) Ajibode
Senior Data Engineer | Cloud & Big Data Specialist | AI Enthusiast
9y
Report this comment
That's a broad question.   What tools and environment are you using in your BigData (dev) environment e.g. Apache Hadoop/Spark/Hive/Pig or Apache Hadoop/Kafka/ZooKeeper/Storm/Hive?  I have been involved in some BigData testing and it was interesting and brain-cracking!
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Transforming Agriculture: Applications of AI in Farming
Dec 30, 2023
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Wim Hardyns on LinkedIn: Kick-off Big Data Policing field test,"Wim Hardyns on LinkedIn: Kick-off Big Data Policing field test
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Wim Hardyns’ Post
Wim Hardyns
Professor of Criminology & Security
9mo
Report this post
Ready for one of the biggest field tests in Europe so far. Thanks to all participating police zones in Belgium!

In the coming years, we will also start new tests in other European countries.

If you want to know more and continue to follow us: follow our new LinkedIn page
BIGDATPOL
to stay informed about future updates and events.

With our research team:
Thom Snaphaan
Marlies Sas
Robin Khalfa
Charlotte Vandenbrande
Naomi T.
Inge Claessens
Arne Dormaels
i4S - Smart Solutions for Secure Societies
UGent TechTransfer
European Research Council (ERC)
BIGDATPOL
588 followers
9mo
Kick-off Big Data Policing field test
BIGDATPOL on LinkedIn
43
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
International Journal of Government Auditing
3,731 followers
1mo
Edited
Report this post
In news from
#InsideINTOSAI
, the
OLACEFS
General Assembly took place in October 2024, highlighting two key technical themes: ""Data science and artificial intelligence as a tool for development and transformation for Supreme Audit Institutions"", and ""Educational Challenges and SAIs: Contribution of Supreme Audit Institutions and OLACEFS to develop equitable and quality education in Latin America and the Caribbean"". Read the full article here:
https://buff.ly/48Ox0TL
14
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Digital Science
17,671 followers
2w
Report this post
It's coming soon! Register to receive our special report: ""The State Of Open Data 2024: Bridging policy & practice in data sharing.""

In this video, hear from
Mark Hahnel
(Digital Science/Figshare) and Graham Smith (Springer Nature) on the transformation of open research.

🔗 Sign up now to receive your copy when it's available:
https://ow.ly/aZmV50UgxZZ
The State of Open Data is a collaboration between
Digital Science
,
Figshare
and
Springer Nature
.
#StateOfOpenData
#OpenResearch
#OpenData
#DataSharing
#OpenScience
#ResearchTransformation
…more
12
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Nadine Kwapil
Machine Learning Specialist at CI Development GmbH | 2+ years experience as a Machine Learning Professional with a background in Cognitive Science
2mo
Report this post
I attended the
#DataEngBytes
Conference in Sydney last week and the event provided great insights into the latest trends and tools in data engineering.

A key highlight for me was
Juan Sequeda
's talk on Increasing the LLM Accuracy for Question Answering on Structured Data: Knowledge Graphs to the Rescue. It's no secret that Knowledge Graphs are crucial for improving the accuracy of LLMs for question answering. But by integrating specifically the Ontology-based Query Checker and LLM Repair approach, we can improve the accuracy even further ensuring more reliable and precise outputs. Anyone working with LLMs should seriously consider incorporating these tools to enhance model performance. I highly recommend reading Juan's paper „INCREASING THE LLM ACCURACY FOR QUESTION ANSWERING: ONTOLOGIES TO THE RESCUE!“ (2024) -
a must-read for anyone looking to improve accuracy in LLMs. Thank you Juan for this valuable key note at DataEngBytes!

And a big thank you to the organisers for putting together such an informative and engaging event! It was a pleasure to connect with new people in the field, exchange ideas, and have some great conversations.
#conference
#dataengineering
#LLMs
#GenAI
#DeepLearning
#MachineLearning
#DataScience
#sydney
21
6 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Benjamin Molina
6mo
Report this post
Dive into data lakes with my 9-minute video on their crucial role in the SCENE project. Find out how we're advancing data technology!
SCENE EU Project
446 followers
6mo
🎥 Check out this video by
Benjamin Molina
from our partner,
Universitat Politècnica de València (UPV)
on the concept of data lakes and their use in the SCENE project.

🔍 What are Data Lakes? Central repositories for all types of data, allowing for comprehensive analysis across various domains. In film-making, data lakes help in audience building, recommendation systems, and more. 
📊 Why is this Important? In SCENE, the data lake supports different tools within the project, forming a key part of the data layer.

📽️ Watch the video here:
https://bit.ly/45nPaKH
Centre for Research & Technology Hellas (CERTH)
|
Hypertech SA
|
LINKS Foundation
|
DigitalTwin Technology GmbH
|
MOG Technologies
|
Fraunhofer-Gesellschaft
|
Aristotle University of Thessaloniki (AUTH)
|
White Research
|
Universitat Politècnica de València (UPV)
|
Athens Development and Destination Management Agency
|
Athens Film Office
|
Fundación Épica La Fura dels Baus
|
Green Olive Films
|
CETMA
Data Lakes in SCENE
https://www.youtube.com/
6
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
SciY
1,126 followers
2mo
Report this post
What is the future of digitalization and the automation of scientific data? What inspired the founding of SciY? What unique synergies does SciY foster? Let's find out in this short interview with
Santi Dominguez
👇

Learn more:
https://lnkd.in/gQ5m4Djj
#SciY
#DataManagement
#SoftwareSolutions
…more
Discover SciY with Santi Dominguez
36
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Kate Longworth
CEO & Founder Gaia Learning 🌏 | Online education improving engagement, re-integration & outcomes for #neurodivergent children 💫
9mo
Edited
Report this post
Ones to watch! 👀
Gaia Learning
have been identified as Ones to Watch in the Liverpool
Tech Climbers
event 2024! 

🤩🙌
#technology
#scalingbusiness
#edtech
#education
#neurodivergent
#data
#liverpool
…more
43
4 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Mestrelab Research S.L.
4,523 followers
2mo
Report this post
Curious about the future of scientific data digitalization and automation? 🌐 Hear from Santi Dominguez on what inspired the creation of SciY and the innovative collaborations it fosters. Don’t miss this insightful interview! 👇
#Innovation
#Digitalization
#SciY
#FutureOfScience
SciY
1,126 followers
2mo
What is the future of digitalization and the automation of scientific data? What inspired the founding of SciY? What unique synergies does SciY foster? Let's find out in this short interview with
Santi Dominguez
👇

Learn more:
https://lnkd.in/gQ5m4Djj
#SciY
#DataManagement
#SoftwareSolutions
Discover SciY with Santi Dominguez
9
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Maastricht University Library
1,907 followers
1mo
Edited
Report this post
🏆 The winners of the Dutch Data Prize 2024 have been announced in The Hague! 

Yesterday, our director
dr. Claudia van Oppen
, who was the Jury chair, awarded the Dutch Data Prize 2024 to three categories of winners. The Dutch Data Prize, an initiative of Research Data Netherlands (RDNL), is a valuable recognition of researchers' contributions to their field and the principles of FAIR (Findable, Accessible, Interoperable, Reusable) data.

🔗 Find out more about the winners of the Dutch Data Prize 2024 and how they’re advancing FAIR data principles:
https://lnkd.in/eQYHdakh
#FAIR
#RDNL
#DutchDataPrize
DANS
1,599 followers
1mo
The
#RDNL
Dutch Data Prize 2024 award ceremony just started. Jury Chair
dr. Claudia van Oppen
of
Universiteit Maastricht
Library kicked-off with announcing the finalists. Stay tuned for the winners!
7
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Fernando Perales (Ph.D., MBA)
Director JOT Research Lab | International Partnership | Basketball N1 Coach | @NandoPerales
2mo
Report this post
Best event to know what's going on in the
#EU
funded projects, network and discuss about new ideas to create the near future.
#DigitalMarketing
#Research
#Innovation
#Data
#AI
#GenAI
#content
#project
enRichMyData
FAME
UPCAST: Universal Platform Components for Safe Fair Interoperable Data Exchange & Monetisation
#DataCloud
JOT Internet Media
1,011 followers
2mo
Next week,
JOT Internet Media
Director of the Research Lab
Fernando Perales (Ph.D., MBA)
will attend
#EuropeanBigDataValueForum
#eBDVForum
organized by
BDVA - Big Data Value Association
.  

Do not hesitate to contact him to discuss on-going projects and new ideas for using and developing
#GenAI
#LLMs
and analytical solutions to keep on pushing
#DigitalMarketing
to the next level.
https://lnkd.in/d3WNrbj
enRichMyData
UPCAST: Universal Platform Components for Safe Fair Interoperable Data Exchange & Monetisation
FAME
#data
#enrichment
#datamonetization
#finance
#horizonEurope
18
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Lisa McKay-Brown
Associate Professor at University of Melbourne
2mo
Report this post
Another great resource from
RaaWee K12 Solutions, Inc.
, authored by
David Heyne, PhD
and
Carolyn Gentle-Genitty, PhD
. This one ""Guess Less Know More"" is focused on using data to support attendance. Do check out the other resources in the series.
https://hubs.ly/Q02NQnsQ0
#attendance
#education
#data
#schools
17
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
1,924 followers
99 Posts
View Profile
Connect
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more industry insights
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Elevating Data Quality using open source DBT-core test framework,"Elevating Data Quality using open source DBT-core test framework
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Elevating Data Quality using open source DBT-core test framework
Report this article
Ganesan Vetriselvan
Ganesan Vetriselvan
Data & Gen AI / Cloud Data Solution Architect / Data Analytics / Snowflake Architect | Azure, AWS
Published Nov 18, 2023
+ Follow
In the realm of data analytics, one undeniable truth stands tall: the
quality of your data directly impacts the accuracy of your insights and the effectiveness of your decisions
. It's the foundation upon which data-driven organizations are built. But as data volumes grow and sources diversify, maintaining data quality becomes increasingly challenging. This is where
DBT-Core's Test Module
steps into the limelight, offering a game-changing solution.
The Data Quality Imperative
Data Quality Frameworks have long been the unsung heroes behind high-quality data. They ensure data accuracy, consistency, and reliability. Yet, as data pipelines become more complex and dynamic, traditional approaches to data quality are proving insufficient.
Enter DBT-Core, an open-source tool for orchestrating data transformations. DBT-Core's Test Module provides a cutting-edge solution to data quality challenges, allowing you to create a Data Quality Framework that's not just powerful but adaptable.
The Power of DBT-Core's Test Module
Automated Testing
: DBT-Core's Test Module empowers you to build automated tests for your data transformations. You can define the expected outcomes for your data models, ensuring that your data stays accurate even as your pipeline evolves.
Continuous Monitoring
: With scheduled testing, DBT-Core keeps an ever-watchful eye on your data quality. It notifies you immediately when anomalies are detected, allowing for swift corrective action.
Data Documentation
: The Test Module enhances your data documentation efforts. It provides clear and structured documentation on the tests you've implemented, making it easier for your team to understand and maintain your data quality measures.
The Business Impact
The integration of a Data Quality Framework with DBT-Core's Test Module offers tangible benefits:
Confident Decision-Making
: High-quality data leads to accurate insights, empowering organizations to make confident decisions.
Cost Reduction
: Early detection and correction of data issues prevent costly errors, compliance violations, and rework.
Efficiency
: Automated testing saves time and resources, allowing teams to focus on value-added activities.
Compliance
: Data quality practices ensure organizations meet regulatory and industry standards.
Conclusion
In an era where data fuels innovation, it's not enough to simply gather data; you must ensure its quality. The marriage of a
Data Quality Framework with DBT-Core's
Test Module is a potent combination that positions your organization for success.
It's about more than data quality; it's about using data as a strategic asset. In a world where data is king, this integration empowers you to wield your data with the confidence and precision it deserves.
Ready to elevate your data quality to new heights? Discover the potential of DBT-Core's Test Module.
#D
a
taQuality #DBTCore #DataTesting #DataAnalytics #DataStrategy #DataDriven
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
11
To view or add a comment,
sign in
More articles by Ganesan Vetriselvan
Using Snowflake Database in Telecommunications
Mar 25, 2023
Using Snowflake Database in Telecommunications
As the #telecommunications industry continues to evolve, companies are facing the challenge of managing large volumes…
7
Snowflake data warehousing
Jan 17, 2023
Snowflake data warehousing
#Snowflake is a cloud-based data warehousing service that allows users to store and analyze large amounts of data using…
6
How to write a resume that will get you a job
Jun 23, 2020
How to write a resume that will get you a job
You could be a fresher building your resume for the first time, or someone who has been with the same company for a…
8
1 Comment
Top 7 things to do to succeed in a Big Data project
Jun 12, 2020
Top 7 things to do to succeed in a Big Data project
1. Ironically, don’t call it as Big Data! Big Data refers to data which due to its 4 Vs, namely Volume, Variety…
4
AI-Bigdata aid for Covid-19
Mar 25, 2020
AI-Bigdata aid for Covid-19
BACKGROUND The Coronavirus (Covid-19) pandemic has once again exposed the underlying inefficiencies of the global…
3
Lambda on Azure & AWS
Mar 20, 2019
Lambda on Azure & AWS
Lambda architecture is a popular pattern in building Big Data pipelines. It is designed to handle massive quantities of…
2
What is Object Storage and why Should we care ?
Sep 10, 2018
What is Object Storage and why Should we care ?
Object storage is still fairly new, when compared with more traditional storage systems such as file or block storage…
3
2 Comments
The Leading APIs of Spark: RDD, DataFrames and Datasets
Mar 13, 2018
The Leading APIs of Spark: RDD, DataFrames and Datasets
While developing Spark application often we come across either one of this three APIs: DataFrames, Datasets and RDDs…
5
What is Big Data ? ......How Big is Big ?......
Jun 22, 2016
What is Big Data ? ......How Big is Big ?......
Dear Friends, What is Big Data, Why such Hype about it ? ..
6
3 Comments
Introducing Apache Kafka – Part One
Jun 3, 2016
Introducing Apache Kafka – Part One
What is Apache Kafka? From http://kafka.apache.
3
1 Comment
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Fix Data Quality Issues in ETL Testing,"How to Fix Data Quality Issues in ETL Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
IT Services
Data Management
How can you identify and resolve data quality issues during ETL testing?
Powered by AI and the LinkedIn community
1
Data loss
2
Data inconsistency
3
Data duplication
4
Data corruption
5
Data format errors
6
Here’s what else to consider
Data quality issues can affect the accuracy, reliability, and usability of your data, especially when you are performing extract, transform, and load (ETL) operations to move data from different sources to a data warehouse or a data lake. ETL testing is a process of verifying that the data is transferred correctly and meets the business requirements and expectations. In this article, you will learn how to identify and resolve some common data quality issues during ETL testing, such as data loss, data inconsistency, data duplication, data corruption, and data format errors.
Top experts in this article
Selected by the community from 36 contributions.
Learn more
Samantha Stanley
Sales/Marketing Director | Project/Change Management
View contribution
5
Mihir Nayak
Talent Acquisition Partner @ IBM | Certified Naukri Maestro Recruiter | Certified LinkedIn Recruiter |
View contribution
3
Sumegha Irashetti
Database & ETL Testing Expert | SQL | ETL Testing | Database Testing | API Testing | QA Professional | ISQTB Certified…
View contribution
3
See what others are saying
1
Data loss
Data loss occurs when some data is missing or not transferred from the source to the target. This can happen due to various reasons, such as network failures, file corruption, incorrect filtering, or incompatible data types. To identify data loss, you can compare the record counts and the key fields between the source and the target, and check for any gaps or mismatches. To resolve data loss, you can review the ETL logic and the data mapping, and ensure that the data is extracted, transformed, and loaded correctly and completely.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Mihir Nayak
Talent Acquisition Partner @ IBM | Certified Naukri Maestro Recruiter | Certified LinkedIn Recruiter |
Copy link to contribution
Report contribution
Set Data Standards: Define accuracy, completeness, and consistency rules.
Review Data: Detect missing info, duplicates, or anomalies.
Automate Checks: Use tools for missing data and anomalies.
Cleanse Data: Remove duplicates and fill gaps.
Validate Accuracy: Continuously ensure data quality.
Collaborate: Stay connected with data providers and users.
Maintain Logs: Keep notes on your work and data rules.
Improve Always: Regularly enhance rules for data quality.
…see more
Like
3
Sumegha Irashetti
Database & ETL Testing Expert | SQL | ETL Testing | Database Testing | API Testing | QA Professional | ISQTB Certified | Microsoft Azure | 6+ Years of Expertise
Copy link to contribution
Report contribution
Based on my experience as an ETL Tester 
we can start with basic validation like validating count between source and target that will ensure if there are any data loss , data duplicates.
If the data is coming from various heterogenous sources and you have no control over the data we can handle those into etl pipelines to avoid unclean data.
…see more
Like
3
Albert Coronado
Staff Solutions Architect | Technical Lead | Full Stack Dev/DevOps/Data/IA
Copy link to contribution
Report contribution
The big problem with data losses is that sometimes no one knows that there is a lack of information. In order to solve that It's important mark when some information cannot be  transferred(For example, with an error).
…see more
Like
2
Ankita Kundu
Manager at PwC - One Consulting - Data Strategy & Governance
(edited)
Copy link to contribution
Report contribution
Based on my prior experience in a major bank-wide data migration project, several crucial steps stood out to minimize data loss
-a complete backup of the source data,a crucial step before migration
- incremental data migration approach to detect and address issues with smaller datasets as we progressed
-Developed a robust data recovery plan , essential to rectify any data loss and to re-migrate any affected data
Following the migration, we observed instances of customer data loss. We implemented the data recovery plan, which revealed that the data loss stemmed from a misconfigured transformation rule. Promptly resolving the issue, we re-migrated the affected records.
As a result, data loss is minimized, and data integrity was maintained
…see more
Like
2
William Lysz
Senior Data Manager at BNY Mellon
Copy link to contribution
Report contribution
God, I’m so sick of this corporate-speak. Yes, obviously!  Data engineering and platform integration isn’t some mystical alchemy that requires The Enchanted Mouse of Ones-and-Zeroes. If you have garbage coming in and you know what you need, any engineer with a modicum of common sense can correct for problems. Test against known variables, talk to your source engineers to make sure you’re both on the same page, and  set up QC checks to alert you of wildly-off metrics. Data engineers shuffle ones and zeroes - we don’t get hearts beating again!  What we do is 99% common sense and 1% figuring out how to Google the code needed for the answer.
…see more
Like
2
Prakhar Srivastava
Turning coffee into strategy, and confusion into curiosity. The only predictable thing? My unpredictability.
Copy link to contribution
Report contribution
I would also add that having in place a strong data quality plan can aid in guarding against data loss in the first place. 

Maintaining data integrity requires routine data profiling, data cleaning, and data validation operations. Additionally, putting in place monitoring systems and error-handling procedures can assist organisations in the real-time detection and resolution of data loss concerns.
…see more
Like
1
Devika Shah
M.S Marketing Analytics at Suffolk University | PAO Graduate Fellow | Ex - ML Data Associate at Amazon
Copy link to contribution
Report contribution
Data loss is a pivotal concern within the realm of ETL processes, as it has the potential to erode data quality, impact decision-making, and compromise the integrity of the data infrastructure. To effectively address this issue during ETL testing, a systematic approach is essential. By identifying data loss through various means, such as scrutinizing data discrepancies between the source and transformed data, comparing record counts, and poring over error logs to pinpoint any issues. 
It's vital to establish strategies for data recovery, including data retention to archive lost or rejected data. Enhancing error handling mechanisms and crafting testing scenarios that focus specifically on data loss can help unveil potential issues.
…see more
Like
1
Laxmi Narayanan Appu Saravanan
Experienced MS in Computer Science graduate from UTSA | Specializing in Data Engineering, Analytics and Data Science | Former TCS professional | Pega CSSA & CSA certified | Actively seeking full-time opportunities.
Copy link to contribution
Report contribution
By handling simple test cases with checking the count, data validation with help of 'with as' query for source ; data ; this compared with target data, validating special fields that requires attention and duplicate check.
…see more
Like
1
Sudhir Kumar
VP Business Development/EPC/INFRA/Govt/Enterprise/Ministries/Advisory/Hospitality/HoReCa/Hospital/RealEstate/Multinational/PSU/CSR/NewMediaIT/DMM/PowerBi/Tableau/ETL/BigData/GrowthHacking/Renewable7/DataPrivacy/GDPR/EU
Copy link to contribution
Report contribution
During extraction of Data it's Transformations and Loading to Data Marts repository or Data Warehouse for further information processing, reporting, visualisation a Times due to error in data format, typing/spelling errors in data, or duplication etc data loss occurs.

To avoid data loss during normalisation utmost attention is required by matching count of records of incoming Data from various sources and final updated Data. Power Query editor in Power Bi is a wonderful data editor which can connect with multiple data sources and help transform the data for further loading and processing for visualisation!
…see more
Like
1
Madhan Mohan Tammineni
Tech Lead
Copy link to contribution
Report contribution
Identifying and resolving data quality issues in ETL testing:

1. Profile source data for anomalies.
2. Validate data against standards.
3. Ensure completeness and accuracy.
4. Apply data cleansing.
5. Implement error handling.
6. Use data profiling tools.
7. Reconcile data.
8. Perform regression testing.
9. Track data quality metrics.
10. Appoint data stewards.
11. Document issues and solutions.
13. Establish monitoring.
14. Maintain a feedback loop.
15. Optimize ETL processes.
16. Provide user training.
17. Conduct root cause analysis.
18. Implement data governance.
19. Foster collaboration.
These actions ensure reliable data for decision-making.
…see more
Like
2
Data inconsistency
Data inconsistency occurs when the same data has different values or formats in different sources or targets. This can happen due to different data standards, definitions, or rules across the data sources, or due to human errors, such as typos or misspellings. To identify data inconsistency, you can perform data profiling and data cleansing, and check for any discrepancies or anomalies in the data values, formats, or structures. To resolve data inconsistency, you can apply data validation and data standardization rules, and ensure that the data is consistent and conforming to the business requirements and expectations.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Albert Coronado
Staff Solutions Architect | Technical Lead | Full Stack Dev/DevOps/Data/IA
Copy link to contribution
Report contribution
When you design a system, It's important identify the ""source of true"". This means identify where is the true data in order to avoid data inconsistency. It's important to have just one ""source of true"" for field.
…see more
Like
3
Kartik Dodiya
Quality 4.0 | GxP Compliance | ISO 9001:2015 QMS Auditor | ISO 37301:2021 CMS | LSS Black Belt | eCFR | Train the Trainer | ISO 31000:2018 Risk Management Expert | CSV | GAMP 5 2nd edition
Copy link to contribution
Report contribution
To manage and improve data inconsistency, follow these steps:
1. Data Profiling
2. Data Standardization 
3. Data Validation
4. Data Cleaning
5. Master Data Management (MDM)
6. Data Governance
7. Documentation
8. Change Management
9. Data Quality Monitoring
10. Data Training
11. Feedback Mechanism
12. Data Auditing
13. Data Enrichment
14. Data Quality Metrics
15. Data Integration
16. Data Archiving
17. Feedback Loop
18. Data Ownership
19. Data Stewardship
20. Continuous Improvement
By following these steps, organizations can effectively manage and improve data consistency, ensuring that data is reliable and accurate for decision-making and reporting purposes.
…see more
Like
Kinangwede Kihara
Business Intelligence Analyst at Benson Informatics Limited(Smart Tanzania)
(edited)
Copy link to contribution
Report contribution
I agree Data inconsistency it happen sometime when wrote improper SQL Scripts though the fetching of Data align on same Tables or same Data source. To resolve this we should adhere to write proper script and well filtered
…see more
Like
3
Data duplication
Data duplication occurs when the same data is repeated or copied in the same or different sources or targets. This can happen due to lack of data governance, data integration, or data deduplication processes, or due to human errors, such as entering or loading the same data multiple times. To identify data duplication, you can perform data matching and data deduplication, and check for any duplicate or redundant records or fields in the data. To resolve data duplication, you can remove or merge the duplicate data, and ensure that the data is unique and distinct.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Albert Coronado
Staff Solutions Architect | Technical Lead | Full Stack Dev/DevOps/Data/IA
Copy link to contribution
Report contribution
Manage data duplication It's very important in order to avoid data inconsistency. But sometimes data duplication is needed, for example, in order to improve performance.
…see more
Like
2
Komal Dalmi, PMP®
PM at S&P Global Mobility || Ex-SingleInterface || Ex-Skribe || Ex- S&P
Copy link to contribution
Report contribution
One needs to implement de-duplication mechanisms within the ETL process. Utilize unique keys and indexes to prevent duplicates during the loading process. Modify ETL logic to identify and merge or eliminate duplicate records. One can also check for duplicate records in the target database or data warehouse.
…see more
Like
1
Kartik Dodiya
Quality 4.0 | GxP Compliance | ISO 9001:2015 QMS Auditor | ISO 37301:2021 CMS | LSS Black Belt | eCFR | Train the Trainer | ISO 31000:2018 Risk Management Expert | CSV | GAMP 5 2nd edition
Copy link to contribution
Report contribution
To manage and minimize data duplication, Data Profiling, Unique Identifiers, Data Standardization, 
Data Validation, Deduplication Tools, Data Matching Algorithm, Data Governance, Automated Workflows, Data Training and regular data audits should be performed to maintain a deduplication process.
…see more
Like
Kinangwede Kihara
Business Intelligence Analyst at Benson Informatics Limited(Smart Tanzania)
Copy link to contribution
Report contribution
I agree with your to avoid data Duplication ensure we use SQL language command  for unique like Distinct,Group by and Order By. This will help to over come duplication on data we provide
…see more
Like
4
Data corruption
Data corruption occurs when the data is altered or damaged during the ETL process, resulting in incorrect or unreadable data. This can happen due to hardware failures, software bugs, malicious attacks, or human errors, such as modifying or deleting the data unintentionally. To identify data corruption, you can perform data verification and data recovery, and check for any errors or warnings in the ETL logs, or any changes or losses in the data quality or integrity. To resolve data corruption, you can restore or repair the corrupted data, and ensure that the data is secure and trustworthy.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Samantha Stanley
Sales/Marketing Director | Project/Change Management
Copy link to contribution
Report contribution
Our team's meticulous approach to examine data for discrepancies, missing values, and inconsistencies, often utilizing tools like MySql or Apache. Once issues are pinpointed, employing data cleansing and transformation strategies  are crucial. 

We find effective collaboration among data engineers, analysts, and business stakeholders is key to achieving data quality. Also regression testing needs to be implemented to ensure that updates and changes don’t introduce data quality issues.

Also we believe in establishing rigorous data governance, maintaining data lineage, and conducting regular monitoring, organizations can uphold data integrity throughout the ETL process.
…see more
Like
5
Yagneshkumar Patel
Founder @ Infinite Intelligence Consulting | Corporate Coach, Mindset Coach, NLP practitioner, Mental wellness
Copy link to contribution
Report contribution
In conclusion, addressing data corruption requires a multi-faceted approach that incorporates cutting-edge technologies, robust monitoring systems, and advanced data integrity measures. These innovative solutions not only help in identifying and resolving data corruption but also work to prevent it from occurring in the first place. By implementing these strategies, organizations can safeguard their data and maintain its accuracy and integrity throughout the ETL process.
…see more
Like
Kartik Dodiya
Quality 4.0 | GxP Compliance | ISO 9001:2015 QMS Auditor | ISO 37301:2021 CMS | LSS Black Belt | eCFR | Train the Trainer | ISO 31000:2018 Risk Management Expert | CSV | GAMP 5 2nd edition
Copy link to contribution
Report contribution
To manage and avoid data corruption, following steps should be followed:
1. Backup: 
2. Data Validation
3. Data Encryption
4. Redundancy
5. Antivirus Software
6. Error Checking
7. Data Integrity Checks
8. Access Control
9. Disaster Recovery Plan
10. Regular Maintenance
11. Data Migration
12. Employee Training
13. Monitoring
14. Data Recovery Tools
15. Data Testing
…see more
Like
5
Data format errors
Data format errors occur when the data does not match the expected or required format or schema in the target. This can happen due to different data types, formats, or schemas across the data sources and targets, or due to human errors, such as entering or loading the data in the wrong format. To identify data format errors, you can perform data conversion and data validation, and check for any errors or exceptions in the ETL process, or any mismatches or incompatibilities in the data formats or schemas. To resolve data format errors, you can convert or transform the data to the desired or expected format or schema, and ensure that the data is compatible and compliant.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Komal Dalmi, PMP®
PM at S&P Global Mobility || Ex-SingleInterface || Ex-Skribe || Ex- S&P
Copy link to contribution
Report contribution
Data Validations help in a lot of data management issues, here as well we need to have data format validation checks in the ETL process to ensure that data adheres to the expected formats.
…see more
Like
2
Kartik Dodiya
Quality 4.0 | GxP Compliance | ISO 9001:2015 QMS Auditor | ISO 37301:2021 CMS | LSS Black Belt | eCFR | Train the Trainer | ISO 31000:2018 Risk Management Expert | CSV | GAMP 5 2nd edition
Copy link to contribution
Report contribution
To manage and minimize data format errors, following steps need to be followed:
1. Data Standardization
2. Validation Rules
3. Data Entry Controls
4. Error Handling
5. Data Transformation
6. Regular Audits
7. Data Mapping
8. Data Governance
9. Collaboration
10. Automation
11. Documentation
12. User Training
13. Feedback Loop
14. Data Integration
15. Continuous Improvement
…see more
Like
6
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Masood Alam 💡
📚 Author of The Data Revolution 🎤 Public Speaker | 🏛️ TOGAF & ☁️ AWS Certified | 🤖 AI | 📜 Data Governance | 🚀 Leadership & Change Management | ⚙️ DevOps | 🛠️ Data Engineering | 🎮 ex-FanDuel | 🌐 ex-Alibababa
Copy link to contribution
Report contribution
Identifying and resolving data quality issues during ETL (Extract, Transform, Load) testing is crucial for maintaining accurate and reliable data. Here are the key steps to follow:

Data Profiling: Start by profiling the source data to identify anomalies, missing values, duplicates, and outliers. ETL tools and specialized software can assist in this process.

Data Cleansing: Implement data cleansing routines to address issues like missing values, incorrect formats, and duplicates. This may involve standardization and validation rules.

Data Transformation and Validation: Define transformation rules and validation checks to ensure data integrity throughout the ETL process. 

Error Handling: Establish a robust error-handling mechanism.
…see more
Like
3
Christos Pattichis
IT Senior Leader in Data Management, Engineering & Analytics | Expert in Digital Transformation & Operational Excellence | Experience in multiple industries: Auto & Computer Manufacturing, Banking & FinTech.
Copy link to contribution
Report contribution
My opinion is that moving forward AI tools will be developed to manage data quality end-to-end. All of the aspects of data quality mentioned in the article are correct and they require some level of human intervention. I believe AI models can be trained to identify and also fix data quality issues incorporating a notion of ""auto-healing"" to address ""bad data""
…see more
Like
2
John 'Femi A.
Lead Solution Architect  ◆ Data Scientist  ◆ Technical Program Manager ◆ Artificial Intelligence ◆ Machine Learning ◆ PhDc, MSc. ◆  International Development
Copy link to contribution
Report contribution
My experience is, AI/ML provides these solutions:-
Identify Data Loss: AI predicts missing data patterns in real-time, preventing gaps.
Tackle Data Inconsistency: ML detects historical irregularities and NLP ensures semantic consistency.
Mitigate Data Duplication: AI-driven clustering flags duplicates, offering process modifications.
Address Data Corruption & Format Errors: Predictive AI foresees data corruption and ML validates data formats automatically.
Continuous Improvement: AI for predictive maintenance monitors ETL in real-time and performs root cause analysis.
AI and ML transform ETL testing, resolving issues and proactively preventing future data quality challenges. Elevate ETL testing for high-quality data and informed decisions.
…see more
Like
2
Komal Dalmi, PMP®
PM at S&P Global Mobility || Ex-SingleInterface || Ex-Skribe || Ex- S&P
Copy link to contribution
Report contribution
We need to implement automated data quality checks within the ETL process to catch any of the issues early and consistently. 

One needs comprehensive data validation using predefined rules and should conduct manual validations as well.

I am a strong advocate of BATCH TESTING in ETL to avoid or minimize any kind of data quality issue.
…see more
Like
Yagneshkumar Patel
Founder @ Infinite Intelligence Consulting | Corporate Coach, Mindset Coach, NLP practitioner, Mental wellness
Copy link to contribution
Report contribution
In the realm of data management, addressing data quality issues during ETL is of utmost importance. It ensures that the data being integrated into a data warehouse or data lake is reliable and accurate, which, in turn, supports informed decision-making and reliable business insights. The paragraph sets the stage for a discussion on how to identify and rectify these issues during ETL testing, underlining the importance of data quality in the data integration process.
…see more
Like
Data Management
Data Management
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Management
No more previous content
You're navigating the demands of innovation and routine. How do you manage data effectively?
4 contributions
You're juggling data governance and agile access demands. How do you strike the right balance?
You're facing pushback from colleagues on system upgrades. How can you get them on board with change?
Your team is divided on KPI priorities. How do you ensure data interpretations align for successful analysis?
Your team is clashing over data entry tasks. How do you resolve the conflict?
No more next content
See all
Explore Other Skills
IT Strategy
System Administration
Technical Support
Cybersecurity
IT Management
Software Project Management
IT Consulting
IT Operations
Information Security
Information Technology
Show more
Show less
More relevant reading
Data Management
How do you handle data validation during ETL?
Database Engineering
What are the limitations of ETL data quality and validation?
Data Management
How can ETL testing address common data management challenges?
Data Warehousing
What are some effective strategies for data deduplication in the ETL process?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
7
36 Contributions"
BIG DATA TESTING,"BIG DATA TESTING
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
BIG DATA TESTING
Report this article
Smriti Saini
Smriti Saini
Talent Acquisition Specialist || Let's grow together!!
Published Oct 10, 2020
+ Follow
BigData testing is defined as testing of Bigdata applications. Big data is a collection of large datasets that cannot be processed using traditional computing techniques.
Testing
of these datasets involves various tools, techniques, and frameworks to process. Big data relates to data creation, storage, retrieval and analysis that is remarkable in terms of volume, variety, and velocity.
Big Data Testing Strategy
Testing Big Data application is more verification of its data processing rather than testing the individual features of the software product. When it comes to Big data testing,
performance and functional testing
are the keys.
In Big data testing, QA engineers verify the successful processing of terabytes of data using commodity cluster and other supportive components. It demands a high level of testing skills as the processing is very fast. Processing may be of three types
Along with this, data quality is also an important factor in Hadoop testing. Before testing the application, it is necessary to check the quality of data and should be considered as a part of database testing.
It involves checking various characteristics like conformity, accuracy, duplication, consistency, validity, data completeness,
etc.
Big Data Testing can be broadly divided into three steps
Step 1: Data Staging Validation
The first step of big data testing also referred as pre-Hadoop stage involves process validation.
Data from various source like RDBMS, weblogs, social media, etc. should be validated to make sure that correct data is pulled into the system
Comparing source data with the data pushed into the Hadoop system to make sure they match
Verify the right data is extracted and loaded into the correct HDFS location
Tools like
Talend
,
Datameer
,
can be used for data staging validation
Step 2: ""MapReduce"" Validation
The second step is a validation of ""MapReduce"". In this stage, the tester verifies the business logic validation on every node and then validating them after running against multiple nodes, ensuring that the
Map Reduce process works correctly
Data aggregation or segregation rules are implemented on the data
Key value pairs are generated
Validating the data after the Map-Reduce process
Step 3: Output Validation Phase
The final or third stage of Big Data testing is the output validation process. The output data files are generated and ready to be moved to an EDW (Enterprise Data Warehouse) or any other system based on the requirement.
Activities in the third stage include
To check the transformation rules are correctly applied
To check the data integrity and successful data load into the target system
To check that there is no data corruption by comparing the target data with the HDFS file system data
Architecture Testing
Hadoop processes very large volumes of data and is highly resource intensive. Hence, architectural testing is crucial to ensure the success of your Big Data project. A poorly or improper designed system may lead to performance degradation, and the system could fail to meet the requirement. At least,
Performance and Failover test
services should be done in a Hadoop environment.
Performance testing includes testing of job completion time, memory utilization, data throughput, and similar system metrics. While the motive of Failover test service is to verify that data processing occurs seamlessly in case of failure of data nodes
Performance Testing
Performance Testing for Big Data includes two main action
Data ingestion and Throughout
: In this stage, the tester verifies how the fast system can consume data from various data source. Testing involves identifying a different message that the queue can process in a given time frame. It also includes how quickly data can be inserted into the underlying data store for example insertion rate into a Mongo and Cassandra database.
Data Processing
: It involves verifying the speed with which the queries or map reduce jobs are executed. It also includes testing the data processing in isolation when the underlying data store is populated within the data sets. For example, running Map Reduce jobs on the underlying HDFS
Sub-Component Performance
: These systems are made up of multiple components, and it is essential to test each of these components in isolation. For example, how quickly the message is indexed and consumed, MapReduce jobs, query performance, search, etc.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by this author
No more previous content
What Is Portfolio Analytics?
Oct 4, 2021
Annuity
Oct 1, 2021
What is Actuarial Modeling?
Sep 30, 2021
Supervised vs. Unsupervised Learning: What’s the Difference?
Sep 29, 2021
APACHE HIVE
Sep 28, 2021
Acceptance testing
Sep 27, 2021
SAP HANA
Sep 25, 2021
Machine Learning Architecture
Sep 23, 2021
AZURE DEVOPS
Sep 22, 2021
Report Building
Sep 21, 2021
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Test Data Quality for Governance and Management,"How to Test Data Quality for Governance and Management
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Data Quality
How do you test data quality for governance and management?
Powered by AI and the LinkedIn community
1
Define data quality dimensions
Be the first to add your personal experience
2
Assess data quality risks
Be the first to add your personal experience
3
Design data quality tests
Be the first to add your personal experience
4
Execute data quality tests
Be the first to add your personal experience
5
Analyze data quality results
Be the first to add your personal experience
6
Improve data quality
Be the first to add your personal experience
7
Here’s what else to consider
Be the first to add your personal experience
Data quality is essential for effective governance and management of any data-driven organization. Poor data quality can lead to inaccurate insights, unreliable decisions, and wasted resources. To ensure data quality, you need to test and validate your data against defined standards and criteria. In this article, we will explain how you can test data quality for governance and management using some practical steps and tools.
Find expert answers in this collaborative article
Experts who add quality contributions will have a chance to be featured.
Learn more
See what others are saying
1
Define data quality dimensions
The first step is to define what data quality means for your specific context and goals. Data quality dimensions are the aspects or characteristics of data that affect its fitness for use. Some common data quality dimensions are accuracy, completeness, consistency, timeliness, validity, and uniqueness. You can use a framework such as the ISO 8000 or the DAMA DMBOK to guide you in defining your data quality dimensions and their metrics.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
2
Assess data quality risks
The next step is to assess the potential risks and impacts of poor data quality on your governance and management processes. Data quality risks are the sources or causes of data quality issues that can affect your data's usability and value. For example, data quality risks can stem from human errors, system failures, integration issues, or external factors. You can use a tool such as the Data Quality Risk Matrix to identify and prioritize your data quality risks based on their likelihood and severity.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Design data quality tests
The third step is to design data quality tests that can measure and verify your data quality dimensions and metrics. Data quality tests are the methods or procedures that can check and validate your data against your data quality standards and criteria. For example, data quality tests can include data profiling, data cleansing, data matching, data auditing, or data certification. You can use a tool such as the Data Quality Test Plan to document and organize your data quality tests and their expected outcomes.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Execute data quality tests
The fourth step is to execute your data quality tests on your data sources and datasets. Data quality testing is the process of applying your data quality tests and collecting the results and evidence of your data quality performance. For example, data quality testing can involve running queries, scripts, or tools on your data and generating reports, dashboards, or logs that show your data quality metrics and indicators. You can use a tool such as the Data Quality Test Report to summarize and communicate your data quality testing results and findings.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Analyze data quality results
The fifth step is to analyze your data quality results and identify any data quality issues or gaps that need to be addressed. Data quality analysis is the process of interpreting and evaluating your data quality performance and comparing it with your data quality expectations and requirements. For example, data quality analysis can involve calculating data quality scores, identifying data quality trends, or performing root cause analysis. You can use a tool such as the Data Quality Issue Tracker to record and track your data quality issues and their resolutions.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Improve data quality
The final step is to improve your data quality by implementing corrective and preventive actions that can resolve or prevent your data quality issues or gaps. Data quality improvement is the process of enhancing and maintaining your data quality performance and aligning it with your governance and management objectives. For example, data quality improvement can involve data cleansing, data enrichment, data standardization, or data governance. You can use a tool such as the Data Quality Improvement Plan to outline and execute your data quality improvement actions and measures.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Quality
Data Quality
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Quality
No more previous content
How can you identify and fill gaps in your data quality strategy?
58 contributions
How can you improve data quality maturity across domains and industries?
53 contributions
How do you tell stakeholders if your data is bad for machine learning?
52 contributions
How do you report data quality results to your audience?
62 contributions
How do you prioritize data quality when resources are limited?
69 contributions
How can you improve data quality documentation and metadata management?
61 contributions
How do you set data quality policies?
54 contributions
How can you establish and enforce data quality standards?
56 contributions
How can you compare data quality improvement techniques?
35 contributions
How can you build a data quality culture using a framework?
51 contributions
How do you automate data quality metrics for different platforms?
49 contributions
How are you improving your data quality?
45 contributions
How can you improve data quality when integrating multiple sources?
12 contributions
What are the best ways to transform data for different audiences?
33 contributions
How do you prepare for emerging data quality trends and challenges?
11 contributions
No more next content
See all
More relevant reading
Business Analysis
How do you follow data quality and governance principles?
Data Management
How can you prevent data cleansing and transformation from affecting data integrity?
Quality Management
What are the key factors for improving data quality?
Data Quality
How can you excel as a data quality manager?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share"
PART 1 - ROAD MAP from Non-Tech to Big Data Testing,"PART 1 - ROAD MAP from Non-Tech to Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
PART 1 - ROAD MAP from Non-Tech to Big Data Testing
Report this article
Krishna Kayaking 🇮🇳
Krishna Kayaking 🇮🇳
Big Data Engineer | AWS | Azure | Python | Spark | SQL | Databricks | YouTuber | Storyteller | Musician | 5k LinkedIn | 3k YT | 100k Insta
Published Oct 1, 2022
+ Follow
I will divide this into 2 parts –
a.
IT
– already working in IT companies or some IT-related role.
b.
& Non-IT
i.   I will consider all – from BA, B. Com, BCA & other degree as non-IT)
ii.   Even those who are working in non-it-related roles like Support
iii.   IT support or HR, or Facility, etc.
It is difficult directly to enter the Big Data space. So, let’s go to the screen & check the ROAD MAP.
I would recommend you all start with ETL/DWH & then the next step should be either cloud or Big Data.
Do not jump! Some may make it & some may slip. It is my humble advice no to jump. Rather learn ETL/DWH Testing first.
FIRST STEP – Learn basic terminologies used in IT
.
For example – what is a table, what is the frontend, and what is the backend? What is a column, row, metadata, flat file, BI tool, users, DBMS, Database, or RDBMS? Etc. etc.
SECOND STEP
– Learn DWH concepts.
What is DWH? History?
Languages used to deal with Data.
Need of DWH?
OLTP, OLAP.
What is ETL?
Characteristics of DWH.
Normalization, denormalization.
ETL architecture.
Types of Data models
Types of schemas – star, snowflake
ETL Project architecture – I have explained that in the previous video. I will link it in the I button on top.
THIRD STEP – SQL Learn & Practice. I would recommend you to learn oracle.
What is SQL?
Learn about Datatype, variables, and syntax.
Learn about DDL, DML, TCL, and DCL.
Create, alter, drop, etc. practice them.
DMLs – insert, update, delete, select, truncate.
As a tester, we will mostly be working on Select commands, but we should have good knowledge of other DDLs, like create, update delete.
Clauses – distinct, order by, where, group by, having.
And & or conditions.
FUNCTIONS (Character/String functions, Aggregate functions, Analytical functions, Date functions)
SET OPERATORS – union, union all, interest, minus, etc.
CONSTRAINTS – unique, not null, primary key, foreign key, etc.
JOINS – inner, cross, self, outer (left, right, full)
SUB-QUERIES
VIEWS
You do not have to master everything. If you just complete the above-mentioned topics, you are good to crack interviews.
I have not mentioned a lot of things, but this all is a good start. There are some more things, I will add them in the coming videos.
FORTH STEP – UNIX – this is one of the most important parts.
This is like the lungs of ETL/DWH/Big Data tester.
SQL is the heart & UNIX is the lungs.
As I mentioned you do not have to be an expert, but you should have good knowledge.
I would recommend you to please install the virtual box & install any of the Linux environments & practices.
Please comment if you need a tutorial on UNIX commands & setting up the environment.
FIFTH STEP – TESTING CONCEPTS
- The video will become lengthy if I discuss this. I would set a target of 50 likes, and comment TESTING in the comments. I will come up with a video on these topics.
LAST BUT MOST IMPORTANT – ETL TESTING CONCEPTS.
This is one of the most important for you to crack the interview.
As I said in the previous section, the video is already lengthy.
Please Comment ETL, I will come up video on this.
More the comment, that topic will come first. You all decide.
Watch the Video on Techie Krishna Kayaking - https://www.youtube.com/user/Kayaking22
VIDEO - ROAD MAP - Non-Tech to Big Data Testing | Krishna Kayaking | Video#005
#testing
#bigdata
#etltesting
- https://youtu.be/aK9jWkfS-wM
------
ROAD MAP FROM NON-TECH TO BIG DATA TESTING
--------------------------------------------------------------------------------------
Part 1 - https://www.linkedin.com/pulse/road-map-from-non-tech-big-data-testing-krishna-kayaking/?trackingId=%2BfsHVpMkQo2sM8Q6PvEl%2FQ%3D%3D
Video 1 - https://www.youtube.com/watch?v=aK9jWkfS-wM
--------
Part 2 - https://www.linkedin.com/pulse/road-map-part-2-from-non-tech-big-data-testing-krishna-kayaking/?trackingId=q2S%2Bw2SXT8ethb5kU8tgrw%3D%3D
Video 2 - https://www.youtube.com/watch?v=sw8gzSqBeqo
--------
Part 3 – https://www.linkedin.com/feed/update/urn:li:ugcPost:6996484383201267714?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3AugcPost%3A6996484383201267714%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29
Video 3 – https://youtu.be/7mgYcvJUkOU
===========================
Follow me on Social Media - Krishna Kayaking -
LinkedIn - https://www.linkedin.com/in/krishnakayaking
Instagram - https://www.instagram.com/krishnakayaking/
FB - https://www.facebook.com/krishnakayaking/
Twitter - https://twitter.com/krishnakayaking
#video
#youtube
#krishnakayaking
#techieKrishnaKayaking
#techie
#techies
#ITlife
#informationtechnology
#engineering
#career
#careergrowth
#education
#testing
#BigData
#ETL
#DWH
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
44
4 Comments
Saurav Kashyap
Seeking Entry-level Programming position || Data Structures  & Algorithms || Java ||JavaScript || Experience with HTML , CSS  || ReactJs || Frontend developer || Content moderation
1y
Report this comment
💖
Like
Reply
1 Reaction
2 Reactions
Krishna Kayaking 🇮🇳
Big Data Engineer | AWS | Azure | Python | Spark | SQL | Databricks | YouTuber | Storyteller | Musician | 5k LinkedIn | 3k YT | 100k Insta
2y
Report this comment
Part 2 is coming soon.
Like
Reply
1 Reaction
Surendra Chikkalli
Data Engineering Enthusiast| PySpark | Data Bricks | Azure Data Factory | SQL
2y
Report this comment
Hi Krishna, Can you please make a video on transforming from etl testing to Big data testing?. It would be appreciated if you consider my kind request. Thanks
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More articles by Krishna Kayaking 🇮🇳
STORY OF ANY MIDDLE-CLASS BOY - Navigating the Abyss: A Heartrending Tale of Struggle and Resilience 💔
Aug 10, 2023
STORY OF ANY MIDDLE-CLASS BOY - Navigating the Abyss: A Heartrending Tale of Struggle and Resilience 💔
STORY OF ANY MIDDLE-CLASS BOY - Navigating the Abyss: A Heartrending Tale of Struggle and Resilience 💔 In the quiet…
9
SQL Database Set-up | Kick-start 
 SQL Practice
Jan 19, 2023
SQL Database Set-up | Kick-start 
 SQL Practice
🔴 A new video on Setting up SQL | Kick-start SQL | SQL | DBeaver | SQLLite | DDL | DML | DQL @TechieKrishnaKayking 🔥…
19
PART 3 – ROADMAP – NON-TECH to BIG DATA TESTING
Nov 10, 2022
PART 3 – ROADMAP – NON-TECH to BIG DATA TESTING
PART 3 – ROADMAP – NON-TECH to BIG DATA TESTING Before you start, I would recommend you complete the below steps. ·…
9
1 Comment
100% INTERVIEW Question | OLTP vs OLAP | Database vs Dataware House | Testing | Krishna Kayaking
Nov 5, 2022
100% INTERVIEW Question | OLTP vs OLAP | Database vs Dataware House | Testing | Krishna Kayaking
OLTP vs OLAP | Database vs Dataware House | ETL Testing | DWH Testing | BI Testing | Manual Testing | Interview…
13
I QUIT | Give up Sports as a Career???
Oct 31, 2022
I QUIT | Give up Sports as a Career???
Give up sports. Do not choose Sports as a Career.
8
1 Comment
Rejected 40 LPA Offer | Big 4 | WHY ???
Oct 31, 2022
Rejected 40 LPA Offer | Big 4 | WHY ???
POINT 1 - WHY DID I APPLY? - I got appraisal, so wanted to check how the market conditions are, so applied. - I keep…
15
What to do after 10th? | Dilemma | No clue | Medical | Engineering | Business | CA | Career ??????
Oct 17, 2022
What to do after 10th? | Dilemma | No clue | Medical | Engineering | Business | CA | Career ??????
What to do? 1. Very important question? 2.
20
PART 2 - ROAD MAP - Non-Tech to Big Data Testing
Oct 13, 2022
PART 2 - ROAD MAP - Non-Tech to Big Data Testing
I am really delighted to see your overwhelming response on part 1 of the road map. As I said, If I get a good response,…
13
3 Comments
S1.E6 – MY LIFE CHANGING MOMENT [POST ENGINEERING]

Episode 6 – Offer letters & Compensations
Sep 10, 2022
S1.E6 – MY LIFE CHANGING MOMENT [POST ENGINEERING]

Episode 6 – Offer letters & Compensations
S1.E6 – MY LIFE CHANGING MOMENT [POST ENGINEERING] Episode 6 – Offer letters & Compensations Thank you all for your…
31
1 Comment
S1.E5 – MY LIFE CHANGING MOMENT [POST ENGINEERING]

Episode 5 – How to fight & stand in Tough Times. You Get To Know Your Actual Advocates (Supporters
Sep 3, 2022
S1.E5 – MY LIFE CHANGING MOMENT [POST ENGINEERING]

Episode 5 – How to fight & stand in Tough Times. You Get To Know Your Actual Advocates (Supporters
S1.E5 – MY LIFE CHANGING MOMENT [POST ENGINEERING] Episode 5 – How to fight & stand in Tough Times.
16
4 Comments
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Data Quality testing,"Data Quality testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Data Quality testing
Report this article
Grant Brodie
Grant Brodie
President, Arbutus Software
Published May 25, 2016
+ Follow
We are all familiar with the phrase “garbage in, garbage out”.  Once data quality gets “off the rails” it can be exceedingly difficult to get it back on track again.  This is even more important in this age of “big data”.  There has never been a more critical need for data quality, both because modern data volumes preclude any manual intervention and because modern business process and business decisions critically depend on that very same data.  Unfortunately, you seldom have control over the data itself, so you need to compensate, which is what this article is about.
The term “garbage” leaves the impression of totally corrupted information.  We can of course test for this, but minor formatting errors are much more common, and even these subtly different variations can have a similar negative effect on your processes.  For example, if the US state might be “NY”, “New York” or “N Y” then you might not correctly consider all the data as part of a single group, resulting in a flawed analysis.  Similar problems might happen with most data elements.  The bottom line is: anything that
can
be standardized probably
should
be, because you never know how the data might be used.  Standardization is the key that allows you to correctly aggregate and link information, with which you can make sound business decisions, but the challenge is to implement this goal easily.  Let me explain.
Most analytics products allow you to test your data; for example maximums, minimums and that the correct (expected) coding format is used, but an ideal solution would allow you to detect and standardize your data, based on rules, in a manner that doesn’t involve “rocket science”.  The problem with “rocket science”, is that the harder something is to understand, the less likely it will be implemented, and the harder it will be to maintain.  A simple example perhaps demonstrates the point. Assume you have an inventory number field that is supposed to be coded like “L27/726” (ie, a PIC of “X99/999”), but you know there are errors that need to be corrected.  You need an expression that creates the PIC for each transaction, so you can identify, classify and correct for the errors.  With our main competitor, the expression you would use for this is:
REGEXREPLACE( REGEXREPLACE( REGEXREPLACE( invno,""\d"", ""9""), ""[a-z]"", ""x""), ""[A-Z]"", ""X"")
.  With Analyzer the expression is simply:
Format(Invno)
.  Both expressions produce the same result, but I’ll let you choose the one you prefer, and would prefer to maintain.
Imagine, if you will, that you there were easy to understand functions for
all
your common data testing requirements, so that data quality is an easily attainable goal.  This is exactly what Arbutus Analyzer offers.  The Format function is but one example of many, that allow powerful testing with no “rocket science”.  Whether you want to identify formatting errors, remove extra blanks, or any of a large range of other manipulations, Analyzer will usually let you do it with ease.
By the way, there is an interesting “off-label” use for Analyzer’s Format function, in fraud and duplicate payment scenarios.  With accounts payable, each vendor has a different invoice number format, but within each vendor the format is usually consistent.  You can summarize on the vendor number and the expression Format(invoice_number) to identify every non-standard vendor invoice format.  This allows you to quickly identify higher risk non-standard transactions.
Check out some of my other weekly posts at
https://www.linkedin.com/today/author/0_1c4mnoBSwKJ9wfyxYP_FLh?trk=prof-sm
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
6
To view or add a comment,
sign in
More articles by this author
No more previous content
Self-serve analytics
Jul 14, 2016
Using Delimited Data (last of a series)
Jul 6, 2016
Using XML Data (part of a continuing series)
Jun 29, 2016
Using print image files (part of a continuing series)
Jun 16, 2016
Success with analytics is everyone's job
Jun 9, 2016
Data Transfer Formats
Jun 8, 2016
Take control of your data, maintain your audit independence
Jun 1, 2016
No Apologies
May 18, 2016
Big Data Analytics
May 12, 2016
Servers: Simpler is better
May 5, 2016
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
481 Senior Clinical Data Manager jobs in United States,"481 Senior Clinical Data Manager jobs in United States
Skip to main content
LinkedIn
Senior Clinical Data Manager in United States
Expand search
This button displays the currently selected search type. When expanded it provides a list of search options that will switch the search inputs to match the current selection.
Jobs
People
Learning
Clear text
Clear text
Clear text
Clear text
Clear text
Join now
Sign in
Any time
Any time (480)
Past month (345)
Past week (163)
Past 24 hours (70)
Done
Company
Clear text
EPM Scientific (5)
Terumo Neuro (2)
Warman O'Brien (2)
MMS (1)
Orca Bio (1)
Done
Job type
Full-time (350)
Part-time (4)
Contract (121)
Internship (1)
Other (5)
Done
Experience level
Internship (1)
Entry level (37)
Associate (52)
Mid-Senior level (265)
Director (45)
Done
Location
Clear text
Alameda, CA (22)
Cambridge, MA (18)
Boston, MA (16)
San Diego, CA (12)
Chicago, IL (11)
Done
Salary
$40,000+ (105)
$60,000+ (104)
$80,000+ (95)
$100,000+ (86)
$120,000+ (77)
Done
Remote
On-site (293)
Hybrid (93)
Remote (92)
Done
Get notified about new
Senior Clinical Data Manager
jobs in
United States
.
Sign in to create job alert
481
Senior Clinical Data Manager Jobs in United States
Lead Clinical Data Manager
Lead Clinical Data Manager
Elixir Associates
United States
Actively Hiring
1 day ago
Lead Clinical Data Manager
Lead Clinical Data Manager
Warman O'Brien
United States
Actively Hiring
             +4 benefits
2 weeks ago
Clinical Data Manager 2
Clinical Data Manager 2
Dexcom
United States
Actively Hiring
1 day ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Mindtris
United States
Actively Hiring
2 months ago
Remote - Clinical Data Manager
Remote - Clinical Data Manager
Beacon Hill
United States
Actively Hiring
1 week ago
Senior Clinical Data Manager (Remote)
Senior Clinical Data Manager (Remote)
MMS
Canton, MI
5 days ago
Senior Clinical Data Manager
Senior Clinical Data Manager
EPM Scientific
San Francisco Bay Area
Actively Hiring
             +5 benefits
2 weeks ago
Clinical Data Manager (remote, full-time)
Clinical Data Manager (remote, full-time)
OnQ Research
United States
14 hours ago
Lead Clinical Data Manager
Lead Clinical Data Manager
Orca Bio
United States
1 week ago
Sr Manager Clinical Data Management
Sr Manager Clinical Data Management
Terumo Neuro
United States
Medical insurance
             +7 benefits
3 weeks ago
Senior Clinical Data Manager
Senior Clinical Data Manager
TalentCraft
San Diego Metropolitan Area
Medical insurance
             +3 benefits
9 hours ago
Senior Data Manager_Remote @ US/CAN (Contract role)
Senior Data Manager_Remote @ US/CAN (Contract role)
ClinChoice
Dallas, TX
Actively Hiring
2 weeks ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Dianthus Therapeutics, Inc.
Waltham, MA
6 days ago
Sr. Director Clinical Data Management
Sr. Director Clinical Data Management
EPM Scientific
San Diego, CA
Actively Hiring
3 weeks ago
Manager, Clinical Data Abstraction
Manager, Clinical Data Abstraction
Natera
United States
Actively Hiring
1 day ago
Remote Senior Clinical Data Manager | WFH
Remote Senior Clinical Data Manager | WFH
Get It - Executive
Austin, TX
2 weeks ago
Director CDM
Director CDM
EPM Scientific
San Diego, CA
Be an early applicant
             +5 benefits
1 week ago
Senior Manager, Clinical Data Management
Senior Manager, Clinical Data Management
Regeneron
United States
Actively Hiring
5 days ago
Senior Clinical Data Manager
Senior Clinical Data Manager
University of Chicago
Chicago, IL
Actively Hiring
             +5 benefits
1 week ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
SPECTRAFORCE
Alameda, CA
Be an early applicant
1 day ago
Principal Clinical Data Manager
Principal Clinical Data Manager
eteraflex connects
Waltham, MA
2 weeks ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Denali Therapeutics
South San Francisco, CA
2 weeks ago
Manager, Clinical Data Management
Manager, Clinical Data Management
Deciphera Pharmaceuticals
Waltham, MA
Actively Hiring
13 hours ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Dianthus Therapeutics, Inc.
United States
Medical insurance
             +7 benefits
6 days ago
Principal Clinical Data Manager (part-time)
Principal Clinical Data Manager (part-time)
SRG
United States
Actively Hiring
2 weeks ago
Senior Clinical Data Coordinator (Data Manager)
Senior Clinical Data Coordinator (Data Manager)
The Fountain Group
Alameda, CA
Be an early applicant
             +3 benefits
6 hours ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
GForce Life Sciences
Oakland, CA
Be an early applicant
             +3 benefits
8 hours ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Revolution Medicines
San Francisco Bay Area
Actively Hiring
2 weeks ago
Clinical Data Manager III
Clinical Data Manager III
Intuitive
Sunnyvale, CA
Actively Hiring
5 days ago
Manager, Clinical Data and Reporting Standards (Remote)
Manager, Clinical Data and Reporting Standards (Remote)
AbbVie
Chicago, IL
Actively Hiring
2 weeks ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Bayside Solutions
San Francisco County, CA
Actively Hiring
1 week ago
Manager, Clinical Data Management
Manager, Clinical Data Management
IntePros
Cambridge, MA
Actively Hiring
1 week ago
Manager, Clinical Data Management
Manager, Clinical Data Management
Regeneron
United States
Actively Hiring
1 week ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Bayside Solutions
Hayward, CA
Actively Hiring
1 week ago
Manager, Clinical Data Management
Manager, Clinical Data Management
Lifelancer
New York, NY
1 week ago
Principal Clinical Data Manager, (Contract, Remote)
Principal Clinical Data Manager, (Contract, Remote)
Elan Solutions
California, United States
1 month ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Bayside Solutions
San Mateo County, CA
Actively Hiring
1 week ago
Director, Clinical Data Management
Director, Clinical Data Management
Kyverna Therapeutics
Emeryville, CA
1 week ago
Sr. Clinical Data Manager
Sr. Clinical Data Manager
Lifelancer
Redwood City, CA
6 days ago
Director, Clinical Data Management
Director, Clinical Data Management
Warman O'Brien
United States
Actively Hiring
2 weeks ago
Contract Clinical Data Manager (Remote)
Contract Clinical Data Manager (Remote)
Hydrogen Group
San Diego, CA
Actively Hiring
2 days ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Meet
Bridgewater, NJ
Actively Hiring
             +5 benefits
4 weeks ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
Intellectt Inc
Alameda, CA
Be an early applicant
9 hours ago
Senior Director, Clinical Data Management
Senior Director, Clinical Data Management
Barrington James
California, United States
Actively Hiring
             +12 benefits
2 weeks ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
HireTalent - Diversity Staffing & Recruiting Firm
Alameda, CA
Actively Hiring
1 day ago
Senior Manager, External Data Management - Clinical Data Science
Senior Manager, External Data Management - Clinical Data Science
Revolution Medicines
San Francisco Bay Area
Actively Hiring
2 weeks ago
AD/Director/Sr Director, Clinical Data Management
AD/Director/Sr Director, Clinical Data Management
Alterome Therapeutics, Inc
San Diego, CA
Medical insurance
             +3 benefits
2 days ago
Manager, Clinical Data Management
Manager, Clinical Data Management
Denali Therapeutics
South San Francisco, CA
1 week ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
GForce Life Sciences
Alameda, CA
Be an early applicant
             +3 benefits
1 day ago
Senior Clinical Data Manager
Senior Clinical Data Manager
Iovance Biotherapeutics, Inc.
San Carlos, CA
1 month ago
Senior Clinical Data Coordinator - C #: 24-08119
Senior Clinical Data Coordinator - C #: 24-08119
HireTalent - Diversity Staffing & Recruiting Firm
Alameda, CA
Be an early applicant
1 day ago
Sr. Manager, Clinical Data Management
Sr. Manager, Clinical Data Management
Terumo Neuro
Aliso Viejo, CA
1 day ago
Senior Manager, Clinical Data and Reporting Standards (Remote)
Senior Manager, Clinical Data and Reporting Standards (Remote)
AbbVie
Chicago, IL
Actively Hiring
1 week ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
TalentBurst, an Inc 5000 company
Alameda, CA
Be an early applicant
9 hours ago
Senior Manager, External Data Management - Clinical Data Science
Senior Manager, External Data Management - Clinical Data Science
Revolution Medicines
San Francisco Bay Area
Actively Hiring
1 week ago
Senior Director of Clinical Data Management (Hybrid)
Senior Director of Clinical Data Management (Hybrid)
GatedTalent - Connecting Top Executive Search Firms And Executives
San Diego, CA
Be an early applicant
4 hours ago
Senior Manager Clinical Data Management
Senior Manager Clinical Data Management
Barrington James
Boston, MA
Be an early applicant
             +12 benefits
10 hours ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
Planet Pharma
Alameda, CA
Be an early applicant
             +4 benefits
1 hour ago
Senior Clinical Data Coordinator
Senior Clinical Data Coordinator
ProKatchers LLC
Alameda, CA
Actively Hiring
1 day ago
Director, Clinical Data Management
Director, Clinical Data Management
EPM Scientific
New York, NY
Actively Hiring
2 days ago
See more jobs
You've viewed all jobs for this search
Search similar titles
Data Quality Manager jobs
Clinical Research jobs
Senior Associate jobs
More searches
More searches
Data Quality Manager jobs
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Sign in to view more jobs
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
"MINESWEEPER AND BIG DATA IN TESTING – THE ROADMAP TO ""COGNITIVE TESTING"" (CHAPTER 1)","MINESWEEPER AND BIG DATA IN TESTING – THE ROADMAP TO ""COGNITIVE TESTING"" (CHAPTER 1)
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
MINESWEEPER AND BIG DATA IN TESTING – THE ROADMAP TO ""COGNITIVE TESTING"" (CHAPTER 1)
Report this article
Shay Cohen
Shay Cohen
Head of Competency Center
Published Dec 19, 2018
+ Follow
Article by @Pamella Admoni , @Shay Cohen
Thanks to Bill Gates’ lack of budget for Windows Entertainment Pack, we got Minesweeper in the 90s. I spent my childhood loving the strict rules of this game and getting an adrenaline boost from the random tough choices I had to make. If you liked it, too, you will definitely relate…
We are constantly collecting data. We watch TED talks, listen to podcasts, browse social networks… We know how empowering information can be. IT systems across all industries have vast amounts of data in pre-production and production. Do you have systems that analyze data? Systems that provide you with recommendations for improvements?
In the digital era, testing must be fast, agile and intelligent. Data can help us achieve this goal. But let’s face it: how much sand do we need to sift through to find what we’re looking for? And where do we start in the “cognitive testing” journey?
The intelligence road starts with Big data and analysis. Testing data exists in many forms: defects, test cases, test scenarios etc... BI systems should analyze different sources of testing data to produce valuable reports with recommendations and predictions. This can give business an edge, whether starting a new project or an ongoing project requires an immediate improvement.
At Amdocs, our minesweeper is BEAT™ Analytics. It’s your first step toward leaping into “Cognitive Testing”. BEAT™ Analytics gives you the clues of the neighboring mines. As an industry leader, we have an extensive list of “red flags” for pre or post-production testing data. The failure statistics help us do risk-based testing, and while there were random tough decisions to make in Minesweeper, BEAT™ Analytics is different. Here you have a list of predefined and custom reports that give you precise analytics that guide you towards immediate actions in order to achieve the desired improvement.
Leveraging over 30 years of experience working with dozens of telco operators worldwide, we are able to benchmark testing recommendations with BEAT™ Analytics. Customers using BEAT™ Analytics benefit from predictive analytic reports providing insights about the expected number of defects, testing team performance, project burn down levels, areas that require maximum effort, business requirement coverage, project defect status, defect fix turnaround time, and RCA reports, to name a few.
Stay tuned for the next installments of “Cognitive Testing”, where we’ll walk through high-end technology such as AI, ML & NLP in testing.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
22
2 Comments
Anna Davidov
Product Manager at Zangula
1y
Report this comment
Shay, thanks for sharing!
Like
Reply
1 Reaction
Andrey Bronfin
Technology executive. Hiring top-notch developers and tech leads.
5y
Report this comment
Very interesting article indeed.. Awaiting the next installments of “Cognitive Testing”
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Christopher Bergh on LinkedIn: Data Observability and Data Quality Testing Certification,"Christopher Bergh on LinkedIn: Data Observability and Data Quality Testing Certification
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Christopher Bergh’s Post
Christopher Bergh
CEO & Head Chef, DataKitchen:  observe & automate every Data Journey so that data teams find problems fast and fix them forever!  Author: DataOps Cookbook, DataOps Manifesto.  Open Source Data Quality & Observability!
5mo
Report this post
Congrats Shurtin!
Shrutin Yemul
Data Analytics I Google Analytics I Business Intelligence
5mo
🎉 Excited to share that I have completed the Data Observability and Data Quality Testing Certification from DataKitchen, a leader in DataOps software and services! 📊✨
The learning experience was incredibly interesting, and the course content provided top-notch knowledge in the field.
A big thank you to
Eric Estabrooks
and
Christopher Bergh
for offering such a comprehensive and valuable course!
#DataObservability
#DataQuality
#DataOps
#ContinuousLearning
#ProfessionalGrowth
https://lnkd.in/dyHdbeWn
Data Observability and Data Quality Testing Certification
learn.datakitchen.io
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
Mitali Gupta
Data Engineer | Data Analyst | Business Intelligence | Data Visualization
4mo
Edited
Report this post
🔠 ABCs of DE: T is for Testing 

Let’s talk about Testing today — an often overlooked but crucial part of analytics engineering.

When I first started working with data models, I barely tested them, running only a handful of tests on large projects. I've since learned that robust data testing is essential. Why? Because we make many assumptions in our work: that data is current, models are sound, and transformed data is accurate. Without testing, we’re operating blindly.

So, what should you test? Here’s a quick guide through the stages of testing adoption:

▶ Laying the Foundation: Start with primary keys and accepted values to catch basic issues.
▶ Getting Proactive: Address data freshness and domain-specific problems with custom schema tests.
▶ Instilling a Culture of Testing: Make testing a team habit and integrate it into your CI/CD processes.
Confronting Alert Fatigue: Avoid over-testing by auditing and adjusting your tests.
▶ The Goldilocks Zone: Achieve a balanced and consistent testing approach.
Testing isn’t just about finding faults; it’s about building confidence in your data and processes. Let’s give data testing the attention it deserves!

Remember, a well-tested data model is like a well-tuned instrument – it plays a symphony of insights that drive better decisions. So, keep testing and keep growing!

To step up your game in data engineering, get the
DataExpert.io
Data Engineering bootcamp with
Zach Wilson
now. Monthly subscriptions are available at $125/month.
#DataEngineering
#Testing
#DBT
#DataQuality
#Analytics
#ABCsOfDE
#LinkedInLearning
20
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Rudolf Groetz
#Guild Lead Engineering (Test & Test Automation) at Raiffeisen Bank International AG#charmingOrganizer of the TestBustersNightVIENNA #communityBuilder #mentor #instructor@TestAutomationU
4mo
Report this post
🌟 Announcing Day 2 of TestBustersDays: Focus on Test Data Management 🌟

We are excited to announce that Day 2 of TestBustersDays will be dedicated to the critical topic of Test Data Management, with
DATPROF - Test Data Simplified
as our track sponsor! 

Join us this October to dive deep into the challenges and solutions surrounding test data in software testing.

🔍 Why Test Data Management Matters: 
Test data management is a significant hurdle in software testing, requiring careful handling to ensure data compliance, protect sensitive information, and maintain data integrity. With evolving regulations and the increasing complexity of software systems, managing test data efficiently is more important than ever.

🗣 Featured Talk: ""Addressing Test Data Management Challenges with DATPROF"" 

In this session, DATPROF will address the advanced test data challenges in today's agile testing world. 

Attendees will learn how to:
- Ensure test data compliance and protect sensitive information.
- Maintain data integrity while making test data accessible.
- Empower engineers to manage their own test data without requiring extra storage.

Imagine having your own test data set in the palm of your hands, accessible whenever you need it. DATPROF will demonstrate how their solutions provide control and accessibility, streamlining your testing processes.

Don't miss this opportunity to gain insights from industry leaders, learn about the latest tools and techniques, and network with peers. Let's tackle the challenges of test data management together!
#TestBustersDays
#TestDataManagement
#SoftwareTesting
#DATPROF
#DataCompliance
#DataIntegrity
#SoftwareEngineering
#Community
#Innovation
4
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
SAS
914,459 followers
8mo
Report this post
Today's theme for
#DataOpsWeek
is ✨ Testing Automation.✨ 

In
#DataOps
, having functional assets and pipelines is just the beginning. Dependencies and unforeseen scenarios can derail operations. That's why automated testing and monitoring are non-negotiable. They ensure seamless integration, catch potential issues before they escalate and keep teams informed in real-time.

For further exploration and resources, check out this SAS Communities page
http://2.sas.com/6048kpzjE
and blog post on 5 pillars for a successful DataOps strategy.
http://2.sas.com/6041kpzjH
#Automation
35
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Lars Skaar
Principal solution architect at SAS who cares about operationalizing analytics and data through the cloud and agile practices
8mo
Report this post
Check out our latest post on how to integrate test automation into your DevOps pipeline as part of your
#DataOps
initiative. We recently celebrated
#DataOpsWeek
and shared our experience and guidance on the 5 pillars of
#DataOps
that we rely on for our data and AI platform
#SASViya
. Follow the link to learn more!
https://lnkd.in/eQf_2aZ9
SAS
914,459 followers
8mo
Today's theme for
#DataOpsWeek
is ✨ Testing Automation.✨ 

In
#DataOps
, having functional assets and pipelines is just the beginning. Dependencies and unforeseen scenarios can derail operations. That's why automated testing and monitoring are non-negotiable. They ensure seamless integration, catch potential issues before they escalate and keep teams informed in real-time.

For further exploration and resources, check out this SAS Communities page
http://2.sas.com/6048kpzjE
and blog post on 5 pillars for a successful DataOps strategy.
http://2.sas.com/6041kpzjH
#Automation
3
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
SERGEI ORLOV
Software QA Engineer at AIDispatcher, LLC
3w
Edited
Report this post
While sharing the basic purpose of data organization, JSON and  CSV formats differ significantly in their capabilities - from handling complex data structures to performance implications and file sizes. This article examines their key differences and unique advantages, helping you choose the right format for specific testing scenarios.
#DataEngineering
#JSON
#CSV
#TestAutomation
#SoftwareTesting
#DataManagement
#TestData
#TechComparison
#QualityAssurance
#DataStructures
#SoftwareDevelopment
#TestingTools
#TechStack
#DataFormats
#TechnicalTesting
#QA
#SDET
#TestFramework
#APITesting
#DataScience
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Shashank Mishra 🇮🇳
Data Engineer @ Prophecy🕵️♂️ Building GrowDataSkills 🎥 YouTuber (177k+ Subs)📚Teaching Data Engineering 🎤 Public Speaker 👨💻 Ex-Expedia, Amazon, McKinsey, PayTm
5mo
Report this post
Start of my Data Engineering journey wasn't that easy because I looked at the cream layer or high level overview of Data Engineering 🌈 like 👇🏻 
 
📍Extract data, implement business rules, load the outcome, and voila - you're done! ✌️

But The Reality of Data Engineering Was 🏋️♀️:
📍Deciphering the right frameworks based on whether data processing is batch or real-time
📍Grasping the data domain and the business value it offers
📍Creating decoupled components when crafting pipelines
📍Managing checkpoints and system state
📍Ensuring scalability and robustness for long-term utility
📍Determining the frequency and schedule of tasks
📍Conducting a thorough suite of tests: unit testing, integration testing, smoke testing, and burst testing
📍Carrying out performance testing with high volume data
📍Maintaining continuous integration and deployment
📍Prioritizing data quality and validation
📍Effectively dealing with failures, including data replay and backfilling
📍Implementing comprehensive alerting and monitoring measures

How were your initial days in Data Engineering?

We at
Grow Data Skills
are conduction FREE MASTERCLASS (Open To All) on ""𝗛𝗢𝗪 𝗧𝗢 𝗨𝗦𝗘 𝗮𝗻𝗱 𝗜𝗡𝗧𝗘𝗚𝗥𝗔𝗧𝗘 𝗔𝗜 𝗜𝗡 𝗗𝗔𝗧𝗔 𝗔𝗡𝗔𝗟𝗬𝗧𝗜𝗖𝗦??""💡

🔥 Register For FREE Here -
https://lnkd.in/gE2hb43W
🗓 Date : 7-July-2024 
🕰️ Time - 7:00 PM IST 

Cheers -
Grow Data Skills
🙂
#dataengineering
#dataanalytics
#challenges
239
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Praveen Dorsala
Data Engineer @ Logward | Data Pipelines, ETL, PySpark, Python
1mo
Report this post
Today, I’m learning about the building blocks of data processing in Spark: RDD Operations! 🚀 Let’s break it down into simple terms.
Basic RDD Operations
RDDs support two types of operations:
1. Transformations 🛠️
These create a new dataset from an existing one. But here’s the trick—they don’t do any work until you actually need the results. It’s like making a to-do list without starting the tasks yet.
Examples: map (change each item), filter (keep only some items).
2. Actions ⚡
Actions trigger Spark to actually do the work and give you the final results. It’s like finally tackling your to-do list and seeing the outcomes.
Examples: collect (get all results), count (number of items).
Let’s See an Example 📚
Imagine we have a list of numbers, and we want to:
Double each number (map transformation).
Keep only numbers greater than 5 (filter transformation).
Get the results (collect action).
#PySpark
#BigData
#DataEngineering
#LearningJourney
9
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
4,290 followers
3000+ Posts
5 Articles
View Profile
Connect
More from this author
The DataOps Manifesto Reaches 5,000 Signatories and Now Translated To 14 Languages
Christopher Bergh
5y
Warring Tribes into Winning Teams: Improving Teamwork in Your Data Organization
Christopher Bergh
5y
Four Great DataOps Articles
Christopher Bergh
7y
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Rethinking Big Data Testing at Glint with Phoropter,"Rethinking Big Data Testing at Glint with Phoropter
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Rethinking Big Data Testing at Glint with Phoropter
Report this article
Jordan Bonilla
Jordan Bonilla
Software Engineer at Taoshi
Published May 12, 2022
+ Follow
Introduction
Glint
, acquired by LinkedIn in November 2018 and set to become part of Viva, Microsoft's employee experience platform, in 2023, is a People Success Platform that leverages real-time people data to help global organizations increase employee engagement, develop their people managers, and improve business results. Because of
GDPR
, Glint must store the data of its European Union (EU) customers in the EU. For this reason, Glint needed to build its own big data platform, independent from existing LinkedIn infrastructure such as DataHub
[1]
or
the Hadoop ecosystem
[2]
.
This led to the creation of
Spectacles -
Glint's new big data platform in Azure. Similar to other big data pipelines, Spectacles handles
ETL
requirements across rapidly evolving datasets. Additionally, the data output by Spectacles serves as a source of truth for many teams and requires high data integrity and reliability. This inspired the design of
Phoropter,
the big data testing suite we built in Azure on top of Spectacles. Because Phoropter needed to be independent from existing LinkedIn infrastructure, we were unable to leverage
Data Sentinel
[3],
LinkedIn’s big data testing solution
.
We hope that by discussing Phoropter's motivations, implementation and usage others are inspired to adopt its strategies to improve their big data pipeline testing.
Existing Testing Solutions
For this section we will compare Phoropter with the big data testing solutions of LinkedIn’s Data Sentinel and the publicly-available testing solution Deequ
[4].
Similarities
At first glance, Data Sentinel, Deequ,
and Phoropter are quite similar. The three solutions overlap in the following ways:
1. Ensure big data pipelines are running properly via assertions of aggregate properties on the datasets produced by those pipelines. These aggregate properties ensure all data is touched in the validation and are efficient to compute. These properties gauge overall data quality, business logic, or both. A few simple examples:
2. Use Apache Spark to efficiently verify aggregate properties of entire datasets, regardless of size. These properties are resistant to schema changes and do not need to check every value in a dataset against a maintenance-heavy “golden dataset” like in traditional unit testing.
3. Use configuration files to organize and store testing specifications.
Differences
Both Deequ (publicly-available) and Phoropter (LinkedIn-internal only) are accessible from Azure Data Factory unlike Data Sentinel. Phoropter and Data Sentinel natively support testing across two tables via joins whereas Deequ only runs on single tables. Phoropter is the only solution that natively allows collection of custom debug information to be displayed with test failures. Deequ is the only solution without a native notification/alerting mechanism. Phoropter is the only solution that does not use data mining/statistical analysis. However,  it compensates with specialized
slowly changing dimension
tests to detect anomalies for this commonly used big data storage strategy. Additionally, the powerful debug information collection feature in Phoropter is a large advantage over the other testing solutions.
The publicly available solution Deequ provides a good starting big data testing solution for any project but its simplicity makes it less suitable for big data pipelines that need advanced testing, alerting, and debugging features. Although testing across tables is possible, the developer would need to handle creation and deletion of an intermediate data table themself. Data Sentinel is a powerful big data testing option for those who are working at LinkedIn and develop on
UMP
or general
Azkaban
flows. Data Sentinel supports “User Defined Functions” and “Dataset Join Configurations” to give developers flexibility. The downsides for our specific use case, when compared to Phoropter, are lack of native support for slowly changing dimension tables and lack of customization of debug information. Although we would have used Data Sentinel if it were accessible from Azure Data Factory, Phoropter ended up being a better solution for our workload. Specifically, because our datasets:
Use slowly changing dimensions
Need powerful debugging capabilities due to rapidly evolving schemas and requirements
Are already within the Azure ecosystem.
Why Phoropter is Built in Azure
With a small development team, it was important for us to build Spectacles and Phoropter to leverage existing infrastructure solutions. This led us to the Azure ecosystem, where we were able to get this out-of-the-box infrastructure and focus our efforts on making the testing suite as robust as possible.
Within the Azure ecosystem, Phoropter integrates with
Azure Databricks
, and
Azure Synapse Analytics
to get autoscaling storage and compute resources.
Azure Data Factory
makes parallelization, synchronization, and error handling as simple as dragging a few graphical elements and selecting dropdown options. Due to this, the Phoropter’s PySpark source code ended up being concise and easy to work with.
The only prerequisite to Phoropter is to build on top of a data pipeline in Azure Data Factory (publically-available in all regions). Although Phoropter can be implemented on top of other platforms, building in Azure avoids potentially complex additional infrastructure integration. The following diagram gives you a look at how Azure Data Factory seamlessly integrates Phoropter tests.
Azure Data Factory (ADF) provides easy to use, visual abstractions for various infrastructure components and their integration with one another. Without having to worry about the underlying infrastructure, ADF gives you the flexibility of
custom pyspark scripts
in Databricks, Azure Synapse Analytics, Blob Storage and powerful
codeless data transformation
+ parallelization + synchronization + scaling
In summary, Phoropter is the combination of the following components, seamlessly integrated in the Azure ecosystem.
YAML configuration files that contain Phoropter testing specifications
Azure Databricks
Runs custom pyspark scripts that perform testing logic
Autoscaling clusters
Azure data factory pipelines
Provide abstraction for parallelization and synchronization of Databricks jobs
Microsoft Teams webhooks for alerting and debugging
Azure Synapse Analytics to store structure test output that contains results and debug info
Using Phoropter Tests Effectively
In this section, we explain best practices for Phoropter. The following strategies will apply to any big data testing solution. However, by using Phoropter all data stays within the Azure ecosystem, allowing increased performance and preventing redundant data copy that could be required with solutions like Data Sentinel. Additionally, using Phoropter keeps developers in the Azure ecosystem and frees them from unnecessary infrastructure and integration work.
Best Practice 1: Create and Test Intermediate Data
The key to using Phoropter effectively is to create and test data at intermediate stages of the underlying data pipeline, blocking downstream progress until all tests at the current stage pass. If any tests fail, production data is not affected and Phoropter is able to pass verbose debug information to developers who can quickly fix this issue and restart from the last successful intermediate stage. This facilitates debugging and maintainability, which is especially critical for rapidly-changing data and transformation logic.
Phoropter tests must be completed before the next ETL stage can begin. This allows detailed inspection of faulty data and helps pinpoint bugs. If bugs need to be fixed, the ETL pipeline can be restarted from the failed substep, preventing redundant computation. Phoropter stores all test results in Azure Synapse Analytics.
Note - the underlying ETL pipeline should
strategically
output data at intermediate phases where a significant amount of logic has been performed. Too many intermediate phases will cause performance degradation.
Best Practice 2: Parallelize over Tables
When the workload involves multiple tables, this strategy allows a significant performance boost. At each of the intermediate pipeline stages, Phoropter tests are run in parallel on an autoscaling Azure Databricks cluster where developers have full control of debug information, alerts, and error handling. The following figure gives a more detailed overview of this process.
This example ETL data pipeline is transforming tables in parallel in ADF. As data is output to intermediate data tables, Phoropter tests are immediately triggered on the Databricks cluster using an asynchronous
“Execute Pipeline” activity
which runs Phoropter logic via Azure Databricks. The Databricks cluster runs tests on these intermediate data tables in parallel.
Implementing parallelization and synchronization with
Phoropter
in ADF is as simple as dragging and dropping a
ForEach activity
and “Execute Pipeline” activity. Because  tests are non-blocking,
Phoropter
requires a synchronization activity as illustrated in Figure 2 to wait for all tests to complete.
Best Practice 3: Run as Many Tests as Possible
It is a good practice to design tests to be compatible with as many tables as possible so that they can be run by default regardless of the data’s structure. By having many of our tests run by default, we were able to increase test coverage and find many unexpected errors. The following table shows examples of some of these “universally compatible” tests and the bugs they unexpectedly uncovered.
Example Phoropter Data Walkthrough
For the sake of simplicity, imagine an ETL pipeline that had the job of importing a single table - “
user_email_addresses
” with the columns
user_uuid
,
user_email
, hashing the email addresses, and saving the new data in a new table with an updated timestamp per row
last_modified_time
. A naive data pipeline might perform all these steps at once and only create one final table which gets tested after being created.
If, instead, we choose to build this ETL pipeline with Phoropter tests, the flow of data would look very differently. The data would be split up into intermediates and tested before moving to the next step. The outputs of Phoropter are saved in a “test results” table in Azure Synapse Analytics. Failures are detected by filtering for rows with “
success
=0”. These rows include detailed debugging information.
The following diagram shows how to split such an ETL pipeline into separate phases for testing.
Data is split into two intermediate forms and tested twice before being written to its production table. If a failure occurs, its source can be pinpointed to a specific stage. Although it is not illustrated here, tests could be run on the final, persisted data. Yellow tables belong to the ETL pipeline. Blue tables belong to Phoropter.
Phoropter Performance
Parallelization is at the heart of this solution. All tests are run asynchronously as output tables are produced so that the impact on our data pipeline’s runtime is insignificant. Additionally, tests are run in parallel using in-memory Spark dataframes on an Azure Databricks cluster. This allows efficient in-memory aggregations on petabyte-scale large data sets
[5]
.
For Spectacles, all the
Phoropter
tests complete in 38 seconds using a 15 node cluster (
Standard_DS5_v2
, 16 cores each). Thus, when considering the overhead to run all tests in Spectacles, the upper bound is 38 seconds plus the time to load data into cluster memory. Note - the time required to load the data into cluster memory varies widely based on data format (e.g., parquet vs relational database).
Conclusion
A big data testing solution like
Phoropter
can be straightforward to build thanks to the Azure ecosystem. When deployed on a well-designed big data pipeline, it can lead to long-lasting success of the project as opposed to a mess of data quality/performance degradation and maintenance costs.
Phoropter’s
key features by category are:
Scalability
Run tests on auto-scaling Azure compute resources to handle arbitrary data sizes
Tests run in parallel across tables within sub-pipeline
Reliability
Tests designed for aggregate properties over
entire
datasets for full coverage and high confidence in data validity
Fine-grained testing options specified per table, including SCD support
Block downstream sub-pipeline on current sub-pipeline’s tests passing
Leverage intermediate data tables and checkpoints for fault tolerance
Maintainability
Detailed debug information auto-generated
Customizable failure alerting
Toggle tests as “warn” or “fatal”
Testing specifications organized by structured configuration files
Test results stored in Azure Synapse Analytics to compare metrics over time
Through the usage of
Phoropter
, our team has avoided countless hours of debugging, regenerating non-checkpointed data, and waiting for tests to complete. We hope others can use our design to gain confidence in their data pipeline output and help establish new industry best practices in data engineering.
Acknowledgements
We would like to thank all members of the Glint team for their feedback and experimentation with the Spectacles data platform, without them this project would not have been possible. Special thanks to
Arrash Yasavolian
,
Ken Smith
,
Rukun Fan
, and
Yiqi Meng
for their continued guidance and support.
References
[1]
https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained
[2]
https://engineering.linkedin.com/blog/topic/hadoop
[3]
https://engineering.linkedin.com/blog/2020/data-sentinel-automating-data-validation
[4]
https://github.com/awslabs/deequ
[5]
https://spark.apache.org/
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
25
2 Comments
Yiqi Meng
Building next-gen commercial softwares at GM
2y
Report this comment
Amazing piece! Go Ginyu!
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Key Considerations on Big Data Application Testing,"Key Considerations on Big Data Application Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Key Considerations on Big Data Application Testing
Report this article
Md. Taukir Hasan
Md. Taukir Hasan
Product & Project Management | Solution Design | Strategy | Operational Service Delivery | FinTech | Payment | Digital Transformation | Scrum
Published Dec 6, 2016
+ Follow
2016 is emerging as the year of Big Data. Those leveraging big data are sure to surge ahead while those who do not will fall behind. According to the Viewpoint Report, “76% (of organizations) are planning to increase or maintain their investment in Big Data over 2 – 3 years”. Data emerging from social networks, mobile, CRM records, purchase histories etc. provide companies with valuable insights to uncover hidden patterns that can help enterprises chart their growth story. Clearly, when we are talking about data, we are talking about huge volumes that amount to almost petabytes, exabytes and sometimes even zettabytes. Along with this huge volume, this data which originates from different sources also needs to be processed at a speed that will make it relevant to the organizations. To make this enterprise data useful, it has to be projected through the users via applications.
As with all other applications, testing forms an important part of Big Data applications as well. However, testing Big Data applications has more to do with verification of the data rather than testing of the individual features. When it comes to testing a Big Data application, there are a few hurdles that we need to cross.
Since data information is fetched from different sources, for it to be useful, it needs live integration. This can be achieved by end-to-end testing of the data sources to ensure that the data used is clean, data sampling and data cataloging techniques are correct and that the application does not have a scalability problem. Along with this, the application has to be tested thoroughly to facilitate live deployment.
The most important thing for a tester, testing a big data application thus becomes the data itself. When testing Big Data applications, the tester needs to dig into unstructured or semi-structured data with changing schema. These applications can also not be tested via ‘Sampling’ as in data warehouse applications. Since Big Data applications contain very large data sets, testing has to be done with the help of research and development. So how does a tester go about testing Big Data applications?
To begin with, testing of Big Data applications demand the testers to verify the large volumes of data by employing the clustering method. The data can be processed interactively, real-time or in batches. Checking the quality of data also becomes of critical importance to check for accuracy, duplication, validity, consistency, completeness etc. We can broadly divide Big Data application testing into three basic categories:
1.
Data Validation:
Data Validation, also known as the Pre-Hadoop testing, ensures that the right data is collected from the right sources. Once this is done, the data is then pushed into the Hadoop system and tallied with the source data to ensure that they match in this system and are pushed into the right location.
2.
Business Logic validation:
Business logic validation is the validation of “Map Reduce” which is the heart of Hadoop. During this validation, the tester has to verify the business logic on every node and then verify it against multiple nodes. This is done to ensure that the Map reduce process works correctly, data segregation and aggregation rules are correctly implemented and key value pairs are generated correctly.
3.
Output validation:
This is the final stage of Big Data testing where the output data files are generated and then moved to the required system or the data warehouse. Here the tester checks the data integrity, ensures that data is loaded successfully into the target system, and warrants that there is no data corruption by comparing HDFS file system data with target data.
Architecture Testing forms a crucial part of Big Data Testing as a poor architecture will lead to poor performance. Also, since Hadoop is extremely resource intensive and processes large volumes of data, architectural testing becomes essential. Along with this, since Big Data applications involve a lot of shifting of data, Performance Testing assumes an even more important role in identifying:
a) Memory utilization
b) Job completion time
c) Data throughput
When it comes to Performance Testing, the tester has to take a very structured approach as it involves testing of huge volumes of structured and unstructured data. The tester has to identify the rate at which the system consumes data from different data sources and the speed at which the Map-Reduce jobs or queries are executed. Along with this, the testers also have to check the sub-component performance and check how each individual component performs in isolation.
Performance testing a Big Data Application needs the testers take a defined approach that begins with:
Setting up of the application cluster that needs to be tested.
Identifying the designing the corresponding workloads.
Preparing individual custom scripts.
Executing the test and analyzing the results.
Re-configuring and re-testing components that did not perform.optimally.
Since the testers are dealing with very large data sets that originates from hyper-distributed environments, they need to make sure that they verify all this data faster. To enable that, testers need to automate their testing efforts. However, since most of the automation testing tools are yet not skilled enough to handle unexpected problems that could arise during the testing cycle and the absence of a single tool that can perform the end-to-end testing, automating Big Data application testing requires technical expertise, great testing skills, and knowledge.
Big Data applications hold much promise in today’s dynamic business environment. But to appreciate its benefits testers have to employ the right test strategies, improve testing quality and identify defects in the early stages to deliver not only on application quality but cost as well.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
2
To view or add a comment,
sign in
More articles by this author
No more previous content
Highly Efficient Product Organizations
Aug 22, 2023
The Many Hats of Product Managers
Aug 20, 2023
Ultimate Testing Checklist
Dec 19, 2016
SDLC Methodologies in Software Testing
Dec 12, 2016
Agile Terminologies and Definitions – Complete Glossary
Dec 12, 2016
Difference Between Scrum, Kanban and XP
Dec 12, 2016
Definitive Guide to Writing Good Agile User Stories
Dec 12, 2016
Pros and Cons of Test Driven Development (TDD)
Dec 12, 2016
BDD Guidelines and Best Practices
Dec 12, 2016
Roles and Responsibilities of a Product Owner in Agile
Dec 12, 2016
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
The 6+3 dimensions of data quality assessment,"The 6+3 dimensions of data quality assessment
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
The 6+3 dimensions of data quality assessment
Report this article
Ankur G.
Ankur G.
Product Marketing at Securiti AI | Enabling Safe Use of Data and AI
Published Apr 8, 2021
+ Follow
Measuring
data quality
is critical to understand if you want to use enterprise data confidently in operational and analytical applications. Only good quality data can power accurate analysis, which in turn can drive trusted business decisions.
According to one
Gartner estimate
, poor data quality can result in additional spend of $15M in average annual costs. Although it is not just about financial loss. Poor quality of data affects your organization at multiple levels:
Higher processing cost: The
rule of ten
states that it costs ten times as much to complete a unit of work when the data is flawed than when the data is perfect
Unreliable analysis: With lower confidence in reporting and analysis, bottomline management is never easy
Poor governance and compliance risk: Compliances are no longer optional, and business survival gets challenging without them
Loss of brand value: When organizations constantly make erroneous operations and decisions, the brand value decreases quickly
Bad quality data impacts an organization’s business strategy of fueling growth and driving innovation. The immediate concern is how an organization can measure data quality and find ways to improve it.
How is data quality measured?
Data quality may be easy to recognize but it is difficult to determine precisely. You can consider multiple attributes of data to get the correct context and measurement approach to data quality. For example, patient data in healthcare must be complete, accurate, and available when required. For a marketing campaign, customer data needs to be unique, accurate, and consistent across all the engagement channels. Data quality dimensions capture the attributes that are specific to your context.
What is a data quality dimension?
Data quality dimensions are measurement attributes of data, which you can individually assess, interpret, and improve. The aggregated scores of multiple dimensions represent data quality in your specific context and indicate the fitness of data for use.
On average
, 47% of recently created data records have at least one critical (e.g., work-impacting) error. High-quality data is the exception, with only 3% of the DQ scores rated acceptable (with >97% acceptability score). So, only 3% of companies’ data meets basic quality standards.
Scores of data quality dimensions are typically expressed in percentages, which set the reference for the intended use. For example, when you use 87% accurate patient data to process billing, 13% of the data cannot guarantee you correct billing. In another example, a 52% complete customer data set implies lower confidence in the planned campaign reaching the right target segment. You can define the acceptable levels of scores for building more trust in data.
Data quality dimensions serve as a guide for selecting the most suitable dataset. When presented with two datasets of 79% accuracy and 92% accuracy, analysts can choose the dataset with higher accuracy to ensure that their analysis has a more trusted foundation.
What are the 6 dimensions of data quality?
You can measure data quality on multiple dimensions with equal or varying weights, and typically the following six key dimensions are used.
1.   Completeness
This dimension can cover a variety of attributes depending on the entity. For customer data, it shows the minimum information essential for a productive engagement. For example, if the customer address includes an optional landmark attribute, data can be considered complete even when the landmark information is missing.
For products or services, completeness can suggest vital attributes that help customers compare and choose. If a product description does not include any delivery estimate, it is not complete. Financial products often include historical performance details for customers to assess alignment with their requirements. Completeness measures if the data is sufficient to deliver meaningful inferences and decisions.
2. Accuracy
Data accuracy is the level to which data represents the real-world scenario and confirms with a verifiable source. Accuracy of data ensures that the associated real-world entities can participate as planned. An accurate phone number of an employee guarantees that the employee is always reachable. Inaccurate birth details, on the other hand, can deprive the employee of certain benefits.
Measuring data accuracy requires verification with authentic references such as birth records or with the actual entity. In some cases, testing can assure the accuracy of data. For example, you can verify customer bank details against a certificate from the bank, or by processing a transaction. Accuracy of data is highly impacted on how data is preserved through its entire journey, and successful
data governance
can promote this dimension of data quality.
High data accuracy can power factually correct reporting and trusted business outcomes. Accuracy is very critical for highly regulated industries such as healthcare and finance.
3. Consistency
This dimension represents if the same information stored and used at multiple instances matches. It is expressed as the percent of matched values across various records. Data consistency ensures that analytics correctly capture and leverage the value of data.
Consistency is difficult to assess and requires planned testing across multiple data sets. If one enterprise system uses a customer phone number with international code separately, and another system uses prefixed international code, these formatting inconsistencies can be resolved quickly. However, if the underlying information itself is inconsistent, resolving may require verification with another source. For example, if a patient record puts the date of birth as May 1st, and another record shows it as June 1st, you may first need to assess the accuracy of data from both sources. Data consistency is often associated with data accuracy, and any data set scoring high on both will be a high-quality data set.
4. Validity
This dimension signifies that the value attributes are available for aligning with the specific domain or requirement. For example, ZIP codes are valid if they contain the correct characters for the region. In a calendar, months are valid if they match the standard global names. Using business rules is a systematic approach to assess the validity of data.
Any invalid data will affect the completeness of data. You can define rules to ignore or resolve the invalid data for ensuring completeness.
5. Uniqueness
Are data duplicated only when necessary? Do different identifiers always represent different things? This dimension indicates if it is a single recorded instance in the data set used. Uniqueness is the most critical dimension for ensuring no duplication or overlaps. Data uniqueness is measured against all records within a data set or across data sets. A high uniqueness score assures minimized duplicates or overlaps, building trust in data and analysis.
Identifying overlaps can help in maintaining uniqueness, while data cleansing and deduplication can remediate the duplicated records. Unique customer profiles go a long way in offensive and defensive strategies for customer engagement. Data uniqueness also improves data governance and speeds up compliance.
6. Integrity
Data journey and transformation across systems can affect its attribute relationships. Integrity indicates that the attributes are maintained correctly, even as data gets stored and used in diverse systems. Data integrity ensures that all enterprise data can be traced and connected.
Data integrity affects relationships. For example, a customer profile includes the customer name and one or more customer addresses. In case one customer address loses its integrity at some stage in the data journey, the related customer profile can become incomplete and invalid.
While you regularly come across these six data quality dimensions, many more dimensions are available to represent distinctive attributes of data. Based on the context, you can also consider data conformity to standards (do data values comply with the specified formats?) for determining data quality. Data quality is multi-dimensional and closely linked with
data intelligence
, representing how your organization understands and uses data.
Measuring data quality dimensions helps you identify the opportunities to improve data quality. With adaptive rules and a continuous ML-based approach,
predictive data quality
brings you trusted data to drive real-time, consistent, innovative business decisions.
Beyond accuracy: What data quality means to data consumers
Data quality from the perspective of data producers and managers focuses mostly on accuracy. Matching data as closely as possible to the real-world entity is their goal. Their data cleaning, fixing, and management efforts are directed towards improving data accuracy.
From
data consumers’ perspective
, we should add three more dimensions to data quality. When data consumers shop for quality data, their challenges are more oriented to the supply-chain of data. Their first requirement is data
accessibility
. They want to know where data resides and how to retrieve it.
Their next concern is
timeliness
. Are the attribute values of the right age to execute the business process? The value of data is in using it. Accessible data has no value if it is not available for timely use. Timeliness defines if the data is available when required. Trusted data available in real-time or near real-time can reduce errors and streamline operational processes. Timely data availability can drive successful business innovation and maintain a competitive edge.
Data consumers want to access data when they want, and they want the most recent data to power their projects.
Source: Gartner (Aug 2020) - Data Quality Fundamentals for Data & Analytics Technical Professionals
Once data consumers are satisfied with data accessibility and timeliness, their focus shifts to
relevance
. They want to shop for data that correctly aligns with their requirements. They do not want to waste their efforts on data that is not immediately relevant to their planned projects. Only then comes data accuracy, which ensures that the selected data will correctly deliver the results.
Going beyond accuracy, data producers and consumers jointly need to evolve a strategy that rethinks data quality. Data consumers must define what’s most important and creators must focus on delivering that most important data. They need to assess the factors impacting effective data shopping, and ask the following questions:
Is the data well understood?
Is it driven by
data intelligence
?
Does the data have sufficient
metadata
to understand how they can use data to power their specific analysis?
Can they access
data lineage
as the data moves between sources and goes through aggregations, manipulations and transformations?
Only then the data quality can be addressed successfully and improved continuously.
Interested in learning more about data quality assessment?
Schedule a data assessment.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
30
To view or add a comment,
sign in
More articles by this author
No more previous content
A CDO's Guide to Unstructured Data in the Generative AI Era
Feb 13, 2024
Data Protection 101: The Convergence of Security, Privacy, and Governance
Oct 31, 2023
Where Rivers Meet: Data and AI Confluence
Oct 21, 2023
7 Lessons from Top Enterprise SaaS CMOs
Oct 7, 2023
Data Warehousing 101: Tracing its Evolution to the Modern Day
Oct 2, 2023
Seven Lessons from My Father's ICU Stay: Navigating Life, Love, and Mortality
Sep 23, 2023
Why Data Observability is Your Business’s New Best Friend
Sep 21, 2023
Navigating the Evolution and Future of Machine Learning Infrastructure
Aug 15, 2023
The Ultimate Quest: Choosing the Right Database for Your Treasure of Data
Aug 9, 2023
Unleashing Possibilities: How Generative AI Impacts Enterprise SaaS
Jun 3, 2023
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Aligning ‘Big Data’ to application testing,"Aligning ‘Big Data’ to application testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Aligning ‘Big Data’ to application testing
Report this article
Charles Meyer Richter
Charles Meyer Richter
Principal information architect & diagnostician at Ripose Pty Limited
Published Aug 19, 2015
+ Follow
I know I wrote that my last post would possible be my last post but I could not resist posting this in response to the article written by Simon Brown called
Unit and integration are ambiguous names for tests
and especially to respond to his his question ‘What do you think about aligning the names of the tests with the things they are testing?'.  I will return to my bucket list after this (or until another person causes me to detour slightly).
So, talking about alignment how about considering this functional decomposition of this concept:
The application:
1) The application (app) is a simple static object encapsulated as an input-process-output engine (or ipo for short). It is a
function
that operates on one or more data elements, processes them and outputs some form of message.
2) The basic data element is a string of alpha numeric characters.
The input of elements into the function are zero, one or many data/facts/attributes and these input elements exist within a data base (represented by an encapsulation of data elements having a common property, namely they are all dependant on the unique row identifier of the data set), on an input screen or in a message that contains the raw data which originally existed only in the mind of a business operative (but where are these fully defined?).
The tests:
1) the ‘unit test’, in order to be as ‘simple static object', should only be responsible for processing the myriad of input elements and return a single output message. The output message is then passed on to the next application/function. So what the 'unit test' will be checking is the validity of all the input permutations and that the output is calculated according to the algorithm/formula set down by a business operative. But the logic behind the algorithm has to be fully understood by the business operative otherwise the application will never work, so where is this algorithm described? I will answer this question shortly.
2) Next the 'component test' checks that the output/message from the unit tested app/function, progresses to the next application/function or ends up as a series of messages (be they in text, video or audio) on an output medium (such as a screen (on any device capable of displaying data) or a hard copy of a report (on any type of printer).
3) The 'system test'  will check the validity of the life cycle of all elements as they pass through all the applications in the order in which they are created, read, updated or destroyed.
4) And finally the 'stress test' will check that the hardware is capable of processing multiple instances of the application simultaneously.
But what about aligning ‘Big Data’ to these tests?
To accomplish this, enterprise architects and their fellow technical architects will have to consider if their framework will answer the following questions:
But what about aligning the data elements within the data base to the corporate knowledge base so that all data elements actually reside in its appropriate knowledge class? Hence, it is here that the business operative can describe whether the element is raw data or needs an algorithm to calculate it. It will also give data base designers the opportunity to test various designs (by distributing the data elements into different data sets). Which in turn could trigger the change management regime (including migration plans) if the original database design had already been implemented.
And what about aligning the encapsulated systems with the corporate knowledge base? This will ensure that time is not wasted on developing multiple redundant systems.
And what about aligning the corporate knowledge base with the performance indicators? This will help measure the cost effectiveness of the data elements within each knowledge class and hence within each data base.
And then what about aligning the performance indicators to their appropriate key performance indicator? This will help encapsulate a number of performance indicators under a common tag and help prepare a preliminary budget.
And then what about aligning the key performance indicators to the business objectives? This will then provide proof positive that the data element being tested is of value and necessary.
And then what about aligning the value (a business objective) to the benefit that the value provides? This will help encapsulate a number of values under a common tag and assist in SWOT analysis.
And finally what about aligning the benefits to the primary purpose all the business operatives first dreamed of delivering? This creates the singularity and describes the complex evolving object called ‘the business/enterprise/organisation’.
Turn this conversation upside down and you have the anatomy of the Ripose Technique. The Caspar (computer assisted strategic planning and reasoning) engine provides the repository and processing for all the above mentioned components (except the testing part), but it also includes the pseudo code designer module which enables a software engineer (aka Ripose grade 6) to describe all the business operatives inputs, processes and outputs in a single unambiguous format.
Perhaps this is the definitive Hitchhiker's guide to 'Big Data'!
© 2015 Ripose Pty Limited. All rights reserved
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
To view or add a comment,
sign in
More articles by this author
No more previous content
How Ripose & Caspar Implemented The Zachman Framework
Sep 8, 2024
IBM’s Business System Planning
Implemented with Ripose & Caspar
Sep 2, 2024
The Golden Thread & The Knowledge of the Ancients
Aug 30, 2024
TRUST &/or FAITH
Aug 27, 2024
How Ripose & Caspar Implemented TOGAF & ArchiMate
Aug 26, 2024
Enterprise Architecture and Supporting Software vs Information Architecture
Aug 16, 2024
Information Architecture And Supporting Software
Aug 16, 2024
Systems Engineering Deliverables
Implemented with Ripose and Caspar
Aug 14, 2024
Benjamin Franklin’s Deliverable
Implemented with Ripose & Caspar
Aug 13, 2024
Ed Yourdon’s Deliverables Implemented with Ripose & Caspar
Aug 13, 2024
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data Bad Data? A Primer on Big Data Testing,"Big Data Bad Data? A Primer on Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Bad Data? A Primer on Big Data Testing
Report this article
Alex Rodov
Alex Rodov
CEO at QA Consultants
Published Oct 28, 2015
+ Follow
Because of the complex nature of both Big Data and the highly distributed, asynchronous systems that process it, organizations have been struggling to define testing strategies and to set up optimal testing environments.
National Research Council of Canada and the Data Sciences team at QA Consultants have completed a research project on the primary challenges of testing Big Data systems and proposed a methodology to overcome those challenges.
As a result
A Primer on Big Data Testing
has been published and can be found in the
Thought Leadership
section of our website or simply by clicking on the link below. Please keep in mind that this is a
primer
on key aspects of methods, tools and processes for Big Data testing and that research on this subject is continuing and will be published as it progresses.
http://qaconsultants.com/wp-content/uploads/2015/10/Primer-on-Big-Data-Testing.pdf
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
20
2 Comments
Vishal Mithaiwala
Data Ingestion Lead | SAFe 6 Agilist | Data Engineering | Data Warehousing | Data Analysis | Data Science | BigData | ETL | Azure | Databricks | Snowflake | Quality Assurance | Business System Analysis | ITIL |
9y
Report this comment
Amazing Article...Which describe insights of big data testing!
Like
Reply
1 Reaction
Hoshedar Mody
Sr. Cloud Engineering /Application Monitoring Consultant (Salesforce)
9y
Report this comment
Fantastic Alex !! Congrats 2 U.. please ensure you start a meet-up group on Testing Big Data and Analytics  .. and ensure that predictive Analytics could be used in Testing Big Data Applications .
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
White Hat, Black Hat, Cyberterrorists...
Nov 29, 2017
Cybercrime: Understanding the landscape
Oct 20, 2017
We cant afford to be surprised like this any longer
Sep 27, 2017
Why Software Projects Go Bad
Sep 13, 2017
Where is all this connectedness getting us?
Sep 6, 2017
Why Software Testing Matters to Us All
Aug 24, 2017
Demystifying Testing
Aug 17, 2017
Demystifying Testing
Aug 17, 2017
QA CONSULTANTS LAUNCHES A NEW WEB PLATFORM
Oct 15, 2015
We did it again!
Jul 27, 2015
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Automation of data quality testing,"Automation of data quality testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Automation of data quality testing
Report this article
Marina Veber
Marina Veber
Strategic Leader Driving Digital Transformation, Product Strategy, Innovation & Operational Excellence | Expert in Financial Services | Cross-Functional Team Leadership | MBA, Yale | MS/BS Computer Science, NYU | CFA
Published Oct 9, 2018
+ Follow
It took the bot two minutes.
At most financial services firms, it would take a human at least 11 hours. This is the power of Intelligent Automation (IA).
Over the past two years, I’ve been part of PwC’s rapidly expanding IA practice. I want to share with you the use case that sparked my interest in this topic: automation of data quality (DQ) testing. First, I want to thank
Jay Srinivasan
and
Ashish Balsaraf
. They’ve helped me implement this important use case as a proof-of-concept (PoC) at one of my clients—and helped me write this blog too.
Let’s start with why this matters so much.
All financial institutions place a lot of emphasis on compliance with Anti-Money Laundering (AML) regulations. Among other things, this includes monitoring transactions for AML risk using a transaction monitoring system. But if the transaction monitoring data is inaccurate, the process breaks down, introducing real risk for the firms.
To address the vulnerability, many firms test the data used for transaction monitoring against six key data quality dimensions—completeness, validity, accuracy, consistency, integrity, and timeliness—either by choice or as directed by a regulator.
Until now, DQ testing has been seen as a very complicated and time-intensive exercise. Figure 1, below, illustrates four components of this exercise: planning and scoping, data collection and data loading, test execution, and review and reporting. At least for now, automation won’t address the heavy lift involved in the planning and scoping process. But by deploying IA, financial services firms may be able to make their risk prevention programs much more consistently effective. Before IA, this wouldn’t have been practical.
Figure 1:
Data quality testing process - manual
You may wonder what DQ testing has to do with IA. It’s actually an ideal use case. DQ testing is a costly exercise, and existing DQ monitoring tools do not monitor against all six aforementioned DQ dimensions (for example, cross-hop testing, which falls under completeness). IA can fill the gap to conduct this type of testing periodically at a fraction of the cost of manual, incremental execution.
Figure 2, below, shows another iteration of the DQ process, using icons to indicate which steps can be automated. For the PoC, we automated steps seven and eight of the process for one product line, foreign currency, composed of 130 scripts. Manual execution of each script and documentation of results takes five minutes, at a minimum. Therefore it takes approximately 11 hours to execute all 130 scripts. In stark contrast, the PoC bot was not only able to execute all 130 scripts in under two minutes, but it was also able to compare the current results to the results of the previous run and highlight the differences in the same time.
In our testing, we saw IA achieve a 99.7% time saving. Obviously, every testing scenario is unique. But this is a clear indication that using IA to support periodic DQ testing may be a viable and economic option for identifying and proactively remediating DQ issues. This is very good news for financial services firms—and to the degree that it adds trustworthiness and reduces risk in the broader financial system, this is very good news for all of us.
Figure 2:
Data quality testing process - potential areas of automation
This is only one of many examples. If you are curious about IA and how it could be leveraged at your firm, please reach out or drop me a comment below.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
79
10 Comments
Ajnavi Kumar
Managing Partner
5y
Report this comment
Great article, very helpful.
Like
Reply
1 Reaction
★ Ernest S.
Cofounder at Veryfi, Inc. (We’re Hiring!) — Dilithium expert. Cabinet at United Federation of Planets.
5y
Report this comment
Great article! Thanks for sharing. ""99.7% time saving"" is a huge productivity booster + dramatic improvement in quality of the results.
Like
Reply
1 Reaction
2 Reactions
Aaron Evans
Test Automation Strategist
5y
Report this comment
Great write up.  Great process description.  Often people forget about data quality, but ""garbage in, garbage out"" or cases like this, it's not the validity of the data in, but the data out.  
ETL is another area that can benefit from data quality checks.
Like
Reply
1 Reaction
2 Reactions
Shashy Inukonda
Quality Engineering | Performance Engineering | Automation
5y
Report this comment
Good read...Automation is everywhere.
Like
Reply
1 Reaction
Dev Kumar
Senior Technologist
6y
Report this comment
Great article by a super talented person!! Keep it up and keep more of these coming..
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Exploring the Different Flavors of AI: From Machine Learning to Generative and Neuro-Symbolic Intelligence
Nov 26, 2024
From Objectives to Outcomes: Setting Up a Value Realization Framework
Nov 22, 2024
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Do Effective Data Quality Testing for Data Warehouse,"How to Do Effective Data Quality Testing for Data Warehouse
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Warehousing
What are the steps for effective data quality testing?
Powered by AI and the LinkedIn community
1
Define data quality requirements
2
Design data quality tests
3
Execute data quality tests
4
Analyze data quality results
5
Resolve data quality issues
6
Monitor and maintain data quality
7
Here’s what else to consider
Data quality testing is a crucial process for ensuring the accuracy, completeness, and consistency of data in a data warehouse. Poor data quality can lead to inaccurate reports, unreliable insights, and wasted resources. In this article, you will learn the steps for effective data quality testing and how to apply them to your data warehouse projects.
Top experts in this article
Selected by the community from 29 contributions.
Learn more
Dan Knox
View contribution
7
Kevin Tracey
Director, Data Architecture & Integration
View contribution
7
Srihari B.
Data Engineer | Cloud, Architect, AWS, PySpark, Kafka, Databricks, SQL, DataLake, ETL, Airflow, Manager
View contribution
6
See what others are saying
1
Define data quality requirements
The first step is to define the data quality requirements for your data warehouse. These are the standards and expectations that you have for the data, such as the format, range, validity, and uniqueness. You can use various sources to define these requirements, such as business rules, data models, data dictionaries, and stakeholder feedback. You should document these requirements clearly and communicate them to your data warehouse team.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Dan Knox
Copy link to contribution
Report contribution
Don't forget the freshness of the data is a quality requirement. Is it ok that it is 1 day old or do you need to make actions on this data more frequent.
…see more
Like
7
Kevin Tracey
Director, Data Architecture & Integration
(edited)
Copy link to contribution
Report contribution
Start a business glossary,  you will be glad you did.  It helps organizations define the business metadata and definitions needed for complex data systems and it also lets you define your KPIs and metrics for downstream Analytics.  Data Quality rules are easily defined once you agree on definition within the glossary.
…see more
Like
7
Srihari B.
Data Engineer | Cloud, Architect, AWS, PySpark, Kafka, Databricks, SQL, DataLake, ETL, Airflow, Manager
Copy link to contribution
Report contribution
Enforcing data quality checks helps improve the trust in the data we serve to our users, catch the data issues early, reduce the amount of time we spend on addressing the data issue which ultimately makes the engineering teams to spend more time on developing new features for the users.   We can start with logical and business checks in our data pipelines. 

- Logical data checks:  These are aimed to ensure that there are no processing errors like partial loads, mismatch of data when compared to sources, avoiding null etc . 

- Business data checks: These checks should be developed by working with business users and understanding how the data will be used.  Examples of business checks could be primary key validations, avoiding bachelor etc.
…see more
Like
6
Kiran Peddireddy
Technical Solutions Consultant |CRM |CPQ
Copy link to contribution
Report contribution
Maintaining high-quality data is pivotal for informed decision-making and streamlined operations. Ensuring this data quality requires a systematic approach that underlines accuracy, consistency, and reliability. Below is a detailed guide on steps to undertake for efficacious data quality testing:

Clarifying Quality Dimensions: Before diving in, it's essential to define what constitutes ""quality"" for your dataset. Metrics that typically stand out include precision, completeness, reliability, consistency, and timeliness.

Set a Benchmark: Capture a snapshot of your current data quality. This snapshot will be indispensable when evaluating progress and effectiveness.
…see more
Like
3
Darren Hinde
Co-Founder Next Systems AI | AI Solutions Architect and Data Engineering
Copy link to contribution
Report contribution
Defining data quality requirements is an interactive process that involves constant communication between data engineers and stakeholders. 
In my experience using a data catalogue tool is crucial to manage expectations and documentation. 

Requirements can be different for different functions for example if the data is to be used for reporting or machine learning or real-time decision-making processes. 

Always prioritize the requirements that align with the business objectives.
…see more
Like
3
Yogita Khaitan
Tranformation Executive - digital business transformation, culture morphing, organization ways of working, Gen Z leadership coach, Generative AI consultant
Copy link to contribution
Report contribution
While it is important for your data team to understand requirements, it is important that the team looks at the solution beyond technology with the understanding of the industry or domain for which the data is being collated. They are in a better situation to validate the data ccuracy and then going forward insights that can be further derived from the data.
…see more
Like
2
Henk de Koning
Copy link to contribution
Report contribution
The first step, in my opinion, is to comprehend and model the semantics of your data. Without a proper understanding of the meaning of what you store, anything from ETL/ELT to querying will be of questionable quality.

So, meta-modeling is the first step and furtunately we are now technically in a state where we can actually formally describe semantics.
…see more
Like
2
Jeroen Bosems
Regional Solutions Architect Europe & Big Data Enthusiast
Copy link to contribution
Report contribution
As @Henk noted understanding your data is vital and is where most companies go wrong. A data team must have a full understanding of the data they’re trying to progress so that they can use frameworks like great expectations to increase their ability to test the data. This should be done with making minimal assumptions so it’s vital that business users are involved in both the data modelling as quality standards. Data engineering is not just building pipelines, it’s about modelling the business so that business processes and quality can be improved
…see more
Like
2
Shantanu Bhattacharya
Cyber Doctor✨ I Help Small Finance/Legal/Retail Businesses Fight Phishing/Ransomware. Get Action Plan & Budget Sensitive & Tailored Tools In 60 DAYS (25+Yrs Exp) 🌟Book A FREE Threat Awareness Call (Link in ""About"" 👇)
Copy link to contribution
Report contribution
There are times where it will be difficult to arrive at a single source of truth for the data. In these cases, the data warehouse should have a clear plan of handling it and still have valuable analysis and insights. Most of the large data warehouses in practice will end up this way since there are always situations where all data sources are not well integrated to eliminate source of truth issues. That’s not the end of the game. The organisation should be aware of the issues and well planned ways of handling them based on business requirements.
…see more
Like
1
Luciano Vasconcelos Filho
Engenheiro de dados e professor da Jornada
Copy link to contribution
Report contribution
The major challenge in implementing new quality processes is the gap in understanding: you don't know business needs fully, and those who do might not comprehend data quality. Our initial challenge is building synergy and earning the business sector's trust.

Here's a kickstarter plan:

- Find an urgent error.
- Create specific data quality tests / requirements.
- Run a quality check, expecting initial failure, highlighting the issue.
- Fix the problem.
- Retest for confirmation.
- Develop an evolving workflow, adding tests for new fixes.
- Merge this system into the main data process.

This method offers quick, reliable solutions, prevents repeat errors, and ensures consistent requirements and data quality tests, enhancing business trust.
…see more
Like
1
2
Design data quality tests
The next step is to design the data quality tests that will verify if your data meets the requirements. These are the checks and validations that you will perform on the data, such as data profiling, data cleansing, data transformation, and data reconciliation. You can use various tools and methods to design these tests, such as SQL queries, ETL scripts, data quality software, and test cases. You should document these tests and their expected outcomes and align them with your data quality requirements.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Wayne Taylor
Senior Director - Data Platform & Governance @ TAG - The Aspen Group | Bsc Honours in Computing
Copy link to contribution
Report contribution
Designing the quality tests and getting it right is hard.

There are so many places it can go wrong.

Data validation at source shouldn’t be forgotten about before we even talk about data quality tests.

Then on the tests ensuring the completeness and accuracy from source. Then as data evolves.

This may seem like an endless game of whack a mole but it’s worth it
…see more
Like
3
Darren Hinde
Co-Founder Next Systems AI | AI Solutions Architect and Data Engineering
Copy link to contribution
Report contribution
To make a data quality test your test must take into consideration that it includes the technical specifications and that the business rules are accounted for.  Document these requirements.

Having a deep understanding of the data needs is valuable to construct efficient and accurate data quality tests. Using automatic testing can help catch problems and help improve accuracy. 
Data quality is a constantly evolving task as requirements change and it is important to design around this so new requirements can be added and tested.
…see more
Like
1
Shantanu Bhattacharya
Cyber Doctor✨ I Help Small Finance/Legal/Retail Businesses Fight Phishing/Ransomware. Get Action Plan & Budget Sensitive & Tailored Tools In 60 DAYS (25+Yrs Exp) 🌟Book A FREE Threat Awareness Call (Link in ""About"" 👇)
Copy link to contribution
Report contribution
Incompleteness in understanding the mapping limitations from real world to data representation will exist in most real time non trivial systems. This might arise due the allied use cases not being addressed by the business’ systems. This is normal as the business might not have solutions for all possible allied use cases. However, it is important for the business to understand the limitations arising out of it and have a plan to address these in their own analysis.
…see more
Like
1
Luciano Vasconcelos Filho
Engenheiro de dados e professor da Jornada
Copy link to contribution
Report contribution
After iterating the initial process, the next step is expanding data quality tests, proactively identifying potential issues rather than waiting for bugs.

Define Checks and Validations: Engage in data profiling, cleansing, transformation, and reconciliation, ensuring data accuracy and consistency. Focus on constructing reusable scripts, avoiding manual tasks. You're aiming for automation at scale. You will do it 1000 times, not only one.

Document Tests: Log each test, success criteria, and expected results. 

Align with Data Quality Requirements: Make sure tests adhere to set standards, ensuring relevance and thoroughness.

This phase cements the quality control process, consistently meeting business demands and expectations.
…see more
Like
1
Yogita Khaitan
Tranformation Executive - digital business transformation, culture morphing, organization ways of working, Gen Z leadership coach, Generative AI consultant
Copy link to contribution
Report contribution
Data reconciliation is a very important step to determine outcome you set to achieve, making sure you are reconciling with accurate reports is hence important.
It is important to also validate with stakeholders the single source of truth with which your cleansed and transformed data would be validated. It is often enough observed that multiple systems carry redundant data and comparing your results as such with multiple sources can result in parity.
…see more
Like
1
Mohan Kumar
Enterprise Data Strategy, Architecture and Transformation Leader
(edited)
Copy link to contribution
Report contribution
Over the past eight years, Data Quality testing has shifted from ETL/ELT pipelines to centralized rules. Key practices include:

1. Starts with Centralized Data Quality Repository concept for consistent standards.
2. Use of 3rd party or custom tools/frameworks.
3. Perform Data Profiling to analyze quality and structure.
4. Define of validation rules aligned with business needs.
5. Document the test processes and expected outcomes.
6. Perform Data cleansing/enrichment (e.g., address standardization) techniques to address inconsistencies, errors, and duplicates in the data.
7. Perform Data transformation to meet business requirements.
8. Have Data reconciliation for remediation plan for violated data sets.
…see more
Like
3
Execute data quality tests
The third step is to execute the data quality tests that you have designed. This is the process of running the tests on the data and capturing the results. You can use various techniques and tools to execute these tests, such as batch processing, real-time processing, automated testing, and manual testing. You should ensure that you have a sufficient sample size and coverage of the data and that you follow a consistent testing procedure.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Darren Hinde
Co-Founder Next Systems AI | AI Solutions Architect and Data Engineering
Copy link to contribution
Report contribution
Executing data quality tests is a dynamic process and requires starting with manual testing and moving to automatic testing, depending on the complexity of the data. 

Ensure tests are repeatable and scalable to accommodate growing data volumes. Finally, document your testing process clearly for future reference and improvement.
…see more
Like
1
Luciano Vasconcelos Filho
Engenheiro de dados e professor da Jornada
Copy link to contribution
Report contribution
Now's the moment for frameworks to shine.

Select a Data Quality Tool (e.g., Great Expectations) and a Unit Test tool (like Pytest if you're a Python guy like me). Integrate them into your pipeline and your CI/CD process, and there you have it.

With every new data entry, KPI, or any addition, you'll maintain concise adherence to data quality standards. This precision streamlines the process, ensuring reliability and efficiency in your data operations.
…see more
Like
1
Mohan Kumar
Enterprise Data Strategy, Architecture and Transformation Leader
Copy link to contribution
Report contribution
Run the Data quality tests using Data Pipelines, 3rd Party data quality tools, or customized SQL queries.

Let's identify and align on the following in order to execute the Data Quality test.

1. What is the frequency you need to run your Data quality job or pipelines: Batch, real-time streaming, automated or Manual run.

2. Identify the sample size required to measure the accurate results: Sample size based on the data population and desired confidence. Use statistical technique to determine the sample size. Ensuring the sample data sets meets the required meets the reliable and actionable insights. Should have the comprehensive coverage across different data sources, attributes, and scenarios to capture a holistic view of data quality.
…see more
Like
4
Analyze data quality results
The fourth step is to analyze the data quality results that you have obtained from the tests. This is the process of reviewing the results and identifying the data quality issues and their root causes. You can use various methods and tools to analyze these results, such as dashboards, reports, metrics, and charts. You should evaluate the results against your data quality requirements and prioritize the issues based on their severity and impact.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Luciano Vasconcelos Filho
Engenheiro de dados e professor da Jornada
Copy link to contribution
Report contribution
Dashboards are helpful, but they're slow.

You need speed. Implement a proactive observability system and integrate it with your ticketing system. Did something go wrong? You need to be notified immediately and automatically.

Set up your system to send all logs and parameters directly to you, preventing wasted time searching for information and enabling prompt action. This real-time approach ensures that you're not just reacting to issues after the fact but are staying ahead, proactively maintaining the integrity and reliability of your data ecosystem.
…see more
Like
3
Darren Hinde
Co-Founder Next Systems AI | AI Solutions Architect and Data Engineering
(edited)
Copy link to contribution
Report contribution
Using a visual data quality dashboard can help present complex data issues in a simplified manner. 
This makes it easier for non-technical to also understand the issue at a high level.

Having tools that bring a common understanding of the issue at hand is important so the correct business decisions and priorities can be made.
…see more
Like
2
Mohan Kumar
Enterprise Data Strategy, Architecture and Transformation Leader
Copy link to contribution
Report contribution
Analyzing data quality involves:

1. Review the results against defined data quality tests, focusing on metrics like accuracy, completeness, consistency, and timeliness.
2. Identifying issues like inaccuracies or missing values and tracing their origins.
3. Using data visualization (dashboard/Reports/Metrics) for insights.
4. Comparing results to expected outcomes.
5. Categorizing and prioritizing issues to fix the data quality issues.
6. Developing action plans for resolution of failed data sets systematically.
…see more
Like
5
Resolve data quality issues
The fifth step is to resolve the data quality issues that you have identified from the analysis. This is the process of fixing the issues and improving the data quality. You can use various strategies and tools to resolve these issues, such as data cleansing, data correction, data enrichment, and data governance. You should document the actions and changes that you have made and verify that they have solved the issues.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Barış Güler
Serverless Team Lead @ Würth Cloud Services GmbH | Cloud-Native Architecture
Copy link to contribution
Report contribution
Considering “Synthetic Data” trained over a real dataset by internalized LLMs can be an opportunity for teams caring about the quality and the correctness of data itself. Generated near-real data may also reduce the lack of depth and increase the satisfaction in tests and encourage generalizing the approach for new test scenarios.
…see more
Like
2
Mohan Kumar
Enterprise Data Strategy, Architecture and Transformation Leader
(edited)
Copy link to contribution
Report contribution
To resolve the data quality issues, we need to have the approved and organization wide strategy as the data needs to be remediated by the business teams.

The violated data sets should be classified and stored based on business units or Data domain or Data Products (Life Cycle Assets journeys). 

Use enrichment techniques like Address Doctor.

Have the Compliance/Data Quality Analyst to review and create and the task for respective Business/Data Stewards/Remediation Analyst to remediate the respective data sets.

I would suggest to have the machine learning algorithms eventually take care of Compliance, Remediation Analyst and Business Owner approvals flow.
…see more
Like
1
6
Monitor and maintain data quality
The final step is to monitor and maintain the data quality of your data warehouse. This is the process of ensuring that the data quality remains high and consistent over time. You can use various practices and tools to monitor and maintain the data quality, such as data audits, data alerts, data lineage, and data stewardship. You should establish a data quality framework and a data quality culture that supports continuous improvement and collaboration.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Anubhav Majumdar
Senior Member of Technical Staff at Oracle | Analytics & Data Engineering | Ex - Amazon | Ex - Mu Sigma
Copy link to contribution
Report contribution
If you are invested in multiple projects, it is practically impossible to keep looking around everyday for misses in data.
Hence, set up an audit dashboard to be informed of data refreshes from source, any delays or misses. You can rely on CDC triggers/ control table identifiers to build this data.
Next, focus on building custom alerts at your BI/analytical layer to be informed of KPI deltas at an object level, for e.g weekly active users drop for a certain category page listed on an app. Use your custom test solution or orchestration tool for these alerts.
…see more
Like
2
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Paolo Iachia
Former IT Executive and Consultant
Copy link to contribution
Report contribution
In a complex environment -  multiple business units (BU), multinational, local data and IT systems - bringing etherogeneous data into the DWH would result in useless or misleading analytical output.
A resource consuming cleansing and standardization effort is a prerequisite to success.

Business sponsors and stakeholders must first determine the level of consolidation of the transactional information they want to analyze.

Static Data structures, such as product hierarchy, (inter-)national customer chains, group regional structure, must be defined centrally up front and implemented in each BU/System source of data.

Same products locally identified with different product references require creating cross-reference tables, where possible.
…see more
Like
2
Data Warehousing
Data Warehousing
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Warehousing
No more previous content
Your data warehouse holds critical information. How do you secure it without sacrificing performance?
1 contribution
You're knee-deep in a data warehouse project. How do you navigate stakeholder demands constantly in flux?
1 contribution
Your business needs are expanding rapidly. How do you keep your data warehouse scalable?
Your business priorities have shifted unexpectedly. How should you adapt your data warehousing roadmap?
Your data warehouse project is racing against the clock. Can you afford to compromise on data quality?
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Data Engineering
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Management
What is the difference between data profiling and data cleansing?
Data Management
What are the key steps to test a data warehouse?
Data Engineering
How can you maintain data quality in a master data file?
Data Architecture
How can you use data dictionaries to standardize data elements and attributes?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
3
29 Contributions"
How to Set Data Quality Testing Objectives,"How to Set Data Quality Testing Objectives
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Data Quality
What are your data quality testing objectives?
Powered by AI and the LinkedIn community
1
Identify data quality issues
Be the first to add your personal experience
2
Measure data quality levels
Be the first to add your personal experience
3
Ensure data quality compliance
Be the first to add your personal experience
4
Improve data quality continuously
Be the first to add your personal experience
5
Align data quality with business goals
Be the first to add your personal experience
6
Here’s what else to consider
Be the first to add your personal experience
Data quality testing is a crucial step in ensuring the reliability, accuracy, and usability of your data. It involves checking and validating your data against various criteria, such as completeness, consistency, conformity, accuracy, and integrity. But before you start testing your data, you need to define your data quality testing objectives. These are the specific goals and expectations that you want to achieve with your data quality testing process. In this article, we will discuss some common data quality testing objectives and how to set them.
Find expert answers in this collaborative article
Experts who add quality contributions will have a chance to be featured.
Learn more
See what others are saying
1
Identify data quality issues
One of the main objectives of data quality testing is to identify any data quality issues that may affect your data analysis, reporting, or decision making. Data quality issues can include errors, missing values, duplicates, outliers, inconsistencies, or violations of business rules or data standards. By identifying these issues, you can determine the root causes, the impact, and the possible solutions. You can use various data quality testing techniques, such as data profiling, data cleansing, data validation, or data reconciliation, to identify data quality issues.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
2
Measure data quality levels
Another objective of data quality testing is to measure the current levels of data quality in your data sources, systems, or processes. Measuring data quality levels can help you assess the performance, efficiency, and effectiveness of your data management practices. It can also help you monitor the progress and improvement of your data quality over time. You can use various data quality metrics, such as completeness, accuracy, consistency, timeliness, or relevance, to measure data quality levels. You can also use data quality scorecards, dashboards, or reports to communicate and visualize your data quality levels.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Ensure data quality compliance
A third objective of data quality testing is to ensure that your data complies with the relevant regulations, standards, or policies that govern your data usage, storage, or sharing. Data quality compliance can help you avoid legal, financial, or reputational risks that may arise from non-compliance. It can also help you build trust and confidence among your data stakeholders, such as customers, partners, or regulators. You can use various data quality testing methods, such as data auditing, data certification, or data governance, to ensure data quality compliance.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Improve data quality continuously
A fourth objective of data quality testing is to improve your data quality continuously and proactively. Improving data quality can help you enhance your data value, usability, and reliability. It can also help you optimize your data processes, systems, and resources. You can use various data quality testing tools, such as data quality software, frameworks, or best practices, to improve your data quality continuously. You can also use feedback loops, root cause analysis, or corrective actions to address and prevent data quality issues.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Align data quality with business goals
A fifth objective of data quality testing is to align your data quality with your business goals and needs. Aligning data quality with business goals can help you ensure that your data supports your strategic objectives, such as increasing revenue, reducing costs, or improving customer satisfaction. It can also help you prioritize your data quality efforts and resources based on the business value and impact of your data. You can use various data quality testing strategies, such as business-driven data quality, data quality dimensions, or data quality assessment, to align data quality with business goals.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Data Quality
Data Quality
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Quality
No more previous content
How can you identify and fill gaps in your data quality strategy?
58 contributions
How can you improve data quality maturity across domains and industries?
53 contributions
How do you tell stakeholders if your data is bad for machine learning?
52 contributions
How do you report data quality results to your audience?
62 contributions
How do you prioritize data quality when resources are limited?
69 contributions
How can you improve data quality documentation and metadata management?
61 contributions
How do you set data quality policies?
54 contributions
How can you establish and enforce data quality standards?
56 contributions
How can you compare data quality improvement techniques?
35 contributions
How can you build a data quality culture using a framework?
51 contributions
How do you automate data quality metrics for different platforms?
49 contributions
How are you improving your data quality?
45 contributions
How can you improve data quality when integrating multiple sources?
12 contributions
What are the best ways to transform data for different audiences?
33 contributions
How do you prepare for emerging data quality trends and challenges?
11 contributions
No more next content
See all
More relevant reading
Data Management
How can you maintain data quality after a data audit?
Analytical Techniques
How do you design and conduct data quality audits to ensure data accuracy and completeness?
Data Cleaning
What are best practices for data cleaning quality assurance and testing?
Data Governance
Your team is struggling with data quality. What can you do to help?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share"
Candidate Personality Testing: A Collision of Big Data and Analytics,"Candidate Personality Testing: A Collision of Big Data and Analytics
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Candidate Personality Testing: A Collision of Big Data and Analytics
Report this article
Danielle Larocca
Danielle Larocca
HCM Executive, Author, Speaker, SAP Mentor, SAP Certified Associate, SuccessFactors Certified Associate, SAP SuccessFactors Confidant, HXM Chairperson ASUG.
Published Jul 1, 2015
+ Follow
There is a great article June's issue of Time Magazine about personality screening for candidates.  the increase in formalized applicant testing is driven by a collision of two of the business world’s hottest trends in HCM: Big Data and Analytics.  Learn more in my blog:
http://www.spinifexit.com/2015/07/01/getting-a-job-in-the-era-of-optimized-hiring-whats-your-xq/
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
9
1 Comment
Tracey Buck
SAP Training Consultant
9y
Report this comment
Fabulous blog
Danielle Larocca
Really interesting and thoughtful. Impressive as usual.
Like
Reply
1 Reaction
To view or add a comment,
sign in
More articles by this author
No more previous content
Allow candidates to apply to your positions with one click
Nov 18, 2024
Prompt Engineering
Oct 12, 2024
Researching #AI and Outsmarting #BeautyBias
Aug 18, 2024
Revolutionizing HR: The Impact of AI Launch on the Human Resources Community
Jan 8, 2024
Did you know that I can speak five languages?
Dec 5, 2023
This is not a picture of me it's made entirely by AI
Sep 26, 2023
Listening to ""Predictions on the Post-COVID Workplace"" chat between Scott Galloway and Josh Bersin
May 31, 2023
Data Syncing for SuccessFactors
Feb 13, 2021
What is HXM Move?
Feb 2, 2021
The Journey to SuccessFactors
Nov 23, 2020
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Day 15: dbt Testing – Ensuring Data Quality with Built-in Tests,"Day 15: dbt Testing – Ensuring Data Quality with Built-in Tests
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Google Image
Day 15: dbt Testing – Ensuring Data Quality with Built-in Tests
Report this article
Surya Ambati
Surya Ambati
Lead Analyst at CRISIL Global Research & Analytics
Published Sep 9, 2024
+ Follow
As you build out your dbt (Data Build Tool) models, it’s critical to ensure that your data remains accurate, reliable, and of high quality. On Day 15 of our dbt journey, we’ll dive into
dbt’s built-in testing framework
, which makes validating your data easy and systematic.
Testing is a key part of data engineering, and dbt provides powerful tools to help you verify that your models are functioning correctly, without having to rely on manual checks.
Why Is Testing Important in dbt?
Data pipelines are dynamic, with frequent changes in upstream sources, business logic, and the structure of your models. Without testing, incorrect data can propagate through your pipeline, leading to inaccurate analyses, faulty reporting, or flawed business decisions.
By incorporating tests into your dbt models, you can:
Validate assumptions
about your data.
Catch anomalies
or data issues early in the process.
Prevent errors
from reaching downstream models and reports.
Automate
quality checks as part of your dbt workflow.
Types of Tests in dbt
dbt comes with two primary ways to test your data:
Schema Tests
Custom Data Tests
1. Schema Tests
Schema tests validate the structure and consistency of your data. These are simple, declarative tests that check specific characteristics of your tables or columns, such as uniqueness, non-nullability, or referential integrity.
Here are some common schema tests in dbt:
Unique
: Ensures that all values in a column are unique.
Not Null
: Ensures that no null values are present in a column.
Relationships
: Ensures that foreign key relationships are valid.
Accepted Values
: Ensures that a column only contains specific, predefined values.
Example: Schema Test for Uniqueness and Not Null
You can define schema tests directly in your schema.yml file. For instance:
In this example, we’re testing that the customer_id column in the customers table has only unique, non-null values.
2. Custom Data Tests
Custom tests allow you to define more complex logic, verifying that your data meets specific business rules or assumptions. These are SQL queries that return zero rows if the data passes the test and one or more rows if the test fails.
Example: Custom Test for Checking Data Ranges
You can create a custom data test by writing a SQL query. Let’s say you want to test if all sales figures in a sales table are positive:
If any rows are returned by this query, the test fails because it means there are negative sales amounts in your table.
Running Tests in dbt
Once you’ve defined your schema and custom tests, running them is simple. You can execute all your tests using the dbt test command:
This command runs all tests across your project, checking each model against the defined schema and custom tests.
Running Specific Tests
If you only want to run tests for a specific model, you can use:
This allows you to focus on testing certain parts of your project without running all tests.
Handling Test Failures
When a test fails, dbt will return details about which model and test failed, along with the problematic rows. This makes it easy to trace and fix data issues.
You can also configure dbt to fail the entire pipeline if a critical test fails, ensuring that incorrect data never moves downstream.
Best Practices for dbt Testing
Test Early and Often
: Integrate tests as soon as you build new models, ensuring that every model is validated before being used in production.
Leverage Schema Tests
: These are quick to set up and cover basic data integrity checks.
Write Custom Tests
: Define business logic and assumptions about your data and use custom tests to validate them.
Automate Testing
: Include tests in your CI/CD pipeline to automate data quality checks.
Conclusion
Testing in dbt is a crucial step in ensuring that your data remains accurate and reliable. By using dbt’s built-in schema tests and writing custom SQL-based tests, you can catch data issues early and prevent them from impacting your downstream processes. Running tests regularly as part of your workflow will give you confidence in the integrity of your data models.
#dbt #DataQuality #DataTesting #SQL #AnalyticsEngineering #DataPipeline #ETL #DataEngineering
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by this author
No more previous content
Day 19: Handling Errors in dbt
Oct 6, 2024
Day 18: Using dbt with GitLab CI/CD Pipeline
Oct 2, 2024
Day 17: Incremental Models in dbt - Efficient Data Processing for Large Datasets
Sep 14, 2024
Day 16: Using dbt Macros to Simplify Your Data Transformations
Sep 11, 2024
C++ Exercise 2: What is Concurrency and Why is it Important?
Sep 10, 2024
Article 3 - Mastering C++ : Understanding Conditional Logic such as Comparison Operators, Logical Operators, and Control Statements
Sep 9, 2024
Day 14: Advanced Jinja Techniques in dbt
Sep 8, 2024
Article 2: Mastering C++ Basics - Naming Conventions, Constants, Input Handling, Arrays, and Type Casting
Sep 8, 2024
Day 13: Building Modular dbt Projects
Sep 6, 2024
C++ Exercise 1 Continuation: Mastering Bitwise XOR Operations
Sep 6, 2024
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
No Title,erro
Technical Testing with Big Data,"Technical Testing with Big Data
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Technical Testing with Big Data
Report this article
Kushan Shalindra Amarasiri
Kushan Shalindra Amarasiri
Director Quality Engineering at Social Catfish
Published Apr 27, 2016
+ Follow
Big data, Internet of Things, Dev Ops are some of the trending topics in software development industry. In the previous article we looked upon the Quality Engineering methodology of Internet of Things. This article we will have an insight of how we can apply Quality Engineering in Big Data applications.
Big data is concept where organizations utilizes large amount of data (which will be structured or unstructured), which cannot be handled by commonly used software tools. The organization store, curate, manage, and process data within a tolerable elapsed time.
Volume.
Organizations collect data from a variety of sources, including business transactions, social media and information from sensor or machine-to-machine data. In the past, storing it would’ve been a problem – but new technologies (such as Hadoop) have eased the burden.
Velocity.
Data streams in at an unprecedented speed and must be dealt with in a timely manner. RFID tags, sensors and smart metering are driving the need to deal with torrents of data in near-real time.
Variety.
Data comes in all types of formats – from structured, numeric data in traditional databases to unstructured text documents, email, video, audio, stock ticker data and financial transactions ( What is Big Data -
http://www.sas.com/en_us/insights/big-data/what-is-big-data.html
).
Lets now look at how we can use technical QA on big data application testing.
1. We know from the above introduction that these applications stores large number of data. Here we can think beyond and utilize data driven test automation framework to input large chunks of data parallel. Large data entered in Excel sheets or any other files can be extracted automatically and entered using an automation framework.
2. When entering data to the application which utilizes Big Data, its a good practice to use the web services testing tool like SOAP UI. Entering data via a web services tool is much more faster than utilizing a UI.
3. Next level of technical test initiative that you can initiate is using parallel execution in your data driven test automation framework. Utilizing TestNG or Selenium Grid (via Multiple node clients) to enter data will also help to test the functionality of the Big Data app.
4. To test whether the database is able to handle the multiple load of data storage, a test engineer can execute a database trace. A DB trace will check what stored procedures are called, the duration to trigger and what unnecessary DB calls are triggered.
5. Its also a good practice to test the hardware usage such as CPU, Memory and Disk usage when entering data to the Big Data app. Utilize the PerfMon or performance monitoring tools to achieve this.
6. We can also use performance testing tools such as JMeter to pump large number of data by large number of virtual users. Performance testing tools can be also utilized to test the performance when querying large data sets from Big Data applications. It helps to measure the average response time when searching information from large chunks of data. It is a good practice to couple this with DB tracing concept which was mentioned above.
This is also another area which more technical testing can be done upon. This article is another area which technical QA can go beyond and catch the stars.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
9
To view or add a comment,
sign in
More articles by this author
No more previous content
Test automation planning and contents of a test automation plan.
Jun 5, 2024
Considerations that should be done when starting test automation in your project or organization
May 14, 2024
Use of AI to shape up testing
May 11, 2024
Let's Start Test Automation with Playwright
Feb 7, 2024
Things to consider when developing Test Automation Frameworks
Feb 3, 2024
Most Common Mistakes We Make In Test Automation
Jan 27, 2024
100% Test Automation is it a Myth or Reality
Jan 24, 2024
Shift Left Testing and How Test Automation Helps
Jan 22, 2024
We talk big on Test Automation but why we fail?
Jan 19, 2024
Mistakes by Management in Test Automation
Aug 24, 2023
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
No Title,erro
How to Align Testing with Big Data Requirements,"How to Align Testing with Big Data Requirements
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Manufacturing
Quality Assurance
What is the best way to align testing with Big Data requirements?
Powered by AI and the LinkedIn community
1
Understand the business goals
2
Define the test pyramid
3
Choose the right tools and frameworks
4
Implement test automation and continuous integration
Be the first to add your personal experience
5
Monitor and optimize the test results
Be the first to add your personal experience
6
Here’s what else to consider
Be the first to add your personal experience
Big data is a term that refers to the collection, processing, and analysis of large and complex datasets that traditional methods cannot handle. Big data projects often involve high volume, variety, velocity, veracity, and value of data, which pose unique challenges and opportunities for testing. As a quality assurance (QA) engineer, you need to align your testing strategy with the big data requirements to ensure the quality, reliability, and performance of the data products. In this article, you will learn about some of the best practices and tools to achieve this alignment.
Top experts in this article
Selected by the community from 6 contributions.
Learn more
David Cui
Senior Big Data QA Analyst & ETL Test Automation Specialist | Expert in Leading & Innovating Test Strategies.
View contribution
1
See what others are saying
1
Understand the business goals
The first step to align your testing with the big data requirements is to understand the business goals and expectations of the data products. You need to know what problems the data products are trying to solve, what value they are delivering, and what metrics they are using to measure success. This will help you define the scope, objectives, and criteria of your testing, as well as identify the relevant stakeholders, data sources, and data consumers.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Md Maruf Rahman
ISTQB® Certified Tester | QA Automation Engineer | Cypress | WebdriverIO | Selenium |
Copy link to contribution
Report contribution
The best way to align testing with Big Data requirements is to understand the business goals behind the Big Data implementation. This involves collaborating closely with stakeholders to grasp the specific objectives like improving customer insights, enhancing operational efficiency, or optimizing decision-making. Once these goals are clear, testers can focus on validating whether the Big Data solution meets these objectives effectively. Testing strategies should then be tailored to ensure data accuracy, reliability, scalability, & performance, aligning closely with the desired outcomes. Regular communication and feedback loops with the business teams are essential to ensure that testing efforts are aligned with the Big Data requirements.
…see more
Like
Paras Patel
Quality Assurance Engineer | Driving Excellence in Manufacturing at Atlas Mechanical Innovations
Copy link to contribution
Report contribution
Understanding the business goals is paramount when aligning testing with Big Data requirements. It ensures that our testing efforts are directly tied to the overarching objectives of the organization. By comprehensively grasping the business goals, we can tailor our testing strategies to validate the functionalities and performance metrics that are most crucial for achieving those goals.
…see more
Like
2
Define the test pyramid
The test pyramid is a model that describes the optimal distribution of different types of tests in a software project. The pyramid consists of three layers: unit tests, integration tests, and end-to-end tests. The idea is to have more unit tests at the bottom, fewer integration tests in the middle, and the least end-to-end tests at the top. This way, you can achieve a high level of test coverage, fast feedback, and low maintenance costs. For big data projects, you need to define the test pyramid according to the data pipeline stages, such as ingestion, transformation, storage, analysis, and visualization.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Md Maruf Rahman
ISTQB® Certified Tester | QA Automation Engineer | Cypress | WebdriverIO | Selenium |
Copy link to contribution
Report contribution
The best way to align testing with Big Data requirements is to define the test pyramid. This concept involves structuring tests into a pyramid shape, with unit tests forming the base, followed by integration tests, and finally, a smaller number of end-to-end tests at the top. For Big Data, this means focusing on unit tests for individual components like data transformations or algorithms, ensuring they function correctly in isolation. Integration tests validate interactions between these components, while end-to-end tests verify the entire system's behavior with real data. This approach provides comprehensive coverage while prioritizing efficiency & scalability, aligning testing efforts effectively with the complexities of Big Data systems.
…see more
Like
Paras Patel
Quality Assurance Engineer | Driving Excellence in Manufacturing at Atlas Mechanical Innovations
Copy link to contribution
Report contribution
Defining the test pyramid is an essential step in ensuring comprehensive testing coverage for Big Data systems. The test pyramid provides a structured approach to categorize and prioritize different types of tests, including unit tests, integration tests, and end-to-end tests. This framework allows us to allocate resources efficiently, focusing on foundational unit tests while also incorporating higher-level tests to validate system behavior and interactions.
…see more
Like
3
Choose the right tools and frameworks
In order to align your testing with the big data requirements, you need to choose the right tools and frameworks that can handle the scale, complexity, and diversity of the data. Additionally, you should consider the compatibility, performance, and usability of these tools and frameworks with your data platform and environment. Popular tools and frameworks for big data testing include Apache Hadoop, a distributed framework that stores and processes large datasets; Apache Spark, a unified analytics engine; Apache Hive, a data warehouse system; Apache Kafka, a distributed streaming platform; Apache Airflow, a platform for orchestrating complex data pipelines; and Selenium, a tool for automating web browser testing.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
David Cui
Senior Big Data QA Analyst & ETL Test Automation Specialist | Expert in Leading & Innovating Test Strategies.
Copy link to contribution
Report contribution
True, Apache Spark is the optimal choice for developing a robust and effective test automation framework suitable for big data projects. 

In my previous data lake projects, where I attempted to use Java+Selenium, Python Pandas, and Spark, only the frameworks built on Spark proved capable of meeting nearly all expectations.
…see more
Like
1
Paras Patel
Quality Assurance Engineer | Driving Excellence in Manufacturing at Atlas Mechanical Innovations
Copy link to contribution
Report contribution
Choosing the right tools and frameworks is crucial for effective testing in the realm of Big Data. Given the unique challenges posed by large volumes of data and complex processing pipelines, selecting tools that are specifically designed for testing Big Data systems is imperative. Whether it's tools for data generation, testing automation, or performance monitoring, investing in the right technologies empowers us to conduct thorough and reliable testing that meets the demands of Big Data environments.
…see more
Like
4
Implement test automation and continuous integration
Test automation and continuous integration are essential practices to align your testing with the big data requirements. Test automation allows you to execute your tests faster, more frequently, and more consistently, while reducing human errors and manual efforts. Continuous integration allows you to integrate your code changes and test results into a shared repository, ensuring the quality and compatibility of your data products. You can use tools like Jenkins, GitLab, or GitHub Actions to set up and run your test automation and continuous integration pipelines.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Monitor and optimize the test results
The final step to align your testing with the big data requirements is to monitor and optimize the test results. You need to collect, analyze, and report the test data, such as test coverage, test duration, test failures, and test defects. You also need to compare the test data with the business metrics, such as data accuracy, data completeness, data timeliness, and data value. This will help you evaluate the quality and performance of your data products, as well as identify the areas of improvement and optimization. You can use tools like Grafana, Kibana, or Tableau to visualize and communicate your test results and insights.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Quality Assurance
Quality Assurance
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Quality Assurance
No more previous content
Stakeholders demand changes that clash with QA standards. How do you handle their feedback?
9 contributions
You're facing a team member who ignores industry standards in QA. How do you address this recurring issue?
7 contributions
Stakeholders are doubting your QA results. How do you handle their concerns?
3 contributions
You're faced with tight project deadlines. How do you ensure both speed and quality in your deliverables?
6 contributions
You're facing testing discrepancies with developers. How do you uphold a professional attitude?
6 contributions
Dealing with late-cycle bugs impacting your team's morale. How can you keep everyone motivated?
2 contributions
Your team is divided on quality metrics in software testing. How do you ensure everyone is on the same page?
2 contributions
Striving to maintain quality in QA can be exhausting. How do you avoid burnout while meeting tight deadlines?
Your test results are questioned by a colleague. How will you defend your findings?
8 contributions
Your team is divided on test case importance. How do you ensure everyone is on the same page?
1 contribution
You're facing delays in QA testing timelines. How can you swiftly identify and resolve bottlenecks?
4 contributions
No more next content
See all
Explore Other Skills
Warehouse Operations
Manufacturing Operations
Lean Manufacturing
Plant Operations
Transportation Management
Logistics Management
Quality Management
Product R&D
Supplier Sourcing
Process Design
Show more
Show less
More relevant reading
Data Engineering
Clients are demanding real-time insights from complex data streams. How will you meet their expectations?
Data Science
What are the steps to choosing a data processing framework for your project?
Data Engineering
How can you extract data from data lakes and data hubs effectively?
Data Engineering
How do you report data pipeline performance?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
3
6 Contributions"
Crash Test Your Business Model - Disruptive Innovation With Big Data & TESLA Motors,"Crash Test Your Business Model - Disruptive Innovation With Big Data & TESLA Motors
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Crash Test Your Business Model - Disruptive Innovation With Big Data & TESLA Motors
Report this article
George Danner
George Danner
President, Business Laboratory LLC, Author - The Executive's How-To Guide to Automation
Published Mar 1, 2016
+ Follow
Disruption and Data Go Together
Last week I had the great privilege of attending CERAWeek 2016 in Houston, Texas, USA.  Oil ministers, researchers, NGOs, CEOs, US Senators.  Even the President of Mexico was there to opine on energy policy and outlook, from economics to technology to politics and back again.
An evening dinner with the Chief Technology Officer (and co-founder) of  Tesla motors, J.B. Straubel, proved the highlight of the conference for me.  Not because the cars they make are cool (they really are); not because their battery technology will fundamentally change our lives in the coming decade (it will); rather, because this is a unique point in history where we get to see a mass production car company in the making in the modern era and contrast that with the last generation of auto manufacturers.  When was the last major automotive “start up” before Tesla?  Answer: 1925.
It is broadly instructive to see how the same industry evolves in one era versus another—my belief is that this gives us insight as to how the whole of the economy will move forward, and the lessons there are invaluable, partly because it is so rare to see such a pure, controlled experiment unfold right before our eyes.  What do the successful companies in X industry look like?  This is probably the most important question among boardroom conversations across the world today.  Let’s see if Tesla’s experience can help us answer it.
Disruptive Innovation
Let’s look at just one aspect of Tesla’s operations: crash testing.  All car companies must do this, and at present all the major players have elaborate, expensive facilities designed, for the most part, to run vehicles into walls or bash them with objects from all angles and then measure the associated forces and damage.  Results are reported to regulators and to the public.
Tesla avoided this expensive route by hiring a large crane and dropping the car to the ground from various heights to achieve precisely the same effect.  They used software to make a variety of corrections for the vertical orientation, yet
duplicated the crash test that the majors do at a fraction of the cost
.  In principle, this vignette is indicative of Tesla’s corporate approach: using software, automation, and “light weight” infrastructure to conduct operations, all the while consuming a fraction of the normal human resources.  Tesla employs 13,000 people, and has a market cap of $25B.  GM employs 215,000 people, and has a market cap of $45B.  When you consider that Tesla can scale production by an order of magnitude with a roughly similar labor pool, the labor content numbers create an even larger divergence with the status quo.
I’ll also argue that the software that Tesla used to manage and analyze crash testing didn’t simply displace labor costs and capital—it also added to the inventory of intellectual property that the company is amassing.
Don’t just take my word for it. Forbes Magazine loves how TESLA “a high-end disruptor” is doing business.
Tesla automobiles look and drive much like other cars, use established infrastructure like roads and confine much of their product innovation to only one aspect: the power system.
FORBES MAGAZINE
So there you have it.  The lesson from Tesla thus far is clear: smart companies use software and automation to reduce the labor content and capital cost of major operations.  Tesla reimagined something as banal as crash testing and created a double whammy of value: better operating earnings and greater shareholder value.
How many brick-and-mortar companies could benefit from this same kind of approach? My answer:
every single one
.
There isn’t a company on the planet that is immune to Tesla-like rethinking of its operations and assets.  My recommendation is that you go through the thought experiment for yourself at your firm…what is my analog for a crash test center and how could that be reborn as a software and automation-driven ghost of its former self?
Happy learning.  Visit Our Website
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
12
6 Comments
Robert Brincheck
Business Development Director at PTC: Helping the Companies That Make Products the World Relies on Successful.
8y
Report this comment
George, 

You are stating, or at least implying, that Tesla deploys more simulation and automation than GM or other OEMs. I've seen no evidence of this and you do not provide any. GM and other OEMs have been performing computer crash simulations for over 30 years and the full vehicle tests you describe, are only done for NHTSA compliance validation. This is also a poor example of contrasting business approaches because the crash labs at GM and other OEMs were developed over decades and were the source of the information that Tesla is able to leverage. It makes no sense for GM to decommission their lab just like it makes little sense for Tesla to invest valuable capital and resources in creating a lab when 3rd party facilities already exist.

Manufacturing process and automation would be a more appropriate example since investments here contribute directly to the bottom line. GM has invested heavily in this area, as have other OEMs, and they have very modern, highly automated factories and they produce 22x more vehicles/employee than Tesla. 

Lastly, market cap is a very misleading metric to use for comparing Tesla to another auto maker because Tesla's market value is grossly inflated relative to other Automotive OEMs. Tesla likes to promote their gross margins of 22.82%, which is very high relative to other OEMs, especially GM. However, Toyota's is even higher, 23.2% and Toyota actually produces a profit, 7.34% net margin. As such, Toyota is rewarded with a higher Price-To-Sales Ratio than their peers (0.75% vs GM's 0.32%). Tesla's 6.7% Price-To-Sales Ratio is 9x Toyota's which assumes that Tesla can:

A. Sell 190,000+ vehicles/yr
B. Maintain their gross margins during that 9x expansion. 

If they actually achieve this feat, and their Price-To-Sales Ratio falls in line with the industry, then Market Cap would be a good metric, but not today,
Like
Reply
1 Reaction
Robert Brincheck
Business Development Director at PTC: Helping the Companies That Make Products the World Relies on Successful.
8y
Report this comment
Unfortunately there are a variety of issues with your article. First and foremost, the NHTSA doesn't allow companies to 'get creative' with their testing. They either self-certify by executing the test exactly as the spec describes in a certified lab or NHTSA can test the final product and let them know the results. NHTSA isn't going to use any of Tesla's crane testing data. 

Also using market cap to compare company efficiency doesn't make any sense. Revenue/employee is a typical metric and for car companies vehicles/employee is another. 

Tesla - $311K/employee & 1.94 vehicles/employee
GM - $709K/employee & 45.58 vehicles/employee

GM is far more efficient because of their scale. Lastly, Tesla can scale their production without adding more indirect employees but they will need to add many more direct employees.
Like
Reply
1 Reaction
John Blyler
Engineer, Tech Journalist, Standards Editor, Lecturer, and Marketer
8y
Report this comment
Virtual software crash test dummies?
Like
Reply
1 Reaction
David Thomas Guerra
Pioneer in AI Superintelligence,  AI Performance Optimization Advisor and Bestselling Author of Superperformance and The Superperforming CEO
8y
Report this comment
Brilliant!
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by George Danner
Not-So-Secret Agents
Aug 5, 2024
Not-So-Secret Agents
Last week I had the honor of hosting a number of executives through a discussion of Practical Applications of AI in…
7
1 Comment
Analytics, Data Science & The Explainability Concept
May 16, 2024
Analytics, Data Science & The Explainability Concept
Young, eager, and bright. These are the qualities that I see in the new generation of analysts and problem solvers that…
7
1 Comment
Are You Artificial Intelligence Future Ready?
May 1, 2024
Are You Artificial Intelligence Future Ready?
Being AI future-ready is a competitive advantage. Is your company prepared for a future with artificial intelligence?…
8
Want to Build a Strong Company? Start by Doubling Down
Feb 28, 2024
Want to Build a Strong Company? Start by Doubling Down
As I move around the business world these days I am encountering a great deal of anxiety about the future. Will…
12
2 Comments
Elon Musk and the New Blueprint for Companies
Jan 19, 2024
Elon Musk and the New Blueprint for Companies
Is this billionaire quietly teaching us how to run companies in this new era? It is amusing how polarizing Elon Musk…
17
4 Comments
Every Company Is A Factory
Oct 9, 2023
Every Company Is A Factory
You must be intrigued by my statement, “AI Simulation: Every Company is a Factory.” Here at Business Laboratory, we get…
10
How to Evaluate Supply Chains
Jun 6, 2023
How to Evaluate Supply Chains
It is very common these days for us to get a message from someone in some part of the world that goes like this: “Hey…
11
1 Comment
ChatGPT: If These Walls Could Talk…Oh, Wait…They Can!
Apr 10, 2023
ChatGPT: If These Walls Could Talk…Oh, Wait…They Can!
Using Generative AI to Allow Our Plants and Factories to Talk to Us When the newest version of ChatGPT dropped at the…
13
3 Comments
What's Next After ChatGPT?
Feb 17, 2023
What's Next After ChatGPT?
So much ink has been dedicated to raving about ChatGPT that I have temporarily shelved my idea to write a post…
15
4 Comments
What the SWA Meltdown Means for You
Dec 30, 2022
What the SWA Meltdown Means for You
By now, you are all aware (some of you painfully so) of the historic failure of Southwest Airlines’ systems this week…
24
5 Comments
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Andrea Biancini on LinkedIn: Big Data Test Infrastructure (BDTI),"Andrea Biancini on LinkedIn: Big Data Test Infrastructure (BDTI)
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Andrea Biancini’s Post
Andrea Biancini
Experienced Managing Director & Angel Investor | Technology and Digital Transformation | Strategic Visionary | NED | Change Management | Innovation
2mo
Edited
Report this post
🌍🔍 Exciting news from the world of Big Data and innovation!

While working for the
European Commission
, I had the privilege of contributing to the Big Data Test Infrastructure (BDTI) initiative. This project aimed to empower public administrations with the tools and capabilities needed to harness the power of big data for more efficient decision-making and innovation.

Fast forward to today,
#BDTI
has evolved into something even more fascinating! With new advancements and expanded capabilities, the initiative is making an even greater impact across sectors, helping organizations experiment, test, and deploy big data solutions with greater ease and flexibility.

It’s incredible to see how this initiative continues to grow and support Europe's digital transformation! 💡🚀
#BigData
#Innovation
#DigitalTransformation
#PublicSector
#BDTI
#AI
#DataScience
#EUDigital
#TechForGood
#AI4Gov
Big Data Test Infrastructure (BDTI)
big-data-test-infrastructure.ec.europa.eu
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
Brian Titterington
Managing Director | Tech Investment Banking @ Harris Williams
8mo
Report this post
Diversification of revenue models, artificial intelligence and machine learning integration, and an increasing focus on ESG continue to drive M&A activity across the data and information services landscape. We share more on these trends and other insights from the sector in our Q1 brief.
Data & Information Services
share.postbeyond.com
16
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Julien Oussadon
8mo
Report this post
Diversification of revenue models, artificial intelligence and machine learning integration, and an increasing focus on ESG continue to drive M&A activity across the data and information services landscape. We share more on these trends and other insights from the sector in our Q1 brief.
Data & Information Services
share.postbeyond.com
4
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
EE World Online
754 followers
7mo
Report this post
At
OFC Conference
2024, one thing became clear: if you’re in the data communications business, you’d better claim that your company helps move AI-related data or risk being left behind. 

Read more from
Martin Rowe (EE World)
:
OFC 2024: It’s AI or die
https://www.eeworldonline.com
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Stefaan Verhulst, PhD
4mo
Edited
Report this post
📣 Looking forward providing the keynote at the upcoming conference on ""Data Governance and Integration for Maximum Development Impact"" in
#Seoul
📅 on 10-11 September 2024
🌏 Organized by
Statistics Korea
and UN ESCAP
 
National statistical offices 📈  are navigating an increasingly complex landscape, characterized by:
🌐 An expanding data ecosystem
🏛️ Evolving regulatory environments
💻 Digital government transformations
🤝 A growing range of stakeholders

With the rising demand for timely and granular data, NSOs are turning to data integration to meet these needs. This shift brings both new opportunities and challenges.

This conference will: ✨ Explore lessons learned so far 🔍 Highlight new trends in data governance and data integration 🌟 Present diverse perspectives on opportunities and challenges for NSOs

💻Learn more:
https://lnkd.in/eGWVCMaP
📝 Detailed Program and Speakers:
https://lnkd.in/eRhiJPVW
✍️ Register:
https://lnkd.in/e9sJAa69
#DataGovernance
#DataIntegration
#Statistics
#AsiaPacific
#NSO
#DigitalTransformation
#DevelopmentImpact
16
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Katharina Morik
Prof. Dr. at TU Dortmund, Speaker of SFB 876, co-founder of the Lamarr Institute
7mo
Report this post
Finally: best paper award paper on @XAI at *Intelligent Data Analysis* by  Björklund et al. from Helsinki. The key procedure: Copy the embedding of the next data point, find similar point in the grid, do prediction based on the local model. This becomes an explained learned model.
27
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
ICPSR at University of Michigan Institute for Social Research
1,978 followers
2mo
Report this post
We love the new website for the Research Data Ecosystem, which is expanding access to research data to spur breakthrough scientific research. The Research Data Ecosystem (RDE) will introduce new tools (including TurboCurator, Explore Data, and the Researcher Passport) to support the entire research data lifecycle on a modern data software platform. Researchers can safely and securely access, connect, store, and manipulate data. More at
https://myumi.ch/Q6keq
#ResearchDataEcosystem
National Science Foundation (NSF)
7
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Dr. Patrick Mackey, CSSBB
Academic Professional
7mo
Report this post
Trust and Data Science are crucial components of modern society, with trust being the foundation upon which advances in data science thrive. Building trust in data-driven technologies is imperative to ensure their ethical and responsible use, fostering transparency and confidence in leveraging data for innovation and progress.
7
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
SmartStream Technologies
45,648 followers
1w
Report this post
Financial firms are increasingly changing how their organisations manage and process reference data for trade data management. Conducted in collaboration with
Acuiti.io
, our new report, titled ‘The Future of Reference Data: From Compliance to Alpha‘ highlights the growing importance of reference data.
Firms Increasingly Leveraging AI Automation to Better Utilise Reference Data, Finds SmartStream | The Fintech Times
https://thefintechtimes.com
21
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Prasad Karpe
Jt. General Manager (Airport Systems)
3mo
Report this post
Despite the growing body of literature on data-driven decision-making (DDDM) and, more recently, big data, empirical analyses on processes and strategies of government agencies toward DDDM are still scarce. To mitigate this gap, this course conducted by Wadhwani Institute of Technology and Policy, aims towards building awareness of Data Analytics among government officials enabling them to make informed data-driven decisions thereby impacting lives towards stronger public service delivery.
10
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Daniel Unogwu
Animator(YAPPI), McKinsey Forward Alumni |FGN 3MTT Fellow |ALx Fellow| CPM| BSc.Microbiology | AI| Personal Transformation Expert|Data Analyst | Google Certified| Chess
5mo
Report this post
Try this for your knowlwdge on Artificial General Intelligence (AGI)
Data Points
mckinsey.com
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
8,273 followers
2,627 Posts
3 Articles
View Profile
Follow
More from this author
Life-long learning? Sì, e life-long teaching!
Andrea Biancini
4y
Keynes, PIL e la situazione italiana
Andrea Biancini
5y
Studio e Passione, il DataLab!
Andrea Biancini
5y
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more industry insights
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
ExfDQ: Test 360 for data quality testing | Exafluence posted on the topic | LinkedIn,"ExfDQ: Test 360 for data quality testing | Exafluence posted on the topic | LinkedIn
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Exafluence’s Post
Exafluence
7,707 followers
1y
Report this post
Data quality testing is important for ensuring the highest quality of output in any analysis, as they say garbage in equals garbage out. Data testing is very important and it becomes much more challenging when it comes to ""data in motion"". Our Test 360, platform under ExfDQ has been built for automated
testing of data in motion and can flag exception automatically bringing it to the attention of the tester. If you are interested in seeing a demo of how Test 360 works, just send us a note on
marketing@exafluence.com
and we will be happy to arrange for the same.
Mohit Kumar
Karthikeyan Sankaran
Ravikiran Dharmavaram
#automatedtesting
#exceptionmanagement
#datainmotion
#exafluence
.
20
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
7,707 followers
View Profile
Follow
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Unlock more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Elevate Your Central Location Tests (CLT): Mastering Data Quality for Product Testing Excellence,"Elevate Your Central Location Tests (CLT): Mastering Data Quality for Product Testing Excellence
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Elevate Your Central Location Tests (CLT): Mastering Data Quality for Product Testing Excellence
Report this article
Gabriel Velez
Gabriel Velez
Strategic Consumer Insights Professional & AI expert | Bridging Consumer Needs and Business Solutions
Published Jan 17, 2024
+ Follow
In the fast-paced world of consumer goods, where innovation is the lifeblood of success, Central Location Tests (CLTs) stand as a pillar of reliability. They are the compass that guides product development, ensuring that every new creation aligns with consumer expectations and preferences.
The Power of CLT
CLTs are a tried-and-true methodology for assessing consumer responses to new products. They provide a controlled environment where participants can interact with a product, share their experiences, and offer invaluable feedback. This controlled setting minimizes external influences, allowing researchers to isolate variables and draw precise conclusions.
For over two decades, Crowd Answers has been at the forefront of CLT execution, conducting over 2000 New Product Development tests. Our journey has been marked by continuous learning, innovation, and a relentless pursuit of data quality excellence.
The Foundation: Data Quality
At the heart of every successful CLT lies the foundation of data quality. The insights derived from these tests are only as valuable as the data itself. Ensuring the highest data quality is not a luxury; it's an imperative.
So, how can you elevate your CLT results and harness the power of data quality? Let's dive into the art and science:
1. Participant Selection:
The quality of your data begins with the participants. Define your target audience meticulously. Ensure that your sample represents your target market's demographics, preferences, and behaviors. A mismatch here can skew your results.
2. Structured Questioning:
The questions you ask matter deeply. Craft questions that are clear, unbiased, and aligned with your research objectives. Ambiguity in questions can lead to ambiguous answers.
3. Rigorous Training:
The moderators and interviewers in a CLT play a pivotal role. They should undergo extensive training to ensure consistency in the administration of the test. Any variation in their approach can introduce bias.
4. Realistic Context:
Replicating real-world conditions as closely as possible is crucial. Whether it's the lighting, the setting, or the product presentation, creating a realistic context ensures that participants' responses mirror real-world behavior.
5. Data Capture Technology:
Leverage technology for data collection. Digital surveys and real-time data capture tools enhance accuracy and efficiency. They also facilitate immediate data analysis.
6. Robust Analysis:
The journey doesn't end with data collection. Robust data analysis techniques are essential to draw meaningful insights. Techniques like conjoint analysis, regression analysis, and MaxDiff analysis can reveal deeper consumer preferences.
7. Continuous Improvement:
CLT is an evolving discipline. Continuously evaluate your methodology, adopt best practices, and stay updated with industry trends. Crowd Answer's extensive experience is a testament to our commitment to ongoing improvement.
The Crowd Answer Edge
What sets Crowd Answers apart in the realm of CLTs is not just our experience, but our dedication to perfection. With a portfolio spanning numerous industries, we've honed our expertise to provide tailored solutions that meet the unique needs of each product and brand.
Our commitment to data quality is unwavering. We employ stringent quality control measures at every stage of a CLT, from planning to execution and analysis. Our moderators and interviewers are meticulously trained to ensure consistency and impartiality. We invest in cutting-edge technology to streamline data capture, enhancing both accuracy and efficiency.
But perhaps our greatest strength lies in our ability to transform data into actionable insights. We don't just provide numbers; we provide a narrative. We help you understand not only what consumers are saying but why they're saying it. This depth of understanding is the key to informed decision-making and product innovation.
Conclusion
In the dynamic landscape of consumer preferences, a commitment to data quality is the compass that guides us to success. Central Location Tests remain an invaluable tool for product development, and mastering them requires a multifaceted approach.
Crowd Answers, with its 20 years of experience and over 2000 New Product Development tests, stands as a testament to this commitment. Our expertise is your asset, and our dedication to data quality ensures that every CLT is a beacon of insights, guiding your product development journey.
Elevate your CLT experiences with Crowd Answers, and unlock the full potential of your product testing endeavors. Embrace data quality, and let it be the cornerstone of your success.
The Crowd Answers Insider
The Crowd Answers Insider
577 followers
+ Subscribe
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
2
To view or add a comment,
sign in
More articles by Gabriel Velez
Harnessing AI for Market Research and Consultancy: Elevating Business Insights
May 15, 2024
Harnessing AI for Market Research and Consultancy: Elevating Business Insights
In today's rapidly evolving market landscape, businesses face the constant challenge of staying ahead of trends and…
8
3 Comments
Crafting Expert AI Prompts for Qualitative Research: A Guide to Unleashing Insights
May 6, 2024
Crafting Expert AI Prompts for Qualitative Research: A Guide to Unleashing Insights
In the rapidly evolving world of artificial intelligence, the quality of outcomes is directly influenced by the input…
1
1 Comment
Mastering AI Integration: Elevating Concept Testing in Market Research
Dec 6, 2023
Mastering AI Integration: Elevating Concept Testing in Market Research
Introduction: Greetings, fellow market research enthusiasts, In the ever-evolving world of market research, staying…
3
20 Prompts to Supercharge AI Market Research
Nov 30, 2023
20 Prompts to Supercharge AI Market Research
Included in this PDF: 5 Prompts for New Product Development 5 Prompts for Segmentation and Customer Understanding 5…
4
1 Comment
Harnessing AI for In-Depth Market Research: Essential Prompts for Product Diferenciation.
Nov 30, 2023
Harnessing AI for In-Depth Market Research: Essential Prompts for Product Diferenciation.
Introduction As the business landscape evolves, leveraging Artificial Intelligence (AI) in market research has become…
10
The AI Revolution: Transforming Concept Testing and Advertising Assessment
Nov 15, 2023
The AI Revolution: Transforming Concept Testing and Advertising Assessment
Introduction: The world of market research is on the cusp of a revolution—one driven by Artificial Intelligence (AI)…
6
Mastering Early Quantitative Concept Tests: Unveiling Best Practices in Market Research
Aug 25, 2023
Mastering Early Quantitative Concept Tests: Unveiling Best Practices in Market Research
Greetings, fellow enthusiasts of market research, In the dynamic landscape of product innovation, the journey from a…
6
The Nightingale's Fairy Tale: Unveiling the Prophetic Parallels of AI and Humanity
Jun 28, 2023
The Nightingale's Fairy Tale: Unveiling the Prophetic Parallels of AI and Humanity
Dear readers, Do you remember the enchanting tale of ""The Nightingale"" by Andersen? It tells the story of two birds—the…
3
Finding Harmony: Embracing Innovation and Statistical Rigor in Product Testing
Jun 21, 2023
Finding Harmony: Embracing Innovation and Statistical Rigor in Product Testing
Dear readers, In the ever-evolving landscape of product testing, embracing new technologies is essential for innovation…
1
1 Comment
Embracing the Power of Insights: Navigating the Challenges of Research Relevance
Jun 9, 2023
Embracing the Power of Insights: Navigating the Challenges of Research Relevance
Dear Colleague, I want to take a moment to acknowledge the incredible dedication and hard work that you put into your…
2
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Mecanica Scientific Services Corporation on LinkedIn: Putting fleet data quality to test for fleet risk | Geotab,"Mecanica Scientific Services Corporation on LinkedIn: Putting fleet data quality to test for fleet risk | Geotab
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Mecanica Scientific Services Corporation’s Post
Mecanica Scientific Services Corporation
747 followers
1mo
Report this post
For over ten years, Mecanica Scientific Services Corporation has collaborated with
Geotab
, a leader in GPS fleet telematics. We're proud to contribute to Geotab's commitment to data quality, a cornerstone of effective fleet management.

In 2022, Geotab published a white paper, ""Putting Fleet Data Quality to the Test,"" showcasing our testing of their curve-based algorithm. The results? Impressive data quality and reduced server load.

This partnership underscores our dedication to innovation and excellence in telematics and highway safety.
https://lnkd.in/gV6PibfP
#Geotab
#FleetManagement
#Telematics
#DataQuality
#Innovation
Putting fleet data quality to test for fleet risk | Geotab
geotab.com
25
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
Neil Cawse
1mo
Report this comment
Thanks
John Steiner, EMBA
for your partnership. You have helped us in our journey to have the best data and we appreciate your leadership.
Like
Reply
9 Reactions
10 Reactions
To view or add a comment,
sign in
More Relevant Posts
Kestrel Insights
1,330 followers
1mo
Report this post
Ready to boost your fleet efficiency with smarter location tracking? Our
Motive
integration makes geofencing effortless, precise, and scalable.

With our automated polygon geofencing embedded in Motive, users can:

- Eliminate manual geofence setup processes — automate and save hours.
- Get precise tracking for arrivals and departures.
- Maximize the telematics experience with accurate, data-driven location intelligence
- Scale as your operations expand, adapting to new sites without hassle.

Whether you’re managing a single fleet or a nationwide operation, this integration brings you the power and precision of automated geofencing. With
Kestrel Insights
, Motive users can handle location needs confidently and efficiently — no matter the scale.

Explore the full capabilities and see how this integration can refine your fleet strategy:
https://lnkd.in/gb3Q8kbF
#motive
#geofencing
#data
#supplychain
#logistics
#shipping
#fleet
#fleetmanagement
#trucking
#transportation
Unleashing Telematics Synergy: Announcing Our Newest Integration | Motive
kestrelinsights.com
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Kestrel Insights
1,330 followers
2mo
Edited
Report this post
Kestrel Insights
automated polygon geofencing is now integrated into the
Motive
platform! Through this integration, Motive users can achieve greater precision and efficiency with geofencing.

Here’s how our geofencing solution benefits Motive users:

- No more manually drawn geofences — automate the process to save valuable time.
- Get pinpoint accuracy for arrivals and departures, minimizing errors and improving decision-making.
- Unlock the full potential of your telematics system with advanced geofencing capabilities.
- Scale to meet growing location needs with precision and simplicity.

At Kestrel Insights, we’re constantly pushing the boundaries to deliver the tools you need for success in freight and fleet management. This integration with Motive is just one way we pursue our mission to bring you cutting-edge geofencing technology.

Curious to learn more? Get all the details here:
https://lnkd.in/gb3Q8kbF
#motive
#geofencing
#data
#supplychain
#logistics
#shipping
#fleet
#fleetmanagement
#trucking
#transportation
Unleashing Telematics Synergy: Announcing Our Newest Integration | Motive
kestrelinsights.com
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Trimble Transportation
18,057 followers
2mo
Report this post
Today we announced an agreement with
Platform Science
to transform the commercial driver experience across the global transportation industry. As part of the agreement, Trimble and Platform Science will bring together our telematics businesses to create a safer, more efficient and more compliant transportation ecosystem.

“We believe combining our global transportation telematics portfolio with Platform Science’s will further advance fleet mobility and provide our customers with a broader portfolio of solutions to solve industry problems,” says our CEO
Rob Painter
. “Increased collaboration between the new Platform Science business and Trimble’s remaining transportation businesses will enhance our ability to provide positive outcomes for our global customers of commercial mapping, transportation management, freight procurement and visibility solutions.”

Read the full release to learn more about Trimble and Platform Science’s shared vision to transform transportation:
https://lnkd.in/gPsWhX44
Platform Science to Acquire Trimble's Global Transportation Telematics Business Units to Drive the Future of Transportation In-Cab Technology
investor.trimble.com
264
6 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Argos Connected Solutions
1,060 followers
9mo
Report this post
Coming out of
GEOTAB
Connect, there are a number of big changes that will be rolled out in the coming months.

One that you don't have to wait for is access to GO Anywhere, the latest asset-tracking device from Geotab.

Whether you are tracking trailers, construction equipment, or your service fleet, these devices can help you improve asset visibility, utilization, and routing.

Get the details on all of the changes and be sure to connect with our team if you are interested in learning more about GO Anywhere.
#fleetmanagement
#telematics
#assettracking
What’s new in MyGeotab — February 2024 | Geotab
geotab.com
8
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ron Heijman
2mo
Edited
Report this post
Transporeon
Trimble Inc.
Platform Science
Today we announced an agreement with Platform Science to transform the commercial driver experience across the global transportation industry.

An other big step to bring logistics in Sync with a sustainable world !!!  

Read the full article below !!!
Trimble Transportation
18,057 followers
2mo
Today we announced an agreement with
Platform Science
to transform the commercial driver experience across the global transportation industry. As part of the agreement, Trimble and Platform Science will bring together our telematics businesses to create a safer, more efficient and more compliant transportation ecosystem.

“We believe combining our global transportation telematics portfolio with Platform Science’s will further advance fleet mobility and provide our customers with a broader portfolio of solutions to solve industry problems,” says our CEO
Rob Painter
. “Increased collaboration between the new Platform Science business and Trimble’s remaining transportation businesses will enhance our ability to provide positive outcomes for our global customers of commercial mapping, transportation management, freight procurement and visibility solutions.”

Read the full release to learn more about Trimble and Platform Science’s shared vision to transform transportation:
https://lnkd.in/gPsWhX44
Platform Science to Acquire Trimble's Global Transportation Telematics Business Units to Drive the Future of Transportation In-Cab Technology
investor.trimble.com
7
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Lytx, Inc.
33,801 followers
6mo
Report this post
One powerful
#videotelematics
solution! We are excited to see this new plan with
GEOTAB
come to life for Latin America. A revolutionary all-in-one video telematics solution now has Zero Upfront Cost. 
Learn more from the link below, you won't want to miss out on this opportunity!
#ArtificialIntelligence
#OperationalEfficiency
#FleetManagment
Geotab
73,698 followers
6mo
Edited
🚚 Did you know that you can now implement the
#videotelematics
solution, with zero upfront cost? 💰

In an industry where movement is constant and innovation is the key to progress, a revolutionary solution emerges: Geotab and
Lytx, Inc.
Two leaders, one powerful video telematics solution! Together, we offer a video telematics solution that provides valuable video-based information and advanced risk detection.

Don't miss this opportunity to improve the safety and efficiency of your fleet with Geotab! Learn more information at:
https://lnkd.in/ggtjsMZG
Sean Killen
#VideoTelematics
#ZeroUpfrontCostPlan
#Geotab
#FleetManagement
#ArtificialIntelligence
#OperativeEfficiency
Geotab® Introduces All in One Video Telematics Plan with Zero Upfront Cost | Geotab
geotab.com
14
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Bianca Barbu
Marketing Professional | Driving Revenue Growth and Customer-Centric Solutions | Expert in IT & Software Marketing | Transforming Business Development & Innovation |  Communication Professional | Wine Communicator
6mo
Report this post
Tip 7: Prioritize Data Accuracy – Better No Data Than Fake Data

Ever faced a situation where the data from your telematics solution didn't match reality? Inaccurate data can lead to costly mistakes and operational inefficiencies. Discover why prioritizing data accuracy is crucial and how it impacts your fleet management decisions. 
Read my latest article for insights and a real-life story from the field (no real names provided).
#Telematics
#DataAccuracy
#FleetManagement
#BusinessSuccess
Tips and Tricks: Prioritize Data Accuracy – Better No Data Than Fake Data
https://biancabarbu.com
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Scott VanMeter
Trusted Advisor | GEOTAB Fleet Telematics Partner | Efficiency and Safety Expert
9mo
Report this post
GEOTAB
is unrivaled in the public sector space. Why?

I could list a dozen reasons. 

But it is their commitment to creating an ecosystem that truly stands out.

The ability to connect different components allows you to address each area of your fleet management strategy, and not get bogged down.

And with the new tools unveiled at Connect last week, the platform is only going to get better.
#fleetmanagement
#government
#publicworks
#telematics
Geotab Wins US General Services Administration Contract for Fleet Management
government-fleet.com
48
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Geotab
73,698 followers
9mo
Report this post
Are you already aware of the benefits that
#Videotelematics
can bring to your fleet? At Geotab, we are dedicated to the safety and efficiency of your assets, which is why we’ve compiled a list of some of the advantages our technology provides:
✔ Enhanced Safety
✔ Cost Reduction
✔ Increased Productivity
✔ Regulatory Compliance
✔ Reputation Protection
It's more than just a tool; it's a strategic partner for fleet managers aiming to optimise efficiency and safety. Are you prepared to make the jump?
https://lnkd.in/gjSJ_VMN
#FleetManagement
#Efficiency
#RoadSafety
Master fleet safety and efficiency with video telematics: Download our eBook today. | Geotab
geotab.com
5
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Track My Truck
255 followers
9mo
Report this post
GPS technology plays a pivotal role in revolutionizing vehicle maintenance, and Track My Truck is at the forefront of this paradigm shift. By leveraging real-time tracking and data analytics, the platform enables businesses to proactively manage and streamline their fleet maintenance processes. 
This not only minimizes downtime by predicting maintenance needs but also enhances overall operational efficiency, making Track My Truck a key player in optimizing vehicle maintenance from a GPS perspective.
.
.
.
http://www.trackmytruck.us
#fleetmanagement
#gps
#routeoptimization
#fieldservice
#maintenancetracking
#telematics
#safetydriving
#realtimemonitoring
#assetmanagement
#cameratechnology
#insuranceclaims
#smartrouteplanning
#fuelsavings
#transparency
#accuratebilling
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
747 followers
View Profile
Follow
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more industry insights
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Unlocking Insights: The Art of Big Data Testing and Data Quality - Part 1,"Unlocking Insights: The Art of Big Data Testing and Data Quality - Part 1
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Credit : Fission Labs
Unlocking Insights: The Art of Big Data Testing and Data Quality - Part 1
Report this article
Satya Prakash Solanki
Satya Prakash Solanki
Quality Management | QA Architect | AI & Data Quality | Big Data & PySpark | AWS & Azure | Docker & Kubernetes | Automation: Cypress, Playwright, Appium, Selenium | API Testing: REST Assured | Performance Testing: JMeter
Published Jul 9, 2023
+ Follow
In today's data-driven era, where businesses and organizations rely heavily on software applications to manage and analyze vast amounts of data, the importance of testing cannot be overstated. Big data testing plays a very crucial and important role in ensuring the accuracy, completeness, and reliability of large volumes of data. With the exponential growth of data, it becomes needful to validate its quality to avoid making critical decisions based on incorrect or misleading information. Hence, Data quality focuses on ensuring that the data is consistent, relevant, and correct match for its actual purpose. High-quality data empowers organizations and businesses to make the right decisions, achieve valuable insights, and drive business growth. Hence Big data testing is a necessary component in utilizing the full potential of data and maximizing its value in today's data-driven world.
Key Advantages of Big Data Testing
Importance
of Big Data Testing:
The significance of Big Data Testing is essential to validate the integrity and reliability of the massive volumes of data being generated. Traditional testing approaches may not be suitable for the complexities of big data. By conducting comprehensive testing, organizations can identify data inconsistencies, validate algorithms, detect anomalies, and ensure data accuracy. Thorough testing mitigates the risk of making critical decisions based on flawed or incomplete information, instilling confidence in the data-driven decision-making process.
Data Quality Assurance:
Data quality assurance is the process of ensuring that data is accurate, consistent, relevant, and fit for its expected purpose. Poor data quality can lead to faulty analysis, incorrect conclusions, and misguided business strategies. Through data profiling, cleansing, validation, and enrichment, organizations can enhance the overall quality of their data. High-quality data sets the foundation for reliable insights, enabling organizations to make informed decisions, drive operational efficiency, and deliver exceptional customer experiences.
Challenges in Testing Big Data and Ensuring Data Quality
Testing big data and ensuring data quality presents unique challenges. The large volume, variety, and velocity of data require scalable testing frameworks and tools capable of handling large datasets. Ensuring data quality requires robust data governance, data cleansing techniques, and ongoing monitoring to maintain consistency over time. Additionally, privacy and security concerns must be addressed to protect sensitive data during testing and quality assurance processes.
The Best Practices for Effective Big Data Testing
The Best Practices for effective big data testing and data quality assurance to unlock valuable insights, organizations should adopt best practices in big data testing and data quality assurance. This includes implementing a comprehensive test strategy that encompasses functional, performance, and security testing. Leveraging automation tools and frameworks enable efficient testing and reduces human error. Establishing data quality standards, conducting regular audits, and implementing data validation checks are important for maintaining high-quality data. Collaboration between data scientists, testers, and data engineers fosters a holistic approach to ensure data accuracy and usability.
Conclusion:
Unlocking insights from big data is a multi-faceted process that depends on effective testing and data quality assurance. By investing in the art of big data testing and data quality, organizations can leverage the true potential of their data, gain valuable insights. With accurate testing techniques, robust data quality practices, and a focus on continuous improvement, organizations can navigate the complexities of big data and harness its power to stay ahead in today's data-driven landscape.
In the next part, we will talk about how can we achieve data quality with tools and best practices for large amounts of data.
#saal
#digixt
#bigdata
#bigdatatesting
#data
#DigiXT
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
64
1 Comment
Anand Prakash
Turning Data into Insights: Data Engineer | Big Data Enthusiast | Spark Advocate | Real-Time Data Processing | Kafka | Cloud
1y
Report this comment
Big data testing just keeps getting bigger and better! 💪 Excited to dive into Part 1 of this series! #bigdata #testing #insights
Like
Reply
1 Reaction
To view or add a comment,
sign in
More articles by this author
No more previous content
Enhancing QA Process with AI: Boosting Productivity and Test Coverage
Jun 28, 2024
The Pillars of Success: Why Client and Customer Satisfaction are Non-Negotiable
Feb 11, 2024
Measuring Performance Excellence with TPC-H Benchmark - Part 1
Aug 6, 2023
Great Expectations: A Data Quality Framework - Part 3
Jul 20, 2023
Unlocking Insights: The Best Practices and frameworks for Data Quality - Part 2
Jul 12, 2023
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Test Data Quality After Cleaning: 4 Methods,"How to Test Data Quality After Cleaning: 4 Methods
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Analytics
How do you test data quality after cleaning?
Powered by AI and the LinkedIn community
1
Data profiling
2
Data validation
3
Data reconciliation
4
Data visualization
Be the first to add your personal experience
5
Here’s what else to consider
Data quality is a crucial aspect of any data analytics project, as it affects the accuracy, reliability, and validity of the results. Data cleaning is the process of identifying and correcting errors, inconsistencies, and outliers in the data, such as missing values, duplicates, typos, or invalid formats. However, data cleaning is not enough to ensure data quality; you also need to test it after cleaning to verify that the data is ready for analysis. In this article, you will learn how to test data quality after cleaning using four common methods: data profiling, data validation, data reconciliation, and data visualization.
Top experts in this article
Selected by the community from 5 contributions.
Learn more
Rahul Setia
Data Lover from years | Senior Consultant @ PwC | Business Intelligence and Data Analytics
View contribution
24
Kunal Soni
Head of India | CLOUDSUFI
View contribution
8
Edidiong Akpan
Information Technology  | Data Analyst | Cybersecurity Analyst | Software Developer | Dashboard Interface | Python |…
View contribution
2
See what others are saying
1
Data profiling
Data profiling is the process of examining the structure, content, and metadata of the data to understand its characteristics and quality. Data profiling can help you identify issues such as data types, ranges, distributions, frequencies, patterns, dependencies, and relationships. For example, you can use data profiling tools or code to check the number of rows and columns, the data types and formats, the minimum and maximum values, the mean and standard deviation, the number and percentage of missing or null values, the number and percentage of unique or duplicate values, the cardinality and correlation of the attributes, and the presence of any outliers or anomalies.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Edidiong Akpan
Information Technology  | Data Analyst | Cybersecurity Analyst | Software Developer | Dashboard Interface | Python | C#| SQL | Project Management | Travelling
Copy link to contribution
Report contribution
Data profiling for me is looking for any outliers, missing values, or odd patterns in the frequency distributions, summary statistics, and histograms for each column.
In terms of data management and analytics, data profiling is a fundamental step. It enables businesses to make wise decisions, enhance the quality of their data, and increase the value of their data assets.
It helps with the underlisted...
Migration and Integration of Data:
Diagram Design:
Knowledge of Data Distribution
Recognizing Redundancy
Exploration and analysis of the data
Risk evaluation and compliance
Data security and privacy:
Modeling and machine learning:
Designing a dashboard and a report
Troubleshooting and Debugging:
Documenting and governing data:
…see more
Like
2
2
Data validation
Data validation is the process of checking whether the data meets the predefined rules, standards, and expectations for the analysis. Data validation can help you ensure that the data is consistent, complete, accurate, and relevant. For example, you can use data validation tools or code to check the integrity and quality of the data sources, the accuracy and completeness of the data extraction and transformation processes, the compliance and conformity of the data values and formats with the business rules and requirements, and the applicability and suitability of the data for the analysis objectives and methods.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Denil Gada
Agile Project Management and Sprint Deliveries | Product Management | Pre-Sales | Specializing in Insurance Systems Implementation & Solution Deployment.
Copy link to contribution
Report contribution
Data Validation is Important , as it helps in eliminating Incorrect/Junk Data which is not useful, or which can lead to Incorrect Output.
Data Validation can be done as part of Integrations, or before performing and Analytics.
…see more
Like
2
3
Data reconciliation
Data reconciliation is the process of comparing and verifying the data across different sources, systems, or stages to ensure that they are aligned and in sync. Data reconciliation can help you detect and resolve any discrepancies or conflicts in the data, such as mismatches, gaps, or overlaps. For example, you can use data reconciliation tools or code to check the consistency and compatibility of the data schemas, definitions, and formats, the balance and agreement of the data values and totals, the accuracy and timeliness of the data updates and changes, and the traceability and auditability of the data flows and transactions.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Indira Jinadasa
Data Analyst at Equifax| MSc in Applied Data Science
Copy link to contribution
Report contribution
Testing data quality after cleaning is a critical step to ensure that your data is accurate, complete, and reliable for analysis or any downstream tasks. Here are some general approaches that I would try out to test the data quality
Domain Validation, Completeness Check, Consistency and Accuracy, Duplicates Detection, Referential Integrity, Pattern Validation, Statistical Analysis, Data Visualization, User Testing, and Documentation.
Keep in mind that data quality is an ongoing process. Even after initial cleaning and testing, you should have processes in place to monitor and maintain data quality as new data comes in or as changes occur in your data sources
…see more
Like
4
Data visualization
Data visualization is the process of creating and displaying graphical representations of the data to explore and communicate its patterns, trends, and insights. Data visualization can help you evaluate and improve the quality of the data by making it easier to spot and understand any issues or opportunities. For example, you can use data visualization tools or code to create charts, graphs, maps, dashboards, or reports that show the distribution, variation, correlation, or relationship of the data attributes, the outliers, anomalies, or errors in the data values or formats, the impact or influence of the data cleaning and preparation steps on the data quality and analysis results, and the feedback or recommendations for further data quality improvement or enhancement.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Rahul Setia
Data Lover from years | Senior Consultant @ PwC | Business Intelligence and Data Analytics
Copy link to contribution
Report contribution
In my experience, After data cleaning 🧹✨, it's vital to ensure data quality. 

Start with data profiling for basic statistics 📊, then use cross-validation to prevent overfitting 🔄. Comparative analysis compares cleaned and original data 👥, while visual inspection spots anomalies 👀. Validate domain rules ✅, check for completeness 📝, ensure consistency 📐, and maintain referential integrity 🔗. Apply statistical tests 📈 and involve users for testing 👥. 

By combining these things, We can establish a comprehensive framework for testing the quality of our data after the cleaning process. This approach ensures that our data remains accurate, reliable, and fit for its intended use.
…see more
Like
24
Kunal Soni
Head of India | CLOUDSUFI
(edited)
Copy link to contribution
Report contribution
Schema validation can be enforced without utilizing any third party tools; only valid data is stored while failures are fixed followed by pipelines rerun. Further Data Quality checks can be performed for selected datasets on a need basis.
…see more
Like
8
Data Analytics
Data Analytics
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Analytics
No more previous content
You're struggling to align primary and secondary data sources. How can you ensure accurate analysis?
9 contributions
You're juggling ongoing data analytics projects. How do you find time to learn new tools?
16 contributions
Your team is clashing over data privacy in your analytics project. How will you resolve it?
3 contributions
Swamped with data analytics tasks and deadlines. Which new tools should you focus on learning?
1 contribution
Your analytics process is crawling due to security protocols. How should you handle the slowdown?
10 contributions
You're faced with unstructured data from various sources. How do you tackle the analysis challenge?
14 contributions
Your team struggles with securing data while keeping it accessible. How do you teach them to balance both?
3 contributions
You're caught in the middle of conflicting data analysis methods. How can you bring harmony to the debate?
9 contributions
You're about to start a large-scale data migration. How do you ensure data accuracy?
8 contributions
Dealing with inconsistent datasets in data analysis. How do you ensure accuracy and integrity?
7 contributions
Balancing conflicting stakeholder priorities in predictive modeling. Are you making fair decisions?
3 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Engineering
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Analytics
What are the key steps for performing data integrity checks?
Data Analytics
How do you prioritize data accuracy over quantity?
Data Management
What strategies can you use to ensure data quality in your analysis?
Data Analysis
How do you handle data cleaning challenges?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
21
5 Contributions"
No Title,erro
How to Test Security and Privacy in Big Data Applications,"How to Test Security and Privacy in Big Data Applications
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Computer Science
What is the best way to test security and privacy in big data applications?
Powered by AI and the LinkedIn community
1
Define your requirements
2
Choose your tools
3
Implement your tests
4
Analyze your results
Be the first to add your personal experience
5
Improve your application
6
Here’s what else to consider
Be the first to add your personal experience
Big data applications are becoming more prevalent and powerful in various domains, such as health care, finance, education, and social media. However, they also pose significant challenges for security and privacy, as they collect, store, process, and share large volumes and varieties of sensitive and personal data. How can you test the security and privacy of your big data applications to ensure that they comply with the relevant regulations and standards, and that they protect the data from unauthorized access, modification, or leakage? In this article, we will explore some of the best practices and methods for testing security and privacy in big data applications.
Top experts in this article
Selected by the community from 4 contributions.
Learn more
Devi Pradeep Vasala
Google DSC Lead '23 | Beta MLSA@Microsoft | Google Cloud Facilitator '23 | Google CCP Facilitator'22 | 7X Azure…
View contribution
6
Mehul Sachdeva
SDE @ Bank of New York | CSE, BITS Pilani | MITACS GRI 2022 | Apache Iceberg, Contributor | Dremio | Samsung Electronics
View contribution
1
Chander Parkash
Creating new Software Engineers 👨💻 | Java | Go (Golang) | Spring Boot | Spring Core & MVC | TypeScript | Nest.js |…
View contribution
1
See what others are saying
1
Define your requirements
Before you start testing your big data applications, you need to define your security and privacy requirements clearly and comprehensively. These requirements should reflect the legal, ethical, and business obligations that your application has to meet, as well as the expectations and preferences of your users and stakeholders. You should also consider the risks and threats that your application may face, such as cyberattacks, data breaches, or insider misuse, and how to mitigate them. Your requirements should cover aspects such as data encryption, access control, authentication, authorization, auditing, logging, anonymization, and data retention.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Mehul Sachdeva
SDE @ Bank of New York | CSE, BITS Pilani | MITACS GRI 2022 | Apache Iceberg, Contributor | Dremio | Samsung Electronics
Copy link to contribution
Report contribution
To test security and privacy in big data applications, one should conduct thorough penetration testing, analyze data access controls, encrypt sensitive data, ensure compliance with regulations like GDPR, and regularly audit system logs for suspicious activities.
…see more
Like
1
2
Choose your tools
To test your security and privacy requirements, you need to choose the appropriate tools and frameworks that suit your big data environment and application. There are many tools and frameworks available for testing different aspects of security and privacy, such as vulnerability scanners, penetration testers, data quality checkers, privacy impact assessment tools, and data anonymization tools. You should select the tools that are compatible with your big data platform, such as Hadoop, Spark, or NoSQL databases, and that can handle the volume, velocity, and variety of your data. You should also evaluate the performance, scalability, reliability, and usability of the tools before using them.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Devi Pradeep Vasala
Google DSC Lead '23 | Beta MLSA@Microsoft | Google Cloud Facilitator '23 | Google CCP Facilitator'22 | 7X Azure Certfied | 94X Skill Badges | Servicenow CSA & CAD | Google Cloud CDL | Google Cloud | AWS | Azure |
Copy link to contribution
Report contribution
Testing security and privacy in big data applications is crucial for ensuring data integrity, confidentiality, and availability.
Penetration Testing Tools like Metasploit and Nmap are playing a key role. Metasploit is a powerful framework for developing, testing, and executing exploit code against a remote target machine. It can help identify security weaknesses in big data systems. Also, Nmap is a network scanning tool that can discover hosts and services on a computer network, thus helping identify potential entry points for attackers.
…see more
Like
6
3
Implement your tests
Once you have chosen your tools, you need to implement your tests according to your security and privacy requirements. You should design your tests to cover all the stages of your big data lifecycle, from data collection to data analysis to data sharing. You should also test your application from different perspectives, such as functional, non-functional, and user. You should use different types of tests, such as unit tests, integration tests, system tests, and acceptance tests, to verify that your application meets the expected security and privacy standards and specifications. You should also use different techniques, such as white-box testing, black-box testing, and gray-box testing, to evaluate your application from different levels of access and knowledge.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Chander Parkash
Creating new Software Engineers 👨💻 | Java | Go (Golang) | Spring Boot | Spring Core & MVC | TypeScript | Nest.js | Angular | Flutter | PostgreSQL
Copy link to contribution
Report contribution
Implementing tests tailored to your security and privacy requirements is paramount in ensuring the robustness of your big data application. Design your tests to encompass every stage of the data lifecycle, from collection to analysis to sharing, to comprehensively evaluate security measures. Adopt a multi-faceted approach by testing from various perspectives, including functional, non-functional, and user viewpoints. Employ a diverse array of test types such as unit tests, integration tests, system tests, and acceptance tests to validate adherence to security standards and specifications.
…see more
Like
1
4
Analyze your results
After you have implemented your tests, you need to analyze your results to identify any security and privacy issues or gaps in your big data application. You should collect and store your test results in a secure and organized manner, using tools such as dashboards, reports, or logs. You should also use metrics and indicators to measure and evaluate the security and privacy performance of your application, such as the number of vulnerabilities detected, the severity of the risks, the compliance rate, or the user satisfaction. You should also compare your results with the best practices and benchmarks in your domain and industry, and with the feedback from your users and stakeholders.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Improve your application
Based on your analysis, you need to improve your big data application to address any security and privacy issues or gaps that you have found. You should prioritize the issues according to their impact and urgency, and assign them to the responsible teams or individuals. You should also implement the appropriate solutions, such as fixing the bugs, patching the vulnerabilities, enhancing the features, or updating the policies. You should also document and communicate the changes and improvements that you have made, and verify that they have resolved the issues or gaps effectively. You should also monitor and maintain your application regularly, and repeat the testing process as needed.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Chander Parkash
Creating new Software Engineers 👨💻 | Java | Go (Golang) | Spring Boot | Spring Core & MVC | TypeScript | Nest.js | Angular | Flutter | PostgreSQL
Copy link to contribution
Report contribution
Based on our analysis, it's imperative to enhance your big data application to mitigate any identified security and privacy vulnerabilities effectively. Prioritize these issues based on their impact and urgency, assigning responsibility to the respective teams or individuals. Implement appropriate solutions such as bug fixes, vulnerability patches, feature enhancements, or policy updates. Document and communicate these changes comprehensively, ensuring transparency and understanding across the organization. Verify the effectiveness of these improvements through rigorous testing and validation procedures. Establish a robust monitoring and maintenance protocol to continuously oversee the application's performance and security posture.
…see more
Like
6
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Computer Science
Computer Science
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Computer Science
No more previous content
Your team has diverse experience levels. How can you smoothly shift to agile methodologies?
2 contributions
Developers are pushing back on technical debt reduction. How will you convince them to prioritize it?
Your tech stack is holding back scalability. How will you overcome this obstacle?
5 contributions
You're facing team conflicts in a remote coding project. How will you navigate the challenges?
13 contributions
Your team is divided on prioritizing technical debt. How do you decide which issue to tackle first?
2 contributions
Your software project is facing scope creep due to external dependencies. How will you regain control?
1 contribution
Your team is facing recurring issues in code reviews. How can you break the cycle and improve the process?
3 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Data Engineering
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Architecture
How do you secure and protect your data during ingestion and transit?
Location Intelligence
How can location intelligence help you protect your semantic geospatial data from cyberattacks?
Data Engineering
How can you secure data privacy when using data querying languages?
Machine Learning
How can you protect data privacy and security when using machine learning?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
2
4 Contributions"
Big Data & New Technologies Enhance Genetic Testing,"Big Data & New Technologies Enhance Genetic Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
graphicstock.com
Big Data & New Technologies Enhance Genetic Testing
Report this article
Christopher Colucci
Christopher Colucci
Vice President, Information Technology | Biotech | Clinical, Commercial, & Emerging Technologies
Published May 21, 2015
+ Follow
Big Data has greatly advanced our ability to harvest vast amounts of information in the world of biomedical genetic testing. Through the advancements of genetic testing and big data, we can now advance our understanding of human health and disease. New tools overcome our precious inability to lack access to the mysteries of the DNA in our genetic history.
The
National Institutes of Health
(NIH) launched the Big Data to Knowledge (BD2K) initiative in 2012. The mission of BD2K along with the NIH was to give a huge push in biomedical research and facilitate discovery using technology to enhance medical breakthroughs.
It's worked. Several new biotech genetic testing companies using big data are emerging every day. Burlingame, Calif.-based
Color Genomics
provided genetic testing for breast and ovarian cancer using data-based analysis at one-sixteenth the cost of the main test that is currently used. Before the emergence of using high-tech advances for  tests including the breast cancer genes BRCA1 and BRCA2, companies like Myriad Genetics  of Salt Lake City, Utah charged about $4,000 for its most expensive test.  Yet, in 2013, Myriad lost a Supreme Court case and it's patent on the genes. This opened the doors  to other companies to do the same tests for much less. Companies like
Ambry Genetics
,
Quest Diagnostics
, and
In Vitae
, have introduced cheaper versions of the test using big data technology and dropping the price by about 75 percent. A big part of the reason is using new genetic sequencing technology from
Illumina
.
Another upstart in the genetic testing industry that has caused great disruption is Palo Alto, Calif. based,
Theranos
. Requiring just a pin-prick, Theranos can perform hundreds of tests previously requiring several tubes of blood drawn from a vein. With just a drop, Theranos can test cholesterol levels to sophisticated genetic analyses. The results are faster and more accurate as well as a lot cheaper than conventional methods.
The implications are staggering. Using the easy access to the messaging in our veins, we have an extraordinary window on our own health. We have an entirely new world of diagnostic tests that could allow us to avert serious diseases from cancer to diabetes to heart disease. Theranos charges less than 50 percent of the standard Medicare and Medicaid reimbursement rates. To make even more patient-friendly, Theranos lists its prices  on its website: blood typing, $2.05; cholesterol, $2.99; iron, $4.45. If all tests in the US were performed at those kinds of prices, the company says, it could save Medicare $98 billion and Medicaid $104 billion over the next decade.
This would never have happened if these new biotech innovators hadn't integrated the technology of big data into genetic testing. By doing so they have made testing easy and inexpensive.
Grow and flourish in the digital age.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
7
3 Comments
Joseph Dubow CPA CGMA MBA Lean Six Sigma Black Belt
CFO | Advisor to CEO/BODs | Coach | Trusted to Guide Critical Business Decisions Driving Shareholder Value | Private Equity
9y
Report this comment
Great insight!
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
Digital Health: IBM making sense of all that Data
Jun 11, 2015
4 Digital Health Companies That Drive Innovative Solutions
Jun 5, 2015
Digital Transformation: Challenge of Business Connection to Customers
May 29, 2015
3 Key Drivers of Mobile Health Adoption
May 6, 2015
Smart Bandage Offers a Novel Digital Health Solution for Wound Care
Apr 16, 2015
Are you ready to lead Digital Health?
Apr 10, 2015
More Doctors Turn To mHealth Apps To Enhance Care
Apr 9, 2015
Clinical Trial Innovation: Speeding the process through technology
Apr 2, 2015
Trends in mHealth: The Heart of the Matter
Mar 26, 2015
Internet of Things in Pharma & Healthcare
Mar 19, 2015
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Define and Optimize Test Data Quality and Coverage,"How to Define and Optimize Test Data Quality and Coverage
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Test Management
How do you define test data quality and coverage criteria?
Powered by AI and the LinkedIn community
1
Test data quality
Be the first to add your personal experience
2
Test data coverage
Be the first to add your personal experience
3
Test data optimization
Be the first to add your personal experience
4
Here’s what else to consider
Be the first to add your personal experience
Test data is the input that you use to execute your test cases and verify the expected outcomes. It is a crucial component of test management, as it can affect the quality, efficiency, and effectiveness of your testing process. But how do you define test data quality and coverage criteria? In this article, we will explore some key aspects and best practices to help you optimize your test data.
Find expert answers in this collaborative article
Experts who add quality contributions will have a chance to be featured.
Learn more
See what others are saying
1
Test data quality
Test data quality is essential for meeting the requirements, specifications, and scenarios of your testing scope. Poor test data quality can lead to false positives, false negatives, missed defects, or wasted time and resources. To ensure test data quality, you should define clear and relevant test data requirements based on your test objectives and design. Additionally, reliable and secure sources of test data should be used such as production data, synthetic data, or data generators. Furthermore, validating and verifying your test data before and after use is also necessary with tools or techniques such as data profiling, cleansing, masking, or anonymization. Finally, managing and maintaining your test data throughout the testing lifecycle is also important with tools or techniques such as version control, backup, refresh, or archiving.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
2
Test data coverage
Test data coverage refers to the breadth and depth of the different aspects, dimensions, and variations of your testing scope. Low test data coverage can result in incomplete testing, undetected defects, or low confidence in your test results. To ensure test data coverage, you should define clear and relevant criteria based on your test objectives and design. Utilize various types of test data, such as positive, negative, boundary, equivalence, or combinatorial data to cover different scenarios, cases, and paths. Additionally, consider using risk-based testing, pairwise testing, or orthogonal arrays to prioritize and optimize your test data selection and generation. Lastly, measure and monitor your test data coverage with tools or metrics such as coverage matrix, coverage percentage, or coverage gap analysis.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Test data optimization
Test data optimization is the process of improving the quality and coverage of test data while minimizing the cost and complexity of creating and managing it. This is a continuous and iterative process that involves planning, designing, creating, using, and maintaining test data. To optimize test data, align your test data strategy with your test strategy and business goals. Additionally, adopt a test data management framework to define roles, responsibilities, processes, tools, and standards for test data activities. Automating test data creation, validation, and maintenance with tools or scripts can generate or refresh test data on demand. You can also review and evaluate test data performance using tools or reports that provide feedback, insights, or recommendations. Test data is an essential part of the test management process. By defining quality and coverage criteria for test data you can guarantee it is fit for purpose and sufficient for testing needs. Optimizing your test data will improve testing efficiency, effectiveness, and value.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Test Management
Test Management
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Test Management
No more previous content
How do you cope with the challenges and trends of test management in the digital era?
8 contributions
How do you prepare for the test management certification exam?
10 contributions
What are the best practices for remote test management in a hybrid work environment?
8 contributions
How do you cope with the increasing complexity and diversity of test management scenarios and requirements?
13 contributions
What are the best practices for writing clear and concise test cases for high-risk scenarios?
26 contributions
How do you evaluate and incorporate feedback and lessons learned from test management activities?
8 contributions
No more next content
See all
More relevant reading
Test Strategy
How do you ensure test data validity and coverage for different types of testing scenarios?
Data Engineering
What challenges do you face when testing data and how can you overcome them?
Data Processing
How do you design test cases and scenarios for complex data transformations?
Data Quality
What data quality tools and techniques do you use in your framework?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
1"
Emulating big data: rainmakers to test massive disaster warnings,"Emulating big data: rainmakers to test massive disaster warnings
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Emulating big data: rainmakers to test massive disaster warnings
Report this article
Ademir Xavier
Ademir Xavier
Physicist at the Brazilian Space Agency - AEB
Published May 6, 2015
+ Follow
S
uppose you have about 2000 sources of data, and that somehow you have to analyze and organize incoming information in real time in order to provide critical warnings based on this data. The link between each source and your data center may not be available with the same efficiency and frequency for all sources at the same time and you have to detect any link anomaly in order to use neighbor senders as redundant sources. There is no ""easy"" data model to help the task.
This is a real case scenario that happens for example if we have a network of rain stations scattered throughout  a vast territory, such as CEMADEN network (National Center for monitoring and warning of Natural Disasters, o
www.cemaden.gov.br)
. Signals for anomalies in rain precipitation, for example, may be generated at different warning levels. However, prior to creating a real warning system and committed to deliver trustable signals, an emulation of the whole network seems a recommended job, since natural data cannot be controlled (one can not make rain).
To simulate such huge dataflow, one needs a way to make rain virtually (Richardson et al 1981). We  must a programming environment in which each rain sensor has certain inner attributes (such as minimum precipitation accuracy, integration time, latitude, longitude etc) and external features related to ""rain properties"". Any “real” simulation of rain hydrographs may take into account the
strong correlation between neighbor places
(due to cloud dimensions) and assume a given statistical distribution of rain (Haan et al 1976).  However, an initial approach is to have no correlation at all and assume statistically independent sources. Such approximation tends to be accurate if each data source is sufficiently separated (geographically) from each other.
Example of simulated rain hydrograph for a pluviometric station in Campinas/SP using sample intervals of 30 minutes. The x-axis is in days (or fractions thereof) while the y-axis is the rain volume in millimeters. The distribution of rain follows a very specific statistics that needs to be calibrated (expected frequency and volumes).
The usage of classes and inheritance concepts as available in C++ for example seems highly recommended, because the work of a single rain station may be easily multiplied thousandfold by defining station collections, making the information flow approach the expected  scenario. Moreover, many network communication problems may be easily simulated by adding delay times and other intermittent issues always calibrated by observed statistical deviates (that is we learn the field statistics and applied it to our model).
In conclusion, a way of dealing with the problem of infering models in big data may be partially solved by
emulating information sources under a certain assumed model
. Since the  model is known a priori, several numerical tools for data analysis may be evaluated in their performance in re-extracting the ""hidden"" data structure. By adding noise to data, the effect of external perturbations can be simulated to the limit when any model extraction become unfeasible. Multidimensionality effects may also be emulated by creating multidimensional models. In our case, making virtual rain has been helpful in defining overflow limits (in a highly improbable situation when all data sources trigger warnings) and in studying the onset of potential failures between data sources and the final warning system.
References
Haan, C. T., Allen, D. M., & Street, J. O. (1976).
A Markov chain model of daily rainfall
. Water Resources Research, 12(3), 443-449.
Richardson, C. W. (1981).
Stochastic simulation of daily precipitation, temperature, and solar radiation
. Water Resources Research, 17(1), 182-190.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
3
To view or add a comment,
sign in
More articles by this author
No more previous content
Como usar a tecnologia de maneira benéfica?
Aug 7, 2019
Usando o MS Excel no Mercado de ações
Feb 24, 2019
A resistência dos cientistas às descobertas científicas
May 11, 2018
Sobre atrasos em projetos de natureza tecnológica
Apr 11, 2018
O índice de Maturidade Tecnológica
Apr 11, 2018
A ESPERADA QUEDA DA ESTAÇÃO ESPACIAL CHINESA TIANGONG 1
Mar 6, 2018
Uma revolução na propulsão espacial está em curso
Jan 16, 2017
Workshops do programa GLOBE no Brasil
Nov 23, 2016
The balloonist manager and the engineer
Mar 6, 2016
Sobre a política na gerência e a gerência na política.
Dec 3, 2015
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Tools and Technologies for Data Quality Management,"Tools and Technologies for Data Quality Management
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Tools and Technologies for Data Quality Management
Report this article
Robert Seltzer
Robert Seltzer
Product and Marketing Leader | AI and Strategic Advisor | Iraq War Veteran | ex-Intel , ex- SOCOM |  Board Member |  AI Newsletter | Real Estate Investor
Published Jun 13, 2024
+ Follow
(SemiIntelligent Newsletter, Vol 3, Issue 29)
Managing and improving data quality is essential for the success of AI initiatives. Fortunately, there are several advanced tools and technologies available that can help organizations ensure their data is accurate, complete, and reliable. Here are some of the latest solutions designed to enhance data quality for AI.
Data Cleaning and Preprocessing Tools
Data cleaning and preprocessing tools are essential for transforming raw data into a usable format for AI training. Tools like Trifacta and Alteryx offer intuitive interfaces and powerful capabilities for detecting and correcting errors, structuring data, and enriching datasets. By automating these processes, these tools ensure that the data fed into AI models is accurate, consistent, and ready for analysis, thereby enhancing the overall quality and reliability of AI outcomes.
Trifacta
: Trifacta offers data wrangling solutions that help in cleaning, structuring, and enriching raw data into a form suitable for analysis. Its intuitive interface and machine learning capabilities assist users in detecting and correcting errors, ensuring high-quality data for AI training.
Alteryx
: Alteryx provides a platform for data preparation, blending, and analytics. It allows users to clean and preprocess data through a drag-and-drop interface, making it easier to handle large datasets and prepare them for AI models.
Data Quality Management Platforms
Data quality management platforms like Talend Data Quality and Informatica Data Quality provide comprehensive solutions for maintaining high data standards. These platforms offer tools for data profiling, cleansing, and enrichment, helping organizations detect anomalies, validate data, and ensure consistency. By utilizing these platforms, businesses can ensure that their AI models are trained on reliable and accurate data, leading to more trustworthy and effective AI systems.
Talend Data Quality:
Talend offers comprehensive data quality solutions, including data profiling, cleansing, and enrichment. Its platform helps detect anomalies, validate data, and ensure consistency, which is crucial for accurate AI model training.
Informatica Data Quality:
Informatica’s suite of data quality tools includes capabilities for data profiling, cleansing, matching, and monitoring. It helps organizations maintain high data standards, ensuring that AI systems are trained on reliable and consistent data.
Data Governance and Compliance Tools
Data governance and compliance tools such as Collibra and IBM InfoSphere QualityStage are crucial for managing data quality and regulatory adherence. These tools facilitate data stewardship by ensuring data is accurate, well-documented, and compliant with industry standards. They help organizations maintain high data quality while meeting regulatory requirements, which is essential for building reliable and ethically sound AI models.
Collibra
: Collibra provides data governance solutions that help organizations manage data quality and compliance. Its platform facilitates data stewardship, ensuring that data is accurate, well-documented, and compliant with regulatory requirements.
IBM InfoSphere QualityStage:
Part of IBM’s InfoSphere suite, QualityStage offers robust data quality management and governance features. It helps organizations standardize, validate, and enhance data, making it fit for AI applications.
Automated Data Annotation Tools
Automated data annotation tools like Labelbox and Scale AI are vital for efficiently creating high-quality labeled datasets for AI training. These tools combine machine learning with human oversight to enhance the accuracy and speed of data labeling. By facilitating collaboration and leveraging intelligent algorithms, they ensure that the annotated data is precise and reliable, which is essential for training effective AI models.
Labelbox
: Labelbox provides a platform for training data labeling and annotation. It supports collaboration between human annotators and automated processes, ensuring high-quality labeled data for AI models.
Scale AI
: Scale AI offers solutions for data annotation, including image, video, text, and LiDAR data. Its tools leverage machine learning to assist human annotators, enhancing the accuracy and efficiency of the labeling process.
AI-Powered Data Quality Solutions
Great Expectations: Great Expectations is an open-source platform that helps automate data quality checks. It allows organizations to define, test, and validate data expectations, ensuring that data used for AI training meets predefined quality standards.
TIBCO Clarity: TIBCO Clarity uses AI to automate data quality improvement processes, including data profiling, cleansing, and enrichment. Its intelligent algorithms detect and correct data quality issues, ensuring high-quality data for AI models.
Summary
By leveraging these tools and technologies, organizations can significantly enhance their data quality management processes. These solutions not only improve the accuracy and reliability of the data used for AI training but also ensure that the data aligns with ethical standards and regulatory requirements. Implementing robust data quality management practices is essential for developing trustworthy and effective AI systems.
Next topic
The Ethics of Data Quality in AI
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by Robert Seltzer
Social Media Detox
Aug 10, 2024
Social Media Detox
I'm taking a break from social media, and this time, I'm not setting a return date. I've realized that across all my…
23
2 Comments
Measuring Data Quality: Metrics and KPIs
Jun 21, 2024
Measuring Data Quality: Metrics and KPIs
(SemiIntelligent Newsletter Vol 3, Issue 32) This is my last newsletter, for now, on data and data quality and its…
7
2 Comments
To Err is Human: Addressing Data Bias in AI Models
Jun 20, 2024
To Err is Human: Addressing Data Bias in AI Models
(SemiIntelligent Newsletter Vol 3, Issue 31) Data bias in AI models can lead to skewed results, unfair treatment, and…
7
2 Comments
Data Augmentation Techniques for AI Training
Jun 17, 2024
Data Augmentation Techniques for AI Training
(SemiIntelligent Newsletter Vol 3, Issue 31) Training AI models with insufficient or low-quality data can lead to…
5
1 Comment
The Ethics of Data Quality in AI
Jun 15, 2024
The Ethics of Data Quality in AI
(SemiIntelligent Newsletter Vol 3, Issue 30) The integrity of AI applications is fundamentally dependent on the quality…
2
The Role of Human Oversight in AI Data Curation
Jun 11, 2024
The Role of Human Oversight in AI Data Curation
(SemiIntelligent Newsletter Vol 3, Issue 28) In the world of AI, data is the bedrock upon which algorithms build their…
10
1 Comment
Case Studies: Overcoming Data Quality Challenges
Jun 7, 2024
Case Studies: Overcoming Data Quality Challenges
(SemiIntelligent Newsletter, Vol 3, Issue 27) Data quality is a critical factor in the success of AI projects. Poor…
2
The Impact of Incomplete Data on AI Models
Jun 6, 2024
The Impact of Incomplete Data on AI Models
(SemiIntelligent Newsletter Vol 3, Issue 26) Incomplete data is a common issue that can severely undermine the…
5
Strategies for Ensuring Data Accuracy in AI Datasets
Jun 3, 2024
Strategies for Ensuring Data Accuracy in AI Datasets
(SemiIntelligent Newsletter Vol 3 Issue 25) I am continuing the data theme in the newsletter. I am also striving to…
5
Common Pitfalls in AI Data Collection
May 30, 2024
Common Pitfalls in AI Data Collection
(SemiIntelligent Newsletter Vol 3, Issue 24) Common Pitfalls in AI Data Collection I want to try and make the series I…
6
1 Comment
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
My Path to Learn Big Data and to Pass the AWS Certified Data Analytics – Specialty Certification Exam,"My Path to Learn Big Data and to Pass the AWS Certified Data Analytics – Specialty Certification Exam
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
My Path to Learn Big Data and to Pass the AWS Certified Data Analytics – Specialty Certification Exam
Report this article
Danny Yin
Danny Yin
Software Development Manager, Shopbop an Amazon company
Published Feb 28, 2021
+ Follow
Disclaimer: This article shares my personal experience and does not represent the position of AWS or Amazon.
Preface
As a Partner Solutions Architect at AWS, one of my personal goals is to be certified in three major areas (AI/ML, database, and data analytics) that are most impactful and helpful for my day to day work with my partners and customers. After another 3 months since the previous one, I passed the
AWS Certified Data Analytics – Specialty certification
exam (the exam) on 2/26/2021.  This is my third, and hopefully the last, AWS specialty certification so I decide to write another blog post to give my specialty certification journey a perfect ending. If you are interested in my
Machine Learning
and
Database
Specialty certification exam experiences, please click on the links accordingly. This is also a relatively newer certification exam AWS provides so I am hoping my learning path can be helpful in your AWS journey.
The exam is not at the associate level so you can expect it more difficult than the three associate certification exams. The exam is also not at the professional level with which you find the questions too long with too many distractions. The exam, based on many AWS training instructors and my personal experience, is at the mid-level and is easier than the machine learning certification exam. However, as a seasoned software development engineer knowing database well, I thought this one more difficult than the database certification exam and it indeed was after I took the exam. The main reason of this exam more difficult than the database certification exam is that it covers pretty wide range of different topics. Similar to machine learning certification exam, this exam covers
data collection
data processing
data analysis and visualization
security
Unlike the machine learning certification exam, this exam does not involve in data cleansing and feature engineering, but the various storage solutions and data management techniques play critical roles in the exam questions. I spent time carefully reviewing the domains in both of this and the database certification exams after I passed the machine learning certification exam. I found that it would be more beneficial to do the database certification exam first so I could focus less on storage and data management in this exam. I will talk about this more at the end of the strategy section.
Strategy
1. Official AWS Certification Page
The first step before I take any AWS certification exam, I will check the
AWS certification page
. Expanding the ""Specialty AWS Certifications"" section you will find another link to the
AWS Certified Data Analytics – Specialty
landing page where you can download the
exam guide
,
sample questions
, and
schedule an exam
. Reading through the exam guide and taking a look at the sample questions will help you understand the format of the exam better.
2. Online Training Courses
When I started the preparation of the AWS database certification exam 6 months ago, there was no training on ACloudGuru (ACG) online training website for either database or data analytics. Fortunately, there is one for each of the certification exam available on ACG today and you can find the training course of this exam
here
. I also found a preview version on
CloudAcademy
.
The preview on CloudAcademy does not follow the AWS official exam guide quite yet but it provides a pretty good overview of various services on AWS and the hands-on labs are very helpful to dive deeper with the services via AWS console. More importantly, the labs on CloudAcademy are designed with checkpoints that you must pass with or without hints before you can complete them. These checkpoints make the labs more challenging and interactive so if you think the courses in the preview are too simple, you can skip them and go ahead with the labs.
I enjoyed the courses on CloudAcademy because they provided me good overviews. To shorten the learning time, I played the courses with 1.5x speed and completed them in 10 hours, including labs. I was able to skip a few courses and labs because they already showed up in the database preparation course.
If you are familiar with ACG, you will find that the data analytics training course is more aligned with the AWS official exam guide. However, the ACG training course is not deep enough nor does it provide me enough hands-on labs. If you have already had good experiences in data analytics in general or on AWS, you will find the ACG course a good refresher. If you are not familiar with data analytics, make sure you do not rely solely on the ACG training during your preparation.
If you followed my previous blogs, you must have found that I am a good note taker and I take pride in that. I did the same for this exam preparation, however, I found my notes not helpful. This is because there are not many things that you need to memorize for this exam. You actually need to understand how to apply different techniques in different use cases.
3. Online Test Exams
I spent a lot of time on
WhizLabs
. If you have had the AWS Certified Cloud Practitioner or at least 1 associate-level AWS certification, I am sure you are familiar with the WhizLabs. On WhizLabs, there are 1 free and 3 paid practice tests for the
AWS Certified Data Analytics – Specialty
certification exam.
There are 215 practice questions in total on WhizlLabs with the following break-down:
15 questions in the free test
65 questions in the (paid) practice test 1
65 questions in the (paid) practice test 2
65 questions in the (paid) practice test 3
I took all of them and each one three times. Unlike the practice exams for machine learning on WhizLabs, the questions do not change in each set and because of this, my second attempt of each practice test was more like a memory check. However, I found the repeated exercises quite helpful because I read through every question very carefully three times. With the skills I learned how to pick the right answer or eliminate the wrong ones, the repetition helped train my brain in the exam mode.
4. AWS Instructor-Led Trainings
Being an AWS employee, I have the benefit of taking AWS-offered instructor-led trainings free. I took the following to gain more hands-on experience and knowledge in data analytics from AWS' perspective:
Big Data on AWS
Data Warehousing on AWS
Exam Readiness: AWS Certified Data Analytics – Specialty
I also went through the training materials after the AWS trainings twice. These trainings aren't cheap and it is $700 per day but it's really worthy every penny. If you don't want to take all or if you are not an Amazon employee, I would suggest you to at least take the exam readiness training. The exam readiness training provides you practice questions throughout the day and a 50-question practice test to take home. These questions are very similar to the ones in the official exam so it's definitely worthy.
More importantly, the instructor of the exam readiness training teaches you how to read the question then pick the right answer or eliminate the wrong ones. As the exam has a lot of situational questions, understanding the questions is crucial and eliminating the wrong answers is also very critical.
5. Amazon Online Documentation
The exam is not different from others and you must get yourself familiar with
AWS online documentations
for the data analytics services. I found the following services very important after taking the exam
Amazon EMR
Amazon Kinesis (Data Stream, Data Firehose and Data Analytics)
Amazon QuickSight
Amazon Glue
Amazon Elasticsearch Service
Amazon Athena
Amazon Redshift
If you have more time, make sure you go through the AWS
FAQ's
and
premium support knowledge center
to learn more about AWS data analytics services.
6. Take database certification exam first if you plan to take both
As I indicated in the preface section, I found it helpful to work on the database certification exam first and then this exam. The main reason is that the
storage and data management
is a very important domain in data analytics and it also covers heavily in the database certification exam. At the same time, the database certification exam is easier than this exam. I actually got more Redshift related questions in this exam than in the database certification exam. Although this was a bit surprising to me, I felt lucky that I hadn't forgot about Redshift too much even I skipped the same courses in CloudAcademy during my preparation.
Afterword
It did not take me a lot of time preparing for this exam although I kept the same pace of one specialty certification every three months. I kept track of my time on this exam and I spent about one hour every week day for two months, excluding the AWS instructor-led trainings.
I spent about 20 hours on ACG and CloudAcademy plus another 20 hours on WhizLabs. Both of the Big Data on AWS and Data Warehousing on AWS trainings are three-day sessions and exam readiness takes another day so you can calculate your level of effort easily.  I took this exam on Friday, 2/26 so I specifically kept that week only for the WhizLab practice tests (for the third time), which really helped me keep my brain in the exam mode.
If you have any questions, please PM me and good luck!
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
39
4 Comments
Don J.
Senior Data Architect at Oracle
2y
Report this comment
Thanks for sharing your experience Danny!!
Like
Reply
1 Reaction
Austin Momoh
Data Scientist
3y
Report this comment
thanks for sharing i was quite confused where to start from in getting aws certified
Like
Reply
1 Reaction
Tony Hsu
3y
Report this comment
Thanks for sharing
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
My Path to Learn and to Pass the AWS Certified Security – Specialty Certification Exam
Jan 14, 2022
My Path to Learn the AWS Way of DevOps and to Pass the AWS Certified DevOps Engineer – Professional Certification Exam
Jul 26, 2021
My Path to Learn AWS Databases and to Pass the AWS Certified Database – Specialty Certification Exam
Nov 20, 2020
My Path to Learn Machine Learning and to Pass the AWS Certified Machine Learning – Specialty Certification Exam
Sep 3, 2020
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Using Models and Tests with dbt and Databricks: Ensuring Data Quality and Accuracy! ✅,"Using Models and Tests with dbt and Databricks: Ensuring Data Quality and Accuracy! ✅
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Using Models and Tests with dbt and Databricks: Ensuring Data Quality and Accuracy! ✅
Report this article
Abdelbarre Chafik
Abdelbarre Chafik
Senior Data Engineer
Published Jul 27, 2023
+ Follow
Now that you’ve mastered the basics of dbt and Databricks integration (
Article 1
), let’s take it a step further and explore how to use models and tests to ensure data quality and accuracy in your data transformation workflows. By leveraging dbt’s modeling capabilities and testing framework, you can confidently transform your data with precision. Let’s dive in!
🔧 Defining Models
1️⃣ Create a Model:
In your dbt project’s `models` directory, you can build models on top of other models to create complex transformations. Simply reference the existing models in your new model definition. For example:
In this example, we create a model called `
customers
` that references
stg_customers
and
stg_orders
. This allows us to perform any necessary data cleaning or transformations specific to the customer data.
To do this
Create a new SQL file,
models/stg_customers.sql
, with the SQL from the customers CTE in our original query.
Create a second new SQL file,
models/stg_orders.sql
, with the SQL from the orders CTE in our original query.
select
    id as customer_id,
    first_name,
    last_name

from dbt_achafik.jaffle_shop_customers
select
    id as order_id,
    user_id as customer_id,
    order_date,
    status

from dbt_achafik.jaffle_shop_orders
2️⃣
Build Models on Top
: To build a model on top of the `
customer
` model, Edit the SQL in your models/customers.sql file as follows and reference the existing model. For instance:
with customers as (

    select * from {{ ref('stg_customers') }}

),

orders as (

    select * from {{ ref('stg_orders') }}

),

customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from orders

    group by 1

),

final as (

    select
        customers.customer_id,
        customers.first_name,
        customers.last_name,
        customer_orders.first_order_date,
        customer_orders.most_recent_order_date,
        coalesce(customer_orders.number_of_orders, 0) as number_of_orders

    from customers

    left join customer_orders using (customer_id)

)

select * from final
Here, we’re building the `
customer
` model on top of the `
stg_orders
` and
stg_customers
model. By reusing and extending existing models, you can create a layered and modular approach to your data transformations.
Execute
dbt run
This time, when you performed a dbt run, separate views/tables were created for stg_customers, stg_orders and customers. dbt inferred the order to run these models. Because customers depends on stg_customers and stg_orders, dbt builds customers last. You do not need to explicitly define these dependencies.
🧪 Writing Tests
1️⃣
Define Tests
:
Writing tests for models built on top of other models follow a similar approach. Create separate test files and reference the corresponding models. For example:
Create a new YAML file in the models directory, named
models/schema.yml
version: 2

models:
  - name: customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null

  - name: stg_orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'completed', 'return_pending', 'returned']
      - name: customer_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_customers')
              field: customer_id
2️⃣ Run Tests:
Execute the tests using the
Run dbt test
dbt test
and confirm that all your tests passed.
dbt will run the defined tests against the transformed data in Databricks, ensuring the integrity and accuracy of your data transformations.
✅ Data Quality Assurance Achieved!
By building models on top of other models in your dbt project, you can create complex and interconnected data transformations while maintaining a modular and reusable structure. The tests validate the column structure, aggregations, and any other criteria you define, ensuring the quality and accuracy of your data.
💡 Next Steps:
Experiment with additional models, explore different testing scenarios, and fine-tune your data transformation workflows. Stay tuned for more articles where we’ll delve deeper into advanced techniques and best practices for using dbt and Databricks effectively!
#dbt
#Databricks
#DataTransformation
#DataAnalytics
#DataEngineering
#DataQuality
#Testing
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
26
To view or add a comment,
sign in
More articles by this author
No more previous content
🔧Article 1: Setting Up dbt with Databricks 🧱: Supercharge Your Data Transformations!
Jul 12, 2023
Maximizing Efficiency and Productivity with Great Expectations on Databricks 🧱
Jun 27, 2023
Step by step : Seamlessly Send Messages from Databricks to Slack
Jun 21, 2023
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Big Data testing,"Big Data testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data testing
Report this article
Nabarun P.
Nabarun P.
Lead @ CoreFlex Solutions, Inc. | ERP Automation, Salesforce Test , Mobile Test Strategy , Performance Engineering
Published Jun 9, 2015
+ Follow
There is a misconception that Big Data testing is same as DWH testing. There are differences -
Volume: DWH deals with gigabytes where as Big Data  deals with petabytes.
Variety :DWH is structured compare to Big Data holds unstructured data which includes image, RFID, log file etc.
Infrastructure : DWH is a RDBMS system but Big Data is a file system.
So, testing approach should be different. Its not possible to do sample testing in case of Big Data which we generally do for ETL testing. ETL testing can be done using excel macros or UI based automation tool however for Big Data testing , the best tool probably Query Surge. It would be great if tester knows MapReduce.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by this author
No more previous content
Test Automation 
�
Jan 25, 2019
Detection of Defect
Jun 3, 2015
No more next content
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Kibi/Kibana ChEMBL test. The smoothest way into Life Sciences Big Data Browsing?,"Kibi/Kibana ChEMBL test. The smoothest way into Life Sciences Big Data Browsing?
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Kibi/Kibana ChEMBL test. The smoothest way into Life Sciences Big Data Browsing?
Report this article
Giovanni Tummarello
Giovanni Tummarello
Tech Lead & Entrepreneur - CEO/CPO @ Octostar.com
Published Nov 27, 2015
+ Follow
Unspoiler: there is a 4 minute video at the end of this post.  :) -----
In Life Sciences there is (finally) a wide understanding that it is valuable for all to share and contribute to open “precompetitive data”.
More and more often, very large open datasets exist.
For example, the ChEMBL bioassay data, for example, ChEMBL or ChEMBLdb is a manually-curated chemical database of bioactive molecules with drug-like properties,  maintained by the
European Bioinformatics Institute
(EBI), of the European Molecular Biology Laboratory (
EMBL
).
ChEMBL is reasonably large.
It is distributed in compressed Postgres, Oracle or Mysql dumps which turn into well over 20 million records,
interconnected across 63 tables.
A quick peek at the schema might give an idea of the complexity involved. (Colors are used to group together related tables).
Chembl is accessible today via a useful
web interface
, which focuses on the search of records via its direct properties.  e.g. specifying the name of a target or some metadata which is otherwise contained in the record.
This leads to record listing, which can be clicked to reveal relatively simply analytics (E.g. counts of activities and aggregates).
The Pain
On one hand, data scientists have such simple tools as the navigation above, on the other hand, they may of course download the SQL dump and process it via very advanced, but hard to use, tools (SQL, Matlab, Python, R, you name it)
While the latter tools can in theory “do all”, they require very specialized competences and are clearly unsuitable for quick effective data browsing e.g. to obtain answer live while dynamically exploring data.
e.g.
“Are there many
targets
that are related to
papers
that have been studied in the last 2 years and have reported
active results
on
molecules
of this kind? (and could you please plot this in barchart clearly highlighting which organisms they refer to, thanks)”
As today, the answer only comes after writing a long, painful, error prone, SQL query + other tools as required for charting.
Now lets try Kibi/Kibana
Kibana
provides the amazing data search and beautifully configurable analytics/charting to answer the second part of the question, but it is thanks to the
Kibi extensions
that we can zoom in using
relational filters
.
All we have to do is simply load things up into an Elasticsearch cluster.
In this very quick example, we used the Logstash JDBC connector to create 5 indexes in Elasticsearch, relationally interconnected via ID properties as follows:
Note that these indexes do not reflect 1 to 1 the original tables as they incorporate data from severa related tables as needed.
E.g. our
“Targets”
index incorporates all the
“Target Synonyms” (
sql table). As Elasticsearch (same as Solr) beautifully handles multi value attributes so its ok to do so as no loss of search precision will occur (it would in SQL if we were to put simply data together).
At this point, the rest is nothing else but simply configuring Kibi from its user interface.
A few “visualizations” and “dashboards” laters (..and thanks to the NIH online service for rendering molecules as a PNG) here it is
Now, just add your secret research data
Just as easily as it was possible to load this data into Kibi (full, quite simple,
configuration here
) it would be straightforward to load any sort of additional data into the same Elasticsearch cluster, starting from the 50 or so tables we left out from Chembl to any
non
precompetitive data one might have.
By doing so, one would be able to browse it and relate it to ChEMBL – as well as any other dataset - just as easily as was demonstrated in the above demo.
And of course, If you’d like some support on this,
we’d love to help.
But how fast/fun interactive is it really?
Here is a 4 minute screencast recorded live, you judge.
Kibi is Open Source, and at version 0.1 is frankly very stable and usable.
Version 0.2 is coming out next week so if this topic interests you you might want to sign up to our mailing list e.g. via the form at the bottom of the
Kibi homepage
.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
33
2 Comments
João Chaves
Senior Software Architect
8y
Report this comment
Brilliant...
Congrats!
Like
Reply
1 Reaction
Matthias Funke
Solutions Engineer | Digital Assets | Market Data | Network Data at Coin Metrics
9y
Report this comment
Brilliant demonstration of the power of open source and a new class of analytical tools that bring data closer to the end-users.
Like
Reply
1 Reaction
2 Reactions
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
""Contact Tracing"" for COVID-19: methods, efficacy and future
Apr 2, 2020
Modern entity resolution: the AI superpower for Knowledge Graphs
Jul 13, 2019
Graph DBs in Enterprise: Top 3 use cases in which they make sense
Jun 22, 2019
My list of 7 great 2018 advancements in Enterprise Knowledge Graphs (and 2019 recommendations)
Jan 3, 2019
What I learnt from the Data Analytics for Pharma conference (and what ""Ontology Driven"" brings to the picture)
Feb 5, 2018
The LAST Kibi is out (and welcome Siren 10 !)
Nov 15, 2017
Extending Elasticsearch & Kibana for Data Intelligence, now backed by top VCs
Feb 12, 2017
Time/Spatial graph analysis available in Kibi 4.6.3
Jan 2, 2017
Keylines is the most powerful Link Analysis Lib there is. (And you can use it now on Elasticsearch interconnected docs via Gremlin and Kibi)
Sep 12, 2016
A tiny sample of an exciting Open Source analytics future - Radar charts for Kibana/Kibi
Jan 19, 2016
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
Christopher Bergh on LinkedIn: Data Observability and Data Quality Testing Certification…,"Christopher Bergh on LinkedIn: Data Observability and Data Quality Testing Certification…
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Christopher Bergh’s Post
Christopher Bergh
CEO & Head Chef, DataKitchen:  observe & automate every Data Journey so that data teams find problems fast and fix them forever!  Author: DataOps Cookbook, DataOps Manifesto.  Open Source Data Quality & Observability!
4mo
Report this post
Data Observability and Data Quality Testing Certification
https://hubs.ly/Q02HsBbz0
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
Christopher Bergh
CEO & Head Chef, DataKitchen:  observe & automate every Data Journey so that data teams find problems fast and fix them forever!  Author: DataOps Cookbook, DataOps Manifesto.  Open Source Data Quality & Observability!
3mo
Report this post
Data Errors got you down?  Do this simple first step: a Data Quality Circle
https://bit.ly/3Tot3ip
#DataQuality
#dataops
#dataobservability
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
DataKitchen
7,242 followers
3mo
Report this post
Data Errors got you down?  Do this simple first step: a Data Quality Circle
https://bit.ly/3B6cukK
#DataQuality
#dataops
#dataobservability
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Great Expectations
4,428 followers
7mo
Report this post
The question isn’t just whether you test your data quality. It’s how you test it. 

There are plenty of approaches to data quality testing, but only some of them will actually tell you what you need to know and help your organization succeed. 

So how do you separate the wheat from the chaff? Look for what we call the three E’s of good data quality tests. Read our new article to learn: 

🤔 The three E’s of data quality testing

🤔 Why they make any data quality test better

🤔 How Expectations encompass all three

If you’ve got the Three E’s, you’ve got a great test. Give the article a read to learn more:
https://hubs.li/Q02xhz6z0
#dataquality
#dataengineer
#dataarchitect
The 3 E’s of good data quality testing
7
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Tonia Spight-Sokoya PMP PM Expert,  CIAM, ACP-SHRM, CBAP, PSM, ITIL4, Jira Certified
Tonia Spight-Sokoya PMP PM Expert,  CIAM, ACP-SHRM, CBAP, PSM, ITIL4, Jira Certified is an Influencer
Researcher, Change Management, Root Cause Problem-Solving Solutions Expert for Remediation of Risk Planning and GRC - CIO Controls Management Overarching and Executive Summary Reporting | PMP Accredited Certifications
5mo
Report this post
Great and Informative YouTube listening knowledge transfer on ""Ask the Data Governance Coach"": 
✅️ When should I be looking at data quality rules and data quality reporting?
https://lnkd.in/gzrJzc_M
…more
When should I be looking at data quality rules and data quality reporting?
https://www.youtube.com/
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
MP Financial Solutions
64 followers
4mo
Report this post
You have heard the saying, ""Junk in, junk out."" In this day and age of data soaring to new levels, this has never been more true. Managing a company's data is vital to reaching company goals and growing into the future!

Who is your data quality expert? 
Do you know what to do with your data? 
Do you know how to read your data? 

No worries! MPFS can help.
https://lnkd.in/gP82aCFB
…more
Data Quality - Who is your Data Quality Expert?
https://www.youtube.com/
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Hernan Alvarez
President & Board Director
7mo
Edited
Report this post
Check out our latest blog on why Expectations from Great Expectations are the right way for you to build trust in your data!
https://lnkd.in/eTCkCRUz
#dataquality
#greatexpectations
#dataengineering
Great Expectations
4,428 followers
7mo
The question isn’t just whether you test your data quality. It’s how you test it. 

There are plenty of approaches to data quality testing, but only some of them will actually tell you what you need to know and help your organization succeed. 

So how do you separate the wheat from the chaff? Look for what we call the three E’s of good data quality tests. Read our new article to learn: 

🤔 The three E’s of data quality testing

🤔 Why they make any data quality test better

🤔 How Expectations encompass all three

If you’ve got the Three E’s, you’ve got a great test. Give the article a read to learn more:
https://hubs.li/Q02xhz6z0
#dataquality
#dataengineer
#dataarchitect
The 3 E’s of good data quality testing
18
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Charles Verleyen
Data Strategist - Passionate about solving problems with data. Google Cloud is my playground | Co-Founder @Astrafy
4mo
Edited
Report this post
👇  Great articles by
Dylan Anderson
on data quality. 

Those articles take a holistic approach to a problem that is too often tackled via one-off / isolated fixes. There is only one way to have a proper mindset about data quality --- consider it as an output that has a multitude of root causes. It's time to stop firefighting the symptoms and fix the real root causes.

""Data Quality is not a surface-level issue that can be fixed with a few band-aids and a ‘get better’ kiss.""
https://lnkd.in/g7EZGUdR
https://lnkd.in/g6Xk7KFy
Issue #15 – The Data Quality Conundrum (Part 1 – Root Causes)
thedataecosystem.substack.com
7
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Abiodun Eesuola
Data Scientist || 4X Certified || Let's talk business.
6mo
Report this post
✴️ Free code snippet of the day:

♻️ Suppose you have data stored in two dictionaries, and you want to combine them into a single dictionary. The `merge_dictionaries` function allows you to merge the key-value pairs from both dictionaries while handling any overlapping keys appropriately. 
➡️ It's commonly used in business scenarios like data preprocessing or combining results from different sources.
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Data Muscle
💪Build and strengthen your data skills!!
2mo
Report this post
🔍 How do you evaluate
#DataQuality
?

 🧐 Did you know that measuring data quality is one of the key steps to improving it? After all, ""You can't improve what you can't measure!"" 📊

But here’s the catch🪝: accurately assessing data quality isn’t as easy as it sounds. Data quality can be tricky because it’s not just about checking one aspect – you need to evaluate multiple factors. This is where
#DataQualityDimensions
come in. 📏

🎥
https://lnkd.in/gzi5Qj_k
#Completeness
#Accuracy
#Consistency
#Integrity
#Timeliness
#Validity
…more
How to Measure Data Quality? 📏 | Key Data Quality Dimensions 📊
https://www.youtube.com/
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Artificial Intelligence Feed
965 followers
3w
Report this post
Your Data Quality Checks Are Worth Less
Your Data Quality Checks Are Worth Less (Than You Think)
openexo.com
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
4,290 followers
3000+ Posts
5 Articles
View Profile
Connect
More from this author
The DataOps Manifesto Reaches 5,000 Signatories and Now Translated To 14 Languages
Christopher Bergh
5y
Warring Tribes into Winning Teams: Improving Teamwork in Your Data Organization
Christopher Bergh
5y
Four Great DataOps Articles
Christopher Bergh
7y
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more industry insights
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Big Data in PSA Test Market: Rapid Growth Amid Challenges,"Big Data in PSA Test Market: Rapid Growth Amid Challenges
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data in PSA Test Market: Rapid Growth Amid Challenges
Report this article
InnoScope Insights
InnoScope Insights
Unlocking Tomorrow's Insights Today: InnoScope Insights
Published Jun 19, 2024
+ Follow
Base Year 2023 | Historical Data Time Period 2019 to 2023 | Forecast Period 2024-2031
The global PSA Test market was valued at US$ 511 million in 2023 and is anticipated to reach US$ 869.8 million by 2030, witnessing a CAGR of 7.8% during the forecast period 2024-2031.
PSA Test Market
Report is thus ideal for
Identifying New Application/End Use
(Screening, Post-treatment Monitoring, Others)
, Market Entry / Exit Consulting, R&D landscape, Data security and privacy concerns Risk Analysis, Pipeline Products, Market Trends, Assumptions, Research Timelines, Secondary Research and Primary Research, Key Insights from Industry Experts, spread across
93+
Pages and More.
Who is the Top Largest Companies (Marketing heads, Regional heads) of PSA Test?
Abbott
Siemens Healthcare
DiaSorin
Roche
Beckman Coulter
PerkinElmer
Tosoh
Ortho Clinical
Fujirebio
Mediwatch
BodiTech
(CAGR XX % and Revenue (Past Year): XX billion) And More….
(this is only a partial list of the key players, and the complete list is provided in the report.)
Get A Sample Copy Of The PSA Test Report 2024
Research Methodology:
The data for the PSA Test Market is compiled through a combination of expert advice, primary research, and secondary research. Primary research involves gathering valuable insights from in-person and phone interviews, surveys, questionnaires, and opinions from industry experts, key opinion leaders (KOLs), and customers. Regular interviews with industry experts are conducted to gain detailed knowledge about the market and to validate the existing data analysis.
Short Description About PSA Test Market:
Prostate-specific antigen (PSA), also known as gamma-seminoprotein or kallikrein-3 (KLK3), is a glycoprotein enzyme encoded in humans by the KLK3 gene.
The PSA test measures the level of PSA in a man’s blood. For this test, a blood sample is sent to a laboratory for analysis. The results are usually reported as nanograms of PSA per milliliter (ng/mL) of blood.
The global PSA Test market was valued at US$ 511 million in 2023 and is anticipated to reach US$ 869.8 million by 2030, witnessing a CAGR of 7.8% during the forecast period 2024-2030.
The major players in global PSA Test market include Abbott, Siemens Healthcare, DiaSorin, etc. The top 3 players occupy about 50% shares of the global market. North America and Europe are main markets, they occupy over 80% of the global market. CLIA is the main type, with a share over 70%. Screening is the key application, which holds a share about 80%.
This report aims to provide a comprehensive presentation of the global market for PSA Test, with both quantitative and qualitative analysis, to help readers develop business/growth strategies, assess the market competitive situation, analyze their position in the current marketplace, and make informed business decisions regarding PSA Test.
Report Scope:
The PSA Test market size, estimations, and forecasts are provided in terms of sales volume (K Units) and revenue ($ millions), considering 2023 as the base year, with history and forecast data for the period from 2019 to 2030. This report segments the global PSA Test market comprehensively. Regional market sizes, concerning products by Type, by Application, and by players, are also provided.
For a more in-depth understanding of the market, the report provides profiles of the competitive landscape, key competitors, and their respective market ranks. The report also discusses technological trends and new product developments.
The report will help the PSA Test manufacturers, new entrants, and industry chain related companies in this market with information on the revenues, sales volume, and average price for the overall market and the sub-segments across the different segments,
Get A Sample Copy Of The PSA Test Report 2024
Geographical Analysis:
North America
(United States, Canada and Mexico)
accounted for the largest share of over XX in 2024. Europe
(Germany, UK, France, Italy, Russia and Turkey etc.)
is projected to be the fastest-growing region over the forecast period 2024 to 2032
Asia-Pacific (China, Japan, Korea, India, Australia, Indonesia, Thailand, Philippines, Malaysia and Vietnam)
South America (Brazil, Argentina, Columbia etc.)
Middle East and Africa (Saudi Arabia, UAE, Egypt, Nigeria and South Africa)
What Factors PSA Test Market Growth?
The burgeoning demand for specific applications globally directly influences Contract Catering Service market growth.
Screening
Post-treatment Monitoring
Others
What Are the Types of PSA Test Available in The Market?
Based on Product Types the Market is categorized into Below types that held the largest Contract Catering Service market share in 2024.
CLIA
ELISA
Others
TO KNOW HOW COVID-19 PANDEMIC AND RUSSIA UKRAINE WAR WILL IMPACT THIS MARKET - REQUEST A SAMPLE
Key Highlights of This Report:
Market size and growth analysis of the PSA Test Market.
Trends impacting demand for PSA Test services.
Cost and profit analysis, including estimations and cost-saving opportunities.
Market share distribution and competitive landscape among industry players.
Strategic developments such as mergers, partnerships, and innovations.
Factors influencing pricing strategies and raw material sourcing.
Identification of growth opportunities and challenges in the market.
Profiles of major industry players and their market strategies.
Analysis of recent industry trends in revenue generation and market entry.
Recommendations for market entry strategies and optimal marketing channels.
Key Questions Answered in This Report:
How has the global PSA Test market performed so far, and what are the projections for its future performance?
What are the main drivers, challenges, and opportunities in the global PSA Test market?
How does each driver, challenge, and opportunity impact the global PSA Test market?
What are the key regional markets for the PSA Test market?
Which countries offer the most potential in the PSA Test market?
How is the market segmented based on the product?
Which product segment is the most appealing in the PSA Test market?
How is the market segmented based on distribution channels?
Which distribution channel is the most promising in the PSA Test market?
What is the competitive landscape of the market?
Who are the leading players/companies in the global PSA Test market?
Inquire more and share questions if any before the purchase on this report at:
https://www.360marketupdates.com/enquiry/pre-order-enquiry/26664573?utm_source=Linkdinandutm_medium=RAMA24andutm_campaign=Linkdin
Detailed TOC of Global PSA Test Market Professional Industry Research Report (2024-2031)
1 PSA Test Market Overview
1.1 Product Overview and Scope of PSA Test
1.2 PSA Test Segment by Type
1.2.1 Global PSA Test Market Value Comparison by Type (2024-2030)
1.2.2 CLIA
1.2.3 ELISA
1.2.4 Others
1.3 PSA Test Segment by Application
1.3.1 Global PSA Test Market Value by Application: (2024-2030)
1.3.2 Screening
1.3.3 Post-treatment Monitoring
1.3.4 Others
1.4 Global PSA Test Market Size Estimates and Forecasts
1.4.1 Global PSA Test Revenue 2019-2030
1.4.2 Global PSA Test Sales 2019-2030
1.4.3 Global PSA Test Market Average Price (2019-2030)
1.5 Assumptions and Limitations
2 PSA Test Market Competition by Manufacturers
2.1 Global PSA Test Sales Market Share by Manufacturers (2019-2024)
2.2 Global PSA Test Revenue Market Share by Manufacturers (2019-2024)
2.3 Global PSA Test Average Price by Manufacturers (2019-2024)
2.4 Global PSA Test Industry Ranking 2022 VS 2023 VS 2024
2.5 Global Key Manufacturers of PSA Test, Manufacturing Sites & Headquarters
2.6 Global Key Manufacturers of PSA Test, Product Type & Application
2.7 PSA Test Market Competitive Situation and Trends
2.7.1 PSA Test Market Concentration Rate
2.7.2 The Global Top 5 and Top 10 Largest PSA Test Players Market Share by Revenue
2.7.3 Global PSA Test Market Share by Company Type (Tier 1, Tier 2 and Tier 3)
2.8 Manufacturers Mergers & Acquisitions, Expansion Plans
3 PSA Test Retrospective Market Scenario by Region
3.1 Global PSA Test Market Size by Region: 2019 Versus 2023 Versus 2030
3.2 Global PSA Test Global PSA Test Sales by Region: 2019-2030
3.2.1 Global PSA Test Sales by Region: 2019-2024
3.2.2 Global PSA Test Sales by Region: 2025-2030
3.3 Global PSA Test Global PSA Test Revenue by Region: 2019-2030
3.3.1 Global PSA Test Revenue by Region: 2019-2024
3.3.2 Global PSA Test Revenue by Region: 2025-2030
3.4 North America PSA Test Market Facts & Figures by Country
3.4.1 North America PSA Test Market Size by Country: 2019 VS 2023 VS 2030
3.4.2 North America PSA Test Sales by Country (2019-2030)
3.4.3 North America PSA Test Revenue by Country (2019-2030)
3.4.4 U.S.
3.4.5 Canada
3.5 Europe PSA Test Market Facts & Figures by Country
3.5.1 Europe PSA Test Market Size by Country: 2019 VS 2023 VS 2030
3.5.2 Europe PSA Test Sales by Country (2019-2030)
3.5.3 Europe PSA Test Revenue by Country (2019-2030)
3.5.4 Germany
3.5.5 France
3.5.6 U.K.
3.5.7 Italy
3.5.8 Russia
3.6 Asia Pacific PSA Test Market Facts & Figures by Country
3.6.1 Asia Pacific PSA Test Market Size by Country: 2019 VS 2023 VS 2030
3.6.2 Asia Pacific PSA Test Sales by Country (2019-2030)
3.6.3 Asia Pacific PSA Test Revenue by Country (2019-2030)
3.6.4 China
3.6.5 Japan
3.6.6 South Korea
3.6.7 India
3.6.8 Australia
3.6.9 China Taiwan
3.6.10 Indonesia
3.6.11 Thailand
3.6.12 Malaysia
3.7 Latin America PSA Test Market Facts & Figures by Country
3.7.1 Latin America PSA Test Market Size by Country: 2019 VS 2023 VS 2030
3.7.2 Latin America PSA Test Sales by Country (2019-2030)
3.7.3 Latin America PSA Test Revenue by Country (2019-2030)
3.7.4 Mexico
3.7.5 Brazil
3.7.6 Argentina
3.8 Middle East and Africa PSA Test Market Facts & Figures by Country
3.8.1 Middle East and Africa PSA Test Market Size by Country: 2019 VS 2023 VS 2030
3.8.2 Middle East and Africa PSA Test Sales by Country (2019-2030)
3.8.3 Middle East and Africa PSA Test Revenue by Country (2019-2030)
3.8.4 Turkey
3.8.5 Saudi Arabia
3.8.6 UAE
4 Segment by Type
4.1 Global PSA Test Sales by Type (2019-2030)
4.1.1 Global PSA Test Sales by Type (2019-2024)
4.1.2 Global PSA Test Sales by Type (2025-2030)
4.1.3 Global PSA Test Sales Market Share by Type (2019-2030)
4.2 Global PSA Test Revenue by Type (2019-2030)
4.2.1 Global PSA Test Revenue by Type (2019-2024)
4.2.2 Global PSA Test Revenue by Type (2025-2030)
4.2.3 Global PSA Test Revenue Market Share by Type (2019-2030)
4.3 Global PSA Test Price by Type (2019-2030)
5 Segment by Application
5.1 Global PSA Test Sales by Application (2019-2030)
5.1.1 Global PSA Test Sales by Application (2019-2024)
5.1.2 Global PSA Test Sales by Application (2025-2030)
5.1.3 Global PSA Test Sales Market Share by Application (2019-2030)
5.2 Global PSA Test Revenue by Application (2019-2030)
5.2.1 Global PSA Test Revenue by Application (2019-2024)
5.2.2 Global PSA Test Revenue by Application (2025-2030)
5.2.3 Global PSA Test Revenue Market Share by Application (2019-2030)
5.3 Global PSA Test Price by Application (2019-2030)
6 Key Companies Profiled
6.1 Abbott
6.1.1 Abbott Corporation Information
6.1.2 Abbott Description and Business Overview
6.1.3 Abbott PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.1.4 Abbott PSA Test Product Portfolio
6.1.5 Abbott Recent Developments/Updates
6.2 Siemens Healthcare
6.2.1 Siemens Healthcare Corporation Information
6.2.2 Siemens Healthcare Description and Business Overview
6.2.3 Siemens Healthcare PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.2.4 Siemens Healthcare PSA Test Product Portfolio
6.2.5 Siemens Healthcare Recent Developments/Updates
6.3 DiaSorin
6.3.1 DiaSorin Corporation Information
6.3.2 DiaSorin Description and Business Overview
6.3.3 DiaSorin PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.3.4 DiaSorin PSA Test Product Portfolio
6.3.5 DiaSorin Recent Developments/Updates
6.4 Roche
6.4.1 Roche Corporation Information
6.4.2 Roche Description and Business Overview
6.4.3 Roche PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.4.4 Roche PSA Test Product Portfolio
6.4.5 Roche Recent Developments/Updates
6.5 Beckman Coulter
6.5.1 Beckman Coulter Corporation Information
6.5.2 Beckman Coulter Description and Business Overview
6.5.3 Beckman Coulter PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.5.4 Beckman Coulter PSA Test Product Portfolio
6.5.5 Beckman Coulter Recent Developments/Updates
6.6 PerkinElmer
6.6.1 PerkinElmer Corporation Information
6.6.2 PerkinElmer Description and Business Overview
6.6.3 PerkinElmer PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.6.4 PerkinElmer PSA Test Product Portfolio
6.6.5 PerkinElmer Recent Developments/Updates
6.7 Tosoh
6.6.1 Tosoh Corporation Information
6.6.2 Tosoh Description and Business Overview
6.6.3 Tosoh PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.4.4 Tosoh PSA Test Product Portfolio
6.7.5 Tosoh Recent Developments/Updates
6.8 Ortho Clinical
6.8.1 Ortho Clinical Corporation Information
6.8.2 Ortho Clinical Description and Business Overview
6.8.3 Ortho Clinical PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.8.4 Ortho Clinical PSA Test Product Portfolio
6.8.5 Ortho Clinical Recent Developments/Updates
6.9 Fujirebio
6.9.1 Fujirebio Corporation Information
6.9.2 Fujirebio Description and Business Overview
6.9.3 Fujirebio PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.9.4 Fujirebio PSA Test Product Portfolio
6.9.5 Fujirebio Recent Developments/Updates
6.10 Mediwatch
6.10.1 Mediwatch Corporation Information
6.10.2 Mediwatch Description and Business Overview
6.10.3 Mediwatch PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.10.4 Mediwatch PSA Test Product Portfolio
6.10.5 Mediwatch Recent Developments/Updates
6.11 BodiTech
6.11.1 BodiTech Corporation Information
6.11.2 BodiTech PSA Test Description and Business Overview
6.11.3 BodiTech PSA Test Sales, Revenue and Gross Margin (2019-2024)
6.11.4 BodiTech PSA Test Product Portfolio
6.11.5 BodiTech Recent Developments/Updates
7 Industry Chain and Sales Channels Analysis
7.1 PSA Test Industry Chain Analysis
7.2 PSA Test Key Raw Materials
7.2.1 Key Raw Materials
7.2.2 Raw Materials Key Suppliers
7.3 PSA Test Production Mode & Process
7.4 PSA Test Sales and Marketing
7.4.1 PSA Test Sales Channels
7.4.2 PSA Test Distributors
7.5 PSA Test Customers
8 PSA Test Market Dynamics
8.1 PSA Test Industry Trends
8.2 PSA Test Market Drivers
8.3 PSA Test Market Challenges
8.4 PSA Test Market Restraints
9 Research Finding and Conclusion
10 Methodology and Data Source
10.1 Methodology/Research Approach
10.1.1 Research Programs/Design
10.1.2 Market Size Estimation
10.1.3 Market Breakdown and Data Triangulation
10.2 Data Source
10.2.1 Secondary Sources
10.2.2 Primary Sources
10.3 Author List
10.4 Disclaimer
Purchase this report (Price 2900 USD for a single-user license) -
https://www.360marketupdates.com/purchase/26664573?utm_source=Linkdinandutm_medium=RAMA24andutm_campaign=Linkdin
About Us:
Market is changing rapidly with the ongoing expansion of the industry. Advancement in the technology has provided today’s businesses with multifaceted advantages resulting in daily economic shifts. Thus, it is very important for a company to comprehend the patterns of the market movements in order to strategize better. An efficient strategy offers the companies with a head start in planning and an edge over the competitors.
360marketupdates.com
is the credible source for gaining the market reports that will provide you with the lead your business needs.
Contact Us:
360 Market Updates
Phone:
US +1 424 253 0807
UK +44 203 239 8187
Email:
sales@360marketupdates.com
Web:
https://www.360marketupdates.com
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
1
To view or add a comment,
sign in
More articles by InnoScope Insights
Dec 2, 2024
Big Innovation in GNSS Simulators Market till 2032
Global Research Report on GNSS Simulators Market which covers Growth Analysis, Worldwide Market Size, Category Market…
1
Dec 2, 2024
Big Innovation in Fitness Application Market till 2032
Global Research Report on Fitness Application Market which covers Growth Analysis, Worldwide Market Size, Category…
Dec 2, 2024
Big Innovation in AI in Fashion Market till 2032
Global Research Report on AI in Fashion Market which covers Growth Analysis, Worldwide Market Size, Category Market…
Dec 2, 2024
Mini and Micro LED Market Size | CAGR of 2.1% | MARKETS COVERED
Global Research Report on Mini and Micro LED Market which covers Growth Analysis, Worldwide Market Size, Category…
Dec 2, 2024
Global Direct Carrier Billing Platform Market Forecasts 2024 Outlook | Mid-Year Update
Global Research Report on Direct Carrier Billing Platform Market which covers Growth Analysis, Worldwide Market Size…
Dec 2, 2024
Security and Vulnerability Management Market Size | CAGR of 5.5% | RESEARCH DATA
Global Research Report on Security and Vulnerability Management Market which covers Growth Analysis, Worldwide Market…
2
Dec 2, 2024
Extended Warranty Market Size Worldwide Business Value [2032] | 360 Market updates
Global Research Report on Extended Warranty Market which covers Growth Analysis, Worldwide Market Size, Category Market…
Dec 2, 2024
Worldwide Aquafeed and Aquaculture Additives Market Revenue by 2032 US$ 3184 million
Global Research Report on Aquafeed and Aquaculture Additives Market which covers Growth Analysis, Worldwide Market…
Dec 2, 2024
Which are the prominent market players covered in the Aerospace & Defense Thermoplastic Composites Market?
Global Research Report on Aerospace & Defense Thermoplastic Composites Market which covers Growth Analysis, Worldwide…
Dec 2, 2024
Contract Lifecycle Management Software Market Global Size | CAGR of 8.6% | Research Scope
Global Research Report on Contract Lifecycle Management Software Market which covers Growth Analysis, Worldwide Market…
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
No Title,erro
No Title,erro
Gil Benghiat on LinkedIn: Data Observability and Data Quality Testing Certification Series,"Gil Benghiat on LinkedIn: Data Observability and Data Quality Testing Certification Series
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Gil Benghiat’s Post
Gil Benghiat
Founder, VP of Products & Implementation @ DataKitchen
6mo
Report this post
Data Observability and Data Quality Testing Certification Series | DataKitchen
https://bit.ly/3QMPgoR
Data Observability and Data Quality Testing Certification Series | DataKitchen
https://datakitchen.io
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
More Relevant Posts
Ulrich Buschbaum
Providing Concrete Steps to Elevate Your Data Quality
9mo
Report this post
Prevention < Remediation < Failure

Simple, but great way to make the potential negative output of missing
#DataQuality
visible.

In practice prevention is not that often possible, but if you have sufficient data quality checks in place, you will be at least able to remediate a potential negative output.
Andrew Jones
Andrew Jones is an Influencer
📝 Principal Engineer. Instructor. Writer. Created data contracts and wrote the book on it.
9mo
Poor data quality has a cost 💸

The 1:10:100 rule, developed by George Labovitz and Yu Sang Chang back in 1992, is a great way to understand those costs.

It states that:

- The cost of preventing poor data quality at source is $1 per record
- The cost of remediation after it is created is $10 per record
- The cost of failure (i.e. doing nothing) is $100 per record

So, the earlier you deal with poor data quality, the cheaper it is.

Tools and techniques such as data observability and data contracts can help you catch data quality issues earlier.

Check out my article on Medium for more 👇
https://lnkd.in/eUJ9tHeb
#DataQuality
#DataObservability
#DataContracts
𝗧𝗵𝗲 𝟭:𝟭𝟬:𝟭𝟬𝟬 𝗿𝘂𝗹𝗲 𝗼𝗳 𝗱𝗮𝘁𝗮 𝗾𝘂𝗮𝗹𝗶𝘁𝘆
andrew-jones.medium.com
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Pietro La Torre
Data Strategy 🎯 | Data Governance👌🏻| Data Engineering 💻 | ✍🏻 I write about data
8mo
Report this post
I've really appreciated
Andrew Jones
's perspective on cost of poor data quality. 
It made me think about two important aspects that I would like to share:

1️⃣ The “cost” of remediation already assumes that the problem caused by poor quality has been identified, and this is far from trivial: often we don’t even realize we have a problem, or the search for the cause can be as complex as the resolution, if not more. So, the cost of remediation is often just the tip of the iceberg

2️⃣ Some consequences of not properly addressing data quality can be invaluable, such as the loss of reputation with customers. This means that there is not exclusively a cost to absorb to “return” to a healthy situation, but some consequences can remain.

These aspects should highlight the importance of proactive data quality management. 💼📊
#dataquality
#dataproducts
#datacontracts
#datamanagement
Andrew Jones
Andrew Jones is an Influencer
📝 Principal Engineer. Instructor. Writer. Created data contracts and wrote the book on it.
9mo
Poor data quality has a cost 💸

The 1:10:100 rule, developed by George Labovitz and Yu Sang Chang back in 1992, is a great way to understand those costs.

It states that:

- The cost of preventing poor data quality at source is $1 per record
- The cost of remediation after it is created is $10 per record
- The cost of failure (i.e. doing nothing) is $100 per record

So, the earlier you deal with poor data quality, the cheaper it is.

Tools and techniques such as data observability and data contracts can help you catch data quality issues earlier.

Check out my article on Medium for more 👇
https://lnkd.in/eUJ9tHeb
#DataQuality
#DataObservability
#DataContracts
𝗧𝗵𝗲 𝟭:𝟭𝟬:𝟭𝟬𝟬 𝗿𝘂𝗹𝗲 𝗼𝗳 𝗱𝗮𝘁𝗮 𝗾𝘂𝗮𝗹𝗶𝘁𝘆
andrew-jones.medium.com
8
1 Comment
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Andrew Jones
Andrew Jones is an Influencer
📝 Principal Engineer. Instructor. Writer. Created data contracts and wrote the book on it.
9mo
Report this post
Poor data quality has a cost 💸

The 1:10:100 rule, developed by George Labovitz and Yu Sang Chang back in 1992, is a great way to understand those costs.

It states that:

- The cost of preventing poor data quality at source is $1 per record
- The cost of remediation after it is created is $10 per record
- The cost of failure (i.e. doing nothing) is $100 per record

So, the earlier you deal with poor data quality, the cheaper it is.

Tools and techniques such as data observability and data contracts can help you catch data quality issues earlier.

Check out my article on Medium for more 👇
https://lnkd.in/eUJ9tHeb
#DataQuality
#DataObservability
#DataContracts
𝗧𝗵𝗲 𝟭:𝟭𝟬:𝟭𝟬𝟬 𝗿𝘂𝗹𝗲 𝗼𝗳 𝗱𝗮𝘁𝗮 𝗾𝘂𝗮𝗹𝗶𝘁𝘆
andrew-jones.medium.com
270
13 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
sharath reddy
Technical Account Manager | Advanced certificate in Machine learning and cloud
9mo
Edited
Report this post
Acceldata
is solving and will help enterprises solve the problem at $1 with its shift left approach. Find an exciting read at
docs.acceldata.io
#acceldata
#dataquality
#dataobservability
#spendintelligence
Andrew Jones
Andrew Jones is an Influencer
📝 Principal Engineer. Instructor. Writer. Created data contracts and wrote the book on it.
9mo
Poor data quality has a cost 💸

The 1:10:100 rule, developed by George Labovitz and Yu Sang Chang back in 1992, is a great way to understand those costs.

It states that:

- The cost of preventing poor data quality at source is $1 per record
- The cost of remediation after it is created is $10 per record
- The cost of failure (i.e. doing nothing) is $100 per record

So, the earlier you deal with poor data quality, the cheaper it is.

Tools and techniques such as data observability and data contracts can help you catch data quality issues earlier.

Check out my article on Medium for more 👇
https://lnkd.in/eUJ9tHeb
#DataQuality
#DataObservability
#DataContracts
𝗧𝗵𝗲 𝟭:𝟭𝟬:𝟭𝟬𝟬 𝗿𝘂𝗹𝗲 𝗼𝗳 𝗱𝗮𝘁𝗮 𝗾𝘂𝗮𝗹𝗶𝘁𝘆
andrew-jones.medium.com
6
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Jitendra Shimpi
Data Engineer | Expert in AWS Glue, Lambda, PySpark, SQL & Snowflake | Building Scalable ETL Pipelines & Optimizing Big Data Workflows
7mo
Report this post
🚀 Excited to share this insightful article on enhancing data pipeline reliability through comprehensive testing strategies! Whether you're new to managing data pipelines or looking to bolster your existing setup, this article offers a structured approach to ensure data quality and correctness.
Check it out here:
https://lnkd.in/g6V5Xfyw
Discover how to:
✅ Conduct end-to-end system testing to prevent disruptions
✅ Implement data quality testing to maintain integrity
✅ Set up monitoring and alerting for prompt anomaly detection
✅ Strengthen your pipeline with unit and contract testing
Let's optimize data pipeline performance together!
#DataEngineering
#DataQuality
#TestingStrategies
Joseph Machado
How to add tests to your data pipelines
startdataengineering.com
2
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Soda
8,658 followers
9mo
Report this post
🌐  In a time where data can rule the world, here's an insightful article from
Andrew Jones
, inspired by
Hannah Davies
at
Big Data LDN
, which champions a proactive and preventive approach to data quality, mirroring
Soda
's philosophy. 

Here are 5️⃣ standout points from his piece that resonate with the importance of embedding quality checks into engineering processes:

1. 💰 The 1:10:100 Rule principle underlines the cost-effectiveness of preventing poor data quality at its source - $1 per record - compared to remediation post-creation - $10 per record - or the exorbitant costs of inaction - $100 per record. It's a universal truth across industries, emphasizing prevention over cure.

2. ⌚ Delving into the stages of data quality - failure, remediation, and prevention - he highlights the unsustainable costs of ignoring data quality issues and the reactive nature of most current remediation efforts, which often come too late after the data has entered production systems.

3. 🔺 At the pyramid's peak lies prevention, where efforts yield the highest return on investment. Shifting the responsibility of data quality ""left"" to those creating it minimizes downstream impacts and prevents common issues from arising. 

4. 🤝  The adoption of data contracts is gaining traction. These agreements ensure that data producers define and adhere to quality standards before data moves through the pipeline, significantly reducing the chance of poor-quality data production.

5. 👯  The discussion extends beyond technical solutions to address the cultural shift needed within organizations. Emphasizing quality from the outset and throughout the data lifecycle is pivotal in unlocking data's true value.

↔  Andrew's article underscores the pivotal shift towards prevention in data quality management. This approach not only mitigates risks but also aligns with Soda's proactive stance on ensuring data quality right from the start. 

👇  Read the full article and subscribe to Andrew's newsletter linked below for daily tips, ideas, inspiration on anything related to data quality, data engineering, and data platforms.

🙌  Thanks Hannah and Andrew for the inspiration!
#dataquality
#dataobservability
#dataengineering
#dataplatforms
#datacontracts
#datagovernance
Andrew Jones
Andrew Jones is an Influencer
📝 Principal Engineer. Instructor. Writer. Created data contracts and wrote the book on it.
9mo
Poor data quality has a cost 💸

The 1:10:100 rule, developed by George Labovitz and Yu Sang Chang back in 1992, is a great way to understand those costs.

It states that:

- The cost of preventing poor data quality at source is $1 per record
- The cost of remediation after it is created is $10 per record
- The cost of failure (i.e. doing nothing) is $100 per record

So, the earlier you deal with poor data quality, the cheaper it is.

Tools and techniques such as data observability and data contracts can help you catch data quality issues earlier.

Check out my article on Medium for more 👇
https://lnkd.in/eUJ9tHeb
#DataQuality
#DataObservability
#DataContracts
𝗧𝗵𝗲 𝟭:𝟭𝟬:𝟭𝟬𝟬 𝗿𝘂𝗹𝗲 𝗼𝗳 𝗱𝗮𝘁𝗮 𝗾𝘂𝗮𝗹𝗶𝘁𝘆
andrew-jones.medium.com
36
2 Comments
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Craig Brown
Executive and Thought Leadership in ""Gen AI"", ""Machine Learning"", ""Artificial Intelligence"", ""Data Science"", ""Multi-Cloud"", ""Hybrid Cloud"", ""AWS"", ""Azure"", Google Cloud"", ""Data Analytics"" ""MLOps"", ""AIOps""
6mo
Report this post
The Past, Present, and Future of Data Quality Management: Understanding Testing, Monitoring, and Data Observability in 2024: Data quality monitoring. Data testing. Data observability. Say that five times fast.  Are they different words for the same thing? Unique approaches to the same problem? Something else entirely? And […]


The post The Past, Present, and Future of Data Quality Management: Understanding Testing, Monitoring, and Data Observability in 2024 appeared first on Datafloq.
#DataScince
#ArtificialIntelligence
#MachineLearning
The Past, Present, and Future of Data Quality Management
https://datafloq.com
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Anomalo
13,857 followers
6mo
Report this post
Integrating data from diverse sources can feel like an uphill battle.

Streamline your integration process with tools designed for seamless data aggregation, ensuring comprehensive and coherent data insights. 

Learn how Anomalo can make data integration effortless.

Read the blog:
https://bit.ly/3ywhUo7
Why Legacy Approaches to Data Quality Don't Work
https://www.anomalo.com
8
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
XenonStack
28,139 followers
1mo
Report this post
Data validation : Methods and Best Practices
https://hubs.la/Q02TPbdy0
Data validation : Methods and Best Practices
xenonstack.com
3
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
Ethan Gardner
Data Catalog - Data Marketplace - Data Products - Data Governance - Data Democracy
8mo
Report this post
Wondering about
#dataquality
standards for your business data? Find out more about building a rock-solid data quality framework that'll stand the test of time.
#DataManagement
Do You Have a Data Quality Framework?
https://www.actian.com
1
Like
Comment
Share
Copy
LinkedIn
Facebook
Twitter
To view or add a comment,
sign in
2,065 followers
204 Posts
View Profile
Follow
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
View more content
Create your free account or sign in to continue your search
Sign in
Welcome back
Email or phone
Password
Show
Forgot password?
Sign in
or
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
or
New to LinkedIn?
Join now
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
."
Big Data Testing Market 2024 : Detailed Analysis of Current Scenario with Growth Forecasts to 2032,"Big Data Testing Market 2024 : Detailed Analysis of Current Scenario with Growth Forecasts to 2032
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
Big Data Testing Market 2024 : Detailed Analysis of Current Scenario with Growth Forecasts to 2032
Report this article
Industry Research & Forecast Reports
Industry Research & Forecast Reports
360 Research Reports provides an impeccable insight into existing market as well as emerging market trends.
Published May 29, 2024
+ Follow
Insights on the ""Big Data Testing Market"" contribution of various segments including Country and Region wise Historic data (2018 to 2023), and Forecast Market Size (2024 to 2032) - (
Get a Sample Copy of the Cold Chain Drug Logistics Report 2024
)
""
Big Data Testing Market
"" is Expected to Grow Significantly During the Forecast Period, Owing to various business drivers like the increasing demand for Reagent Transport, Vaccine Transport, Tablet Transport is Also Responsible for Driving the Market's Growth.
Browse Detailed TOC of Big Data Testing Market report which is spread across
115+
Pages, Tables and Figures with Charts that provides exclusive data, information, vital statistics, trends, and competitive landscape details in this niche sector.
Who is the largest manufacturers of Big Data Testing Market worldwide?
HFGHFGHGTEN Technologies
Testplant
Robotium tech
IBM Corporation
Tricentis
Real-Time Technology Solutions
Codoid
Infosys Limited
Cigniti Technologies Limited
BSFI
IT and Telecommunications
Transportation and Logistics
Manufacturing
Government and Defence
E-commerce
Healthcare
Energy and Utilities
Retail
Media and Entertainment
On Premise
On DemandBSFI
IT and Telecommunications
Transportation and Logistics
Manufacturing
Government and Defence
E-commerce
Healthcare
Energy and Utilities
Retail
Media and Entertainment
On Premise
On DemandOn Premise
On Demand
Get a Sample PDF of report -
https://www.360researchreports.com/enquiry/request-sample/24604782
Short Description About Big Data Testing Market:
The Global Big Data Testing market is anticipated to rise at a considerable rate during the forecast period, between 2023 and FFFFF. In 2022, the market is growing at a steady rate and with the rising adoption of strategies by key players, the market is expected to rise over the projected horizon.
According to the latest research, the global Big Data Testing market size was valued at USD XX million in 2022 and is expected to expand at a CAGR of XX% during the forecast period, reaching USD XX million by 2028.
This report elaborates on the market size, market characteristics, and market growth of the Big Data Testing industry between the year 2018 to 2028, and breaks down according to the product type, downstream application, and consumption area of Big Data Testing. The report also introduces players in the industry from the perspective of the value chain and looks into the leading companies.
Get a Sample Copy of the Big Data Testing Report 2024
What are the factors driving the growth of the Big Data Testing Market?
Growing demand for below applications around the world has had a direct impact on the growth of the Big Data Testing
vxcvx BSFI
IT and Telecommunications
Transportation and Logistics
Manufacturing
Government and Defence
E-commerce
Healthcare
Energy and Utilities
Retail
Media and Entertainment ,
What are the types of Big Data Testing available in the Market?
Based on Product Types the Market is categorized into Below types that held the largest Big Data Testing market share In 2023.
HFGHFGHOn Premise
On Demand
Which regions are leading the Big Data Testing Market?
North America (United States, Canada and Mexico)
Europe (Germany, UK, France, Italy, Russia and Turkey etc.)
Asia-Pacific (China, Japan, Korea, India, Australia, Indonesia, Thailand, Philippines, Malaysia and Vietnam)
South America (Brazil, Argentina, Columbia etc.)
Middle East and Africa (Saudi Arabia, UAE, Egypt, Nigeria and South Africa)
Inquire more and share questions if any before the purchase on this report at -
https://www.360researchreports.com/enquiry/pre-order-enquiry/24604782
This Big Data Testing Market Research/Analysis Report Contains Answers to your following Questions
What are the global trends in the Big Data Testing market? Would the market witness an increase or decline in the demand in the coming years?
What is the estimated demand for different types of products in Big Data Testing? What are the upcoming industry applications and trends for Big Data Testing market?
What Are Projections of Global Big Data Testing Industry Considering Capacity, Production and Production Value? What Will Be the Estimation of Cost and Profit? What Will Be Market Share, Supply and Consumption? What about Import and Export?
Where will the strategic developments take the industry in the mid to long-term?
What are the factors contributing to the final price of Big Data Testing? What are the raw materials used for Big Data Testing manufacturing?
How big is the opportunity for the Big Data Testing market? How will the increasing adoption of Big Data Testing for mining impact the growth rate of the overall market?
How much is the global Big Data Testing market worth? What was the value of the market In 2020?
Who are the major players operating in the Big Data Testing market? Which companies are the front runners?
Which are the recent industry trends that can be implemented to generate additional revenue streams?
What Should Be Entry Strategies, Countermeasures to Economic Impact, and Marketing Channels for Big Data Testing Industry?
Big Data Testing Market - Covid-19 Impact and Recovery Analysis:
We were monitoring the direct impact of covid-19 in this market, further to the indirect impact from different industries. This document analyzes the effect of the pandemic on the Big Data Testing market from a international and nearby angle. The document outlines the marketplace size, marketplace traits, and market increase for Big Data Testing industry, categorised with the aid of using kind, utility, and patron sector. Further, it provides a complete evaluation of additives concerned in marketplace improvement in advance than and after the covid-19 pandemic. Report moreover done a pestel evaluation within the business enterprise to study key influencers and boundaries to entry.
Our studies analysts will assist you to get custom designed info to your report, which may be changed in phrases of a particular region, utility or any statistical info. In addition, we're constantly inclined to conform with the study, which triangulated together along with your very own statistics to make the marketplace studies extra complete for your perspective.
Final Report will add the analysis of the impact of Russia-Ukraine War and COVID-19 on this Big Data Testing Industry.
TO KNOW HOW COVID-19 PANDEMIC AND RUSSIA UKRAINE WAR WILL IMPACT THIS MARKET - REQUEST SAMPLE
Detailed TOC of Global Big Data Testing Market Research Report, 2023-2031
1 Market Overview
1.1 Product Overview and Scope of Big Data Testing 1.2 Classification of Big Data Testing by Type 1.2.1 Overview: Global Big Data Testing Market Size by Type: 2017 Versus 2022 Versus 2031 1.2.2 Global Big Data Testing Revenue Market Share by Type in 2022 1.3 Global Big Data Testing Market by Application 1.3.1 Overview: Global Big Data Testing Market Size by Application: 2017 Versus 2022 Versus 2031 1.4 Global Big Data Testing Market Size and Forecast 1.5 Global Big Data Testing Market Size and Forecast by Region 1.6 Market Drivers, Restraints and Trends 1.6.1 Big Data Testing Market Drivers 1.6.2 Big Data Testing Market Restraints 1.6.3 Big Data Testing Trends Analysis
2 Company Profiles
2.1 Company 2.1.1 Company Details 2.1.2 Company Major Business 2.1.3 Company Big Data Testing Product and Solutions 2.1.4 Company Big Data Testing Revenue, Gross Margin and Market Share (2020,2021,2022, and 2023) 2.1.5 Company Recent Developments and Future Plans
3 Market Competition, by Players
3.1 Global Big Data Testing Revenue and Share by Players (2020,2021,2022, and 2023) 3.2 Market Concentration Rate 3.2.1 Top3 Big Data Testing Players Market Share in 2022 3.2.2 Top 10 Big Data Testing Players Market Share in 2022 3.2.3 Market Competition Trend 3.3 Big Data Testing Players Head Office, Products and Services Provided 3.4 Big Data Testing Mergers and Acquisitions 3.5 Big Data Testing New Entrants and Expansion Plans
4 Market Size Segment by Type
4.1 Global Big Data Testing Revenue and Market Share by Type (2017-2023) 4.2 Global Big Data Testing Market Forecast by Type (2023-2031)
5 Market Size Segment by Application
5.1 Global Big Data Testing Revenue Market Share by Application (2017-2023) 5.2 Global Big Data Testing Market Forecast by Application (2023-2031)
6 Regions by Country, by Type, and by Application
6.1 Big Data Testing Revenue by Type (2017-2031) 6.2 Big Data Testing Revenue by Application (2017-2031) 6.3 Big Data Testing Market Size by Country 6.3.1 Big Data Testing Revenue by Country (2017-2031) 6.3.2 United States Big Data Testing Market Size and Forecast (2017-2031) 6.3.3 Canada Big Data Testing Market Size and Forecast (2017-2031) 6.3.4 Mexico Big Data Testing Market Size and Forecast (2017-2031)
7 Research Findings and Conclusion
8 Appendix 8.1 Methodology 8.2 Research Process and Data Source 8.3 Disclaimer
9 Research Methodology
10 Conclusion
Continued….
Purchase this report (Price 3370 USD for a single-user license) -
https://www.360researchreports.com/purchase/24604782
About Us:
360 Research Reports is the credible source for gaining the market reports that will provide you with the lead your business needs. At 360 Research Reports, our objective is providing a platform for many top-notch market research firms worldwide to publish their research reports, as well as helping the decision makers in finding most suitable market research solutions under one roof. Our aim is to provide the best solution that matches the exact customer requirements. This drives us to provide you with custom or syndicated research reports.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
To view or add a comment,
sign in
More articles by Industry Research & Forecast Reports
Nov 29, 2024
Digital Fashion Market Is Expected to See Significant Competition with a Projected CAGR of XX from 2024 to 2032
🌐 Digital Fashion Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Digital…
Nov 29, 2024
Digital Smart Waste Management Solution Market Share | Growth Report, 2032
🌐 Digital Smart Waste Management Solution Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s…
Nov 29, 2024
Adapter Market Is Expected to See Significant Competition with a Projected CAGR of XX from 2024 to 2032
🌐 Adapter Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Adapter Market's…
Nov 29, 2024
Automotive Slack Adjuster Market Share | Growth Report, 2032
🌐 Automotive Slack Adjuster Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the…
Nov 29, 2024
Photodiode Market Share | Growth Report, 2032
🌐 Photodiode Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Photodiode…
Nov 29, 2024
Transmission Electron Microscope (TEM) Market Is Expected to See Significant Competition with a Projected CAGR of XX from 2024 to 2032
🌐 Transmission Electron Microscope (TEM) Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s…
Nov 29, 2024
Conveyor Belts Market Is Expected to See Significant Competition with a Projected CAGR of XX from 2024 to 2032
🌐 Conveyor Belts Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Conveyor…
Nov 29, 2024
Rail Market Is Expected to See Significant Competition with a Projected CAGR of XX from 2024 to 2032
🌐 Rail Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Rail Market's…
1
Nov 29, 2024
Neurofeedback Market Share | Growth Report, 2032
🌐 Neurofeedback Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Neurofeedback…
Nov 29, 2024
Dust Sensors Market Share | Growth Report, 2032
🌐 Dust Sensors Market Outlook 2024-2032: Unlocking Growth with Advanced Analytics 🌐 What’s Driving the Dust Sensors…
Show more
See all articles
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
PART 2 - ROAD MAP - Non-Tech to Big Data Testing,"PART 2 - ROAD MAP - Non-Tech to Big Data Testing
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
PART 2 - ROAD MAP - Non-Tech to Big Data Testing
Report this article
Krishna Kayaking 🇮🇳
Krishna Kayaking 🇮🇳
Big Data Engineer | AWS | Azure | Python | Spark | SQL | Databricks | YouTuber | Storyteller | Musician | 5k LinkedIn | 3k YT | 100k Insta
Published Oct 13, 2022
+ Follow
I am really delighted to see your overwhelming response on part 1 of the road map. As I said, If I get a good response, I will get ETL Testing & Testing related topics to cover.
In this article & in this video, we will cover ETL Testing & Testing Practice-related topics that you will need to cover to crack interviews.
In the Part 1 - Covered all the main topics.
Watch that Video – I button before watching this video.
Watch that Video & come back.
Take notes as per requirement.
Most of the details – LinkedIn Article.
ROAD MAP FROM NON-TECH TO BIG DATA TESTING
--------------------------------------------------------------------------------------
Part 1 - https://www.linkedin.com/pulse/road-map-from-non-tech-big-data-testing-krishna-kayaking/?trackingId=%2BfsHVpMkQo2sM8Q6PvEl%2FQ%3D%3D
Video 1 - https://www.youtube.com/watch?v=aK9jWkfS-wM
-------
Part 2 - https://www.linkedin.com/pulse/road-map-part-2-from-non-tech-big-data-testing-krishna-kayaking/?trackingId=q2S%2Bw2SXT8ethb5kU8tgrw%3D%3D
Video 2 - https://www.youtube.com/watch?v=sw8gzSqBeqo
--------
Part 3 – https://www.linkedin.com/feed/update/urn:li:ugcPost:6996484383201267714?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3AugcPost%3A6996484383201267714%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29
Video 3 – https://youtu.be/7mgYcvJUkOU
___________
This is Part 2 of the Road Map
Starting with TEST PRACTICES: -
1.   SDLC – Software Development Life Cycle & Types
Sequential Model - waterfall & V-Model
Iterative Model - Agile ModelAd
Advantages & Disadvantages
2.   Learn the details of AGILE METHODOLOGY
All the different phases in detail
This is one of the most important questions
Learn about the different roles –
Comment “
AGILE
” if you would like to me do one video separately on this topic
3.   Learn Scrum Framework
Picture – refer to the Article
Comment “SCRUM Framework” if you would like me to make a separate video on this.
4.   Please like the video & comment, I will come up with a video on what all questions will be asked on the Agile & Scrum framework.
5.   STLC – Software Testing Life cycle
a.   All the different phases
b.   Since now you will be giving an interview as tester, so you must be very good at this.
c.   Comment STLC, if you would like to me to make a video on this.
6.   Refer to Testing documents like
a.   BRD – Comment the Full form
b.   Test plan
c.   Test Report
d.   Test summary/coverage
7.   Defect life cycle – Comment DLC, if you want me to make a separate video
8.   Black box, White box, Grey box – basic knowledge
9.   Different Roles in a project –
a.   I have made a separate video on this topic
b.   Check the I button, after completing this video
10. Levels of testing
a.   Basic knowledge of all of it
b.   Unit, Smoke, Sanity, System, Regression, UAT, etc.
c.   Comment
“Levels of Testing”
if you want me to make one separate video on this topic.
11. Test Case template.
a.   I will make a separate video on this, on how I design test cases & what template I use.
b.   Comment.
12. Requirement Traceability matrix
13. Test Strategy
14. Black box testing
a.   Equivalence class partition
b.   Boundary value Analysis
c.   Decision Table Testing
d.   Cosmetic testing
e.   Exploratory Testing
Thank you for watching the video.
Please comment on the topic on which you would like me to make videos. The more the comment on that video, the faster the video will come.
Since the video is long, I will come up with Part 3 of the Road Map, with ETL/DWH Testing Topics.
Check the Video Description for the Articles, and save it for future reference. My success is when you get benefitted from my knowledge.
Also, I am getting a lot of requests for guidance, so I am planning to come up with a Live Q & A session. Please take part in the LinkedIn poll to help me decide. Link in description.
Thank you for watching, see you in the next video. Thursday 7 PM.
Like
Comment
Copy
LinkedIn
Facebook
Twitter
Share
13
3 Comments
Tanveer Hussain
FULL STACK JAVA DEVELOPER
2y
Report this comment
If u guide me i will joon your course
Like
Reply
1 Reaction
See more comments
To view or add a comment,
sign in
More articles by this author
No more previous content
STORY OF ANY MIDDLE-CLASS BOY - Navigating the Abyss: A Heartrending Tale of Struggle and Resilience 💔
Aug 10, 2023
SQL Database Set-up | Kick-start 
 SQL Practice
Jan 19, 2023
PART 3 – ROADMAP – NON-TECH to BIG DATA TESTING
Nov 10, 2022
100% INTERVIEW Question | OLTP vs OLAP | Database vs Dataware House | Testing | Krishna Kayaking
Nov 5, 2022
I QUIT | Give up Sports as a Career???
Oct 31, 2022
Rejected 40 LPA Offer | Big 4 | WHY ???
Oct 31, 2022
What to do after 10th? | Dilemma | No clue | Medical | Engineering | Business | CA | Career ??????
Oct 17, 2022
PART 1 - ROAD MAP from Non-Tech to Big Data Testing
Oct 1, 2022
S1.E6 – MY LIFE CHANGING MOMENT [POST ENGINEERING]

Episode 6 – Offer letters & Compensations
Sep 10, 2022
S1.E5 – MY LIFE CHANGING MOMENT [POST ENGINEERING]

Episode 5 – How to fight & stand in Tough Times. You Get To Know Your Actual Advocates (Supporters
Sep 3, 2022
No more next content
See all
Sign in
Stay updated on your professional world
Sign in
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
New to LinkedIn?
Join now
Explore topics
Sales
Marketing
IT Services
Business Administration
HR Management
Engineering
Soft Skills
See All
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language"
How to Test Data Quality with User Feedback,"How to Test Data Quality with User Feedback
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Engineering
How do you use user feedback to test data quality?
Powered by AI and the LinkedIn community
1
Collect user feedback
2
Analyze user feedback
Be the first to add your personal experience
3
Validate user feedback
Be the first to add your personal experience
4
Implement user feedback
Be the first to add your personal experience
5
Learn from user feedback
Be the first to add your personal experience
6
Here’s what else to consider
Data quality is a crucial aspect of data engineering, as it affects the reliability, usability, and value of data products and services. However, data quality is not only a technical issue, but also a user-centric one. User feedback can provide valuable insights into how data quality meets or fails the expectations and needs of the end-users. In this article, you will learn how to use user feedback to test data quality and improve your data engineering processes and outcomes.
Top experts in this article
Selected by the community from 2 contributions.
Learn more
Liz H.
NED | Strategic Business & Technology Advisor | Digital, AI & Data Transformations | Speaker | Thought Leader | STEM…
View contribution
4
See what others are saying
1
Collect user feedback
The first step to use user feedback to test data quality is to collect it from various sources and channels. You can use surveys, interviews, focus groups, reviews, ratings, comments, bug reports, support tickets, or any other method that allows you to gather user opinions and experiences. You should aim to collect feedback from a representative sample of your user base, covering different segments, profiles, and use cases. You should also collect feedback regularly and consistently, to monitor changes and trends over time.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Chibili Mugala
MSc Data Science • Consultant • Google Certified Analyst • Visual Analytics • AI Researcher
Copy link to contribution
Report contribution
Make continuous improvements iteratively ensuring customer satisfaction. This is not a once-off thing, it shall be done repeatedly ensuring data lifecycle management is kept at a low cost with high efficiency.
…see more
Like
2
Analyze user feedback
The next step is to analyze the user feedback and extract relevant information and insights. You can use qualitative and quantitative methods, such as text analysis, sentiment analysis, topic modeling, clustering, or scoring, to identify common themes, issues, preferences, and expectations. You should also look for patterns, correlations, outliers, and anomalies in the feedback data, and compare them with your data quality metrics and standards. You should also prioritize the feedback according to its impact, urgency, and feasibility.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
3
Validate user feedback
The third step is to validate the user feedback and verify its accuracy and relevance. You can use various techniques, such as A/B testing, usability testing, beta testing, or pilot testing, to test different data quality scenarios and solutions with a subset of users. You should also use data quality tools, such as data profiling, data cleansing, data validation, or data auditing, to check the integrity, completeness, consistency, and timeliness of your data. You should also use data quality dashboards and reports to monitor and communicate your data quality performance and progress.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
4
Implement user feedback
The fourth step is to implement the user feedback and apply it to your data engineering processes and products. You can use agile and iterative methods, such as scrum, kanban, or devops, to incorporate user feedback into your data engineering lifecycle and deliver value faster and more frequently. You should also use data quality frameworks, such as DAMA, ISO, or TDWI, to align your data quality goals and practices with your business objectives and requirements. You should also use data quality best practices, such as data governance, data lineage, or data cataloging, to ensure your data quality is maintained and improved over time.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
5
Learn from user feedback
The final step is to learn from the user feedback and use it to enhance your data engineering skills and knowledge. You can use data quality feedback loops, such as PDCA, to plan, do, check, and act on your data quality improvement initiatives. You should also use data quality maturity models, such as CMMI or DMM, to assess your data quality capabilities and gaps, and identify areas for improvement. You should also use data quality communities and resources, such as blogs, podcasts, or courses, to stay updated and informed on the latest data quality trends and best practices.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
6
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Liz H.
NED | Strategic Business & Technology Advisor | Digital, AI & Data Transformations | Speaker | Thought Leader | STEM Ambassador | Empowering Organisations to Unlock the Value of their Data by Making Data Relatable
Copy link to contribution
Report contribution
In other things to consider: Data literacy.  Why and how do your colleagues care for DQ unless you support them with why it's important; the benefits of greater accuracy; how they can help improve DQ.
All key in maturing your data culture with the right behaviours and beliefs for developing a data driven organisation to grow a successful organisation.
…see more
Like
4
Data Engineering
Data Engineering
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Engineering
No more previous content
Clients demand both real-time and batch processing. How do you prioritize their needs?
2 contributions
You're faced with data accuracy and tight ETL deadlines. How do you strike a balance between the two?
3 contributions
You're facing a surge in data volume. How can you fine-tune ETL pipelines to manage it effectively?
4 contributions
You're facing a complex data migration. How do you prevent data loss?
20 contributions
You're facing conflicting opinions on data infrastructure upgrades. How do you prioritize tasks effectively?
3 contributions
Your organization is hesitant about cloud migration. How can you convince them of the benefits?
4 contributions
Struggling to balance data quality and processing speed in ETL workflows?
13 contributions
You've faced a data anomaly in your analysis. How do you prevent it from happening again?
4 contributions
You're facing data pipeline performance issues. How can you optimize them without causing disruption?
6 contributions
Clients are frustrated with delays in real-time data delivery. How do you handle their complaints?
7 contributions
You need to onboard new team members on data governance. How do you make it effective and engaging?
11 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Analytics
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Quality
How do you prepare for emerging data quality trends and challenges?
Data Engineering
What are the most effective techniques for improving data quality in engineering leadership?
Data Analytics
How can you find the most efficient data analysis tools for automating repetitive tasks?
Data Engineering
What techniques can you use to ensure business requirements are met by data collection?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
3
2 Contributions"
How to Test Big Data Analytics Project Results,"How to Test Big Data Analytics Project Results
Agree & Join LinkedIn
By clicking Continue to join or sign in, you agree to LinkedIn’s
User Agreement
,
Privacy Policy
, and
Cookie Policy
.
Skip to main content
LinkedIn
Articles
People
Learning
Jobs
Games
Join now
Sign in
All
Engineering
Data Analytics
What are the best ways to test big data analytics project results?
Powered by AI and the LinkedIn community
1
Data quality
2
Data transformation
3
Data analysis
4
Data security
5
Data governance
6
Data communication
7
Here’s what else to consider
Big data analytics projects can deliver valuable insights and solutions for various domains and challenges, but they also require rigorous testing to ensure accuracy, reliability, and relevance of the results. Testing big data analytics project results is not a trivial task, as it involves dealing with large volumes, variety, and velocity of data, complex algorithms, and multiple stakeholders. In this article, you will learn about some of the best ways to test big data analytics project results, based on the following aspects:
Top experts in this article
Selected by the community from 9 contributions.
Learn more
Abdullah Abo Milhim
Educator | Corporate Trainer | Keynote Speaker | External Examiner | Mentor | Fashion & Luxury Consultant | Digital…
View contribution
4
David Heraty
Senior Commercial Manager | Oracle Advisory | Oracle Cloud Solutions Partner
View contribution
1
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data…
View contribution
1
See what others are saying
1
Data quality
Data quality is the foundation of any big data analytics project, as it affects the validity and usability of the results. Data quality testing involves checking the data for completeness, correctness, consistency, timeliness, and uniqueness, as well as identifying and resolving any issues such as missing values, outliers, duplicates, or errors. Data quality testing can be done using various tools and techniques, such as data profiling, data cleansing, data validation, and data auditing.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Abdullah Abo Milhim
Educator | Corporate Trainer | Keynote Speaker | External Examiner | Mentor | Fashion & Luxury Consultant | Digital Transformation Consultant | Higher Education Consultant | Financial Analyst  | Change Management Expert
Copy link to contribution
Report contribution
Data quality testing needs to be an ongoing process, and deeply integrated into the development and deployment lifecycle of big data analytics projects / ventures. When it comes to analytics results, it is vital that we conduct regular monitoring and validation of data quality in the context of the management systems and processes that correlate to them.
…see more
Like
4
David Heraty
Senior Commercial Manager | Oracle Advisory | Oracle Cloud Solutions Partner
Copy link to contribution
Report contribution
Oracle knows that having trustworthy data really matters for getting good results from analysis. That's why when they check data quality, they look at elements like making sure data is complete, accurate, consistent, up-to-date, and one-of-a-kind
…see more
Like
1
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data Solutions, Business Transformation
Copy link to contribution
Report contribution
Data quality is the cornerstone of any big data analytics project. Ensuring data quality involves verifying that the data is accurate, complete, consistent, and relevant to the project's objectives. Testing techniques include:

a. Data Profiling: Analyze the data's structure, characteristics, and statistical properties to identify anomalies and inconsistencies.

b. Data Cleaning: Identify and correct errors, missing values, and inconsistencies in the data.

c. Data Validation: Check if the data adheres to defined rules, constraints, and business logic.
…see more
Like
2
Data transformation
Data transformation is the process of converting the raw data into a suitable format and structure for analysis, such as aggregating, filtering, sorting, joining, or enriching the data. Data transformation testing involves verifying that the data transformation logic is correct and that the transformed data meets the specifications and expectations of the analysis. Data transformation testing can be done using various methods, such as comparing the source and target data, applying business rules and calculations, or using test cases and scenarios.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data Solutions, Business Transformation
Copy link to contribution
Report contribution
Create data transformation scripts or use data transformation tools to convert raw data into a format suitable for analysis.
Implement data transformation logic to extract, transform, and load (ETL) the data into a data warehouse or data lake.
Validate the transformed data to ensure no errors or inconsistencies were introduced during the transformation process.


Using data transformation tools like Databricks, Azure Synapse to automate data transformation tasks.
…see more
Like
3
Data analysis
Data analysis is the core of any big data analytics project, as it involves applying various techniques and tools to explore, model, and interpret the data, such as statistics, machine learning, visualization, or reporting. Data analysis testing involves ensuring that the data analysis methods are appropriate and effective, and that the results are accurate and meaningful. Data analysis testing can be done using various approaches, such as reviewing the analysis design and assumptions, validating the analysis outputs and outcomes, or evaluating the analysis performance and scalability.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data Solutions, Business Transformation
Copy link to contribution
Report contribution
Let say we are analyzing customer behavior data to identify trends and patterns. However, the analysis is complex and the results are difficult to interpret.

We need to use right visualization tools to create charts, graphs, and maps to present the data insights in a clear and concise manner.

Employ data storytelling techniques to communicate the insights in a persuasive and engaging way.

Subject the analysis and results to peer review to validate its soundness and identify potential biases or limitations.

One thing which I found helpful is Using data visualization tools is to present a storytelling technique using Tableau or Power BI.
…see more
Like
4
Data security
Data security is the protection of the data from unauthorized access, use, modification, or disclosure, especially when dealing with sensitive or confidential data. Data security testing involves assessing the data security policies and practices, and identifying and mitigating any risks or vulnerabilities. Data security testing can be done using various techniques, such as encryption, authentication, authorization, auditing, or monitoring.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data Solutions, Business Transformation
Copy link to contribution
Report contribution
Everyone in the project is responsible for protecting sensitive customer data in a big data environment. However, there are concerns about unauthorized access, modification, or disclosure of data.
1. Implement access control mechanisms to restrict access to data based on user roles and privileges.
2. Encrypt data at rest and in transit to protect against unauthorized access.
3. Implement data loss prevention (DLP) solutions to prevent accidental or intentional data leaks.
4. Conduct regular security audits to identify and remediate potential vulnerabilities.

I have used data security tools like Apache Ranger, Cloudera Sentry, and DataStax Pulsar to enforce data access control and protect data privacy.
…see more
Like
1
5
Data governance
Data governance is the management of the data lifecycle, from acquisition to disposal, including the roles, responsibilities, standards, and processes involved. Data governance testing involves ensuring that the data is handled in a compliant and ethical manner, and that the data quality and security are maintained throughout the project. Data governance testing can be done using various mechanisms, such as documentation, metadata, lineage, or traceability.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data Solutions, Business Transformation
Copy link to contribution
Report contribution
When managing the data governance program for any project, there are challenges in ensuring data consistency, compliance, and ethical use.
Establishing data governance policies and procedures to define the roles and responsibilities for data management.
Implement data quality standards to ensure data consistency and accuracy.
Integrate data governance tools into the data management workflow to automate data quality checks and compliance monitoring.
…see more
Like
6
Data communication
Data communication is the presentation and dissemination of the data analysis results to the intended audience, such as stakeholders, customers, or decision-makers. Data communication testing involves ensuring that the results are clear, concise, and compelling, and that they address the objectives and expectations of the project. Data communication testing can be done using various methods, such as feedback, reviews, or surveys.
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Piyush Tamaskar
Data Architect - Data Analytics & AI @ Nagarro | Power BI, Microsoft Fabric | Strategic Vision, Cloud-Based Data Solutions, Business Transformation
Copy link to contribution
Report contribution
1. Tailor the communication to the audience: Understand the audience's level of technical knowledge, interests, and decision-making needs. Craft the message accordingly, using appropriate language, visuals, and storytelling techniques.
2. Emphasize the business value: Clearly articulate how the data insights can translate into tangible business benefits, such as increased revenue, reduced costs, or improved customer satisfaction.
3. Utilize effective data visualization: Create engaging and informative charts, graphs, and dashboards to present complex data in a clear and understandable manner.
…see more
Like
7
Here’s what else to consider
This is a space to share examples, stories, or insights that don’t fit into any of the previous sections. What else would you like to add?
Add your perspective
Help others by sharing more (125 characters min.)
Cancel
Add
Save
Sabikah Mukhi
Data Analytics | Data Visualisation
Copy link to contribution
Report contribution
These points are amazing but I would just like to add a fee ways to test the analysis results once these efforts are done.
Testing is an ongoing process for atleast some time for the purpose of continuous improvement.
1. Hypothesis testing is a very effective and majorly used method. 
2. Automated and hybrid testing.
3. Simulation testing
…see more
Like
Data Analytics
Data Analytics
+ Follow
Rate this article
We created this article with the help of AI. What do you think of it?
It’s great
It’s not so great
Thanks for your feedback
Your feedback is private. Like or react to bring the conversation to your network.
Tell us more
Cancel
Submit
Done
Report this article
More articles on Data Analytics
No more previous content
You're struggling to align primary and secondary data sources. How can you ensure accurate analysis?
9 contributions
You're juggling ongoing data analytics projects. How do you find time to learn new tools?
16 contributions
Your team is clashing over data privacy in your analytics project. How will you resolve it?
3 contributions
Swamped with data analytics tasks and deadlines. Which new tools should you focus on learning?
1 contribution
Your analytics process is crawling due to security protocols. How should you handle the slowdown?
10 contributions
You're faced with unstructured data from various sources. How do you tackle the analysis challenge?
14 contributions
Your team struggles with securing data while keeping it accessible. How do you teach them to balance both?
3 contributions
You're caught in the middle of conflicting data analysis methods. How can you bring harmony to the debate?
9 contributions
You're about to start a large-scale data migration. How do you ensure data accuracy?
8 contributions
Dealing with inconsistent datasets in data analysis. How do you ensure accuracy and integrity?
7 contributions
Balancing conflicting stakeholder priorities in predictive modeling. Are you making fair decisions?
3 contributions
No more next content
See all
Explore Other Skills
Programming
Web Development
Agile Methodologies
Machine Learning
Software Development
Computer Science
Data Engineering
Data Science
Artificial Intelligence (AI)
Cloud Computing
Show more
Show less
More relevant reading
Data Engineering
What are the most effective data quality and integrity checks for predictive analytics?
Data Science
What are the most common data quality standards for data science?
Analytic Problem Solving
What are the tools and techniques that you use to monitor and audit data quality in your analytic workflows?
Data Science
What is the best way to validate and cleanse data during integration?
Are you sure you want to delete your contribution?
Are you sure you want to delete your reply?
Cancel
Delete
LinkedIn
© 2024
About
Accessibility
User Agreement
Privacy Policy
Your California Privacy Choices
Cookie Policy
Copyright Policy
Brand Policy
Guest Controls
Community Guidelines
العربية (Arabic)
বাংলা (Bangla)
Čeština (Czech)
Dansk (Danish)
Deutsch (German)
Ελληνικά (Greek)
English (English)
Español (Spanish)
فارسی (Persian)
Suomi (Finnish)
Français (French)
हिंदी (Hindi)
Magyar (Hungarian)
Bahasa Indonesia (Indonesian)
Italiano (Italian)
עברית (Hebrew)
日本語 (Japanese)
한국어 (Korean)
मराठी (Marathi)
Bahasa Malaysia (Malay)
Nederlands (Dutch)
Norsk (Norwegian)
ਪੰਜਾਬੀ (Punjabi)
Polski (Polish)
Português (Portuguese)
Română (Romanian)
Русский (Russian)
Svenska (Swedish)
తెలుగు (Telugu)
ภาษาไทย (Thai)
Tagalog (Tagalog)
Türkçe (Turkish)
Українська (Ukrainian)
Tiếng Việt (Vietnamese)
简体中文 (Chinese (Simplified))
正體中文 (Chinese (Traditional))
Language
Like
Copy
LinkedIn
Facebook
Twitter
Share
9 Contributions"
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
No Title,erro
design - Designing a big data web app - Software Engineering Stack Exchange,"design - Designing a big data web app - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Designing a big data web app
Ask Question
Asked
5 years, 9 months ago
Modified
5 years, 9 months ago
Viewed
1k times
2
How do you design a website that allows users to query a large amount of user data, more specifically:
there are ~100 million users with ~100TB of data, data is stored in
HDFS
(not a database)
number of (concurrent) queries is not important, but each query should be as fast as possible
support some simple queries such as: get user info by id, get
accumulated data like monthly logins and monthly online time
query result is little (1 number, or a few hundred rows) so frontend performance doesn't matter
I'm more interested in the thought process on how to approach this requirement. For example:
at 100 users, what is the design?
at 1,000,000 users, what needs to be changed?
at 100,000,000 users, what is the design now?
I've searched around and see a lot of people talking about caching, load balancing,... Of course, those techniques are useful and can be used but how do you know it can help handling N users? Nobody seems to explain this point.
design
distributed-system
big-data
Share
Improve this question
Follow
edited
Feb 19, 2019 at 4:15
Minh Thai
asked
Feb 18, 2019 at 3:45
Minh Thai
Minh Thai
141
5
5 bronze badges
6
You haven’t told us much about the results of the queries, which is what matters in the context of web sites. For example, if you run a search over the 100 million users and return a single value in 100ms, then all of the engineering effort is going to be on the back end and doesn’t really have anything to do with web sites. However if you need to display a table with a million rows, that’s a different problem. So you need to be more specific.
–
RibaldEddie
Commented
Feb 19, 2019 at 2:26
You're are right, that is an important part. I've updated the question.
–
Minh Thai
Commented
Feb 19, 2019 at 4:15
1
Thanks. Now I want to ask you a question— forget the web site part. How would you make this work if the queries were submitted at the command line?
–
RibaldEddie
Commented
Feb 19, 2019 at 4:17
I would setup something that allows SQL queries and can connect with HDFS, for example
Spark
. I will add more servers until it cannot go faster. Then to save time on the same query later, I cache the result.
–
Minh Thai
Commented
Feb 19, 2019 at 5:35
1
great. Now you know what you need to build: an interface for defining queries that can be submitted to Spark.
–
RibaldEddie
Commented
Feb 20, 2019 at 1:26
|
Show
1
more comment
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
1
It's fairly basic math.
The bottleneck is unlikely your database, but bandwidth.
Take your max bandwidth, divide by expected # of users, and subtract 15% for overhead.
If you really have unlimited bandwidth, then do the same calculation using your database throughput.
Share
Improve this answer
Follow
answered
Feb 19, 2019 at 1:50
Nelson
Nelson
861
6
6 silver badges
11
11 bronze badges
Add a comment
|
1
At this time in cloud tech, I would employ what others have already designed to handle data loads.  Though you have a bit of data, I would put these data and future records into something akin to
Google's BigQuery
:
Easy to query via SQL,
Pay by query,
Handles many, many pebibytes,
Easily embedded into web/mobile app,
Maintains
cache already
,
By design, there is some non-cached query inertia time, but I would run away fast from trying to design, scale, script, pay-for, and maintain all the above.
Share
Improve this answer
Follow
answered
Feb 19, 2019 at 14:39
Jé Queue
Jé Queue
3,897
2
2 gold badges
32
32 silver badges
37
37 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
design
distributed-system
big-data
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
I'm creating my own scalable, rapid prototyping web server. How should I design it?
4
How does Google store search trends in backend?
1
Monitoring App: Client side or Server Side?
4
implementing dynamic query handler on historical data
0
Help me design this use case
1
Caching Layer in libraries: what's the most correct design?
3
Aggregation and storage system design for user event processing?
6
How does a CDN get ""data"" when there's hundreds of terabytes stored in data centers?
5
How to architecture the big files download?
1
Back of the envelope estimation question
Hot Network Questions
What does “going off” mean in ""Going off the age of the statues""?
What's a modern term for sucker or sap?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
Humans try to help aliens deactivate their defensive barrier
Should I expect a call from my future boss after signing the offer?
When looking at the first DCM page, where is the next DCM page documented?
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
Is there greater explanatory power in laws governing things rather than being descriptive?
Physical interpretation of selection rules for different multipole orders
How to estimate the latency of communication?
Building a Statistically Sound ML Model
US phone service for long-term travel
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
Hole, YHWH and counterfactual present
Why does it take so long to stop the rotor of a helicopter after landing?
Does DOS require partitions to be aligned at a cylinder boundary?
Time travelling paedo priest novel
What's the difference between '\ ' and tilde character (~)?
How to use a symbol as both function and head?
What 1970s microcomputers supported ≥ 512 pixels/line NTSC output?
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
How to remove clear adhesive tape from wooden kitchen cupboards?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
node.js - Is this Big Data architecture good enough to handle many requests per second? - Software Engineering Stack Exchange,"node.js - Is this Big Data architecture good enough to handle many requests per second? - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Is this Big Data architecture good enough to handle many requests per second?
Ask Question
Asked
7 years, 10 months ago
Modified
7 years, 10 months ago
Viewed
947 times
1
I want to ask for a review of my big data app plan. I haven’t much experience in that field, so every single piece of advice would be appreciated.
Here is a link to a diagram of the architecture:
My architecture overview
Our app is webanalytics app. I know that already there are many of them however, we use it to analyse our traffic data and we have already built-in some useful features. Our app (like each traffic analyser app) should have ability to grow and process large volumes of data. I designed a system having that principle in mind. Not everything in that plan is my original idea. I have drawn inspiration from lambda architecture (from Bigdata book by Nathan Marz, twitter architect). So after a quick introduction, let's have a look at my idea.
On the bottom, we have 'client'. This is a person which uses app to analyse their traffic. In front of the client, I put Nginx proxy to load balance traffic. I heard that node is not that good in serving static files, and Nginx is better (Apache would be equivalent, for example, Eran Hammer posted on Github article where he wrote that Wallmart uses Apache to handle https traffic, and then it proxies it to node js). Then we have node js app (running with Upstart), which is the heart of system, it has to analyse data and saves configuration to relational database (I think, that relational database is better, because in our system which currently uses MongoDB we have some ridiculous nesting, and some documents can have in one collection from 200 bytes up to unlimited space, and if we consider padding factor which mongo would create for that bigger, and then when someone edit small document it has side effect - mongo has to rewrite everything in that document and store it at the end of collection. We can set manual padding factor, however, it still feels little dirt and isn't best practice, (and probably wouldn't prevent rewriting). I have chosen Oracle, but my boss suggested using PostgreSQL because Oracle might be little costly. I thought, maybe we can go one step further and just use MySQL. That database shouldn't have any kind of big load, so let's keep it simple.
Now let's move to other side and analyze how data goes to the system. Every report goes to the reporting endpoint. It is obvious that this is some kind of ajax request, so the path to endpoint would be there in JavaScript files on our server. I thought, that we can manipulate that path in case of big overload, so we have reporting 'endpoints' not 'endpoint' (maybe some subdomains). Every endpoint should be able to send reports in two places. To Apache Hadoop and Cassandra. Apache Hadoop is awesomely scalable and has ability to store petabytes of data without any problem. Here we can store immutable data. If anything goes wrong, we can then restore data or process it again and again if we wish to. It is batch layer so it has to have big latency. To store and partition data we can use Pail, for processing Cascalog (two Java libraries) or just raw MapReduce. Cassandra should recompense latency. When batch processing is finished, equivalent data from Cassandra should be erased. The batch layer should generate 'reports ('images' of queries for hours, days, moths etc.) for client', and then store it in ElephantDB, which is also well scalable platform developed by Nathan Marz among others. Having images is a big advantage over querying raw data because we can do it much faster. I want to compensate big latency of data in ElephantDB by querying Cassandra only for the most recent period, which wasn't already processed from Hadoop to ElephantDB. The batch layer will process data without end in 'while (true)' loop, generating new images or updating older.
Now if you read everything, I'm very impressed with your patience ;). If you have any suggestions or you see that something is wrong, missing or want clarification, let me know, and I will respond to every feedback messages that you post.
My architecture overview
PS. Here is an example of write and read (query) request processing. We have (on top of my plan) requests with info that page has been visited. Endpoint sends (with unique id, so when network is divided we can then, while processing remove duplicates) request to Apache Hadoop, and Cassandra. Hadoop processes all incoming requests and generates for example (format of data can be diffrent, but this is generally idea):
//by day
 _______ ________________ ______________ 
|  url  |      views     |     date     |
|  '/'  |      3455      |  13-02-2016  |

//by hour
 _______ ________________ __________________
|  url  |     views      |       hour       |
|  '/'  |      355       | 13-02-2016 08:00 |
Then, when we want to get views for specyfic url, and specyfic period we don't have to query whole our master dataset, just few documents with our pre-processed data images. When someone wants to get more recent data, we send also request to Cassandra, where we want to have that kind of images too, and they should be updated with every query request (but this depends on how much data we expect to have in specyfic table/collection, sometimes it can be better to just update on every write request or don't update anything at all and don't generate images). We have also in that way 'caching' of request in 'images', and if someone changes period, we have already processed data to query on, and just have to add some little amount of most recent data.
Is connection between each part of system ok and makes sense or am I missing something important and for example I shouldn't use relational database or proxy for any reason, as I said I have only theoretical experience, and I know that someone more experienced may have some useful advices. And if everything is ok, someone who wants to build that kind of architecture can use it as example for future
node.js
big-data
hadoop
cassandra
nginx
Share
Improve this question
Follow
edited
Feb 14, 2017 at 13:45
enderland
12.1k
4
4 gold badges
52
52 silver badges
64
64 bronze badges
asked
Feb 13, 2017 at 16:29
Alan Mroczek
Alan Mroczek
137
4
4 bronze badges
0
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
5
The architecture is good enough to handle many requests per second, as long as you
test it
and
profile it
and it proves to handle the load that it is required to handle.
Let me quote Donald Knuth, Computer Programming as an Art, 1974:
The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming.
What does it even mean to handle ""many requests per second""? Ten request? Ten thousand? Ten million? This is all very relative, any recommendation will be very subjective, and even if you spend ten years analyzing it before implementation, you will most likely get very surprised the first time it actually starts handling the real load.
Share
Improve this answer
Follow
answered
Feb 13, 2017 at 17:23
rsp
rsp
7,868
1
1 gold badge
18
18 silver badges
10
10 bronze badges
1
1
Thanks for reply. My goal is to handle as many requests as I possibly can. I know that this is very unprecise but the best scenario is that my system can handle 100 request per second for example, and if I want to handle more, I just attach new servers and it goes on, without stopping. This is why I think Hadoop is so great. If I attach new machines I have more and more space and processing power. And I would like to know if all what I wrote can be easily scalable, or I am missing something about this tools and should change to different software.
–
Alan Mroczek
Commented
Feb 13, 2017 at 17:48
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
node.js
big-data
hadoop
cassandra
nginx
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
4
Asynchronous Java
0
QFS (Quantcast File System): Scalable for medium ""big data"" problems or only for extremely large
6
Design of high performance file processing web application
1
Is hadoop designed only for ""simple"" data processing jobs, where communications between the distributed nodes are sparse?
2
Handling thousands of cassandra insert/updates
4
Best practices for dashboard of near real-time analytics
1
#Apache-flink: Stream processing or Batch processing using Flink
3
Why is the whole Hadoop ecosystem written in Java?
1
What's the use of a wide-column NoSQL db such as Cassandra over a K-V store or a document based db?
Hot Network Questions
Only selecting Features that have another layers feature on top
Is there greater explanatory power in laws governing things rather than being descriptive?
Didactic tool to play with deterministic and nondeterministic finite automata
US phone service for long-term travel
Why does it take so long to stop the rotor of a helicopter after landing?
Two types difinition of the distance function
What is meaning of forms in ""they are even used as coil forms for inductors?""
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
When to use cards for communicating dietary restrictions in Japan
How much of a structural/syntactic difference is there between an oath and a promise?
How should I connect a light fixture with UK wire colors to US wiring?
how do I correctly check that some aggregated results are correct?
When to start playing the chord when a measure starts with a rest symbol?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
How to remove clear adhesive tape from wooden kitchen cupboards?
Covering a smoke alarm horn
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
What is ""B & S"" a reference to in Khartoum?
Can a hyphen be a ""letter"" in some words?
Topology of a horocycle
What's a modern term for sucker or sap?
Why are Jersey and Guernsey not considered sovereign states?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Physical interpretation of selection rules for different multipole orders
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
c# - Reading and saving big data to db - Software Engineering Stack Exchange,"c# - Reading and saving big data to db - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Reading and saving big data to db
Ask Question
Asked
7 years, 10 months ago
Modified
7 years, 10 months ago
Viewed
1k times
3
I have a method for reading data from file. The problem is how to handle files that are too big for a simple read and save to database? I was thinking about reading a chunk of it and saving it to database, but I don't know if having an asynchronous method with callback is a good idea.
Basically I think that a reader class shouldn't be aware of any database interface, so in order to notify of successfully reading a chunk of data it has to have a callback. I don't know if this is a good approach or not.
private const int Buffer = 100000;

    public Task ReadAsync(Action<Tuple<DataTable, int>> statusCallback) {
        DataTable data;
        return await Task.Run(() => {
            var totalRows = GetRowsCount(); // iterates file to calculate total number of rows
            var progress = 0;
            if(totalRows < Buffer) {
                /**Read whole file...*/
              progress = 100;
            }
            else {
              while(/**loop until end of file*/) {
                   for(var rowIndex = 0; rowIndex < Buffer; rowIndex++) { 
                      var row = reader.Read();
                      /**Split, parse, etc...*/
                      data.Add(row);
                   }
                progress += Buffer/totalRows * 100; // Add read rows to total result %
                statusCallback(new Tuple(data, progress));
                }
            }
        }

    }
And then save it
public void Start() {
    _reader.ReadAsync(ReadingProgress);
}

private void ReadingProgress(DataTable data, int progress) {
     _loadingBar.Update(progress);
     using(var tran = _database.BeginTrans()) {
         foreach(var row in data.Rows) 
         {
             _database.Insert(row);
         }
         tran.Commit();
     }
}
For some reason it seems wrong to me, but I don't know why. Any ideas how I could improve this?
EDIT:
I would like to notify users of how much the program read of the file, so I need to iterate through the whole file once and read how many lines it has. This bothers me, because it means I have to iterate a file two times.
One approach I thought about was get the byte size of first line and then divide the size of the file by that size. It would give me estimated count of lines in a file, but I'm not really sure if the approximation error wouldn't be too big.
c#
object-oriented
database
Share
Improve this question
Follow
edited
Jan 24, 2017 at 8:42
FCin
asked
Jan 22, 2017 at 15:47
FCin
FCin
522
1
1 gold badge
7
7 silver badges
23
23 bronze badges
9
A producer consumer with a throttle an the producer if it gets too far ahead.
–
paparazzo
Commented
Jan 22, 2017 at 16:26
I miss the loop over all chunks of data in your code. In the current form, the code will only read the first chunk. When you add the missing loop, you have to make sure the calls to
_database.Insert
stay in the right order, even when multiple threads are involved. Or, you have to make sure the order of insert does not matter.
–
Doc Brown
Commented
Jan 22, 2017 at 20:21
... now things got even more confusing. Inserts by column? Still not showing where the code loops over the chunks? Voting to close as ""unclear"".
–
Doc Brown
Commented
Jan 23, 2017 at 21:21
... is this a try to fix the question? Inserting all rows of one chunk still does not show how multiple chunks are processed. If you do not understand my comments, ask instead of applying wild guesses.
–
Doc Brown
Commented
Jan 24, 2017 at 6:47
@DocBrown I fixed what I could before going to work. I've just added the rest. I don't need exact line by line fix. I just want to know if my thinking is good in terms of keeping everything separate and clean.
–
FCin
Commented
Jan 24, 2017 at 7:33
|
Show
4
more comments
0
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
Know someone who can answer? Share a link to this
question
via
email
,
Twitter
, or
Facebook
.
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Browse other questions tagged
c#
object-oriented
database
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
4
Sanity of design for my in-memory object representations of database rows
2
Effective implementation of ""array"" of type Int X String -> String in .NET or in general
7
I need advice developing a sensitive data transfer/storage/encryption system
5
NoSQL and BIG DATA
1
Big amount of text: database vs file
1
Deduplicating data during batch processing
4
Repository that performs API calls internally - DDD
11
Concurrent fault-safe data structure
Hot Network Questions
Physical interpretation of selection rules for different multipole orders
What are these 16-Century Italian monetary symbols?
What is the purpose of `enum class` with a specified underlying type, but no enumerators?
Can two wrongs ever make a right?
How can dragons heat their breath?
What is meaning of forms in ""they are even used as coil forms for inductors?""
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
Is it possible that the committee contacts only one reference while applicants need to provide two?
Should I expect a call from my future boss after signing the offer?
When was ""to list"" meaning ""to wish"" lost?
White perpetual check, where Black manages a check too?
Does DOS require partitions to be aligned at a cylinder boundary?
What should machining (turning, milling, grinding) in space look like
How to use a symbol as both function and head?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Only selecting Features that have another layers feature on top
Longest bitonic subarray
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Indian music video with over the top CGI
How *exactly* is divisibility defined?
Protecting myself against costs for overnight weather-related cancellations
What technique is used for the heads in this LEGO Halo Elite MOC?
When looking at the first DCM page, where is the next DCM page documented?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-cs
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
"java - SRP in the ""big data"" setting - Software Engineering Stack Exchange","java - SRP in the ""big data"" setting - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
SRP in the ""big data"" setting
Ask Question
Asked
8 years, 4 months ago
Modified
8 years, 4 months ago
Viewed
130 times
2
We have a codebase at work that:
Ingests (low) thousands of small files.  Each of these input files contains about 50k “micro-items”
These “micro-items” are then clustered together to find “macro-items”
The “macro-items” become the input to a variety of other important business computations and analyses.  These macro-items are the life-blood of our organization.
Does all of this work using Apache’s Crunch library (which is backed by Hadoop)
It is true that some of these steps above are hard to do without Crunch.  The “clustering problem"" in particular is probably impossible to do strictly in-memory on one machine.  There are just too many items that need to be considered in the clustering step.
You could also argue that we have so much data that any solution that isn't built for scale isn’t worth having.
That said, I feel like our code-base break SRP left and right.  For example, the algorithms that parse raw input files cannot be easily separated from the Crunch “do it enmasse” classes. Meaning if I have just one input file I cannot parse it without running a full-scale Crunch job.    Additionally, I cannot easily access or even test the “clustering algorithm” and the “important business computations”.
Is this a common problem in the big data space?  Does SRP fly out the window when I have tons of data?
Is trying to separate this code base into two project A and B a reasonable goal?   I am assuming Project (A) would define and properly test the parsing algorithms, the clustering logic, and the important business computation while project (B) would depend on project (A) and use Crunch to do all of these things at scale.
Part of the reason I want to advocate a strict separation is to vastly improve the testing on all the non-distributed computations.  It is horrible when a distributed Crunch job fails and its hard to pin down why.
java
single-responsibility
distributed-computing
hadoop
Share
Improve this question
Follow
asked
Aug 5, 2016 at 14:14
Ivan
Ivan
575
4
4 silver badges
9
9 bronze badges
4
The purpose of SRP is to improve maintainability.  Do you feel that your current solution has a maintainability problem?  Also, given the context you've described, ""Clustering"" is probably a single responsibility.  Some programmers view ""SRP"" as ""Do One Thing,"" but that's not exactly what it means.
–
Robert Harvey
Commented
Aug 5, 2016 at 19:05
I absolutely think it is a maintainability issue.  In particular we have a difficult time verifying to others that our computations are done properly
–
Ivan
Commented
Aug 5, 2016 at 19:35
That should be your focus.  SRP is just a guideline; your real effort should be focused on answering the question:
how can we make this more maintainable?
–
Robert Harvey
Commented
Aug 5, 2016 at 20:55
Doesn't sound like big data.
–
Den
Commented
Aug 8, 2016 at 9:14
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
1
SRP happens at the Module (or library), Package (or namespace), Class, and Function level.
What this means is:
A Library has one reason to change. For instance, you build a network library. Valid reasons for it to change include: supporting a new protocol, fixing a bug in an existing protocol. Invalid reasons to change: client or server endpoints change.
A namespace has one reason to change: Taking the network library a step further. The MyCompany.Networking.Http namespace should only change with regard to changes in the implementation of HTTP. It should not be concerned with FTP, or SMTP, you would only change the package to support changes to HTTP, bugs in your implementation or new versions of the protocol.
A class has only one reason to change: MyCompany.Networking.Http.HttpClient should...well you get the picture.
If Crunch is just a utility that your system uses to scale the operations, then yes I would argue that the operations themselves should be implemented separately from crunch.
The division you mentioned makes perfect logical sense. The operations themselves would be bound by SRP to only change to support changes in your business logic. The coordinator would do the orchestration between your operations and Crunch. (If you feel the need, you can protect the coordinator from changes in whether you use crunch for the large scale processing but a rule of thumb for SRP is that you protect against a class of changes as they become applicable. I.e if the chances of you switching task engines are slim to none, it doesn't make much since to provide abstraction from it.
Share
Improve this answer
Follow
answered
Aug 8, 2016 at 2:32
Michael Brown
Michael Brown
21.8k
3
3 gold badges
47
47 silver badges
83
83 bronze badges
Add a comment
|
0
I'm not a Java programmer, but the problem you described is more general.
Right now you have a solution that is not 100% reliable. From what I understood you need to
maintain
it. There is a
technical debt
that you would want to pay off in order to reduce the pain you experiencing during ""recovery sessions"".
Try to extract
single pieces
from the codebase. Usually after playing with the code you understand it better and feel more confident what to refactor. Separating every single line of code is a win for you.
Later on you would see patterns or clusters of similar functions. Merge them in to one class and provide it as DI.
Share
Improve this answer
Follow
edited
Aug 8, 2016 at 7:10
Tulains Córdova
39.5k
13
13 gold badges
99
99 silver badges
155
155 bronze badges
answered
Aug 8, 2016 at 6:19
klm_
klm_
234
1
1 silver badge
9
9 bronze badges
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
java
single-responsibility
distributed-computing
hadoop
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
2
Promoting scalability in small business applications
1
How should subsytem in different locations interact and communicate?
2
How to organize all classes derived from SRP?
2
Collect and prcess data from multiple threads in Java
5
Distributing Java code into packages using a clustering approach
3
How to create simulator for distributed algorithms written in simple language
2
Separating data and behavior in Java
8
Data handling in SRP (single responsibility principle)
Hot Network Questions
What does it mean when folks say that universe is not ""Locally real""?
When to use cards for communicating dietary restrictions in Japan
If someone falsely claims to have a Ph.D. on the jacket of a book and they profit from that claim, is that criminal fraud?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
Why does it take so long to stop the rotor of a helicopter after landing?
What does “going off” mean in ""Going off the age of the statues""?
Useful aerial recon vehicles for newly colonized worlds
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
reverse engineering wire protocol
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
What's the difference between '\ ' and tilde character (~)?
Protecting myself against costs for overnight weather-related cancellations
A Pandigital Multiplication
How to keep meat in a dungeon fresh, preserved, and hot?
How does this Paypal guest checkout scam work?
Bash script that waits until GPU is free
How did Jahnke and Emde create their plots
What is meaning of forms in ""they are even used as coil forms for inductors?""
Time travelling paedo priest novel
Indian music video with over the top CGI
How can dragons heat their breath?
Passphrase entropy calculation, Wikipedia version
Need an advice to rig a spaceship with mechanicals part
Is there anyway to make Mathematica output only the solution to an integral that is real? Eg. Integrate[D[ArcSin[2 x/3], x], x]
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
lang-java
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
performance - Dealing with big data - Software Engineering Stack Exchange,"performance - Dealing with big data - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Dealing with big data [closed]
Ask Question
Asked
9 years ago
Modified
9 years ago
Viewed
107 times
1
Closed
. This question needs
details or clarity
. It is not currently accepting answers.
Want to improve this question?
Add details and clarify the problem by
editing this post
.
Closed
9 years ago
.
Improve this question
I am on a project dealing with a lot of data in the form of images and videos (Data related to wind engineering). My requirement is to build a predictive algorithm based on the data I have. I have found many tools with which I can analyse the data where each tool has its own advantages and disadvantages. Big data being really new to me, I find it very difficult to choose a platform to start with. There should be other people here who should have dealt with similar situations.
What criteria should I mainly take into account before selecting a
tool for analysing big data?
I could update my question with the tools that I have found so far if that would be relevant for the question and my requirement.
UPDATE
Some of the Criteria that I have taken into account : Visualization, Interaction, Security, Data Access and integration, Speed of response, Integrated Data Mining, Pattern Matching, Ease of use etc. As you can see the list that I have made for the criteria comes from the extensive reading of different articles on the topic. But I can't narrow down the list nor find the individual contribution of these criteria in the various tools available for analysis.
Let me also list some of the tools that I found after googling : Knime, Statistica 2, Rapidminer, Orange, WEKA, KEEL, R and RATTLE.
On what basis could I choose a tool from a list of tools that perform similar tasks?
performance
data
tools
big-data
Share
Improve this question
Follow
edited
Nov 25, 2015 at 7:56
gnat
20.9k
29
29 gold badges
115
115 silver badges
294
294 bronze badges
asked
Nov 25, 2015 at 7:10
Vini
Vini
216
1
1 gold badge
2
2 silver badges
12
12 bronze badges
9
It would be really nice to know what was missing in my question to be downvoted? I would really like to add any relevant data that would aid in finding me an answer. :)
–
Vini
Commented
Nov 25, 2015 at 8:02
1
I think this question is at its heart a request for a software recommendation.  You seem aware that such a question would be off topic, and have constructed the question accordingly, but I have a hard time seeing how it can be answered usefully as constructed.  Yes, you are right about the criteria you should be considering, but the next question is how well the various tools meet these criteria.  Expert advice about the specific tools seems most useful at this stage.  Have you considered asking at
softwarerecs.stackexchange.com
?
–
user82096
Commented
Nov 25, 2015 at 8:22
Can the question be migrated to softwarerecs?
–
Vini
Commented
Nov 25, 2015 at 8:24
2
Those criteria you listed are totally irrelevant for your question. The only criteria which matter are ""what exactly are your requirements"", ""how good do the existing tools match those requirements"", ""what will they cost (including additional effort to adapt the specific tool to your case"", and ""can you develop a solution for a given problem in the required quality without a specific tool for less"".
–
Doc Brown
Commented
Nov 25, 2015 at 12:14
3
@ViniVasundharan You are doing things backwards.  Your first step is to clearly identify all of your requirements for predictive analysis.  From there you can gather any questions about how to fulfill certain requirements you are confused about.  Only then should you find a tool that can solve your problem.  Requirements present problems.  Tools provide solutions to those problems.  Nobody can help you pick a tool if you cannot define what your problems are.
–
maple_shaft
♦
Commented
Nov 25, 2015 at 12:23
|
Show
4
more comments
0
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
Browse other questions tagged
performance
data
tools
big-data
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
24
What is the definition of ""Big Data""?
11
Why Big Data Needs To Be Functional?
3
Big Data: Can it be pre-processed?
2
increasing website pipeline performance by replacing for loops with vector statements perturbs output
1
Is this Big Data architecture good enough to handle many requests per second?
0
How to choose a data model?
3
How to synchronize changes between Data Engineers and Designers?
Hot Network Questions
Why did Crimea’s parliament agree to join Ukraine?
Why is the speed graph of a survey flight a square wave?
How much of a structural/syntactic difference is there between an oath and a promise?
Does DOS require partitions to be aligned at a cylinder boundary?
Can two wrongs ever make a right?
Topology of a horocycle
How to keep meat in a dungeon fresh, preserved, and hot?
Only selecting Features that have another layers feature on top
When was ""to list"" meaning ""to wish"" lost?
How to remove clear adhesive tape from wooden kitchen cupboards?
Time travelling paedo priest novel
How do mathematical realists explain the applicability and effectiveness of mathematics in physics?
Can a hyphen be a ""letter"" in some words?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Indian music video with over the top CGI
How to delete edges of curve based on their length
how do I correctly check that some aggregated results are correct?
Didactic tool to play with deterministic and nondeterministic finite automata
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
What's the safest way to improve upon an existing network cable running next to AC power in underground PVC conduit?
How should I connect a light fixture with UK wire colors to US wiring?
What are these 16-Century Italian monetary symbols?
What technique is used for the heads in this LEGO Halo Elite MOC?
How to balance authorship roles when my contributions are substantial but I am evaluated on last authorship?
more hot questions
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
domain specific languages - Can Fluent DSL's exist in big data environments? - Software Engineering Stack Exchange,"domain specific languages - Can Fluent DSL's exist in big data environments? - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Can Fluent DSL's exist in big data environments?
Ask Question
Asked
9 years, 10 months ago
Modified
9 years, 10 months ago
Viewed
296 times
4
The way I understand Fluent Domain Specific Languages I am able to use method chaining to have a conversation with the code.  For example, if the business requirement is to call the database in order to ""Get all customers with InActive accounts"" I could use:
Customers().WithInActiveAccount()
Customers has millions of rows.  Pulling all customers into memory is not efficient when I only need a subset of customers (and maybe not even possible given memory constraints).  I suspect ORM's solve this problem by treating code as data, lazy-loading and building a complete query based off the entire expression.  So the final query may be
SELECT * CUSTOMERS WHERE InActive = true
IME, when dealing with highly normalized tables ORM's produce inefficient DB queries.  Rolling yet another custom ORM to solve such an issue feels like a death march waiting to happen.  And stored procedures written by a DB professional are going to be efficient.
In this simple case I can simply change customers to an object:
Customers.WithInactiveAccount()
What if I need to do something more complex?
Customers.WithInactiveAccount().BornAfter(October 1, 1990)
How do I efficiently build up queries as I build more advanced expressions that potentially draw in other entities?  This is a question I'm sure every ORM asks themselves right in the early stages of development.  Do I have to limit myself to ""dumb queries"" to maintain performance?  If this a technique that exists?
These are the types of questions I find myself getting from developers like me that have experienced across the board performance problems with ORM's in the big data world.
So when dealing with these types of normalized Databases is a fluent DSL a practical option?  (I'm assuming a fluent DSL for DB access requires an underlying ORM to function)
domain-specific-languages
Share
Improve this question
Follow
edited
Feb 11, 2015 at 16:33
P.Brian.Mackey
asked
Feb 11, 2015 at 15:00
P.Brian.Mackey
P.Brian.Mackey
11.1k
8
8 gold badges
52
52 silver badges
88
88 bronze badges
7
1
It's unclear what you are asking. You write the
Customers()
method. If you don't want it to go to the database and load all rows, then, well, don't write it to do that! There's really not more to it than that.
–
Jörg W Mittag
Commented
Feb 11, 2015 at 16:09
This is a contrived example.  Yes, you are correct in this simple example I can do just that.  But, the question of fluent DSL's for more complicated queries still stands.  Looking at ORM's performance in real time on a real DB I experience real performance problems from sub-optimal queries.
–
P.Brian.Mackey
Commented
Feb 11, 2015 at 16:16
3
I still don't see what this has to do with whether you are building your query using a fluent DSL or not. The performance depends on how you optimize the query, not whether you built it by chaining methods.
–
Jörg W Mittag
Commented
Feb 11, 2015 at 16:37
@JörgWMittag - ""I'm assuming a fluent DSL for DB access requires an underlying ORM to function.""  If an ORM can produce optimized queries in a big data normalized environment (which I explained I have not seen one that does) then I can build fluent interfaces that use said ORM.  Or, if I can build a fluent DSL that does not rely on an ORM that also allows me to construct complicated queries then that's a DIFFERENT option.  Anyone that has not worked in a big data normalized environment with DBA's will be unlikely to answer this question.
–
P.Brian.Mackey
Commented
Feb 11, 2015 at 16:45
If your ORM, is generating poorly performing queries because of the normalisation of your tables, have you considered linking it to a hand-constructed view rather than the raw tables?
–
Jules
Commented
Feb 11, 2015 at 19:57
|
Show
2
more comments
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
3
First
, let's clarify terms a little...
The term
DSL
is enormously wide. SQL, HTML, LOGO, Mathematica, are all DSLs. You are talking about referring\ querying your data model according to its actual structure in a strongly typed manner.
Fluent means method chaining so your source looks more like English and less like a programming language. like so:
Noun().Adjective().Verb().Adverb()
. This is not the only or even the best way to form queries.
Big-Data usually refers to data that
can not
be efficiently stored and queried using RDBMS. This means Big-data and ""normalized"" are
mostly
mutually exclusive.
Now
regarding your question
. First of all I'm answering based on my experiance of several years you using C#, F#, some C++, and some Java, NHibernate, MS-SQL, PostgreSQL, and some MongoDB, and some Hadoop, mostly on pretty big data-sets.
""Fluent"" is a bad idea. It's usually harder to write, and tends to be misleading to the reader. it's also a lot less ""discoverable"" you need to learn an entire vocabulary to use and understand a given ""fluent"" API.
Using an ORM (NHibernate, Hibernate, Entity Framework), is better than manipulating data by yourself. This is not always true, and you should always test, optimize, and understand what your ORM is doing and why. This involves a pretty significant learning curve, you need to understand your ORM, you need to understand how to create a correct mapping, and how to control the way queries are generated. On the other hand if you know what you are doing about ~98% of the time using an ORM is the fastest way to create the
best and most performant
solutions, with the
least effort
. ~2% of the time you end up going to the DBA, you write a stored procedure or some SQL, and you use it from within the ORM...
You should have a proper DAL layer, handling data manipulation. Using an ORM doesn't remove the need to build DAL.
Writing queries and manipulating data in your programming language, in a strongly typed way is a great idea. It's fast, verified by the compiler, and very convenient. C# has a special feature called LINQ that enables querying various data sources, these include: C#'s collections, XML, RDBMSes, ODATA sources, many other structured, non structured, real big-data (MongoDB, Cassandra(?), Hadoop), and ORMs such as NHibernate and Entity Framework. Hibernate\NHibernate also have a ""Fluent"" quarry language called Criteria, and a special non-strongly typed (strings) language called HQL. NHibernate's Linq provider also has some limitations. Usually the strongly typed options are preferable, but still it's very important to understand them thoroughly.
You seem to not ""believe"" in ORMs... I think this comes from not being familiar, and lacking experience of working with them. I assure you all of the questions you are asking have been considered, and addressed, by some of the best developers in the industry.
Share
Improve this answer
Follow
edited
Feb 11, 2015 at 21:29
user7146
answered
Feb 11, 2015 at 19:14
AK_
AK_
744
3
3 silver badges
10
10 bronze badges
5
I believe I clarified the specific type of DSL system I'm looking for in the question. The point of this DSL is not to generate excitement, rather to assist with business communication. At a large hospital system I worked on a DB with many TB's of data.  The system is SQL Based (RDBMS).  However, the data was too broad and spread out to create performant queries.  In this scenerio RDBMS and big data is not mutually excluse.  You can have an RDBMS that is not query-able.  I've worked with MS master SQL DBA's that had nothing but negative comments regarding ORM's and their performance penalties.
–
P.Brian.Mackey
Commented
Feb 11, 2015 at 19:27
@P.Brian.Mackey What isn't true? Have you personally used an ORM extensively? Have you optimized applications that used an ORM? Of course that if a seasoned DBA would write code to get something from a DB he will do a better job then a
generic
ORM. That's not the point. it's a cost- benefit question. An ORM allow's you to write more succinct, more business oriented, and very performant code, faster.
–
AK_
Commented
Feb 11, 2015 at 19:30
Performance is the point when dealing with the specific subset of Big Data for reasons you stated in this answer.
–
P.Brian.Mackey
Commented
Feb 11, 2015 at 19:32
Look, what your'e asking for is possible. Creating a layer that allows writing fluent quarries on a specific data source. But it's a pretty hard problem. you can go and look how to implement a LINQ provider using Expression Trees, or look at the NHibernate source to see how Criteria is implemented. The thing is it's a pretty bad idea, unless you are just going to wrap NHibernate or another decent ORM. You would be much better building a decent DAL maybe with a different module to handle ""business  quarries""
–
AK_
Commented
Feb 11, 2015 at 19:50
Thanks.  I appreciate your answer and your feedback.  I will give it some more thought and come back later.  I'm also researching another technique that may help answer this difficult question, CQRS:
martinfowler.com/bliki/CQRS.html
–
P.Brian.Mackey
Commented
Feb 11, 2015 at 20:11
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
domain-specific-languages
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Hot Network Questions
What's a modern term for sucker or sap?
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
When to start playing the chord when a measure starts with a rest symbol?
When was ""to list"" meaning ""to wish"" lost?
How to delete edges of curve based on their length
Passphrase entropy calculation, Wikipedia version
What's the difference between '\ ' and tilde character (~)?
Does DOS require partitions to be aligned at a cylinder boundary?
Should I expect a call from my future boss after signing the offer?
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
A cartoon about a man who uses a magic flute to save a town from an invasion of rats, and later uses that flute to kidnap the children
Realization of fundamental group endomorphism
How can we be sure that the effects of gravity travel at most at the speed of light
Indian music video with over the top CGI
Find all unique quintuplets in an array that sum to a given target
How could a city build a circular canal?
Didactic tool to play with deterministic and nondeterministic finite automata
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
how do I correctly check that some aggregated results are correct?
reverse engineering wire protocol
What's the justification for implicitly casting arrays to pointers (in the C language family)?
What is ""B & S"" a reference to in Khartoum?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
serialization - Efficiently save big data structures - Software Engineering Stack Exchange,"serialization - Efficiently save big data structures - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Efficiently save big data structures [closed]
Ask Question
Asked
9 years, 11 months ago
Modified
9 years, 11 months ago
Viewed
1k times
-1
Closed
. This question needs to be more
focused
. It is not currently accepting answers.
Want to improve this question?
Update the question so it focuses on one problem only by
editing this post
.
Closed
9 years ago
.
Improve this question
I need to save big data structure (Tree with specific nodes) into file efficiently. Data is always changing, so I need to rewrite whole data structure very often, so simple serialization is not good option.
Which algorithms can be used to efficient saving of big data structures?
I used simple serialization, but i want to try algorihms which save only delta of my structure and then union those changes. In which direction should I move?
data-structures
serialization
Share
Improve this question
Follow
edited
Jan 14, 2015 at 9:29
Артыков Акмалжон
asked
Jan 14, 2015 at 9:22
Артыков Акмалжон
Артыков Акмалжон
3
3
3 bronze badges
2
Sharing your research helps everyone
. Tell us what you've tried and why it didn’t meet your needs. This demonstrates that you’ve taken the time to try to help yourself, it saves us from reiterating obvious answers, and most of all it helps you get a more specific and relevant answer. Also see
How to Ask
–
gnat
Commented
Jan 14, 2015 at 9:26
BTW, trees often grow but do not change... So you should explain how data is changing
–
Basile Starynkevitch
Commented
Jan 14, 2015 at 12:45
Add a comment
|
2 Answers
2
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
1
I tend to agree with
Doc Brown's answer
: you'll better
use a database
. That might be
sqlite
or a real database server:
mongodb
or
postgresql
, etc.... You could perhaps use an indexed file like
GDBM
(instead of a database) in some limited circumstances. If the data is small (which apparently it is not) you might consider persisting all of it in some textual formats like
JSON
(which you could also use inside database tables).
You should also define if your data structure is a real tree (without shared sub-nodes) or if it is a graph. You may have to manage the set of already dumped items, and handle shared data. Shared pointers and smart pointers may matter a lot.
You could store, in every significant object in memory, some database identifier (or id number), and perhaps other meta-information related to saving the data (saving timestamp, change time, etc...)
BTW,
copying Garbage Collection
algorithms may be relevant (since persisting to some database is quite similar to a copying GC). Read also the
GC handbook
.
You might be interested by
persistence
and
application checkpointing
.
You should design the persistence machinery quite early if possible.
Share
Improve this answer
Follow
edited
Apr 12, 2017 at 7:31
Community
Bot
1
answered
Jan 14, 2015 at 12:28
Basile Starynkevitch
Basile Starynkevitch
32.7k
6
6 gold badges
87
87 silver badges
131
131 bronze badges
Add a comment
|
1
Short answer: consider to use a database.
Databases allow you easily to save only the changed part of a whole bunch of data very quickly, whilst you don't have to care for the unchanged parts.
And
here is a former SO post
dealing with the topic of storing tree structures in databases.
Share
Improve this answer
Follow
edited
May 23, 2017 at 12:40
Community
Bot
1
answered
Jan 14, 2015 at 11:56
Doc Brown
Doc Brown
214k
34
34 gold badges
394
394 silver badges
603
603 bronze badges
0
Add a comment
|
Not the answer you're looking for? Browse other questions tagged
data-structures
serialization
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
12
Are trees organised by a ""firstchild, nextsibling"" structure? If not, why not?
3
Algorithms for data structures in distributed system
8
How do we keep dependent data structures up to date?
5
How to store satellite data in C data structures
4
Data structures and algorithms for a directed rooted tree with inherited properties?
1
Efficient storing and multidirectional lookup of hierachical data
2
Where to put data for tree structure which every node requires?
12
Workaround for implementing operations on doubly linked or circular data structures in languages with immutable data
4
What is the most suitable data structure for IPv4 addresses with intersecting ranges?
Hot Network Questions
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
What's the justification for implicitly casting arrays to pointers (in the C language family)?
What should machining (turning, milling, grinding) in space look like
How *exactly* is divisibility defined?
What is ""B & S"" a reference to in Khartoum?
US phone service for long-term travel
How manage inventory discrepancies due to measurement errors in warehouse management systems
What's a modern term for sucker or sap?
The coherence of physicalism: are there any solutions to Hempel's dilemma?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Two types difinition of the distance function
Indian music video with over the top CGI
Realization of fundamental group endomorphism
On a sheet of choir music, how do you interpret two notes represented by two heads on a single stem?
Longest bitonic subarray
How to delete edges of curve based on their length
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
Need an advice to rig a spaceship with mechanicals part
How did Jahnke and Emde create their plots
How to achieve infinite rage?
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
How can Rupert Murdoch be having a problem changing the beneficiaries of his trust?
Is it possible that the committee contacts only one reference while applicants need to provide two?
Bash script that waits until GPU is free
more hot questions
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
Big Data: Can it be pre-processed? - Software Engineering Stack Exchange,"Big Data: Can it be pre-processed? - Software Engineering Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Engineering
help
chat
Software Engineering Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Big Data: Can it be pre-processed?
Ask Question
Asked
10 years, 9 months ago
Modified
10 years, 9 months ago
Viewed
364 times
3
My question is about ""big data"". Basically, big data involves the analysis of a large amount of data to make meaningful insights from it.
I would like to know:
Whether or not large amounts of data can be pre-processed? (like say for example you are running some matching service for people, so you take all the information you have on the people and you process it at a certain point for use later on)
If pre-processing is possible, how would you normally go about doing this?
To help narrow the scope of my question, please look at this hypothetical scenario.
Say I have a customer database and my company is a global retailer
  that is using some type of points system to reward the shoppers (for
  arguments sake, the points are tallied up on a type of electronic card
  or mobile app).
So based on my rewards system, I am now able to fully aware of exactly
  what a shopper is purchasing and when they normally make purchases of
  recurring items.
My database is growing all the time with this information and I would
  now like to make recommendations (or send notifications) to shoppers
  about special offers of products they buy or related products that may
  interest them, when they enter 1 of the stores.
Instead of processing all the accumulated data when a shopper enters
  the store, I would like to continually process the data-stream as the
  data comes in (meaning from previous shopping experiences), so that
  when it comes time to make a recommendation (for the next time a
  shopper walks into the store), it is simply a matter of retrieving the
  recommendations and providing a list of it to the shopper.
With this method in mind, I can easily space out my CPU-intensive
  tasks, instead of say: processing all customer data on a busy day when
  foot-traffic is at peak volumes.
By asking how I would do this, I would be referring to common methods available for achieving this. This can include any special databases or programming techniques or even specialized software that can carry out these timed calculations that can ""pre-process"" the data at specific times, in order to balance out CPU-intensive tasks.
You can consider the customer-recommendation scenario as the ""situation"". It is the best example scenario I could think of that would explain why ""pre-processing"" (or calculating the recommendations at specific times) would make sense.
data
big-data
analytics
Share
Improve this question
Follow
edited
Feb 25, 2014 at 14:29
user53019
asked
Feb 24, 2014 at 15:46
Joe
Joe
608
3
3 silver badges
9
9 bronze badges
4
5
preprocessing is usually a necessity to make large amounts of data usable. The map-reduce strategy is basically built around a cascade of filters and combinators. However, “big data” is a really hazy term to which different people ascribe very different meaning. Could you
edit
your question to make it clearer what your problem is? As it stands, it's just far to broad, and will likely be put on hold as such.
–
amon
Commented
Feb 24, 2014 at 16:10
Also, by 'how', do you mean implementation specifics like what kind of database, database design, and how to start processing jobs? Or are you asking more of a design question, such as 'what factors will influence how we construct our preprocessing?'
–
Snagulus
Commented
Feb 24, 2014 at 16:13
2
pre-processing is a generic term which is almost as unspecific as ""processing"" - which means there is no ""normal"" way of doing it. Its like asking for ""how do I solve a problem"" without telling which problem. This is why you can expect your question to be closed soon as ""too broad"".
–
Doc Brown
Commented
Feb 24, 2014 at 16:55
Edited question and added more details.
–
Joe
Commented
Feb 25, 2014 at 13:31
Add a comment
|
3 Answers
3
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
4
Typically I've heard of this being handled via the
OLTP vs. OLAP
model.  Essentially the T in OLTP means ""transactional"", so this is the typical databased used for day-to-day operations.  Then you write some kind of translational logic that transforms the OLTP database into an OLAP database (the A stands for analytical).
Basically you're talking about the same data represented 2 different ways.  The OLTP database focuses on normalization but the OLAP database is structured in more of a ""star"" pattern with a lot more data repetition.  It's read-only and optimized for querying.
Then the engineering is in figuring out how to do the translation from OLTP to OLAP, how often to do it, and if you can do it incrementally so the OLAP database isn't too far behind ""real-time"".
Share
Improve this answer
Follow
answered
Feb 25, 2014 at 19:34
Scott Whitlock
Scott Whitlock
21.9k
5
5 gold badges
63
63 silver badges
89
89 bronze badges
Add a comment
|
2
In a past job, I was a DBA for a global solutions company where databases with millions and billions of rows were the norm.
As datasets got larger, it became more and more problematic to turn around complex queries in a timely manner.
Among many strategies we adopted, 4 spring to mind:
Result sets for common queries were stored in what we called ""strips"". These were basically index-organised tables that stored keys to stop repeated joins in subsequent queries
Denormalising tables yielded huge benefits to reduce the number of joins
Tables were partitioned in line with common queries e.g. postcode/zip code etc
Whilst all data was available in the repository, only fully formed data and cleansed data was allowed thru to the mart for querying
On top of this you can overlay pre-calculated segments. For example - rather than try to pull say, all blue collar workers in the country, you can use segmentation to drill down only in those areas which are predominantly blue collar.
EDIT (following Joe's update)
In that case you might want a reporting mart in addition to the mart and repository I described above which is lean and mean and optimised for fast queries and MI reports.
Share
Improve this answer
Follow
edited
Feb 26, 2014 at 12:13
answered
Feb 24, 2014 at 16:09
Robbie Dee
Robbie Dee
9,815
2
2 gold badges
25
25 silver badges
53
53 bronze badges
Add a comment
|
0
Sure that's what an
incremental map reduce
is for. Essentially you perform an operation on the collection which processes the existing documents and puts them in a new collection, and as you add new documents you merge those rows into the derived collection.
Share
Improve this answer
Follow
answered
Feb 24, 2014 at 16:22
Sign
Sign
2,703
21
21 silver badges
22
22 bronze badges
1
MapReduce whilst undoubtedly a useful tool is not a silver bullet. The techniques described in the canonical MapReduce paper pre-date Google by several decades. The PL/I implementation I had to code-walk as a junior programmer was dated 1981!
–
Robbie Dee
Commented
Feb 24, 2014 at 17:03
Add a comment
|
Your Answer
Reminder:
Answers generated by artificial intelligence tools are not allowed on Software Engineering Stack Exchange.
Learn more
Thanks for contributing an answer to Software Engineering Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
data
big-data
analytics
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
6
Design of high performance file processing web application
4
Best practices for dashboard of near real-time analytics
1
Is this Big Data architecture good enough to handle many requests per second?
3
Data to pre-populate a database kept in text files
0
How to minimize size of data given the need for a null pointer?
Hot Network Questions
When to start playing the chord when a measure starts with a rest symbol?
White perpetual check, where Black manages a check too?
Convert pipe delimited column data to HTML table format for email
Can a hyphen be a ""letter"" in some words?
Find all unique quintuplets in an array that sum to a given target
What would cause species only distantly related and with vast morphological differences to still be able to interbreed?
What's the difference between '\ ' and tilde character (~)?
How does this Paypal guest checkout scam work?
Humans try to help aliens deactivate their defensive barrier
Passphrase entropy calculation, Wikipedia version
Why is the speed graph of a survey flight a square wave?
Time travelling paedo priest novel
How manage inventory discrepancies due to measurement errors in warehouse management systems
In mobile iOS apps should the bottom tabs remain visible when navigating to nested screens?
Protecting myself against costs for overnight weather-related cancellations
Only selecting Features that have another layers feature on top
Can two wrongs ever make a right?
Building a Statistically Sound ML Model
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
Why does it take so long to stop the rotor of a helicopter after landing?
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
Find a fraction's parent in the Stern-Brocot tree
A website asks you to enter a Microsoft/Google/Facebook password. How do you know it is safe?
What does “going off” mean in ""Going off the age of the statues""?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Software Engineering
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
test management - Big data application testing - Software Quality Assurance & Testing Stack Exchange,"test management - Big data application testing - Software Quality Assurance & Testing Stack Exchange
Skip to main content
Stack Exchange Network
Stack Exchange network consists of 183 Q&A communities including
Stack Overflow
, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers.
Visit Stack Exchange
Loading…
Tour
Start here for a quick overview of the site
Help Center
Detailed answers to any questions you might have
Meta
Discuss the workings and policies of this site
About Us
Learn more about Stack Overflow the company, and our products
current community
Software Quality Assurance & Testing
help
chat
Software Quality Assurance & Testing Meta
your communities
Sign up
or
log in
to customize your list.
more stack exchange communities
company blog
Log in
Sign up
Software Quality Assurance & Testing
Home
Questions
Tags
Users
Jobs
Companies
Unanswered
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Try Teams for free
Explore Teams
Teams
Ask questions, find answers and collaborate at work with Stack Overflow for Teams.
Explore Teams
Teams
Q&A for work
Connect and share knowledge within a single location that is structured and easy to search.
Learn more about Teams
Big data application testing
Ask Question
Asked
5 years, 10 months ago
Modified
5 years ago
Viewed
56 times
0
I am working on a project which has big data components in it. What would be testing artifacts (types of testing) of it which consists of data batch jobs, data ingestion framework, HDFS, tableau visualisation, email scheduler?
Ideally we should have testing at all the layers :
1. Data ingestion
2. Data processing
3. HDFS
4. Data visualisation
But what types of testing should be dpne at all layers ?
test-management
data-quality
data-analysis
data-validation
data-warehouse-testing
Share
Improve this question
Follow
asked
Feb 11, 2019 at 5:47
brij
brij
185
5
5 bronze badges
Add a comment
|
1 Answer
1
Sorted by:
Reset to default
Highest score (default)
Date modified (newest first)
Date created (oldest first)
1
This will depend on a number of factors, including: time/budget, data quality policy of your organization, data quality of the source dataset, complexity of each layer, etc.
A general guideline is, as with any testing, to ask yourself ""what could go wrong here?"" at each step.
For example, you could do data integrity at 1 and 2. But if you had a low budget or you were quite sure that the data being ingested is pretty clean (according to your organization's data quality policy) and/or the data ingestion process is fairly simple, you could do data integrity in 2 only.
You might also want to test data consistency and data completeness. All these can be tested at all steps, it just depends on the constraints I mentioned in the first paragraph.
Share
Improve this answer
Follow
answered
Dec 8, 2019 at 16:22
Maira Bay
Maira Bay
194
3
3 bronze badges
Add a comment
|
Your Answer
Thanks for contributing an answer to Software Quality Assurance & Testing Stack Exchange!
Please be sure to
answer the question
. Provide details and share your research!
But
avoid
…
Asking for help, clarification, or responding to other answers.
Making statements based on opinion; back them up with references or personal experience.
To learn more, see our
tips on writing great answers
.
Draft saved
Draft discarded
Sign up or
log in
Sign up using Google
Sign up using Email and Password
Submit
Post as a guest
Name
Email
Required, but never shown
Post as a guest
Name
Email
Required, but never shown
Post Your Answer
Discard
By clicking “Post Your Answer”, you agree to our
terms of service
and acknowledge you have read our
privacy policy
.
Not the answer you're looking for? Browse other questions tagged
test-management
data-quality
data-analysis
data-validation
data-warehouse-testing
or
ask your own question
.
The Overflow Blog
“You don’t want to be that person”: What security teams need to understand...
Featured on Meta
The Winter 2024 Community Asks Sprint has been moved to to March 2025 (and...
Stack Overflow Jobs is expanding to more countries
Related
7
What are the tiers of testing that should be done on a large scale distributed system?
2
Testing database transformation mappings
12
Keeping automation code away from application source code
2
What is the best practice for testing against modules which collect statistic
2
How can I validate CSV exports from my application?
1
How to perform Testing Process-Automation/ manual -web application
1
Any advice on testing sprocs that feed into an API?
7
Quality assurance when people's lives are at stake
2
Testing migrated data vs source data
1
Testing components between software teams
Hot Network Questions
PSE Advent Calendar 2024 (Day 11): A Sparkling Sudoku
suspected stars and bars problem considered incorrect, in need for some further insight
Covering a smoke alarm horn
Did the Japanese military use the Kagoshima dialect to protect their communications during WW2?
How much of a structural/syntactic difference is there between an oath and a promise?
How to explain why I don't have a reference letter from my supervisor
When was ""to list"" meaning ""to wish"" lost?
A Pandigital Multiplication
Manhwa about a man who, right as he is about to die, goes back in time to the day before the zombie apocalypse
how do I correctly check that some aggregated results are correct?
What is the smallest and ""best"" 27 lines configuration? And what is its symmetry group?
Is decomposability of polynomials ∈ℤ[𝑋] over ℚ an undecidable problem?
What's a modern term for sucker or sap?
Can two wrongs ever make a right?
Math contents does not align when subscripts are used
What are these 16-Century Italian monetary symbols?
How to write a function in LATEX whose value is different in [0,1/3), [1/3,1/2) ...?
When to use cards for communicating dietary restrictions in Japan
How to set individual columns in the siunitx package to boldface? It it a bug?
How to set image from Sitecore media librarary as an element's background image in Sitecore XP?
Why is the speed graph of a survey flight a square wave?
Indian music video with over the top CGI
Useful aerial recon vehicles for newly colonized worlds
How to estimate the latency of communication?
more hot questions
Question feed
Subscribe to RSS
Question feed
To subscribe to this RSS feed, copy and paste this URL into your RSS reader.
Software Quality Assurance & Testing
Tour
Help
Chat
Contact
Feedback
Company
Stack Overflow
Teams
Advertising
Talent
About
Press
Legal
Privacy Policy
Terms of Service
Cookie Settings
Cookie Policy
Stack Exchange Network
Technology
Culture & recreation
Life & arts
Science
Professional
Business
API
Data
Blog
Facebook
Twitter
LinkedIn
Instagram
Site design / logo © 2024 Stack Exchange Inc;
user contributions licensed under
CC BY-SA
.
rev 2024.12.12.20262"
