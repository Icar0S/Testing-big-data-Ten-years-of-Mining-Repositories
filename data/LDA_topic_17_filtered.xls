Link,Tools_Found,Methods_Found,Post_Text,LDA_Topic
https://dev.to/dbvismarketing/change-data-capture-a-comprehensive-guide-2k5g,,,"change data capture: a comprehensive guide - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse dbvisualizer posted on jan 2, 2024 ‚Ä¢ originally published at dbvis.com change data capture: a comprehensive guide # datacapture the table (253 part series) 1 optimizing queries for performance: mysql edition 2 json vs. jsonb in postgresql: a complete comparison ... 249 more parts... 3 architecting for security: mysql edition 4 the data you‚Äòve left behind ‚Äì an attacker‚Äòs perspective 5 how to deal with a database with billions of records 6 writing reusable sql queries for your application with dbvisualizer scripts 7 how to design and build a database in postgres 8 top five query tuning techniques for microsoft sql server 9 using postgresql to manage business inventory data and visualize it 10 sql triggers: what they are and how to use them 11 stored procedures in sql: a complete tutorial 12 ssh tunneling: the good, the bad, and the ugly 13 how to create a table like another table in mysql 14 firing database actions: a guide to using sql triggers with dbvisualizer 15 a comprehensive guide to data structures in sql 16 database transactions 101: the essential guide 17 the complete guide to sql subqueries 18 how to secure data in a postgres database: a guide 19 working with null in databases. turn your frustration into delight 20 acid in database systems: all you need to know 21 database security 101: best practices to secure your data 22 best practices for encrypting data in your database 23 getting started with multiversion concurrency control (mvcc) in postgresql 24 how to use a foreign key referring to the source table in postgres 25 blob data type: everything you can do with it 26 a guide to acid in mysql 27 preventing illicit uploads in mysql ‚Äì secure_file_priv 28 the ultimate guide to the mysql port 29 alter table explained 30 deadlocks in databases: a guide 31 a guide to subqueries in mysql 32 10x query performance with a database index 33 character sets vs. collations in a mysql database infrastructure 34 best practices for sql coding and development 35 a guide to multithreading in sql 36 er model, er diagram, and relational schema: what's the difference? 37 the ultimate guide to sql generated columns 38 mysql user management: a guide 39 why are your databases slow with large tables? an overview 40 solved: sudo mysql_secure_installation command not found in mariadb 41 how to connect to mysql using admin account remotely 42 how to list databases in postgresql 43 extracting time and date in ms sql server: a comprehensive guide 44 how to set up postgres using docker 45 understanding postgresql data integrity 46 parameterized queries in sql ‚Äì a guide 47 mysql substring function: the complete guide 48 creating custom visualizations with dbvisualizer 49 how to update a value incrementally (+1) in mysql 50 how to make your sql server more secure 51 how to pronounce sql: sql or sequel? 52 storing unstructured data with hstore in postgresql 53 the top four sites for database professionals seeking new job opportunities 54 migrating data between databases using dbvisualizer 55 archiving data in mysql using partitions 56 exploring the mysql daemon ‚Äì a guide 57 why do we call software bugs, bugs? 58 mastering advanced sql queries with dbvisualizer and postgresql 59 mysql‚Äôs utf-8: is it real? 60 postgresql coalesce function: handling null values effectively 61 insert queries - advanced crud explanation part 1 62 automating database operations with ansible and dbvisualizer 63 select queries - advanced crud explanation part 2 64 update queries - advanced crud explanation part 3 65 delete queries ‚Äì advanced crud explanation part 4 66 designing a multi-language database 67 preventing sql injection attacks with dbvisualizer 68 how to delete elasticsearch indices: an in-depth tutorial 69 a guide to data types in oracle 70 bridging the gap between sql and nosql in postgresql with json 71 exploring mongodb data structure with dbvisualizer's nested tree-table view 72 how to kill all connections to a database in postgresql 73 postgres on delete cascade - a guide 74 mastering the trino connection: unleash the power of dbvisualizer! 75 querying data with dbvisualizer and prestodb 76 postgresql jsonpath: dealing with sql/json path¬†language 77 deleting a column in sql: everything you need to know 78 postgresql isnull: the missing function 79 postgresql's string_agg() function - a comprehensive guide 80 describe table postgresql alternatives 81 comprehensive guide on like and fulltext search wildcards 82 understanding self joins in sql 83 oracle create database link 84 nulls are not the same ‚Äì a guide 85 using the explain plan to analyze query execution in postgresql 86 your database doesn‚Äôt like your data ‚Äì reserved words in mysql 87 google bigquery: a beginner‚Äôs guide 88 no-code ways of generating sql queries. 89 mysql operators ‚Äì a guide 90 discover dbvisualizer security features for mysql 91 working with numeric data types in mysql: a comprehensive guide 92 sql interview questions (mysql): part 1 93 how to work with sql query optimization 94 error establishing a database connection - common reasons and solutions 95 sql cheat sheet: a comprehensive guide to sql commands and queries 96 postgres list users: two different approaches 97 sql vs nosql databases: which is better? 98 change data capture: a comprehensive guide 99 mariadb vs mysql: the ultimate comparison 100 leveraging snowflake's time travel 101 inner join in sql: a comprehensive guide 102 postgresql materialized views: a beginner's guide 103 casting in postgresql: handling data type conversions effectively 104 date formatting in postgres: a comprehensive guide 105 view vs materialized view in databases: differences and use cases 106 sql deep dive: mastering datediff with code examples 107 automating databases with sql triggers 108 postgresql's on delete cascade feature 109 simplified overview of json vs. jsonb in postgresql 110 sql column deletion techniques 111 implementing self joins in sql: a guide for developers 112 postgresql - how to kill connections 113 oracle data types: an overview 114 er models, diagrams, and schemas: a developer's brief 115 introduction to the blob data type 116 quick guide to parameterized queries in sql 117 sql joins: easy guide for joining tables 118 a beginner's guide to database transactions 119 postgres in docker: a simplified setup process 120 resolving ""sudo mysql_secure_installation: command not found"" in mariadb 121 efficient date and time extraction in ms sql server 122 sql vs. sequel: navigating the pronunciation landscape 123 a quick guide to mysql port setup 124 date formatting in postgresql 125 quick mysql table creation: a developer's guide 126 maximizing sql server on macos: top clients for enhanced database management 127 simplify your development workflow: how to deploy postgresql using docker 128 understanding sql subqueries: a brief overview 129 a quick look at why we call them ""software bugs"" 130 postgresql and nosql capabilities with json and jsonb 131 postgresql for mysql users - alternatives to the describe statement 132 essentials of postgresql ctes 133 quick sql guide and cheat sheet: essential commands 134 exploring postgresql jsonpath for advanced json 135 how postgresql's coalesce function simplifies null value handling 136 database encryption: secure your data with best practices 137 database drivers: what you need to know 138 quick guide to postgresql's mvcc 139 handling nulls in postgresql: alternatives to isnull 140 how to use dbvisualizer for oracle database links 141 views vs. materialized views: what‚Äôs the difference? 142 efficiently managing unstructured data with postgresql hstore 143 best sql server clients for mac: simplify your database management 144 innodb's ibdata1: essential management tips for mysql 145 efficient ways to list postgresql databases 146 essential sql window functions for beginners 147 key data structures for sql database efficiency 148 leveraging postgresql cast for data type conversions 149 ssh tunneling: essential guide 150 postgresql backups simplified with pg_dump 151 mastering sql server: top 5 query tuning techniques 152 effective strategies for mysql user management 153 streamlining sql data management with generated columns 154 streamline mysql deployment with docker and dbvisualizer 155 essentials of acid in mysql 156 enhancing sql efficiency with multithreading 157 an introduction to sql stored procedures 158 building a postgresql database for e-commerce 159 using secure_file_priv to prevent illicit mysql uploads 160 utilizing mysql wildcards: like and fulltext 161 mastering sql: best practices for developers 162 how to handle databases with billions of records 163 simplified methods to list postgresql users 164 postgresql vs mysql: a concise comparison 165 mastering sql views: simplify and optimize your queries 166 optimizing data analysis with trino and dbvisualizer 167 essential sql commands for efficient database management 168 how to fix mysql shutdown unexpectedly error 169 efficiently analyzing postgresql queries with explain 170 essential guide to mysql subqueries 171 mariadb vs mysql: what developers need to know 172 acid in databases: a brief overview 173 postgresql distinct: an easy way to remove duplicates 174 maintaining data integrity in postgresql with constraints 175 sql server contains function: quick start guide 176 postgresql and dbvisualizer for better inventory management 177 simplifying database management with ansible and dbvisualizer 178 simple ways to enhance sql server security 179 optimizing postgresql queries with materialized views 180 navigating mongodb data structures with dbvisualizer 181 understanding sql coalesce for null handling 182 sql inner joins simplified 183 essential mysql operators and their applications 184 mysql daemon overview and tools 185 mysql numeric data types explained 186 quick guide to insert queries in mysql 187 boosting performance with select queries in crud 188 unlocking the power of update queries in databases 189 a brief guide to sql delete queries 190 quick guide to mysql math functions for sql users 191 simplify data migration between databases using dbvisualizer 192 database deadlocks: causes, examples, and solutions 193 sql interview prep for mysql: key questions to know 194 boost mysql performance with data partitioning 195 why large tables slow down your database 196 sql vs nosql databases: choosing the right tool 197 alter table: a quick overview for developers 198 avoiding errors with mysql reserved words 199 streamlining data auditing with snowflake time travel 200 change data capture: a beginner‚Äôs overview 201 avoiding the pitfalls of duplicate indexes in mysql 202 simplify distributed queries with presto and dbvisualizer 203 how to use substring_index in mysql 204 simplifying postgresql user management: two methods 205 how to quickly fix the ""error establishing a database connection"" 206 a practical guide to null in sql databases 207 advanced sql techniques for postgresql using dbvisualizer 208 secure your database: how dbvisualizer prevents sql injection 209 practical sql query optimization tips you should know 210 a beginner‚Äôs guide to mysql character sets and collations 211 mysql utf8 vs utf8mb4: which one should you use? 212 how database indexes improve sql query performance 213 mysql insert queries made simple 214 faster select queries: key tips for efficient database reads 215 quick guide to update queries: how to modify data efficiently 216 simplified guide to sql delete queries 217 database security made simple: essential practices 218 mastering mysql security: key steps to protect your database 219 simplify sql queries with coalesce 220 fixing mysql shutdown unexpectedly error 221 ace your sql interview with these query tips 222 getting started with sql distinct 223 substring_index in mysql: a simple guide to string extraction 224 mastering sql server contains for advanced text search 225 postgresql data types: an essential guide 226 sql datediff explained: syntax and database differences 227 pg_dump postgresql backups: a quick and practical guide 228 sql temporary tables: a quick and practical guide 229 handling big data in sql databases ‚Äì pitfalls to avoid 230 essential postgresql data types explained 231 three practical ways to create a database in mysql 232 how to use sql views to simplify data access and reporting 233 sql ddl commands explained: structuring your database 234 outer joins in sql: the key to keeping unmatched rows 235 postgresql case: build smarter queries with conditional logic 236 postgresql case: conditional logic for powerful sql queries 237 efficient string splitting in postgresql: 5 essential functions 238 how to start with database migrations 239 clean sql with comments: syntax and practices for all engines 240 database schema design: how to structure your app‚Äôs data effectively 241 schemas in postgresql: a practical guide for developers 242 alter table add column in sql: syntax, tips & tools 243 postgres text vs varchar: when to use which 244 why data visualization tools matter for developers, managers, and analysts 245 enhancing sql insert into performance: tips and tools 246 make your sql cleaner with common table expressions (ctes) 247 postgresql upsert: insert on conflict explained 248 learn sql‚Äôs cast function for clean type conversion 249 sql add to date: simple guide for mysql, postgresql, sql server & oracle 250 a developer‚Äôs guide to sql not in: smarter queries, faster results 251 understanding substring_index in sql: real use cases 252 essential sql commands for developers (with code samples) 253 how to add and subtract dates in sql (mysql, postgresql, sql server) this tutorial will empower you to implement change data capture (cdc), allowing you to actively track and manage data changes within your database environment. tools used in this tutorial dbvisualizer , top rated database management tool and sql client. in this tutorial, we'll take you through the entire spectrum of cdc, starting from its foundational concepts and extending to its practical execution. whether you're a developer, a dedicated database administrator, or simply someone passionate about data, this comprehensive guide offers you the knowledge and resources to adeptly monitor and manage data alterations within your database environment. let's plunge into the world of change data capture! prerequisites for you to make the most of this tutorial, you'll need: knowledge of databases: access to a database system such as mysql, postgresql, sql server, or oracle would be very beneficial. this guide will provide examples using mysql, so having a mysql server or equivalent will be great. dbvisualizer : a database tool that can greatly simplify database management and trigger creation. basic sql knowledge: familiarity with sql queries, database table creation, and crud operations (create, read, update, delete) will be helpful. introduction to change data capture (cdc) change data capture, often abbreviated as cdc, is a technique used to identify and capture changes made to data in a database. the primary purpose of cdc is to track modifications, additions, or deletions of data, enabling organizations to maintain an accurate and up-to-date record of their data history. it is very important in database management because data is the lifeblood of modern businesses, and having a historical record of data changes is crucial for various reasons, including compliance, auditing, analytics, and troubleshooting. cdc allows you to capture these changes efficiently and reliably. benefits of change data capture cdc offers several benefits, such as: real-time insights: cdc enables you to analyze data changes as they happen, providing timely insights. efficient data synchronization: cdc aids in synchronizing data between different systems, databases, or environments. accurate auditing: tracking changes helps maintain a transparent and accountable data history for regulatory compliance. reduced etl load: cdc can reduce the load on your extract, transform, load (etl) processes by focusing only on changed data. faster issue resolution: with a clear record of changes, troubleshooting, and issue resolution become faster and more effective. use cases of change data capture change data capture (cdc) offers a versatile set of applications that address various data management challenges. by capturing and tracking data changes, cdc becomes an essential tool in several critical scenarios, enhancing efficiency, accuracy, and decision-making. let's explore some of the prominent use cases where cdc shines: data warehousing: cdc is essential for maintaining accurate and up-to-date data in a data warehousing environment. by capturing changes from source systems and updating the data warehouse, organizations ensure that business users and analysts have access to the most current information for decision-making and analysis. this enables data-driven insights and reporting based on the latest data.
-** data migration:** during data migration projects, whether it's transitioning to a new database platform or consolidating data from multiple sources, cdc is instrumental in ensuring data integrity and consistency. by capturing changes in the source system and applying them to the target system, cdc reduces migration time, minimizes downtime, and ensures that the migrated data accurately reflects the latest changes. real-time analytics: real-time analytics requires access to the most current data. cdc facilitates this by continuously capturing and delivering data changes to analytical systems. this empowers organizations to respond quickly to market trends, customer behavior, and emerging opportunities. for instance, financial institutions can monitor stock market changes in real time to make informed investment decisions.
-** maintaining data integrity:** data integrity is crucial for maintaining trust in databases. cdc helps organizations maintain data integrity by tracking changes and providing an audit trail of data modifications. this is particularly vital in industries with regulatory requirements, such as healthcare and finance. cdc ensures that changes are recorded, enabling quick identification of unauthorized modifications and preserving data accuracy. understanding change data capture mechanisms let's take a closer look at the mechanisms that power cdc. by understanding these mechanisms, you'll gain insights into how cdc captures and processes data changes, paving the way for effective implementation. log-based cdc log-based cdc involves capturing changes from the database transaction logs. these logs record every transaction, allowing cdc mechanisms to identify and extract data modifications. what happens is that when a change (insert, update, delete) is made to a database, the corresponding transaction log entry is created. cdc mechanisms read these logs, interpret the changes, and apply them to a separate cdc repository or system. transaction logs store a sequential record of all database activities, including changes to data, metadata, and structural elements. log records include information about the affected rows, columns, and the type of operation performed. pros and cons of log-based cdc the pros of log-based cdc are: it offers near-real-time data capture. it has minimal impact on source database performance. it supports high transaction volumes. while its cons are: implementation complexity varies across dbms. requires access to transaction logs, which might be restricted. might not capture historical changes if logs are not retained. trigger-based cdc trigger-based cdc relies on database triggers, which are special procedures executed automatically when specific events occur in the database. triggers are set on specific tables to activate when a data change operation (insert, update, delete) occurs. these triggers execute custom logic to capture and transmit the changed data. when a trigger event occurs, the associated trigger code identifies the affected data and pushes it to a cdc system or repository for further processing. pros and cons of trigger-based cdc the pros of trigger-based cdc are: it offers more control over capturing logic. it works across different database systems. it can handle complex data transformations. the cons of trigger-based cdc are: it can impact source database performance, especially with high-frequency changes. managing and maintaining triggers can be challenging. it may require additional development effort. metadata-based cdc metadata-based cdc centers around alterations to the database schema and metadata. rather than directly monitoring data modifications, this method captures shifts in the database's structural elements and metadata, offering insights into potential data changes. by observing adjustments to table structures, column attributes, and relationships, metadata-based cdc deduces possible modifications to the data. pros and cons of metadata-based cdc the pros of metadata-based cdc are: less impact on source database performance. useful when direct access to transaction logs is restricted. can provide insights into potential data changes. the cons of trigger-based cdc are: not as granular as log-based or trigger-based cdc. limited to changes in metadata. requires careful interpretation of metadata changes. selecting the right cdc approach for your environment selecting the most suitable change data capture (cdc) mechanism is a critical decision influenced by several key factors. your choice should align with your specific database system, performance expectations, and the frequency of data changes. each cdc mechanism offers unique advantages that cater to distinct scenarios: log-based cdc: ideal for real-time scenarios: log-based cdc excels in real-time scenarios where capturing data changes with minimal delay is essential. by tapping into the transaction logs, this approach can swiftly detect and transmit changes as they occur. it is particularly beneficial for applications that demand up-to-the-minute insights, such as financial trading platforms or real-time monitoring systems. trigger-based cdc: providing control: trigger-based cdc offers a high level of control over capturing logic. it is suitable for environments where you need customized handling of data changes. by attaching triggers to specific tables, you can dictate precisely how changes are captured, transformed, and propagated. this mechanism empowers you to implement complex business rules and data transformations, making it valuable for scenarios like data quality control and nuanced etl processes. ** metadata-based cdc:** suited for limited access to logs: metadata-based cdc steps in when direct access to transaction logs is restricted or impractical. this approach observes changes in the database schema and metadata to deduce potential data modifications. it is particularly useful in situations where you might not have full access to transaction logs due to security constraints or database system limitations. metadata-based cdc allows you to infer changes indirectly, making it a valuable option for scenarios where other mechanisms might not be feasible. by carefully evaluating your database system's capabilities, your performance expectations, and the nature of data changes, you can make an informed decision about which cdc mechanism aligns best with your specific use case. whether you require real-time responsiveness, precise control over data capture, or flexibility in constrained environments, there's a cdc approach tailored to meet your needs. configuring and enabling cdc in the database 1. log-based cdc for log-based cdc , start by enabling transaction log capturing within your database system. this ensures that every data-changing operation is meticulously recorded in the transaction logs. subsequently, set up dedicated cdc agents or connectors that extract and interpret these logs. these agents act as conduits, transferring the captured changes to a designated cdc repository or system for further processing. 2. trigger-based cdc in the case of trigger-based cdc , the process entails creating triggers on the relevant database tables. these triggers automatically activate when a data modification (insert, update, delete) occurs. within these triggers, you'll define the custom logic required to capture and transmit the altered data. once triggered, this logic extracts the changed data and propels it towards the cdc repository or system. 3. metadata-based cdc for metadata-based cdc , the focus shifts to monitoring alterations in the database schema and metadata. as tables, columns, and relationships evolve, these changes signify potential data modifications. by actively observing these shifts, you can anticipate and deduce potential changes to the data. unlike other methods, metadata-based cdc does not capture changes directly; rather, it infers modifications based on metadata alterations. adhering to the specific steps aligned with your chosen cdc mechanism ensures a robust implementation that effectively captures and processes data changes for your intended purposes. setting up trigger-based cdc in mysql for this tutorial, we will set up one of the cdc methods; trigger-base cdc. let‚Äôs explore the query to set up trigger-based cdc in the mysql database environment: creating the cdc table:
first, create a table to store the captured changes. this table will hold information about the changes, such as the operation type (insert, update, or delete), the affected table, and the changed data. create table cdc_table (
  change_id int auto_increment primary key,
  table_name varchar(255),
  operation enum('insert','update','delete'),
  changed_data json,
  timestamp timestamp default current_timestamp
); enter fullscreen mode exit fullscreen mode creating triggers for data changes:
next, create triggers on the tables you want to track for changes. here's a generic example of an employees table: -- the data capture trigger for insert
@delimiter %%%;
create
trigger cdc_employees_insert
after insert on employees
for each row
begin
insert into cdc_table (table_name, operation, changed_data)
values ('employees', 'insert', json_object('id', new.id,'name'
, new.name,'salary', new.salary));
end;

%%%
@delimiter ; 
%%%

-- the data capture trigger for update
@delimiter %%%;
create
trigger cdc_employees_update
after update on employees
for each row
begin
insert into cdc_table (table_name, operation, changed_data)
values ('employees','update', json_object('id', new.id,'name'
, new.name, 'salary', new.salary));
end;

%%%
@delimiter ; 
%%%
-- the data capture trigger for delete
@delimiter %%%;
create trigger cdc_employees_delete
after delete on employees
for each row
begin
insert into cdc_table (table_name, operation, changed_data)
values ('employees', 'delete', json_object('id', old.id,'name'
, old.name, 'salary', old.salary));
end;

%%%
@delimiter ; 
%%% enter fullscreen mode exit fullscreen mode in this example, three triggers ( cdc_employees_insert , cdc_employees_update , and cdc_employees_delete ) are created for the employees table. each trigger fires after the respective operation (insert, update, delete) occurs on the employees table. the trigger logic inserts corresponding records into the cdc_table with information about the change operation and the changed data. creating a trigger in dbvisualizer. you can also create a trigger using the dbvisualizer interface by right-clicking on your database table and selecting ‚Äúcreate trigger‚Äù as in the image above. then entering insert your trigger code logic in the dialog that pops up and click on ‚Äúexecute‚Äù. entering the trigger logic in dbvisualizer. capturing changes: once the triggers are set up, any change made to the employees table will trigger the appropriate trigger, which, in turn, captures and stores the change information in the cdc_table . viewing the cdc table. keep in mind that trigger-based cdc can impact database performance, especially with high-frequency changes. carefully consider the performance implications and test the setup in a controlled environment. by following these steps, you can successfully implement trigger-based cdc in mysql to track and store data changes for the specified tables. remember to adapt the example to match your database schema and requirements. challenges and limitations of change data capture change data capture (cdc) offers valuable data tracking and management capabilities, but it also presents specific challenges and limitations that organizations should be aware of: performance impact: implementing cdc mechanisms, such as log-based or trigger-based approaches, can introduce additional overhead on the source database. frequent capturing and processing of changes may lead to increased resource consumption, potentially affecting overall database performance. mitigating this impact requires careful resource allocation and performance tuning. data volume and storage: in high-velocity data environments, where changes occur rapidly, the volume of captured data can quickly grow. this influx of data might strain storage resources, leading to higher storage costs. organizations must implement effective data retention strategies and compression techniques to manage the storage demands of cdc-generated data. latency and real-time processing: achieving real-time or near-real-time data capture is a goal of cdc, but actual latency can vary. delays might arise due to network congestion, processing time, or synchronization issues. organizations relying on immediate data availability must assess the chosen cdc mechanism's capabilities against their latency requirements. complexity in high-frequency scenarios: in scenarios characterized by high-frequency data changes, managing and processing an abundance of changes can become complex. ensuring the cdc process can handle and scale with the increased workload requires careful planning, distribution of tasks, and robust infrastructure. security and compliance: cdc involves capturing and replicating sensitive data changes, which raises security concerns. organizations must implement robust security measures to safeguard captured data, comply with data protection regulations, and ensure that the cdc process doesn't introduce vulnerabilities. while change data capture (cdc) offers valuable data tracking and management capabilities, it comes with challenges. these include potential performance overhead, managing data volume and storage, achieving real-time processing, handling complexity in high-frequency scenarios, and ensuring security and compliance. organizations must strategize and plan to harness cdc's benefits while effectively addressing these limitations. conclusion congratulations on completing our comprehensive guide on change data capture (cdc). throughout this tutorial, we've explored capturing and managing data changes in your database environment, from fundamentals to practical implementation. an integral part of this journey was dbvisualizer, a powerful tool that streamlined trigger creation and execution. if you haven't already, we encourage you to explore dbvisualizer's capabilities for smoother cdc implementation. cdc plays a vital role in modern data management, providing historical accuracy, compliance support, and efficient data workflows. as you continue, adapt cdc mechanisms to your organization's needs. stay engaged with the evolving cdc landscape, and consider how tools like dbvisualizer can enhance your data management. with cdc and dbvisualizer, you're well-equipped for successful data tracking and management. happy exploring and happy data tracking! faq (frequently asked questions) what is change data capture (cdc), and how does it work? change data capture (cdc) is a method to track and record changes in a database, achieved through log-based, trigger-based, or metadata-based mechanisms. how can i implement cdc in mysql using triggers? to implement cdc in mysql, you'll set up triggers on relevant tables to automatically capture and store data changes in a designated cdc table. what are the benefits of change data capture (cdc) in databases? cdc provides real-time insights, efficient data synchronization, accurate auditing, reduced etl load, and quicker issue resolution. what are the use cases of change data capture (cdc)? cdc is used for data warehousing, data migration, real-time analytics, and maintaining data integrity. what challenges can arise with change data capture (cdc) implementation? implementing cdc can impact performance, create data volume and latency issues, introduce complexity, and raise security concerns. about the author ochuko onojakpor is a full-stack python/react software developer and freelance technical writer. he spends his free time contributing to open source and tutoring students on programming in collaboration with google dsc. the table (253 part series) 1 optimizing queries for performance: mysql edition 2 json vs. jsonb in postgresql: a complete comparison ... 249 more parts... 3 architecting for security: mysql edition 4 the data you‚Äòve left behind ‚Äì an attacker‚Äòs perspective 5 how to deal with a database with billions of records 6 writing reusable sql queries for your application with dbvisualizer scripts 7 how to design and build a database in postgres 8 top five query tuning techniques for microsoft sql server 9 using postgresql to manage business inventory data and visualize it 10 sql triggers: what they are and how to use them 11 stored procedures in sql: a complete tutorial 12 ssh tunneling: the good, the bad, and the ugly 13 how to create a table like another table in mysql 14 firing database actions: a guide to using sql triggers with dbvisualizer 15 a comprehensive guide to data structures in sql 16 database transactions 101: the essential guide 17 the complete guide to sql subqueries 18 how to secure data in a postgres database: a guide 19 working with null in databases. turn your frustration into delight 20 acid in database systems: all you need to know 21 database security 101: best practices to secure your data 22 best practices for encrypting data in your database 23 getting started with multiversion concurrency control (mvcc) in postgresql 24 how to use a foreign key referring to the source table in postgres 25 blob data type: everything you can do with it 26 a guide to acid in mysql 27 preventing illicit uploads in mysql ‚Äì secure_file_priv 28 the ultimate guide to the mysql port 29 alter table explained 30 deadlocks in databases: a guide 31 a guide to subqueries in mysql 32 10x query performance with a database index 33 character sets vs. collations in a mysql database infrastructure 34 best practices for sql coding and development 35 a guide to multithreading in sql 36 er model, er diagram, and relational schema: what's the difference? 37 the ultimate guide to sql generated columns 38 mysql user management: a guide 39 why are your databases slow with large tables? an overview 40 solved: sudo mysql_secure_installation command not found in mariadb 41 how to connect to mysql using admin account remotely 42 how to list databases in postgresql 43 extracting time and date in ms sql server: a comprehensive guide 44 how to set up postgres using docker 45 understanding postgresql data integrity 46 parameterized queries in sql ‚Äì a guide 47 mysql substring function: the complete guide 48 creating custom visualizations with dbvisualizer 49 how to update a value incrementally (+1) in mysql 50 how to make your sql server more secure 51 how to pronounce sql: sql or sequel? 52 storing unstructured data with hstore in postgresql 53 the top four sites for database professionals seeking new job opportunities 54 migrating data between databases using dbvisualizer 55 archiving data in mysql using partitions 56 exploring the mysql daemon ‚Äì a guide 57 why do we call software bugs, bugs? 58 mastering advanced sql queries with dbvisualizer and postgresql 59 mysql‚Äôs utf-8: is it real? 60 postgresql coalesce function: handling null values effectively 61 insert queries - advanced crud explanation part 1 62 automating database operations with ansible and dbvisualizer 63 select queries - advanced crud explanation part 2 64 update queries - advanced crud explanation part 3 65 delete queries ‚Äì advanced crud explanation part 4 66 designing a multi-language database 67 preventing sql injection attacks with dbvisualizer 68 how to delete elasticsearch indices: an in-depth tutorial 69 a guide to data types in oracle 70 bridging the gap between sql and nosql in postgresql with json 71 exploring mongodb data structure with dbvisualizer's nested tree-table view 72 how to kill all connections to a database in postgresql 73 postgres on delete cascade - a guide 74 mastering the trino connection: unleash the power of dbvisualizer! 75 querying data with dbvisualizer and prestodb 76 postgresql jsonpath: dealing with sql/json path¬†language 77 deleting a column in sql: everything you need to know 78 postgresql isnull: the missing function 79 postgresql's string_agg() function - a comprehensive guide 80 describe table postgresql alternatives 81 comprehensive guide on like and fulltext search wildcards 82 understanding self joins in sql 83 oracle create database link 84 nulls are not the same ‚Äì a guide 85 using the explain plan to analyze query execution in postgresql 86 your database doesn‚Äôt like your data ‚Äì reserved words in mysql 87 google bigquery: a beginner‚Äôs guide 88 no-code ways of generating sql queries. 89 mysql operators ‚Äì a guide 90 discover dbvisualizer security features for mysql 91 working with numeric data types in mysql: a comprehensive guide 92 sql interview questions (mysql): part 1 93 how to work with sql query optimization 94 error establishing a database connection - common reasons and solutions 95 sql cheat sheet: a comprehensive guide to sql commands and queries 96 postgres list users: two different approaches 97 sql vs nosql databases: which is better? 98 change data capture: a comprehensive guide 99 mariadb vs mysql: the ultimate comparison 100 leveraging snowflake's time travel 101 inner join in sql: a comprehensive guide 102 postgresql materialized views: a beginner's guide 103 casting in postgresql: handling data type conversions effectively 104 date formatting in postgres: a comprehensive guide 105 view vs materialized view in databases: differences and use cases 106 sql deep dive: mastering datediff with code examples 107 automating databases with sql triggers 108 postgresql's on delete cascade feature 109 simplified overview of json vs. jsonb in postgresql 110 sql column deletion techniques 111 implementing self joins in sql: a guide for developers 112 postgresql - how to kill connections 113 oracle data types: an overview 114 er models, diagrams, and schemas: a developer's brief 115 introduction to the blob data type 116 quick guide to parameterized queries in sql 117 sql joins: easy guide for joining tables 118 a beginner's guide to database transactions 119 postgres in docker: a simplified setup process 120 resolving ""sudo mysql_secure_installation: command not found"" in mariadb 121 efficient date and time extraction in ms sql server 122 sql vs. sequel: navigating the pronunciation landscape 123 a quick guide to mysql port setup 124 date formatting in postgresql 125 quick mysql table creation: a developer's guide 126 maximizing sql server on macos: top clients for enhanced database management 127 simplify your development workflow: how to deploy postgresql using docker 128 understanding sql subqueries: a brief overview 129 a quick look at why we call them ""software bugs"" 130 postgresql and nosql capabilities with json and jsonb 131 postgresql for mysql users - alternatives to the describe statement 132 essentials of postgresql ctes 133 quick sql guide and cheat sheet: essential commands 134 exploring postgresql jsonpath for advanced json 135 how postgresql's coalesce function simplifies null value handling 136 database encryption: secure your data with best practices 137 database drivers: what you need to know 138 quick guide to postgresql's mvcc 139 handling nulls in postgresql: alternatives to isnull 140 how to use dbvisualizer for oracle database links 141 views vs. materialized views: what‚Äôs the difference? 142 efficiently managing unstructured data with postgresql hstore 143 best sql server clients for mac: simplify your database management 144 innodb's ibdata1: essential management tips for mysql 145 efficient ways to list postgresql databases 146 essential sql window functions for beginners 147 key data structures for sql database efficiency 148 leveraging postgresql cast for data type conversions 149 ssh tunneling: essential guide 150 postgresql backups simplified with pg_dump 151 mastering sql server: top 5 query tuning techniques 152 effective strategies for mysql user management 153 streamlining sql data management with generated columns 154 streamline mysql deployment with docker and dbvisualizer 155 essentials of acid in mysql 156 enhancing sql efficiency with multithreading 157 an introduction to sql stored procedures 158 building a postgresql database for e-commerce 159 using secure_file_priv to prevent illicit mysql uploads 160 utilizing mysql wildcards: like and fulltext 161 mastering sql: best practices for developers 162 how to handle databases with billions of records 163 simplified methods to list postgresql users 164 postgresql vs mysql: a concise comparison 165 mastering sql views: simplify and optimize your queries 166 optimizing data analysis with trino and dbvisualizer 167 essential sql commands for efficient database management 168 how to fix mysql shutdown unexpectedly error 169 efficiently analyzing postgresql queries with explain 170 essential guide to mysql subqueries 171 mariadb vs mysql: what developers need to know 172 acid in databases: a brief overview 173 postgresql distinct: an easy way to remove duplicates 174 maintaining data integrity in postgresql with constraints 175 sql server contains function: quick start guide 176 postgresql and dbvisualizer for better inventory management 177 simplifying database management with ansible and dbvisualizer 178 simple ways to enhance sql server security 179 optimizing postgresql queries with materialized views 180 navigating mongodb data structures with dbvisualizer 181 understanding sql coalesce for null handling 182 sql inner joins simplified 183 essential mysql operators and their applications 184 mysql daemon overview and tools 185 mysql numeric data types explained 186 quick guide to insert queries in mysql 187 boosting performance with select queries in crud 188 unlocking the power of update queries in databases 189 a brief guide to sql delete queries 190 quick guide to mysql math functions for sql users 191 simplify data migration between databases using dbvisualizer 192 database deadlocks: causes, examples, and solutions 193 sql interview prep for mysql: key questions to know 194 boost mysql performance with data partitioning 195 why large tables slow down your database 196 sql vs nosql databases: choosing the right tool 197 alter table: a quick overview for developers 198 avoiding errors with mysql reserved words 199 streamlining data auditing with snowflake time travel 200 change data capture: a beginner‚Äôs overview 201 avoiding the pitfalls of duplicate indexes in mysql 202 simplify distributed queries with presto and dbvisualizer 203 how to use substring_index in mysql 204 simplifying postgresql user management: two methods 205 how to quickly fix the ""error establishing a database connection"" 206 a practical guide to null in sql databases 207 advanced sql techniques for postgresql using dbvisualizer 208 secure your database: how dbvisualizer prevents sql injection 209 practical sql query optimization tips you should know 210 a beginner‚Äôs guide to mysql character sets and collations 211 mysql utf8 vs utf8mb4: which one should you use? 212 how database indexes improve sql query performance 213 mysql insert queries made simple 214 faster select queries: key tips for efficient database reads 215 quick guide to update queries: how to modify data efficiently 216 simplified guide to sql delete queries 217 database security made simple: essential practices 218 mastering mysql security: key steps to protect your database 219 simplify sql queries with coalesce 220 fixing mysql shutdown unexpectedly error 221 ace your sql interview with these query tips 222 getting started with sql distinct 223 substring_index in mysql: a simple guide to string extraction 224 mastering sql server contains for advanced text search 225 postgresql data types: an essential guide 226 sql datediff explained: syntax and database differences 227 pg_dump postgresql backups: a quick and practical guide 228 sql temporary tables: a quick and practical guide 229 handling big data in sql databases ‚Äì pitfalls to avoid 230 essential postgresql data types explained 231 three practical ways to create a database in mysql 232 how to use sql views to simplify data access and reporting 233 sql ddl commands explained: structuring your database 234 outer joins in sql: the key to keeping unmatched rows 235 postgresql case: build smarter queries with conditional logic 236 postgresql case: conditional logic for powerful sql queries 237 efficient string splitting in postgresql: 5 essential functions 238 how to start with database migrations 239 clean sql with comments: syntax and practices for all engines 240 database schema design: how to structure your app‚Äôs data effectively 241 schemas in postgresql: a practical guide for developers 242 alter table add column in sql: syntax, tips & tools 243 postgres text vs varchar: when to use which 244 why data visualization tools matter for developers, managers, and analysts 245 enhancing sql insert into performance: tips and tools 246 make your sql cleaner with common table expressions (ctes) 247 postgresql upsert: insert on conflict explained 248 learn sql‚Äôs cast function for clean type conversion 249 sql add to date: simple guide for mysql, postgresql, sql server & oracle 250 a developer‚Äôs guide to sql not in: smarter queries, faster results 251 understanding substring_index in sql: real use cases 252 essential sql commands for developers (with code samples) 253 how to add and subtract dates in sql (mysql, postgresql, sql server) top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct ‚Ä¢ report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse dbvisualizer follow dbvisualizer is used for development, analytics, maintenance, and more, by database professionals all over the world. it connects to all popular databases and runs on win, macos & linux. location nacka, sweden joined jan 31, 2023 trending on dev community hot meme monday # discuss # watercooler # jokes introducing dev education tracks: expert-guided tutorials for learning new skills and earning badges # deved # career # ai # gemini üíª 10 genius technical projects that can 10x your resume in 2025 üíº # programming # webdev # career # development üíé dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community ‚Äî a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem ‚Äî the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community ¬© 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",17
https://dev.to/dbvismarketing/postgresql-jsonpath-dealing-with-sqljson-path-language-1458,,,"postgresql jsonpath: dealing with sql/json path¬†language - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse dbvisualizer posted on oct 16, 2023 ‚Ä¢ originally published at dbvis.com postgresql jsonpath: dealing with sql/json path¬†language # postgres # json the table (252 part series) 1 optimizing queries for performance: mysql edition 2 json vs. jsonb in postgresql: a complete comparison ... 248 more parts... 3 architecting for security: mysql edition 4 the data you‚Äòve left behind ‚Äì an attacker‚Äòs perspective 5 how to deal with a database with billions of records 6 writing reusable sql queries for your application with dbvisualizer scripts 7 how to design and build a database in postgres 8 top five query tuning techniques for microsoft sql server 9 using postgresql to manage business inventory data and visualize it 10 sql triggers: what they are and how to use them 11 stored procedures in sql: a complete tutorial 12 ssh tunneling: the good, the bad, and the ugly 13 how to create a table like another table in mysql 14 firing database actions: a guide to using sql triggers with dbvisualizer 15 a comprehensive guide to data structures in sql 16 database transactions 101: the essential guide 17 the complete guide to sql subqueries 18 how to secure data in a postgres database: a guide 19 working with null in databases. turn your frustration into delight 20 acid in database systems: all you need to know 21 database security 101: best practices to secure your data 22 best practices for encrypting data in your database 23 getting started with multiversion concurrency control (mvcc) in postgresql 24 how to use a foreign key referring to the source table in postgres 25 blob data type: everything you can do with it 26 a guide to acid in mysql 27 preventing illicit uploads in mysql ‚Äì secure_file_priv 28 the ultimate guide to the mysql port 29 alter table explained 30 deadlocks in databases: a guide 31 a guide to subqueries in mysql 32 10x query performance with a database index 33 character sets vs. collations in a mysql database infrastructure 34 best practices for sql coding and development 35 a guide to multithreading in sql 36 er model, er diagram, and relational schema: what's the difference? 37 the ultimate guide to sql generated columns 38 mysql user management: a guide 39 why are your databases slow with large tables? an overview 40 solved: sudo mysql_secure_installation command not found in mariadb 41 how to connect to mysql using admin account remotely 42 how to list databases in postgresql 43 extracting time and date in ms sql server: a comprehensive guide 44 how to set up postgres using docker 45 understanding postgresql data integrity 46 parameterized queries in sql ‚Äì a guide 47 mysql substring function: the complete guide 48 creating custom visualizations with dbvisualizer 49 how to update a value incrementally (+1) in mysql 50 how to make your sql server more secure 51 how to pronounce sql: sql or sequel? 52 storing unstructured data with hstore in postgresql 53 the top four sites for database professionals seeking new job opportunities 54 migrating data between databases using dbvisualizer 55 archiving data in mysql using partitions 56 exploring the mysql daemon ‚Äì a guide 57 why do we call software bugs, bugs? 58 mastering advanced sql queries with dbvisualizer and postgresql 59 mysql‚Äôs utf-8: is it real? 60 insert queries - advanced crud explanation part 1 61 postgresql coalesce function: handling null values effectively 62 automating database operations with ansible and dbvisualizer 63 select queries - advanced crud explanation part 2 64 update queries - advanced crud explanation part 3 65 delete queries ‚Äì advanced crud explanation part 4 66 designing a multi-language database 67 preventing sql injection attacks with dbvisualizer 68 how to delete elasticsearch indices: an in-depth tutorial 69 a guide to data types in oracle 70 bridging the gap between sql and nosql in postgresql with json 71 exploring mongodb data structure with dbvisualizer's nested tree-table view 72 how to kill all connections to a database in postgresql 73 postgres on delete cascade - a guide 74 mastering the trino connection: unleash the power of dbvisualizer! 75 querying data with dbvisualizer and prestodb 76 postgresql jsonpath: dealing with sql/json path¬†language 77 deleting a column in sql: everything you need to know 78 postgresql isnull: the missing function 79 postgresql's string_agg() function - a comprehensive guide 80 describe table postgresql alternatives 81 comprehensive guide on like and fulltext search wildcards 82 understanding self joins in sql 83 oracle create database link 84 nulls are not the same ‚Äì a guide 85 using the explain plan to analyze query execution in postgresql 86 your database doesn‚Äôt like your data ‚Äì reserved words in mysql 87 google bigquery: a beginner‚Äôs guide 88 no-code ways of generating sql queries. 89 mysql operators ‚Äì a guide 90 discover dbvisualizer security features for mysql 91 working with numeric data types in mysql: a comprehensive guide 92 sql interview questions (mysql): part 1 93 how to work with sql query optimization 94 error establishing a database connection - common reasons and solutions 95 sql cheat sheet: a comprehensive guide to sql commands and queries 96 postgres list users: two different approaches 97 sql vs nosql databases: which is better? 98 change data capture: a comprehensive guide 99 mariadb vs mysql: the ultimate comparison 100 leveraging snowflake's time travel 101 inner join in sql: a comprehensive guide 102 postgresql materialized views: a beginner's guide 103 casting in postgresql: handling data type conversions effectively 104 date formatting in postgres: a comprehensive guide 105 view vs materialized view in databases: differences and use cases 106 sql deep dive: mastering datediff with code examples 107 automating databases with sql triggers 108 postgresql's on delete cascade feature 109 simplified overview of json vs. jsonb in postgresql 110 sql column deletion techniques 111 implementing self joins in sql: a guide for developers 112 postgresql - how to kill connections 113 oracle data types: an overview 114 er models, diagrams, and schemas: a developer's brief 115 introduction to the blob data type 116 quick guide to parameterized queries in sql 117 sql joins: easy guide for joining tables 118 a beginner's guide to database transactions 119 postgres in docker: a simplified setup process 120 resolving ""sudo mysql_secure_installation: command not found"" in mariadb 121 efficient date and time extraction in ms sql server 122 sql vs. sequel: navigating the pronunciation landscape 123 a quick guide to mysql port setup 124 date formatting in postgresql 125 quick mysql table creation: a developer's guide 126 maximizing sql server on macos: top clients for enhanced database management 127 simplify your development workflow: how to deploy postgresql using docker 128 understanding sql subqueries: a brief overview 129 a quick look at why we call them ""software bugs"" 130 postgresql and nosql capabilities with json and jsonb 131 postgresql for mysql users - alternatives to the describe statement 132 essentials of postgresql ctes 133 quick sql guide and cheat sheet: essential commands 134 exploring postgresql jsonpath for advanced json 135 how postgresql's coalesce function simplifies null value handling 136 database encryption: secure your data with best practices 137 database drivers: what you need to know 138 quick guide to postgresql's mvcc 139 handling nulls in postgresql: alternatives to isnull 140 how to use dbvisualizer for oracle database links 141 views vs. materialized views: what‚Äôs the difference? 142 efficiently managing unstructured data with postgresql hstore 143 best sql server clients for mac: simplify your database management 144 innodb's ibdata1: essential management tips for mysql 145 efficient ways to list postgresql databases 146 essential sql window functions for beginners 147 key data structures for sql database efficiency 148 leveraging postgresql cast for data type conversions 149 ssh tunneling: essential guide 150 postgresql backups simplified with pg_dump 151 mastering sql server: top 5 query tuning techniques 152 effective strategies for mysql user management 153 streamlining sql data management with generated columns 154 streamline mysql deployment with docker and dbvisualizer 155 essentials of acid in mysql 156 enhancing sql efficiency with multithreading 157 an introduction to sql stored procedures 158 building a postgresql database for e-commerce 159 using secure_file_priv to prevent illicit mysql uploads 160 utilizing mysql wildcards: like and fulltext 161 mastering sql: best practices for developers 162 how to handle databases with billions of records 163 simplified methods to list postgresql users 164 postgresql vs mysql: a concise comparison 165 mastering sql views: simplify and optimize your queries 166 optimizing data analysis with trino and dbvisualizer 167 essential sql commands for efficient database management 168 how to fix mysql shutdown unexpectedly error 169 efficiently analyzing postgresql queries with explain 170 essential guide to mysql subqueries 171 mariadb vs mysql: what developers need to know 172 acid in databases: a brief overview 173 postgresql distinct: an easy way to remove duplicates 174 maintaining data integrity in postgresql with constraints 175 sql server contains function: quick start guide 176 postgresql and dbvisualizer for better inventory management 177 simplifying database management with ansible and dbvisualizer 178 simple ways to enhance sql server security 179 optimizing postgresql queries with materialized views 180 navigating mongodb data structures with dbvisualizer 181 understanding sql coalesce for null handling 182 sql inner joins simplified 183 essential mysql operators and their applications 184 mysql daemon overview and tools 185 mysql numeric data types explained 186 quick guide to insert queries in mysql 187 boosting performance with select queries in crud 188 unlocking the power of update queries in databases 189 a brief guide to sql delete queries 190 quick guide to mysql math functions for sql users 191 simplify data migration between databases using dbvisualizer 192 database deadlocks: causes, examples, and solutions 193 sql interview prep for mysql: key questions to know 194 boost mysql performance with data partitioning 195 why large tables slow down your database 196 sql vs nosql databases: choosing the right tool 197 alter table: a quick overview for developers 198 avoiding errors with mysql reserved words 199 streamlining data auditing with snowflake time travel 200 change data capture: a beginner‚Äôs overview 201 avoiding the pitfalls of duplicate indexes in mysql 202 simplify distributed queries with presto and dbvisualizer 203 how to use substring_index in mysql 204 simplifying postgresql user management: two methods 205 how to quickly fix the ""error establishing a database connection"" 206 a practical guide to null in sql databases 207 advanced sql techniques for postgresql using dbvisualizer 208 secure your database: how dbvisualizer prevents sql injection 209 practical sql query optimization tips you should know 210 a beginner‚Äôs guide to mysql character sets and collations 211 mysql utf8 vs utf8mb4: which one should you use? 212 how database indexes improve sql query performance 213 mysql insert queries made simple 214 faster select queries: key tips for efficient database reads 215 quick guide to update queries: how to modify data efficiently 216 simplified guide to sql delete queries 217 database security made simple: essential practices 218 mastering mysql security: key steps to protect your database 219 simplify sql queries with coalesce 220 fixing mysql shutdown unexpectedly error 221 ace your sql interview with these query tips 222 getting started with sql distinct 223 substring_index in mysql: a simple guide to string extraction 224 mastering sql server contains for advanced text search 225 postgresql data types: an essential guide 226 sql datediff explained: syntax and database differences 227 pg_dump postgresql backups: a quick and practical guide 228 sql temporary tables: a quick and practical guide 229 handling big data in sql databases ‚Äì pitfalls to avoid 230 essential postgresql data types explained 231 three practical ways to create a database in mysql 232 how to use sql views to simplify data access and reporting 233 sql ddl commands explained: structuring your database 234 outer joins in sql: the key to keeping unmatched rows 235 postgresql case: build smarter queries with conditional logic 236 postgresql case: conditional logic for powerful sql queries 237 efficient string splitting in postgresql: 5 essential functions 238 how to start with database migrations 239 clean sql with comments: syntax and practices for all engines 240 database schema design: how to structure your app‚Äôs data effectively 241 schemas in postgresql: a practical guide for developers 242 alter table add column in sql: syntax, tips & tools 243 postgres text vs varchar: when to use which 244 why data visualization tools matter for developers, managers, and analysts 245 enhancing sql insert into performance: tips and tools 246 make your sql cleaner with common table expressions (ctes) 247 postgresql upsert: insert on conflict explained 248 learn sql‚Äôs cast function for clean type conversion 249 sql add to date: simple guide for mysql, postgresql, sql server & oracle 250 a developer‚Äôs guide to sql not in: smarter queries, faster results 251 understanding substring_index in sql: real use cases 252 essential sql commands for developers (with code samples) write jsonpath expressions in postgresql through the sql/json path language to query json data with no effort. tools used in this tutorial dbvisualizer , top rated database management tool and sql client. the postgresql database postgresql supports the storage of json data in your tables through the jsonb data types. this opens up many opportunities that go beyond the traditional possibilities of sql. however, this functionality would be limited without a way to naturally and efficiently query json. here is where the postgresql jsonpath feature comes in! this special data type allows you to specify expressions in the sql/json path language to access, filter, and extract json items. in this article, you will learn what jsonpath is, why it is useful, what it has to offer, and how to use it in some examples. let‚Äôs dive in! what is postgresql jsonpath? the jsonpath data type adds support for the postgresql sql/json path language, which involves expressions aimed at efficiently querying json data. you can think of that language as a sort of xpath but for json and in an sql environment. in other words, an sql/json path expression consists of a sequence of elements allowed by the postgresql jsonpath data type. when the dbms encounter those expressions, it passes them to the internal path engine for execution. if the evaluation is successful, it returns the json element or set of elements matching the json query logic of the expression. you cannot use jsonpath expressions directly in postgresql queries, but you must pass them to the json functions that accept them as arguments. the engine will first execute the expressions and then call the function with their results. sql/json path language syntax a jsonpath expression follows the sql/json path language and consists of a sequence of path elements. the allowed ones are: json primitive types: text, numeric, true, false, or null. parentheses: to define the order of evaluation or for writing filter sub-expressions. path variables, accessors, operators and methods: special elements to select, filter, or access json data. we will now dig into what these mean. a filter expression begins with a question mark and accepts a condition in round parentheses: ? (condition) an expression can contain one or more filter expressions. these work similarly to the where clause in sql and are executed first. after that step, the result set will include only json items that satisfy the provided condition with a true value. in detail, an sql/json condition can return one of three values: true , false , or unknown . the unknown value plays the same role as null in sql. the most popular filter predicates are: == , != or <>: , < , <= , > , >= , && , || , ! is unknown : tests whether a condition returns the unknown value. like_regex : tests whether the first operand matches the regular expression given by the second operand. starts with : tests whether the second operand is an initial substring of the first operand. to provide a natural way of working with json data, the jsonpath syntax relies on some javascript conventions: . : the dot character is used for accessing members. [] : square brackets are used for accessing elements in arrays. note that sql/json arrays start from 0 and not from 1 like regular sql arrays. postgresql jsonpath expressions must be written in queries as sql strings. so, you have to enclose them in single quotes '. string values inside the expressions should be enclosed with double quotes "". take a look at an example of a jsonpath expression: $.user.addresses[0].city this selects the city associated with the first address of a user. the $ character corresponds to the root of the json value being queried. you can use it in a query as follows: 1 select jsonb_path_query(""data"", '$.user.addresses[0].city') as ""city""
2 from ""user_data""
3 where user_id = 1 enter fullscreen mode exit fullscreen mode remember that you cannot use the sql/json path language directly in the select clause. to execute the expression, you need to pass it to the jsonb_path_query() function. if you are wondering why the function has ‚Äújsonb‚Äù and not ‚Äújson‚Äù in its name, check out our json vs jsonb article . sql/json path expressions are evaluated from left to right, following parentheses to determine the order of operations. postgresql jsonpath: variables, accessors, operators and methods as mentioned before, jsonpath supports some special elements. let‚Äôs explore them all. variables $ : represents the root of json value being queried. $ : a named variable that can be set in the vars argument accepted by several json functions. (e.g., jsonb_path_exists('{""values"":[1, 2, 3, 4, 5]}', '$.values[*] ? (@ >= $min && @ <= $max)', '{""min"":2, ""max"":4}') returns whether the json data has at least one value between 2 and 4) @ : represents the result of the path evaluation in a filter expression. accessors the most important accessors you should know are: . : returns an object member with the specified name. if the key name does not meet the javascript rules for an identifier, it must be enclosed in double quotes (e.g., .""player-config"" ). .""$"" : returns the value of the named variable that can be set by the vars parameter of several json processing functions. .* : returns the values of all members at the top level of the current object. .** : processes all levels of the current object's json hierarchy and returns all member values, regardless of their nesting level. [index] : returns the single array element specified by the index. [start_index, end_index] : returns a slice of the array based on the index range, including beginning and ending elements. [*] : returns all array elements. operators and methods the most interesting are: + , - , * , / , % : addition, subtraction, multiplication, division, and modulo. .type() : returns the type of a json item. .size() : returns the size of the json item (number of elements in an array, or 1 if it is not an array). jsonpath examples assume you have the following json data stored in a postgresql table: 1 {
2     ""game"": ""villageday"",
3     ""players"": [
4         {
5             ""username"": ""ninjohn"",
6             ""score"": 31830,
7             ""achievements"": [
8                 ""first victory""
9             ]
10         },
11         {
12             ""username"": ""janethebest"",
13             ""score"": 2714685,
14             ""achievements"": [
15                 ""first victory"",
16                 ""100 doubles"",
17                 ""100 victories"",
18                 ""100 triples""
19             ]
20         },
21         {
22             ""username"": ""mary84"",
23             ""score"": 0,
24             ""achievements"": [
25             ]
26         }
27     ]
28 } enter fullscreen mode exit fullscreen mode in detail, you have it in the first row of a configs table in the games database: note that the result printed by dbvisualizer matches the json content presented above. time to see some postgresql jsonpath expressions in action in real-world queries. retrieving all usernames 1 select jsonb_path_query_array(""data"", '$.players[*].username') as usernames
2 from ""configs""
3 where ""id"" = 1; enter fullscreen mode exit fullscreen mode use jsonb_path_query_array() instead of jsonb_path_query() for expressions that return an array instead of plain json values. this query will produce: [""ninjohn"", ""janethebest"", ""mary84""] executing the sql/json path query in dbvisualizer. find the players who have achieved the ‚Äúvictory‚Äù accomplishment in a tabular format 1 select jsonb_path_query(""data"", '$.players[*] ? (@.achievements[*] == ""first victory"")') as player
2 from ""configs""
3    
4 where ""id"" = 1; enter fullscreen mode exit fullscreen mode the query returns: this time the result set is a table and not a single value. note the use of the filter expression. when jsonb_path_query involves several items, it returns them in tabular format. this means you can use the result of this query in in clauses or other selects . retrieve the username of the players with a score greater than or equal to 1000 1 select jsonb_path_query(""data"", '($.players[*] ? (@.score >= $min_score)).username', '{""min_score"": 1000}') as username
2 from ""configs""
3 where ""id"" = 1; enter fullscreen mode exit fullscreen mode this will get the following result: the resulting table matches the expected result. note the use of the $min_score named variable. get the number of players 1 select jsonb_path_query(""data"", '$.players.size()') as total_players
2 from ""configs""
3    
4 where id = 1; enter fullscreen mode exit fullscreen mode this returns: 3 there are 3 players in the json data. note the use of the size() method to get the elements in the players array. congrats! you are now a postgresql jsonpath master! conclusion postgresql supports json data through jsonb data type, which is a perfect solution if you are looking for nosql-like functionality. jsonpath further extends those capabilities by offering a language that allows data in json format to be intuitively explored and accessed. that language is not that complex, but some of operators and methods are not so easy to understand. here is why you need to test your sql/json queries in a database client that fully supports postgresql, such as dbvisualizer. this tool allows you to explore data from dozens of dbmss while offering query optimization features, visual data exploration functionality, and full support for most database-specific features, including jsonpath . download dbvisualizer for free now! faq how does the sql/json path language differ from regular sql in postgresql? the sql/json path language in postgresql is an extension of regular sql that allows querying and extracting data from json documents within the database. while regular sql focuses on relational data, sql/json path language enables navigation through nested json structures and retrieval of specific json elements. what are the two modes of handling structural errors in sql/json path expressions? the two modes of handling structural errors in sql/json path expressions are: lax: if a path expression encounters a structural error, it returns a null value without raising an error. this is the default mode. specify this behavior with the following syntax: lax . strict: a structural error causes an error. enable with mode with the following syntax: strict . can jsonpath be used with other postgresql features, such as indexing, full-text search, or triggers? yes, jsonpath can be used with other postgresql features. functional indexes support efficient querying of data through json functions, full-text queries enable searching within json text, and triggers can use sql/json path expressions. what are some common mistakes when using the sql/json path language? some common mistakes in sql/json path language usage include incorrect path expressions, improper handling of null values, forgetting to use jsonb_path_query_array when expecting multiple results, and not considering the implications of the lax or strict modes. how does sql/json path language compare to other json querying languages or libraries available in postgresql? the sql/json path language in postgresql and python jsonpath are both used for querying json data. the first relies on sql-like syntax, is integrated with postgresql, and represents an ideal solution for database-related json querying. the second uses xpath-like syntax and is suitable for standalone python apps. about the author antonello zanini is a software engineer, and often refers to himself as a technology bishop. his mission is to spread knowledge through writing. the table (252 part series) 1 optimizing queries for performance: mysql edition 2 json vs. jsonb in postgresql: a complete comparison ... 248 more parts... 3 architecting for security: mysql edition 4 the data you‚Äòve left behind ‚Äì an attacker‚Äòs perspective 5 how to deal with a database with billions of records 6 writing reusable sql queries for your application with dbvisualizer scripts 7 how to design and build a database in postgres 8 top five query tuning techniques for microsoft sql server 9 using postgresql to manage business inventory data and visualize it 10 sql triggers: what they are and how to use them 11 stored procedures in sql: a complete tutorial 12 ssh tunneling: the good, the bad, and the ugly 13 how to create a table like another table in mysql 14 firing database actions: a guide to using sql triggers with dbvisualizer 15 a comprehensive guide to data structures in sql 16 database transactions 101: the essential guide 17 the complete guide to sql subqueries 18 how to secure data in a postgres database: a guide 19 working with null in databases. turn your frustration into delight 20 acid in database systems: all you need to know 21 database security 101: best practices to secure your data 22 best practices for encrypting data in your database 23 getting started with multiversion concurrency control (mvcc) in postgresql 24 how to use a foreign key referring to the source table in postgres 25 blob data type: everything you can do with it 26 a guide to acid in mysql 27 preventing illicit uploads in mysql ‚Äì secure_file_priv 28 the ultimate guide to the mysql port 29 alter table explained 30 deadlocks in databases: a guide 31 a guide to subqueries in mysql 32 10x query performance with a database index 33 character sets vs. collations in a mysql database infrastructure 34 best practices for sql coding and development 35 a guide to multithreading in sql 36 er model, er diagram, and relational schema: what's the difference? 37 the ultimate guide to sql generated columns 38 mysql user management: a guide 39 why are your databases slow with large tables? an overview 40 solved: sudo mysql_secure_installation command not found in mariadb 41 how to connect to mysql using admin account remotely 42 how to list databases in postgresql 43 extracting time and date in ms sql server: a comprehensive guide 44 how to set up postgres using docker 45 understanding postgresql data integrity 46 parameterized queries in sql ‚Äì a guide 47 mysql substring function: the complete guide 48 creating custom visualizations with dbvisualizer 49 how to update a value incrementally (+1) in mysql 50 how to make your sql server more secure 51 how to pronounce sql: sql or sequel? 52 storing unstructured data with hstore in postgresql 53 the top four sites for database professionals seeking new job opportunities 54 migrating data between databases using dbvisualizer 55 archiving data in mysql using partitions 56 exploring the mysql daemon ‚Äì a guide 57 why do we call software bugs, bugs? 58 mastering advanced sql queries with dbvisualizer and postgresql 59 mysql‚Äôs utf-8: is it real? 60 insert queries - advanced crud explanation part 1 61 postgresql coalesce function: handling null values effectively 62 automating database operations with ansible and dbvisualizer 63 select queries - advanced crud explanation part 2 64 update queries - advanced crud explanation part 3 65 delete queries ‚Äì advanced crud explanation part 4 66 designing a multi-language database 67 preventing sql injection attacks with dbvisualizer 68 how to delete elasticsearch indices: an in-depth tutorial 69 a guide to data types in oracle 70 bridging the gap between sql and nosql in postgresql with json 71 exploring mongodb data structure with dbvisualizer's nested tree-table view 72 how to kill all connections to a database in postgresql 73 postgres on delete cascade - a guide 74 mastering the trino connection: unleash the power of dbvisualizer! 75 querying data with dbvisualizer and prestodb 76 postgresql jsonpath: dealing with sql/json path¬†language 77 deleting a column in sql: everything you need to know 78 postgresql isnull: the missing function 79 postgresql's string_agg() function - a comprehensive guide 80 describe table postgresql alternatives 81 comprehensive guide on like and fulltext search wildcards 82 understanding self joins in sql 83 oracle create database link 84 nulls are not the same ‚Äì a guide 85 using the explain plan to analyze query execution in postgresql 86 your database doesn‚Äôt like your data ‚Äì reserved words in mysql 87 google bigquery: a beginner‚Äôs guide 88 no-code ways of generating sql queries. 89 mysql operators ‚Äì a guide 90 discover dbvisualizer security features for mysql 91 working with numeric data types in mysql: a comprehensive guide 92 sql interview questions (mysql): part 1 93 how to work with sql query optimization 94 error establishing a database connection - common reasons and solutions 95 sql cheat sheet: a comprehensive guide to sql commands and queries 96 postgres list users: two different approaches 97 sql vs nosql databases: which is better? 98 change data capture: a comprehensive guide 99 mariadb vs mysql: the ultimate comparison 100 leveraging snowflake's time travel 101 inner join in sql: a comprehensive guide 102 postgresql materialized views: a beginner's guide 103 casting in postgresql: handling data type conversions effectively 104 date formatting in postgres: a comprehensive guide 105 view vs materialized view in databases: differences and use cases 106 sql deep dive: mastering datediff with code examples 107 automating databases with sql triggers 108 postgresql's on delete cascade feature 109 simplified overview of json vs. jsonb in postgresql 110 sql column deletion techniques 111 implementing self joins in sql: a guide for developers 112 postgresql - how to kill connections 113 oracle data types: an overview 114 er models, diagrams, and schemas: a developer's brief 115 introduction to the blob data type 116 quick guide to parameterized queries in sql 117 sql joins: easy guide for joining tables 118 a beginner's guide to database transactions 119 postgres in docker: a simplified setup process 120 resolving ""sudo mysql_secure_installation: command not found"" in mariadb 121 efficient date and time extraction in ms sql server 122 sql vs. sequel: navigating the pronunciation landscape 123 a quick guide to mysql port setup 124 date formatting in postgresql 125 quick mysql table creation: a developer's guide 126 maximizing sql server on macos: top clients for enhanced database management 127 simplify your development workflow: how to deploy postgresql using docker 128 understanding sql subqueries: a brief overview 129 a quick look at why we call them ""software bugs"" 130 postgresql and nosql capabilities with json and jsonb 131 postgresql for mysql users - alternatives to the describe statement 132 essentials of postgresql ctes 133 quick sql guide and cheat sheet: essential commands 134 exploring postgresql jsonpath for advanced json 135 how postgresql's coalesce function simplifies null value handling 136 database encryption: secure your data with best practices 137 database drivers: what you need to know 138 quick guide to postgresql's mvcc 139 handling nulls in postgresql: alternatives to isnull 140 how to use dbvisualizer for oracle database links 141 views vs. materialized views: what‚Äôs the difference? 142 efficiently managing unstructured data with postgresql hstore 143 best sql server clients for mac: simplify your database management 144 innodb's ibdata1: essential management tips for mysql 145 efficient ways to list postgresql databases 146 essential sql window functions for beginners 147 key data structures for sql database efficiency 148 leveraging postgresql cast for data type conversions 149 ssh tunneling: essential guide 150 postgresql backups simplified with pg_dump 151 mastering sql server: top 5 query tuning techniques 152 effective strategies for mysql user management 153 streamlining sql data management with generated columns 154 streamline mysql deployment with docker and dbvisualizer 155 essentials of acid in mysql 156 enhancing sql efficiency with multithreading 157 an introduction to sql stored procedures 158 building a postgresql database for e-commerce 159 using secure_file_priv to prevent illicit mysql uploads 160 utilizing mysql wildcards: like and fulltext 161 mastering sql: best practices for developers 162 how to handle databases with billions of records 163 simplified methods to list postgresql users 164 postgresql vs mysql: a concise comparison 165 mastering sql views: simplify and optimize your queries 166 optimizing data analysis with trino and dbvisualizer 167 essential sql commands for efficient database management 168 how to fix mysql shutdown unexpectedly error 169 efficiently analyzing postgresql queries with explain 170 essential guide to mysql subqueries 171 mariadb vs mysql: what developers need to know 172 acid in databases: a brief overview 173 postgresql distinct: an easy way to remove duplicates 174 maintaining data integrity in postgresql with constraints 175 sql server contains function: quick start guide 176 postgresql and dbvisualizer for better inventory management 177 simplifying database management with ansible and dbvisualizer 178 simple ways to enhance sql server security 179 optimizing postgresql queries with materialized views 180 navigating mongodb data structures with dbvisualizer 181 understanding sql coalesce for null handling 182 sql inner joins simplified 183 essential mysql operators and their applications 184 mysql daemon overview and tools 185 mysql numeric data types explained 186 quick guide to insert queries in mysql 187 boosting performance with select queries in crud 188 unlocking the power of update queries in databases 189 a brief guide to sql delete queries 190 quick guide to mysql math functions for sql users 191 simplify data migration between databases using dbvisualizer 192 database deadlocks: causes, examples, and solutions 193 sql interview prep for mysql: key questions to know 194 boost mysql performance with data partitioning 195 why large tables slow down your database 196 sql vs nosql databases: choosing the right tool 197 alter table: a quick overview for developers 198 avoiding errors with mysql reserved words 199 streamlining data auditing with snowflake time travel 200 change data capture: a beginner‚Äôs overview 201 avoiding the pitfalls of duplicate indexes in mysql 202 simplify distributed queries with presto and dbvisualizer 203 how to use substring_index in mysql 204 simplifying postgresql user management: two methods 205 how to quickly fix the ""error establishing a database connection"" 206 a practical guide to null in sql databases 207 advanced sql techniques for postgresql using dbvisualizer 208 secure your database: how dbvisualizer prevents sql injection 209 practical sql query optimization tips you should know 210 a beginner‚Äôs guide to mysql character sets and collations 211 mysql utf8 vs utf8mb4: which one should you use? 212 how database indexes improve sql query performance 213 mysql insert queries made simple 214 faster select queries: key tips for efficient database reads 215 quick guide to update queries: how to modify data efficiently 216 simplified guide to sql delete queries 217 database security made simple: essential practices 218 mastering mysql security: key steps to protect your database 219 simplify sql queries with coalesce 220 fixing mysql shutdown unexpectedly error 221 ace your sql interview with these query tips 222 getting started with sql distinct 223 substring_index in mysql: a simple guide to string extraction 224 mastering sql server contains for advanced text search 225 postgresql data types: an essential guide 226 sql datediff explained: syntax and database differences 227 pg_dump postgresql backups: a quick and practical guide 228 sql temporary tables: a quick and practical guide 229 handling big data in sql databases ‚Äì pitfalls to avoid 230 essential postgresql data types explained 231 three practical ways to create a database in mysql 232 how to use sql views to simplify data access and reporting 233 sql ddl commands explained: structuring your database 234 outer joins in sql: the key to keeping unmatched rows 235 postgresql case: build smarter queries with conditional logic 236 postgresql case: conditional logic for powerful sql queries 237 efficient string splitting in postgresql: 5 essential functions 238 how to start with database migrations 239 clean sql with comments: syntax and practices for all engines 240 database schema design: how to structure your app‚Äôs data effectively 241 schemas in postgresql: a practical guide for developers 242 alter table add column in sql: syntax, tips & tools 243 postgres text vs varchar: when to use which 244 why data visualization tools matter for developers, managers, and analysts 245 enhancing sql insert into performance: tips and tools 246 make your sql cleaner with common table expressions (ctes) 247 postgresql upsert: insert on conflict explained 248 learn sql‚Äôs cast function for clean type conversion 249 sql add to date: simple guide for mysql, postgresql, sql server & oracle 250 a developer‚Äôs guide to sql not in: smarter queries, faster results 251 understanding substring_index in sql: real use cases 252 essential sql commands for developers (with code samples) top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct ‚Ä¢ report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse dbvisualizer follow dbvisualizer is used for development, analytics, maintenance, and more, by database professionals all over the world. it connects to all popular databases and runs on win, macos & linux. location nacka, sweden joined jan 31, 2023 more from dbvisualizer postgres text vs varchar: when to use which # postgres # text # varchar pg_dump postgresql backups: a quick and practical guide # pgdump # postgres postgresql data types: an essential guide # datatype # postgres üíé dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community ‚Äî a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem ‚Äî the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community ¬© 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",17
https://medium.com/dbsql-sme-engineering/how-to-build-an-end-to-end-testing-pipeline-with-dbt-on-databricks-cb6e179e646c,,Unit Testing,"how to build an end-to-end testing pipeline with dbt on databricks | by databricks sql sme | dbsql sme engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in dbsql sme engineering ¬∑ publication for content from the dbsql sme group and the surrounding community on dbsql best practices, new design patterns, creative solutions, and real world user stories all in one place. how to build an end-to-end testing pipeline with dbt on databricks databricks sql sme 12 min read ¬∑ oct 15, 2024 -- 4 listen share data pipelines with embedded data quality & testing author: tobi sam , solutions architect @ databricks introduction in previous articles, the dbsql sme group has introduced how to perform basic performant etl on dbt for all things databricks ( here and here ). now we dive into the next stage: data quality & pipeline testing. data quality is essential in any analytics pipeline. this blog post outlines a robust approach to ensuring data integrity throughout your dbt workflow. we will explore a series of tests such as anomaly detection, unit tests, and data contracts that will help you maintain high-quality data from the source to the final output. databricks provides a unified platform for data processing and analytics that allows users to build, test, deploy, and monitor data products all in one place! we will leverage dbt (data build tool) ‚Äî an open-source command-line tool that helps analysts and engineers transform data in their warehouse more effectively ‚Äî to implement robust testing techniques to our data pipeline. we will use the medallion architecture, a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from bronze/raw ‚áí silver/cleaned ‚áí gold/serving layer tables). prerequisites & configurations dbt comes in two flavors dbt-core and dbt-cloud, and setting either one up is pretty straightforward. we will use dbt-core in this article. please follow the installation instructions here and you should have a dbt project enabled. databricks ensure you have a databricks sql warehouse created and ready to be used setup oauth or personal t oken your local machine on your local machine, create a virtual environment and install the dbt-databricks adapter clone this repository as we will use it throughout this article https://github.com/kamalendubiswas-db/dbt-dss-demo/tree/dev regarding the profiles.yml file , the best practice is to gitignore this file, as it relates to an individuals configuration and is created automatically when you set up dbt locally. this dbt yaml file lives in the .dbt/ directory of your user/home directory. update your profiles.yml file from the repo to point to the databricks sql warehouse you created above. now test that you can connect to your databricks sql warehouse from your terminal with dbt debug. if all goes well, you should see this output ‚Äúall checks passed!‚Äù if not, please troubleshoot using the error messages (i find them very helpful actually). configure your dbt project sync your dbt metadata with unity catalog to ensure that all your dbt table and column-level descriptions are synchronized with unity catalog, you can enable persist_docs in your dbt_projects.yml file. this really enriches the unity catalog view, adds rich lineage, and enhances the genai capabilities of the databricks platform. how to sync table metadata from dbt to unity catalog once you enable this, all your metadata is synchronized with unity catalog in databricks syncing metadata from dbt automatically to uc optimise your databricks compute resources in your profiles.yml file, you can increase the value of the ‚Äúthreads‚Äù configuration to increase your project‚Äôs parallelism and fully take advantage of compute concurrency on the databricks lakehouse. threads help parallelize node execution in the dbt‚Äôs (dag) . the default number of threads is currently 4, but there is no maximum, and dbt will allow you to go up to your databricks sql maximum limit. as a starting point, increase this number to 10 with a medium sql warehouse but check out this in-depth analysis for more details on the best combination. to see how databricks sql manages multiple queries, click here . testing your models in dbt now that we have databricks and dbt setup, we will break down our testing strategy into 3 phases (you can customise this based on your use case). validating the source data validating the ingested data validating the data transformations and output validate source data before diving into transformations, it‚Äôs crucial to verify the integrity of your source data. this step ensures that you‚Äôre working with fresh, accurate data from the start. freshness checks one of the first things to check is the freshness of your data. to check the freshness of your source data in your pipeline in dbt, you can use dbt‚Äôs freshness check block. a freshness block is defined within your models/sources.yml file and is used to define the acceptable amount of time between the most recent record in the source, and now, for a table to be considered ‚Äúfresh‚Äù. example record of freshness let‚Äôs assume that it is currently 15:00 on september 15th, 2024, and we expect one new row every day. we can configure the freshness block to throw an error if the most recent row is older than 1 day and warn us if we haven‚Äôt received a new row within the last 12 hours. freshness check on dbsql when we run dbt source freshness on the table, our check will fail as expected. freshness checks are very helpful as they notify you when your data becomes stale so you can address them before proceeding further in your pipeline. validate ingested data once you‚Äôve confirmed the source data‚Äôs integrity, the next step is to ensure that data is loaded correctly into your warehouse. validation at this stage ensures a solid foundation and eliminates a stage in debugging any subsequent issues. row count comparison one of the first things i like to do at this stage is to compare the rows of the source data to the bronze layer. note that this is only valid when doing a full data load or if the source data is never deleted. you could write custom sql to compare this but there are robust testing packages such as dbt-expectations that have a lot of pre-built tests ready to use. using the expect_table_row_count_to_equal_other_table test, all we have to do is specify the two tables to compare: dbt tests when you run dbt test dbt will pass if both tables have equal row counts or it would fail otherwise. easy! to install the dbt-expectations package, include this in your packages.yml file and run the following: dbt init installing expectations packages feel free to add other tests at this stage that may be relevant to your project. validate data transformations & output with clean source data and a successful load, it‚Äôs time to validate your transformations. this step ensures that your business logic is correctly applied and that the resulting data meets your quality standards. one-off data tests sometimes, you may have those scenarios which require a test that is very specific and cannot be easily reused. in dbt, these are called singular data tests . these are simply custom sql queries that check for specific data quality issues or business rules in your models. however, if you find yourself reusing your singular tests in multiple models, then it may be time to ‚Äúupgrade‚Äù them to generic data tests which we will come to shortly. let us create a simple singular test that asserts that there are no future order dates in the order table, as that would be strange. data quality tests create this file and save it in the tests directory. now run dbt test ‚Äîselect assert_no_future_order_dates to run this test. i included 99 records in the future and thus, my test failed as expected: dbt run with test fail reusable data tests now, when it comes to checking for nulls, uniqueness etc, these are tests you‚Äôd typically apply to several dbt models; and this brings us to generic data tests. these are tests in dbt that take in certain parameters and can be reused across all models across your project. think about them like a function in any programming language that expects a parameter and returns a result. this makes generic tests dry and easily reusable. dbt comes with 4 generic tests out of the box, but you can build your own custom generic tests. however, before you go build yours, check out these open-source packages ( dbt-utils and dbt-expectations which we looked at already) to see if the test you have in mind hasn‚Äôt already been created. in dbt, a good practice is to include test definitions directly in the schema.yml file alongside your model definitions, centralizing your data quality checks, making them easier to manage and maintain. below is an example of the 4 built-in generic tests being used for the orders model in a schema.yml file: in plain english, these data tests translate to: unique : the order_id column in the orders model should be unique not_null : the order_id column in the orders model should not contain null values accepted_values : the status column in the orders table should be one of ‚Äòplaced‚Äô, ‚Äòshipped‚Äô, ‚Äòcompleted‚Äô, or ‚Äòreturned‚Äô relationships : each customer_id in the orders model exists as an id in the customers table (also known as referential integrity ) enforcing referential integrity an important test to point out here is the ‚Äú relationships ‚Äù generic data test. in databricks, primary key and foreign key constraints are not enforced as they are only informational . if your application requires enforcing referential integrity, including this test in your pipeline will ensure that only datasets that meet this constraint are published into your schema. anomaly detection in time series data dbt-expectations has another handy test, that i call ‚Äú z-sigma ‚Äù. it‚Äôs a simple anomaly test based on the assumption that differences between periods in a given time series follow a log-normal distribution. this statistically measures a certain data point‚Äôs relationship to the mean of the entire dataset. try it out, very easy to use. constraints & data contracts dbt constraints apply additional validation on data as it is being inserted into a new or pre-existing table using databricks constraints . when enforced, a constraint guarantees that you will never see invalid data in the table materialized by your model. to enable constraints, you need to enable dbt contracts , which are configurations that guarantee the shape of a model while it is being built to avoid surprises or breaking changes for downstream queries. let‚Äôs take a look at an example of some constraints embedded within a model contract: dbt contracts you may notice that constraints are very similar to data tests, and in some cases, you can replace data tests with their equivalent constraint. data tests validate the content of your model after it is built while constraints check this at build time. see here and here for more details. at the moment with dbt-databricks, once you implement constraints, you get an error message; however, dbt only validates the data once it has already been inserted into the table, meaning that if the constraints and/or constraint_check fails, the table with the failing data will still exist in unity catalog/databricks sql. there is an open issue to update the implementation. unit tests unit tests examine the smallest testable parts of your models. we implement unit tests to validate specific transformations or calculations in our models, especially when there is complex logic. these can be written as singular tests or using dbt‚Äôs recently released unit test framework. let‚Äôs consider a scenario where you‚Äôre calculating customer lifetime value (clv) in an e-commerce dataset. this calculation involves multiple steps and business rules, making it an ideal candidate for unit testing. testable code logic in sql to ensure the clv calculation is correct, you can create unit tests where we pass our sample data (‚Äúgiven‚Äù) and then our expected output (‚Äúexpect‚Äù): inputs & expected outputs in dbt for unit testing of pipeline logic to run the test, run this command in your terminal: dbt test - select test_type:unit this unit test verifies that: the annual value calculation is correct: (1000/4) * 12 =3000 the lifetime value calculation is correct: (3000 *3) =9000 unit tests like this are invaluable when dealing with complex calculations, ensuring that your dbt models produce accurate and reliable results. since the inputs of unit tests are static, it is recommended that they be run only in development or ci environments. additional tips to enhance your testing strategy, consider implementing these additional features: test severity another feature that i particularly like is test severity. this allows you to configure your tests to return a warning instead of an error. generally, if your test query returns at least one row, this will return an error. i recently tackled a project involving an important model named ‚Äúopportunity‚Äù with a crucial field called ‚Äúcustomer_id.‚Äù naturally, it was important for all opportunities to be linked to a customer_id. unfortunately, the sales team overlooked the customer_id for 33 opportunity records. despite this, i still wanted dbt to run while ensuring the integrity of this field. to achieve this, i configured a test to ‚Äúwarn‚Äù me if there were any failing rows, and ‚Äúfail‚Äù when the records exceeded 33, as this would suggest the identification of new records that were not initially taken into account. below is a sample of what the configuration would look like: here is a simple breakdown: not_null : a generic not_null test where : in the test configuration, you can include a filter to apply the tests to. in this case, it was only important that ‚Äúclosed won‚Äù opportunities had a customer id assigned to them. config : with severity set to error (which is the default), dbt will check the error_if condition first. if the error condition is met, the test will return and error, if not, it checks the warn_if condition and ‚Äúwarns‚Äù if that condition is met, or else, the test passes if neither the ‚Äúerror‚Äù nor the ‚Äúwarn‚Äù conditions are met. to run the test in your environment, simply run dbt test in your terminal. we will see how you can add alerts using databricks workflows in the monitoring and alerting in the next part of this article. failing fast sometimes, during development, if there is a failure during your build or test, you may want to exit dbt immediately instead of waiting for all the models to complete. this will help you save some time and money on your warehouse especially when you have a lot of models. dbt has a little-known flag called ‚Äî fail-fast or -x which immediately exits dbt if any resource fails to build. you can find out more about it here . conclusion we looked at many approaches to implementing data quality checks in your data pipeline in databricks. remember that data quality is not a one-time effort but an ongoing process. it is essential to regularly review and update your tests as your data models evolve. in summary, you should now be familiar with the following: how to set up a basic dbt project for databricks including a rule of thumb for compute sizing, how to implement unit tests and custom one-off tests on dbt models, how to validate data and perform freshness checks of sources, how to implement constraints and data contracts to prevent models from being populated with invalid data, the importance of testing at each stage of your data lifecycle strategy to isolate errors and identify if they are occurring at source, during ingestion or during transformation, and the benefits of leveraging persist_docs to push metadata from your dbt models directly into unity catalog for data discovery. in the next article, we will look at how to automate monitoring and alerting using databricks workflows and databricks sql in a ci/cd pipeline. databricks sql databricks dbt data quality data mesh -- -- 4 published in dbsql sme engineering 1.1k followers ¬∑ last published 3 days ago publication for content from the dbsql sme group and the surrounding community on dbsql best practices, new design patterns, creative solutions, and real world user stories all in one place. written by databricks sql sme 3.6k followers ¬∑ 42 following one stop shop for all technical how-tos, demos, and best practices for building on databricks sql responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@wyaddow/maintain-data-quality-with-data-refactoring-tests-f46580d0b43e,,Regression Testing,"maintain data quality with data refactoring tests | by wayne yaddow | medium sitemap open in app sign up sign in medium logo write sign up sign in maintain data quality with data refactoring tests wayne yaddow 5 min read ¬∑ apr 3, 2024 -- listen share ensuring data excellence through data model modifications, synchronizing storage, and rigorous testing. a recent article on dzone.com subtly reminds readers of the need to test all types of data refactoring thoroughly. data refactoring is a process by which the structure of an existing database or dataset is changed ‚Äî typically without altering its meaning or content. this process is akin to code refactoring in software development, which aims to improve the code‚Äôs structure and readability without changing its external behavior. refactoring is designed to make data more organized, accessible, and easier to work with, thus improving the efficiency and reliability of data management and data-driven applications. data refactoring is often complex due to the frequent impact of interconnected data structures and processes. what can be affected when refactoring data? the results of data refactoring can affect various areas of the data architecture, data management practices, and the data itself. therefore, tests should be considered to ensure that refactoring works as required and that no troublesome effects have emerged. 1) data architecture schema design : refactoring can lead to changes in the database schema, such as table structures, relationships, and indexes. this can improve query performance, data integrity, and ease of data access. data storage : changes in how data is structured may influence the choice of storage technology (e.g., relational databases, nosql databases, data lakes) and how data are distributed across these storage solutions. data integration : refactoring can necessitate updates to data integration pipelines and etl processes to accommodate new data structures or to optimize data flows. 2) data management refactoring can impact data governance policies and procedures, particularly data quality, privacy, and regulatory compliance. adjustments may be needed to ensure that the refactored data meets all regulatory and internal standards. data quality management : improved data structures can enhance data quality by reducing redundancy, improving consistency, and simplifying data cleansing and validation processes. performance management : refactoring can significantly impact the performance of data queries and analyses by optimizing the organization and structure of data, potentially requiring updates to performance monitoring and optimization strategies. 3) the data itself data accessibility : refactoring can enhance data accessibility by making it easier for users to understand, query, and extract insights from the data. this can lead to more effective data-driven decision-making across the organization. data security : changes to data structures may require updates to security measures and access controls to adequately protect sensitive data. data usage : as the ease of use and performance of the data improves, it may lead to broader or more practical uses of the data, potentially unlocking new opportunities for analysis, insights, and data-driven products and services. data refactoring is critical to maintaining an agile, efficient, scalable data environment. by thoughtfully addressing how data is structured, stored, and managed, organizations can ensure their data architecture remains robust and responsive to changing business needs and technological advancements. common challenges and mitigations when trying to test complex data refactoring complex data refactoring entails substantial changes that can impact the data‚Äôs structure, relationships, or integrity on a broader scale than minor refactoring activities. here are two instances of intricate data restructuring and methods to assess each to maintain data quality and integrity. *** integrating multiple data sources into a single system integrating multiple data sources into a single system is a complex refactoring task that involves unifying data from various origins into a coherent database or data warehouse. this process is critical for organizations looking to aggregate and analyze data across different systems for comprehensive insights. it addresses challenges like data format inconsistencies, duplication, and the reconciliation of different data models. the goal is to create a unified view that supports better decision-making and reporting. testing such integrations involves ensuring data consistency and integrity and that the merged data provides accurate, actionable insights without losing critical information from the sources. testing challenges: heterogeneity of data sources : the diversity in formats, standards, and data quality across sources complicates validation and transformation processes. ensuring consistency and integrity across such varied data sets requires sophisticated mapping and transformation logic, which can be challenging to test comprehensively. data quality issues : inherent inconsistencies, duplicates, and errors in the original data sources can propagate through to the integrated system unless identified and corrected early, requiring extensive data cleaning and quality checks that can be time-consuming and complex. testing scenarios: data validation : implement comprehensive validation checks to ensure that data from all sources conforms to unified format, type, and scale specifications. this could involve checking that date formats are standardized across all integrated data and that numeric data uses the same scale and precision. data profiling : perform extensive data profiling to identify and resolve discrepancies in data distribution, such as unexpected nulls or outliers, ensuring that data integration hasn‚Äôt introduced anomalies. regression testing : to ensure accuracy and completeness, develop and run tests on common queries and reports spanning newly integrated data sources, comparing the results with those obtained from isolated sources. *** transforming complex data etls complex etl transformations for data warehousing involve intricate processes of extracting data from various sources, transforming it to fit operational needs, and loading it into a data warehouse. this process is fundamental in building a data warehouse that consolidates diverse data into a unified format, making it readily available for analysis and decision-making. complex transformations might include data cleansing, deduplication, integration, and aggregation to ensure high quality and relevance. testing these transformations requires a thorough examination of data flow from source to destination, validation of transformation logic, and ensuring that the data loaded into the warehouse accurately reflects the source data while meeting the informational needs of the business. testing challenges: ¬∑ transformation logic complexity : the complexity of etl transformation logic, especially when involving conditional processing, data cleansing, and aggregation from multiple sources, makes it challenging to ensure accuracy. testing must cover many scenarios and edge cases, requiring extensive test cases and validation rules. ¬∑ end-to-end process reliability : ensuring the reliability and efficiency of the entire etl process requires comprehensive end-to-end testing, which can be challenging to orchestrate, especially in dynamic environments where source data characteristics may change over time. testing scenarios: transformation logic testing : execute detailed unit tests on each transformation rule or function to ensure it correctly processes the input data and produces the expected output. this might involve testing individual functions for data cleansing (e.g., removing duplicates, standardizing formats) and aggregation (e.g., summing sales data by region). end-to-end etl testing : conduct comprehensive tests that run the full etl process on a subset of production data to validate the end-to-end data flow, transformations, and loading. this helps identify any integration issues or bottlenecks in the etl pipeline. data integrity testing : after etl execution, integrity checks are performed on the data warehouse to ensure that all expected data has been accurately transferred and transformed and that referential integrity is maintained across tables. complex refactoring requires testing to verify correct execution and prevent adverse effects on data quality, performance, and usability. these testing procedures are crucial for reducing risks related to significant changes in data design and management practices. #dataobservability #datapipeline #datapipelinequality #datatesting #dataengineertesting data pipeline test big data data engineer skills etl testing data testing -- -- written by wayne yaddow 117 followers ¬∑ 250 following senior data quality analyst. wyaddow@gmail.com no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@sutherl99/implementing-a-shift-left-data-quality-process-in-azure-3f67586999a5,,,"implementing a shift-left data quality process in azure | by dan sutherland | medium sitemap open in app sign up sign in medium logo write sign up sign in implementing a shift-left data quality process in azure dan sutherland 6 min read ¬∑ oct 21, 2024 -- listen share data quality is related to the accuracy with which the data reflects reality. an organization‚Äôs actions, if based on a ‚Äúflawed reality,‚Äù may create costly mistakes for themselves. organizations need to recognize that not all data is relevant and assess what data is critical to their operations. focusing on this ‚Äúcritical‚Äù data allows an organization to assess the quality of its data without overwhelming the organization. shift left meaning from a data quality perspective, shift left means that data is captured and validated at the source before publication to a shared enterprise environment like an enterprise data platform, data lakehouse, data warehouse or to any other data consumer. shift left data quality principles in a previous article , i laid out the case for defining clear standards and ownership for data but then executing on those standards and policies as part of your data architecture thus enabling your data for greater business value. to assist in this effort, there are a small set of guiding principles to guide you towards that goal. shift left use case to help make this concept real, a use case was developed to illustrate what this could look like in ‚Äúreal‚Äù life using azure. the use case is to design a data quality approach using a shift left architecture and philosophy. all incoming data from data producers would be checked and validated, and any warnings/errors would be identified and either the data would be passed through with warnings or rejected and sent to an exception queue for remediation. process flow following is a process flow diagram depicting the shift-left data engineering process in azure. azure services used: azure devops, azure data factory mapping flows, azure data factory, azure databricks, azure logic apps, azure monitor, azure log analytics approach 1 ‚Äî define a data contract with the data content and rules for each data provider providing data . this aims to ensure clear agreements on data definitions, quality, and usage guidelines between data producer and data consumer. you can use azure devops or a dedicated wiki system to store and manage these contracts. look at including the following components: schema: detailed descriptions of each data field, including data types, allowed values, and units of measure. data quality metrics: define acceptable thresholds for data quality dimensions like completeness, accuracy, consistency, and timeliness. data quality checks: review requirements and incorporate the following checks, if applicable: a) data type validation: ensure data types match the defined contract. b) range checks: verify numeric values fall within acceptable ranges. c) pattern matching: validate string data using regular expressions. d) cross-field validation: check for consistency across related data fields. e) referential integrity: ensure relationships between data entities are valid. slas: service level agreements outlining expectations for data delivery, processing time, and issue resolution. 2 ‚Äî rules are ingested from the data contract (azure devops or wiki) into azure data factory (adf) mapping data flows or azure databricks with either serving as the configurable rules engine and visual authoring tool for business rules. azure data factory (adf) mapping data flows: adf offers a visual data flow designer with built-in transformations for data validation and rule enforcement. you can define rules based on your data contract directly within the data flow. azure databricks: for more complex rules or custom logic, you can leverage azure databricks with its support for python, scala, and sql. this allows you to write sophisticated validation code and integrate with other azure services. ensure that the following features are used: versioning: track changes to rules and easily revert to previous versions. testing and debugging: provide robust tools for testing and debugging rules before deployment. integration with monitoring: seamlessly integrate with your monitoring tools for real-time feedback on rule performance. 3 ‚Äî validate/test/deploy data pipeline to azure data factory. test and validate the pipelines to ensure they adhere to the defined data and business rules defined in the data contract and identify and address any errors or inconsistencies. deploy the rules to azure data factory. adf provides connectors to a wide variety of data sources, including azure blob storage, azure data lake storage, azure sql database, and on-premises databases. adf is the primary service for orchestrating and executing data pipelines in azure. you can schedule pipelines, monitor their execution, and manage dependencies. for orchestrating complex data pipelines with multiple steps and error handling use azure logic apps . 4 ‚Äî data validation and rule enforcement azure data factory executes the pipeline, applying the defined business rules during data transformation and loading. if data violates any of the defined rules, the pipeline will be configured to handle errors or failures by throwing an exception or warning and having it directed to the appropriate data steward/owner for remediation. 5 ‚Äî exception management framework use azure data factory‚Äôs error handling to catch exceptions during pipeline execution or create custom error handling logic within your adf pipeline using python or other supported languages. categorize exceptions based on severity (e.g., critical, warning, informational). you can configure error handling at the activity level and define custom error handling logic. use azure monitor to log detailed information about exceptions, including timestamp, error message, data context, rule that was violated, etc. based on this, create alerts based on specific conditions and visualize data quality trends. send exception notifications to relevant stakeholders based on exception severity utilizing tools like azure logic apps or email services. define clear escalation paths for unresolved exceptions. explore options for automating the remediation of common exceptions. 6 ‚Äî continuous monitoring & logging use azure monitor monitoring capabilities to ensure data quality, track job performance and identify bottlenecks. create data quality dashboards to visualize key data quality metrics and track trends over time. set up alerts for critical data quality issues and pipeline failures. monitor pipeline performance to identify bottlenecks and optimize resource utilization. use azure log analytics to analyze log data, create dashboards, and gain insights into data quality trends. conclusion leveraging a shift left data quality approach will bring the following benefits: early issue detection: identify data quality issues early in the pipeline, reducing the cost and effort of remediation. improved data quality: prevent bad data from propagating downstream, ensuring higher quality data for analysis and decision-making. increased efficiency: automate data quality checks and exception handling, freeing up data engineers to focus on higher-value tasks. reduced risk: minimize the risk of data-related errors impacting business operations. by incorporating these enhancements, you can create a robust and effective shift left data quality engineering process that ensures high quality data and streamlines your data pipelines. if you have any feedback, questions and/or comments please leave a comment or message me on linkedin . dan is a former ibm distinguished engineer and is currently the cto for protiviti‚Äôs enterprise data & analytics practice . connect with me on linkedin and share your thoughts on your data-driven journey. disclaimer: this blog represents my own personal view and is not related to my current employer or past employers. data quality azure data contract data governance data engineering -- -- written by dan sutherland 254 followers ¬∑ 237 following dan is senior director of technology at protiviti. he advises fortune 1000 clients on data and cloud data management strategies and modern data architectures. no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@sutherl99/designing-a-shift-left-data-engineering-process-in-gcp-e8eac5fe9a1f,,,"implementing a shift-left data quality process in gcp | by dan sutherland | medium sitemap open in app sign up sign in medium logo write sign up sign in implementing a shift-left data quality process in gcp dan sutherland 5 min read ¬∑ oct 17, 2024 -- listen share data quality is related to the accuracy with which the data reflects reality. an organization‚Äôs actions, if based on a ‚Äúflawed reality,‚Äù may create costly mistakes for themselves. organizations need to recognize that not all data is relevant and assess what data is critical to their operations. focusing on this ‚Äúcritical‚Äù data allows an organization to assess the quality of its data without overwhelming the organization. shift left meaning from a data quality perspective, shift left means that data is captured and validated at the source before publication to a shared enterprise environment like an enterprise data platform, data lakehouse, data warehouse or to any other data consumer. shift left data quality principles in a previous article , i laid out the case for defining clear standards and ownership for data but then executing on those standards and policies as part of your data architecture thus enabling your data for greater business value. to assist in this effort, there are a small set of guiding principles to guide you towards that goal. shift left use case to help make this concept real, a use case was developed to illustrate what this could look like in ‚Äúreal‚Äù life using google cloud platform (gcp). the use case is to design a data quality approach using a shift left architecture and philosophy. all incoming data from data producers would be checked and validated, and any warnings/errors would be identified and either the data would be passed through with warnings or rejected and sent to an exception queue for remediation. process flow following is a process flow diagram depicting the shift-left data engineering process in gcp. gcp services used: cloud data fusion, dataflow, cloud pub/sub, cloud functions, cloud logging approach 1 ‚Äî define a data contract with the data content and rules for each data provider providing data . this aims to ensure clear agreements on data definitions, quality, and usage guidelines between data producer and data consumer. look at including the following components: schema: detailed descriptions of each data field, including data types, allowed values, and units of measure. data quality metrics: define acceptable thresholds for data quality dimensions like completeness, accuracy, consistency, and timeliness. data quality checks: review requirements and incorporate the following checks, if applicable: a) data type validation: ensure data types match the defined contract. b) range checks: verify numeric values fall within acceptable ranges. c) pattern matching: validate string data using regular expressions. d) cross-field validation: check for consistency across related data fields. e) referential integrity: ensure relationships between data entities are valid. slas: service level agreements outlining expectations for data delivery, processing time, and issue resolution. 2 ‚Äî data is ingested from the data contract into cloud data fusion. also, cloud data fusion serves as the configurable rules engine and visual authoring tool for business rules. ensure that the following cloud data fusion features are used: versioning: track changes to rules and easily revert to previous versions. testing and debugging: provide robust tools for testing and debugging rules before deployment. integration with monitoring: seamlessly integrate with your monitoring tools for real-time feedback on rule performance. 3 ‚Äî validate/test/deploy data pipeline to dataflow. test and validate the pipelines to ensure they adhere to the defined data and business rules defined in the data contract and identify and address any errors or inconsistencies. export the pipeline from cloud data fusion as a dataflow template. deploy the template to dataflow , creating a dataflow job. 4 ‚Äî data validation and rule enforcement dataflow executes the pipeline, applying the defined business rules during data transformation and loading. if data violates any of the defined rules, the pipeline will be configured to handle errors or failures by throwing an exception or warning and having it directed to the appropriate data steward/owner for remediation. 5 ‚Äî exception management framework use dataflow‚Äôs built-in error handling to catch exceptions during pipeline execution or create custom error handling logic within your dataflow pipeline using python or other supported languages. categorize exceptions based on severity (e.g., critical, warning, informational). log detailed information about exceptions, including timestamp, error message, data context, rule that was violated, etc. send exception notifications to relevant stakeholders based on exception severity utilizing tools like cloud pub/sub, cloud functions, or email services. define clear escalation paths for unresolved exceptions. explore options for automating the remediation of common exceptions. 6 ‚Äî continuous monitoring & logging use dataflow‚Äôs monitoring capabilities to ensure data quality, track job performance and identify bottlenecks. create data quality dashboards to visualize key data quality metrics and track trends over time. set up alerts for critical data quality issues and pipeline failures. monitor pipeline performance to identify bottlenecks and optimize resource utilization. use cloud logging for centralized logging. conclusion leveraging a shift left data quality approach will bring the following benefits: early issue detection: identify data quality issues early in the pipeline, reducing the cost and effort of remediation. improved data quality: prevent bad data from propagating downstream, ensuring higher quality data for analysis and decision-making. increased efficiency: automate data quality checks and exception handling, freeing up data engineers to focus on higher-value tasks. reduced risk: minimize the risk of data-related errors impacting business operations. by incorporating these enhancements, you can create a robust and effective shift left data quality engineering process that ensures high quality data and streamlines your data pipelines. if you have any feedback, questions and/or comments please leave a comment or message me on linkedin . dan is a former ibm distinguished engineer and is currently the cto for protiviti‚Äôs enterprise data & analytics practice . connect with me on linkedin and share your thoughts on your data-driven journey. disclaimer: this blog represents my own personal view and is not related to my current employer or past employers. data quality gcp data contract data engineering data governance -- -- written by dan sutherland 254 followers ¬∑ 237 following dan is senior director of technology at protiviti. he advises fortune 1000 clients on data and cloud data management strategies and modern data architectures. no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@massimocapobianco/dbt-tests-for-null-and-unexpected-values-from-basic-to-advanced-cfebd8b6c184,,,"dbt tests for null and unexpected values: from basic to advanced | by massimo capobianco | medium sitemap open in app sign up sign in medium logo write sign up sign in dbt tests for null and unexpected values: from basic to advanced massimo capobianco 7 min read ¬∑ feb 29, 2024 -- listen share automated tests are a crucial aspect of any dbt project, and they can help ensuring the accuracy and consistency of your models. this article will explore the implementation of dbt tests, specifically in the context of null or unexpected values tests, by going from a very basic setup to a more advanced level. not_null tests having null values in table ‚Äã‚Äãcan cause unexpected results when computing or aggregating your data. similarly, if you filter data based on a column with a null value, the results may be surprising. ultimately if your dataset contains missing or unknown data, it can be difficult to get accurate conclusions from your analysis. so let‚Äôs start by looking at the simplest way to test for null values in a model built in dbt. dbt comes with a bunch of built-in tests (see documentation ), and one of them is in fact the not_null test. in order to set this up, we need to add the test in the relevant .yml file of the model we want to test. version: 2 models: - name: stg_customers description: this table contains customer identification and dimensions columns: - name: customer_id tests: - not_null as tests are simply sql queries, when setting up the not_null test the code running in the background will be: select customer_id from stg_customers where customer_id is null; the test is going to fail, if this query will return any record. depending on the type of table and the model layer you are testing (more about model layers here ), we might want to add some clauses to our tests. let‚Äôs imagine for example that we are testing a refinement model where we know there is an issue with a portion of the source data: still, we want to ensure that the not_null test runs for the rest of the data in that model, and we don‚Äôt want to be misled by the test failure caused by the expected missing data. this can be achieved through adding a where in the test configuration. - name: customer_id tests: - not_null: config: where: ""country not in ('us', 'canada')"" the query running in the background will then simply include a subquery to first select the data set that we want to test against: select customer_id from (select * from stg_customers where country not in ('us', 'canada')) dbt_subquery where customer_id is null; once again, the test will fail if the query returns any record given the set conditions. although it‚Äôs possible to configure a warn test severity , and avoid returning errors in case of test failures, this approach can results into a less attentive or reacting behaviour to test results and eventually can lead in my opinion to a loss of control on the data quality of your models. test severity can also be set so that the task will return an error or a warn depending on a specific amount of records (null records in this case). however again, this might not always be an optimal setup, as it implies that we expect that given threshold to stay the same while the tables count increases. that‚Äôs where we want instead to set up a not_null_proportion test. not_null_proportion test as the quality of data source tables can often be far from optimal, and there can be expected issues at different levels, we want to make sure that we establish thresholds and slas on our testing strategy. considering what we mentioned before in terms of test severity , a good approach to test for null values is to use a not_null_proportion test. this test is not available as built-in feature in dbt, but requires the installation of the dbt package called dbt_utils . at this point, this is a must-have package in your project, so we recommend to go through the very simple installation steps documented here (make sure you install the package version compatible with your dbt version) to be able to use this test. this is how the configuration for the not_null_proportion test will look like in the model .yml file. it will return an error if the count of null records for customer_city represents more than the 1 % of the entire table count. version: 2 models: - name: stg_customers description: this table contains customer identification and dimensions columns: - name: customer_city tests: - dbt_utils.not_null_proportion: at_least: 0.99 the compiled sql query will then be: with validation as ( select sum(case when customer_city is null then 0 else 1 end) / cast(count(*) as numeric) as not_null_proportion from stg ), validation_errors as ( select not_null_proportion from validation where not_null_proportion < 0.99 or not_null_proportion > 1 ) select * from validation_errors just like for the simple not_null test, here as well we can use the where clause to add a specific condition to our test. in the .yml we then add the following configuration: - name: customer_city tests: - dbt_utils.not_null_proportion: at_least: 0.99 config: where: ""country not in ('us', 'canada')"" and the query that runs is then going to be very similar, with the addition of the subquery in the first cte. with validation as ( select sum(case when customer_city is null then 0 else 1 end) / cast(count(*) as numeric) as not_null_proportion from (select * from stg_customers where country not in ('us', 'canada')) dbt_subquery ), validation_errors as ( select not_null_proportion from validation where not_null_proportion < 0.99 or not_null_proportion > 1 ) select * from validation_errors by setting this proportional test we can be notified with an error whenever our thresholds are hit and an investigation is needed. so while on one hand we still keep control of our data quality and we can take actions to address the issue to the relevant data producers and/or inform the consumers, on the other hand we make sure that we are not getting over-notified by minor issues or expected table behaviours. testing for unexpected values while null values can be a common issue across any table, we still want to ensure that a set of domain-specific or table-specific rules are respected across the data models in our dbt project. once again we can have two strategies here and either test for the presence of any record in a column that doesn‚Äôt meet a given condition, or implement a proportional configuration. for the first use case, we can stick with the dbt_utils package and set up an expression_is_true test. let‚Äôs see an example on a time field that represent the duration of an order delivery, which in this case we want to ensure that it‚Äôs never negative or equal to zero: version: 2 models: - name: ref_orders description: this table contains all data related to order shipping and delivery columns: - name: delivery_time tests: - dbt_utils.expression_is_true: expression: "" > 0"" in the background we have this code running: with meet_condition as ( select * from ref_orders where 1=1 ) select * from meet_condition where not(delivery_time  > 0) when at least one value in the column violates the given rule, we will get back an error as the test failed. if instead we want to opt for a proportional test, and therefore return an error only if the amount of records that don‚Äôt meet a given rule are crossing our slas, then we need to implement our own generic test. note: dbt offers the possibility of setting up singular or generic tests. singular tests are only usable for individual tables and use cases, so in the interest of code reusability we want to work with generic tests to build a query that can be adapted to different models. generic tests live in the tests folder in your project and make use of the test block. through the addition of arguments such {{ model }} , {{column_name}} , etc. we can re-use this test across any model in our project and set any rule that is relevant for a specific table. the code for our generic test can then look as follows, where the arguments are first listed into the {% test %} block and then added in the query that will be executed. {% test acceptance_percentage_threshold(model, column_name, rule, failing_perc) %} with test_target as ( select count(case when {{ column_name }} {{ rule }} then {{ column_name }} end) / count(*) as percentage from {{ model }} ) select * from test_target where percentage > {{ failing_perc}} -- acceptance ratio to give us a failure {% endtest %} in order to have this running we need then to set this up in the relevant .yml file and specify the values for the arguments. - name: delivery_time tests: - acceptance_percentage_threshold: rule:  < 0 failing_perc: 0.02 the compiled sql code will then be: with test_target as ( select count(case when delivery_time < 0 then delivery_time end) / count(*) as percentage from ref_orders ) select * from test_target where percentage > 0.02 an error will be triggered only if the results of the percentage computation is crossing the given threshold. in conclusion, implementing tests for null and unexpected values is an essential aspect of any dbt project. by following the steps outlined in this article, we can ensure the accuracy and consistency of our models, establish thresholds and slas on the testing strategy, and take action when the quality agreements are compromised. it is important to use the right test for the right model, preserve code reusability, and stay vigilant about data quality. with these best practices in mind, we can build robust and reliable data pipelines that will help making informed decisions and driving business success. dbt data quality data testing data analytics data governance -- -- written by massimo capobianco 20 followers ¬∑ 10 following analytics engineer / data analyst based in berlin. no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/agoda-engineering/how-to-design-maintain-a-high-performing-data-pipeline-63b1603b8e4a,,,"how to design and maintain a high-performing data pipeline | by agoda engineering | agoda engineering & design | medium sitemap open in app sign up sign in medium logo write sign up sign in agoda engineering & design ¬∑ learn about how products are developed at agoda, what is being done under the hood, from engineering to design, to provide users a seamless experience at agoda.com . how to design and maintain a high-performing data pipeline data pipeline best practices agoda engineering 11 min read ¬∑ sep 27, 2023 -- 2 listen share by lalit ratanapusdeekul data pipelines are essential for managing data flow from various sources to a target destination. the bi-infra-ops team at agoda has presented a comprehensive guide on best practices for designing, monitoring, and ensuring the quality of data pipelines. what is a data pipeline data pipelines are processes where data is extracted from the source, single or multiple, transformed into a suitable format, and loaded into a target, mostly a table. designing a healthy data pipeline when creating a data pipeline, one of the most important things that should be considered is how to create a healthy one. a healthy data pipeline is critical for ensuring we can use its data effectively for strategic decision-making and operational efficiency. if our data pipelines are unhealthy, the data will not be able to be delivered on time or even cause data issues for downstream users. designing a data pipeline properly could also make it reliable and minimize data loss or system downtime. it should have error handling, data recovery, and monitoring mechanisms to detect and address issues promptly. when our data pipelines are well designed and optimized, the data will be processed much faster and cost-effectively, significantly reducing the time for data to be ready. moreover, as our data requirements change fast, our data pipelines should also be flexible and adaptable to these changing requirements. new data sources, new data formats, and new data schemas should be accommodated with little effort. a well-designed data pipeline can scale to handle increasing data volumes and sources without significant performance degradation. this scalability is crucial as organizations grow and generate more data. our data grows as we grow. our data pipeline should be able to scale and handle continuously increasing data volumes without decreasing performance. as a result, several factors must be considered to have a healthy data pipeline. this includes: data : where is the data stored? what is the data behavior? resource used : how much resources should be allocated for our data pipeline? partitioning : how should we partition our table (in hadoop)? job scheduling : how frequently should our data pipeline run? data dependency : does the pipeline depend on the data of other tables? we will go through each factor individually and give some generic examples of how these factors affect how we design our data pipeline at agoda. resources must be allocated appropriately to ensure that the pipeline runs efficiently. partitioning can improve write performance and decrease run time. job scheduling must take into account the data usage, sla, job duration, and freshness of the data source. data dependency must be considered when one job needs another job to finish first before starting. 1. data the data source and data behavior determine how the data will be processed. the data source refers to where the source of data is stored (e.g., in mssql or hadoop), as well as where the data will be stored. the data behavior could be whether the data changes over time or what the granularity (level of detail) of the data is. in one of our data pipelines at agoda, we divide the process into three sub-processes based on the characteristics of the data we are working with. this data pipeline loads data into a fact table in our data warehouse. this fact table is called the fact booking table. this fact table stores data about bookings at a booking level. our design is based on the star schema method. the three sub-processes include: original column process a. this processes column that has no change (static) b. implementation: snap the value since the booking was created (ignore change) c. source : mssql + hadoop current column process a. this processes column that can be updated over time (dynamic) b. implementation : all values can be changed if updated in source c. source : mssql + hadoop flatten process a. update dimensional columns b. implementation : get value by joining with dimension tables c. source : hadoop each sub-process is suitable for different types of data and needs to be implemented differently. this shows why understanding the data is crucial in designing a pipeline. 2. resource used the resource used refers to the amount of resources used for processing our data in a data pipeline. the amount of resources allocated for our data pipeline is another factor that should be able to be specified. if too little resources are allocated - pipeline takes too much time to finish - risk of pipeline failure due to not enough resources if too many resources are allocated - negatively affect other pipelines - waste of resources at agoda, most of our data pipeline runs on apache spark. allocating resources to apache spark applications is essential for ensuring efficient and optimal performance of the data pipelines. spark allows you to set resource configurations for individual spark jobs or stages within jobs to fine-tune resource allocation. you could use apache yarn, or kubernetes to help manage and allocate resources to spark applications. back to our fact booking table. apart from data, we also separate the processes in our pipeline based on how much data is consumed in each round. hence, resources are allocated to each process based on data volume. processes that consume higher data volume are run less often to avoid consuming too much resource too frequently. in spark, you can adjust several things such as the driver memory and number executors that best suit your pipeline. for example, a process that consumes a high volume of data tends to require more driver memory and executors. at agoda, we also have dashboards made by our engineering team to track several spark metrics used by our pipelines. this dashboard can be used to monitor our spark applications. 3. partitioning in hadoop, partitioning refers to the process of dividing and organizing data into smaller, more manageable subsets called partitions. we mostly partition our tables based on particular columns in our tables. partitioning the table properly can improve the writing performance and decrease the run time of your data pipeline. these are some factors that could be used to identify the table partition. size and number of partitions a. the size and number of partitions should not be too much or too little. distribution of data a. we want the data to be distributed equally to all the partitions. back to our fact booking table. in the fact booking table, we use datamonth , which is the month of the booking date, as a partitioning column. why month of the booking date? - the number of bookings is distributed equally across months of booking date. why month, not day level? - if we use the day of the booking date, the size of the partitions would be too small, and there would be too much number of partitions. in reality, it‚Äôs often more practical to start with a reasonable partition size based on an initial analysis and then fine-tune it through performance testing. measure the impact of different partition sizes on query performance and resource utilization to find the optimal balance for each use case. 4. job scheduling in normal circumstances, we would not manually submit and run our pipelines. job scheduling is the way that we automate our data pipeline. with job scheduling, we can determine in advance when and how frequently our pipeline should run. the main consideration when setting up a job scheduling pipeline is the frequency of your job schedule. the frequency of the job scheduling pipeline can depend on: data usage - if data is used once a day (e.g., for a daily report), we could schedule the job to run once a day. - if data is used heavily, we could schedule the job to run more frequently. sla of data - less sla time means that the pipeline should be run more frequently pipeline duration - how much time is needed to run a single round of job freshness of data source - if source data is updated once a day, we do not have to schedule the job so frequently. 5. data dependency sometimes, your data pipeline might rely on a data source that is loaded by another upstream pipeline. this means that there is a data dependency going on in your pipeline. data dependency should also be taken into account when designing a data pipeline. here is an example of why data dependency is crucial when designing your data pipeline. example use cases: pipeline b loads data into table b (runs daily at 10:30 am) pipeline b loads data by using table a as a source table. pipeline a is a job that loads data into table a (run daily at 10:00) pipeline b depends on data in table a if no data dependency rule is set on pipeline b, it would start at 10:30 am, and the data loaded into table b would be incomplete. monitoring your data pipeline after you have the data pipeline in place, what is needed next is a way to monitor the pipeline. monitoring your data pipeline is crucial to ensure that it runs smoothly and that any issues are quickly identified and resolved. in the bi team at agoda, we mainly monitor the duration and the status of the submitted data pipeline. when our pipelines are submitted to spark, there are log tables that store information about the submitted pipeline. this information includes when the pipeline started and ended, whether the pipeline succeeded or failed, and many more. the hadoop data team is responsible for the platform used to run our pipelines and manage all these logs. as a bi team, we created a pipeline, or etl, that consolidated the data from logs and loaded the data into the destination table, where the data is ready to be used in dashboards. then, we created several dashboards for monitoring our submitted pipelines. these are some examples of our dashboards used for monitoring. dashboard for monitoring the duration of pipelines. dashboard for monitoring the duration of pipelines dashboard for monitoring the status of the pipeline (succeeded or failed) dashboard for monitoring the status of the pipeline (succeeded or failed) ensuring data quality the product of our data pipeline is data. so, ensuring data quality is essential. data quality can be measured in terms of freshness, integrity, consistency, completeness, and accuracy. we‚Äôll go through each aspect individually and how we test each data quality aspect here at agoda. freshness the freshness of the data refers to the timeliness of the data. this is to ensure that the data is available to be used in a timely manner. at agoda, we built an in-house tool to track the freshness of the data in our tables. for example, we set our freshness tool in the fact booking table to track the booking_datetime column. as our fact booking table sla is 6 hours, the tool will notify us if the latest booking_datetime is older than 6 hours. various reasons can cause data delays. these could be the pipelines failing or taking longer time than usual. integrity and completeness in most cases, data integrity refers to the uniqueness of the data. data completeness refers to the completeness of the data (no null or empty value) for example, table a has column ‚Äúid‚Äù. this ‚Äúid‚Äù column uniquely identifies each record in the table. so, there should be no two records that have the same id. another example of completeness of the data is when we do not want some value in the table to be null or empty. in the above picture case, we do not want value1 to be null. at agoda, we validate data before writing it into the target table. if the data fails the integrity check or completeness test, the pipeline will not write the data into the target table. accuracy data accuracy ensures that the data is accurate by checking the current data against the previous trend. at agoda, our bi team uses a tool called thirdeye to help us detect anomalies in our data. thirdeye can investigate deviations in metrics (anomalies) with interactive root-cause analysis. the above picture shows an example of how thirdeye helps us detect anomalies in our data. it compared the current value against the predicted value based on the previous trend. if the difference exceeds the threshold, thirdeye will notify us about the anomaly. thirdeye is also very easy to use as it provides four basic instant algorithms to calculate the predicted value. more about thirdeye : https://github.com/project-thirdeye/thirdeye consistency data consistency ensures consistent data between two places (source and destination). our bi team uses a tool called quilliup to check the consistency of the data after the data pipeline successfully loads the data into the table. the main concept of quilliup is that we have a source dataset that contains data from source tables and a destination dataset that contains data from the destination table. the data in the source and destination datasets should match. we typically trigger the quilliup test to run once the pipeline completes testing the consistency of the loaded data. more about quilliup: https://quilliup.zendesk.com/hc/en-us data monitoring data monitoring is the process of collecting, analyzing, and responding to the result of the data quality testing result. at agoda, we have an in-house centralized alert system called hedwig. hedwig can receive testing results from various data quality testing tools mentioned in the previous sections and send alerts through email and slack to notify us when the testing fails. this allows for quick and efficient responses to any unexpected changes in the data. we also automate the creation of jira tickets when hedwig sends alerts. this allows us to track the start and end time of the data issue, the root cause, and how it is solved. example of email and slack alert from hedwig we also have a dashboard that summarizes all the jira tickets created by hedwig when it sends alerts. conclusion in conclusion, designing, monitoring, and ensuring the quality of data pipelines is crucial for managing the flow of data. by following the best practices presented by the bi-infra-ops team, data pipelines can be designed and maintained to ensure their efficiency and accuracy. disclaimer : the approaches listed in this blog post are tailored to agoda‚Äôs specific requirements, and all the tools we mentioned in the blog are used internally only. big data data engineering data pipeline hadoop engineering -- -- 2 published in agoda engineering & design 3.1k followers ¬∑ last published 6 days ago learn about how products are developed at agoda, what is being done under the hood, from engineering to design, to provide users a seamless experience at agoda.com . written by agoda engineering 4.6k followers ¬∑ 5 following learn more about how we build products at agoda and what is being done under the hood to provide users with a seamless experience at agoda.com . responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/lyft-engineering/from-big-data-to-better-data-ensuring-data-quality-with-verity-a996b49343f6,,,"from big data to better data: ensuring data quality with verity | by michael mcphillips | lyft engineering sitemap open in app sign up sign in medium logo write sign up sign in lyft engineering ¬∑ follow publication stories from lyft engineering. follow publication from big data to better data: ensuring data quality with verity michael mcphillips 11 min read ¬∑ oct 3, 2023 -- 1 listen share high-quality data is necessary for the success of every data-driven company. it enables everything from reliable business logic to insightful decision-making and robust machine learning modeling. it is now the norm for tech companies to have a well-developed data platform. this makes it easy for engineers to generate, transform, store, and analyze data at the petabyte scale. as such, we have reached a point where the quantity of data is no longer a boundary. yet this has come at the cost of quality. in this post we will define data quality at a high-level and explore our motivation to achieve better data quality. we will then introduce our in-house product, verity, and showcase how it serves as a central platform for ensuring data quality in our hive data warehouse . in future posts we will discuss how verity addresses data quality elsewhere in our data platform. what and where is data quality? data quality is an amorphous term, with various definitions depending on the context. in verity, we defined data quality as follows: verity‚Äôs definition of data quality the measure of how well data can be used as intended. the data is semantically correct, consistent, complete, unique, well-formed, and timely . five aspects of data quality with the definition in italics and an example in quotes. now that we have defined what data quality is, where should we measure it? data quality should be assessed across various components, each of which provides its own strengths and evaluates different aspects of data quality. for example, we can almost instantly validate that each record is well-formed and complete during event generation. streaming compute however, empowers more complex window queries on semantic correctness. we can also evaluate timeliness of partitions and table landing times in data orchestration. finally, as the subject of this blog post, we can assess data quality via batch compute analytics on our data warehouse, providing a comprehensive albeit slower evaluation compared to the previously mentioned methods. hive: lyft‚Äôs data warehouse lyft‚Äôs largest source of consumable data is our hive data warehouse. it empowers most of the analysis and experimentation done at lyft. this is important as our business relies heavily on accurate machine learning modeling of marketplace trends ‚Äî such as predicting the best price to charge for a ride or what a coupon‚Äôs utilization rate will be. as such, hive was the first target of verity‚Äôs data quality assessment. our analytic event lifecycle below demonstrates the workflow of how much of our data gets to hive. analytic event lifecycle lyft reads and writes petabytes of data every day to hive ‚Äî much of it coming from analytic events. when a service at lyft performs a unit of work, such as a customer interaction or a state change, we generate a record of what happened. we log these events asynchronously at the order of millions per second. these flow through kafka , our event streaming platform, before being processed by flink , our streaming compute framework. flink writes data into hive for analytic usage. after events reach hive, airflow etls (extract-transform-load) create derived data sets, analysis is performed, and data for model training is extracted. simplified view of the analytic event lifecycle in lyft‚Äôs data platform examples of issues in lyft‚Äôs hive event data to illustrate how the previous definition of data quality relates to lyft‚Äôs analytic event data in hive, let us examine three real-world examples: completeness ‚Äî the core.rider_events derived dataset has some records with the session_id as null , caused by a bug in a trinosql transformation inside airflow . semantic correctness ‚Äî the core.rider_events derived dataset shows a drastic increase in today‚Äôs cancels volume, caused by a bug in the origin web service creating the event. timeliness ‚Äî the raw_events.bike_sessions data should land in hive within 5 minutes of being generated. a backup in flink causes records to arrive 30 minutes late. consequences of bad hive data poor data quality in hive caused tainted experimentation metrics, inaccurate machine learning features, and flawed executive dashboards. these incidents were hard to troubleshoot, as we had no unified approach to assessing data quality and no centralized repository for results. this delay increased the difficulty and cost of data backfills. the lack of centralization in data quality also made the data discovery process inefficient, making it hard for data scientists and data engineers to identify trustworthy data. verity our solution was to build verity, our in-house data quality platform. verity is currently the center of all things related to data quality at lyft. verity for hive data: the user story data producers and consumers can define their data quality checks and verify the data when it is produced or before it is consumed inside airflow or flyte . science and product teams can also create checks and orchestrate them on a fixed schedule. if a contract is breached, stakeholders can be alerted with a link to a result history view on our ui for expedited debugging. this ui also provides a full-text search interface to find all existing contracts by dataset name or owning team, providing clear observability on current coverage and past performance. high-level user story of a verity customer verity implementation verity goals make adding checks, viewing their result histories, and receiving alerts as easy and transparent as possible. to not be tightly coupled to a particular data orchestration engine, data store, or compute technology. be reliable, fault-tolerant, and highly scalable ‚Äî particularly handle extreme request volume spikes from daily event-processing etls. high-level concepts check definition ‚Äî the user-written configuration file defining the data quality contract and who to notify if it is breached. check result‚Äî the numeric measurement of data quality at a point in time, a boolean pass/fail value, and metadata about this run. check development ‚Äî the process of interactively creating and testing check definitions with our veritycli . check orchestration ‚Äî how a previously configured check definition is dispatched. check execution ‚Äî how the data quality check is performed within verity‚Äôs web services after being initiated by check orchestration . verity ui ‚Äî the verity website where check results and check definitions can be viewed. check definition the verity product begins with a user-written yaml definition of data quality called the check definition . query ‚Äî this field generates a numeric measurement of data quality ‚Äî such as a count or an average. it uses either raw sql or our domain-specific language (dsl). our dsl provides a fast, sql-less short-hand for the most common queries. condition ‚Äî this field describes how the query result is to be evaluated into a pass or a fail. it can be a fixed threshold or a statistical one. metadata ‚Äî this includes a human-readable name, a universally unique identifier (uuid), ownership information, and tags (arbitrary semantic aggregations like ‚Äòml-feature‚Äô or ‚Äòbusiness-reporting‚Äô). notifier ‚Äî this lists the target(s) to notify upon failure ‚Äî via pagerduty, slack, or email. three example check definitions the first check addresses the completeness issue from our first example ‚Äî that our rider_events.session_id is never null . core rider events session_id is not null: # check name metadata: id: 90bde4fa-148b-4f06-bd5f-f15b3d2ad759 ownership_slack: #dispatch-service-dev tags: [rides, core-data, high-priority] query: type: dsl data_source_id: hive.core.rider_events filters: - session_id = null condition: type: fixed_threshold max: 0 notifier_group: pagerduty_policy: dispatch-service email: dispatch-service@lyft.pagerduty.com the next check addresses the issue from our second example. it will ensure the number of canceled rides for this day is not more than 3 standard deviations outside the 90-day historic mean. this check will be dispatched daily at 4 am by our scheduler. core rider events daily canceled volume is inside 3 sds: # check name metadata: id: 3cb75736-1fbe-4f6d-bad5-67bf613f5d62 ownership_slack: #dispatch-service-dev query: type: dsl data_source_id: hive.core.rider_events filters: - final_state = canceled condition: type: z_score min: -3 max: 3 history: 90 days schedule: ## field read by verityscheduler type: cron expression: ""0 0 4 1/1 * ? *"" ## 4 am daily notifier_group: slack: #dispatch-service-alerts email: dispatch-service@lyft.pagerduty.com this last check addresses the timeliness issue from our third example. it will ensure that the raw_events.bike_sessions table has no entries where the last_updated_ms (hive timestamp) is more than 5 minutes later than the occurred_at _ms (generation timestamp). raw bike sessions data is more than 5 minutes late: # check name metadata: id: 6eb84cc-efe3-466e-ab48-a7e1fec6ddq6 ownership_slack: #tbs-dev query: type: dsl data_source_id: hive.raw_events.bike_sessions filters: - occurred_at_ms < last_updated_ms + 5 * 60 * 1000 condition: type: fixed_threshold max: 0 check development in order to develop these check definitions , we made the veritycli. it enables customers to validate, backtest, and backfill their checks across date ranges interactively before committing them. this same validation is done on each pull request to ensure no bad configurations are committed, using github ci actions . for example, our backtest command of the completeness check looks like this: (veritydata venv)mmcphillips@abchelloo veritydata % veritycli backtest \ --check_id 90bde4fa-148b-4f06-bd5f-f15b3d2ad759 --ds 2023‚Äì10‚Äì15 =!======================================================================== beginning backtest for 1 date(s) and 1 check(s). check_ids: ['90bde4fa-148b-4f06-bd5f-f15b3d2ad759'] ds_dates: ['2023‚Äì10‚Äì15t00:00:00+00:00'] =!======================================================================== sql query: select count(*) as result from ""hive"".""core"".""rider_events"" where ds = '2023‚Äì10‚Äì15' and session_id is null result set: result 2.00 maximum value: 0.00 check result: failure =!======================================================================== overall command finished in : 2.176988840103149 seconds aggregate results: success : 0 failure : 1 internal_error : 0 client_error : 0 =!======================================================================== check orchestration airflow and flyte data engineers can dispatch these checks inside flyte, airflow, or other systems which create or consume hive data. to do this, we created the verityairflowoperator and verityflyteoperator . these operators dispatch checks and poll for the results. however, the operators are merely delegators ‚Äî they add their own typed exceptions and retry strategies and delegate the real business logic to our veritysdk for better maintainability. in airflow, we instantiate the verityairflowoperator by citing the check_id previously created in the check definition. we then add it to the airflow dag (directed acyclic graph) in the desired position: with dag(dag_id=""rider_events_example"") as dag: .... dq_check_blocking = verityoperator( task_id=""completeness_dq_blocking"", check_id=""90bde4fa-148b-4f06-bd5f-f15b3d2ad759"", check_datetime=ds, is_blocking=true ) create_staged_data >> dq_check_blocking >> exchange_data the verityairflowoperator can be used in a blocking fashion to halt a dag upon a check failure, preventing bad data from ever reaching production. this utilizes the ‚Äú stage-check-exchange ‚Äù pattern: we create data in a staged schema, verify the data with a blocking operator, then promote it to production if it passes quality checks. verity scheduled checks data analysts and data scientists can dispatch verity scheduled checks , which are checks orchestrated at specified times using verity‚Äôs job scheduler. this is useful because these users are often not familiar with etl tooling. to orchestrate verity scheduled checks , they simply write the cron expression into the check definition as seen in example 2 above. check execution now that we have seen what a user must do to onboard to verity, let us see what happens after they do. our system design mainly follows an asynchronous compute engine pattern. we have four web-services in the back-end (shown in teal): the scheduler , api server , executor, and notifier . this loosely-coupled, service-oriented approach allows us to evolve and scale each component independently, while limiting the blast-radius of failures. component diagram of verity‚Äôs backend architecture, orchestration, and ui verity scheduler ‚Äî this job scheduler performs our periodic tasks. it consumes configurations, like the check definitions, and sends them to the api server to be persisted. it also orchestrates verity scheduled checks. verity scheduled checks are isolated from any data orchestration engine, so they still run even if airflow or flyte are completely down. this remedies a common problem of checks not alerting because the airflow task never ran. verity api server ‚Äî this service handles all the external apis regarding running checks as well as persisting and retrieving their results. the api server does not execute any checks, but rather writes a message to our check queue , which utilizes simplequeueservice (sqs). check results are written and read by only check_id and time . as such, dynamodb was a natural choice as a nosql key-value store. this gives us low latency and high scalability with schema flexibility and no database maintenance. verity executor ‚Äî this service picks up a message from our check queue , executes the trinosql using our compute gateway, verifies the condition, then saves the results. if the check fails, we drop a message into our notifier queue (also sqs). we use sqs to manage both executor and notifier tasks because it provides scalability and consumer-side fault tolerance. with sqs, verity can withstand high peak volume and automatically retry system errors. we deploy a check queue and executor pair for each traffic grouping ( e.g. airflow-blocking, veritycli) . this yields infrastructure-level isolation and scalability; we can ensure our airflow-blocking checks run very fast while also ensuring that our veritycli checks do not interfere with them. verity notifier ‚Äî this consumes messages from the notifier queue and sends a notification to the indicated recipients. because notification dependencies can be flaky, independently retrying failed notifications avoids re-running costly sql executions. client errors from bad sql will automatically page the check author and prevent breaking checks from accruing. verity ui the verityui provides a streamlined data discovery experience via the verity homepage. our full-text search on the check definition metadata lets users see all the checks currently being enforced and their check results . this has useful aggregations like owning team, table name, and tags. verity homepage ui viewing a team‚Äôs check definitions. an on-call engineer being alerted will be deep-linked to a result history view where they can quickly gain context on all past results of that check. verity result history ui viewing an example check‚Äôs results conclusion with verity, we were able to improve data quality in lyft‚Äôs hive data warehouse and provide a centralized platform for vetting hive data. verity made it easy to onboard with our seamless check orchestration integrations and check development tooling. we enhanced our offering with our rich verityui and notifications. verity‚Äôs standardized assessment of data quality increased observability, reliability, and operational efficiency in our hive data warehouse. wins so far: we currently support over 70 teams at lyft, covering 2,500 hive tables with 140,000 data quality validations per week. we reduced the: number of production data incidents by verifying hive data as it is created and before it is released (over 13,000 incidents prevented to date). duration of incidents that do happen. with quick alerting and a centralized ui showing historic check results , users can troubleshoot faster. time scientists and analysts spend in data discovery through providing a holistic view of all check definitions being enforced and their respective check results. related work this solution for ensuring data quality in lyft‚Äôs hive data warehouse was the first step in a larger effort to comprehensively evaluate data quality in lyft‚Äôs data platform. next we tackled: real-time checks to evaluate records of event data as they are produced. this has an almost instant time-to-detect, improving verity's initial offering. anomaly detection with statistical data quality checks to express complex patterns in hive data and to remove much of the human burden of check development . we expanded and scaled our offering by utilizing apache spark and python data science libraries. acknowledgements thank you to the verity team: yanhong ju, marcos iglesias, jason carey, valentine lin, liuyin cheng, bill graham, knowl baek, leo luo, and evan brim. if you‚Äôre interested in working on big data problems like these, then take a look at our careers page . data quality big data data science data data governance -- -- 1 follow published in lyft engineering 16.1k followers ¬∑ last published may 28, 2025 stories from lyft engineering. follow written by michael mcphillips 15 followers ¬∑ 3 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@yjaisingh/data-quality-6e3dec6fb6b,,,"understanding data quality the easy way | medium sitemap open in app sign up sign in medium logo write sign up sign in data quality yatin jaisingh 5 min read ¬∑ jan 30, 2023 -- listen share yatin jaisingh (lead consultant, thoughtworks) author: smash- love‚Äôs double fault https://amzn.in/d/adljxem https://a.co/d/hirft2i today, when i started writing this article, i searched for the meaning of quality on the web. though, having worked as a design engineer during my early career days and having conducted many quality inspections, i had my own understanding of what quality means. the first definition that came across the search is reproduced as-is below: ‚Äúthe standard of something as measured against other things of a similar kind; the degree of excellence of something.‚Äù though the above definition is valid, to add to it, in my understanding quality is a measurement of closeness of the outcome achieved when compared to the committed standards or specifications . also, in the first article on ‚Äú deciphering mdm ‚Äù we read about trusted data as: ‚Äúdata influences our day-to-day decision making and we assume the information is accurate. the price we see on the web, the product that we order, the information that is being displayed. the expectation is simple ‚Äî the information should be accurate.‚Äù if you focus on the expectation in the above statement, it will be easier to understand the real hidden meaning of data quality amongst the complex jargons that we encounter in our everyday work life. data quality can be defined as a measure of how well the data set conforms to the intended use or purpose (as specified). a data set can be considered as high quality data if it is fit for the intended use and accurately conforms to the defined set of standards. ‚Äúit is also important to understand that, for the same set of data, different end users may have different sets of expectations. a customer data set may be termed as of high quality for sales, but may be a low quality data for finance, depending on the set of attributes in use and expectations of the respective user groups‚Äù dimensions of data quality: completeness: this dimension measures the availability of the minimum set of attributes required by the end user to meet business objectives. accuracy: this dimension measures the correctness of the data and can be measured by verifiable sources. consistency: this dimension is the measure of whether the same set of data is uniform across different systems. validity: this dimension measures the adherence to the defined set of rules & standards. uniqueness: this dimension as the name suggests, is the measure of redundancy of the same record in the data set. a high quality data set from the lens of uniqueness, should be de-duplicated. timeliness: this dimension is the measure of two things- availability when needed and also of how current or updated the data set is. data quality management: data quality management is a set of practices or discipline which is aimed at maintaining high quality data in an organisation. these cover all the aspects, right from data acquisition to setting up process, distribution and analysis. pillars of data quality management: data quality management team: the process of defining data quality rules: causes of poor data quality: impact of poor data quality: the importance & benefits of high data quality: better decision making: with the help of good quality data, the decision making is more accurate, efficient and realistic. it is like a strong foundation that helps deliver high confidence decisions saving the organisation from cost overheads of decisional errors. better business outcomes: high quality data gives a better view of the customers, partners, vendors etc. like, in the case of customers, a good quality data may mean knowing the requirements well and accurate anticipation of their needs. in case of prospects, it may mean better targeted marketing campaigns resulting in increased conversion of prospects to customers. competitive advantage: organisations with good quality data as the backbone of decision making, have a clearer picture of the market dynamics. their data driven actions result in higher customer satisfaction and strong goodwill associated with the brand. such organisations certainly have a competitive advantage against their rivals, by accurate predictions of market needs. conclusion: for a successful data quality management strategy implementation, the key pillars are always the people, processes and technology/ tools. hiring the right set of people with the understanding of the organisation‚Äôs data strategy is the first step in the process. when the people are trained the next focus is on the standards, policies & tools. the data quality dimensions, rules & policies must be adhered and data quality metrics monitored, to maintain high quality data impacting desired business decisions and improving organisation‚Äôs performance. the data quality pyramid below represents how ultimately, an organisation‚Äôs performance is a representation of how good the underlying data quality is. so, do you find any of the insights above, helpful? please drop a message of you have any comments/ questions/ feedback etc. *if you enjoyed reading my story, please do hit the follow button on my profile! data quality master data management data management business analyst role mdm -- -- written by yatin jaisingh 125 followers ¬∑ 25 following i am a lead consultant at thoughtworks and write about data management & also about topics relevant for a business analyst role. do click the follow button ! no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/the-bounce-io/the-issues-of-tracking-qa-solutions-and-what-a-better-one-could-look-like-a883ed527b1,Mocha; Selenium,,"issues of tracking qa solutions, and what a better one could look like | by lukas oldenburg | the bounce sitemap open in app sign up sign in medium logo write sign up sign in the bounce ¬∑ follow publication digital analytics, product & marketing analytics and other unimportant things follow publication issues of tracking qa solutions, and what a better one could look like automated qa of data collection, part 1 lukas oldenburg 10 min read ¬∑ aug 30, 2022 -- listen share t he futile (?) dream of many digital analytics/tag management/data collection specialists: a tracking implementation where every measurement point in any technical constellation is automatically validated against non-trivial, up-to-date rules and serves you errors on an actionable silver plate: perfect, automated qa of data collection (aka ‚Äútracking‚Äù). there are good solutions, but they are limited, and they focus on the less relevant. it is difficult to squeeze the automated qa of data collection into a business case. it suffers from similar ailments like it security or documentation ‚Äî they are often neglected as long as they do not seem acute. harvesting some good-looking data layer errors nevertheless, i had a key moment last year: a client switched their entire underlying shop system. it was quite the bugfest. data in all tag-management-based (=data-layer-based) systems was broken in any way imaginable. however, thanks to our testing framework, we could detect 90% of the bugs quickly, and just as quickly test them again when they were fixed. the developers could even test their fixes themselves: the mocha/chai testing framework (see part ii) gave them feedback on their fix before playing it back to the digital analytics team for testing. the server-side-tag-management-based automated testing of all data layer events (part iii) plus the adobe analytics alerts nowadays make sure we learn about every bug as soon as possible, and we know how much data is affected. so without the testing framework, we would have discovered only a small part of the bugs right away, the test-fix-retest cycles would have taken longer, and the data quality would have suffered for months instead of weeks. mochachai-based tests (part ii) allow you to test anything in the browser while it happens and output the testing results nicely all in all, getting to an implementation culture where every new tracking event gets a proper test written for it makes you sleep better during releases, and it frees your mind from the daily uncertainties (‚Äúis this really tracked correctly?‚Äù). set reasonable goals for your testing framework of course, no testing framework will ever be complete. there will always be these unexpected bugs for which you did not or simply could not specify a test within reasonable effort. so don‚Äôt let that get you down. set reasonable goals. my main goal of a good testing framework was to lower the median time to detection (ttd) to less than 3 days (that happens when 80‚Äì90% of the bugs can be identified very quickly) and to even less for major bugs. because a high ttd is costly , and at this particular client, it was at over 20 days. issues of existing tracking qa solutions there are many automated testing solutions for ‚Äútracking qa‚Äù. most of these tools check requests to ‚Äútags‚Äù like adobe or google analytics against large sets of rules. you can design complex scenarios (selenium-style), like going through a checkout, and then have them run regularly. other solutions like waaila simply test when you activate them in your browser. a more scalable, but still limited method are randomized crawlers (that was among my favorite features in observepoint). these crawlers start on a page of your choice, then click up to x random links, then do the same on the next page until they have reached a certain depth (e.g. max n pages after the starting page). on each page, they check a global ruleset again. if you have a good global ruleset, you can test a lot of pages for a lot of errors. monitoring of automated server-side tms-based data layer testing is part of the setup we will show in part iii. here, something obviously happened yesterday which caused a lot of events to fail their tests. while all these tools and approaches have their value, they have important limitations and/or they focus on something i don‚Äôt think it‚Äôs worth focusing on, given the immense time it takes to define the tests. and i am going out on a limb here now, since i have tried out far from everything on the market. i am sure somebody will come along and yell ‚Äúbut solution x solves that!‚Äù. that‚Äôs fine, i‚Äôll be happy to learn. anyway, here are my issues with existing qa solutions: they don‚Äôt scale well apart from the above-mentioned randomized crawlers, scenario-based testing solutions run the same scenarios over and over again. however, making your scenarios mimick even 80% of the most common actual user journeys is a lot of tedious detail work to set up and maintain ‚Äî even in a rather restricted flow like a checkout funnel. scenario-based solutions, moreover, often break because they hinge on the ever-changing html / css of those pages, which often differs depending on the screen size. if your scenario requires a login or an external payment process to complete, it can get really complicated to impossible to set up. and sometimes the crawlers executing the scenarios are blocked by the next infrastructure/firewall/bot management solution update. most solutions work only for websites, ‚Ä¶ ‚Ä¶ but not for mobile apps or iot. that is quite a limitation with more and more server-side tracking around, and it is another hindrance for scaling: what if you want to run the same data layer tests on your app? they don‚Äôt test real traffic while you could theoretically run client-side js tests behind the scenes on your real users (the errors would be sent to some qa data collection server or your analytics solution), you really don‚Äôt want to run hundreds of validations in the browser, as this slows down the user experience and wastes client resources. they don‚Äôt account for the complexity of reality getting them to run under not-so-typical circumstances (say, testing as a logged-in user with a certain account status in mobile safari with a weak 3g network) is usually impossible or excessively tedious. most solutions run in certain virtual machines which use a certain browser (which is usually some chrome or firefox). even if you put in a lot of work, you will never get nowhere near your actual users‚Äô landscape of devices, browsers, cookie settings and network conditions, so your tests will always be blind on many eyes. and i have to reiterate that since many developers nowadays naively assume that (with polyfills, babel etc.) browser incompatibilities are a thing of the past: in the last half-year alone, i remember a bug where even the all-important order confirmation tracking failed in some safari browsers on apple mobile devices, and another one where it failed in some firefox versions (which was impactful because it was due to some some larger b2b clients‚Äô company networks which did not upgrade their ff versions regularly). they are too simple the types of tests that are allowed are often limited. while i think that tealium‚Äôs server-side event specifications are the right approach in terms of where they run (server) and what they focus on (data layer, not tags), they only allow you to specify whether a variable is required and of what type (e.g. array of strings) it should be. a good testing system should at least allow regex and js functions for validation. proprietary test definition formats defining tests takes time. so you absolutely don‚Äôt want vendor lock-in here. vendor lock-in happens when you have to define the tests in a vendor-specific manner and cannot use the same definitions in another solution. for example, if you have defined a test for a product detail event, it should be in a form that developers can use for another validator when they e.g. write unit tests in their code base. methods like json schemas make tests more (im)portable. version control/ide integration with json schemas come things like proper integrations into the professional developer toolstack, e.g. git integrations for version control of the test definitions, the ability to write code (incl. schemas) in a proper ide, etc. i have not seen this in any testing solution out there. even if json schemas are used, a lot of copy-paste is needed. most are focused on testing requests, not data layers last and most importantly, your priority should be to test the data layer since all the tags consume the data layer data (unless you implement tracking like in 2010). of course, there is nothing wrong with setting up additional tests for your google/adobe analytics/facebook pixel/tool xyz requests. but given the time it takes to set up a solid testing framework, you need to prioritize . and if i can prioritize between setting up tests for the data layer or individual tests for requests to my tags, then i always go with the data layer, as issues with the data layer will be responsible for 90% of the issues i will have with any of these tags . if the product quantity in the data layer is wrong, it will be wrong in google analytics, adobe analytics, google ads, facebook pixel and so on. side note: setting up tests for your google analytics request first (or only) is as short-sighted as using the ga4/gtag event data as the main data stream for your server-side data mapping/processing logic (an approach that markus baersch and michael jansen btw completely rip in their podcast ). my experience with two qa solutions claiming to be able to also test the data layer was that both failed when it came to actually doing that. or they simply tested a js object on pageload, which is, of course, an understanding of data layers that was already outdated in 2015. which ‚Äúdata layer‚Äù? to avoid confusion, with ‚Äúdata layer‚Äù , i do not mean simply the ‚Äú input ‚Äù data layer (the payload of the data layer event when it is received by the tms), but the ‚Äú output ‚Äù data layer (the data layer after processing the event, just when it is ready for mapping to tags). this is an important distinction because it allows to detect both application bugs (wrong input data layer sent) as well as bugs in your tag management setup (e.g. error in deriving variable x from input data layer variable y) . the concept of input vs. output data layer comes from tealium . see this ugly diagram for an explanation: while in gtm or launch, you create a ‚Äúvariable‚Äù or ‚Äùdata element‚Äù (e.g. the result of a lookup table to look up the ga measurement id), in tealium that ‚Äúvariable‚Äù is simply a data layer property, and you can create, change or even delete this property at any time during the data layer event processing via b[""theproperty""] . so the input and output data layer are the same object (called b ). that object is simply enriched (or changed) along the way while it passes through your ‚Äúextensions‚Äù in your defined processing order until it becomes the ‚Äúoutput‚Äù data layer. that output data layer is what you can then map to tags‚Äô parameters (e.g. custom dimensions), and it is what you usually forward to tealium‚Äôs server-side tms (via the tealium collect tag) similar to the ‚Äúmain data stream‚Äù in gtm. since for gtm and launch, there is technically only an ‚Äúinput data layer‚Äù that cannot be altered or enriched on the fly, we need to translate this concept a bit for other tms: in google tag manager (gtm), the ‚Äúoutput‚Äù data layer would be an object with the values of all the ‚Äúvariables‚Äù after a data layer event has been received and all ‚Äúvariables‚Äù have been evaluated. your ‚Äúoutput‚Äù data layer could be the payload you send to your server-side gtm as the main data stream. in adobe launch , assuming you use an event-driven data layer, it could ideally be all variables of that data layer plus all data elements after the event has been processed. what a better solution could look like so i had long wanted a scalable solution that: tests real traffic, all the time executes tests in a non-invasive manner (not in the client) focuses on testing data layer events instead of requests to tags was based on non-proprietary, portable test definitions makes it easy to reference test definitions or stack them on top of each other to avoid redundancies (via json subschemas or references), so that e.g. ‚Äúadd_fromproductpage‚Äù, ‚Äúadd_fromlist‚Äù, and ‚Äúadd_fromcart‚Äù events can share 99% of their test definitions. allows test definitions to include any js logic, i.e., more than just type or regex checks, but also function validations is well-integrated with a(ny) tag management solution has a solution for alerting has a solution for providing and analyzing the data on failed tests (e.g. what did the complete data layer look like during that error / device info etc.) is capable of testing more than web browsers the first inspiration i got into this direction was an sdec presentation by stewart schilling whose presentation on data layer validation ticked many of the boxes above. he showed one of the most popular adobe launch extensions, the ‚Äúdata layer manager‚Äù which comes with a validator (disclaimer: my knowledge of the tool is limited to that presentation). it uses json schemas for validating data layer events, it has an integration with an alerting system (airbrake, requires separate license), and it is nicely integrated into the extension that handles the data layer events in the first place. it furthermore allows ‚Äústacking‚Äù, i.e. one event schema can import the definitions of another schema to avoid redundancies. it is of course a paid and an adobe-launch-web-browser-only solution. you also need to go through the tedious launch build and publish process any time you need to edit a comma in your schemas. there is no git integration or sth like ‚Äúpull the schemas from url x‚Äù. still, i wish a client would have committed the budget to take a closer look at this solution. coming up‚Ä¶ two years later, we can show a solution that does (almost) all the things i want a data collection qa solution to do, and it does so without requiring licensing fees (apart from a couple of dollars monthly on google cloud platform). so stay tuned for part 2 , where we show how to use mocha / chai for client-side ad-hoc validation, and part 3 , where we take the whole thing server-side in a scalable way. web analytics tag management automated testing tracking datalayer -- -- follow published in the bounce 132 followers ¬∑ last published apr 13, 2025 digital analytics, product & marketing analytics and other unimportant things follow written by lukas oldenburg 727 followers ¬∑ 117 following digital analytics expert. owner of dim28.ch . creator of the adobe analytics component manager for google sheets: https://bit.ly/component-manager no responses yet help status about careers press blog privacy rules terms text to speech",17
https://fithis2001.medium.com/adding-some-minio-to-your-standalone-apache-spark-cluster-d068b4f4fdab,,,"adding some minio to your standalone apache spark cluster | by vasileios anagnostopoulos | medium sitemap open in app sign up sign in medium logo write sign up sign in adding some minio to your standalone apache spark cluster disaggregated compute and storage for the apprentice data engineer vasileios anagnostopoulos 9 min read ¬∑ aug 26, 2022 -- 1 listen share background one of the typical problems, a beginner in apache spark encounters is to have an apache spark cluster and an apache hadoop cluster to practice the locality of compute and storage through yarn or follow apache spark examples by interacting with an hdfs system. this typically entails connecting your apache spark to an already running hdfs system, or just run your apache spark as a yarn job in an existing hadoop cluster. the approach in most of the cases amounts to some kind of the following options: download distribution for apache hadoop and apache spark and run them locally. use a docker distribution that is all-inclusive and preset like hortonworks sandbox use a more modularized docker distribution like big data europe . pay for a preset distribution in various cloud providers. of course, (1) does not qualify as clustered if you need more than 1 instances and takes some steps to set up. on the other extreme, (4) may cost you $$$. solution (3) downloads a lot and since so many things are preset, it does not qualify as educational, especially when maintenance is left on volunteers and can be behind the current versions of the packages. solution (2) is too fat for my taste. still, a lot is preset. however, if you have a fairly capable laptop, you can create your own setup, and learn on the way. one of my main requirements was to have something that can be setup easily. this rules out hadoop/yarn since it has too many moving parts and knobs to make it work as a cluster. making my own docker image was a no-go at that time (but could take an existing one and upgrade it, of course). using some other‚Äôs distribution was also a no-go because i saw a lot of them that quickly become unmaintained or get updates sporadically. coming to spark, by ruling out yarn or paying $$$ for educational purposes, i see the standalone cluster mode as the best solution to my constraints since i was not interested at this point in learning kubernetes. instead i should focus on apache spark. but still, the standalone mode needs some storage to pull data. while a relational database is a very viable (and overkill if you do not already use it for something else) option, i wanted something like hadoop-like. minio to the rescue one possible solution was to use something like s3, but without having to use amazon web services. leofs is an option, but there is a lot of hype about minio and good documentation ‚Ä¶ and docker images. minio promises disaggregated compute and storage. the value proposition is that storage and compute should scale independently . this is in accord with cloud computing. contrary to hadoop it is not a file storage. in hdfs a file is split and replicated across various nodes. but minio is an object storage. you have files+metadata which both make an object. after a couple of minutes of thought, it is obvious that hdfs can be emulated easily. after all, it is a game of path names. but you lose the co-location of compute and storage, which is exactly the same if your apache spark cluster and apache hadoop cluster live in different servers. in this particular case, minio has broken the speed barrier against hdfs. this is actually what i was looking since i jumped outside the hadoop ecosystem. the only remaining question is how to use it with apache spark? it turns out that i‚Äôm not the only one having this question. fortunately, there are answers like this and this (from medium): big data without hadoop/hdfs? minio tested on jupter + pyspark the takeover of hortonworks by cloudera ended the free distribution of hadoop. therefore, a lot of people are looking‚Ä¶ python.plainenglish.io unfortunately, these are either outdated or they use apache spark from the sdk and not through a standalone cluster. they can also be complicated, and in some cases the answers are scattered across many sites. sometimes they lead to dead ends. after a lot of trial and error and posts and blogs, this is the documentation of my setup. the purpose is to be simple, maintainable and oriented towards beginners. let‚Äôs start. setup minio and first sanity check we will use the minio docker image from minio . the bitnami one is not maintained anymore. i provide a docker-compose.yml for the code of this article. you can start minio as docker compose up minio minio takes some time to start and here you are if you access it at http://127.0.0.1:9001 the minio console we will use the data from here . it is a csv file with addresses for demo purposes, which you need to download. log in, create a bucket as ‚Äúmybucket‚Äù and upload your csv file. this is the end result bucket with files you can also join the docker network and list the contents without problems by using the outdated bitnami client docker run -it ‚Äî rm ‚Äî name minio-client \ ‚Äî env minio_server_host=‚Äùmy-minio-server‚Äù \ ‚Äî env minio_server_access_key=‚Äùtheroot‚Äù \ ‚Äî env minio_server_secret_key=‚Äùtheroot123"" \ ‚Äî network app-tier ‚Äî volume $home/mcconf:/.mc \ bitnami/minio-client ls minio/mybucket the output is shown in the next screenshot accessing minio programmatically through python for sanity check, we will access through python our minio server from our pc. first, we need to install python minio support . pip install minio open your favorite editor, ide or cli environment and execute access minio through python you can also join the docker network through jupyter . create in your current folder a sub-folder, e.g. jupyter-workspace . now you can run docker run -it --rm --network app-tier  -p 10000:8888 -v ""${pwd}""/jupyter-workspace:/home/jovyan/ jupyter/scipy-notebook:latest connect to http://localhost:10000 take the token from the console output and login with it. create a python3 notebook and execute a modified version of the above code. first in a cell run !pip install minio in case you have any problems, look here . in the next cell run the modified code success!!! accessing minio through local hadoop client now it is time for our first encounter with the apache ecosystem. it came to me as a pleasant surprise that hadoop can ‚Äúsee‚Äù an s3 file system as another distributed file store. it is not only for hdfs. there is extensive documentation on this . minio is s3 compatible, so we are in business. hdfs is so advertised that it is easy to miss the point that hadoop actually is both an implementation and a driver for hdfs. let‚Äôs get to work! first, we need to download the hadoop distribution, since the plan is to access our ‚Äúdistributed‚Äù minio from hadoop and apply its commands . download the latest release from here (3.3.4). i keep everything in my downloads/apache folder. so the next reasonable step now is to have export hadoop_home=$home/downloads/apache/hadoop-3.3.4 export path=$hadoop_home/bin:$path the idea is now to naively access it as a s3 file system with the s3a scheme which call the appropriate driver. hadoop fs -ls s3a://mybucket/addresses.csv of course, it fails. we have not configured where s3a is !!!. head over to $hadoop_home/etc/hadoop backup core-site.xml (e.g. make it _core-site.xml) and ‚Äústeal‚Äù some of the contents from here . a more up-to-date list can be found here . we put the bare minimum necessary (in my repo it is this file ): core-site.xml for minio access our attempt now fails again however now we are in a better position. it cannot find a library. the real hint is in overview section . in summary: export hadoop_optional_tools=‚Äùhadoop-aws‚Äù this time we succeed, yay!!!! so we can use apache hadoop to interact with another distributed filesystem. a huge deal. now our hopes for spark access get better and better since it uses hadoop as its jdbc for file systems. accessing minio through local spark we will first make sure that we can access the cluster through spark locally. head over to the downloads section of apache spark and download the latest release, without any built-in hadoop. you can always use the all-inclusive. but here for tutorial reasons we do the bare minimum. after this article you will have the knowledge to handle that situation. download it, unzip it and set it up export hadoop_home=$home/downloads/apache/hadoop-3.3.4 export spark_home=$home/downloads/apache/spark-3.3.0-bin-without-hadoop export path=$spark_home/bin:$hadoop_home/bin:$path export hadoop_optional_tools=‚Äùhadoop-aws‚Äù we have a script for accessing minio through spark. it reads while the python distribution has the python packages we need, i will also use it through pip (python 3.10.6), for autocompletion reasons and easy setup. pip3 install pyspark let‚Äôs give it a try in thonny. oooops!!!! what went wrong? hadoop can talk to minio and spark talks to hadoop. it cannot talk though, since we have a spark without hadoop. the fix is usually the documentation . export spark_dist_classpath=$(hadoop classpath) we succeed this time. the file is there (you can also verify through ui) or with hadoop!!! now we are ready to provide our apache spark standalone cluster with some storage. running our spark standalone cluster our cluster is based on this repo . there are some differences though. latest dependencies spark is without bundled hadoop i add a latest hadoop distribution enable s3 in hadoop so as to be able to run commands from workers or master i also have added a docker-compose.yml that sets everything up. the other thing i would like to mention is that since hadoop comes with an empty core-site.xml , it should be provided either directly, by linking an external one to the hadoop folder, or link a spark-defaults.conf which puts the same variables in a non-xml format with a hadoop prefix. i included both. feel free to connect to the container and execute hadoop file system commands. let's spin our full cluster with minio and spark (master url is in 127.0.0.1:8080). docker compose up we delete the parquet file hadoop fs -rmr s3a://mybucket/output_addresses.parquet and now we will try to re-create it from our spark cluster!!! we definitely need our env variables before submitting in a new terminal export hadoop_home=$home/downloads/apache/hadoop-3.3.4 export spark_home=$home/downloads/apache/spark-3.3.0-bin-without-hadoop export path=$spark_home/bin:$hadoop_home/bin:$path export hadoop_optional_tools=‚Äùhadoop-aws‚Äù export spark_dist_classpath=$(hadoop classpath) now we can submit spark-submit ‚Äî master spark://127.0.0.1:7077 spark-access-minio.py we were too optimistic, sigh!!!! crash and burn! not really obvious what is happening here. some months ago i opened this bug . months passed, day to day work took over. at some point in mid-july i decided to track it down. i had a very hard time. the hadoop in the container resolves just fine. eventually i added a magic line in my /etc/hosts to resolve it to my localhost in case it comes from the local spark client. guess what! it worked this time and the file is there, though it is a mystery to me why. i suspect driver is validating data as it is described by others ! i am very interested in an explanation. conclusion i decided to deviate from the typical hadoop/spark combo because of complexity. i opted instead for a different solution that is easier and equally cloud native. that path needed a lot of experimentation and studying. the gains in knowledge were non-trivial. i re-discovered that it can pay off to do things differently. i shared my journey to make the life of others easier and as a means of self-documentation. i used an x64 mac with mac os monterey, python 3.10.6 and latest openjdk11. code is in this repository . as usually i am very happy for corrections and suggestions for improvement. spark minio hadoop data engineering -- -- 1 written by vasileios anagnostopoulos 65 followers ¬∑ 38 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@sergey.kotlov/unit-testing-of-spark-applications-cf0162a5cb3c,,Unit Testing,"unit testing of spark applications | by sergey kotlov | medium sitemap open in app sign up sign in medium logo write sign up sign in unit testing of spark applications sergey kotlov 8 min read ¬∑ mar 29, 2022 -- listen share in this post, we‚Äôll look at one of the ways to unit test spark applications and prepare test datasets. the motivation for using the described approach at joom is the large number of constantly evolving data sources (including more than 1000 different spark tables) used in our spark applications. and as our spark infrastructure developed, we began to create some tools to simplify unit testing. photo by ildefonso polo on unsplash introduction usually, production spark code performs operations on spark datasets . you can cover it with tests using a local sparksession and creating spark datasets of the appropriate structure with test data. there are the following basic ways to create the test datasets: creating test data manually. typically, these will be small datasets, which allow you to check the correctness of your code in various situations. creating arbitrary datasets using generators. using samples from production data (this post does not cover this approach). in my opinion, the easiest and most effective way to fully test application logic is to create the necessary test data manually for each test case. you can easily create a test spark dataset/dataframe using scala case classes that match the required data structure (we call them ‚Äútest data classes‚Äù). for example, if a spark application uses a table with orders, you could define the case class with its structure: next, you create test data suitable for your test scenario and turn it into dataset or dataframe. it is convenient to have all fields optional because production tables (and their corresponding classes) may consist of several dozens or even hundreds of fields, but specific spark applications often use only a subset of those fields. optional fields allow you to set only the required data in your tests. repository of test data classes as previously mentioned, we have a large number of different data tables. we have a separate project in our code repository that contains all the test data classes (like order described earlier) for all spark tables and other data structures that can be used as input for our spark applications. this project is a centralized repository of these test data classes, and it also contains our test api. all spark projects that have unit tests have this project as a dependency. the requirements for this test data classes repository: our developers should have ready-to-use test data classes for all data structures that can be used as a data source in their spark applications. this simplifies the creation of test data. the definition of these test data classes must match the real tables and be updated when they change. thus, the code of all applications will be tested against the real data structures with which it will have to deal in the production environment. if some tables change in the future and become incompatible with the existing applications code, we will know about it at the test launch stage. these test data classes should be created and updated automatically. to achieve these goals, we use automatic code generation of the test data classes based on the structure of existing spark tables. we use custom gradle tasks to perform the code generation. these gradle tasks are located in the build.gradle file of this project with the test infrastructure. we have implemented two alternative options for automatically adding or updating the test data classes. 1. automatic generation based on the description of spark tables in hive metastore. it works as follows: we run a custom gradle task to perform this code generation. in the parameters of this gradle task, you can set a list of databases and tables for which you want to generate these classes since often, not all of the existing tables may be used in your spark apps. this gradle task, through a special custom service, gets the actual schemas (in json format) for all required tables from hive metastore and converts them into structtype objects. then, we pass the structtype objects to our custom code generator, which for each of them creates a <yourtablename>.scala file containing the code of the test data class. these classes have structures identical to the corresponding tables. the generated files are saved to the desired location of the test infrastructure project. all these files are organized by packages corresponding to databases. an example of using such gradle task in the build.gradle file of the project: 2. automatic generation based on production classes containing table creation logic. for some spark tables, there are special classes in our projects that define the logic for creating these tables. also, these classes specify the spark schemas of the resulting tables (primarily for documenting fields) and implement a special trait: trait joomtable { val tablename: string val database: string val schema: structtype } it is possible to generate the test data classes based on such production classes. in this case, the code generator discovers the classes that implement the joomtable trait and extracts the structtype objects from them using reflection . the advantage of this approach is that when you edit or add any such table, you don‚Äôt need to run your spark app to save the new version of the table schema into hive metastore in order to get the updated test data class. the disadvantage is that you have to describe the resulting table schema in your code, which you may consider as additional overhead. it is worth mentioning that spark tables are not the only source of input for our spark apps. we also have data in the form of so-called events . events can be of different types (for example, productpurchase ‚Äî the fact of purchasing a product, productopen ‚Äî the fact of viewing a product card), and each type has its own structure. spark apps access the event dataframe through a special custom api. our test framework also supports the automatic generation of test data classes for different types of events so that they can also be used in unit tests. these gradle tasks for creating/updating the test data classes can be run manually, and you could also set them to run automatically on a schedule together with unit tests. you can find the source code of the code generator here . structure of the test data classes in theory, the table/event structure can be quite complex, with any nesting depth, with various combinations of arraytype, maptype, structtype. our code generator supports structures of any complexity. the code generator creates a root class. and all nested structures are wrapped in case classes named after the corresponding fields and placed in a root object to avoid name conflicts in the case of the same field names in different tables/events. for example, for the product table with the structure: root |-- product_id: string |-- merchant_id: string |-- category_id: string |-- name: string |-- price: decimal(38,18) |-- images: array |    |-- element: struct |    |    |-- url: string |    |    |-- height: integer |    |    |-- width: integer |    |    |-- properties: map |    |    |    |-- key: string |    |    |    |-- value: string |-- attributes: map |    |-- key: string |    |-- value: array |    |    |-- element: string the code generator will create the product.scala file. all fields of such case classes are optional. because often, a particular spark app uses only a small subset of the fields available in the tables. and their optionality allows you to set in the test code only those fields which you need for each specific test scenario. test api in the production code of all our spark applications, we use a special class called platformapi . this class provides the following functionality: determines an appropriate configuration for each spark application. creates a sparksession with the necessary configuration, spark listeners, etc. provides an instance of a class called warehousecontext , which has getters that return dataframes for the most commonly used tables and data structures. in most cases, these getters use sparksession to read some spark tables and return its dataframes, but there may be more complex logic (for example, in the case of our events api). the project containing the test data classes also contains the test version of all these infrastructure classes so they can be used in unit tests instead of the production version of platformapi. for example, testplatformapi : provides a local sparksession. we use dataframesuitebase from spark-testing-base for this. specifies the local version of the configuration for tests (for example, in the production, we use s3 to store spark tables data, but in tests, we replace the s3 paths with local ones). returns the test version of warehousecontext, which in your test code can be configured to return the test dataframes you created instead of reading the production tables. finally, in our tests, we use a custom extender for funsuite called platformapisuite ( class platformapisuite extends funsuite with dataframesuitebase ). platformapisuite provides testplatformapi, contains a set of helper methods to facilitate test writing, clears temporary data after test completion. all of our unit tests use this platformapisuite class. an example of what such a test might look like: you can also write any additional helper methods for the needs of specific tests. for example, in some situations, it may be convenient to set test data not in the form of tables but in the form of higher-level business entities. for example, some applications may work with the concept of user data, which includes user events and records in various tables (user, order, etc.). in this case, you may define the test data using the high-level userdata class (it may be much more convenient), and some utility code will add the necessary records to the test tables/events with which the application works. it may not be necessary to cover all existing spark applications with full-fledged tests. but it is recommended that for each spark application, there be at least a minimal test on empty tables so that you can easily verify that the application does not crash on the table structures available in the production environment. this reduces the time needed to detect many errors. for example, if using the spark sql api we mistyped any field name or used an operation that is incorrect for the type of field, then it will be revealed during the next local test run. this is some kind of protection against incorrect changes in existing tables that make them incompatible with existing applications. in this case, after regenerating the test data classes, the unit tests will report which applications are broken. testing on automatically generated arbitrary datasets this method of creating test data is not used by us often. but in some cases, it can be useful to check the correctness of some applications on a large number of different input values. using scala case classes is convenient because there are enough libraries to work with them. below is an example in which we generate collections of test data classes with arbitrary data, which we then pass to the code under test. we use scalacheck-shapeless to create an instance of the specified class with arbitrary data. you can override the rules for generating arbitrary values (see trait arbitrarytypes). for some fields, it is desirable to replace completely arbitrary values with values from the provided list (for example, for a productive sql join). it is convenient to use quicklens for such modification of fields in deeply nested classes. conclusion in this post, i described our approach to writing unit tests for spark applications. we looked at the general approach to testing spark applications, the automatic creation of the test data classes, our api for writing unit tests, and a way to generate arbitrary test data automatically. spark unit testing scala big data -- -- written by sergey kotlov 132 followers ¬∑ 27 following big data engineer at joom no responses yet help status about careers press blog privacy rules terms text to speech",17
https://3d-points.medium.com/006-does-leica-blk2gos-data-worth-the-money-19bb0b0d0333,,,"#006- does leica blk2go‚Äôs data worth the money? | by lukasz wiszniewski | medium sitemap open in app sign up sign in medium logo write sign up sign in #006- does leica blk2go‚Äôs data worth the money? lukasz wiszniewski 16 min read ¬∑ aug 15, 2021 -- listen share it is a continuation of the previous article with a strong focus on the quality and usability of blk2go‚Äôs data. pict. unfiltered blk2go data with waypoints generated every 2 meters. after many hours and a few kilometers walked with the blk2go scanner ( check part1 of this review ), came time to take a look at the point clouds. in this article i will focus on: uploading data data quality data usability as long as i was impressed by the portability of leica‚Äôs device, the data itself gives me mixed feelings. pict. a single run with the scanner. transferring data to the leica cyclone register 360 first things first. before you would be able to see the data on the screen of your pc, you should transfer them from the scanning device. you have only one option here which is a usb-c cable. in my opinion, is the fastest and most stable data transferring possibility, and i am glad it is used in blk2go. usb socket is placed under the battery, so you turn off the scanner, pull out the battery and connect the scanner by a cord to your computer. cyclone register 360 will recognize connected devices and display projects available in its internal storage. now you have three options: import data to the register360 project import data to the register360 project and save raw data to b2g format export directly to e57 file without importing data to the project pict. export e57 users can also create waypoints with preferred intervals e.g. every 5 meters. these waypoints are nothing more than spots in a scanning walk wherefrom will be generated panoramic images. extracted pano images are used for truview. this option is available only with a publisher pro license. pict. on the very bottom of the import panel in cyclone register360 is an option to create waypoints leica developers implemented a few months ago a new feature called filter. it is meant to be used for removing the spaghetti effect in the point cloud - the point cloud doesn‚Äôt have nicely structured rows and columns of points in comparison with the tls data. unfortunately, the filtering not only enhances the readability of the point cloud but also removes some percentage of points, thereby removes smaller details of scanned space. pict. data filtering during import (from the left: no filter, high filter, low filter) pict. amount of imported points with used filtering (from the left: low and high filtering) you should notice that the same raw data imported to the cyclone register360 and directly exported to e57 gives a different amount of points in the point cloud. another odd thing is that low and high filter applied to the same walk has a diverse length of the trajectory. why does it happen? first, look i am going to mention shortly about data quality comes from my scanning test in april 2020 and then i take a look at data from march and may 2021. pict. the same office area was scanned with blk2go (no filters applied) and rtc360. generally speaking, blk2go data from 2020 ‚Äî the first release- was useless in most cases. as you can see in the picture above, visibility and recognizability are poor and it is a blockage for using it in the modeling process or analysis/clash detections. i juxtaposed blk2go point cloud with rtc360 to show a huge difference and let you answer the question does the leica‚Äôs mls data would be easy to use. luckily a year later -2021 blk2go began to deliver much nicer point clouds. they are more consistent and have much better visibility. the availability of the filtering options in register360 also increased the value of the data in most cases. i am saying in most cases because filtering drastically dumps the number of scanned details. blk2go competitors mls systems like gexcel heron ms-2 color, leica pegasus backpack, or geoslam delivers very noisy point clouds and often fragmented/layered data. by layering, i mean that flat surfaces, in reality, were presented by several surfaces of points so as a consequence slice through a wall‚Äôs face looked like many lines instead of a single line. it happened because of the mismatch between the following point clouds created by a single rotation of the scanner‚Äôs head. pict. orthographic view of the geoslam data. notice the data representing the ground in front of the building visible in the center of the picture should be shown as a single line but you can see at least 4 extra lines below the ground. blk2go point cloud impressed me from the very beginning. even in 2020 slice through a wall never occurred as several lines. there was no layering whatsoever. of course, there was noticeable noise but not so heavy in my opinion. horizontal cross-section through the vertical surface showed spaced-out points with a maximum distance of 18mm in perpendicular measurement to the sliced surface. it is much better than the leica pegasus backpack, heron ms-2 or geoslam horizon. pict. spaced-out points are visible in the gexcel heron ms-2 backpack data (without filtering/denoising). notice that the average 2d distance between the extreme points representing a single surface is approx. 30mm point cloud quality scans of the outdoor environment on a cloudy day are nicely colorized. unfortunately too bright (direct sunlight or artificial light) or too dark scan areas have applied noticeable overexposed colors so the user has to rely on a single color to recognize scanned elements. the view can be switched to black&white or an intensity color palette so it kind of solves the problem. when it comes to checking the entire walk and its coherence then results seem to be not so promising. as long as your walk doesn‚Äôt start and end in the same spot, you are mostly safe. in case you would like to scan in the loop so let‚Äôs say you walk around the building, then you will most likely end up with a mismatch of the first and last seconds of your walk. and by loop i mean the start position and the end position are the same, and scan data overlap in the start/end spot. it happens because drifts occurred along the trajectory. i assume there can be many factors that lead to distorted data and lost spatial consistency. i am listing some of them: human and road traffic nearby a scanner operator movement of neighbor vegetation e.g. leaves in the wind reflective, transparent and semi-transparent surfaces improper walk speed too even scanned environment ‚Äî long corridors or tunnels without any features/things which would make the space less homogenous e.g. furniture, paintings on a wall too small features in scanned space ‚Äî laser beam would hit only a few times e.g. let‚Äôs imagine we scan a room which is empty but has only lamps on the ceiling, hooks in the walls, electrical boxes, for the men eye is plenty of elements (and not only bare walls) but from the algorithm perspective view there is nothing that can help in the data processing scanner shaking to quick turns during a walk and probably a few more, i can‚Äôt remember now mentioned mismatch at the end of the walk‚Äôs loop happened every time i chose this strategy. i tested it out for outdoor and indoor scanning and in both variants, the result was the same ‚Äî a mismatch! probably you wonder what was the value of it, it varied from 20cm to 120cm. at that point, my impression drastically changed 180 degrees. a very important factor is the walk duration. in my cases, walks took from approx. 7 to 23 minutes. i was assured by leica authorities that i can walk as long as i want/need. i don‚Äôt think 7 or 15 minutes of walk is long for mls equipment and it should be doable to deliver consistent data. i also think, that in a blk2go case, 30 or 60 minutes walk is too long in a matter of such a small imu implemented in this device. unfortunately, leica cyclone doesn‚Äôt give any chance to fix drifts, straight up a trajectory, or remove some unfixable pieces of data. the user has three options send the data to leica support and pray for it to be repairable take a walk again ‚Äî can be costly to get back on the scanning spot create super-smart workflow small digression: i had a chat on one of facebook‚Äôs groups with the very satisfied blk2go user regarding super-smart workflow. he told me that he with his colleagues created a very good workflow consist of their scanning technics and max. 3 minutes blk2go walks which gives them satisfying results. he didn‚Äôt want to share the details, which i fully respect but mentioned that it took approx. a year to achieve a stable and reliable workflow. you, my dear reader, can decide for yourself is it long or not. i believe it is an expensive journey because during that year i can only imagine how many tests they did, how many projects they have redone, how many times mls data had to be supplemented or replaced by tls data. let‚Äôs leave the digression about building a workflow and go back to the two other options in the case of the mismatched point cloud. personally, i tried both. i have redone a walk of the same area twice‚Äî two floors of a parking lot connected by two separate staircases and a very wide parking ramp. each of these three attempts failed, so i can say it will not always be a solution to get good scanning results with a clear conscience. i did twice the same walk of another area. that was outdoor scanning of building facades ‚Äî a mix of bricks, concrete, glass, and steel with a predominance of bricks. results were the same as in the parking example. you probably wonder how did i know i should redo scanning? in the parking, i noticed a mismatch in the blk2go live app, in the outdoor scanning i just did it to be sure. i also took advantage of the third option ‚Äî ask leica support for help. i shared with them two samples of my raw data with the smallest misalignment ‚Äî one of outdoor and one of indoor scanning. after almost a week later i got back the results of support‚Äôs data repairing. they managed to decrease error in the outdoor walk and couldn‚Äôt do anything with an indoor walk (parking lot). below you can read the message i got. outdoor walk‚Äôs error was changed from horizontal misalignment to horizontal (smaller than before) and vertical misalignment. the issues of misalignment in the point clouds are most due to the scenes that are challenging for slam as combination of few features, long walk and for one case also initialization phase that was not ideal. in the following, you will find all the details for the datasets you shared: customer 1 ‚Äî outdoor: ‚Äúoutside.b2g‚Äù no issues are found in the dataset, and it‚Äôs switched to fallback option. i believe the offset is coming due to the complexity of the scan environment where we see quite of downhill & uphill areas. fortunately, we resolved this offset with parameter tuning as the below slices of the misalignment-free point cloud. customer 1 ‚Äî indoor: ‚Äúgarage5‚Äì4floor.b2g‚Äù in terms of misalignment in the slam solution, the scan environment is quite challenging for slam due to less features available (barely empty parking lot) and ramp (which is the most difficult part for slam). in addition, we have seen some issue during initialization of the device. this and the complexity of the scene led to slam to fail. pict. outdoor dataset: ‚Äúoutside.b2g‚Äù pict. outdoor dataset: ‚Äúoutside.b2g‚Äù pict. indoor dataset: ‚Äúgarage5‚Äì4floor.b2g‚Äù as i mentioned, the result of fixing by the leica support ended up with errors you can check in the pictures below. i also don‚Äôt agree with the support‚Äôs comment regarding the indoor walk. i did three similar walks with the same results. i rather wait extra 5‚Äì10 seconds before beginning a walk with the scanner than disturbed the initialization process so this could happen as an internal scanner‚Äôs failure, i suppose. pict. 12cm error in xy plane, reduced from 130cm error by leica support. pict. when the error in the xy plane was reduced, a new error was created in the z plane - approx. 20cm. so what would happen in the case of an unfixable point cloud? probably you should go back to the project‚Äôs area to do acquisition one more time. of course, it is up to you how would you fix this issue but in my case i did the scanning with rtc360 in the case of blk2go data failure. another outdoor test walk it was one of many walks with blk2go but this one was very simple. that‚Äôs why i take it as an example of another outdoor test. i walked around the church on saturday afternoon. pict. top view of the scanning area. in the center is the scanned church. the green line represents trajectory. there was light car traffic on the street and some people walking nearby the church. i would describe it as a lazy saturday‚Äôs area. the acquisition took approx. 4 minutes on the distance of 200m so you can assume it was a very slow walk. the overall quality of the point cloud seemed to be good until i found layering of the point cloud. i found it by slicing the data with 20cm vertical and horizontal cross-sections. the misalignment among points reached 5cm. pict. the vertical cross-section through the front part of the church. notice dy values indicate layering. pict. front view of the vertical cross-section. the noticeable layering of the point cloud. pict. top view. the visible layering of the point cloud (another spot than the one above). pict. range of the collected data (mostly up to 20 meters). the last test after the firmware 2.0.2 update in mai 2021, i tested blk2go last time. the reason was an updated firmware which supposes to fix most of the known issues. were done two walks in a parking lot (another than previously) and one outdoor walk. pict. perspective view on the outdoor data. indoor point clouds looked descent, unlike outdoor data. the walk around the parking lot‚Äôs building took approx. 10 minutes and 350m. it was closed-loop - i finished the walk in the same spot where i started. pict. 350m walk took 10min and 32 sec. on the face of it, the point cloud generated from this walk looked proper. cross-sections have shown again some misalignments/layering. this time the error equaled 50cm! the point cloud of the beginning and the very end had some rotation in relation to each other. pict. marked spot with the 498mm error. pict. top view with shown layering. surprisingly enough, my colleague in leica imported the same data also to leica cyclone 360 but couldn‚Äôt find any error in the same spot. this is odd! i decided to reproduce the same error and imported the same data again. unfortunately, i have updated cyclone 360 to the newest version in the meantime, and the reimport didn‚Äôt contain the previous error. was it the incompatibility of software and firmware versions? i have no idea. anyway, an error was still there near the spot of the walk‚Äôs beginning/end. this error was about 10cm and was along the trajectory. it would signalize that imu had a shift at some point and the walk‚Äôs loop didn‚Äôt close. yes, it is not a big deal, but when you look at the issue from a wider perspective, you cannot be sure when and how errors happen, and how to avoid them. perhaps, performing a multitude of measurements would show some correlations between errors and the acquisition's environment or the method of data collection (fast vs. slow walking, the way of scanning narrow places, etc.). pict. the top view shows two (the same) walls - layering. pict. the horizontal cross-section shows the same error. as the last issue, i noticed, were differences between indoor point clouds. each of them comes from two different levels of the parking lot. a common area of them is approx. 30m of the driveway and it was scanned in each of these walks. that means i had 30 meters of common point cloud for both levels which i could use for constraining them. i gave a try for these point clouds several times and, i can promise you, it was impossible to merge them properly. first of all, they didn‚Äôt match each other because of some bending. pict. bending one of the point clouds. second of all, on these common 30 meters, one of the scans was longer approx. +/- 1,5cm. not a big deal but i wonder would this error multiply in the case of 300 meters? would we get 15cm error then? pict. comparison between indoor scans in the common 30 meters. notice the yellow color on almost the entire walls in the right driveway. useability. panorama pictures i would start this topic from take a look at the photos produced by cameras. blk2go has three 2mpx built-in cameras which are responsible for producing panoramas. my first thought was: ‚Äúreally? only 2mpx at times when not even high-end smartphones have 108mpx?‚Äù pano-images don‚Äôt cover 360 degrees range of view. the area where is placed scanner‚Äôs operator during a walk is not covered in the panoramas (i think it is about 60 to 70 degrees). i think it is a good solution that reduces the time of blurring/editing each individual picture in the case we don‚Äôt want to have an operator in the pictures. on the other hand, the operator has to always remember to change the walking direction so the entire scanning area is covered by pictures. unfortunately, low-quality cameras are good enough for colorizing a point cloud but rather useless for a truview which can be a huge blockage for some of leica‚Äôs customers. in good weather/light conditions, not too bright, not too dark, pictures are just ok, with poor readability, especially when you zoom. another issue with the pano images is that most of the shakes during acquisition are visible as a type of blurryish artifacts. the third disadvantage is an ugly connection between single pictures in each panorama. perhaps i am too tacky but according to me if any other laser scanner from leica‚Äôs stable delivers (except older c and hds-series) beautiful pano pictures then a user is used to the quality. we have 2021 and brands like leica cannot deliver low-quality products. no hdr ugly joins in stitching areas of single pictures very low quality of panoramas shaking artifacts visible on panoramas the very important thing that needs to be mentioned is that a user to be able to export waypoints with a point cloud needs a publisher pro license! at least in the case of exporting to e57 without importing to the cyclone. pict. poor stitching between single pictures to a single panorama pict. 3x zoom. visible artifacts, texture errors/missed pixels, and unstraight edges pict. poor quality in low-light conditions. high overexposure. pict. stitching errors and different exposure levels between subpart pictures. pict. fatamorgana effect. usability. point cloud when we consider a point cloud and its quality we have to look at two cases: accuracy and readability. accuracy, as you could read earlier, is random. you can‚Äôt predict how accurate the point cloud you will get. would it be misaligned or not, leveled or not? if you want to use it for any other purpose than visualizing, you have to consider it twice if it is worth using blk2go. readability has to be considered from the project‚Äôs perspective. in construction projects where the focus is only on the walls, floors, ceilings, beams, etc. which are relatively spacy then the filtered point cloud is mostly suitable for bim modeling. things get tough when the project requires the modeling of small elements like e.g. fire sprinklers. no matter what filtering level would be used, blk2go will not deliver details like these. this is nicely shown in the pictures presenting the underground parking lot area. on the photograph is a visible floor drain. the same area is shown from the same perspective in the point cloud (without filter applied) where mentioned the floor drain is not recognizable. that‚Äôs how unreliable data is, concerning detail modeling. pict. a floor drain is visible next to the pillar. pict. zoom to a floor drain pict. the same area in the unfiltered point cloud. the arrow shows the spot where is a floor drain. pict.black and white pipes. pict. the same area in the unfiltered point cloud with visible only white pipes. black color absorbed the laser light. usability. targets until now, we know that the point cloud is not highly detailed. would be good to supplement its data with the tls point cloud. the best and most accurate way is to use targets (b/w, spheres, or blinkers). unfortunately, leica failed again. none of those types of targets are recognizable in the blk2go point cloud. no matter what the walk‚Äôs velocity is (you can even stop by next to a target), the distance between the scanner and a target, or scanning environment - outdoor or indoor, either manual or automatic target recognition in cyclone software is not possible. it is most likely to happen because of a lack of points representing a target in a point cloud. summary blk2go is a cool device and it‚Äôs simplicity lower the entry threshold for most enthusiast of laser scanning. unfortunately, the data quality and usability of it are low. bim modeling based on this data would be difficult or very difficult depends on lod. pano pictures (waypoints) for truview are mostly useless. in my opinion, the user never knows when can rely on a point cloud generated by this device. a genuine engineering project needs precise 3d data which can be used in the entire bim cycle. lidar 3d laser scanner 3d scanning enterpreneur architects -- -- written by lukasz wiszniewski 8 followers ¬∑ 0 following i am a geomatician and software developer with over decade experience in reality capturing and bim. more info read on 3d-points.com no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/nerd-for-tech/apache-spark-installation-how-to-use-it-31ab4768634a,,,"apache spark ‚Äî installation & how to use it | by arif zainurrohman | nerd for tech | medium sitemap open in app sign up sign in medium logo write sign up sign in nerd for tech ¬∑ nft is an educational media house. our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. to know more about us, visit https://www.nerdfortech.org/ . apache spark ‚Äî installation & how to use it arif zainurrohman 6 min read ¬∑ mar 10, 2021 -- listen share dag visualization nowadays, many social applications are being developed that result in massive data improvements every time, and when we talk about the millions of users who connect every time, information is shared whenever users interact with social media or other websites, so the question arises that how this huge amount of data is handled and through what media or tools the data is processed and stored. this is where big data unfolds. so the first question is what is big data? big data is a term that describes the large volume of data ‚Äî both structured and unstructured ‚Äî that inundates a business on a day-to-day basis. but it‚Äôs not the amount of data that‚Äôs important. it‚Äôs what organizations do with the data that matters. big data can be analyzed for insights that lead to better decisions and strategic business moves. the concept of big data has been around for years and now understands that if they capture all the data, they can implement analytics and benefit significantly from it. with great interest and investment in big data technology, fields such as data analytics and data engineering have the most valuable value. some big data tools are based on python and java making it easier for programmers who are already working on these languages to easily learn about big data analytics and analytics tools. with the help of visualization tools such as power bi, qlikview, tableau, etc., users can easily analyze data and present new strategies. but before that, of course, it is necessary to do good and correct data processing against large data and a lot of it. for that apache spark is present. apache spark apache spark‚Ñ¢ is a unified analytics engine for large-scale data processing. apache spark is an open-source unified analytics engine for large-scale data processing. spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. originally developed at the university of california, berkeley‚Äôs amplab, the spark codebase was later donated to the apache software foundation, which has maintained it since. speed run workloads 100x faster. apache spark achieves high performance for both batch and streaming data, using a state-of-the-art dag scheduler, a query optimizer, and a physical execution engine. speed 2. ease of use write applications quickly in java, scala, python, r, and sql. spark offers over 80 high-level operators that make it easy to build parallel apps. and you can use it interactively from the scala, python, r, and sql shells. load data 3. generality combine sql, streaming, and complex analytics. spark powers a stack of libraries including sql and dataframes, mllib for machine learning, graphx, and spark streaming. you can combine these libraries seamlessly in the same application. generality 4. runs everywhere spark runs on hadoop, apache mesos, kubernetes, standalone, or in the cloud. it can access diverse data sources. you can run spark using its standalone cluster mode, on ec2, on hadoop yarn, on mesos, or on kubernetes. access data in hdfs, alluxio, apache cassandra, apache hbase, apache hive, and hundreds of other data sources. runs everywhere installation a) choose a spark release b) choose a package type c) choose a download type: (direct download) d) download spark. keep in mind if you download a newer version, you will need to modify the remaining commands for the file you downloaded. make sure you have java 7+ installed on your machine. download set environment setx spark_home ~your folder\spark-3.1.1-bin-hadoop2.7 setx hadoop_home ~your folder\spark-3.1.1-bin-hadoop2.7 setx pyspark_driver_python ipython setx pyspark_driver_python_opts notebook add ~your folder\spark-3.1.1-bin-hadoop2.7\bin to your path. environment environment run run the ‚Äî master parameter is used for setting the master node address. here we launch spark locally on 2 cores for local testing. spark 3.1.1 simple practice open new jupyter notebook. if you are new to spark and need to have a setup of spark in window‚Äôs machine using jupyter notebook, then complete the setup first. create an entry point to access spark through spark session. create access and then, we can go ahead a create an input data frame by reading the csv file. follow the below snippet of code to create a spark data frame. data frame spark ui apache spark provides a suite of web ui/user interfaces (jobs, stages, tasks, storage, environment, executors, and sql) to monitor the status of your spark/pyspark application, resource consumption of spark cluster, and spark configurations. we can click spark ui spark ui spark jobs tab the details that i want you to be aware of under the jobs section are scheduling mode, the number of spark jobs, the number of stages it has, and description in your spark job. jobs tab stages tab we can navigate into stage tab in two ways. 1. select the description of the respective spark job (shows stages only for the spark job opted) 2. on the top of spark job tab select stages option (shows all stages in application) in our application, we have a total of 7 stages. the stage tab displays a summary page that shows the current state of all stages of all spark jobs in the spark application the number of tasks you could see in each stage is the number of partitions that spark is going to work on and each task inside a stage is the same work that will be done by spark but on a different partition of data stages stage detail details of stage showcase directed acyclic graph (dag) of this stage, where vertices represent the rdds or data frame and edges represent an operation to be applied. dag visualization conclusion the importance of big data analytics leads to intense competition and increased demand for big data professionals. therefore, it becomes important for a professional to always keep up with the development of technology and always add expertise. apache spark is here to help face the challenges of this big data era. references big data: what it is and why it matters | sas apache spark‚Ñ¢ ‚Äî unified analytics engine for big data apache spark ‚Äî wikipedia spark web ui ‚Äî understanding spark execution ‚Äî sparkbyexamples install spark on windows (pyspark) | by michael galarnyk | medium how to view full content of dataframe in apache spark (learntospark.com) pyspark ‚Äî word count example ‚Äî python examples big data python apache spark hadoop etl -- -- published in nerd for tech 12.5k followers ¬∑ last published 3 days ago nft is an educational media house. our mission is to bring the invaluable knowledge and experiences of experts from all over the world to the novice. to know more about us, visit https://www.nerdfortech.org/ . written by arif zainurrohman 150 followers ¬∑ 74 following corporate data analytics. enthusiast in all things data, personal finance, and fintech. no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/agile-lab-engineering/spark-remote-debugging-371a1a8c44a8,,,"spark remote debugging. hi everybody! i‚Äôm a big data engineer @‚Ä¶ | by lorenzo pirazzini | agile lab engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in agile lab engineering ¬∑ ideas go software! spark remote debugging lorenzo pirazzini 8 min read ¬∑ dec 15, 2020 -- listen share hi everybody! i‚Äôm a big data engineer @ agile lab , a remote-first big data engineering and r&d firm located in italy. our main focus is to build big data and ai systems, in a very challenging ‚Äî yet awesome ‚Äî environment. in agile lab we use in various projects, among other technologies, apache spark as a processing engine. spark is fast and it is simple for a developer to write some code that can be run right away, but as for regular programs, it is not always easy to understand what a spark job is doing. this article will focus on how a developer can remotely debug a running spark scala/java application (running on yarn ) using intellij idea , but all the spark and environment configurations hold also for other ides. agent jdwp, licence to debug to perform remote debugging of a spark job, we leverage the jdwp agent (java debug wire protocol) that defines a communication protocol between a debugger and a running jvm. jdwp defines only the format and layout of packets exchanged by the debugger and the target jvm, while the transport protocol can be chosen by the user. usually, the available transport mechanisms are shared memory ( dt_shmem ) and socket ( dt_socket ) but only the latter, which uses a tcp socket connection to communicate, can be used for remote debugging. so in order to enable remote debugging, we must configure the target jvm with the following java property in order to make it acting as a jdwp server to which our ide can connect: -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4747 the property above tells the jvm to load the jdwp agent and wait for a socket connection on the specified port. in particular: transport=dt_socket tells the agent to use socket as the desired transport mechanism. server=y means that the jvm will act a jdwp server: it will listen for a debugger client to attach to it. suspend=y tells the jvm if it must wait for a debugger connection before executing the main function. if this is set to false (n), the main function will start while listening for the debugger connection anyway. address=4747 specifies the port at which the debug socket will listen on. in the example, the target jvm will listen on port 4747 for incoming client connections. we will leverage the jdwp agent for all the following remote debugging scenarios, so remember that you can always adjust the configurations listed above to fit your use case. you must choose your spark deployment ‚Ä¶but choose wisely before delving into the debug of your application, here‚Äôs a quick recap of how a spark job executes on a cluster; each spark job requires: * a process called driver that performs all the standard java code * one or more executor processes that will perform all the code defined inside the transformations and actions of the rdds/datasets. this means that in a realistic scenario we will have different jvms running at the same time (often on different nodes): one for the driver and one for each executor . a typical spark application will run on multiple nodes, each containing one or more processes spark allows the developer to run a job in different modes depending on the requirement of the desired use case. in particular, we will focus on two configurations of the spark-submit command: deploy-mode and master . those two configurations allow a developer to decide how the spark application will be deployed and run: master : this configuration tells spark which is the master url of the cluster. a complete list of all the allowed values can be found in the official spark documentation . deploy-mode : whether to deploy your driver on one of the worker nodes ( cluster ) or locally as an external client ( client ). in client mode, the driver is started in the same node where the spark-submit is launched. even in one of the allowed value for master is local , which allows running the application on the local machine specifying how many threads should be used, we will not explore it: there is no need for remote debugging if the spark application runs with master local since everything runs on the same local jvm. this article will focus only on spark applications launched with a spark-submit run against a yarn cluster ( master yarn configuration), but the same considerations will hold also for other resource managers. your honour, my client pleads not guilty when the spark-submit command is invoked with client deploy-mode , spark will spawn the driver in the client process that performed the submit command. executors will spawn into nodes of the cluster depending on the resources associated with them. in client mode, the driver is spawned in the same process used to start the spark-submit command if you are performing the spark-submit command from an edge node of your cluster, you can debug the driver code by simply passing the jdwp agent configuration as a driver extra java option: spark-submit --class org.example.myjob \ --master yarn \ --deploy-mode client \ --conf ""spark. driver .extrajavaoptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747"" \ myjob.jar the command above will start the spark job with the driver running on the edge node and listening on port 4747 (this value is arbitrary, you can choose any available port number here). now we can setup our ide to start a remote debug towards the edge node ip and port 4747: define a remote debug configuration with the edge node ip as host we can start debugging our code with the configuration just defined by clicking on the debug icon: always check that the chosen debug configuration is the remote one code execution will stop at each breakpoint we define in our code; remember that since we are debugging the driver we can set up breakpoints anywhere in our code except for the code defined inside rdds/datasets, that will be performed by the executors. you and what nodes army? to debug the executor code we can focus on how a spark job behaves in cluster deploy-mode : the driver is spawned into one of the cluster nodes as well as the executors. so in this configuration connecting to the driver or one of the executors will require us to check where the processes are actually running. in cluster mode the driver will be spawned in one of the cluster nodes, as done for the executors if we need to debug the driver we can pass the same configurations presented above for the client mode, while if we need to debug one of the executors we should pass the agent properties in the executor options instead of the driver options: spark-submit --class org.example.myjob \ --master yarn \ --deploy-mode client \ --conf ""spark. executor .extrajavaoptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747"" \ myjob.jar as discussed, in cluster mode if we want to debug the driver or one of the executors we first need to find out where the actual process is running. to do so we can leverage the spark ui: whenever we start the job we can access its spark ui using the link printed by the spark-submit command. then we can access the executors section of the spark ui , where all the running processes associated to our job are listed. from here we can see the driver node and all the nodes where executors are running, so we can find the executor ip and use it in our ide debug configuration. in this case, the driver was spawned on cluster.node2 and the single executor on cluster.node1 at this point, we need to check the spark ui to find out the ip addresses of the desired nodes. since we defined the port in the agent configuration, we should keep the same debug port, changing only the ip address. if we need to debug the executor code, since in the spark ui we saw that the executor was cluster.node1, we can set that address in the remote debug configuration to simplify the debugging mechanism it is advised to start your spark job with only one executor: debugging a spark job with multiple executors dramatically increases its complexity. for example, if two executors are spawned on the same node, they will have the same ip address and debug port, so it could lead to inconsistencies. for debug purposes, you could scale it down to only one executor. spark-submit --class org.example.myjob \ --master yarn \ --deploy-mode client \ --num-executors 1 \ --conf ""spark.executor.extrajavaoptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747"" \ myjob.jar if we decide to debug one of the executors, code execution will stop at each breakpoint we define inside rdds/datasets transformations and actions; remember that since we are debugging the executor, all breakpoints set up outside rdds/datasets will not be reached since that code will be performed only by the driver process. in a cluster far, far away‚Ä¶ in a real scenario, the spark job will run inside a cluster which is not accessible from the outside world: you can only access a cluster edge node, but you are not able to access directly the nodes where the driver and executors processes will run. the edge node can access the nodes since it is co-located in the cluster. in a real scenario, the cluster is not accessible from the outside and you can only communicate with the edge nodes of the cluster in this situation we need to take advantage of a mechanism called port forwarding: we can forward the port of our target node to a port of the edge node. in this way, we can use our edge node (for which we have access) as a proxy for the desired target node (for which we don‚Äôt). a very common way to perform the port forwarding is to use ssh : ssh -l 4848:target.node:4747 user@edge.node the command above, run from your local machine, will connect to the edge node ( edge.node ) with the specified username ( user ). obviously, you must provide your identity to the remote machine, the edge node, using one of several methods depending on the protocol version used. the -l specifies that the given port on the local host ( 4848 ) has to be forwarded to the given host ( target.node ) and port ( 4747 ) on the remote side. in summary, in order to perform remote debugging of your code for a spark job running in a cluster like the one described above you will need to: add the desired agent configuration to the spark-submit command start the job open the spark ui and find out where your process is running use the ssh command to forward the port specified in the agent from the target node to your local machine through the edge node start the remote debug from your ide using as ip and port localhost and the forwarded port since we forwarded the remote port 4747 to our local port 4848 we can edit the remote debug configuration to listen for localhost on port 4848 if you made it this far, you may be interested in other spark-related articles that you can find on our blog . stay tuned because other spark articles are coming! spark debug cluster java ssh -- -- published in agile lab engineering 287 followers ¬∑ last published jun 20, 2025 ideas go software! written by lorenzo pirazzini 36 followers ¬∑ 4 following senior big data engineer at agile lab no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@adilqayyumk/big-data-qa-a-concise-overview-becc74de5760,,,"big data & qa ‚Äî a concise overview | by adil qayyum | medium sitemap open in app sign up sign in medium logo write sign up sign in big data & qa ‚Äî a concise overview adil qayyum 5 min read ¬∑ jul 4, 2019 -- 1 listen share assume you must test 100 tbs of unstructured, unindexed data that has no cache attached. can you feel the panic arising from all the things that could go wrong with this project? are bottlenecks and slow processing the first things that come to mind? add uncleaned data, unknown errors, transmission faults and ensuring that operations are applied to the whole volume, and you are still not even close to what big data testing means. what is big data first let‚Äôs get our definition straight on what constitutes big data. a common approach is to define big data (or the lack thereof) in terms of the 3v‚Äôs of data: volume , velocity , and variety . high volume is the biggest clue but not the only one. velocity ‚Äî speed of creation ‚Äî is critical, as is a wide variety of data types thanks to unstructured data. unlike structured data, unstructured data does not have a defined data model. unstructured data includes social media like twitter and facebook, email and chat applications, video and audio files, digital photos, voicemail, call center records, and photos. in these are just human-generated files. once you get into machine-generated files then you‚Äôre talking about massive and fat-growing volumes of data. key components of big data application testing as big data is described through the above-mentioned three vs, you need to know how to process all this data through its various formats at high speed. this processing can be split into three basic components. to be successful, qa engineers will have to be aware of these components. 1. data validation: understandably, this is one of the most important components of data collection. to ensure the data is not corrupted or is accurate, it is important that it is validated. for this purpose, the sources will be checked. the information procured is validated against actual business requirements. the initial data will be fed into hadoop distributed file system (hdfs), and this will also be validated. the file partition will be checked thoroughly, followed by copying them into different data units. tools like datameer, talent and informatica are used for step-by-step validation. data validation is also known as pre-hadoop testing, and makes it certain that the collected data is from the right resources. once that step is completed, it is then pushed into the hadoop testing system for tallying with the source data. 2. process validation: once the data and the source are matched, they will be pushed to the right location. this would be the business logic validation or process validation, where the qa engineer will verify the business logic, node by node, and then verify it against different nodes. business logic validation is the validation of map reduce, the heart of hadoop. the qa engineer will validate the map-reduce process and check if the key-value pair is generated correctly. through ‚Äúreduce‚Äù operation, the aggregation and consolidation of data is checked out. 3. output validation: output validation is the next important component. here the generated data is loaded into the downstream system. this could be a data repository and the data goes through analysis and further processing. this is then further checked to make sure the data is not distorted, by comparing hdfs file system with target data. architecture testing is another crucial part of big data testing, as having poor architecture will make the whole effort go wasted. luckily, hadoop is highly resource intensive, and is capable of processing huge amounts of data and for this, architectural testing becomes mandatory. it is also important to ensure that there is no data corruption, and compare the hdfs file system data with target ui or business intelligence system. big data testing challenges this process requires a high level of automation given massive data volumes, and the speed of unstructured data creation. however, even with automated tool-sets big data testing isn‚Äôt easy. good source data and reliable data insertion: ‚Äúgarbage in, garbage out‚Äù applies. you need good source data to test, and a reliable method of moving the data from the source into the testing environment. test tools require training and skill: automated testing for unstructured data is highly complex with many steps. in addition, there will always be problems that pop up during a big data test phase. qa engineers will need to know how to problem-solve despite unstructured data complexity. setting up the testing environment takes time and money: hadoop eases the pain because it was created as a commodity-based big data analytics platform. however, it still needs to buy, deploy, maintain, and configure hadoop clusters as needed for testing phases. even with a hadoop cloud provider, provisioning the cluster requires resources, consultation, and service level agreements. virtualization challenges: few business application vendors do not develop for virtual environments, so virtualized testing is a necessity. virtualized images can introduce latency into big data tests, and managing virtual images in a big data environment is not a straightforward process. no end-to-end big unstructured data testing tools: no vendor tool-set can run big data tests on all unstructured data types. qa engineers need to invest in and learn multiple tools depending on the data types they need to test. big data automation testing tools testing big data applications is significantly more complex than testing regular applications. big data automation testing tools help in automating the repetitive tasks involved in testing. any tool used for automation testing of big data applications must fulfill the following needs: allow automation of the complete software testing process since database testing is a large part of big data testing, it should support tracking the data as it gets transformed from the source data to the target data after being processed through the mapreduce algorithm and other etl transformations. scalable but at the same time, it should be flexible enough to incorporate changes as the application complexity increases integrate with disparate systems and platforms like hadoop, teradata, mongodb, aws, other nosql products etc integrate with dev ops solutions to support continuous delivery good reporting features that help you identify bad data and defects in the system conclusion transforming data with intelligence is a huge concern. as big data is integral to a company‚Äôs decision making strategy, it is not even possible to begin asserting the importance of arming yourself with reliable information. big data processing is a very promising field in today‚Äôs complex business environment. applying the right dose of test strategies, and following best practices would help ensure qualitative software testing. the idea is to recognize and identify the defects in the early stages of testing and rectify them. this helps in cost reduction and better realization of company goals. through this process, the problems that qa engineers faced during quality assurance planning & software verification are all solved now because the testing approaches are all driven by data. big data quality assurance software engineering testing information technology -- -- 1 written by adil qayyum 35 followers ¬∑ 3 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/the-telegraph-engineering/dbt-a-new-way-to-handle-data-transformation-at-the-telegraph-868ce3964eb4,,,"dbt: a new way to transform data and build pipelines at the telegraph | by stefano solimito | the telegraph engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in the telegraph engineering ¬∑ the telegraph digital engineering and product team powering telegraph.co.uk , the telegraph mobile apps, google amp, google cloud, amazon echo skills and facebook articles dbt: a new way to transform data and build pipelines at the telegraph stefano solimito 10 min read ¬∑ aug 23, 2019 -- 4 listen share the telegraph is a 164-old-company where data has always had a central role. with the advent of the cloud and the need to build a platform able to process a huge quantity of data in 2015, we started to build our big data platform. we decided to use google cloud and, since delivering our first poc, over the years we have kept improving our platform to better support the business. the challenge during the last 4 years, i had multiple discussions on how to handle data transformation or, more extensively, (etl) extract transform and load processes. the number of tools that you can choose on the market is overwhelming and committing to the wrong technology could have a negative impact on your capabilities to effectively support different business units and drive decisions that are based on reliable figures. at the telegraph, the datalake has been built on the top of cloud storage and bigquery and, according to google, the natural choice to perform etl or elt should be dataflow ( apache beam ) . for most companies, this might be true. but when you go outside the general use-cases exposed in the ‚Äúgetting started‚Äù guides and you start to relate with real-world challenges, what is supposed to be an easy choice might be not so easy . in our case, adopting apache beam has proven not to be the easiest solution for the following reasons: java sdk is much more supported than python sdk and most of the members of our team have already large expertise in python, but they are not proper java developers. also, our data scientists only work in python and it would mean having codebases in multiple languages to support, making it hard to rotate engineers across different projects. most of our batch data pipelines are structured to be just a series of queries to run against bigquery. considering this, apache beam doesn‚Äôt add much value on the etl process. dataflow connects really well with google products, but in 2015 the number of connectors was limited and we needed to interact with aws, on-premise servers etc. our analysts and data scientists tend to speak in sql and it‚Äôs much easier to collaborate with them if, in engineering, we don‚Äôt need to translate the sql logic that they are producing in java or python. as a side note, we adopted apache beam in a second phase, but only for real-time data pipelines. in fact, in this specific case having a windowing strategy and being able to perform operations on a stream of records was paramount for the success of certain projects. the second product you might be tempted to use if you are using google cloud platform (gcp) might be dataproc . if you already have a spark cluster or hadoop in place and you want to migrate on gcp, it would make perfect sense to consider this option. but in our case, we only had a small hadoop cluster and to rewrite the logic of the pipelines that were running there was not a problem. the third product that we considered and even used for a while is talend (free version). if your company wants to fully commit to talend and buy its enterprise version then it is a great choice, but if you don‚Äôt have a strong enough case and you decide to adopt the free version, you might face some of the following challenges: it is hard to apply version control to your data pipelines. you have to come up with your own ci/cd and the testability of your artefacts is limited. you have to rely on components provided by the community that might become outdated and eventually not supported. or you might decide to develop your own components committing resources on building and keeping them up to date. you have to find talend experts who are able to drive the development, enforcing best practices to produce high-quality pipelines. if the decision to adopt the tool to perform etl is not shared with the entire company you could end with a scattered set of technologies that you have to maintain in a few years that are performing similar tasks. for the reasons above we considered building our own python etl library as a wrapper of functionalities provided by google and aws in order to make our life easier when interacting with the main services of both clouds. even this approach has proved to be far from perfect. due to the effort required to design and develop our own library and all the maintenance required to keep it updated and include new features, we started to look for something that could integrate well with this approach and reduce the scope of the library. in june 2019, we started to test dbt for the transformation part with the idea of continuing to perform extraction and load using the python library and relying on apache beam for real-time data processing. what is dbt? dbt ( data building tool ) is a command-line tool that enables data analysts and engineers to transform data in their warehouses simply by writing select statements . dbt performs the t (transform) of etl but it doesn‚Äôt offer support for extraction and load operations. it allows companies to write transformations as queries and orchestrate them in a more efficient way. there are currently more than 280 companies running dbt in production and the telegraph is among them. dbt‚Äôs only function is to take code, compile it to sql, and then run against your database. multiple databases are supported, including: postgres redshift bigquery snowflake presto dbt can be easily installed using pip (the python package installers) and it comes with both cli and a ui. dbt application is written in python and is opensource , which can potentially allow any customization that you might need. the cli offers a set of functionalities to execute your data pipelines: run tests , compile , generate documentation, etc. the ui doesn‚Äôt offer the possibility to change your data pipeline and it is used mostly for documentation purposes. in the image below you can see how data lineage of a certain table is highlighted in dbt ui. this helps to quickly understand which data sources are involved in a certain transformation and the flow of the data from source to target. this type of visualisation can facilitate discussion with less technical people who are not interested in the detailed implementation of the process, but want an overall view. initialising a project in dbt is very simple; running ‚Äúdbt init‚Äù in the cli automatically creates the project structure for you. this will ensure that all engineers will work with the same template and thereby enforces a common standard. dbt also offers the maximum flexibility and if, for some reason, the project structure produced doesn‚Äôt fit your needs, it can be customised by editing the project configuration file (dbt_project.yml) to rearrange folders as you prefer. one of the most important concepts in dbt is the concept of model . every model is a select statement that has to be orchestrated with the other models to transform the data in the desired way. every model is written using the query language of your favourite data warehouse (dw). it can also be enriched using jinja2 , allowing you to: write tidier parameterised queries . encapsulate any functionality that you might want to recycle in macros that can be used as functions in your queries. hide the complexity of your transformation to allow the reader to focus on the logic itself. below an example of a model using bigquery standard sql syntax. in this specific case, jinja is used to inject in the output a set of technical rows that are not present in the source. jinja allows iteration on the matrix of values and reshapes each row in a way that can be included in the main table, enabling you to keep your query more concise. the output of each model can be stored in different ways, depending on the desired behaviour: materialise a table ‚Äî full refresh append to a table ‚Äî incrementally build your output ephemeral ‚Äî the output will not be stored in your dw but it can be used as a data source from other models. every dbt model can be complemented with a schema definition . this means that the documentation lives in the same repository as your codebase, making it easier to understand every step of what has been developed. also, since the documentation is always in front of the engineers who are working on the pipelines and since it is directly generated from the codebase, minimal maintenance is required to keep it up to date. in data-driven companies, ensuring the quality of the data delivered to the business is always of paramount importance. dbt helps to serve high-quality data, allowing you to write different typologies of tests to check your data . simple tests can be defined using yaml syntax, placing the test file in the same folder as your models. for this specific example, the defined tests are: sk_interaction, bk_source_driver are both assuming unique values and are never null count_interactions is never null fk_interaction_detail is not null, assumes unique values and all the foreign keys fk_interaction_detail are able to join with the surrogate keys sk_interaction_detail in the dimension interaction detail. this is called referential integrity test and helps to ensure that your star schema has been built in the proper way. fk_interaction_text has similar test criteria. performance_band can assume only the defined array of values. more advanced testing can be implemented using sql syntax. the query below ensures that the ‚Äúbk_source_driver‚Äù field from model ‚Äúfact_interaction‚Äù doesn‚Äôt have more than 5% of the values set as null. models on dbt rely on the output of other models or on data sources. data sources can be also defined using yaml syntax and are reusable and documentable entities that are accessible in dbt models. in the example below, you can see how it is possible to define a source on top of a daily sharded bigquery tables. it is also possible to use variables to dynamically select the desired shard. in this specific case, the ‚Äúexecution_date‚Äù variable is passed in input to dbt and defines which shards are used during the transformation process. dbt offers also the possibility to write your own functions ( macros ) these can be used to simplify models but also create more powerful queries adding more expressive power to your sql without sacrificing readability. the macro in the example below is used to unite multiple daily shards of the same table depending on the ‚Äúexecution_date‚Äù variable passed and the number of past shards we want to take into consideration. conclusions the telegraph‚Äôs data engineering team has tested dbt (core version) for the past two months and it‚Äôs proved to be a great tool for all of the projects that required data transformation. as a summary of our experience, here is a list of the tool‚Äôs pros and cons. pros: it is opensource and open to customization. it is easy to apply version control the documentation lives with your dbt project and it is automatically generated from your codebase. it doesn‚Äôt require any specific skills on the jobs market. if your engineers are familiar with sql and have a basic knowledge of python, that‚Äôs enough to approach dbt. the template of each project is automatically generated running dbt init. this enforces a standard for all of our data pipelines. all of the computational work is pushed towards your dw. this allows you to attain high performance when using a technology similar to bigquery or snowflake. because of the point above, orchestrating a dbt pipeline requires minimal resources. it allows you to test your data (schema tests, referential integrity tests, custom tests) and ensures data quality. it makes it easier to debug complex chains of queries . they can be split into multiple models and macros that can be tested separately. it‚Äôs well documented and the learning curve is not very steep. cons: sql based ; it might offer less readability compared with tools that have an interactive ui. lack of debugging functionalities is a problem, especially when you write complex macros . sometimes you will find yourself overriding dbt standard behaviour, rewriting macros that are used behind the scenes. this requires an understanding of the source code. the ui is for documentation-only purposes . it helps you to visualise the transformation process, but it‚Äôs up to your data engineers to keep the dbt project tidy and understandable. having an interactive ui that allows you to visually see the flow of the pipeline and amend the queries can be helpful, especially when it comes to complex data pipelines. documentation generation for bigquery is time-consuming due to a poor implementation that scans all of the shards inside a dataset. it covers only the t of etl , so you will need other tools to perform extraction and load. if you are interested in practical tips to get the best out of dbt have a look at this series of articles: part 1: https://medium.com/photobox-technology-product-and-design/practical-tips-to-get-the-best-out-of-data-building-tool-dbt-part-1-8cfa21ef97c5 part 2: https://medium.com/photobox-technology-product-and-design/practical-tips-to-get-the-best-out-of-data-build-tool-dbt-part-2-a3581c76723c part 3: https://medium.com/photobox-technology-product-and-design/practical-tips-to-get-the-best-out-of-data-build-tool-dbt-part-3-38cefad40e59 stefano solimito is a principal data engineer at the telegraph. you can follow him on linkedin . big data data engineering google cloud platform data etl -- -- 4 published in the telegraph engineering 941 followers ¬∑ last published apr 3, 2025 the telegraph digital engineering and product team powering telegraph.co.uk , the telegraph mobile apps, google amp, google cloud, amazon echo skills and facebook articles written by stefano solimito 826 followers ¬∑ 8 following head of data engineering @ zeplin responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/engineered-publicis-sapient/scalable-qe-automation-framework-9e442f954ce,Cucumber; JUnit,,"scalable qe automation framework. single solution to perform data testing‚Ä¶ | by publicis sapient | engineered @ publicis sapient | medium sitemap open in app sign up sign in medium logo write sign up sign in engineered @ publicis sapient ¬∑ we fuse startup thinking and agile methods to help established companies increase value, drive efficiencies and thrive in an evolving world scalable qe automation framework publicis sapient 6 min read ¬∑ sep 24, 2019 -- listen share single solution to perform data testing on distributed and non-distributed environment sumit gupta ‚Äî director, technology; rachit sharan ‚Äî manager, qa and prashant gupta , senior associate, platform abstract we are living in a data-driven digital world, where every business decision and analytics depend on the data. we are heavily dependent on it, and its accuracy and quality are of utmost significance. data testing plays an important role in achieving the same. data persists on diverse platforms. hence, a single solution for data testing on different platforms is quintessential. the objective of this paper is to showcase a qe framework which would enable testing for both ‚Äî distributed secured platform (big data) and non-distributed platform (rdbms). why is data testing a challenge for organizations? the complexity and diversity embedded in the data application raises several challenges for testing in many aspects. thus, choosing the right testing empirical can mean the difference between success and failure. a few organization-level challenges are: 1. absence of a common framework across the organization ¬∑ teams work in silos and create utilities to expedite their testing, leading to duplication of work across the organization ¬∑ no common/reusable lib/test cases that can be used throughout the organization ¬∑ diverse innovation stacks and assortment of testing techniques ¬∑ having a etl tool-agnostic framework 2. ingestion of different patterns organizations deal with different patterns of structured, semi-structured and unstructured data (e.g., csv, txt, avro, praquet, json, etc.) and testing all such patterns through a single framework is a major challenge. 3. high volume of data increase in the magnitude of data and the reconciliation of such data are challenges for organizations. verification of the data quality along with the processing of huge volume of data is the key to any testing. 4. testing on distributed platforms organizations are gradually shifting to distributed platforms for faster processing and persistence of data. testing on a distributed platform is a challenge because of: ¬∑ various integration points like rdbms, hive, kafka, nosqls, etc. ¬∑ supports structured and unstructured data ¬∑ way of data processing 5. lack of business knowledge possessing the right business knowledge plays a vital role in organizations. 6. lack of common ways of reporting different teams report their testing results in different manners, which leads to the major organizational challenge of reconciliation and comparison of results. having a common way of reporting as well as a common platform for reporting is key for success of an organization. idea for the framework building scalable distributed qe automation framework solution is developed keeping the above challenges in mind. this helps qes expedite project delivery by providing them a predefined, exhaustive list of user libraries to automate their scenarios. it provides the flexibility of running in local (ide) as well as in distributed environment (cluster). the framework also supports bdd, which enables collaboration between dev and ba. logical architecture of the framework key features in the framework that solves the above challenges: re-usable libraries having a set of reusable libraries which is open to crowdsourcing by having a governance e- model is the key to any framework in the organization. teams can create new libraries and feed into the reusable library repository, which can be used by anybody in the organization for their testing. the updating of the lib can be controlled through a governance model which is a defined code review and approval process. a few samples of reusable libraries are listed below: sample method from reusable library for file exist check in hadoop file system public boolean fileexistinhdfs (string inputhdfspath){ distributedfilesystem dfs = new distributedfilesystem(); dfs.initialize(new uri(‚Äúuri to hdfs‚Äù), new configuration()); return fs.exists(new path(inputhdfspath) } benefits of cucumber with spark this creates a compelling mix when consolidating apache spark for data processing and cucumber for bdd that can show the authority of volumetric data validation with a business focus. this empowers the purpose as well as can get different intends to effectively test situations that have been commonly difficult to demonstrate. ¬∑ cucumber enables us to write a part of our software in a straightforward, language-based strategy and focuses on detailing the outcomes that we want. ¬∑ apache spark is often one of the many components in data processing, which can promote various test frameworks. ¬∑ the system testing can be readily created, read and validated by technical, non-technical and business team members. challenges : choosing the correct dependency for spark and cucumber integration was a major challenge, so marking down the required dependencies. sample construction flow of the project 1. maven-based spark project with cucumber and required libraries ¬∑ cucumber dependency: - info.cukes (for cucumber-java or cucumber-scala) - cucumber-core, cucumber-junit, cucumber-jvm-deps, gherkin - cucumber-reporting, hamcrest-core ¬∑ spark dependency : spark-core, spark-sql, org.scalatest, spark-hive 2. runner file to test bdd feature package com.pack.foo; import cucumber.api.cucumberoptions; import cucumber.api.junit.cucumber; import org.junit.runner.runwith; @runwith(cucumber.class) @cucumberoptions(plugin = {""pretty"", ""html:target/cucumber""}) public class myrunner { } 3. a feature file, this is a file written in an executable language called gherkin. this will have a simple format: 4. our gherkin file is written in plain english which can be easily understood and explained by business as well as non-business users. so now we need to execute it and find out how our plain text understandable file will execute a test. @given(""^ todo list is empty $"") public void todo_list_is_empty () throws throwable { // write code here that turns the phrase above into concrete actions throw new pendingexception(); } 5. the cucumber framework has auto-generated the method we need to implement. now we need to hook our feature file together with the actual calls that are running the functional tests. so, let‚Äôs fill it in, sample snippet: once development is completed, one can submit as a spark application, it will execute cucumber runner class, cucumber will start reading feature file steps and given a statement of the scenario. as soon as cucumber finds given the statement, same given statement will be searched for your source files. if the same step is found in the source file, cucumber will perform the specified function for the same step also. spark will assist in carrying out test cases in a distributed fashion to speed up the execution of the test cases. conclusion quality testing framework is a change agent in data testing, providing the natural link between development and business from the technological and cultural aspects, in parallel to supporting agile development and the devops approach. this means we get a healthy mix of professionals with varying perspectives and solutions, trying to solve roadblocks and business challenges for organizations. testing automation quality engineering data -- -- published in engineered @ publicis sapient 1.1k followers ¬∑ last published jun 18, 2025 we fuse startup thinking and agile methods to help established companies increase value, drive efficiencies and thrive in an evolving world written by publicis sapient 424 followers ¬∑ 5 following a digital transformation partner helping established organizations get to their future, digitally-enabled state, in the way they work and serve their customers. no responses yet help status about careers press blog privacy rules terms text to speech",17
https://medium.com/expedia-group-tech/unit-testing-apache-spark-applications-using-hive-tables-ec653c6f25be,JUnit,Unit Testing,"unit testing apache spark applications using hive tables | by neeraj prithyani | expedia group technology | medium sitemap open in app sign up sign in medium logo write sign up sign in expedia group technology ¬∑ stories from the expedia group technology teams unit testing apache spark applications using hive tables techniques for creating and managing unit tests of spark batch applications neeraj prithyani 4 min read ¬∑ feb 25, 2019 -- 2 listen share at homeaway , we have many batch applications that use apache spark to process data from hive tables based on s3 datasets. these applications perform spark sql transformations to generate their final output. the transformations are written similarly to an sql stored procedure in an rdbms. we wanted to bake unit tests into the intermediate steps to ensure our applications are robust and can catch any breaking changes in the future. however, we faced some challenges. challenges hive tables and s3 datasets are not available on developers‚Äô local machines (or on build servers like jenkins) to run tests without additional setup. the code needs a local spark session to run. we needed a way to get a spark session for local (and jenkins) that does not connect to the hive metastore. the team evaluated many options and established best practice to use the spark-testing-base library for unit testing such applications. below are some key techniques that helped us unit test our spark application. toolset spark-testing-base spark-testing-base is a library that simplifies the unit testing of spark applications. it provides utility classes to create out-of-the-box spark sessions and dataframe utility methods that can be used in assert statements. scalatest scalatest is a powerful tool that can be used to unit test scala and java code. it is similar to junit for java. setup add a dependency for spark-testing-base to your pom.xml : <dependency> <groupid>com.holdenkarau</groupid> <artifactid>spark-testing-base_2.11</artifactid> <version>${spark.version}_0.10.0</version> <scope>test</scope> </dependency> add the scalatest maven plugin to your pom.xml : <plugin> <groupid>org.scalatest</groupid> <artifactid>scalatest-maven-plugin</artifactid> <version>2.0.0</version> <configuration> <reportsdirectory>${project.build.directory}/surefire-reports</reportsdirectory> <junitxml>.</junitxml> <filereports>wdf testsuite.txt</filereports> </configuration> <executions> <execution> <id>test</id> <goals> <goal>test</goal> </goals> </execution> </executions> </plugin> key techniques we used using the out-of-the-box local spark session using trait sharedsparkcontext makes a locally generated spark session available to use without doing anything else. out-of-the-box local spark session we refactored our classes to receive this spark session from the main class using dependency injection. in the production environment, this points to the production hive metastore. during local testing and unit tests this points to the metastore in the jvm. test bed (test database) setup all the databases and tables that the application uses can be defined up front pointing to a temporary location. this can be done in a utility class that does this test bed setup. we will then call this utility class before the tests. moreover, any tables that are needed for the code to work, but not necessarily needed for unit testing, can be defined up front as empty tables. we‚Äôll discuss defining tables with specific data for unit testing in the next section. test bed setup using csv files to populate hive tables we found that using csv was pretty simple for defining data that any of our tables needed for unit testing. once set up, csvs were simple to edit for various data scenarios. steps: use structtype to define the table schema. only define columns that your program uses. there is no point in defining all the columns that are in the hive table if they are not used in our app! use spark csv reader to create a dataframe pointing to csv files stored in test/resources store csv with all data combinations in test/resources use spark‚Äôs saveastable method to define a hive table from this dataframe defining and loading tables for unit tests create csv files in test/resources dataframe assert method trait dataframesuitebase provides method named assertsdataframeequals that can be used to compare two dataframe s. assertdataframeequals method using scalatest matchers to assert dataframe elements scalatest trait matchers provides easy to read assert checks. we used these to test individual elements in the test result dataframes. scalatest matchers conclusion these were some techniques we used to unit test our spark batch applications. there are some features that spark-testing-base provides like generating test datasets, dataframes, and resilient distributed datasets (rdds) that are useful. the streamingsuitebase also looks very promising and easy to use for spark streaming applications. being new to scala and then looking at all the features scalatest provides was refreshing. we used funsuite, but there are many different styles we can explore and incorporate. this was an important step for our team towards improving our unit testing coverage for spark batch applications. testing scala-based spark code snippets in notebooks and then porting them in a scala app improves time to release. we are excited to continue building more unit testing techniques for other spark based applications! apache spark data science unit testing hive scala -- -- 2 published in expedia group technology 5k followers ¬∑ last published jun 3, 2025 stories from the expedia group technology teams written by neeraj prithyani 41 followers ¬∑ 2 following software developer at expedia group. passion for big data processing. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@josh.temple/automated-testing-in-the-modern-data-warehouse-d5a251a866af,,Unit Testing,"automated testing in the modern data warehouse | by josh temple | medium sitemap open in app sign up sign in medium logo write sign up sign in automated testing in the modern data warehouse josh temple 8 min read ¬∑ nov 15, 2018 -- 10 listen share toward a testing philosophy for the data warehouse over time, software engineers have developed a strong philosophy for testing applications. concepts like unit testing, the test pyramid , code coverage , and continuous integration have made application testing robust and have established solid design patterns. good testing practices are taught and practiced in most computer science programs. in my experience, a unified testing philosophy is missing in the data world. as a data professional, i tell people that my goal is to provide accurate and timely information to enhance decision-making. however, if i supply our decision-makers with inaccurate data, they might make far-reaching, strategic mistakes. if our website goes down, it‚Äôs certainly an issue (we could lose customers, revenue, and our brand could be harmed), but generally bugs in application code don‚Äôt have as pervasive consequences as bugs in data. if data issues have lasting consequences, why are we less sophisticated at testing than our software developing counterparts? i think there are a few reasons for this. many data professionals (myself included) were not formally trained as software engineers, but entered the field out of curiosity and the insatiable demand for more data people . when i started my first job as a data analyst, i knew i probably should have been testing my code more, but i didn‚Äôt have a proper understanding of how to do it. testing data is hard ‚Äî data is complex, interrelated, and changes constantly. tutorials on unit testing python class methods didn‚Äôt feel relevant to my sql queries and etl jobs. in fact, many of the testing paradigms developed for software applications break down when applied to databases. testing can feel like a nuisance. it‚Äôs extra work to write and maintain tests, and it can make refactoring harder if you have to refactor your tests in parallel. as a junior analyst, what did i do instead? i usually tested by visual inspection ‚Äî that is, i would write a select * query, check that the first 10‚Äì20 results looked normal, and move on. by doing so, i made the implicit assumption that the rest of my data looked basically the same. when we test this way, we‚Äôre prone to make incorrect assumptions about our data. we might assume that a column only contains values in a given range, that a join didn‚Äôt create fan out, or that a foreign key exists as a primary key in the table it references. these faulty assumptions can have ripple effects, especially when we write downstream code based on them. for example, let‚Äôs say we assumed that the order_id column in our orders table is unique. perhaps we inadvertently introduce a bug into our load job that loads a few duplicate orders from the previous day. if we don‚Äôt test our assumption, we would be slightly over-reporting product sales ‚Äî a slippery bug that may not even be noticed by our end users! the purpose of testing is to validate our assumptions . good tests catch bad assumptions. every time you make an assumption about the state of your data or the result of a query, you should write a test to confirm that assumption. a successful test means we can be more confident that the assumptions we‚Äôve built upon are firm. of course, it‚Äôs impossible to test everything, but with each test, we decrease the risk of bad assumptions that lead to bad decisions. test all the things if we want to be as comprehensive as possible with our testing, where should we begin? logically, when testing a process, you should test its inputs, outputs, and any intermediate stages within. in our case, we should test at the boundary of our warehouse, where data enters and exits, and we should test all internal transformations within. this gives us three specific stages of our process to test: input. extract and load jobs that insert data into or update data in the warehouse. does the input data to our process match our assumptions? transform. queries that transform data within the warehouse. does our process create new data from input data in a way that matches our assumptions? output. queries that extract data from the warehouse for other purposes (e.g. a business intelligence tool or predictive model). does our process give us the data we assume it will, in a format that is useful? this leaves us with our basic approach to data warehouse testing: test assumptions about data entering, being transformed within, and exiting our warehouse. but how do you actually write a data test? anatomy of a data test a data test, which you can compare to a unit test in software development, should have the following characteristics: specific , so it‚Äôs clear what a test failure means. one test that tests three conditions will always be harder to debug than three tests for individual conditions. automated , so it can be put into a test suite and run easily. having an automated test suite means you can quickly assess the data warehouse-wide impact of introducing new sql. fast , so you‚Äôre not waiting forever for the the test suite to finish. if a test takes longer than a minute or so, i‚Äôll attempt to break it up into multiple tests or i‚Äôll test a smaller sample. independent , so it can be run at any time and in any order. as an example, one of the most important tests you can implement is a uniqueness test for your primary keys. data duplication can easily occur across your data warehouse due to errors in loads or improper joins. at milk bar , every table in our warehouse is tested repeatedly for unique primary keys. the query to check for duplicate primary keys is simple. select my_primary_key from my_table where my_primary_key is not null group by 1 having count(*) > 1 if this test returns any rows, we consider the test to have failed and we go fix the duplicate rows. you could also select count(*) and fail the test if the count is greater than zero. at milk bar , we use dbt to manage our data transformations and as our test framework. dbt is a powerful command-line tool that extends sql with jinja templating and allows us to build a large test suite of stock and custom tests with very little effort. with dbt, creating and executing the uniqueness test above is as simple as writing the following configuration and running dbt test . - name: my_primary_key tests: - unique dbt is hands-down the best framework i‚Äôve found for creating a robust, automated test suite for your data warehouse and i highly recommend checking it out if you‚Äôre serious about upgrading testing in your data warehouse. deciding what to test deciding what to test can be difficult, as it requires you to anticipate what could go wrong with your data. this is even more challenging when you‚Äôre testing code that you wrote, where you might have blind spots. i prefer a risk-based approach to testing, which basically says: prioritize testing whatever is most likely to fail with the greatest impact to the business. we think about risk as probability of failure * impact of failure . a null value in a column you don‚Äôt ever use doesn‚Äôt pose much risk to the business, so it‚Äôs reasonable to consider not testing it in favor of a faster test suite. remember, testing is all about assumptions, so start by attacking the assumptions that would really backfire if you were wrong. i generally find reading exhaustive lists of test categories to be less helpful than thinking through my data models on my own. take some time to brainstorm how bad input data could break things, or how buggy transformations could affect calculations of important metrics. with that said, here are a few important assumptions we test every day at milk bar as part of our data loads and during development. is the primary key in this table unique? is [important column name] free of null values? do these foreign keys exist as primary keys in their referenced tables ( referential integrity )? does the numeric data in this table fall within an expected range? do item-level purchase amounts sum to order-level amounts? do the necessary columns for external work exist in the final table? can we preemptively run popular dashboard queries from our business intelligence tool? continuous integration and beyond once you‚Äôve built out a solid test suite and are confident you‚Äôve covered your high-risk assumptions, you should absolutely consider adopting continuous integration (ci). ci is where automated testing starts to get really powerful. the idea behind continuous integration is that it‚Äôs better to test and merge new code into the master branch as soon as it‚Äôs written than to wait until ‚Äúrelease day‚Äù when every developer is attempting to merge an entire sprint‚Äôs worth of work at the same time. this accelerated integration is made possible by a comprehensive, automated test suite ‚Äî if my commits pass all tests (and probably a bit of manual code review), i can be confident that merging them into master should be safe. at milk bar , we use gitlab ci/cd and dbt to manage this. when i push a series of commits to gitlab, gitlab ci/cd launches a pipeline. this pipeline tells dbt to build a replica of our data warehouse in a staging schema and run our test suite against it. if the tests pass and we approve the merge request into master , gitlab ci/cd triggers dbt to automatically rebuild the production schema using the new changes. we‚Äôre still sorting out the best approach, but so far it‚Äôs been an incredible workflow. i can commit modifications to a sql query, push to gitlab, wait for the tests to pass, merge it, and know that my changes will be live in production in a few minutes. ci makes my development cycles faster and increases my confidence in the commits i‚Äôm merging into production. designing an effective ci pipeline for your data warehouse deserves its own post, but know that the setup is remarkably useful and not as hard to configure as you might think. all tests passed! as people in the business of providing accurate information, we should strive to push the limits of automated testing in the data warehouse and beyond: extract and load (el) code, data science models, business intelligence, and more. testing our assumptions will make us better engineers and analysts and will increase confidence in our work and data trust across the business. if you have an interesting approach to automated testing in your data warehouse or suggestions to improve this one, i‚Äôd love to hear about it in the comments! if you enjoyed this article, please give it some claps (on a scale of 1‚Äì50) below! follow me here for more content about data engineering and analytics in small to mid-size companies. data testing sql analytics -- -- 10 written by josh temple 1.1k followers ¬∑ 16 following co-founder of @spectaclesci, previously @spotify, @milk bar responses ( 10 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/microsoft-design/if-you-want-to-be-creative-dont-be-data-driven-55db74078eda,,,"if you want to be creative, don‚Äôt be data driven | by bill pardi | microsoft design | medium sitemap open in app sign up sign in medium logo write sign up sign in microsoft design ¬∑ stories from the thinkers and tinkerers at microsoft. sharing sketches, designs, and everything in between. if you want to be creative, don‚Äôt be data driven 3 ideas that will completely change how you think about data bill pardi 11 min read ¬∑ may 23, 2017 -- 153 listen share there is no inherent value in any piece of data because all information is meaningless in itself. why? because information doesn‚Äôt tell you what to do. ‚Äî beau lotto as i write this i‚Äôm sitting in a small conference room on the second floor of an office building. the view from the windows is a paved courtyard down below roughly 25 feet from the building with some tables, chairs, and well-manicured landscaping. i can see that the sun is shining, and it looks like a lovely day. based on that data, should i go work outdoors? consider your answer, and we‚Äôll come back to the question later. if you are a designer, engineer, or in any role that creates things, you probably hear a lot about ‚Äúbig data‚Äù and being ‚Äúdata driven.‚Äù the assumption is that data equals insight and direction. but does it? data, any data, in any amount brings with it problems that make it very dangerous to rely on alone. let‚Äôs consider a few of them: first, data is just information and alone does not represent objective reality. next, whatever data you have is never, ever complete, and finally, getting more data does not necessarily mean more clarity. let‚Äôs look at these in more detail. data is not reality humans are great at making decisions based on their context and history, but we‚Äôre pretty bad at seeing the possibilities beyond that. here‚Äôs an example. read the text below aloud: if you read ‚Äúwhat are you reading now?‚Äù you did what many english readers would do with the same ‚Äúdata.‚Äù you did this even though there is not a single english word in that sentence. you‚Äôre able to read something meaningful by taking both the context of this article and your history with the english language and filling in the blanks. also note that i asked you to read the sentence, so you saw the word ‚Äúreading‚Äù in the letters. that priming helped determine the outcome. not everyone reads it the same way however. if you were munching on something, or sitting in a restaurant, you might just as easily have read ‚Äúwhat are you eating now?‚Äù and for anyone who doesn‚Äôt read english the letters would be what they really are, gibberish. the point here is that how we process data is highly contextualized by the individual doing the processing. often we will come to the same conclusions based on our shared history or context, but just as often we can come to different conclusions from the exact same data for the same reasons. all data is missing something small data, big data, it doesn‚Äôt matter. all data is incomplete at some level. to demonstrate, let‚Äôs imagine you have to create a software product and decide that the best way to focus your work is to create a profile of your intended customer. your expectation is that the profile will provide some insight as to what to build. you create a ‚Äúpersona‚Äù named linda from data you collected. linda is: a female 31 years old single outspoken smart a philosophy major is deeply concerned with issues of discrimination and social justice as a student, participated in anti-nuclear demonstrations. while this data might be useful for a profile, almost no one would call this a complete view of a person let alone a population. that being the case, based on the data given, which is more probable about linda, choosing from the following scenarios? linda is a bank teller linda is a bank teller also active in woman‚Äôs rights if you are like more 80% of the people confronted with the linda problem , you say that scenario 2 is more likely to fit linda. that response however, violates the logic of probability. if the question is which is more likely, then between number 1 and number 2 the answer has to be number 1 because the set of feminist bank tellers is included in the number of bank tellers, but the reverse is not true. so it‚Äôs more likely that linda is a bank teller than a feminist bank teller. why do we make this mistake? without getting into all the behavioral psychology, the basic idea is that scenario 2 tells a better story, so we prefer it. put another way, there is a lot missing in this data set, so our brains take what is there and fill in the rest. the apparent specificity allows us to construct a story that seems to make the most sense, but the logical reality is the opposite. in this case most people are just ignoring an obvious logical error, but you might imagine even more data about linda ‚Äî how she dresses, where she lives, who she associates with, etc. that might lead you to build an even more complete profile of her. but that profile would still be missing information and could very well be wholly inaccurate in terms of understanding what product a real customer actually needs. more data, less clarity? data alone can also have the effect of clouding our ability to see creative solutions, even to simple problems. here‚Äôs an example: you need to attach a candle to a wall (a cork board) and light it in such a way that candle wax won‚Äôt drip onto a table directly below. to do so, you may only use the following along with the candle: a book of matches a box of thumbtacks any ideas? let me give you more data, say a ‚Äúbigger data‚Äù version of the problem, and see if that helps: this little experiment, known as duncker‚Äôs candle problem , has been tested on a variety of subjects all over the world, and while they come up with a lot of creative ideas most don‚Äôt solve the problem. in the rare cases where they do, the solution is usually terribly complex or inefficient. the best (and simplest) solution is to empty the box and tack the box to the wall to hold the candle. so simple, right? but that is not what most people come up with, at least not right away. the description of the problem is pretty limited, but it seems as if providing a picture in addition to the description doesn‚Äôt provide more help, and may even reduce the ability to find the solution. what is happening here? first, the problem statement is that we have to attach the candle to the wall, and we have a preconception that thumbtacks are used for attaching stuff to walls. in addition, the description and corresponding picture are establishing that the box is a container for the tacks. these descriptions create biases about the objects that are not easy for most people to overcome, making it very difficult to see other ways the materials can be used. over the years researchers have tried different ways to improve the ability to see the solution more quickly. some ways that have worked include changing the description of the available items to: a book of matches a box thumbtacks in a similar vein the picture is changed to: these subtle but important changes make a big difference, and increase the chances that participants will find the solution, or find it more quickly. in essence, it improves creativity. why? this second description and picture help remove some of the bias noted above, and allows us to see more clearly that the box could be used as a shelf. a box and thumbtacks is a different way of looking at the data than a box of thumbtacks. voil√†! the bad news to all this is that data by itself is at best meaningless, and at its worse, misleading. in most cases it will tell you very little or nothing about what to do. unfortunately, that‚Äôs not how many professionals treat the data they are given. i often hear colleagues, in the midst of needing a design or business decision, ask ‚Äúwhat does the data tell us to do?‚Äù the real answer: not much. if we stopped here things might look bad for our data-rich future, but it‚Äôs not hopeless. here are three ways you can approach data that will enrich your creativity and enable you to use the information you get in very powerful ways. 1. experiment with the recipe i work in a group that creates things that may be used by half a billion people. i manage the data science team for that group and we are increasingly getting requests for more data about our business. from my experience, there‚Äôs one piece i‚Äôve left out of this discussion that makes all the data we have, and will continue to collect, useful. that piece is you and your creativity. see, data is meaningless only if we are expecting objective truth from it without factoring in our perceptions and assumptions and getting past those with our creativity. what i mean by creativity in this context is the process of asking questions and experimenting. creativity allows us to take the data we have, question our starting assumptions about what the data is telling us, and experiment until we make something useful out of it. the title of this article is about not being data driven, with ‚Äúdriven‚Äù being the key word. the idea here is that we should use data as information, not as insight. put another way, it‚Äôs not about the ingredients, it‚Äôs about the cook. ingredients alone don‚Äôt make a meal (at least, not a good one). and even great recipes don‚Äôt come without a lot of experimentation and failed attempts by the people who create them. in the same way, the human part of the data pipeline is the most valuable part, and this is especially true for those of us in creative or innovative fields. for data to support truly creative or innovative outcomes, we must allow it to inform us of the facts so we can ask questions and experiment with the ‚Äú adjacent possible ‚Äù to discover the insights and potential that the raw data doesn‚Äôt provide. this is true for the following reasons: experimentation leaves many possibilities open experimenters expect, and even celebrate, failure and uncertainty experimentation keeps the process open to change and adaptable to discoveries experimentation is a bit of a cold, clinical word, but you could also use the words exploration or even play. experimentation supports the idea that there are no preconceived outcomes, leaving open many possible results. for this to happen, we have to start with the idea that ‚Äúsuccess‚Äù can come in many forms, or even no form at all. that means when you improvise with the recipe you may completely fail at making a dish you want to eat, or you may invent a whole new cuisine! 2. question everything experimentation and play are ways to explore new possibilities. the best way to put exploration into practice is to start with questions. to put some of the ideas above to the test, go back to the candle problem and see how you might take the data and question it to come up with new possibilities. for example, given the candle, the wall, the box, and the thumbtacks, i might ask some of the following: what would happen if i removed an item from the list, does that help me in any way? what if i turned everything upside down, does that make a difference? what would i see if i take all the matchsticks out of the matchbook? what if i took the thumbtacks out of the box? what if i tried to stick everything to the wall with the tacks? this is just a small set of what you‚Äôre probably seeing is a large set of questions i can ask about the candle problem data. and the last two questions in my short list start to get toward a possible solution as i‚Äôm changing the idea that the box must hold the tacks while still using the tacks for their intended purpose. it becomes an almost magical transformation of my thinking. and this is what i am able to do alone, but there‚Äôs even more magic when i include others. 3. think inclusively i mentioned the ‚Äúadjacent possible‚Äù above. for most of us, our creativity allows us to explore not all possible outcomes, but only a small portion of what is possible, limited by our history, biases, and perspectives. this is how our brains evolved. we create memories throughout our lives and draw from those memories, or our ‚Äúhistory‚Äù, when we need to make decisions about the future (any future, immediate or long-term). this is why we interpret data differently. we only have our own history to draw from, and everyone‚Äôs history is slightly to vastly different from each other. the more diverse a person‚Äôs history the more adjacent possibles she has to draw from, but the number of possibilities is still limited ‚Äî one person‚Äôs brain can only hold so much. enter the diverse team. the more a team consists of a diversity of backgrounds, perspectives, culture, education and even professions, the more diverse the adjacent possibles the team will bring to any given problem or set of information. data, rather than being the driver of creativity, brings opportunities for different perceptions, ideas and, most importantly, questions. the more homogeneous a team is the more efficient it might be, but it‚Äôs almost certain to be less creative, and creativity is what you desperately need when solving difficult problems. while diversity is not a magic bullet ‚Äî teams must be willing to get out of their comfort zones and embrace their differences ‚Äî diverse teams are generally smarter than homogeneous ones. being creative in a data-rich world data is becoming an increasingly important part of our personal lives, our businesses, and our work. those of us who spend our days solving difficult problems will rely on this data as a tool to help us understand our world and do new things. but data should not drive us. it should be a signal from the wider world that we use to help answer questions and ask new ones. the insights must come from us. here we‚Äôve explored some reasons why relying on data as a driver is a bad idea. but we‚Äôve also looked at ways to turn information into creation: acknowledge that we, and those we work with, bring own history to any given set of data, biasing our judgement. experiment, explore and even play with the data through questions. bring diverse points of view and unique perspectives to problems, getting as many ‚Äúadjacent possibles‚Äù as we can. so next time you‚Äôre faced with a ‚Äúdata-driven‚Äù scenario do this: instead of looking for the answers the data provides, look for the questions it generates. as to whether i decided to go work out in the courtyard? well, i left out a critical piece of data. it‚Äôs early spring here in the pacific northwest, and it‚Äôs only 49 degrees outside. though the sun was tempting, before heading out i wondered how cold it was, and that was definitely a question worth asking. i‚Äôd love to hear your comments! i‚Äôve been involved in creating software most of my career, and currently manage a data science team at microsoft. follow me on twitter and linkedin . to stay in-the-know with microsoft design , follow us on dribbble , twitter and facebook , or join our windows insider program . and if you are interested in joining our team, head over to aka.ms/designcareers . data science big data data driven data data analysis -- -- 153 published in microsoft design 40k followers ¬∑ last published jun 12, 2025 stories from the thinkers and tinkerers at microsoft. sharing sketches, designs, and everything in between. written by bill pardi 1.6k followers ¬∑ 38 following i do software | strategy | writing | coaching | speaking. views are my own. responses ( 153 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://mrpowers.medium.com/designing-easily-testable-spark-code-df0755ef00a4,,,"designing easily testable spark code | by matthew powers | medium sitemap open in app sign up sign in medium logo write sign up sign in designing easily testable spark code matthew powers 5 min read ¬∑ aug 24, 2017 -- 3 listen share well designed code is reusable, modular, maintainable‚Ä¶ and easily testable . it‚Äôs hard to quantify the modularity or maintainability of code, but it‚Äôs easy to measure the code‚Äôs testability. just take a chunk of code and try to test it üòú this blog post demonstrates how to write dataframe transformations and user defined functions that are easy to test, so you can enjoy the benefits of beautiful spark code. why should we design code that‚Äôs testable? sandi metz wrote a masterful description of how intentional testing reduces costs in her book practical object oriented design in ruby . the most common arguments for having tests are that they reduce bugs and provide documentation, and that writing tests first improves application design. these benefits, however valid, are proxies for a deeper goal. the true purpose of testing, just like the true purpose of design, is to reduce costs. if writing, maintaining, and running tests consumes more time than would otherwise be needed to fix bugs, write documentation, and design applications tests are clearly not worth writing and no rational person would argue otherwise. it is common for programmers who are new to testing to find themselves in the unhappy state where the tests they write do cost more than the value those tests provide, and who therefore want to argue about the worth of tests. these are programmers who believed themselves highly productive in their former test-not lives but who have crashed into the test-first wall and stumbled to a halt. their attempts at test-first programming result in less output, and their desire to regain productivity drives them to revert to old habits and forgo writing tests. the solution to the problem of costly tests, however, is not to stop testing but instead to get better at it. getting good value from tests requires clarity of intention and knowing what, when, and how to test. spark that‚Äôs backed by a test suite is easier to reuse, maintain, and refactor. it‚Äôs a code smell when spark code isn‚Äôt backed by a test suite. a code smell is a surface indication that usually corresponds to a deeper problem in the system. ‚Äî martin fowler a lot of spark coders don‚Äôt have a strong programming background and it‚Äôs hard for them to imagine how tests reduce the costs of developing code. let‚Äôs start by showing the skeptics how they can design code that‚Äôs a lot easier to test. once they‚Äôre writing well designed code, we can try to upsell them on the idea of writing some tests üòâ structuring code with dataframe transformations let‚Äôs take a look at some code that doesn‚Äôt use a dataframe transformation. val sourcedf = seq ( (2), (10) ).todf( ""wave_height"" ) val fundf = sourcedf.withcolumn( ""stoke_level"" , when ( $""wave_height"" > 6, ""radical"" ).otherwise( ""bummer"" ) ) fundf.show() +-----------+-----------+ |wave_height|stoke_level| +-----------+-----------+ |          2|     bummer| |         10|    radical| +-----------+-----------+ this code is really difficult to test and impossible to reuse. we‚Äôd need to copy and paste the withcolumn function to invoke the code elsewhere üò± let‚Äôs refactor the code with a dataframe transformation. val sourcedf = seq ( (2), (10) ).todf( ""wave_height"" ) def withstokelevel()(df: dataframe): dataframe = { df.withcolumn( ""stoke_level"" , when ( $""wave_height"" > 6, ""radical"" ).otherwise( ""bummer"" ) ) } val fundf = sourcedf.transform(withstokelevel()) the withstokelevel dataframe transformation can be easily reused and tested üéä it‚Äôs usually easy to wrap code in a dataframe transformation and this is the easiest way to up your spark game! structuring code with user defined functions let‚Äôs take a look at some code that doesn‚Äôt leverage a user defined function. val sourcedf = seq ( ( ""tulips are pretty"" ), ( ""apples are yummy"" ) ).todf( ""joys"" ) val happydf = sourcedf.withcolumn( ""joys_contains_tulips"" , $""joys"" .contains( ""tulips"" ) ) happydf.show() +-----------------+--------------------+ |             joys|joys_contains_tulips| +-----------------+--------------------+ |tulips are pretty|                true| | apples are yummy|               false| +-----------------+--------------------+ this code isn‚Äôt testable or reusable. let‚Äôs refactor the code with a user defined function. val sourcedf = seq ( ( ""tulips are pretty"" ), ( ""apples are yummy"" ) ).todf( ""joys"" ) def containstulips(str: string): boolean = { str.contains( ""tulips"" ) } val containstulipsudf = udf [boolean, string](containstulips) val happydf = sourcedf.withcolumn( ""joys_contains_tulips"" , containstulipsudf( $""joys"" ) ) fantastic ‚Äî the containstulips and containstulipsudf functions can now be tested and reused! it‚Äôs a lot easier and faster to test regular scala functions than user defined functions. e.g. containstulips is easier to test than containstulipsudf . in real world applications, containstulips will have complex logic and we can use standard scala testing techniques to cover all the edge cases. we can use a single integration test to make sure containstulipsudf functions as expected. writing single purpose dataframe transformations methods, like classes, should have a single responsibility. ‚Äî sandi metz dataframe transformations should only do one thing, consistent with the unix philosophy and the single responsibility principle . functions that do one thing are easier to reuse and maintain. let‚Äôs look at a dataframe transformation that does two things. def doesstuff()(df: dataframe): dataframe = { df .filter( $""num"" % 2 === 0) .withcolumn( ""food"" , lit ( ""rum ham"" )) } val sourcedf = seq ( (2), (5), (10) ).todf( ""num"" ) val sunnydf = sourcedf.transform(doesstuff()) sunnydf.show() +---+-------+ |num|   food| +---+-------+ |  2|rum ham| | 10|rum ham| +---+-------+ the doesstuff transformation filters out all of the odd numbers from the dataframe and appends a food column. let‚Äôs split up doesstuff into two dataframe transformations to make the code more modular and testable. def filteroddnumbers()(df: dataframe): dataframe = { df.filter( $""num"" % 2 === 0) } def withrumham()(df: dataframe): dataframe = { df.withcolumn( ""food"" , lit ( ""rum ham"" )) } val sourcedf = seq ( (2), (5), (10) ).todf( ""num"" ) val sunnydf = sourcedf .transform(filteroddnumbers()) .transform(withrumham()) filteroddnumbers and withrumham can be used independently, so the code is more modular. these dataframe transformations can also be tested independently ‚Äî it‚Äôs always easier to test single purpose functions. testing methods that chain multiple dataframe transformations ideally your code will be broken up into a bunch of single purpose dataframe transformations and user defined functions. when you need to run all the code, you can chain the functions together. def model()(df: dataframe): dataframe = { .transform(standardizefirstname()) .transform(standardizelastname()) .withcolumn(""age_bucket"", agebucketizer($""age"")) } dataframe transformations like model can be really hard to test. you might need to build up very complex dataframes to account for all the dataframe requirements and their interrelation. scala doesn‚Äôt have a good mocking framework, so it‚Äôs not easy to test this method with stubs or doubles either. i recommend testing the individual components (e.g. test standardizefirstname , standardizelastname , and agebucketizer ) and writing a ‚Äúdoes not blow up‚Äù test for really complex transformations like model . i‚Äôll write a blog post on advanced spark testing techniques to cover this approach in more detail. disclaimers on the code snippets in this post i intentionally simplified some of the code snippets in this post, but we can do better with out production code. let‚Äôs look at the filteroddnumbers dataframe transformation. def filteroddnumbers()(df: dataframe): dataframe = { df.filter( $""num"" % 2 === 0) } if we only want the filteroddnumbers transformations to work on dataframes with a num column, we should add a dataframe validation, as described in this blog post . def filteroddnumbers()(df: dataframe): dataframe = { validatepresenceofcolumns(df, seq ( ""num"" )) df.filter( $""num"" % 2 === 0) } it‚Äôd probably be better to structure filteroddnumbers as a schema independent transformation, so it‚Äôs more flexible. def filteroddnumbers(colname: string)(df: dataframe): dataframe = { df.filter( col (colname) % 2 === 0) } conclusion we can craft well designed spark code with dataframe transformations and user defined functions. well designed code is easier to refactor, maintain, and reuse, which lowers the overall cost of our application. well designed code is easily testable and vice versa. it‚Äôs a code smell when programs are difficult to test. don‚Äôt shy away from the challenge of properly testing your code. lean in, redesign your code so it‚Äôs more testable, and write some specs to start improving your code coverage üòâ versions used in this post scala: 2.11 spark 2.2.0 apache spark testing software design scala dataframes -- -- 3 written by matthew powers 2.7k followers ¬∑ 49 following spark coder, live in colombia / brazil / us, love scala / python / ruby, working on empowering latinos and latinas in tech responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/@rhwolniewicz/building-a-rest-service-with-kotlin-spring-boot-mongodb-and-junit-14d10faa594b,JUnit; Postman,Test-Driven Development; Unit Testing,"building a rest service with kotlin, spring boot, mongodb, and junit | by richard wolniewicz | medium sitemap open in app sign up sign in medium logo write sign up sign in building a rest service with kotlin, spring boot, mongodb, and junit richard wolniewicz 13 min read ¬∑ oct 22, 2017 -- 1 listen share this year i‚Äôve been ramping up on kotlin by migrating some existing spring - & java-based rest services. when spring 5.0 released this summer with improved kotlin support, i dove in to explore the new options. however, as i searched online, i became frustrated at the lack of a comprehensive tutorial on getting started with the technologies i had chosen: spring boot, spring data, mongodb, and junit. most tutorials touched on a single piece of the puzzle, but each used different idioms, making it difficult to assemble a complete service. in particular, few tutorials show how to integrate test-driven development into writing a kotlin spring service. and many tutorials focus on direct-wiring rest apis to database access, but are hard to adapt to rest apis with significant service logic. this led me to assemble a sample rest service , which i present here. it is by no means comprehensive in terms of functionality, but does cover the core use cases for a ‚Äúreal‚Äù rest api. the project uses test-driven development. in particular, the rest apis have unit tests, and both internal services and the rest apis use the fongo (‚Äúfake mongo‚Äù) in-memory mongodb implementation. this means they do not require a running mongodb instance, and failing unit tests have no impact on persistent data. junit is used for unit testing. two spring projects typically used in rest apis are used here: spring boot and spring data . gradle is used to automate the build process. a typical domain-driven development approach is used, separating model, repository, service, and controller classes. the step-by-step construction of the project is broken out in separate folders in the github project repository containing the project source code. tools and technologies the tutorial here uses intellij idea , gradle 4.2, and postman for coding, building, and rest api testing. i‚Äôm running on windows 10 using powershell, though there is no scripting and very little command line work, so other shells should work as well. all software is written in kotlin 1.1.51, with spring 5.0 and spring boot 2.0 (still a milestone release at the time of this writing). mongodb 3.4 community server is used for persistence. from here, i‚Äôll assume you have jdk 8.0, intellij, gradle, mongodb, and postman installed. player score service this rest service isn‚Äôt much: it keeps track of player scores and can produce a very simple leader board. my goal is to illustrate the principles without cluttering up the code with too much domain-specific logic. players are tracked by handles, which can be arbitrary strings of letters and numbers. i don‚Äôt perform validation, however (a no-no; maybe i‚Äôll add it in a future version of the tutorial). a post operation is used to add points to a player‚Äôs score; the player handle is part of the uri. a get operation to the leader board uri returns the top-3 player handles in rank order. the player collection in the mongodb database has the data about each player, keyed by player handle. this includes the score history. i made this document more complex than your usual relational database table to show how to work with structured documents in a mongodb. everything stored, received, and returned by the service is formatted as json. some obvious gaps include the lack of an api to get player information and history, validation, use of domain types, etc. i kept them out in order to focus on the rest, database, and testing infrastructure. also, note that i‚Äôm omitting the package and import statements from the code below, to focus on the important parts. look to the github repository if you need to see the actual import statements. also in the interest of compact presentation, i‚Äôm not writing the documentation comments for classes and functions, which really should be there. step 1: spring initialzr the spring initalizr bootstraps a new spring application. do note that i‚Äôve changed the value of every box from its default. spring initializr setup select gradle project , kotlin , and spring boot 2.0 . pick an appropriate group; i‚Äôm using my own domain name wolniewicz.com . the artifact name is the name of our rest service: playerscore . the two dependencies: mongodb and web . generating the project downloads a .zip file with the initial project structure. unzip the project and import it into intellij. this is also a good time to add it to your repository: i‚Äôm using github and smartgit . after importing, the project should look something like this: project viewed in intellij with folders expanded step 2: model and repository player model i start with the player model object, in model/player.kt . @typealias(""player"") data class player(@id val handle: string, val totalscore: int = 0, val history: list<scoreevent> = listof ()) { operator fun plus(score: int) = player(handle, totalscore + score, history + scoreevent(score)) } data class scoreevent(val time: string, val points: int) { constructor(points: int) : this(dateformat.format(date()), points) companion object { val dateformat = simpledateformat(""yyyy-mm-dd hh:mm:ss"") } } a player has three elements: the player‚Äôs handle (also its unique identifier), the total score, and the history of score events. each score event is just a timestamp and the points scored. this is not a particularly good data model: normally we wouldn‚Äôt want to carry around a heavyweight, ever-changing block of data like the score history inside an otherwise lightweight structure like player. and yes, i know totalscore could be calculated instead of stored. and actually using dates instead of strings for time would be more useful for clients of the model. the purpose here is to provide a simple example without a lot of extra code. i‚Äôve made some choices specifically to simplify the json document structure as it will appear in the mongodb: the spring data mongodb package will map the identifier to a field called ‚Äú_id‚Äù (required by mongodb). using the id annotation allows us to specify the field to use as the identifier without using the name ‚Äúid‚Äù for that field. a field called ‚Äú_class‚Äù is also auto-injected by spring data mongodb. by default this is the fully-qualified java class name. i personally don‚Äôt like to embed the package name in persistent data, since that may change if code is refactored; typealias lets us specify the name which should be used. this definitely makes the json smaller and easier to read. to understand alternative options here, i recommend reading the reference documentation for spring data mongodb, section 7.6 . this is an area where your needs may differ substantially from my approach. i‚Äôve limited myself to spring data annotations. there are some mongodb-specific annotations, such as document , which provide additional functionality at the cost of tying your code to mongodb. i‚Äôm choosing to restrict myself to generic spring data functionality, so i can switch to another data store in the future if needed. player repository the player repository, repository/playerrepository.kt , is where the real magic of spring data kicks in. interface playerrepository : crudrepository<player, string> { fun findtop3byorderbytotalscoredesc() : list<player> } extending crudrepository is sufficient to deliver basic read / write / delete operations. as with the annotations, there is a mongorepository base interface which can be used to access additional, mongodb-specific functionality, at the cost of increasing your code‚Äôs binding to mongodb. the single function defines the query to get the top 3 players ranked by total score, which we will use when implementing the leader board. the defining query methods section of the spring data mongodb reference discusses options for query names supported for mongodb. no implementation is required; spring data will provide one for you. step 3: player service playerservice next we implement player actions in service/playerservice.kt . these implement the methods we will use when writing controllers. our goal is to keep domain logic out of the controller code. there is nothing wrong with having controllers talk directly to repositories directly, but i prefer to provide a service interface even for basic repository calls, such as the leaders method below. in my experience, direct calls to repositories tends to result in small bits of business logic showing up in controllers. furthermore, spring data repositories are constrained in method names, while a service interface can map to method names more meaningful in the context of the application domain. interface playerservice { fun leaders() : list<player> fun score(handle: string, points: int) : int } @service(""playerservice"") class playerserviceimpl : playerservice { @autowired lateinit var playerrepository: playerrepository override fun leaders(): list<player> = playerrepository.findtop3byorderbytotalscoredesc() override fun score(handle: string, points: int) : int { val player = playerrepository .findbyid(handle).orelse(player(handle)) + points playerrepository.save(player) return player.totalscore } } of course, in true test-driven development fashion, i actually wrote the unit tests below against stubbed versions of the interface implementation, then wrote the implementations to pass the unit tests. in-memory mongodb unit testing to speed up unit testing and avoid issues with persistent databases living beyond unit tests ‚Äî or worse, having a database you want to keep corrupted by a unit test ‚Äî a good practice is to use an in-memory database configured from scratch in unit tests. for mongodb, the fake mongo, or fongo, project provides this. configuring the unit tests to use fongo requires three steps: modify build.gradle to add the test package dependencies. set up the test configuration for spring boot to use fongo. configure the fongo database in the unit tests. note that we can also delete the initializr-generated playerscoreapplicationtests.kt file; we will be writing our own unit tests. first we add the necessary test dependencies to build.gradle in the dependencies block: testcompile('org.jetbrains.kotlin:kotlin-test') testcompile('org.jetbrains.kotlin:kotlin-test-junit') testcompile('com.github.fakemongo:fongo:2.1.0') the first two lines add the standard kotlin library testing infrastructure for junit. the third line adds the fongo package. all three are only used during testing, thus testcompile instead of compile . important note: at this point you will need to refresh your gradle project in intellij to force it to pull in the new libraries. if you see errors in the next step about not seeing the fongo library, this is why. second we create a custom configuration for use during testing, testconfiguration.kt . @configuration class testconfiguration : abstractmongoconfiguration() { @autowired lateinit var env: environment override fun getdatabasename() = env.getproperty(""mongo.db.name"", ""test"") override fun mongoclient(): mongoclient { logger.info(""instantiating fongo with name $ databasename ."") return fongo( databasename ). mongo } companion object { val logger: logger = loggerfactory.getlogger(testconfiguration::class. java ) } } here we‚Äôve overridden the mongoclient bean to instantiate fongo. the database name is read from the environment, but if it isn‚Äôt defined, ‚Äútest‚Äù is used instead. i‚Äôm using slf4j interfaces for logging here. spring will default to logback , but feel free to use your preferred logging package if you wish. third, we need to set up individual unit tests which need the underlying database to use fongo. details can be found at github.com/fakemongo/fongo . i prefer to abstract this kind of functionality into a base class, so writing actual unit tests is easier and faster. here is playerscoretestwithfongo.kt . @runwith(springrunner::class) @springboottest abstract class playerscoretestwithfongo(val initializetestdata: boolean = true) { @get:rule val fongorule = fongorule() @autowired lateinit var playerrepository: playerrepository @before fun setuptestdatabase() { if (initializetestdata) { playerrepository.save(test_player_1) playerrepository.save(test_player_2) playerrepository.save(test_player_3) playerrepository.save(test_player_4) playerrepository.save(test_player_5) } } companion object { val test_player_1 = player(""alice"", 20) val test_player_2 = player(""bob"", 15) val test_player_3 = player(""charlie"", 25) val test_player_4 = player(""dawn"", 30) val test_player_5 = player(""ed"", 10) } } notable details: the runwith and springboottest annotations are standard for a unit test class with spring boot, and in fact come right out of the playerscoreapplicationtests file auto-generated by spring initializr, which we deleted. the class is abstract , which prevents junit from attempting to run the base class as a unit test directly. the initializetestdata flag lets me control how the in-memory database is configured. in this way, different unit tests can share the base class but still customize their configuration setup. i usually use a more full-featured, builder-pattern-based test configuration, but am keeping it simple here. fongorule triggers the usage of fongo for this unit test. if the unit test requested test data, then the setuptestdatabase method creates 5 test players. playerservicetest with all of the above infrastructure, we can write a very normal unit test for playerservice, in service/playerservicetest.kt . virtually all of the code in the unit test is about the test of the service functionality; only the base class for the unit test is required to make full use of fongo. class playerservicetest : playerscoretestwithfongo() { @autowired lateinit var playerservice: playerservice @test fun testleaders() { logger.info(""begin testleaders"") // verify that the leaders are as expected. val leaders = playerservice.leaders() assertequals (3, leaders.size, ""there should be 3 leaders."") assertequals (test_player_4, leaders[0], ""the first leader should be dawn."") assertequals (test_player_3, leaders[1], ""the second leader should be charlie."") assertequals (test_player_1, leaders[2], ""the third leader should be alice."") logger.info(""end testleaders"") } @test fun testscore() { logger.info(""begin testscore"") playerrepository.save(player(test_player_handle)) // score 10 points. playerservice.score(test_player_handle, 10) val player = playerrepository.findbyid(test_player_handle).get() assertequals (10, player.totalscore, ""total score should be 10 after the first scoring event."") assertequals (1, player.history.size, ""the history should have a single element."") assertequals (10, player.history[0].points, ""the recorded points should be 10."") // score 5 more points. playerservice.score(test_player_handle, 5) val player2 = playerrepository.findbyid(test_player_handle).get() assertequals (15, player2.totalscore, ""total score should be 15 after the second scoring event."") assertequals (2, player2.history.size, ""the history should have a single element."") assertequals (10, player2.history[0].points, ""the first recorded points should be 10."") assertequals (5, player2.history[1].points, ""the second recorded points should be 5."") logger.info(""end testscore"") } companion object { val logger: logger = loggerfactory.getlogger(playerservicetest::class. java ) const val test_player_handle = ""testplayer"" } } at this point, the unit tests should run and pass. step 4: controllers we are ready to build and test the controllers. as usual with spring, we want to limit the code in the controllers to just that required for the restful interface. domain logic is best kept in the services and the model classes. here i only demonstrate two interfaces: a get to return the leaders board and a post to add points to a player. any realistic service would have more than this (you can‚Äôt even get a player‚Äôs current score!), but the additional methods don‚Äôt really add anything new, so i‚Äôm leaving them as an exercise for the reader :-) get leaders the spring restcontroller and getmapping annotations do all of the heavy lifting here, in controller/leaderscontroller.kt . @restcontroller class leaderscontroller { @autowired lateinit var playerservice: playerservice @getmapping(""/leaders"") fun getleaders(): list<string> = playerservice.leaders().stream().map { it .handle } . tolist () } controller unit testing with mockmvc as above, we move reusable portions of the controller test setup into a base class, controller/playerscoretestwithfongoandmockmvc.kt . we are using spring‚Äôs mockmvc to mock http requests and responses in unit tests. @autoconfiguremockmvc abstract class playerscoretestwithfongoandmockmvc( initializetestdata: boolean = true) : playerscoretestwithfongo(initializetestdata) { @autowired lateinit var mvc: mockmvc } ok, the class names are getting very wordy, in true spring fashion. i‚Äôd prefer something more compact in production code; your mileage may vary. and here is the unit test for leaderscontroller, in controller/leaderscontrollertest.kt . class leaderscontrollertest : playerscoretestwithfongoandmockmvc() { @test fun getleaderstest() { logger.info(""begin getleaderstest"") val expectedjson = """""" |[""${test_player_4.handle}"", |""${test_player_3.handle}"", |""${test_player_1.handle}""] """""". trimmargin () mvc.perform(mockmvcrequestbuilders.get(""/leaders"")) .andexpect(mockmvcresultmatchers.status(). isok ) .andexpect(mockmvcresultmatchers.content() .json(expectedjson)) logger.info(""end getleaderstest"") } companion object { val logger: logger = loggerfactory.getlogger(leaderscontrollertest::class. java ) } } post player score to add points to a player‚Äôs score, we post an integer value to ‚Äú/player/{handle}/score‚Äù. the only trickiness here is the need to receive the request body as a string and convert it to an integer, since the spring / jackson json conversion won‚Äôt handle a kotlin int directly. let‚Äôs put this in a separate controller from the leaders; controller/playercontoller.kt . @restcontroller class playercontroller { @autowired lateinit var playerservice: playerservice @postmapping(""/player/{handle}/score"") fun postplayerscore(@pathvariable handle: string, @requestbody points: string) : string { val score = playerservice.score(handle, points. toint ()) return ""$handle now has a total score of $score."" } } and the controller/playercontrollertest.kt unit test, very similar to that above, with the use of mockmvc‚Äôs post functionality. class playercontrollertest : playerscoretestwithfongoandmockmvc() { @test fun postplayerscoretest() { logger.info(""begin postplayerscoretest"") val points = 5 val expectedtotalscore = test_player_1.totalscore + points val expectedresult = ""${test_player_1.handle} now has a total score of "" + ""$expectedtotalscore."" // add 5 points to test_player_1's score. mvc.perform(mockmvcrequestbuilders .post(""/player/${test_player_1.handle}/score"") .content(points.tostring())) .andexpect(mockmvcresultmatchers.status(). isok ) .andexpect(mockmvcresultmatchers.content() .string(expectedresult)) logger.info(""end postplayerscoretest"") } companion object { val logger: logger = loggerfactory.getlogger(playercontrollertest::class. java ) } } all of the unit tests should pass at this point. below is the source code tree in its final form: source code tree in intellij after step 4 running the service and testing with postman to run the service and test it in postman, i like to open 3 powershell windows. in the first, run mongodb: in another, run the mongodb shell: in the last, use gradle to build and run the service: testing with postman is a simple way to interact with a local rest service. one easy thing to forget is setting the request body type to json for the posts. initially the leader board is empty. get the initial (empty) leaders board. let‚Äôs score points for alice, bob, and charlie. alice scores 30 points. bob scores 15 points. charlie scores 25 points and now the leaders board ‚Ä¶ leaders board showing ranked players. back in the mongodb shell, let‚Äôs check the state of the database. final database state. and that‚Äôs it! i hope you‚Äôve found this all-in-one tutorial helpful in getting started with kotlin and spring 5.0. please let me know if you find any errors above, have requests for changes, or would like to see similar tutorials on related topics. kotlin spring boot mongodb junit rest api -- -- 1 written by richard wolniewicz 25 followers ¬∑ 19 following computer scientist in boulder, co, with a machine learning and big data focus. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/teconomics-blog/5-tricks-when-ab-testing-is-off-the-table-f2637e9f15a5,,,"5 tricks when ab testing is off the table | by emily glassberg sands | teconomics | medium sitemap open in app sign up sign in medium logo write sign up sign in teconomics ¬∑ economists in tech 5 tricks when ab testing is off the table an applied introduction to causal inference in tech emily glassberg sands 18 min read ¬∑ mar 24, 2016 -- 9 listen share https://unsplash.com/photos/9di3g8owhii this article was co-authored with duncan gilchrist . sample code, along with basic simulation results, is available on github . at least a dozen times a day we ask, ‚Äúdoes x drive y?‚Äù y is generally some kpi our companies care about. x is some product, feature, or initiative. the first and most intuitive place we look is the raw correlation. are users engaging with x more likely to have outcome y? unfortunately, raw correlations alone are rarely actionable. the complicating factor here is a set of other features that might affect both x and y. economists call these confounding variables . in ed-tech, for example, we want to spend our energy building products and features that help users complete courses. does our mobile app meet that criteria? certainly the raw correlation is there: users who engage more with the app are on average more likely to complete. but there are also important confounders at play. for one, users with stronger innate interest in the product are more likely to be multi-device users; they are also more likely to make the investments required to complete the course. so how can we estimate the causal effect of the app itself on completion? the knee-jerk reaction in tech is to get at causal relationships by running randomized experiments, commonly referred to as ab tests. ab testing is powerful stuff: by randomly assigning some users and not others an experience, we can precisely estimate the causal impact of the experience on the outcome (or set of outcomes) we care about. it‚Äôs no wonder that for many experiences ‚Äî different copy or color, a new email campaign, an adjustment to the onboarding flow ‚Äî ab testing is the gold standard. for some key experiences, though, ab tests can be costly to implement. consider rolling back your mobile app‚Äôs full functionality from a random subset of users. it would be confusing for your users, and a hit to your business. on sensitive product dimensions ‚Äî pricing, for example ‚Äî ab testing can also hurt user trust. and if tests are perceived as unethical, you might be risking a full-on pr disaster . here‚Äôs the good news: just because we can‚Äôt always ab test a major experience doesn‚Äôt mean we have to fly blind when it matters most. a range of econometric methods can illuminate the causal relationships at play, providing actionable insights for the path forward. econometric methods first, a quick recap of the challenge: we want to know the effect of x on y, but there exists some set of confounding factors, c, that affects both the input of interest, x, and the outcome of interest, y. in stats 101, you might have called this omitted variable bias . the solution is a toolkit of five econometric methods we can apply to get around the confounding factors and credibly estimate the causal relationship: controlled regression regression discontinuity design difference-in-differences fixed-effects regression instrumental variables (with or without a randomized encouragement trial) this post will be applied and succinct. for each method, we‚Äôll open with a high-level overview, run through one or two applications in tech, and highlight major underlying assumptions or common pitfalls. some of these tools will work better in certain situations than others. our goal is to get you the baseline knowledge you need to identify which method to use for the questions that matter to you, and to implement effectively. let‚Äôs get started. method 1: controlled regression the idea behind controlled regression is that we might control directly for the confounding variables in a regression of y on x. the statistical requirement for this to work is that the distribution of potential outcomes, y, should be conditionally independent of the treatment, x, given the confounders, c. let‚Äôs say we want to know the impact of some existing product feature ‚Äî e.g., live chat support ‚Äî on an outcome, product sales. the ‚Äúwhy we care‚Äù is hopefully self-evident: if the impact of live chat is large enough to cover the costs, we want to expand live chat support to boost profits; if it‚Äôs small, we‚Äôre unlikely to expand the feature, and may even deprecate it altogether to save costs. we can easily see a positive correlation between use of chat support and user-level sales. we also probably have some intuition around likely confounders. for example, younger users are more likely to use chat because they they are more comfortable with the chat technology; they also buy more because they have more slush money. since youth is positively correlated with chat usage and sales, the raw correlation between chat usage and sales would overstate the causal relationship. but we can make progress by estimating a regression of sales on chat usage controlling for age. in r: fit <- lm(y ~ x + c, data = ...) summary(fit) the primary ‚Äî and admittedly frequent ‚Äî pitfall in controlled regression is that we often do not have the full set of confounders we‚Äôd want to control for. this is especially true when confounders are unobservable ‚Äî either because the features are measurable in theory but are not available to the econometrician (e.g., household income), or because the features themselves are hard to quantify (e.g., inherent interest in the product). in this case, often the best we can do within the controlled regression context is to proxy for unobservables with whatever data we do have. if we see that adding the proxy to the regression meaningfully impacts the coefficient on the primary regressor of interest, x, there‚Äôs a good chance regression won‚Äôt suffice. we need another tool. but before moving on, let‚Äôs briefly cover the concept of bad controls, or why-we-shouldn‚Äôt-just-throw-in-the-kitchen-sink-and-see-what-sticks (statistically). suppose we were concerned about general interest in the product as a confounder: the more interested the user is, the more she engages with our features, including live chat, and also the more she buys from us. we might think that controlling for attributes like the proportion of emails from us she opens could be used as a proxy for interest. but insofar as the treatment (engaging in live chat) could in itself impact this feature (e.g., because she wants to see follow-up responses from the agent), we would actually be inducing included variable bias . the take-away? be wary of controlling for variables that are themselves not fixed at the time the ‚Äútreatment‚Äù was determined. method 2: regression discontinuity design regression discontinuity design, or rdd, is a statistical approach to causal inference that takes advantage of randomness in the world. in rdd, we focus in on a cut-off point that, within some narrow range, can be thought of as a local randomized experiment. suppose we want to estimate the effect of passing an ed-tech course on earned income. randomly assigning some people to the ‚Äúpassing‚Äù treatment and failing others would be fundamentally unethical, so ab testing is out. since several hard-to-measure things are correlated with both passing a course and income ‚Äî innate ability and intrinsic motivation to name just two ‚Äî we also know controlled regression won‚Äôt suffice. luckily, we have a passing cutoff that creates a natural experiment: users earning a grade of 70 or above pass while those with a grade just below do not. a student who earns a 69 and thus does not pass is plausibly very similar to a student earning a 70 who does pass. provided we have enough users in some narrow band around the cutoff, we can use this discontinuity to estimate the causal effect of passing on income. in r: library(rdd) rdestimate(y ~ d, data = ..., subset = ..., cutpoint = ...) here‚Äôs what an rdd might look like in graphical form: let‚Äôs introduce the concepts of validity : results are internally valid if they are unbiased for the subpopulation studied. results are externally valid if they are unbiased for the full population. in randomized experiments, the assumptions underlying internal and external validity are rather straightforward. results are internally valid provided the randomization was executed correctly and the treatment and control samples are balanced. and they are externally valid so long as the impact on the experimental group was representative of the impact on the overall population. in non-experimental settings, the underlying assumptions depend on the method used. internal validity of rdd inferences rests on two important assumptions: assumption 1: imprecise control of assignment around the cutoff . individuals cannot control whether they are just above (versus just below) the cutoff. assumption 2: no confounding discontinuities . being just above (versus just below) the cutoff should not influence other features. let‚Äôs see how well these assumptions hold in our example. assumption 1: users cannot control their grade around the cutoff. if users could, for example, write in to complain to us for a re-grade or grade boost, this assumption would be violated. not sure either way? here are a couple ways to validate: check whether the mass just below the cutoff is similar to the mass just above the cutoff; if there is more mass on one side than the other, individuals may be exerting agency over assignment. check whether the composition of users in the two buckets looks otherwise similar along key observable dimensions. do exogenous observables predict the bucket a user ends up in better than randomly? if so, rdd may be invalid. assumption 2: passing is the only differentiator between a 69 and a 70. if, for example, users who get a 70 or above also get reimbursed for their tuition by their employer, this would generate an income shock for those users (and not for users with lower scores) and violate the no confounding discontinuities assumption. the effect we estimate would then be the combined causal effect of passing the class and getting tuition reimbursed, and not simply the causal effect of passing the class alone. what about external validity ? in rdd, we‚Äôre estimating what is called a local average treatment effect (late). the effect pertains to users in some narrow, or ‚Äúlocal‚Äù, range around the cutoff. if there are different treatment effects for different types of users (i.e., heterogeneous treatment effects ), then the estimates may not be broadly applicable to the full group. the good news is the interventions we‚Äôd consider would often occur on the margin ‚Äî passing marginally more or marginally fewer learners ‚Äî so a late is often exactly what we want to measure. method 3: difference-in-differences the simplest version of difference-in-differences (or dd) is a comparison of pre and post outcomes between treatment and control groups. dd is similar in spirit to rdd in that both identify off of existing variation. but unlike rdd, dd relies on the existence of two groups ‚Äî one that is served the treatment (after some cutoff) and one that never is. because dd includes a control group in the identification strategy, it is generally more robust to confounders. let‚Äôs consider a pricing example. we all want to know whether we should raise or lower price to increase revenue. if price elasticity (in absolute value) is greater than 1, lowering price will increase purchases by enough to increase revenue; if it‚Äôs less than 1, raising prices will increase revenue. how can we learn where we are on the revenue curve? the most straightforward method would be a full randomization through ab testing price. whether we‚Äôre comfortable running a pricing ab test probably depends on the nature of our platform, our stage of development, and the sensitivity of our users. if variation in price would be salient to our users, for example because they communicate to each other on the site or in real life, then price testing is potentially risky. serving different users different prices can be perceived as unfair, even when random. variation in pricing can also diminish user trust, create user confusion, and in some cases even result in a negative pr storm. a nice alternative to ab testing is to combine a quasi-experimental design with causal inference methods. we might consider just changing price and implementing rdd around the date of the price change. this could allow us to estimate the effect of the price on the revenue metric of interest, but has some risks. recall that rdd assumes there‚Äôs nothing outside of the price change that would also change purchasing behavior over the same window. if there‚Äôs an internal shock‚Äî a new marketing campaign, a new feature launch ‚Äî or, worse, an external shock ‚Äî e.g., a slowing economy ‚Äî we‚Äôll be left trying to disentangle the relative effects. rdd results risk being inconclusive at best and misleading at worst. a safer bet would be to set ourselves up with a dd design. for example, we might change price in some geos (e.g., states or countries), but not others. this gives a natural ‚Äúcontrol‚Äù in the geos where price did not change. to account for any other site or external changes that might have co-occurred, we can use the control markets ‚Äî where price did not change ‚Äî to calculate the counterfactual we would have expected in our treatment markets absent the price change. by varying at the geo (versus user) level, we also reduce the salience of the price variation relative to ab testing. below is a graphical representation of dd. you can see treatment and control geos in the periods pre and post the change. the delta between the actual revenue (here per session cookie) in the treatment group and the counterfactual revenue in that group provides an estimate of the treatment effect: while the visualization is helpful (and hopefully intuitive), we can also implement a regression version of dd. at its essence, the dd regression has an indicator for treatment status (here being a treatment market), an indicator for post-change timing, and the interaction of those two indicators. the coefficient on the interaction term serves is the estimated effect of the price change on revenue. if there are general time trends, we can control for those in the regression, too. in r: fit <- lm(y ~ post + treat + i(post * treat), data = ...) summary(fit) the key assumption required for internal validity of the dd estimate is parallel trends : absent the treatment itself, the treatment and control markets would have followed the same trends. that is, any omitted variables affect treatment and control in the same way. how can we validate the parallel trends assumption? there are a few ways to make progress, both before and after rolling out the test. before rolling out the test, we can do two things: make the treatment and control groups as similar as possible . in the experimental set-up, consider implementing stratified randomization . although generally unnecessary when samples are large (e.g., in user-level randomization), stratified randomization can be valuable when the number of units (here geos) is relatively small. where feasible, we might even generate ‚Äúmatched pairs‚Äù ‚Äî in this case markets that historically have followed similar trends and/or that we intuitively expect to respond similarly to any internal product changes and to external shocks. in their 1994 paper estimating the effect of minimum wage increases on employment , david card and alan krueger matched restaurants in new jersey with comparable restaurants in pennsylvania just across the border; the pennsylvania restaurants provided a baseline for determining what would have happened in new jersey if the minimum wage had remained constant. after the stratified randomization (or matched pairing), check graphically and statistically that the trends are approximately parallel between the two groups pre-treatment . if they aren‚Äôt, we should redefine the treatment and control groups; if they are, we should be good to go. ok, so we‚Äôve designed and rolled out a good experiment, but with everyone moving fast, stuff inevitable happens. common problems with dd often come in two forms: problem 1: confounders pop up in particular treatment or control markets. maybe mid-experiment our bd team launches a new partnership in some market. and then a different product team rolls out a localized payment processor in some other market. we expect both of these to affect our revenue metric of interest. solution: assuming we have a bunch of treatment and control markets, we can simply exclude those markets ‚Äî and their matches if it‚Äôs a matched design ‚Äî from the analysis. problem 2: confounders pop up across some subset of treatment and control markets. here, there‚Äôs some change ‚Äî internal or external ‚Äî that we‚Äôre worried might impact a bunch of our markets, including some treatment and some control markets. for example, the euro is taking a plunge and we think the fluctuating exchange rate in those markets might bias our results. solution: we can add additional differencing by that confounder as a robustness check in what‚Äôs called a difference-in-difference-in-differences estimation (ddd). ddd will generally be less precise than dd (i.e., the point estimates will have larger standard errors), but if the two point estimates themselves are similar, we can be relatively confident that that confounder is not meaningfully biasing our estimated effect. pricing is an important and complicated beast probably worthy of additional discussion. for example, the estimate above may not be the general equilibrium effect we should expect: in the short run, users may be responding to the change in price, not just to the new price itself; but in the long-run, users likely only to respond to the price itself (unless prices continue to change). there are several ways to make progress here. for example, we can estimate the effect only on new users who had not previously been served a price and so for whom the change would not be salient. but we‚Äôll leave a more extended discussion of pricing to a subsequent post. method 4: fixed effects regression fixed effects is a particular type of controlled regression, and is perhaps best illustrated by example. a large body of academic research studies how individual investors respond (irrationally) to market fluctuations. one metric a fin-tech firm might care about is the extent to which it is able to convince users to (rationally) stay the course ‚Äî and not panic ‚Äî during market downturns. understanding what helps users stay the course is challenging. it requires separating what we cannot control ‚Äî general market fluctuations and learning about those from friends or news sources ‚Äî from what we can control ‚Äî the way market movements are communicated in a user‚Äôs investment returns. to disentangle, we once again go hunting for a source of randomness that affects the input we control, but not the confounding external factors. our approach is to run a fixed effects regression of percent portfolio sold on portfolio return controlling (with fixed effects) for the week of account opening. since the fixed effects capture the week a user opened their account, the coefficient on portfolio return is the effect of having a higher return relative to the average return of other users funding accounts in that same week . assuming users who opened accounts the same week acquire similar tidbits from the news and friends, this allows us to isolate the way we display movements in the user‚Äôs actual portfolio from general market trends and coverage. sound familiar? fixed effects regression is similar to rdd in that both take advantage of the fact that users are distributed quasi-randomly around some point. in rdd, there is a single point; with fixed effects regression, there are multiple points ‚Äî in this case, one for each week of account opening. in r: fit <- lm(y ~ x + factor(f), data = ‚Ä¶) summary(fit) the two assumptions required for internal validity in rdd apply here as well. first, after conditioning on the fixed effects, users are as good as randomly assigned to to their x values ‚Äî in this case, their portfolio returns. second, there can be no confounding discontinuities, i.e., conditional on the fixed effects, users cannot otherwise be treated differently based on their x. for the fixed effects method to be informative, we of course also need variation in the x of interest after controlling for the fixed effects. here, we‚Äôre ok: users who opened accounts the same week do not necessarily have the same portfolio return; markets can rise or fall 1% or more in a single day. more generally, if there‚Äôs not adequate variation in x after controlling for fixed effects, we‚Äôll know because the standard errors of the estimated coefficient on x will be untenably large. method 5: instrumental variables instrumental variable (iv) methods are perhaps our favorite method for causal inference. recall our earlier notation: we are trying to estimate the causal effect of variable x on outcome y, but cannot take the raw correlation as causal because there exists some omitted variable(s), c. an instrumental variable , or instrument for short, is a feature or set of features, z, such that both of the following are true: strong first stage : z meaningfully affects x. exclusion restriction : z affects y only through its effect on x. who doesn‚Äôt love a good picture? if these conditions are satisfied, we can proceed in two steps: first stage : instrument for x with z second stage: estimate the effect of the (instrumented) x on y in r: library(aer) fit <- ivreg(y ~ x | z, data = df) summary(fit, vcov = sandwich, df = inf, diagnostics = true) ok, so where do we find these magical instruments? economists often find instruments in policies. josh angrist and alan krueger instrument for years of schooling with the vietnam draft lottery ; steve levitt instruments for prison populations with prison overcrowding litigation . although good instruments in the real world can generate incredible insights, they are notoriously hard to come by. the good news is that instruments are everywhere in tech. as long as your company has an active ab testing culture, you almost certainly have a plethora of instruments at your fingertips. in fact, any ab test that drives a specific behavior is a contender for instrumenting ex post for the effect of that behavior on an outcome you care about. suppose we are interested in learning the causal effect of referring a friend on churn. we see that users who refer friends are less likely to churn, and hypothesize that getting users to refer more friends will increase their likelihood of sticking around. (one reason we might think this is true is what psychologists call the ikea effect : users care more about products that they have invested time contributing to.) looking at the correlation of churn with referrals will of course not give us the causal effect. users who refer their friends are de facto more committed to our product. but if our company has a strong referral program, it‚Äôs likely been running lots of ab tests pushing users to refer more ‚Äî email tests, onsite banner ad tests, incentives tests, you name it. the iv strategy is to focus on a successful ab test ‚Äî one that increased referrals ‚Äî and use that experiment‚Äôs bucketing as an instrument for referring. (if iv sounds a little like rdd, that‚Äôs because it is! in fact, iv is sometimes referred to as ‚Äúfuzzy rdd‚Äù.) iv results are internally valid provided the strong first stage and exclusion restriction assumptions (above) are satisfied: we‚Äôll likely have a strong first stage as long as the experiment we chose was ‚Äúsuccessful‚Äù at driving referrals. (this is important because if z is not a strong predictor of x, the resulting second stage estimate will be biased.) the r code above reports the f-statistic, so we can check the strength of our first stage directly. a good rule of thumb is that the f-statistic from the first stage should be least 11. what about the exclusion restriction ? it‚Äôs important that the instrument, z, affect the outcome, y, only through its effect on the endogenous regressor, x. suppose we are instrumenting with an ab email test pushing referrals. if the control group received no email, this assumption isn‚Äôt valid: the act of getting an email could in and of itself drive retention. but if the control group received an otherwise-similar email, just without any mention of the referral program, then the exclusion restriction likely holds. a quick note on statistical significance in iv. you may have noticed that the r code to print the iv results isn‚Äôt just a simple call to summary(fit). that‚Äôs because we have be careful about how we compute standard errors in iv models. in particular, standard errors have to be corrected to account for the two stage design, which generally makes them slightly larger. want to estimate the causal effects of x on y but don‚Äôt have any good historical ab tests on hand to leverage? ab tests can also be implemented specifically to facilitate iv estimation! these ab tests even come with a sexy name ‚Äî randomized encouragement trials . it‚Äôs no surprise that we invest in building predictive models to understand who will do what when. it‚Äôs always fun to predict the future. but it‚Äôs even more fun to improve that future. where, when, how, and with whom can we intervene for better outcomes? by shedding light on the mechanisms driving the outcomes we care about, causal inference gives us the insights to focus our efforts on investments that better serve our users and our business. today, we briefly covered a range of methods for causal inference when ab testing is off the table. we hope these methods will help you uncover some of the actionable insights that can move your company‚Äôs mission forward. comments? suggestions? reach out at emily@coursera.org and duncan@wealthfront.com . just want to do fun stuff with us? we‚Äôd love to hear from you! plus, we‚Äôre always hiring. big data startup technology data science economics -- -- 9 published in teconomics 986 followers ¬∑ last published dec 31, 2017 economists in tech written by emily glassberg sands 1.7k followers ¬∑ 243 following head of data science @coursera, harvard econ phd responses ( 9 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://balazskegl.medium.com/the-data-science-ecosystem-678459ba6013,,,"the data science ecosystem. actors, incentives, challenges | by bal√°zs k√©gl | medium sitemap open in app sign up sign in medium logo write sign up sign in the data science ecosystem bal√°zs k√©gl 9 min read ¬∑ apr 16, 2015 -- 1 listen share actors, incentives, challenges data science initiatives have been popping up at an increasing pace in the last couple of years all around the world. in the us , following the announcement of the national big data r&d initiative of the white house in 2012, both the national funding agencies and individual universities engage in large-scale top-down actions in order to promote data science and research on big data. new york university opens its center for data science , university of washington founds its escience institute , berkeley launches its institute for data science , and the moore and sloan foundations announce a five-year 37.8m$ cross-institutional initiative to support the three previous institutes. columbia opens its institute for data sciences and engineering , the university of rochester announces a 100m$ commitment to create and house its institute for data science , and umass launches its center for data science . europe follows with the university of amsterdam creating its data science research center , edinburgh university launching its center for doctoral training in data science , delft university of technology initiating delft data science , and our newly inaugurated universit√© paris-saclay opening its paris-saclay center for data science . whereas we hear plenty of similar top-down mission statements about what data science is and how we should bring it to domain sciences, there is relatively little information on what these centers do , what bottom-up tools they create to enable data science research, and especially on what they do to tackle the unique new challenges of this fast-moving multi-disciplinary domain. at the pscds we actually believe in these mission statements. we believe that data science is the key to solve major scientific and societal problems. but we also know, experience shows, that our current ways of doing science just does not work for this domain . even with the best intentions, nothing good will happen if we put five physicists and five computer scientists in the room and tell them: ‚Äúgo at it‚Äù. we will need new tools, software but mainly management, to get the juices of collective creativity going. some of these tools exist, but most of them will have to be invented or adapted to the unique needs of this domain. our goal here is to initiate a conversation among those interested in the self-reflective research of how we will do data science. we start off the conversation with an essay defining the data science ecosystem with its constituents, actors, and challenges. data science is a deeply interdisciplinary domain. besides the usual challenges of projects involving experts of two distinct domains, potentially successful data science projects also have to include a third pole of software and system engineers who can implement the methods developed by data scientists, maintain the tools, and sometimes run the software in production mode. for a complete the ecosystem, we also need to define and fill the role of experts at the interfaces of these three poles. the following figure sketches a fully developed data science ecosystem, outlining the activities and the actors. the data science ecosystem: activities and actors the roles in this figure should be filled in a fully functioning data science ecosystem. we do not mean to identify and develop experts for these roles in isolation; real profiles usually overlap so researchers and engineers can fill several of these roles at the same time. nevertheless, it is useful to analyze the different roles and incentives in order to understand the challenges in building and running a data science ecosystem. the domain scientist experimental domain scientists build instruments and detectors, collect data, and analyze the data in order to study new phenomena or to discover new laws of nature. they usually work with engineers both at the instrumentation/product development and at the data acquisition interfaces. they may also be acquainted by the data science and software engineering aspects of the analysis chain, but their main drive is scientific discovery and their carrier incentive is publishing scientific results. they are less interested in advancing the state of the art in data science as long as the data analysis gets done reasonably efficiently. they may be interested in developing and maintaining single-purpose software tools which can be reused in other experiments within the same scientific domain. the data scientist data scientists design and analyze algorithms. their main drive is to propose new or improved methods or to analyze them with new techniques. improvement is measured on standard and well known-problems and benchmark data . their carrier incentive is to publish technical papers on methods. they are less interested in solving actual problems as long as the motivation to develop the method is plausible and/or well accepted within the technical community. they are especially turned off when a problem can be solved by existing techniques which are not publishable in technical venues. they are interested in building tools which are flexible enough to allow wide methodological experimentation , but which do not necessarily have the quality and efficiency to be used in large-scale production. the software engineer software engineers implement existing techniques, design and maintain software, and run large-scale production on large data and large computational resources. their main drive and carrier incentive is to build tools that are used by a large community . software engineers are often employed by domain science laboratories, so these tools often focus on the problems of a given scientific community. for the same reason, software engineers may also be acquainted by domain science but rarely with the latest research in data science. softwares are built for particular application domains, so most of the developed tools are not shared in a horizontal fashion among different domains . software engineers also work with system engineers who build and run large computational infrastructures, and who develop the middleware to provide flexible ways to access the infrastructure. the data engineer data engineers are familiar with the latest developments in data science and, at the same time, with state-of-the-art software engineering tools and techniques. their goal is to develop general purpose software tools that can be reused across several domains. the applied scientist applied scientist are familiar with the latest developments in data science and, at the same time, with the problems and vocabulary of a domain science or an application domain. their goal is to bring state-of-the art data science techniques to domain scientists , to formalize domain-specific problems in a language that can be understood by data scientists, and, in a strategic role, to identify innovative directions within a data-intensive application domain or domain science that can only be proposed by someone who has a broad vision of the available data science techniques. the data trainer data trainers train domain scientists to use data science software tools for solving their domain science problems. they have a broad knowledge about existing data science techniques and tools, as well as about training techniques. they do not necessarily carry out research in either data science or domain science, and they also do not necessarily participate in the software development. today this ecosystem exists only in a small number of large multinational it companies and in some data-intensive sciences with large experimental projects. in scientific domains with smaller experiments and in smaller companies with a primary focus other than data, challenges related to developing and managing the data science ecosystem come in all shapes and sizes. that said, we can identify and describe some of the typical challenges, generalizable across multiple domains. manpower to a large extent, the major bottleneck is the lack of manpower. we have arguably enough domain scientists and software engineers, but there is a major mismatch between supply and demand in any of the remaining four roles related to data. the recent success of data science in the it industry means that a handful of multinational companies, largely concentrated in north america, are draining data scientists and engineers from the academic sector, making it hard to trigger an exponential growth of the number of data scientists. most of these companies work on highly lucrative applications (e.g., optimizing advertisement and product recommendation), so it is increasingly difficult to find competent data scientists to work on less lucrative but highly important public societal domains (e.g., health care, transportation, energy), let alone (domain) science. incentives most data scientists, as other scientists, are trained and incentivized to do research on highly specialized domains . they search scientific visibility in their international community, which is equally highly specialized, because their carrier advancement is almost entirely based on peer-reviewed publications. even when they would have the expertise, they have little incentive to venture into the tool builder (data engineer) role since software authorship has little value in their evaluation, and it can only serve them implicitly through the visibility they gain in the community of tool users. by the same token, they have little incentive to venture into domain sciences and to tackle economic or societal challenges. it is possible that a domain science requires new techniques which then can be published in data science venues, but this is not guaranteed at all. it usually takes heavy investment of time and effort to be able to understand domain problems, so excursions into domain sciences are highly risky. even when such collaborations are established, data scientists have a strong prior to use their highly specialized expertise which is not necessarily the best solution for a given problem . finally, data scientists have little incentive to bring the project to full fruition, and they often ‚Äúrun away‚Äù with an abstract data science problem (and solution) extracted from the project. symmetrically, domain scientist have no incentive to advance data science and to develop and publish new techniques, as long as their data science problems get solved. when they venture into tool development, they have little incentive fordeveloping general purpose tools . finally, none of the researchers have interest in taking on the crucial data trainer role . they are incentivized to teach (as part of their full-time or part-time contract), but teaching in general is usually secondary in their carrier development, and within teaching, they usually prefer to train highly-skilled students (master and ph.d.) who can participate in research. they are also not necessarily equipped to teach tools to domain scientists because they themselves are not familiar with a broad set of tools. data and software engineers have, again, little incentive to venture into training. access even though they are not interested in advancing data science, domain scientists are very much interested in collaborating with data scientists and data engineers to solve their data analysis problems. on the other hand, data scientists may be interested in applicative domains if they can transfer their narrow but deep expertise, but only if it takes relatively little effort (risk aversion). they may also be interested in collaborating with data engineers to implement their techniques in general purpose tools, for reaping the indirect benefits related to visibility. the difficulty is that there are no well-developed channels to identify the right experts for a given problem , so most of the collaboration happens through ad-hoc channels, essentially by chance. the matching usually requires a special expert , the applied scientist (see figure), who has a broad overview of existing techniques and available experts, and who can also converse with domain experts. such experts are rare. tools even when the right experts have been identified and they are motivated, there are few tools that can help them to collaborate efficiently . the process of a data scientist picking up domain science expertise or vice versa is long and laborious. success stories usually involve some form of ‚Äúembedding‚Äù researchers in each other‚Äôs teams , most often a data scientist visiting a domain science lab for an extended period of time. this analysis has been naturally shaped by the realities of the french research system. nevertheless, we believe that most of these challenges are universal, and that nobody with the ambition of running a successful data science ecosystem can ignore them. solving some of the challenges are beyond what a local center can do. for example, changing deeply interiorized incentive structures would require strong top-down signals; here our role is limited to lobbying and hammering the message. on other fronts, such as improving access and communication channels or building tools to use data science expertise efficiently, data science centers can have a crucial role. at the pscds , we have been designing and learning to manage such tools. we will soon start communicating about them so that other centers can learn from our experience. by the same token, it would also be great to read about the experience of other data science initiatives around the world. if you like what you read, follow me on medium , linkedin , & twitter . data science -- -- 1 written by bal√°zs k√©gl 724 followers ¬∑ 224 following head of ai research, huawei france, previously head of the paris-saclay center for data science. podcast: https://www.youtube.com/@i.scientist.balazskegl responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",17
https://medium.com/wunderlist-engineering/amazon-redshift-s-udf-b3d4205ae732,,,"amazon redshift‚Äôs udf. as you have probably heard, amazon‚Ä¶ | by bence faludi | wunderlist engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in wunderlist engineering ¬∑ stories from the team working on wunderlist in berlin amazon redshift‚Äôs udf bence faludi 8 min read ¬∑ oct 15, 2015 -- 5 listen share as you have probably heard, amazon released user defined functions for amazon redshift and now you can write your own stored procedures in python . i, myself, was looking forward to it. however, after having conducted a few tests, i have developed mixed feelings towards amazon redshift‚Äôs udf feature. let‚Äôs do some simple tests! in our data warehouse, we store client identifiers for every event in a character varying column. they can be literally anything and we use the same condition in multiple sql files when we want to decode them into understandable, human readable names. this isn‚Äôt a good approach ‚Äî in fact, it‚Äôs terribly risky because if we change the condition in only one file, then our dataset will be inconsistent. as all of us working with data know, data inconsistency is never good. so we decided to try to fix the problem with the use of a udf. the original condition was the following: case when client_id = ‚Äò784cc8a1ebf89f‚Äô then ‚Äòmac‚Äô when client_id = ‚Äò39087b8db19be5‚Äô then ‚Äòios‚Äô when client_id = ‚Äò905eb7572309c0‚Äô then ‚Äòios‚Äô when client_id = ‚Äò9f6f660b58b49e‚Äô then ‚Äòandroid‚Äô when client_id = ‚Äò58e7d0fd643292‚Äô then ‚Äòandroid‚Äô when client_id = ‚Äò8383ce7c3eb0e5‚Äô then ‚Äòwindowsphone‚Äô when client_id = ‚Äòa2fa2bb0e727b7‚Äô then ‚Äòwindowsstore‚Äô when client_id = ‚Äò14eaa2a06ae9e5‚Äô then ‚Äòweb‚Äô when client_id = ‚Äò4d6012e4f7620e‚Äô then ‚Äòchrome‚Äô when client_id = ‚Äò52f0c56e495f6b‚Äô then ‚Äòwindowswrapper‚Äô when client_id = ‚Äò8074ebdb3d5e9c‚Äô then ‚Äòwindows10‚Äô else null end there are multiple solutions for this problem like: create a new table with the values and names and join the tables when it‚Äôs necessary. the advantage is we only need to change the client_id/platform pairs in one table. the downside is the size of the tables. we have a lot of tables with more than 2 billion records and joining tables isn‚Äôt the quickest thing in redshift. create a user defined function and write these conditions in python. no join necessary. my assumption was that using udfs would be slower than the original one, but it would be quicker than joining the tables with left outer join. however, the problem was still there, so i had to count the misclassified records. it was my test case for this problem. i‚Äôve tried the queries on our aggregated events table with 471,895,959 records on ssd. the first comparison‚Äòs (i‚Äôve removed the condition but it‚Äôs the same as above) average execution time of several runs was 3 218.347 ms. select e.platform, case when <...> end as ‚Äúvalid_platform‚Äù, sum(e.count) as ‚Äúevent_count‚Äù from real.events_counted_month_2015_07_01 as e where case when <...> end != e.platform group by 1, 2 order by 3 desc; i was really curious about the udf feature . i created it right after i got the original result. create function f_get_platform_by_client_id ( client_id varchar ) returns varchar immutable as $$ if client_id == ‚Äò784cc8a1ebf89f‚Äô: return ‚Äòmac‚Äô elif client_id == ‚Äò39087b8db19be5‚Äô: return ‚Äòios‚Äô ... else: return none $$ language plpythonu; ‚Ä¶ and executed the long-awaited query. select e.platform, f_get_platform_by_client_id(e.client_id) as ‚Äúvalid_platform‚Äù, sum(e.count) as ‚Äúevent_count‚Äù from real.events_counted_month_2015_07_01 as e where f_get_platform_by_client_id(e.client_id) != e.platform group by 1, 2 order by 3 desc; the result was surprisingly awful. it finished under ~ 54 761.971 ms. it‚Äôs more than 10 times slower than the original condition. my third test case was the new const table solution. i created it and inserted the client_id and platform pairs. finally i joined these tables together to get the counts. the average execution time was 3 414.535 ms . create table temp.client_platform ( client_id varchar(50), platform varchar(127) ) sortkey(client_id); insert into temp.client_platform (client_id, platform) values ...; select e.platform, p.platform as ‚Äúvalid_platform‚Äù, sum(e.count) as ‚Äúevent_count‚Äù from real.events_counted_month_2015_07_01 as e left outer join temp.client_platform as p on ( p.client_id = e.client_id ) where p.platform != e.platform group by 1, 2 order by 3 desc; based on the results of the tests, i cannot recommend anyone to use udf‚Äôs for short scripts when it‚Äôs possible to do with pure sql. let‚Äôs kill the snake. of course there are other cases when the udf support can be a game changer. until now if you wanted to do something which was not possible within sql you had to dump your data from redshift into s3 download it into your server or your local machine transform and evaluate the data upload into s3 load into redshift it was pain in the back to be honest. but after the arrival of udf support we can do these transformations within redshift. heck yeah! i started to roll out new research to get a quick overview on user‚Äôs language usage within wunderlist . we wanted to determine the language of every list we have. i chose to detect lists because i believed the task could be accomplished within reasonable amount of time. my goal was to spend no more than a day on this research. we had about 84 million lists and 11 million unique titles at the time. i saw an interesting and usable python package in pypi , called langdetect . this package was ported from java and doing naive bayes with character n-grams to detect the language possibilities . maybe it wasn‚Äôt the best tool but it was good enough to help with performing the examination. so i crafted this function: create or replace function detect_language( title varchar ) returns varchar immutable as $$ import langdetect if not title: return none if max(map(len, title.split())) <= 4 and len(title) <= 12: return none try: return langdetect.detect(title.decode(‚Äòutf-8‚Äô)) except: return none $$ language plpythonu; it‚Äôs kind of a small and stupid script. all that left was to install the required library. i was naive because i executed the following command without hesitation: create library langdetect language plpythonu from ‚Äò https://pypi.python.org/packages/source/l/langdetect/langdetect-1.0.5.zip' ; it worked but when i tried to test the function i got the following error message: importerror: no module named langdetect. the directory structure of any python package of pypi won‚Äôt fit into redshift‚Äôs expected udf structure. they want to get a package with the following structure: langdetect.zip __init__.py langdetect/ __init__.py ... ‚Ä¶ and the average python package‚Äôs directory structure is something like this: packagename.zip setup.cfg setup.py packagename/ __init__.py ... probably you noticed that the problem is the __init__.py file in the root. if you want to use a python package you have to download it. create an empty __init__.py file. compress the content into a zip file. upload it into s3. install the library. recursively repeat it with every dependency the package has. plus, you have to be really careful and wary. they said the following: also, functions that write files or access the network are not supported. ‚Äî aws official blog only one thing is missing! they don‚Äôt let you to read files either. if your python package want to open any file then the package will fail instantly. i‚Äôve created another test python library, called mypkg which does the following: import os def get_dirname(): return os.path.dirname(__file__) def get_file_content(): path = os.path.join(get_dirname(),'otherfile.txt') fd = open(path, 'r') content = fd.read() fd.close() return content i created the otherfile.txt too with a few words in it. the get_dirname() function is working properly and i got a reasonable result: /rdsdbdata/user_lib/0/0/197285.zip/mypkg get_file_content() function didn‚Äôt work. error : error: ioerror: (20, ‚Äònot a directory‚Äô, ‚Äò/rdsdbdata/user_lib/0/0/197285.zip/mypkg/otherfile.txt‚Äô). personally, i think it‚Äôs a shame because some python package uses json or pickle files to contain extra information. if your package is like that then you have to modify your package and have to relocate the related data blobs within the python code itself. the langdetect package is like this. it contains the languages‚Äô n-gram data in a json file per language. i changed the package to work well and removed every file opening when i met a new interesting problem: unicode support! i think if you have ever used python 2.7 before you know exactly how painful it can be to deal with unicodedecode errors. furthermore, i think you also know how problematic redshift‚Äôs unicode support is. for me it was a strange lesson when i got some mysterious errors but the package performed well in the console. error : error: internal error while processing udf request sadly, i didn‚Äôt find the unsupported letter in half of the language profiles. however, it works most of the time. # -*- coding: utf-8 -*- def get_unicode_chars(): return u‚Äô–Ω–µ—Ç or Ê≤íÊúâ‚Äô i was more than happy when i was finally able to create a working library that can detect languages within redshift. nevertheless, i was not able to port every language within an hour because of internal server errors. finally i got a package that can detect 24 languages. (the original package can detect 53 languages). let‚Äôs see some benchmark comparison between the redshift‚Äôs udf and the python code working on a csv file in linux. select l.title, detect_language(l.title) as ‚Äúlanguage‚Äù from temp.latest_lists as l; the result was interesting and unpredictable. it appears the execution time is directly proportional to the number of records in the dataset. i ran this test on our 18 x dc1.large cluster on ssd. 1,000/1,000 records: 1 377.072 ms 1,000/10,000 records: 5 764.857 ms 10,000/10,000 records: 5 790.040 ms 1,000/100,000 records: 46 050.904 ms 100,000/100,000 records: 46 032.125 ms 1,000/1,000,000 records: 101 904.887 ms 1,000/1,000,000 records with subquery: 11 161.097 ms 1,000,000/1,000,000 records: 440 353.064 ms 1,000/84,000,000 records: 106 628.889 ms 1,000/84,000,000 records with subquery: 10 315.718 ms that means if you want to test your function you have to create a temporary table or use a subquery with a few records for testing and if it‚Äôs working as expected, then you can execute it on your original data. after several execution of the queries on the same dataset the results did not change drastically. therefore, i really miss caching on the immutable data. let‚Äôs see some baseline number to understand the speed above. running the detection on a local computer (2,7 ghz intel core i5) on a single core with python 3.5 will decode 1,000 records in 7 594 ms and 10,000 records in 73.81 seconds . it contains the time when it‚Äôs creating the output csv file. i‚Äôve rolled out 100,000 records‚Äô detection with splitting and merging the files together on 4 virtual cores. the execution time was 287.49 seconds . i believe the performance of redshift‚Äôs udf sounds good. however if you build a c3.8xlarge computer (32 vcpu) it will finish sooner than redshift (36 vcpu in our cluster) with almost zero load. conclusion although, i only checked these use cases, i have mixed feelings. my guess is that redshift‚Äôs udfs are really good for calculating metrics and doing complex math with the pre-installed packages like numpy , pandas , scipy . otherwise i don‚Äôt recommend it at the moment because of the lack of python package support and the difficulties to make it work. we should test it again occasionally because it has the potential to be something really useful for our daily work. if you know any other use cases that perform well, please let me know. i am really interested in hearing more about this topic. i‚Äôm looking forward to see the following features: better unicode support and helpful error messages compatibility with pypi packages and auto-install dependencies increased performance caching of immutable function results i am a data & applied scientist at microsoft in berlin, germany. working on wunderlis t‚Äôs data infrastructure. you can find me on twitter , linkedin , if you‚Äòd like to connect. python redshift data -- -- 5 published in wunderlist engineering 91 followers ¬∑ last published oct 21, 2015 stories from the team working on wunderlist in berlin written by bence faludi 81 followers ¬∑ 30 following data & applied scientist @microsoft, working on @wunderlist responses ( 5 ) see all responses help status about careers press blog privacy rules terms text to speech",17
