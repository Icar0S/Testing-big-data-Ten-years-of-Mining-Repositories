Link,Tools_Found,Methods_Found,Post_Text,Cluster
https://dev.to/shittu_olumide_/how-to-install-pyspark-on-your-local-machine-nn4,,,"how to install pyspark on your local machine - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse shittu olumide posted on dec 9, 2024 how to install pyspark on your local machine # python # machinelearning # ai # programming if you’re stepping into the world of big data, you have likely heard of apache spark , a powerful distributed computing system. pyspark , the python library for apache spark, is a favorite among data enthusiasts for its combination of speed, scalability, and ease of use. but setting it up on your local machine can feel a bit intimidating at first. fear not — this article walks you through the entire process, addressing common questions and making the journey as straightforward as possible. what is pyspark, and why should you care? before going into installation, let’s understand what pyspark is. pyspark allows you to leverage the massive computational power of apache spark using python. whether you’re analyzing terabytes of data, building machine learning models, or running etl ( extract , transform , load ) pipelines, pyspark allows you to work with data more efficiently than ever. now that you understand pyspark, let’s go through the installation process. step 1: ensure your system meets the requirements pyspark runs on various machines, including windows , macos , and linux . here’s what you need to install it successfully: java development kit (jdk) : pyspark requires java (version 8 or 11 is recommended). python : ensure you have python 3.6 or later. apache spark binary : you’ll download this during the installation process. to check your system readiness: open your terminal or command prompt . type java -version and python —version to confirm java and python installations. if you don’t have java or python installed, follow these steps: for java : download it from oracle’s official website . for python : visit python’s download page . step 2: install java java is the backbone of apache spark . to install it: 1. download java : visit the java se development kit download page. choose the appropriate version for your operating system. 2. install java : run the installer and follow the prompts. on windows, you’ll need to set the java_home environment variable. to do this: copy the path variable, go to the local disk on your machine, select program files , look for the java folder open it you will see jdk-17 (your own version may not be 17). open it, and you will be able to see your path and copy like below search for environment variables in the windows search bar. under system variables , click new and set the variable name as java_home and the value as your java installation path you copied above ( e.g., c:\program files\java\jdk-17 ). 3. verify installation : open a terminal or command prompt and type java-version . step 3: install apache spark 1. download spark : visit apache spark’s website and select the version compatible with your needs. use the pre-built package for hadoop (a common pairing with spark). 2. extract the files : on windows , use a tool like winrar or 7-zip to extract the file. on macos/linux , use the command tar -xvf spark-.tgz 3. set environment variables : for windows : add spark’s bin directory to your system’s path variable. for macos/linux : add the following lines to your .bashrc or .zshrc file: export spark_home =/ path / to / spark export path = $ spark_home / bin : $ path enter fullscreen mode exit fullscreen mode 4. verify installation : open a terminal and type spark-shell . you should see spark’s interactive shell start. step 4: install hadoop (optional but recommended) while spark doesn’t strictly require hadoop , many users install it for its hdfs (hadoop distributed file system) support. to install hadoop: download hadoop binaries from apache hadoop’s website . extract the files and set up the hadoop_home environment variable. step 5: install pyspark via pip installing pyspark is a breeze with python’s pip tool. simply run: pip install pyspark enter fullscreen mode exit fullscreen mode to verify, open a python shell and type: pip install pysparkark . __version__ ) enter fullscreen mode exit fullscreen mode if you see a version number, congratulations! pyspark is installed 🎉 step 6: test your pyspark installation here’s where the fun begins. let’s ensure everything is working smoothly: create a simple script : open a text editor and paste the following code: from pyspark.sql import sparksession spark = sparksession . builder . appname ( "" pysparktest "" ). getorcreate () data = [( "" alice "" , 25 ), ( "" bob "" , 30 ), ( "" cathy "" , 29 )] columns = [ "" name "" , "" age "" ] df = spark . createdataframe ( data , columns ) df . show () enter fullscreen mode exit fullscreen mode save it as test_pyspark.py run the script : in your terminal, navigate to the script’s directory and type: python test_pyspark . py enter fullscreen mode exit fullscreen mode you should see a neatly formatted table displaying the names and ages . troubleshooting common issues even with the best instructions, hiccups happen. here are some common problems and solutions: issue : java.lang.noclassdeffounderror solution : double-check your java_home and path variables. issue : pyspark installation succeeded, but the test script failed. solution : ensure you’re using the correct python version. sometimes, virtual environments can cause conflicts. issue : the spark-shell command doesn’t work. solution : verify that the spark directory is correctly added to your path. why use pyspark locally? many users wonder why they should bother installing pyspark on their local machine when it’s primarily used in distributed systems. here’s why: learning : experiment and learn spark concepts without requiring a cluster. prototyping : test small data jobs locally before deploying them to a larger environment. convenience : debug issues and develop applications with ease. boost your pyspark productivity to get the most out of pyspark, consider these tips: set up a virtual environment : use tools like venv or conda to isolate your pyspark installation. integrate with ides : tools like pycharm and jupyter notebook make pyspark development more interactive. leverage pyspark documentation : visit apache spark’s documentation for in-depth guidance. engage with the pyspark community getting stuck is normal, especially with a powerful tool like pyspark. engage with the vibrant pyspark community for help: join forums : websites like stack overflow have dedicated spark tags. attend meetups : spark and python communities often host events where you can learn and network. follow blogs : many data professionals share their experiences and tutorials online. conclusion installing pyspark on your local machine may seem daunting at first, but following these steps makes it manageable and rewarding. whether you’re just starting your data journey or sharpening your skills, pyspark equips you with the tools to tackle real-world data problems. pyspark, the python api for apache spark, is a game-changer for data analysis and processing. while its potential is immense, setting it up on your local machine can feel challenging. this article breaks down the process step-by-step, covering everything from installing java and downloading spark to testing your setup with a simple script. with pyspark installed locally, you can prototype data workflows, learn spark’s features, and test small-scale projects without needing a full cluster. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand shittu olumide shittu olumide shittu olumide follow skilled software engineer and technical writer. location lagos, nigeria education olusegun agagu university of science and technology. pronouns he/him work founder velcast (bit.ly/velcast) joined apr 27, 2020 • apr 18 dropdown menu copy link hide follow for more content like this :) i also have a youtube channel where i post video tutorials; feel free to check it out here: youtube.com/channel/ucnhfxpk6hgt5u... thanks like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse shittu olumide follow skilled software engineer and technical writer. location lagos, nigeria education olusegun agagu university of science and technology. pronouns he/him work founder velcast (bit.ly/velcast) joined apr 27, 2020 more from shittu olumide functional programming in python: leveraging lambda functions and higher-order functions # programming # python # learning # beginners overview of statsmodels # programming # beginners # python # learning build a rag-powered research paper assistant # programming # ai # tutorial # python 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/alexmercedcoder/getting-started-with-data-analytics-using-pyarrow-in-python-4bnl,,,"getting started with data analytics using pyarrow in python - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse alex merced posted on oct 15, 2024 getting started with data analytics using pyarrow in python # python # database # analytics apache iceberg crash course: what is a data lakehouse and a table format? free copy of apache iceberg the definitive guide free apache iceberg crash course iceberg lakehouse engineering video playlist introduction overview of the docker environment in this guide, we will explore data analytics using pyarrow , a powerful library designed for efficient in-memory data processing with columnar storage. we will work within a pre-configured environment using the python data science notebook docker image . this environment includes all the essential libraries for data manipulation, machine learning, and database connectivity, making it an ideal setup for performing analytics with pyarrow. to get started, you can pull and run the docker container by following these steps: build the docker image: docker pull alexmerced/datanotebook enter fullscreen mode exit fullscreen mode run the container: docker run -p 8888:8888 -v $( pwd ) :/home/pydata/work alexmerced/datanotebook enter fullscreen mode exit fullscreen mode access jupyter notebook: open your browser and navigate to http://localhost:8888 to access the notebook interface. this setup provides a user-friendly experience with jupyter notebook running on port 8888, where you can easily write and execute python code for data analysis. why pyarrow? apache arrow is an open-source framework optimized for in-memory data processing with a columnar format. pyarrow, the python implementation of arrow, enables faster, more efficient data access and manipulation compared to traditional column-based libraries like pandas. here are some key benefits of using pyarrow: faster data processing: pyarrow uses a columnar memory layout that accelerates access to large datasets. lower memory usage: thanks to arrow’s efficient memory format, you can handle larger datasets with less memory. interoperability: pyarrow integrates smoothly with other systems and languages, making it a versatile tool for multi-language environments. better support for large datasets: pyarrow is designed to handle big data tasks, making it ideal for workloads that pandas struggles with. section 1: understanding key pyarrow objects pyarrow provides a set of data structures that are specifically optimized for in-memory analytics and manipulation. in this section, we will explore the key objects in pyarrow and their purposes. pyarrow's core data structures: table : a table in pyarrow is a collection of columnar data, optimized for efficient processing and memory usage. it can be thought of as similar to a dataframe in pandas but designed to work seamlessly with arrow’s columnar format. tables can be partitioned and processed in parallel, which improves performance with large datasets. example : import pyarrow as pa data = [ pa . array ([ 1 , 2 , 3 ]), pa . array ([ ' a ' , ' b ' , ' c ' ]), ] table = pa . table . from_arrays ( data , names = [ ' column1 ' , ' column2 ' ]) print ( table ) enter fullscreen mode exit fullscreen mode recordbatch : a recordbatch is a collection of rows with a defined schema. it allows for efficient in-memory processing of data in batches. it's useful when you need to process data in chunks, enabling better memory management. example: batch = pa . recordbatch . from_pandas ( df ) print ( batch ) enter fullscreen mode exit fullscreen mode array : an array in pyarrow is a fundamental data structure representing a one-dimensional, homogeneous sequence of values. arrays can be of various types, including integers, floats, strings, and more. pyarrow provides specialized arrays for different types of data. example: array = pa . array ([ 1 , 2 , 3 , 4 , 5 ]) print ( array ) enter fullscreen mode exit fullscreen mode schema: a schema defines the structure of data in a table or recordbatch. it consists of the names and data types of each column. schemas ensure that all data being processed follows a consistent format. example: schema = pa . schema ([ ( ' column1 ' , pa . int32 ()), ( ' column2 ' , pa . string ()) ]) print ( schema ) enter fullscreen mode exit fullscreen mode chunkedarray: a chunkedarray is a sequence of array objects that have been split into smaller chunks. this allows for parallel processing on chunks of data, improving efficiency when working with larger datasets. example: chunked_array = pa . chunked_array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ]]) print ( chunked_array ) enter fullscreen mode exit fullscreen mode summary these core objects are essential for working with pyarrow and enable efficient data processing in memory. by utilizing pyarrow's columnar format and its efficient handling of large datasets, you can perform complex data manipulations with ease. as we continue, you'll see how these objects interact to make reading, writing, and analyzing data faster and more memory-efficient. section 2: reading and writing parquet files with pyarrow parquet is a columnar storage file format that is widely used in big data analytics. its efficient compression and encoding make it ideal for storing large datasets. in this section, we will explore how to use pyarrow to read from and write to parquet files. why use parquet? efficient storage : parquet’s columnar format allows for efficient compression, reducing the storage size of large datasets. faster querying : by storing data in columns, parquet files allow analytical queries to scan only the relevant columns, reducing i/o and improving performance. interoperability : parquet is a widely supported format that can be read and written by many different systems, making it ideal for data exchange. reading parquet files using pyarrow, you can easily read a parquet file into memory as a pyarrow table . this table can then be used for further data processing or manipulation. example : import pyarrow.parquet as pq # reading a parquet file table = pq . read_table ( ' sample_data.parquet ' ) # displaying the contents of the pyarrow table print ( table ) enter fullscreen mode exit fullscreen mode in this example, the pq.read_table() function reads the parquet file and returns a table object. this table can now be used for in-memory operations such as filtering, joining, or aggregating data. writing parquet files to store data as parquet, you can write a pyarrow table back to disk in parquet format. pyarrow provides methods for this purpose, allowing you to save your data efficiently. example: import pyarrow as pa import pyarrow.parquet as pq # create a simple pyarrow table data = [ pa . array ([ 1 , 2 , 3 , 4 ]), pa . array ([ ' a ' , ' b ' , ' c ' , ' d ' ]) ] table = pa . table . from_arrays ( data , names = [ ' column1 ' , ' column2 ' ]) # writing the table to a parquet file pq . write_table ( table , ' output_data.parquet ' ) enter fullscreen mode exit fullscreen mode in this example, a pyarrow table is created and saved to disk as a parquet file using the pq.write_table() function. working with large datasets one of the key advantages of parquet is its ability to handle large datasets efficiently. when reading a parquet file, you can load only specific columns into memory, which is especially useful when working with large datasets. example: # reading only specific columns from a parquet file table = pq . read_table ( ' sample_data.parquet ' , columns = [ ' column1 ' ]) print ( table ) enter fullscreen mode exit fullscreen mode this code demonstrates how to read only the relevant columns, reducing the memory footprint when loading the dataset. summary by using pyarrow to read and write parquet files, you gain access to a highly efficient, compressed, and columnar data format that works well for large datasets. pyarrow simplifies working with parquet by providing easy-to-use functions for loading and saving data, while also supporting advanced operations like selective column reads to optimize performance. section 3: basic analytical operations with pyarrow pyarrow not only provides efficient tools for reading and writing parquet files but also enables you to perform basic data analytics operations like filtering, joining, and aggregating data in memory. these operations can be performed directly on pyarrow table objects, offering a significant performance boost when dealing with large datasets. filtering data pyarrow allows you to filter rows based on conditions, similar to how you would with pandas. this operation is highly efficient due to the columnar nature of pyarrow's data structures. example : import pyarrow.compute as pc # assume we have a table with two columns: 'column1' and 'column2' table = pq . read_table ( ' sample_data.parquet ' ) # apply a filter to keep rows where 'column1' > 2 filtered_table = table . filter ( pc . greater ( table [ ' column1 ' ], 2 )) print ( filtered_table ) enter fullscreen mode exit fullscreen mode in this example, we use pyarrow’s compute module to filter the data. the pc.greater() function returns a boolean mask, and the filter() method applies this mask to the table, returning only rows where 'column1' is greater than 2. joining data just like in sql or pandas, pyarrow allows you to join two tables based on a common column. this operation is particularly useful when combining datasets. example: import pyarrow as pa # creating two tables to join left_table = pa . table ({ ' key ' : [ 1 , 2 , 3 ], ' value_left ' : [ ' a ' , ' b ' , ' c ' ]}) right_table = pa . table ({ ' key ' : [ 1 , 2 , 3 ], ' value_right ' : [ ' x ' , ' y ' , ' z ' ]}) # performing an inner join on the 'key' column joined_table = left_table . join ( right_table , keys = ' key ' ) print ( joined_table ) enter fullscreen mode exit fullscreen mode here, we use pyarrow’s join method to perform an inner join on two tables, combining them based on the common column 'key'. the result is a new table with data from both tables. aggregation operations aggregation operations like summing, counting, and averaging are essential for data analytics. pyarrow provides efficient methods to perform these operations on large datasets. example: import pyarrow.compute as pc # assume we have a table with a numerical column 'column1' table = pq . read_table ( ' sample_data.parquet ' ) # perform aggregation: sum of 'column1' sum_column1 = pc . sum ( table [ ' column1 ' ]) print ( f "" sum of column1: { sum_column1 . as_py () } "" ) enter fullscreen mode exit fullscreen mode in this example, we use the pc.sum() function to calculate the sum of a column. similarly, you can apply other aggregation functions like pc.mean() , pc.min() , or pc.max() . combining operations: filter and aggregate pyarrow allows you to chain operations together, such as filtering the data first and then applying aggregation. example: # filter the table where 'column1' > 2 filtered_table = table . filter ( pc . greater ( table [ ' column1 ' ], 2 )) # sum the filtered data in 'column1' sum_filtered = pc . sum ( filtered_table [ ' column1 ' ]) print ( f "" sum of filtered column1: { sum_filtered . as_py () } "" ) enter fullscreen mode exit fullscreen mode in this case, we first filter the data and then apply the aggregation function on the filtered subset. this combination of operations enables more complex analyses with just a few lines of code. summary pyarrow’s powerful analytical capabilities make it a great choice for performing data operations on large datasets. by leveraging its efficient in-memory structures, you can filter, join, and aggregate data in a way that is both fast and memory-efficient. whether you are working with small or large datasets, pyarrow provides the tools to handle your data analytics tasks with ease. section 4: working with json, csv, and feather files using pyarrow in addition to parquet, pyarrow supports a wide variety of file formats, including json, csv, and feather. these formats are commonly used for data storage and interchange, and pyarrow makes it easy to read from and write to them efficiently. reading and writing json files json (javascript object notation) is a lightweight data-interchange format that is widely used for data transfer. while it may not be as efficient as columnar formats like parquet, json is still commonly used, especially for web data. reading json files pyarrow allows you to read json data and convert it into a pyarrow table for further processing. example : import pyarrow.json as paj # reading a json file into a pyarrow table table = paj . read_json ( ' sample_data.json ' ) # display the contents of the table print ( table ) enter fullscreen mode exit fullscreen mode writing json files pyarrow can also write table data back into json format, making it convenient for exchanging data in systems where json is the preferred format. example: import pyarrow as pa import pyarrow.json as paj # create a simple pyarrow table data = { ' column1 ' : [ 1 , 2 , 3 ], ' column2 ' : [ ' a ' , ' b ' , ' c ' ] } table = pa . table . from_pydict ( data ) # writing the table to a json file paj . write_json ( table , ' output_data.json ' ) enter fullscreen mode exit fullscreen mode reading and writing csv files csv (comma-separated values) is one of the most common file formats for structured data, particularly in data science and analytics. pyarrow makes it easy to work with csv files by converting them to table objects. reading csv files pyarrow’s csv reader allows for fast parsing of large csv files, which can then be converted into tables for in-memory analytics. example: import pyarrow.csv as pac # reading a csv file into a pyarrow table table = pac . read_csv ( ' sample_data.csv ' ) # display the table print ( table ) enter fullscreen mode exit fullscreen mode writing csv files you can also write pyarrow tables back to csv format, which is helpful for data sharing and reporting. example: import pyarrow as pa import pyarrow.csv as pac # create a simple pyarrow table data = { ' column1 ' : [ 1 , 2 , 3 ], ' column2 ' : [ ' a ' , ' b ' , ' c ' ] } table = pa . table . from_pydict ( data ) # writing the table to a csv file pac . write_csv ( table , ' output_data.csv ' ) enter fullscreen mode exit fullscreen mode reading and writing feather files feather is a binary columnar file format that provides better performance compared to csv and json, while maintaining interoperability between python and r. pyarrow natively supports feather, allowing for efficient storage and fast reads. reading feather files feather files are ideal for fast i/o operations and work seamlessly with pyarrow. example: import pyarrow.feather as paf # reading a feather file into a pyarrow table table = paf . read_table ( ' sample_data.feather ' ) # display the table print ( table ) enter fullscreen mode exit fullscreen mode writing feather files pyarrow can write table objects to feather format, offering a balance between ease of use and performance, particularly for in-memory data sharing. example: import pyarrow as pa import pyarrow.feather as paf # create a simple pyarrow table data = { ' column1 ' : [ 1 , 2 , 3 ], ' column2 ' : [ ' a ' , ' b ' , ' c ' ] } table = pa . table . from_pydict ( data ) # writing the table to a feather file paf . write_feather ( table , ' output_data.feather ' ) enter fullscreen mode exit fullscreen mode summary pyarrow’s support for various file formats—such as json, csv, and feather—makes it a versatile tool for data analytics. whether you're working with structured csvs, exchanging json data, or aiming for faster performance with feather files, pyarrow simplifies the process of reading and writing these formats. this flexibility allows you to handle a wide range of data tasks, from data ingestion to efficient storage and retrieval. section 5: using apache arrow flight with pyarrow apache arrow flight is a high-performance data transport layer built on top of apache arrow. it provides an efficient way to transfer large datasets between systems. one of its key benefits is the ability to perform fast, scalable data transfers using grpc for remote procedure calls. in this section, we will explore how to use apache arrow flight with pyarrow with an example of connecting to dremio , a popular data platform that supports arrow flight for query execution. connecting to dremio using pyarrow flight below is an example of how to connect to dremio using pyarrow flight, execute a query, and retrieve the results. from pyarrow import flight from pyarrow.flight import flightclient import os # step 1: set the location of the arrow flight server location = "" grpc+tls://data.dremio.cloud:443 "" # step 2: obtain the authentication token (from environment variables in this case) token = os . getenv ( "" token "" ) # step 3: define the headers for the flight requests
# here, we pass the bearer token for authentication headers = [ ( b "" authorization "" , f "" bearer { token } "" . encode ( "" utf-8 "" )) ] # step 4: write the sql query you want to execute query = "" select * from table1 "" # step 5: create a flightclient instance to connect to the server client = flightclient ( location = location ) # step 6: set up flightcalloptions to include the authorization headers options = flight . flightcalloptions ( headers = headers ) # step 7: request information about the query's execution flight_info = client . get_flight_info ( flight . flightdescriptor . for_command ( query ), options ) # step 8: fetch the results of the query results = client . do_get ( flight_info . endpoints [ 0 ]. ticket , options ) # step 9: read and print the results from the server print ( results . read_all ()) enter fullscreen mode exit fullscreen mode explanation of each step set the flight server location: location = "" grpc+tls://data.dremio.cloud:443 "" enter fullscreen mode exit fullscreen mode the location variable holds the address of the dremio server that supports apache arrow flight. here, we use grpc over tls for a secure connection to dremio cloud. authentication with bearer token: token = os . getenv ( "" token "" ) enter fullscreen mode exit fullscreen mode the token is retrieved from an environment variable using os.getenv() . this token is required for authenticating requests to dremio’s arrow flight server. setting request headers: headers = [ ( b "" authorization "" , f "" bearer { token } "" . encode ( "" utf-8 "" )) ] enter fullscreen mode exit fullscreen mode the headers include an authorization field with the bearer token, which is required for dremio to authenticate the request. we use the flightcalloptions to attach this header to our request later. sql query: query = "" select * from table1 "" enter fullscreen mode exit fullscreen mode this is the sql query we will execute on dremio. you can replace ""table1"" with any table or a more complex sql query as needed. creating the flightclient: client = flightclient ( location = location ) enter fullscreen mode exit fullscreen mode the flightclient is the main object used to interact with the arrow flight server. it is initialized with the location of the server, allowing us to send requests and receive results. setting flight call options: options = flight . flightcalloptions ( headers = headers ) enter fullscreen mode exit fullscreen mode here, flightcalloptions is used to attach the headers (including our authentication token) to the requests made by the flightclient. fetching flight information: flight_info = client . get_flight_info ( flight . flightdescriptor . for_command ( query ), options ) enter fullscreen mode exit fullscreen mode the get_flight_info() function sends the query to dremio and returns information about the query’s execution, such as where the results are located. the flightdescriptor.for_command() method is used to wrap the sql query into a format understood by the flight server. retrieving the query results: results = client . do_get ( flight_info . endpoints [ 0 ]. ticket , options ) enter fullscreen mode exit fullscreen mode the do_get() function fetches the results of the query from the server. it takes in a ticket, which points to the data location, and the options to pass authentication headers. reading and printing results: print ( results . read_all ()) enter fullscreen mode exit fullscreen mode finally, the read_all() function is called to read all of the results into memory, and print() displays the data. benefits of using apache arrow flight high performance: arrow flight is optimized for fast, high-volume data transfers, making it ideal for large datasets. grpc communication: the use of grpc allows for more efficient, low-latency communication between systems. cross-language support: arrow flight works across multiple programming languages, providing flexibility in how data is accessed and processed. summary apache arrow flight with pyarrow offers an efficient and powerful way to transport data between systems, especially in high-performance environments. using the example above, you can easily connect to dremio, execute queries, and retrieve data in a highly optimized fashion. the combination of arrow's in-memory data structures and flight's fast data transport capabilities makes it an excellent tool for scalable, real-time data analytics. conclusion in this blog, we explored the powerful capabilities of pyarrow for data analytics and efficient data handling. we began by setting up a practice environment using a python data science notebook docker image , which provides a comprehensive suite of pre-installed libraries for data manipulation and analysis. we discussed the core benefits of pyarrow over traditional libraries like pandas, focusing on its performance advantages, particularly for large datasets. pyarrow's columnar memory layout and efficient in-memory processing make it a go-to tool for high-performance analytics. throughout the blog, we covered key pyarrow objects like table , recordbatch , array , schema , and chunkedarray , explaining how they work together to enable efficient data processing. we also demonstrated how to read and write parquet , json , csv , and feather files, showcasing pyarrow's versatility across various file formats commonly used in data science. additionally, we delved into essential data operations like filtering, joining, and aggregating data using pyarrow. these operations allow users to handle large datasets efficiently while performing complex analyses with minimal memory usage. lastly, we introduced apache arrow flight as a high-performance transport layer for data transfer. we provided a detailed example of how to connect to dremio , execute sql queries, and retrieve results using arrow flight, highlighting its benefits for scalable, real-time data access. with these tools and techniques, you are equipped to perform efficient data analytics using pyarrow, whether you're working with local files or connecting to powerful cloud-based platforms like dremio. by leveraging pyarrow's capabilities, you can handle big data tasks with speed and precision, making it an indispensable tool for modern apache iceberg crash course: what is a data lakehouse and a table format? free copy of apache iceberg the definitive guide free apache iceberg crash course iceberg lakehouse engineering video playlist top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse alex merced follow i like to code, teach and promote. subscribe to the web dev 101 and datanation podcasts. blogs and videos at devnursery.com and grokoverflow.com. find everything at alexmerced.com location orlando, fl work head of devrel @ dremio joined dec 8, 2019 more from alex merced introduction to data engineering concepts |18| the power of dremio in the modern lakehouse # data # datascience # dataengineering # database introduction to data engineering concepts |17| apache iceberg, arrow, and polaris # data # datascience # database # dataengineering introduction to data engineering concepts |16| data lakehouse architecture explained # data # datascience # database # dataengineering 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/shittu_olumide_/how-to-use-pyspark-for-machine-learning-62l,,,"how to use pyspark for machine learning - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse shittu olumide posted on dec 4, 2024 how to use pyspark for machine learning # tutorial # python # machinelearning since the release of apache spark (an open-source framework for processing big data), it has become one of the most widely used technologies for processing large amounts of data in parallel across multiple containers — it prides itself on efficiency and speed compared to similar software that existed before it. working with this amazing technology in python is feasible through pyspark, a python api that allows you to interact with and tap into apachespark’s amazing potential using the python programming language. in this article, you will learn and get started with using pyspark to build a machine-learning model using the linear regression algorithm. note : having prior knowledge of python, an ide like vscode, how to use a command prompt/terminal and familiarity with machine learning concepts is essential for proper understanding of the concepts contained in this article. by going through this article, you should be able to: understand what apachespark is. learn about pyspark and how to use it for machine learning. what’s pyspark all about? according to the apache spark official website , pyspark lets you utilize the combined strengths of apachespark (simplicity, speed, scalability, versatility) and python (rich ecosystem, matured libraries, simplicity) for “ data engineering, data science, and machine learning on single-node machines or clusters .” image source pyspark is the python api for apachespark, which means it serves as an interface that lets your code written in python communicate with the apachespark technology written in scala. this way, professionals already familiar with the python ecosystem can quickly utilize the apachespark technology. this also ensures that existing libraries used in python remain relevant. detailed guide on how to use pyspark for machine learning in the ensuing steps, we will build a machine-learning model using the linear regression algorithm: install project dependencies : i’m assuming that you already have python installed on your machine. if not, install it before moving to the next step. open your terminal or command prompt and enter the code below to install the pyspark library . pip install pyspark enter fullscreen mode exit fullscreen mode you can install these additional python libraries if you do not have them. pip install pandas numpy enter fullscreen mode exit fullscreen mode create a file and import the necessary libraries : open vscode, and in your chosen project directory, create a file for your project, e.g pyspart_model.py . open the file and import the necessary libraries for the project. from pyspark.sql import sparksession from pyspark.ml.feature import vectorassembler from pyspark.ml.classification import logisticregression from pyspark.ml.evaluation import binaryclassificationevaluator import pandas as pd enter fullscreen mode exit fullscreen mode create a spark session : start a spark session for the project by entering this code under the imports. spark = sparksession . builder . appname ( "" logisticregressionexample "" ). getorcreate () enter fullscreen mode exit fullscreen mode read the csv file (the dataset you will be working with) : if you already have your dataset named data.csv in your project directory/folder, load it using the code below. data = spark . read . csv ( "" data.csv "" , header = true , inferschema = true ) enter fullscreen mode exit fullscreen mode exploratory data analysis : this step helps you understand the dataset you are working with. check for null values and decide on the cleansing approach to use. # display the schema my data . printschema () # show the first ten rows data . show ( 10 ) # count null values in each column missing_values = df . select ( [ count ( when ( isnull ( c ), c )). alias ( c ) for c in df . columns ] ) # show the result missing_values . show () enter fullscreen mode exit fullscreen mode optionally, if you are working with a small dataset, you can convert it to a python data frame and directory and use python to check for missing values. pandas_df = data . topandas () # use pandas to check missing values print ( pandas_df . isna (). sum ()) enter fullscreen mode exit fullscreen mode data preprocessing : this step involves converting the columns/features in the dataset into a format that pyspark’s machine-learning library can easily understand or is compatible with. use vectorassembler to combine all features into a single vector column. # combine feature columns into a single vector column feature_columns = [ col for col in data . columns if col != "" label "" ] assembler = vectorassembler ( inputcols = feature_columns , outputcol = "" features "" ) # transform the data data = assembler . transform ( data ) # select only the 'features' and 'label' columns for training final_data = data . select ( "" features "" , "" label "" ) # show the transformed data final_data . show ( 5 ) enter fullscreen mode exit fullscreen mode split the dataset : split the dataset in a proportion that is convenient for you. here, we are using 70% to 30%: 70% for training and 30% for testing the model. train_data , test_data = final_data . randomsplit ([ 0.7 , 0.3 ], seed = 42 ) enter fullscreen mode exit fullscreen mode train your model : we are using the logistic regression algorithm for training our model. create an instance of the logisticregression class and fit the model. lr = logisticregression ( featurescol = "" features "" , labelcol = "" label "" ) # train the model lr_model = lr . fit ( train_data ) enter fullscreen mode exit fullscreen mode make predictions with your trained model : use the model we have trained in the previous step to make predictions predictions = lr_model . transform ( test_data ) # show predictions predictions . select ( "" features "" , "" label "" , "" prediction "" , "" probability "" ). show ( 5 ) enter fullscreen mode exit fullscreen mode model evaluation : here, the model is being evaluated to determine its predictive performance or its level of correctness. we achieve this by using a suitable evaluation metric. evaluate the model using the auc metric evaluator = binaryclassificationevaluator ( rawpredictioncol = "" rawprediction "" , labelcol = "" label "" , metricname = "" areaunderroc "" ) # compute the auc auc = evaluator . evaluate ( predictions ) print ( f "" area under roc: { auc } "" ) enter fullscreen mode exit fullscreen mode the end-to-end code used for this article is shown below: from pyspark.sql import sparksession from pyspark.ml.feature import vectorassembler from pyspark.ml.classification import logisticregression from pyspark.ml.evaluation import binaryclassificationevaluator # start spark session spark = sparksession . builder . appname ( "" logisticregressionexample "" ). getorcreate () # load and preprocess data data = spark . read . csv ( "" data.csv "" , header = true , inferschema = true ) assembler = vectorassembler ( inputcols = [ col for col in data . columns if col != "" label "" ], outputcol = "" features "" ) data = assembler . transform ( data ). select ( "" features "" , "" label "" ) # split the data train_data , test_data = data . randomsplit ([ 0.7 , 0.3 ], seed = 42 ) # train the model lr = logisticregression ( featurescol = "" features "" , labelcol = "" label "" ) lr_model = lr . fit ( train_data ) # make predictions predictions = lr_model . transform ( test_data ) predictions . select ( "" features "" , "" label "" , "" prediction "" , "" probability "" ). show ( 5 ) # evaluate the model evaluator = binaryclassificationevaluator ( rawpredictioncol = "" rawprediction "" , labelcol = "" label "" , metricname = "" areaunderroc "" ) auc = evaluator . evaluate ( predictions ) print ( f "" area under roc: { auc } "" ) enter fullscreen mode exit fullscreen mode next steps 🤔 we have reached the end of this article. by following the steps above, you have built your machine-learning model using pyspark. always ensure that your dataset is clean and free of null values before proceeding to the next steps. lastly, make sure your features all contain numerical values before going ahead to train your model. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand shittu olumide shittu olumide shittu olumide follow skilled software engineer and technical writer. location lagos, nigeria education olusegun agagu university of science and technology. pronouns he/him work founder velcast (bit.ly/velcast) joined apr 27, 2020 • apr 18 dropdown menu copy link hide follow for more content like this :) i also have a youtube channel where i post video tutorials; feel free to check it out here: youtube.com/channel/ucnhfxpk6hgt5u... thanks like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse shittu olumide follow skilled software engineer and technical writer. location lagos, nigeria education olusegun agagu university of science and technology. pronouns he/him work founder velcast (bit.ly/velcast) joined apr 27, 2020 more from shittu olumide functional programming in python: leveraging lambda functions and higher-order functions # programming # python # learning # beginners overview of statsmodels # programming # beginners # python # learning build a rag-powered research paper assistant # programming # ai # tutorial # python 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/jader_lima_b72a63be5bbddc/creating-a-data-pipeline-using-dataproc-workflow-templates-and-cloud-schedule-267d,,,"creating a data pipeline using dataproc workflow templates and cloud schedule - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse jader lima posted on aug 21, 2024 creating a data pipeline using dataproc workflow templates and cloud schedule # pyspark # gcp # dataproc # pipelines about this post data pipelines are processes of acquiring, transforming and enriching data, orchestrated and scheduled, which process information from different sources and with countless possible destinations and applications. there are several systems that help in creating data pipelines, in this post we will cover creating data pipelines on google cloud platform, using the dataproc workflow template and creating a schedule with cloud schedule description of services used in gcp apache spark apache spark is an open-source unified analytics engine for large-scale data processing. it is known for its speed, ease of use, and sophisticated analytics capabilities. spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance, making it suitable for a wide variety of big data applications. google dataproc google dataproc is a fully managed cloud service that simplifies running apache spark and apache hadoop clusters in the google cloud environment. it allows users to easily process large datasets and integrates seamlessly with other google cloud services such as cloud storage. dataproc is designed to make big data processing fast and efficient while minimizing operational overhead. cloud storage google cloud storage is a scalable and secure object storage service for storing large amounts of unstructured data. it offers high availability and strong global consistency, making it suitable for a wide range of scenarios, such as data backups, big data analytics, and content distribution. workflow templates workflow templates in google cloud allow you to define and manage complex workflows that automate interactions between different cloud services. this feature simplifies the process of building, scheduling, and executing intricate workflows, ensuring better management of resources and tasks. cloud scheduler google cloud scheduler is a fully managed cron job service that allows you to run arbitrary functions at specified times without needing to manage the infrastructure. it is useful for automating tasks such as running reports, triggering workflows, and executing other scheduled jobs. ci/cd process with github actions incorporating a ci/cd pipeline using github actions involves automating the build, test, and deployment processes of your applications. for this project, github actions simplifies the deployment of code and resources to google cloud. this automation leverages github's infrastructure to trigger workflows based on events such as code pushes, ensuring that your applications are built and deployed consistently and accurately each time code changes are made. github secrets and configuration utilizing secrets in github actions is vital for maintaining security during the deployment process. secrets allow you to store sensitive information such as api keys, passwords, and service account credentials securely. by keeping this sensitive data out of your source code, you minimize the risk of leaks and unauthorized access. gcp_bucket_bigdata_files secret used to store the name of the cloud storage gcp_bucket_datalake secret used to store the name of the cloud storage gcp_bucket_dataproc secret used to store the name of the cloud storage gcp_service_account gcp_sa_key secret used to store the value of the service account key. for this project, the default service key was used. project_id secret used to store the project id value creating a gcp service account key to create computing resources in any cloud, in an automated or programmatic way, it is necessary to have an access key. in the case of gcp, we use an access key linked to a service account, for the project the default account was used. in gcp console, access : iam &admin service accounts select default service account, default name is something like compute engine default service account in selected service account, menu keys , add key , create new key , key type json download the key file, use the content as key value in your secret in github for more details, access: https://cloud.google.com/iam/docs/keys-create-delete creating github secret to create a new secret: in project repository, menu settings security , secrets and variables ,click in access action new repository secret , type a name and value for secret. for more details , access : https://docs.github.com/pt/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions architecture diagram deploying the project every time a push to the main branch happens, github actions will be triggered, running the yml script. the yml script contains 3 jobs which are explained in more detail below, but basically github actions uses the credentials of the service account with rights to create computing resources, if you authenticate to gcp, perform the steps described in the yml file on : push : branchs : [ main ] enter fullscreen mode exit fullscreen mode workflow file yaml explanation environments needed here's a brief explanation of the environment variables needed in your workflows based on the yaml file you provided: env : bronze_datalake_files : bronze transient_datalake_files : transient bucket_datalake_folder : transient bucket_bigdata_jar_folder : jars bucket_bigdata_pyspark_folder : scripts pyspark_ingestion_script : ingestion_csv_to_delta.py region : us-east1 zone : us-east1-b dataproc_cluster_name : dataproc-bigdata-multi-node-cluster dataproc_worker_type : n2-standard-2 dataproc_master_type : n2-standard-2 dataproc_num_workers : 2 dataproc_image_version : 2.1-debian11 dataproc_worker_num_local_ssd : 1 dataproc_master_num_local_ssd : 1 dataproc_master_boot_disk_size : 32 dataproc_worker_disk_size : 32 dataproc_master_boot_disk_type : pd-balanced dataproc_worker_boot_disk_type : pd-balanced dataproc_components : jupyter dataproc_workflow_name : departments_etl dataproc_workflow_ingestion_step_name : ingestion_countries_csv_to_delta jar_lib1 : delta-core_2.12-2.3.0.jar jar_lib2 : delta-storage-2.3.0.jar app_name : ' countries_ingestion_csv_to_delta' subject : departments step1 : countries step2 : departments step3 : employees step4 : jobs time_zone : america/sao_paulo schedule : "" 20 12 * * *"" schedule_name : schedule_departments_etl service_account_name : dataproc-account-workflow custom_role : workflowcustomrole step1_name : step_countries step2_name : step_departments step3_name : step_employees step4_name : step_jobs enter fullscreen mode exit fullscreen mode deploy buckets job this job is responsible for creating three google cloud storage buckets: one for transient files, one for jar files, and one for pyspark scripts. it checks if each bucket already exists before attempting to create them. additionally, it uploads specified files into these buckets to prepare for later processing. jobs : deploy-buckets : runs-on : ubuntu-22.04 timeout-minutes : 10 steps : - name : checkout uses : actions/checkout@v4 - name : authorize gcp uses : ' google-github-actions/auth@v2' with : credentials_json : ${{ secrets.gcp_sa_key }} # step to create gcp bucket - name : create google cloud storage - files run : |- if ! gsutil ls -p ${{ secrets.project_id }} gs://${{ secrets.gcp_bucket_bigdata_files }} &> /dev/null; \ then \ gcloud storage buckets create gs://${{ secrets.gcp_bucket_bigdata_files }} --default-storage-class=nearline --location=${{ env.region }} else echo ""cloud storage : gs://${{ secrets.gcp_bucket_bigdata_files }}  already exists"" ! fi # step to create gcp bucket - name : create google cloud storage - dataproc run : |- if ! gsutil ls -p ${{ secrets.project_id }} gs://${{ secrets.gcp_bucket_dataproc }} &> /dev/null; \ then \ gcloud storage buckets create gs://${{ secrets.gcp_bucket_dataproc }} --default-storage-class=nearline --location=${{ env.region }} else echo ""cloud storage : gs://${{ secrets.gcp_bucket_dataproc }}  already exists"" ! fi # step to create gcp bucket - name : create google cloud storage - datalake run : |- if ! gsutil ls -p ${{ secrets.project_id }} gs://${{ secrets.gcp_bucket_datalake }} &> /dev/null; \ then \ gcloud storage buckets create gs://${{ secrets.gcp_bucket_datalake }} --default-storage-class=nearline --location=${{ env.region }} else echo ""cloud storage : gs://${{ secrets.gcp_bucket_datalake }}  already exists"" ! fi # step to upload the file to gcp bucket - transient files - name : upload transient files to google cloud storage run : |- target=${{ env.transient_datalake_files }} bucket_path=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }} gsutil cp -r $target gs://${bucket_path} # step to upload the file to gcp bucket - jar files - name : upload jar files to google cloud storage run : |- target=${{ env.bucket_bigdata_jar_folder }} bucket_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }} gsutil cp -r $target gs://${bucket_path} # step to upload the file to gcp bucket - pyspark files - name : upload pyspark files to google cloud storage run : |- target=${{ env.bucket_bigdata_pyspark_folder }} bucket_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }} gsutil cp -r $target gs://${bucket_path} # step to create dataproc cluster - name : upload pyspark files to google cloud storage run : |- target=${{ env.bucket_bigdata_pyspark_folder }} bucket_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }} gsutil cp -r $target gs://${bucket_path} enter fullscreen mode exit fullscreen mode explanation: this job begins by checking out the code and authorizing google cloud credentials. it then checks for the existence of three specified cloud storage buckets—one for transient files, one for jar files, and one for pyspark scripts. if these buckets do not exist, it creates them with gcloud. finally, it uploads the relevant files to the corresponding buckets using gsutil. deploy dataproc workflow template job this job deploys a dataproc workflow template in google cloud. it begins by checking if the workflow template already exists; if not, it creates one. it also sets up a managed dataproc cluster with specific configurations such as the machine types and number of workers. subsequently, it adds various steps (jobs) to the workflow template to outline the processing tasks for data ingestion. code snippet: deploy-dataproc-workflow-template : needs : [ deploy-buckets ] runs-on : ubuntu-22.04 timeout-minutes : 10 steps : - name : checkout uses : actions/checkout@v4 - name : authorize gcp uses : ' google-github-actions/auth@v2' with : credentials_json : ${{ secrets.gcp_sa_key }} - name : create dataproc workflow run : |- if ! gcloud dataproc workflow-templates list --region=${{ env.region}} | grep -i ${{ env.dataproc_workflow_name}} &> /dev/null; \ then \ gcloud dataproc workflow-templates create ${{ env.dataproc_workflow_name }} --region ${{ env.region }} else echo ""workflow template : ${{ env.dataproc_workflow_name }} already exists"" ! fi - name : create dataproc managed cluster run : > gcloud dataproc workflow-templates set-managed-cluster ${{ env.dataproc_workflow_name }} --region ${{ env.region }} --zone ${{ env.zone }} --image-version ${{ env.dataproc_image_version }} --master-machine-type=${{ env.dataproc_master_type }} --master-boot-disk-type ${{ env.dataproc_master_boot_disk_type }} --master-boot-disk-size ${{ env.dataproc_master_boot_disk_size }} --worker-machine-type=${{ env.dataproc_worker_type }} --worker-boot-disk-type ${{ env.dataproc_worker_boot_disk_type }} --worker-boot-disk-size ${{ env.dataproc_worker_disk_size }} --num-workers=${{ env.dataproc_num_workers }} --cluster-name=${{ env.dataproc_cluster_name }} --optional-components ${{ env.dataproc_components }} --service-account=${{ env.gcp_service_account }} - name : add job ingestion countries to workflow run : |- if gcloud dataproc workflow-templates list --region=${{ env.region}} | grep -i ${{ env.dataproc_workflow_name}} &> /dev/null; \ then \ if gcloud dataproc workflow-templates describe ${{ env.dataproc_workflow_name}} --region=${{ env.region}} | grep -i ${{ env.step1_name  }} &> /dev/null; \ then \ echo ""workflow template : ${{ env.dataproc_workflow_name }} already has step : ${{ env.step1_name  }} "" ! else pyspark_script_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }}/${{ env.pyspark_ingestion_script }} jars_path=gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib1 }} jars_path=${jars_path},gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib2 }} transient=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }}/${{ env.subject }}/${{ env.step1 }} bronze=${{ secrets.gcp_bucket_datalake }}/${{ env.bronze_datalake_files }}/${{ env.subject }}/${{ env.step1 }} gcloud dataproc workflow-templates add-job pyspark gs://${pyspark_script_path} \ --workflow-template ${{ env.dataproc_workflow_name }}  \ --step-id ${{env.step1_name }} \ --region ${{ env.region }} \ --jars ${jars_path} \ -- --app_name=${{ env.app_name }}${{ env.step1 }} --bucket_transient=gs://${transient} \ --bucket_bronze=gs://${bronze} fi else echo ""workflow template : ${{ env.dataproc_workflow_name}} not exists"" ! fi - name : add job ingestion departments to workflow run : |- if gcloud dataproc workflow-templates list --region=${{ env.region}} | grep -i ${{ env.dataproc_workflow_name}} &> /dev/null; \ then \ if gcloud dataproc workflow-templates describe ${{ env.dataproc_workflow_name}} --region=${{ env.region}} | grep -i ${{ env.step2_name }} &> /dev/null; \ then \ echo ""workflow template : ${{ env.dataproc_workflow_name }} already has step : ${{ env.step2_name  }} "" ! else pyspark_script_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }}/${{ env.pyspark_ingestion_script }} jars_path=gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib1 }} jars_path=${jars_path},gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib2 }} transient=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }}/${{ env.subject }}/${{ env.step2 }} bronze=${{ secrets.gcp_bucket_datalake }}/${{ env.bronze_datalake_files }}/${{ env.subject }}/${{ env.step2 }} gcloud dataproc workflow-templates add-job pyspark gs://${pyspark_script_path} \ --workflow-template ${{ env.dataproc_workflow_name }}  \ --step-id ${{ env.step2_name }} \ --start-after ${{ env.step1_name }} \ --region ${{ env.region }} \ --jars ${jars_path} \ -- --app_name=${{ env.app_name }}${{ env.step2 }} --bucket_transient=gs://${transient} \ --bucket_bronze=gs://${bronze} fi else echo ""workflow template : ${{ env.dataproc_workflow_name}} not exists"" ! fi - name : add job ingestion employees to workflow run : |- if gcloud dataproc workflow-templates list --region=${{ env.region}} | grep -i ${{ env.dataproc_workflow_name}} &> /dev/null; \ then \ if gcloud dataproc workflow-templates describe ${{ env.dataproc_workflow_name}} --region=${{ env.region}} | grep -i ${{ env.step3_name }} &> /dev/null; \ then \ echo ""workflow template : ${{ env.dataproc_workflow_name }} already has step : ${{ env.step3_name }} "" ! else pyspark_script_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }}/${{ env.pyspark_ingestion_script }} jars_path=gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib1 }} jars_path=${jars_path},gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib2 }} transient=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }}/${{ env.subject }}/${{ env.step2 }} pyspark_script_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }}/${{ env.pyspark_ingestion_script }} jars_path=gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib1 }} jars_path=${jars_path},gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib2 }} transient=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }}/${{ env.subject }}/${{ env.step3 }} bronze=${{ secrets.gcp_bucket_datalake }}/${{ env.bronze_datalake_files }}/${{ env.subject }}/${{ env.step3 }} gcloud dataproc workflow-templates add-job pyspark gs://${pyspark_script_path} \ --workflow-template ${{ env.dataproc_workflow_name }}  \ --step-id ${{ env.step3_name }} \ --start-after ${{ env.step1_name }},${{ env.step2_name }} \ --region ${{ env.region }} \ --jars ${jars_path} \ -- --app_name=${{ env.app_name }}${{ env.step3 }} --bucket_transient=gs://${transient} \ --bucket_bronze=gs://${bronze} fi else echo ""workflow template : ${{ env.dataproc_workflow_name}} not exists"" ! fi - name : add job ingestion jobs to workflow run : |- if gcloud dataproc workflow-templates list --region=${{ env.region}} | grep -i ${{ env.dataproc_workflow_name}} &> /dev/null; \ then \ if gcloud dataproc workflow-templates describe ${{ env.dataproc_workflow_name}} --region=${{ env.region}} | grep -i ${{ env.step4_name }} &> /dev/null; \ then \ echo ""workflow template : ${{ env.dataproc_workflow_name }} already has step : ${{ env.step4_name }} "" ! else pyspark_script_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }}/${{ env.pyspark_ingestion_script }} jars_path=gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib1 }} jars_path=${jars_path},gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib2 }} transient=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }}/${{ env.subject }}/${{ env.step2 }} pyspark_script_path=${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_pyspark_folder }}/${{ env.pyspark_ingestion_script }} jars_path=gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib1 }} jars_path=${jars_path},gs://${{ secrets.gcp_bucket_bigdata_files }}/${{ env.bucket_bigdata_jar_folder }}/${{ env.jar_lib2 }} transient=${{ secrets.gcp_bucket_datalake }}/${{ env.bucket_datalake_folder }}/${{ env.subject }}/${{ env.step4 }} bronze=${{ secrets.gcp_bucket_datalake }}/${{ env.bronze_datalake_files }}/${{ env.subject }}/${{ env.step4 }} gcloud dataproc workflow-templates add-job pyspark gs://${pyspark_script_path} \ --workflow-template ${{ env.dataproc_workflow_name }}  \ --step-id ${{ env.step4_name }} \ --start-after ${{ env.step1_name }},${{ env.step2_name }},${{ env.step3_name }} \ --region ${{ env.region }} \ --jars ${jars_path} \ -- --app_name=${{ env.app_name }}${{ env.step4 }} --bucket_transient=gs://${transient} \ --bucket_bronze=gs://${bronze} fi else echo ""workflow template : ${{ env.dataproc_workflow_name}} not exists"" ! fi enter fullscreen mode exit fullscreen mode explanation this job follows a systematic approach to deploying a dataproc workflow template. it first checks if the workflow template exists and creates it if it does not. next, a managed dataproc cluster is configured with specified properties (e.g., number of workers, machine type). the job also adds specified steps for data ingestion tasks to the workflow template, detailing how data should be processed. the remaining steps for add job are structured similarly, each focusing on different data ingestion tasks within the workflow. deploy cloud schedule job this job sets up a scheduling mechanism using google cloud scheduler. it creates a service account specifically for the scheduled job, defines a custom role with specific permissions, and binds the custom role to the service account. finally, it creates the cloud schedule to trigger the execution of the workflow at defined intervals. deploy-cloud-schedule : needs : [ deploy-buckets , deploy-dataproc-workflow-template ] runs-on : ubuntu-22.04 timeout-minutes : 10 steps : - name : checkout uses : actions/checkout@v4 - name : authorize gcp uses : ' google-github-actions/auth@v2' with : credentials_json : ${{ secrets.gcp_sa_key }} # step to authenticate with gcp - name : set up cloud sdk uses : google-github-actions/setup-gcloud@v2 with : version : ' >= 363.0.0' project_id : ${{ secrets.project_id }} # step to configure docker to use the gcloud command-line tool as a credential helper - name : configure docker run : |- gcloud auth configure-docker - name : create service account run : |- if ! gcloud iam service-accounts list | grep -i ${{ env.service_account_name}} &> /dev/null; \ then \ gcloud iam service-accounts create ${{ env.service_account_name }} \ --display-name=""scheduler dataproc workflow service account"" fi - name : create custom role for service account run : |- if ! gcloud iam roles list --project ${{ secrets.project_id }} | grep -i ${{ env.custom_role }} &> /dev/null; \ then \ gcloud iam roles create ${{ env.custom_role }} --project ${{ secrets.project_id }} \ --title ""dataproc workflow template scheduler"" --description ""dataproc workflow template scheduler"" \ --permissions ""dataproc.workflowtemplates.instantiate,iam.serviceaccounts.actas"" --stage alpha fi - name : add the custom role for service account run : |- gcloud projects add-iam-policy-binding ${{secrets.project_id}} \ --member=serviceaccount:${{env.service_account_name}}@${{secrets.project_id}}.iam.gserviceaccount.com \ --role=projects/${{secrets.project_id}}/roles/${{env.custom_role}} - name : create cloud schedule for workflow execution run : |- if ! gcloud scheduler jobs list --location ${{env.region}} | grep -i ${{env.schedule_name}} &> /dev/null; \ then \ gcloud scheduler jobs create http ${{env.schedule_name}} \ --schedule=""30 12 * * *"" \ --description=""dataproc workflow "" \ --location=${{env.region}} \ --uri=https://dataproc.googleapis.com/v1/projects/${{secrets.project_id}}/regions/${{env.region}}/workflowtemplates/${{env.dataproc_workflow_name}}:instantiate?alt=json \ --time-zone=${{env.time_zone}} \ --oauth-service-account-email=${{env.service_account_name}}@${{secrets.project_id}}.iam.gserviceaccount.com fi enter fullscreen mode exit fullscreen mode explanation in this job, a service account is created specifically for handling the scheduled workflow execution. it also defines a custom role that grants the necessary permissions for the service account to instantiate the workflow template. this custom role is then associated with the service account to ensure it has the required permissions. finally, the job creates a cloud schedule that triggers the workflow execution at predetermined times, ensuring automated execution of the data processing workflow. resources created after deploy process dataproc workflow template after deploying the project, you can access the dataproc service to view the workflow template. in the workflow tab, you can explore various options, including monitoring workflow executions and analyzing their details. when you select the created workflow, you can see the cluster used for processing and the steps that comprise the workflow, including any dependencies between the steps. this visibility allows you to track the workflow's operational flow. additionally, within the dataproc service, you can monitor the execution status of each job. it provides details about each execution, including the performance of individual steps within the workflow template, as illustrated below. cloud scheduler by accessing the cloud scheduler service, you'll find the scheduled job created during the deployment process. the interface displays the last run status, the defined schedule for execution, and additional details about the target url and other parameters. cloud storage as part of the deployment process, several cloud storage buckets are created: one bucket for storing data related to the data lake, another for the dataproc cluster, and a third for the pyspark scripts and libraries used in the project. the dataproc service itself creates a cluster to manage temporary data generated during processing. after the data processing is complete, a new directory is established in the designated cloud storage bucket to save the ingested data from the data lake. the transient directory, created during the deployment phase, serves as the location where data was copied from the github repository to cloud storage. in a production environment, another application would likely handle the ingestion of data into this transient layer. conclusion data pipelines are crucial components in the landscape of data processing. while there are robust and feature-rich tools available, such as azure data factory and apache airflow, simpler solutions can be valuable in certain scenarios. the decision on the most appropriate tool ultimately rests with the data and architecture teams, who must assess the specific needs and context to select the best solution for the moment. links and references github repo dataproc workflow documentation top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse jader lima follow with over 10 years of experience in data engineering, business intelligence, and development. i have a solid track record in developing batch and streaming data pipelines using azure technologies such joined aug 3, 2024 more from jader lima using google cloud functions for three-tier data processing with google composer and automated deployments via github actions # gcp # python # airflow # serverless using cloud functions and cloud schedule to process data with google dataflow # gcp # dataflow # cloudfunctions # cloudscheduler loading data to google big query using dataproc workflow templates and cloud schedule # gcp # dataproc # bigquery # bigdata 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/alexmercedcoder/exploring-data-operations-with-pyspark-pandas-duckdb-polars-and-datafusion-in-a-python-notebook-52ck,,,"exploring data operations with pyspark, pandas, duckdb, polars, and datafusion in a python notebook - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse alex merced posted on oct 7, 2024 exploring data operations with pyspark, pandas, duckdb, polars, and datafusion in a python notebook # python # database # datascience # dataengineering apache iceberg crash course: what is a data lakehouse and a table format? free copy of apache iceberg the definitive guide free apache iceberg crash course iceberg lakehouse engineering video playlist data engineers and scientists often work with a variety of tools to handle different types of data operations—from large-scale distributed processing to in-memory data manipulation. the alexmerced/spark35nb docker image simplifies this by offering a pre-configured environment where you can experiment with multiple popular data tools, including pyspark, pandas, duckdb, polars, and datafusion. in this blog, we'll guide you through setting up this environment and demonstrate how to perform basic data operations such as writing data, loading data, and executing queries and aggregations using these tools. whether you’re dealing with large datasets or just need to manipulate small, in-memory data, you'll see how these different libraries can complement each other. section 1: setting up your environment 1.1 pull the docker image to get started, you'll first need to pull the alexmerced/spark35nb docker image from docker hub. this image comes with a pre-configured environment that includes spark 3.5.2, jupyterlab, and many popular data manipulation libraries like pandas, duckdb, and polars. run the following command to pull the image: docker pull alexmerced/spark35nb enter fullscreen mode exit fullscreen mode next, run the container using the following command: docker run -p 8888:8888 -p 4040:4040 -p 7077:7077 -p 8080:8080 -p 18080:18080 -p 6066:6066 -p 7078:7078 -p 8081:8081  alexmerced/spark35nb enter fullscreen mode exit fullscreen mode once the container is up and running, open your browser and navigate to localhost:8888 to access jupyterlab, where you will perform all your data operations. now that you have your environment set up, we can move on to performing some basic data operations using pyspark, pandas, duckdb, polars, and datafusion. section 2: working with pyspark 2.1 what is pyspark? pyspark is the python api for apache spark, an open-source engine designed for large-scale data processing and distributed computing. it allows you to work with big data by distributing data and computations across a cluster. while spark is usually run in a distributed cluster, this setup allows you to run it locally on a single node—perfect for development and testing. using pyspark, you can perform data manipulation, sql queries, machine learning, and more, all within a framework that handles big data efficiently. in this section, we'll walk through how to write and query data using pyspark in the jupyterlab environment. 2.2 writing data with pyspark let’s start by creating a simple dataset in pyspark. first, initialize a spark session, which is necessary to interact with spark's functionality. we will create a small dataframe with sample data and display it. from pyspark.sql import sparksession # initialize the spark session spark = sparksession . builder . appname ( "" pyspark example "" ). getorcreate () # sample data: a list of tuples containing names and ages data = [( "" alice "" , 34 ), ( "" bob "" , 45 ), ( "" catherine "" , 29 )] # create a dataframe df = spark . createdataframe ( data , [ "" name "" , "" age "" ]) # show the dataframe df . show () enter fullscreen mode exit fullscreen mode in this example, we created a dataframe with three rows of data, representing people's names and ages. the df.show() function allows us to display the contents of the dataframe, making it easy to inspect the data we just created. 2.3 loading and querying data with pyspark next, let’s load a dataset from a file and run some basic queries. pyspark can handle various file formats, including csv, json, and parquet. for this example, let’s assume we have a csv file with more data about people, which we’ll load into a dataframe. then we’ll demonstrate a simple filter query and aggregation to count the number of people in each age group. # load a csv file into a dataframe df_csv = spark . read . csv ( "" data/people.csv "" , header = true , inferschema = true ) # show the first few rows of the dataframe df_csv . show () # filter the data to only include people older than 30 df_filtered = df_csv . filter ( df_csv [ "" age "" ] > 30 ) # show the filtered dataframe df_filtered . show () # group by age and count the number of people in each age group df_grouped = df_csv . groupby ( "" age "" ). count () # show the result of the grouping df_grouped . show () enter fullscreen mode exit fullscreen mode in this example, we loaded a csv file into a pyspark dataframe using spark.read.csv() . then, we applied two different operations: filtering : we filtered the dataframe to show only rows where the age is greater than 30. aggregation : we grouped the data by age and counted how many people are in each age group. with pyspark, you can perform more complex queries and aggregations on large datasets, making it a tool for big data processing. in the next section, we'll explore pandas, which is great for smaller, in-memory data operations that don't require distributed processing. section 3: data manipulation with pandas 3.1 what is pandas? pandas is one of the most widely used python libraries for data manipulation and analysis. it provides easy-to-use data structures, like dataframes, which allow you to work with tabular data in an intuitive way. unlike pyspark, which is designed for large-scale distributed data processing, pandas works in-memory, making it ideal for small to medium-sized datasets. with pandas, you can read and write data from various formats, including csv, excel, and json, and perform common data operations like filtering, aggregating, and merging data with simple and readable syntax. 3.2 loading data with pandas let’s start by loading a dataset into a pandas dataframe. we’ll read a csv file, which is a common file format for data storage, and display the first few rows. import pandas as pd # load a csv file into a pandas dataframe df_pandas = pd . read_csv ( "" data/people.csv "" ) # display the first few rows of the dataframe print ( df_pandas . head ()) enter fullscreen mode exit fullscreen mode in this example, we read the csv file people.csv using pd.read_csv() and loaded it into a pandas dataframe. the head() method lets you view the first few rows of the dataframe, which is useful for quickly inspecting the data. 3.3 basic operations with pandas now that we have loaded the data, let’s perform some basic operations, such as filtering rows and grouping data. pandas allows you to apply these operations easily with simple python syntax. # filter the data to show only people older than 30 df_filtered = df_pandas [ df_pandas [ "" age "" ] > 30 ] # display the filtered data print ( df_filtered ) # group the data by 'age' and count the number of people in each age group df_grouped = df_pandas . groupby ( "" age "" ). count () # display the grouped data print ( df_grouped ) enter fullscreen mode exit fullscreen mode here, we filtered the data to include only people older than 30 using a simple boolean expression. then, we used the groupby() function to group the dataframe by age and count the number of people in each age group. pandas is incredibly efficient for in-memory data operations, making it a go-to tool for smaller datasets that can fit in your machine's memory. in the next section, we’ll explore duckdb, a sql-based tool that enables fast querying over in-memory data. section 4: exploring duckdb 4.1 what is duckdb? duckdb is an in-memory sql database management system (dbms) designed for analytical workloads. it offers high-performance, efficient querying of datasets directly within your python environment. duckdb is particularly well-suited for performing complex sql queries on structured data, like csvs or parquet files, without needing to set up a separate database server. duckdb is lightweight, yet powerful, and can be used as an alternative to tools like sqlite, especially when working with analytical queries on large datasets. 4.2 writing data into duckdb duckdb can easily integrate with pandas, allowing you to transfer data from a pandas dataframe into duckdb for sql-based queries. here’s how to create a table in duckdb using the data from pandas. import duckdb # connect to an in-memory duckdb instance conn = duckdb . connect () # create a table in duckdb from the pandas dataframe conn . execute ( "" create table people as select * from df_pandas "" ) # show the content of the 'people' table conn . execute ( "" select * from people "" ). df () enter fullscreen mode exit fullscreen mode in this example, we connected to duckdb and created a new table people from the pandas dataframe df_pandas. duckdb’s execute() function allows you to run sql commands, making it easy to interact with data using sql queries. 4.3 querying data in duckdb once your data is loaded into duckdb, you can run sql queries to filter, aggregate, and analyze your data. duckdb supports a wide range of sql functionality, making it ideal for users who prefer sql over python for data manipulation. # query to select people older than 30 result = conn . execute ( "" select name, age from people where age > 30 "" ). df () # display the result of the query print ( result ) # query to group people by age and count the number of people in each age group result_grouped = conn . execute ( "" select age, count(*) as count from people group by age "" ). df () # display the grouped result print ( result_grouped ) enter fullscreen mode exit fullscreen mode in this example, we used sql to filter the people table, selecting only those who are older than 30. we then ran a grouping query to count the number of people in each age group. duckdb is an excellent choice when you need sql-like functionality directly in your python environment. it allows you to leverage the power of sql without the overhead of setting up and managing a database server. in the next section, we will explore polars, a dataframe library known for its speed and efficiency. section 5: leveraging polars for fast dataframe operations 5.1 what is polars? polars is a dataframe library designed for high-performance data manipulation. it’s known for its speed and efficiency, particularly when compared to libraries like pandas. polars is written in rust and uses an optimized query engine to handle large datasets quickly and with minimal memory usage. it also provides a similar interface to pandas, making it easy to learn and integrate into existing python workflows. polars is particularly well-suited for processing large datasets that might not fit into memory as easily or for scenarios where performance is a critical factor. 5.2 working with polars let’s start by creating a polars dataframe from a python dictionary. we’ll then perform some basic operations like filtering and aggregating data. import polars as pl # create a polars dataframe df_polars = pl . dataframe ({ "" name "" : [ "" alice "" , "" bob "" , "" catherine "" ], "" age "" : [ 34 , 45 , 29 ] }) # display the polars dataframe print ( df_polars ) enter fullscreen mode exit fullscreen mode in this example, we created a polars dataframe using a python dictionary. the syntax is similar to pandas, but the operations are optimized for speed. polars offers lazy evaluation, which means it can optimize the execution of multiple operations at once, reducing computation time. 5.3 filtering and aggregating with polars now, let’s perform some common data operations such as filtering and aggregating the data. these operations are highly optimized in polars and can be done using a simple and expressive syntax. # filter the dataframe to show only people older than 30 df_filtered = df_polars . filter ( pl . col ( "" age "" ) > 30 ) # display the filtered dataframe print ( df_filtered ) # group by 'age' and count the number of people in each age group df_grouped = df_polars . groupby ( "" age "" ). count () # display the grouped result print ( df_grouped ) enter fullscreen mode exit fullscreen mode in this example, we filtered the data to show only rows where the age is greater than 30, and then we grouped the data by age to count how many people are in each group. these operations are highly efficient in polars due to its optimized memory management and query execution engine. polars is ideal when you need the speed of a dataframe library for both small and large datasets, and when performance is a key requirement. next, we will explore datafusion, a tool for sql-based querying over apache arrow data. section 6: datafusion for query execution 6.1 what is datafusion? datafusion is an in-memory query execution engine built on top of apache arrow, an efficient columnar memory format for analytics. it provides a powerful sql engine that allows users to run complex queries over structured data stored in arrow format. datafusion is part of the apache arrow ecosystem, which aims to provide fast data interoperability across different data processing tools. datafusion is particularly well-suited for scenarios where you need to query large in-memory datasets using sql without the overhead of traditional databases. its integration with arrow ensures that the data processing is both fast and memory-efficient. 6.2 writing and querying data with datafusion datafusion allows you to execute sql queries on in-memory data using apache arrow. let’s first create a dataframe using datafusion and then perform a few sql queries on it. from datafusion import sessioncontext # initialize a datafusion session ctx = sessioncontext () # create a dataframe with some data data = [ { "" name "" : "" alice "" , "" age "" : 34 }, { "" name "" : "" bob "" , "" age "" : 45 }, { "" name "" : "" catherine "" , "" age "" : 29 } ] # register the dataframe as a table df = ctx . create_dataframe ( data ) ctx . register_table ( "" people "" , df ) # query the data to select people older than 30 result = ctx . sql ( "" select name, age from people where age > 30 "" ). collect () # display the result print ( result ) enter fullscreen mode exit fullscreen mode in this example, we used datafusion’s sessioncontext to create a dataframe and registered it as a table. we then performed a simple sql query to filter the data for people older than 30. datafusion allows you to combine the power of sql with the speed and efficiency of apache arrow’s in-memory format. 6.3 aggregating data with datafusion just like in duckdb, we can perform aggregation queries to group data by a specific field and count the number of records in each group. let’s see how this works in datafusion. # group by 'age' and count the number of people in each age group result_grouped = ctx . sql ( "" select age, count(*) as count from people group by age "" ). collect () # display the grouped result print ( result_grouped ) enter fullscreen mode exit fullscreen mode in this query, we grouped the data by the 'age' column and counted how many people were in each age group. datafusion’s sql execution engine ensures that queries run efficiently, even on large datasets stored in-memory. datafusion is a great tool for users who need fast, sql-based querying of large in-memory datasets and want to take advantage of apache arrow’s high-performance columnar data format. it’s particularly useful for building analytical pipelines that involve heavy querying of structured data. bonus section: integrating dremio with python what is dremio? dremio is a powerful data lakehouse platform that helps organizations unify and query their data from various sources. it enables users to easily govern, join, and accelerate queries on their data without the need for expensive and complex data warehouse infrastructures. dremio's ability to access and query data directly from formats like apache iceberg, delta lake, s3, rdbms, and json files, along with its performance enhancements, reduces the workload on traditional data warehouses. dremio is built on top of apache arrow, a high-performance columnar in-memory format, and utilizes arrow flight to accelerate the transmission of large datasets over the network. this integration provides blazing-fast query performance while enabling interoperability between various analytics tools. in this section, we will demonstrate how to set up dremio in a docker container and use python to query dremio's data sources using the dremio-simple-query library. 6.1 setting up dremio with docker to run dremio on your local machine, use the following docker command: docker run -p 9047:9047 -p 31010:31010 -p 45678:45678 -p 32010:32010 -e dremio_java_server_extra_opts = -dpaths .dist = file:///opt/dremio/data/dist --name try-dremio dremio/dremio-oss enter fullscreen mode exit fullscreen mode once dremio is up and running, navigate to http://localhost:9047 in your browser to access the dremio ui. here, you can configure your data sources, create virtual datasets, and explore the platform's capabilities. 6.2 querying dremio with python using dremio-simple-query the dremio-simple-query library allows you to query dremio using apache arrow flight, providing a high-performance interface for fetching and analyzing data from dremio sources. with this library, you can easily convert dremio queries into pandas, polars, or duckdb dataframes, or work directly with apache arrow data. here’s how to get started: step 1: install the necessary libraries make sure you have the dremio-simple-query library installed (it is pre-installed on the alexmerced/spark35nb image). you can install it using pip: pip install dremio-simple-query enter fullscreen mode exit fullscreen mode step 2: set up your connection to dremio you’ll need your dremio credentials to retrieve a token and establish a connection. here’s a basic example: from dremio_simple_query.connect import get_token , dremioconnection from os import getenv from dotenv import load_dotenv # load environment variables (token and arrow_endpoint) load_dotenv () # login to dremio and get a token login_endpoint = "" http://{host}:9047/apiv2/login "" payload = { "" username "" : "" your_username "" , "" password "" : "" your_password "" } token = get_token ( uri = login_endpoint , payload = payload ) # dremio arrow flight endpoint, make sure to put in the right host for your dremio instance arrow_endpoint = "" grpc://{host}:32010 "" # establish connection to dremio using arrow flight dremio = dremioconnection ( token , arrow_endpoint ) enter fullscreen mode exit fullscreen mode if you are running this locally using the docker run command, the host should be the ip address of the dremio container on the docker network which you can find by running docker inspect . in this code, we use the get_token function to retrieve an authentication token from dremio's rest api and establish a connection to dremio's arrow flight endpoint. step 3: query dremio and retrieve data in various formats once connected, you can use the connection to query dremio and retrieve results in different formats, including arrow, pandas, polars, and duckdb. here’s how: querying data and returning as arrow table: # query dremio and return data as an apache arrow table stream = dremio . toarrow ( "" select * from my_table; "" ) arrow_table = stream . read_all () # display arrow table print ( arrow_table ) enter fullscreen mode exit fullscreen mode converting to a pandas dataframe: # query dremio and return data as a pandas dataframe df = dremio . topandas ( "" select * from my_table; "" ) print ( df ) enter fullscreen mode exit fullscreen mode converting to a polars dataframe: # query dremio and return data as a polars dataframe df_polars = dremio . topolars ( "" select * from my_table; "" ) print ( df_polars ) enter fullscreen mode exit fullscreen mode querying with duckdb: # query dremio and return as a duckdb relation duck_rel = dremio . toduckdb ( "" select * from my_table "" ) # perform a query on the duckdb relation result = duck_rel . query ( "" my_table "" , "" select * from my_table where age > 30 "" ). fetchall () # display results print ( result ) enter fullscreen mode exit fullscreen mode with the dremio-simple-query library, you can efficiently query large datasets from dremio and immediately start analyzing them with various tools like pandas, polars, and duckdb, all while leveraging the high-performance apache arrow format under the hood. 6.3 why use dremio? dremio provides several benefits that make it a powerful addition to your data stack: governance: centralize governance over all your data sources, ensuring compliance and control. data federation: join data across various sources, such as iceberg, delta lake, json, csv, and relational databases, without moving the data. performance: accelerate your queries with the help of dremio's query acceleration features and apache arrow flight. cost savings: by offloading workloads from traditional data warehouses, dremio can reduce infrastructure costs. dremio's close relationship with apache arrow ensures that your queries are both fast and efficient, allowing you to seamlessly integrate various data sources and tools into your analytics workflows. conclusion in this blog, we explored how to use a variety of powerful tools for data operations within a python notebook environment. starting with the alexmerced/spark35nb docker image, we demonstrated how to set up a development environment that includes pyspark, pandas, duckdb, polars, and datafusion—each optimized for different data processing needs. we showcased basic operations like writing, querying, and aggregating data using each tool’s unique strengths. pyspark enables scalable, distributed processing for large datasets, perfect for big data environments. pandas offers in-memory, easy-to-use data manipulation for smaller datasets, making it the go-to tool for quick data exploration. duckdb provides an efficient, in-memory sql engine, ideal for analytical queries without the need for complex infrastructure. polars brings lightning-fast dataframe operations, combining performance and simplicity for larger or performance-critical datasets. datafusion , with its foundation in apache arrow, allows for high-performance sql querying, particularly for analytical workloads in memory. finally, we introduced dremio , which integrates with apache arrow to enable lightning-fast queries across a range of data sources. with the dremio-simple-query library, dremio allows analysts to quickly fetch and analyze data using tools like pandas, polars, and duckdb, ensuring that data is available when and where it's needed without the overhead of traditional data warehouses. whether you're working with small datasets or handling massive amounts of data in distributed environments, this setup provides a versatile, efficient, and scalable platform for any data engineering or data science project. by leveraging these tools together, you can cover the full spectrum of data processing, from exploration to large-scale analytics, with minimal setup and maximum performance. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse alex merced follow i like to code, teach and promote. subscribe to the web dev 101 and datanation podcasts. blogs and videos at devnursery.com and grokoverflow.com. find everything at alexmerced.com location orlando, fl work head of devrel @ dremio joined dec 8, 2019 more from alex merced introduction to data engineering concepts |18| the power of dremio in the modern lakehouse # data # datascience # dataengineering # database introduction to data engineering concepts |17| apache iceberg, arrow, and polaris # data # datascience # database # dataengineering introduction to data engineering concepts |16| data lakehouse architecture explained # data # datascience # database # dataengineering 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/auyer/how-i-decreased-etl-cost-by-leveraging-the-apache-arrow-ecosystem-1fob,,,"how i decreased etl cost by leveraging the apache arrow ecosystem - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse rafael passos posted on feb 22, 2023 • edited on apr 6, 2023 how i decreased etl cost by leveraging the apache arrow ecosystem # etl # python # data # dataengineering in the field of data engineering, the apache spark framework is one of the most known and powerful ways to extract and process data. it is well-trusted, and it is also very simple to use once you get the infrastructure set up. understandably, most engineers will choose it for every task. however, in a lot of ways, it can be overkill. and a very expensive one. our data platform journey in our stack, we manage several databases for different microservices that our team built. we also have a few legacy databases for two platforms built by external companies. to bring insights and enable data-driven decision-making for our directors, we needed a way to run analytical queries on all of them. enabling it requires we run etls (extract, transform, and load scripts) to get data from the databases into our data lake. at the time of writing this article, i am the professional responsible for most of the data integrations and extractions at my company. and i was also the lead architect for our data platform and infrastructure, and i will explain how it evolved into what we have today. extraction jobs: what an etl looks like etl (extract transform load) diagram the first iteration of our etl system used the aws glue jobs service. it was easy to get started with since it is a serverless offering of apache spark (with some customizations on top). these extractions would take all data from the databases and save all of it as apache parquet files in an s3 bucket, separating databases into folders, and their tables as subfolders. we use aws athena (similar to the open source presto/trino project) to run sql queries on all these files as if it was one big data-warehouse. at the end of the stack, we have metabase as our visualization tool, for making beautiful dashboards. these last two are still part of our stack. we've tested different projects, like dremio and trino for running the queries, and also apache superset for the viz, but ended up sticking with our first choices. what i changed the most is how we run the extraction scripts. the first change was to migrate a few etls to aws emr (managed apache spark clusters). these scripts had become too complex to run on glue, and on emr we can scale the cluster as we wish. the cost of running them was also a good reason to migrate since aws glue can be a lot more expensive than the alternatives. the second step was to relinquish amazon's help in managing these clusters and do it myself on kubernetes. i used spark spark-on-k8s-operator to run our spark jobs. it made them a lot faster (start-up times for emr are painfully slow), cheaper, and easier to manage. with this operator, jobs are submitted as kubernetes custom resources, and the operator creates all necessary pods to run the scripts. up to this point, we've been writing apache spark scripts and only changing how and where we run them. the last step is different. using apache arrow and just simple containers, i made most of the old extractions obsolete. apache spark vs apache arrow (not equivalent) apache spark is made for distributed work. for this, it is usually set up in a cluster with one or a few driver/master nodes, and at least a few executor nodes. this is amazing when you need to work with large datasets, and they can't fit into the memory of one reasonably priced machine. but there is always a downside. even when this is the case, distributing the processing will not always be perfect. it could be necessary to share data between nodes, and this causes a lot of network traffic. and some operations just need data to be in memory. in the other case, when the workload is not that large, distributing it will yield no real gain. it will most likely hurt it due to various types of overhead like synchronization and transport. you can run apache spark without a cluster, but it was not made for that. for this reason, i decided to test some new projects that would do what i needed without being distributed. the tools i used to create our new extractions are polars and connectorx . polars is a dataframe library, like pandas, but implemented in rust with the apache arrow data model. it is a columnar data model, and it was created to be implemented in any language. but that's not the impressive part: you can share data between totally different codebases, like rust, java, and python, without serializing and even reallocating it. if you can access the same space in memory, you can use it with the arrow ecosystem. when i said spark shuffles cause a lot of traffic over the network, it also requires all this data to be serialized before sending and deserialized at the receiving end. this wastes a lot of time and resources. code time connectorx is integrated into polars, and if both are installed, you can call polars.read_sql . i will use it direclty though: import connectorx as cx

arrow_table = cx.read_sql(
    query=""select * from table_x"",
    conn=""postgres://user:pass@host:port/database"",
    return_type=""arrow2"",
    protocol=""binary"",
) enter fullscreen mode exit fullscreen mode my example uses postgres, but connectorx supports several other databases. the output of this function is an apache arrow table. arrow2 means it uses the unofficial rust implementation arrow2 . there is also the official apache arrow implementation in rust, and a c++ implementation used by pyarrow. this next call instructs polars to read the arrow table memory space. and as the docs say: this operation will be zero copy for the most part. types that are not supported by polars may be cast to the closest supported type. import polars as pl
df = pl.from_arrow(arrow_table) enter fullscreen mode exit fullscreen mode once you loaded it into polars, you can manipulate this data at will. there is an option to turn it into a lazy operation like it is done with apache spark. this is very useful because it allows polars to optimize the query plan when possible. this also enables the use of streaming, for larger-than-memory operations. in this sample, i will keep it simple and write it to a file directly. this could be done to a local path, or almost any destination fsspec supports. for example, one valid path could be s3://bucket/database/folder/ . if you do not specify the file name, it will generate a random one. if you want to keep a single file, or want to replace an existing one, make sure to specify the file name. df.write_parquet(""output.snappy.parquet"", compression:""snappy"") enter fullscreen mode exit fullscreen mode it is also possible to use pyarrow to do this. as i said before, pyarrow uses the c++ implementation of arrow. but data can flow between them seamlessly without the need for serialization, or even memory copying. import pyarrow.parquet as pq

pq.write_table(
    df.to_arrow(),
    where=""output.snappy.parquet"",
    compression=""snappy"",
) enter fullscreen mode exit fullscreen mode i created an example repository in github that puts all of this together. check it out! github.com/auyer/polars-extraction conclusion apache spark is an established framework for building complex etls. but it carries a heavy jvm stack behind it. as we discussed here, it is not a good choice for small datasets if you are worried about cost (you should be). the apache arrow ecosystem is growing. it can't replace spark just yet, but one day i bet it will. but when doing what it is able to do now, it does it a lot faster and consumes fewer machine resources. with new features being implemented into polars almost every week, it will soon be the ubiquitous tool for data frames. connectorx is an important piece for this success. it does not have all the features it needs to make polars fully replace spark for me, as it does not support all postgres types. i implemented support for a few, like enum and ltree, but others are still missing, like string arrays. it could receive more love from the community. hope this article was worth reading! thanks! originally posted on https://rcpassos.me/post/apache-arrow-future-of-data-engineering top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse rafael passos follow education bachelor's degree in computer science work staff software engineer | lead data engineer joined feb 15, 2023 trending on dev community hot move over llama: tencent's new open llm is ready to self-host # ai # opensource # news # python 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development rediscovering my passion: from burnout back to excitement # devjournal # developer # career # leadership 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/grayhat/building-machine-learning-models-with-pysparks-pysparkml-library-a-comprehensive-guide-4g5h,,,"building machine learning models with pyspark's pyspark.ml library: a comprehensive guide. - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse mwenda harun mbaabu posted on aug 17, 2023 building machine learning models with pyspark's pyspark.ml library: a comprehensive guide. welcome to the comprehensive guide on building machine learning models using pyspark's pyspark.ml library. in this tutorial, we will explore the powerful capabilities that pyspark offers for creating and deploying machine learning solutions in a distributed computing environment. apache spark has revolutionized big data processing by providing a fast and flexible framework for distributed data processing. pyspark, the python interface to apache spark, brings this power to python developers, enabling them to harness the capabilities of spark for building scalable and efficient machine learning pipelines. throughout this guide, we will cover the fundamental concepts of the pyspark.ml library, including data preprocessing, feature engineering, model selection, hyperparameter tuning, and model evaluation. we will delve into various machine learning algorithms available in pyspark, demonstrating how to apply them to different types of tasks such as classification, regression, clustering, and recommendation. whether you are new to machine learning or an experienced practitioner, this tutorial will provide you with the knowledge and tools you need to leverage pyspark's pyspark.ml library to develop powerful and scalable machine learning models for your data-driven projects. let's get started on our journey to mastering machine learning with pyspark! as i have already said earlier, pyspark.ml is the machine learning library within pyspark, which is the python interface to apache spark. it provides a high-level api for building and working with machine learning pipelines, algorithms, and models in a distributed computing environment. the pyspark.ml library is designed to simplify the process of creating and deploying machine learning solutions on large datasets using the parallel processing capabilities of spark. key components and concepts within pyspark.ml include: 1). dataframe: dataframe is a core concept in pyspark. it's a distributed collection of data organized into named columns. dataframes are similar to tables in relational databases or dataframes in libraries like pandas. they provide a structured way to represent and manipulate data, making it suitable for machine learning tasks. 2). transformer: a transformer is an abstraction that represents a transformation applied to a dataframe. it can be used to convert one dataframe into another through a defined transformation process. examples of transformers include vectorassembler (combining features into a vector) and stringindexer (converting categorical strings to numerical indices). 3). estimator: an estimator is an algorithm or model that can be trained on data to generate a transformer. it's a machine learning algorithm that has an internal state and can be fit to data to learn a model. examples of estimators include classification models like logisticregression and clustering models like kmeans. 4). pipeline: a pipeline is a sequence of stages, where each stage can be either a transformer or an estimator . pipelines allow you to assemble a sequence of data processing steps, making it easy to ensure that data transformations are consistent across training and testing sets. this is particularly useful for avoiding data leakage and maintaining code organization. 5). parameter grid and hyperparameter tuning: the paramgridbuilder class allows you to specify hyperparameter grids for hyperparameter tuning. hyperparameter tuning involves searching through various combinations of hyperparameters to find the best-performing model. 6). model persistence: pyspark's pyspark.ml library allows you to save and load trained models to/from disk. this is crucial for deploying and using trained models in production environments without having to retrain them. 7). model evaluation: the pyspark.ml.evaluation module provides classes for evaluating model performance using various metrics, such as classification accuracy, f1-score, and regression metrics like rmse (root mean squared error). 8). feature engineering: pyspark.ml.feature contains classes for feature extraction, transformation, and selection. it includes tools for converting raw data into suitable formats for machine learning algorithms. 9). algorithms: pyspark's pyspark.ml.classification, pyspark.ml.regression, pyspark.ml.clustering, and other sub-packages contain various algorithms and models for different machine learning tasks. pyspark.ml provides a wide range of machine learning algorithms and models for various tasks, such as classification, regression, clustering, recommendation, and more. here are some of the commonly used algorithms available in pyspark.ml: 1). classification algorithms: logistic regression: a linear algorithm used for binary or multi-class classification. decision trees: tree-based algorithm that splits data into branches based on feature values to make predictions. random forest: ensemble of decision trees that combines multiple trees to improve predictive accuracy. gradient-boosted trees (gbt): an ensemble algorithm that builds multiple decision trees in a sequential manner, with each tree correcting the errors of the previous ones. support vector machines (svm): algorithms that find a hyperplane that best separates classes in a high-dimensional space. naive bayes: a probabilistic algorithm based on bayes' theorem used for classification tasks. multilayer perceptron (mlp): a feedforward neural network for classification tasks. 2).regression algorithms: linear regression: a linear algorithm used for regression tasks. decision trees (for regression): similar to classification trees, but used for predicting continuous values. random forest (for regression): an ensemble algorithm for regression tasks. gradient-boosted trees (gbt for regression): an ensemble algorithm for regression tasks. 3). clustering algorithms: k-means: an algorithm that divides data into clusters by minimizing the variance within each cluster. bisecting k-means: a hierarchical clustering algorithm that repeatedly bisects clusters to form a tree. 4). recommendation algorithms: alternating least squares (als): a matrix factorization technique used for collaborative filtering in recommendation systems. 5).dimensionality reduction: principal component analysis (pca): a technique used to reduce the dimensionality of data while preserving its variance. 6).feature selection: chi-square selector: a method for selecting important features based on the chi-squared statistic. feature hasher: a technique for transforming categorical features into numerical features. vector slicer: a tool for selecting and slicing elements from a feature vector. these are just some of the algorithms available in pyspark.ml. each algorithm comes with its own set of hyperparameters that you can tune to optimize the model's performance. additionally, pyspark's paramgridbuilder allows you to create grids of hyperparameters to perform systematic hyperparameter tuning. when using these algorithms, you typically construct a machine learning pipeline that includes data preprocessing, model training, and evaluation stages. this pipeline ensures consistent application of transformations and models to both training and testing datasets, helping to prevent data leakage and ensure reproducibility. example: logistic regression from pyspark.sql import sparksession from pyspark.ml.feature import vectorassembler from pyspark.ml.classification import logisticregression from pyspark.ml import pipeline # create a spark session spark = sparksession . builder . appname ( "" logisticregressionexample "" ). getorcreate () # load your data into a dataframe data = spark . read . csv ( "" data.csv "" , header = true , inferschema = true ) # define feature columns feature_columns = [ "" feature1 "" , "" feature2 "" , "" feature3 "" ] # create a vectorassembler for feature transformation assembler = vectorassembler ( inputcols = feature_columns , outputcol = "" features "" ) # create a logisticregression model lr = logisticregression ( featurescol = "" features "" , labelcol = "" label "" , maxiter = 10 , regparam = 0.01 ) # create a pipeline pipeline = pipeline ( stages = [ assembler , lr ]) # split data into training and test sets train_data , test_data = data . randomsplit ([ 0.8 , 0.2 ]) # fit the pipeline on the training data model = pipeline . fit ( train_data ) # make predictions on the test data predictions = model . transform ( test_data ) # close the spark session spark . stop () enter fullscreen mode exit fullscreen mode top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse mwenda harun mbaabu follow a data engineer and technical with over 5 years of experience. let discuss your next project: mbaabuharun8@gmail.com location kenya education bsc( mathematics and computer science) pronouns he/him joined jan 5, 2020 more from mwenda harun mbaabu the ultimate linux command cheat sheet for data engineers and analysts # dataengineering # bash # datascience # python understanding mcp and how ai engineers can leverage it introducing luxdevhq data science, artificial intelligence, and analytics prep program 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/mage_ai/getting-started-with-apache-flink-a-guide-to-stream-processing-e19,,,"getting started with apache flink: a guide to stream processing - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse mage ai posted on may 15, 2023 getting started with apache flink: a guide to stream processing # flink # streamprocessing # dataprocessing # dataengineering tldr this guide introduces apache flink and stream processing, explaining how to set up a flink environment and create simple applications. key flink concepts are covered along with basic troubleshooting and monitoring techniques. it ends with resources for further learning and community support. outline introduction to apache flink and stream processing setting up a flink development environment a simple flink application walkthrough: data ingestion, processing, and output understanding flink’s key concepts (datastream api, windows, transformations, sinks, sources) basic troubleshooting and monitoring for flink applications conclusion introduction to apache flink and stream processing apache flink is an open-source, high-performance framework designed for large-scale data processing, excelling at real-time stream processing. it features low-latency and stateful computations, enabling users to process live data and generate insights on-the-fly. flink is fault-tolerant, scalable, and provides powerful data processing capabilities that cater to various use cases. stream processing, on the other hand, is a computing paradigm that allows real-time data processing as soon as it arrives or is produced. unlike traditional batch processing systems that deal with data at rest, stream processing handles data in motion. this paradigm is especially useful in scenarios where insights need to be derived immediately, such as real-time analytics, fraud detection, and event-driven systems. flink's powerful stream-processing capabilities and its high-throughput, low-latency, and exactly-once processing semantics make it an excellent choice for such applications. source: giphy setting up a flink development environment setting up a development environment for apache flink is a straightforward process. here's a brief step-by-step guide: install java : flink requires java 8 or 11, so you need to have one of these versions installed on your machine. you can download java from the oracle website or use openjdk. download and install apache flink : you can download the latest binary of apache flink from the official flink website. once downloaded, extract the files to a location of your choice. start a local flink cluster : navigate to the flink directory in a terminal, then go to the 'bin' folder. start a local flink cluster using the command ./start-cluster.sh (for unix/linux/macos) or start-cluster.bat (for windows). check flink dashboard : open a web browser and visit http://localhost:8081 , you should see the flink dashboard, indicating that your local flink cluster is running successfully. set up an integrated development environment (ide) : for writing and testing your flink programs, you can use an ide such as intellij idea or eclipse. make sure to also install the flink plugin if your ide has one. create a flink project : you can create a new flink project (refer - apache flink playground ) using a build tool like maven or gradle. flink provides quickstart maven archetypes to set up a new project easily. once you've set up your flink development environment, you're ready to start developing flink applications. remember that while this guide describes a basic local setup, a production flink setup would involve a distributed cluster and possibly integration with other big data tools. source: giphy a simple flink application walkthrough: data ingestion, processing and output a simple apache flink application can be designed to consume a data stream, process it, and then output the results. let's walk through a basic example: data ingestion (sources) : flink applications begin with one or more data sources. a source could be a file on a filesystem, a kafka topic, or any other data stream. data processing (transformations) : once the data is ingested, the next step is to process or transform it. this could involve filtering data, aggregating it, or applying any computation. data output (sinks) : the final step in a flink application is to output the processed data, also known as a sink. this could be a file, a database, or a kafka topic. job execution : after defining the sources, transformations, and sinks, the flink job needs to be executed. here's a complete example that reads data from a kafka topic, performs some basic word count processing on the stream, and then writes the results into a cassandra table. this example uses java and flink's datastream api. import org.apache.flink.api.common.functions.flatmapfunction;
import org.apache.flink.api.java.tuple.tuple2;
import org.apache.flink.streaming.api.datastream.datastream;
import org.apache.flink.streaming.api.environment.streamexecutionenvironment;
import org.apache.flink.streaming.connectors.cassandra.cassandrasink;
import org.apache.flink.streaming.connectors.kafka.flinkkafkaconsumer;
import org.apache.flink.util.collector;
import org.apache.kafka.common.serialization.simplestringschema;

import java.util.properties;

public class kafkatocassandraexample {

    public static void main(string[] args) throws exception {

        final streamexecutionenvironment env =
streamexecutionenvironment.getexecutionenvironment();

        properties properties = new properties();
        properties.setproperty(""bootstrap.servers"", ""localhost:9092""); // address of your
kafka server
        properties.setproperty(""group.id"", ""test""); // specify your kafka consumer group

        datastream<string> stream = env.addsource(new flinkkafkaconsumer<>(""topic"", new
simplestringschema(), properties));

        datastream<tuple2<string, integer>> processedstream = stream
                .flatmap(new tokenizer())
                .keyby(0)
                .sum(1);

        cassandrasink.addsink(processedstream)
                .setquery(""insert into wordcount.word_count (word, count) values (?, ?);"")
                .sethost(""127.0.0.1"") // address of your cassandra server
                .build();

        env.execute(""kafka to cassandra word count example"");
    }

    public static final class tokenizer implements flatmapfunction<string, tuple2<string,
integer>> {
        @override
        public void flatmap(string value, collector<tuple2<string, integer>> out) {
            // normalize and split the line into words
            string[] words = value.tolowercase().split(""\\w+"");

            // emit the words
            for (string word : words) {
                if (word.length() > 0) {
                    out.collect(new tuple2<>(word, 1));
                }
            }
        }
    }
} enter fullscreen mode exit fullscreen mode source: giphy understanding flink’s key concepts datastream api : flink's main tool for creating stream processing applications, providing operations to transform data streams. windows : defines a finite set of stream events for computations, based on count, time, or sessions. transformations : operations applied to data streams to produce new streams, including map, filter, flatmap, keyby, reduce, aggregate, and window. sinks : the endpoints of flink applications where processed data ends up, such as a file, database, or message queue. sources : the starting points of flink applications that ingest data from external systems or generate data internally, such as a file or kafka topic. event time vs. processing time : flink supports different notions of time in stream processing. event time is the time when an event occurred, while processing time is the time when the event is processed by the system. flink excels at event time processing, which is crucial for correct results in many scenarios. cep (complex event processing) : flink supports cep, which is the ability to detect patterns and complex conditions across multiple streams of events. table api & sql : flink offers a table api and sql interface for batch and stream processing. this allows users to write complex data processing applications using a sql-like expression language. stateful functions (statefun) : statefun is a framework by apache flink designed to build distributed, stateful applications. it provides a way to define, manage, and interact with a dynamically evolving distributed state of functions. operator chain and task : flink operators (transformations) can be chained together into a task for efficient execution. this reduces the overhead of thread-to-thread handover and buffering. savepoints : savepoints are similar to checkpoints, but they are triggered manually and provide a way to version and manage the state of flink applications. they are used for planned maintenance and application upgrades. state management : flink provides fault-tolerant state management, meaning it can keep track of the state of an application (e.g., the last processed event) and recover it if a failure occurs. watermarks : these are a mechanism to denote progress in event time. flink uses watermarks to handle late events in stream processing, ensuring the system can handle out-of-order events and provide accurate results. checkpoints : checkpoints are a snapshot of the state of a flink application at a particular point in time. they provide fault tolerance by allowing an application to revert to a previous state in case of failures. source: giphy basic troubleshooting and monitoring in flink troubleshooting and monitoring are essential aspects of running apache flink applications. here are some key concepts and tools: flink dashboard : this web-based user interface provides an overview of your running applications, including statistics on throughput, latency, and cpu/memory usage. it also allows you to drill down into individual tasks to identify bottlenecks or issues. logging : flink uses slf4j for logging. logs can be crucial for diagnosing problems or understanding the behavior of your applications. log files can be found in the log directory in your flink installation. metrics : flink exposes a wide array of system and job-specific metrics, such as the number of elements processed, bytes read/written, task/operator/jobmanager/taskmanager statistics, and more. these metrics can be integrated with external systems like prometheus or grafana. exceptions : if your application fails to run, flink will throw an exception with a stack trace, which can provide valuable information about the cause of the error. reviewing these exceptions can be a key part of troubleshooting. savepoints/checkpoints : these provide a mechanism to recover your application from failures. if your application isn't recovering correctly, it's worth investigating whether savepoints/checkpoints are being made correctly and can be successfully restored. backpressure : if a part of your data flow cannot process events as fast as they arrive, it can cause backpressure, which can slow down the entire application. the flink dashboard provides a way to monitor this. network metrics : flink provides metrics on network usage, including buffer usage and backpressure indicators. these can be useful for diagnosing network-related issues. remember, monitoring and troubleshooting are iterative processes. if you notice performance degradation or failures, use these tools and techniques to investigate, identify the root cause, and apply a fix. then monitor the system again to ensure that the problem has been resolved. source: giphy conclusion in conclusion, apache flink is a robust and versatile open-source stream processing framework that enables fast, reliable, and sophisticated processing of large-scale data streams. starting with a simple environment setup, we've walked through creating a basic flink application that ingests, processes, and outputs data. we've also touched on the foundational concepts of flink, such as the datastream api, windows, transformations, sinks, and sources, all of which serve as building blocks for more complex applications. in episode 4 of apache flink series, we'll see how to consume data from kafka in real time and process it with mage. link to the original blog : https://www.mage.ai/blog/getting-started-with-apache-flink-a-guide-to-stream-processing top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse mage ai follow 🧙 your ai data engineer! build, deploy, and run data pipelines through an intuitive interface in minutes. run at any scale instantly with mage pro. location santa clara, ca joined aug 22, 2021 more from mage ai streamline data transfer: the ultimate data integration guide sftp to bigquery # dataengineering # dataintegration # bigquery # mageai we're back and better than ever! # mageai # ai # datapipelines # dataengineering aws redshift: robust and scalable data warehousing # aws # redshift # datawarehousing # dataengineering 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/kirekov/apache-spark-hive-and-spring-boot-testing-guide-mdp,JUnit,Integration Testing,"apache spark, hive, and spring boot — testing guide - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse semyon kirekov posted on apr 22, 2022 apache spark, hive, and spring boot — testing guide # bigdata # testing # java # docker big data is trending. the companies have to operate with a huge amount of data to compete with others. for example, this information is used to show you the relevant advertisements and recommend you the services that you may find interesting. the problem with big data software systems is their complexity. testing becomes tough. how could you verify the app behaviour locally when it's tuned to connect to the hdfs cluster? in this article, i'm showing you how to create a spring boot app that loads data from apache hive via apache spark to the aerospike database . more than that, i'm giving you a recipe for writing integration tests for such scenarios that can be run either locally or during the ci pipeline execution. the code examples are taken from this repository . firstly, let's get over some basic concepts of the big data stack we're using. don't worry, it won't take long. but it's necessary to understand the core idea. basics of hdfs hdfs (hadoop distributed file system) is a distributed file system designed to run on many physical servers. so, a file in hdfs is an abstraction that hides the complexity of storing and replicating the data between multiple nodes. why do we need hdfs? there are some reasons. hardware failures hard disk drives crash. that's the reality we have to deal with. if a file is split between multiple nodes, individual failures won't affect the whole data. besides, data is replicated in hdfs. so, even after a disk crash, the information can be restored from the other sources. really large files hdfs allows building of a network of not so powerful machines into a huge system. for example, if you have 100 nodes with 1tb disk storage on each one, then you possess 100tb of hdfs space. if the replication factor equals 3, it's possible to store a single file with a size of 33tb. not to mention that lots of local file systems do not support so large files, even if you have the available disk space. the speed of reading if you read the file sequentially, it will take you n . but if the file is split into 10 chunks between 10 nodes, you can get its content in n/10 time! because each node can read the chunk in parallel. so, hdfs is not only about safety. it's about swiftness. we have omitted the time spend on network communications. but if files are huge, this part is just a fraction. basics of apache hive apache hive is the database facility running over hdfs. it allows querying data with hql (sql-like language). regular databases (e.g. postgresql, oracle) act as an abstraction layer over the local file system. while apache hive acts as an abstraction over hdfs. that's it. basics of apache spark apache spark is a platform for operating and transforming huge amounts of data. the key idea is that apache spark workers run on multiple nodes and store the intermediate results in ram. it's written in scala but it also supports java and python. take a look at the schema below. it's the common representation of the apache spark batch job. apache spark loads data from data producer , proceeds some operations on it, and puts the result to data consumer (in our case, apache hive is data producer and aerospike is data consumer). apache spark application is a regular .jar file that contains the transformation logic. take a look at the example below. javardd < string > textfile = sc . textfile ( ""hdfs://raw_data.txt"" ); javapairrdd < string , integer > counts = textfile . flatmap ( s -> arrays . aslist ( s . split ( "" "" )). iterator ()) . maptopair ( word -> new tuple2 <>( word , 1 )) . reducebykey (( a , b ) -> a + b ); counts . saveastextfile ( ""hdfs://words_count.txt"" ); enter fullscreen mode exit fullscreen mode it's a simple word-count application. firstly, we load the content of the raw_data.txt hdfs file. then we split each line by "" "" , assign 1 for every word, and reduce the result by words to summarize the whole numbers. then the obtained pairs are saved to word_count.txt . the flow is similar to java stream api . the difference is that every lambda expression is executed on the workers. so, spark transfers the code to the remote machines, performs the calculation, and returns the obtained results. if we owe a sufficient number of workers, we can proceed with the amount of data that is measured by terabytes or even zettabytes. the apache spark approach of delivering code to data has some drawbacks. we'll discuss it when we get to the development. another important aspect is laziness . just like stream api, apache spark does not start any calculations until terminal operation invocation. in this case, reducebykey is the one. the rest operations build the pipeline rules but do not trigger anything. build configuration let's start the development process. firstly, we need to choose the java version. at the moment of writing the latest stable apache spark release is 3.2.1 . it supports java 11. so, we gonna use it. currently apache spark does not support java 17. make sure you don't use it for running integration tests. otherwise, you'll get bizarre error messages. the project is bootstrapped with spring initializr . nothing special here. but the dependencies list should be clarified. dependencies resolution ext { set ( 'testcontainersversion' , '1.16.2' ) set ( 'sparkversion' , '3.2.1' ) set ( 'slf4jversion' , '1.7.36' ) set ( 'aerospikeversion' , '5.1.11' ) } dependencies { annotationprocessor 'org.springframework.boot:spring-boot-configuration-processor' implementation ( 'org.springframework.boot:spring-boot-starter-validation' ) { exclude group: 'org.slf4j' } implementation ( ""com.aerospike:aerospike-client:${aerospikeversion}"" ) { exclude group: 'org.slf4j' } compileonly ""org.apache.spark:spark-core_2.13:${sparkversion}"" compileonly ""org.apache.spark:spark-hive_2.13:${sparkversion}"" compileonly ""org.apache.spark:spark-sql_2.13:${sparkversion}"" compileonly ""org.slf4j:slf4j-api:${slf4jversion}"" testimplementation 'org.apache.derby:derby' testimplementation ""org.apache.spark:spark-core_2.13:${sparkversion}"" testimplementation ""org.apache.spark:spark-hive_2.13:${sparkversion}"" testimplementation ""org.apache.spark:spark-sql_2.13:${sparkversion}"" testimplementation 'org.springframework.boot:spring-boot-starter-test' testimplementation ""org.slf4j:slf4j-api:${slf4jversion}"" testimplementation 'org.codehaus.janino:janino:3.0.8' testimplementation 'org.testcontainers:junit-jupiter' testimplementation 'org.awaitility:awaitility:4.2.0' testimplementation 'org.hamcrest:hamcrest-all:1.3' } enter fullscreen mode exit fullscreen mode core dependencies first comes apache spark dependencies. the spark-core artefact is the root. the spark-hive enables data retrieving from apache hive. and the spark-sql dependency gives us the ability to query data from apache hive with sql usage. note that all the artefacts have to share the same version (in our case, it is 3.2.1 ). as a matter of fact, the apache spark dependencies' version should match the one that runs the production cluster in your company. all spark dependencies have to be marked as compileonly . it means that they won't be included in the assembled .jar file. apache spark will provide the required dependencies in runtime. if you include them as implementation scope, that may lead to hard-tracking bugs during execution. then we have aerospike-client dependency. you have probably noticed that the org.slf4j group is excluded everywhere and included as a compileonly dependency as well. we'll talk about this later when we get to the apache spark logging facility. test dependencies and finally, here comes test scoped artefacts. apache spark ones are included as testimplementation . because integration tests will start the local spark node. so, they are required during the runtime. the slf4j-api is also the runtime dependency. testcontainers will be used to run the aerospike instance. the janino is required by apache spark during the job execution. and we need apache derby to tune apache hive for local running. we'll get to this point soon. logging configuration apache spark applies log4j with the slf4j wrapper. but the default spring boot logger is logback . this setup leads to exceptions during spring context initializing due to multiple logging facilities present in the classpath. the easiest way to solve it is to exclude all auto-configured spring boot logging features. that's not a big deal. anyway, apache spark provides its own slf4j implementation during the runtime. so, we just need to include this dependency as compileonly . that is sufficient. excluding logback from the spring boot project is easy with gradle. take a look at the example below. configurations { compileonly { extendsfrom annotationprocessor } all { exclude group: 'org.springframework.boot' , module: 'spring-boot-starter-logging' exclude group: 'org.springframework.boot' , module: 'snakeyaml' } } enter fullscreen mode exit fullscreen mode possible application.yml issues the snakeyml exclusion requires special attention. spring boot uses the library to parse properties from .yml files (i.e. application.yml ). some apache spark versions use the same library for internal operations. the thing is that the versions required by spring boot and apache spark differ. if you exclude it from spring boot dependency and rely on the one provided by apache spark, you will face the nosuchmethoderror (spring boot invokes the method that is absent in the version provided by apache spark). so, i would recommend sticking with the .properties format and removing spring boot yaml auto-configuration. that will help you to avoid unnecessary difficulties. take a look at the code example below. @springbootapplication ( exclude = { gsonautoconfiguration . class }) public class sparkbatchjobapplication { public static void main ( string [] args ) { springapplication . run ( sparkbatchjobapplication . class , args ); } } enter fullscreen mode exit fullscreen mode fat jar the result .jar is going to submitted to apache spark cluster (e.g. spark-submit command ). so, it should contain all runtime artefacts. unfortunately, the standard spring boot packaging does not put the dependencies in the way apache spark expects it. so, we'll use shadow-jar gradle plugin . take a look at the example below. plugins { id 'org.springframework.boot' version '2.6.3' id 'io.spring.dependency-management' version '1.0.11.release' id 'java' id 'com.github.johnrengelman.shadow' version '2.0.4' } ... shadowjar { zip64 true mergeservicefiles () append 'meta-inf/spring.handlers' append 'meta-inf/spring.schemas' append 'meta-inf/spring.tooling' transform ( propertiesfiletransformer ) { paths = [ 'meta-inf/spring.factories' ] mergestrategy = ""append"" } } enter fullscreen mode exit fullscreen mode now we can run all tests and build the artefact with the ./gradlew test shadowjar command. starting development now we can get to the development process. apache spark configuration we need to declare javasparkcontext and sparksession . the first one is the core apache spark for all operations. whilst sparksession is the part of spark-sql projects. it allows us to query data with sql (which is quite handy for apache hive). take a look at the spring configuration below. @configuration public class sparkconfig { @value ( ""${spring.application.name}"" ) private string appname ; @bean @profile ( local ) public sparkconf localsparkconf () throws ioexception { final var localhivepath = files . createtempdirectory ( ""hivedatawarehouse"" ); filesystemutils . deleterecursively ( localhivepath ); return new sparkconf () . setappname ( appname ) . setmaster ( ""local"" ) . set ( ""javax.jdo.option.connectionurl"" , ""jdbc:derby:memory:local;create=true"" ) . set ( ""javax.jdo.option.connectiondrivername"" , ""org.apache.derby.jdbc.embeddeddriver"" ) . set ( ""hive.stats.jdbc.timeout"" , ""80"" ) . set ( ""spark.ui.enabled"" , ""false"" ) . set ( ""spark.sql.session.timezone"" , ""utc"" ) . set ( ""spark.sql.catalogimplementation"" , ""hive"" ) . set ( ""spark.sql.warehouse.dir"" , localhivepath . toabsolutepath (). tostring ()); } @bean @profile ( prod ) public sparkconf prodsparkconf () { return new sparkconf () . setappname ( appname ); } @bean public javasparkcontext javasparkcontext ( sparkconf sparkconf ) { return new javasparkcontext ( sparkconf ); } @bean public sparksession sparksession ( javasparkcontext sparkcontext ) { return sparksession . builder () . sparkcontext ( sparkcontext . sc ()) . config ( sparkcontext . getconf ()) . enablehivesupport () . getorcreate (); } } enter fullscreen mode exit fullscreen mode sparkconf defines configuration keys for the apache spark job. as you have noticed, there are two beans for different spring profiles . local is used for integration testing and prod is applied in the production environment. the prod configuration does not declare any properties because usually they are passed as command-line arguments in the spark-submit shell script. on the contrary, the local profile defines a set of default properties required for proper running. here are the most important ones. setmaster(""local"") tells apache spark to start a single local node. javax.jdo.option.connectionurl and javax.jdo.option.connectiondrivername declare the jdbc connection for apache hive meta-storage. that's why we added apache derby as the project dependency spark.sql.catalogimplementation means that local files shall be stored in the apache hive compatible format spark.sql.warehouse.dir is the directory for storing apache hive data. here we're using temporary directory. javasparkcontext accepts the defined sparkconf as the constructor arguments. meanwhile sparksession wraps the existing javasparkcontext . note that apache hive support should be enabled manually ( enablehivesupport ). creating apache hive tables when we submit an application to the production apache spark cluster, we probably won't need to create any apache hive tables. most likely the tables have already been created by someone else. and our goal is to select rows and transfer the data to another storage. but when we run integration tests locally (or in the ci environment), there are no tables by default. so, we need to create them somehow. in this project, we're working with one table - media.subscriber_info . it consists of two columns. msisdn (phone number) and some subscriber id. before each test run, we have to delete previous data and add new rows to ensure verifying rules' consistency. the easiest way to achieve it is to declare scripts for table creation and dropping. we'll keep them in the resources directory. take a look at the structure below. v1_media.hql creates media database if it's absent. create database if not exists media enter fullscreen mode exit fullscreen mode v2__media.subscriber_info.hql creates subscriber_info table if it's absent. create table if not exists media . subscriber_info ( subscriber_id string , msisdn string ) row format delimited fields terminated by ',' lines terminated by ' \n ' stored as textfile enter fullscreen mode exit fullscreen mode drop v1__mediatv_dds.subscriber_info.hql drops the subscriber_info table. drop table if exists media . subscriber_info enter fullscreen mode exit fullscreen mode v[n] prefixes are not obligatory. i put them to ensure that each new table script will be executed as the last one. it is helpful to make tests work deterministically. ok, now we need a handler to process those hql queries. take a look at the example below. @component @profile ( local ) public class inithive { private final sparksession session ; private final applicationcontext applicationcontext ; public void createtables () { executesqlscripts ( getresources ( applicationcontext , ""classpath:hive/ddl/create/*.hql"" )); } public void droptables () { executesqlscripts ( getresources ( applicationcontext , ""classpath:hive/ddl/drop/*.hql"" )); } private void executesqlscripts ( resource [] resources ) { for ( resource resource : resources ) { session . sql ( readcontent ( resource )); } } } enter fullscreen mode exit fullscreen mode the first thing to notice is @profile(local) usage. because we don't need to create or drop tables in the production environment. the createtables and droptables methods provide the list of resources containing the required queries. getresources is the utility function that reads files from the classpath. you can discover the implementation here . so, now we're ready to write the business code! business code facade the core interface is enricherservice public interface enricherservice { void proceedenrichment (); } enter fullscreen mode exit fullscreen mode we're expecting that it might have many implementations. each one represent a step in whole batch process. then we have enricherservicefacade that encapsulates all implementations of enricherservice and run them one by one. @service public class enricherservicefacade { private final list < enricherservice > enricherservices ; public void proceedenrichment () { list < enrichmentfailedexception > errors = new arraylist <>(); for ( enricherservice service : enricherservices ) try { service . proceedenrichment (); } catch ( exception e ) { errors . add ( new enrichmentfailedexception ( ""unexpected error during enrichment processing"" , e )); } if (! errors . isempty ()) { throw new enrichmentfailedexception ( errors ); } } } enter fullscreen mode exit fullscreen mode we're trying to run every provided enrichment step. if any of them fails, we throw the exception that combines all errors into a solid piece. finally, we need to tell spring to execute enricherservicefacade.proceedenrichment on application startup. we could add it directly to the main method, but it's not the spring way . therefore, it makes testing harder. the better option is @eventlistener . @component @profile ( prod ) public class mainlistener { private final enricherservicefacade enricherservicefacade ; @eventlistener public void proceedenrichment ( contextrefreshedevent event ) { final long startnano = system . nanotime (); log . info ( ""starting enrichment process"" ); try { enricherservicefacade . proceedenrichment (); log . info ( ""enrichment has finished successfully. it took "" + duration . ofnanos ( system . nanotime () - startnano )); } catch ( exception e ) { string err = ""enrichment has finished with error. it took "" + duration . ofnanos ( system . nanotime () - startnano ); log . error ( err , e ); throw new enrichmentfailedexception ( err , e ); } } } enter fullscreen mode exit fullscreen mode the proceedenrichment method is being invoked, when the spring context is started. by the way, only the active prod profile will trigger the job. enricherservice implementation we're going to deal with a single enricherservice implementation. it simply selects all rows from the media.subcriber_info table and puts the result in the aerospike database. take a look at the code snippet below. @service public class subscriberidenricherservice implements enricherservice , serializable { private static final long serialversionuid = 10l ; private final sparksession session ; private final aerospikeproperties aerospikeproperties ; @override public void proceedenrichment () { dataset < row > dataset = session . sql ( ""select subscriber_id, msisdn from media.subscriber_info "" + ""where msisdn is not null and subscriber_id is not null"" ); dataset . foreachpartition ( iterator -> { final var aerospikeclient = newaerospikeclient ( aerospikeproperties ); iterator . foreachremaining ( row -> { string subscriberid = row . getas ( ""subscriber_id"" ); string msisdn = row . getas ( ""msisdn"" ); key key = new key ( ""my-namespace"" , ""huawei"" , subscriberid ); bin bin = new bin ( ""msisdn"" , msisdn ); try { aerospikeclient . put ( null , key , bin ); log . info ( ""record has been successfully added {}"" , key ); } catch ( exception e ) { log . error ( ""fail during inserting record to aerospike"" , e ); } }); } ); } } enter fullscreen mode exit fullscreen mode there are multiple points that has to be clarified. serialization apache spark applies a standard java serialization mechanism. so, any dependencies used inside lambdas ( map , filter , groupby , foreach , etc.) have to implement the serializable interface. otherwise, you'll get the notserializableexception during the runtime. we have a reference to aerospikeproperties inside the foreachpartition callback. therefore, this class and the subscriberidenricherservice itself should be allowed for serializing (because the latter one keeps aerospikeproperties as a field). if a dependency is not used within any apache spark lambda, you can mark it as transient . and finally, the serialversionuid manual assignment is crucial. the reason is that apache spark might serialize and deserialize the passed objects multiple times. and there is no guarantee that each time auto-generated serialversionuid will be the same. it can be a reason for hard-tracking floating bugs. to prevent this you should declare serialversionuid by yourself. the even better approach is to force the compiler to validate the serialversionuid field presence on any serializable classes. in this case, you need to mark -xlint:serial warning as an error. take a look at the gradle example. tasks . withtype ( javacompile ) { options . compilerargs << ""-xlint:serial"" << ""-werror"" } enter fullscreen mode exit fullscreen mode aerospike client instantiation unfortunately, the java aerospike client does not implement the serializable interface. so, we have to instantiate it inside the lambda expression. in that case, the object will be created on a worker node directly. it makes serialization redundant. i should admit that aerospike provides aerospike connect framework that allows transferring data via apache spark in a declarative way without creating any java clients. anyway, if you want to use it, you have to install the packed library to the apache spark cluster directly. there is no guarantee that you'll have such an opportunity in your situation. so, i'm omitting this scenario. partitioning the dataset class has the foreach method that simply executes the given lambda for each present row. however, if you initialize some heavy resource inside that callback (e.g. database connection), the new one will be created for every row (in some cases there might billions of rows). not very efficient, isn't it? the foreachpartition method works a bit differently. apache spark executes it once per the dataset partition. it also accepts iterator<row> as an argument. so, inside the lambda, we can initialize ""heavy"" resources (e.g. aerospikeclient ) and apply them for calculations of every row inside the iterator. the partition size is calculated automatically based on the input source and apache spark cluster configuration. though you can set it manually by calling the repartition method. anyway, it is out of the scope of the article. testing aerospike setup ok, we've written some business code. how do we test it? firstly, let's declare aerospike setup for testcontainers . take a look at the code snippet below. @contextconfiguration ( initializers = integrationsuite . initializer . class ) public class integrationsuite { private static final string aerospike_image = ""aerospike/aerospike-server:5.6.0.4"" ; static class initializer implements applicationcontextinitializer < configurableapplicationcontext > { static final genericcontainer <?> aerospike = new genericcontainer <>( dockerimagename . parse ( aerospike_image )) . withexposedports ( 3000 , 3001 , 3002 ) . withenv ( ""namespace"" , ""my-namespace"" ) . withenv ( ""service_port"" , ""3000"" ) . waitingfor ( wait . forlogmessage ( "".*migrations: complete.*"" , 1 )); @override public void initialize ( configurableapplicationcontext applicationcontext ) { startcontainers (); aerospike . followoutput ( new slf4jlogconsumer ( loggerfactory . getlogger ( ""aerospike"" )) ); configurableenvironment environment = applicationcontext . getenvironment (); mappropertysource testcontainers = new mappropertysource ( ""testcontainers"" , createconnectionconfiguration () ); environment . getpropertysources (). addfirst ( testcontainers ); } private static void startcontainers () { startables . deepstart ( stream . of ( aerospike )). join (); } private static map < string , object > createconnectionconfiguration () { return map . of ( ""aerospike.hosts"" , stream . of ( 3000 , 3001 , 3002 ) . map ( port -> aerospike . gethost () + "":"" + aerospike . getmappedport ( port )) . collect ( collectors . joining ( "","" )) ); } } } enter fullscreen mode exit fullscreen mode the integrationsuite class is used as the parent for all integration tests. the integrationsuite.initializer inner class is used as the spring context initializer. the framework calls it when all properties and bean definitions are already loaded but no beans have been created yet. it allows us to override some properties during the runtime. we declare the aerospike container as genericcontainer because the library does not provide out-of-box support for the database. then inside the initialize method we retrieve the container's host and port and assign them to the aerospike.hosts property. apache hive utilities before each test method we are suppose to delete all data from apache hive and add new rows required for the current scenario. so, tests won't affect each other. let's declare a custom test facade for apache hive. take a look at the code snippet below. @testcomponent public class testhiveutils { @autowired private sparksession sparksession ; @autowired private inithive inithive ; public void cleanhive () { inithive . droptables (); inithive . createtables (); } public < t , e extends hivetable < t >> e insertinto ( function < sparksession , e > tablefunction ) { return tablefunction . apply ( sparksession ); } } enter fullscreen mode exit fullscreen mode there are just two methods. the cleanhive drops all existing and creates them again. therefore, all previous data is erased. the insertinto is tricky. it serves the purpose of inserting new rows to apache hive in a statically typed way. how is that done? first of all, let's inspect the hivetable<t> interface. public interface hivetable < t > { void values ( t ... t ); } enter fullscreen mode exit fullscreen mode as you see, it's a regular java functional interface. though the implementations are not so obvious. public class subscriberinfo implements hivetable < subscriberinfo . values > { private final sparksession session ; public static function < sparksession , subscriberinfo > subscriberinfo () { return subscriberinfo: : new ; } @override public void values ( values ... values ) { for ( values value : values ) { session . sql ( format ( ""insert into %s values('%s', '%s')"" , ""media.subscriber_info"" , value . subscriberid , value . msisdn ) ); } } public static class values { private string subscriberid = ""4121521"" ; private string msisdn = ""88005553535"" ; public values setsubscriberid ( string subscriberid ) { this . subscriberid = subscriberid ; return this ; } public values setmsisdn ( string msisdn ) { this . msisdn = msisdn ; return this ; } } } enter fullscreen mode exit fullscreen mode the class accepts sparksession as a constructor dependency. the subscriberinfo.values are the generic argument. the class represents the data structure containing values to insert. and finally, the values implementation performs the actual new row creation. the key is the subscriberinfo static method. what's the reason to return function<sparksession, subscriberinfo> ? its combination with testhiveutils.insertinto provides us with statically typed insert into statement. take a look at the code example below. hive . insertinto ( subscriberinfo ()) . values ( new subscriberinfo . values () . setmsisdn ( ""msisdn1"" ) . setsubscriberid ( ""subscriberid1"" ), new subscriberinfo . values () . setmsisdn ( ""msisdn2"" ) . setsubscriberid ( ""subscriberid2"" ) ); enter fullscreen mode exit fullscreen mode an elegant solution, don't you think? spark integration test slice spring integration tests require a specific configuration. it's wise to declare it once and reuse it. take a look at the code snippet below. @springboottest ( classes = { sparkconfig . class , sparkcontextdestroyer . class , aerospikeconfig . class , propertiesconfig . class , inithive . class , testhiveutils . class , testaerospikefacade . class , enricherservicetestconfiguration . class } ) @activeprofiles ( local ) public class sparkintegrationsuite extends integrationsuite { } enter fullscreen mode exit fullscreen mode inside the springboottest we have listed all the beans that are used during tests running. testaerospikefacade is just a thin wrapper around the java aerospike client for test purposes. its implementation is rather straightforward but you can check out the source code by this link . the enricherservicetestconfiguration is the spring configuration declaring all implementations for the enricherservice interface. take a look at the example below. @testconfiguration public class enricherservicetestconfiguration { @bean public enricherservice subscriberenricherservice ( sparksession session , aerospikeproperties aerospikeproperties ) { return new subscriberidenricherservice ( session , aerospikeproperties ); } } enter fullscreen mode exit fullscreen mode i want to point out that all enricherservice implementations should be listed inside the class. if we apply different configurations for each test suite, the spring context will be reloaded. mostly that's not a problem. but apache spark usage brings obstacles. you see, when javasparkcontext is created, it starts the local apache spark node. but when we instantiate it twice during the application lifecycle, it will result in an exception. the easiest way to overcome the issue is to make sure that javasparkcontext will be created only once. now we can get to the testing process. integration test example here is a simple integration test that inserts two rows to apache spark and checks that the corresponding two records are created in aerospike within 10 seconds. take look at the code snippet below. class subscriberidenricherserviceintegrationtest extends sparkintegrationsuite { @autowired private testhiveutils hive ; @autowired private testaerospikefacade aerospike ; @autowired private enricherservice subscriberenricherservice ; @beforeeach void beforeeach () { aerospike . deleteall ( ""my-namespace"" ); hive . cleanhive (); } @test void shouldsaverecords () { hive . insertinto ( subscriberinfo ()) . values ( new subscriberinfo . values () . setmsisdn ( ""msisdn1"" ) . setsubscriberid ( ""subscriberid1"" ), new subscriberinfo . values () . setmsisdn ( ""msisdn2"" ) . setsubscriberid ( ""subscriberid2"" ) ); subscriberenricherservice . proceedenrichment (); list < keyrecord > keyrecords = await () . atmost ( ten_seconds ) . until (() -> aerospike . scanall ( ""my-namespace"" ), hassize ( 2 )); assertthat ( keyrecords , allof ( hasrecord ( ""subscriberid1"" , ""msisdn1"" ), hasrecord ( ""subscriberid2"" , ""msisdn2"" ) )); } } enter fullscreen mode exit fullscreen mode if you tune everything correctly, the test will pass. the whole test source is available by this link . conclusion that's basically all i wanted to tell you about testing apache hive, apache spark, and aerospike integration with spring boot usage. as you can see, the big data world is not so complicated after all. all code examples are taken from this repository . you can clone it and play around with tests by yourself. if you have any questions or suggestions, please leave your comments down below. thanks for reading! resources repository with examples hdfs (hadoop distributed file system) apache hive apache spark apache derby aerospike database aerospike connect framework java stream api spring initializr spring profiles testcontainers gradle plugin shadow-jar top comments (4) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand citronbrick citronbrick citronbrick follow developper work junior front end engineer joined jan 11, 2021 • aug 8 '22 dropdown menu copy link hide rdd s are oudated since spark 2. dataset should be used instead. note that, before spark 2.0, the main programming interface of spark was the resilient distributed dataset (rdd). after spark 2.0, rdds are replaced by dataset, which is strongly-typed like an rdd, but with richer optimizations under the hood. spark docs like comment: like comment: 1 like like comment button reply collapse expand worldofsolitaires worldofsolitaires worldofsolitaires follow joined may 10, 2021 • apr 28 '22 dropdown menu copy link hide thanks a lot for this detailed and specific guide super mario bros like comment: like comment: 2 likes like comment button reply collapse expand akarsh- akarsh- akarsh- follow joined may 16, 2023 • may 16 '23 dropdown menu copy link hide hi, thanks for explanation. is there a way we can use spring version 3.0.5+ and spark. i am getting servlet error class servletcontainer is not a javax.servlet.servlet like comment: like comment: 1 like like comment button reply collapse expand semyon kirekov semyon kirekov semyon kirekov follow java team lead, conference speaker, and technical author.
telegram for contact: @kirekov location russia, moscow education polzunov altai state technical university work java team lead, a conference speaker, and a lecturer joined sep 20, 2021 • may 16 '23 • edited on may 16 • edited dropdown menu copy link hide i think you just need to remove dependency spring-web-starter like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse semyon kirekov follow java team lead, conference speaker, and technical author.
telegram for contact: @kirekov location russia, moscow education polzunov altai state technical university work java team lead, a conference speaker, and a lecturer joined sep 20, 2021 more from semyon kirekov jooq is not a replacement for hibernate. they solve different problems # java # jooq # hibernate # database rich domain model with spring boot and hibernate # java # hibernate # jpa # springboot integration tests for n + 1 problem in java # java # testing # docker # performance 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/wardaliaqat01/big-data-processing-emr-with-spark-and-hadoop-python-pyspark-4jo4,,,"big data processing, emr with spark and hadoop | python, pyspark - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse warda liaqat posted on mar 27, 2022 big data processing, emr with spark and hadoop | python, pyspark introduction: aws's cool data analysis services can be of significant help when it comes to processing and analyzing large amounts of data. use case: to demonstrate our data processing job, we will use emr cluster and s3 (as a storage medium for data) along with python code and the pyspark library. we will execute python code on a data set of stack overflow annual developer survey 2021 and print out some results based on that data. those results will then be stored in s3. in case you are just starting with big data, i would like to introduce you to some terms we are going to work with, you may skip below few lines if you're already familiar. emr (elastic mapreduce): amazon emr is a managed cluster platform that simplifies running big data frameworks, such as apache hadoop and apache spark , on aws to process and analyze vast amounts of data. wanna dig more dipper? amazon s3: amazon s3 is object storage built to store and retrieve any amount of data from anywhere. s3 is a global service. simply say it works like google drive. wanna dig more dipper? apache spark: apache spark is an open-source, distributed processing system used for big data workloads. wanna dig more dipper? apache hadoop: apache hadoop is an open source framework that is used to efficiently store and process large datasets ranging in size from gigabytes to petabytes of data. wanna dig more dipper? hadoop is designed to handle batch processing efficiently whereas spark is designed to handle real-time data efficiently. hadoop is a high latency computing framework, which does not have an interactive mode whereas spark is a low latency computing and can process data interactively. pyspark: pyspark is the python api for apache spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. wanna dig more dipper? outline downloading a data set from stack overflow setup an amazon s3 bucket with different folders setting up an emr cluster write a python code to perform some analysis on data and print results connecting to cluster via ssh and using pyspark to load data from amazon s3 viewing analysis results cleanup recap guided procedure 1. downloading a data set from stack overflow go to stack overflow annual developer survey and download the latest data set for 2021 ultimately, it will download four files, but in this case, we will only be using the ""survey results public"" file 2. setup an amazon s3 bucket with different folders login to aws management console navigate to s3 and buckets create a new bucket named as big-data-demo-bucket with versioning enabled and encryption true click on your bucket once you've created it create two folders named as bigdata-emr-logs (for storing emr logs) and data-source (for storing our source data file) with encryption enabled place your source data file in the source data folder 3. setting up an emr cluster search for emr click on create cluster an s3 bucket is needed to store emr logs. when you don't want to do it manually, emr will automatically create a bucket for you to store logs. select a bucket and folder that we created in previous step select spark with hadoop and zeppelin in the software configurations in terms of hardware configurations, you can choose ec2 type based on your needs for testing purposes, 3 instances would be sufficient. you may create as many as you need enable auto-termination, which will terminate your cluster if any error occurs during the creation process you can also set the cluster to automatically terminate when it goes into idle state for long it is also completely up to you whether or not to scale select a key pair under security and access to ssh after launching the cluster if you do not have a keypair, you can create easily from ec2 dashboard once you click on the create cluster button, your cluster will be created. it usually takes 10 to 15 minutes for the cluster to become operational 4. write a python code to perform some analysis on data and print results now let's write some code. a spark job will run this code to analyze the data and print out the results. from pyspark.sql import sparksession
from pyspark.sql.functions import col

s3_data_source_path = 's3://big-data-demo-bucket/data-source/survey_results_public.csv'
s3_data_output = 's3://big-data-demo-bucket/data-output'


def main():
    spark = sparksession.builder.appname('bigdatademoapp').getorcreate()
    all_data = spark.read.csv(s3_data_source_path, header=true)
    print('total number of records in dataset: %s' % all_data.count())
    selected_data = all_data.where((col('country') == 'united states of america') & (col('age1stcode') == '11 - 17 years'))
    print('total number of engineers who work more than 45 hours in the us is : %s' % selected_data.count())
    selected_data.write.mode('overwrite').parquet(s3_data_output)
    print('selected data was successfully saved to: %s' % s3_data_output)

if __name__ == '__main__':
    main() enter fullscreen mode exit fullscreen mode what does this code do? setting up a spark session data reading from s3 printing some results based on certain conditions a s3 folder is created for storing the results 5. connecting to cluster via ssh and using pyspark to load data from amazon s3 don't forget to enable ssh connections before trying to ssh into your cluster. the port 22 needs to be added to the cluster's master security group follow the instructions accordingly to connect to your cluster if you are using windows or mac i’m on windows so, i used putty for the connection use the vi main.py command to create a python file using the vim editor press i on your keyboard and paste your code press the ecs key to exit the insert mode type :wq to quite the editor with saving your changes if you type cat main.py , you can view your code to submit this spark job use spark-submit [filename] your job will begin executing 6. viewing analysis results your three print results can be viewed in the logs after completion you can now see that the latest logs are stored in the logs folder in s3. additionally, you'll notice a new folder named data-output that contains all output results with success files has been created for you 7. cleanup you can then terminate the cluster to save money when you terminate a cluster, all ec2 associated with it will also be terminated 8. recap this article showed how you can use emr and amazon s3 to process and analyze a vast amount of data collected from stack overflow developer survey to extract some useful insights welcome to the end of this article, happy clouding! let me know what do you think about it?? top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse warda liaqat follow a diligent computer engineering major with 3 years of work experience in cloud computing especially aws. i deal with day-to-day cloud operations and infrastructure stuff (devops). location lahore, pakistan education bs software engineering work consultant - cloud services @ systems ltd. || aws community builder joined jun 13, 2021 more from warda liaqat building the name match memory game: my experience with amazon q cli # python # flask # aws # amazonq revolutionize your aws app development: aws application composer and our helpdesk project # awsapplicationcomposer # vscode # devops # aws ci/cd pipeline hands-on | aws code pipeline, elastic beanstalk, github # node # aws # github # elastic 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/txfs19260817/create-a-hadoop-playground-with-docker-desktop-on-windows-in-minutes-10im,,,"create a hadoop playground with docker desktop on windows in minutes - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse zishuo ding posted on mar 25, 2022 create a hadoop playground with docker desktop on windows in minutes # docker # java # bigdata # codenewbie this semester, i've chosen to take a course about parallel computing. one of the projects involves writing a mapreduce program on hadoop in java. connecting to the school's computing resources might be difficult at times, especially when a due date is approaching. as a result, i looked for an easy way to set up a local hadoop environment with docker on my windows laptop, so that i could conduct some experiments quickly. preparations docker docker saves us from having to go through complicated installation procedures for certain softwares (including hadoop in this post), and it also allows us to clearly delete them if we need to free up some disk space. the first step is to download and install a docker desktop for windows (for mac if your os is mac) on your computer. now, docker desktop supports using wsl 2 (windows subsystem for linux 2) instead of hyper-v as the backend. if you do not have wsl on your windows machine, you could follow this official guide to enable it. you may check the versions by typing the following commands in your terminal (powershell/wsl shell) to test the correct installation of both docker and docker compose once the docker desktop is installed and running. $ docker --version docker version 20.10.13, build a224086 $ docker-compose --version docker compose version v2.3.3 enter fullscreen mode exit fullscreen mode it's also possible to check docker's functioning by launching a sample docker container. $ docker run -d -p 80:80 --name myserver nginx enter fullscreen mode exit fullscreen mode vscode i'm sure every developer has installed vscode, so you just need to make sure you have the plugin remote development installed in your vscode. this enables you to develop in a container, on a remote machine, or in wsl. let’s go hadoop setup as you can see from the steps below, setting up a hadoop environment with docker is rather simple. clone the repo big-data-europe/docker-hadoop under a certain path, then setup the hadoop cluster via docker-compose . git clone git@github.com:big-data-europe/docker-hadoop.git cd docker-hadoop
docker-compose up -d enter fullscreen mode exit fullscreen mode now, you are all set :). after a few moments, you can check if it is working properly by visiting http://localhost:9870/ . get on the train it's finally time to meet your new hadoop cluster. start vs code, then go to the left panel and select the remote development plugin. select ""containers"" from the dropdown above, then locate and connect to a container named ""namenode"" by clicking ""attach to the container"" icon. you've arrived in the hadoop world! hello ""wordcount"" this is how we will test the hadoop cluster. we will run the word count example (from source code in java) to see how it works. but don't hurry up just yet. here are some more things to do. we need to make some sample input data. open the terminal in the vs code. then run: mkdir input echo ""hello world"" > input/f1.txt echo ""hello docker"" > input/f2.txt enter fullscreen mode exit fullscreen mode the inputs we have created are stored in your local (more precisely, in the docker container local). we also need to copy them to the hdfs. hadoop fs -mkdir -p input
hdfs dfs -put ./input/ * input enter fullscreen mode exit fullscreen mode after the preparation, you can get the official wordcount example from this link . import java.io.ioexception ; import java.util.stringtokenizer ; import org.apache.hadoop.conf.configuration ; import org.apache.hadoop.fs.path ; import org.apache.hadoop.io.intwritable ; import org.apache.hadoop.io.text ; import org.apache.hadoop.mapreduce.job ; import org.apache.hadoop.mapreduce.mapper ; import org.apache.hadoop.mapreduce.reducer ; import org.apache.hadoop.mapreduce.lib.input.fileinputformat ; import org.apache.hadoop.mapreduce.lib.output.fileoutputformat ; public class wordcount { public static class tokenizermapper extends mapper < object , text , text , intwritable >{ private final static intwritable one = new intwritable ( 1 ); private text word = new text (); public void map ( object key , text value , context context ) throws ioexception , interruptedexception { stringtokenizer itr = new stringtokenizer ( value . tostring ()); while ( itr . hasmoretokens ()) { word . set ( itr . nexttoken ()); context . write ( word , one ); } } } public static class intsumreducer extends reducer < text , intwritable , text , intwritable > { private intwritable result = new intwritable (); public void reduce ( text key , iterable < intwritable > values , context context ) throws ioexception , interruptedexception { int sum = 0 ; for ( intwritable val : values ) { sum += val . get (); } result . set ( sum ); context . write ( key , result ); } } public static void main ( string [] args ) throws exception { configuration conf = new configuration (); job job = job . getinstance ( conf , ""word count"" ); job . setjarbyclass ( wordcount . class ); job . setmapperclass ( tokenizermapper . class ); job . setcombinerclass ( intsumreducer . class ); job . setreducerclass ( intsumreducer . class ); job . setoutputkeyclass ( text . class ); job . setoutputvalueclass ( intwritable . class ); fileinputformat . addinputpath ( job , new path ( args [ 0 ])); fileoutputformat . setoutputpath ( job , new path ( args [ 1 ])); system . exit ( job . waitforcompletion ( true ) ? 0 : 1 ); } } enter fullscreen mode exit fullscreen mode save it with the filename wordcount.java . now, let's find out if it works. export hadoop_classpath = $java_home /lib/tools.jar
hadoop com.sun.tools.javac.main wordcount.java
jar cf wordcount.jar wordcount * .class
hadoop jar wordcount.jar wordcount input output enter fullscreen mode exit fullscreen mode it looks like there are a lot of logs, but what we care about is the output. print them out with cat command. $ hdfs dfs -cat output/part-r-0000 * docker 1
hello 2
world 1 enter fullscreen mode exit fullscreen mode hooray! we did it! clean-up this is simple. using this command will make your computer's life easier. docker-compose down enter fullscreen mode exit fullscreen mode acknowledgement this article significantly references this article by josé lise. i've added some content on how to develop with vscode. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse zishuo ding follow education carnegie mellon university silicon valley joined jul 11, 2021 trending on dev community hot rediscovering my passion: from burnout back to excitement # devjournal # developer # career # leadership what was your win this week?! # weeklyretro # discuss meme monday # discuss # watercooler # jokes 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/lighthouse-intelligence/why-we-dont-use-spark-4ihh,,,"why we don’t use spark - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse karel vanden bussche for lighthouse posted on sep 7, 2022 why we don’t use spark # python # spark # googlecloud # bigdata big data & spark most people working in big data know spark (if you don't, check out their website ) as the standard tool to extract, transform & load (etl) their heaps of data. spark, the successor of hadoop & mapreduce, works a lot like pandas, a data science package where you run operators over collections of data. these operators then return new data collections, which allows the chaining of operators in a functional way while keeping scalability in mind. for most data engineers, spark is the go-to when requiring massive scale due to the multi-language nature, the ease of distributed computing or the possibility to stream and batch. the many integrations with different persistent storages, infrastructure definitions and analytics tools make it a great solution for most companies. even though it has all these benefits, it is still not the holy grail. especially if your business is built upon crunching data 24/7. at ota insight, our critical view on infrastructure made us choose to go a different route, focused on our needs as a company, both from a technical, a people perspective and a long term vision angle. humble beginnings early on you have only 1 focus: building a product that solves a need and that people want to pay for, as quickly as possible. this means that spending money on things that accelerate you getting to this goal - is a good thing. in the context of this article this means: you don’t want to spend time managing your own servers, or fine-tuning your data pipeline’s efficiency. you want to focus on making it work. specifically, we heavily rely on managed services from our cloud provider, google cloud platform (gcp), for hosting our data in managed databases like bigtable and spanner. for data transformations, we initially heavily relied on dataproc - a managed service from google to manage a spark cluster. managing managed services our first implementation was a self-hosted spark setup, paired with a kafka service containing our job-queue. this had clear downsides and in hindsight we don’t consider it managed. a lot of side-developments had to be done to cover all edge-cases of the deployment & its scaling needs. things like networking, node failures and concurrency should be investigated, mitigated and modelled. this would have put a heavy strain on our development efficiency. secondly, pricing of running a full spark cluster with a 100% uptime was quite high and creating auto-scaling strategies for it was quite hard. our second implementation was migrated to use the same kafka event stream that streamed workload messages into the spark dataproc instances instead of the initially self-hosted spark instance. the kafka-dataproc combination served us well for some time, until gcp released its own message queue implementation: google cloud pub/sub . at the time, we investigated the value of switching. there is always an inherent switching cost, but what we had underestimated with kafka is that there is a substantial overhead in maintaining the system. this is especially true if the ingested data volume increases rapidly. as an example: the kafka service requires you to manually shard the data streams while a managed service like pub/sub does the (re)sharding behind the scenes. pub/sub on the other hand also had some downsides, e.g. it didn’t allow for longer-term data retention which can easily be worked around by storing the data on cloud storage after processing. persisting the data and keeping logs on the interesting messages made kafka obsolete for our use case. now, as we had no kafka service anymore, we found that using dataproc was also less effective when paired with pub/sub relative to the alternatives. after researching our options regarding our types of workloads, we chose to go a different route. it is not that dataproc was bad for our use cases, but there were some clear downsides to dataproc and some analysis taught us that there were better options. first, dataproc, at the time, had scaling issues as it was mainly focussed on batch jobs while our main pipelines were all running on streaming data. with the introduction of spark streaming, this issue was alleviated a bit, though not fully for our case. spark streaming still works in a (micro-)batched way under the hood, which is required to conform to the exactly-once delivery pattern. this gives issues for workloads that do not have uniform running times. our processors require fully real-time streaming, without exactly-once delivery, due to the idempotency of our services. secondly, the product was not very stable at the time, meaning we had to monitor quite closely what was happening and spent quite some time on alerts. lastly, most of our orchestration & scheduling was done by custom written components, making it hard to maintain and hard to update to newer versions. building for the future it was clear we needed something that was built specifically for our big-data saas requirements. dataflow was our first idea, as the service is fully managed, highly scalable, fairly reliable and has a unified model for streaming & batch workloads. sadly, the cost of this service was quite large. secondly, at that moment in time, the service only accepted java implementations, of which we had little knowledge within the team. this would have been a major bottleneck in developing new types of jobs, as we would either need to hire the right people, or apply the effort to dive deeper in java. finally, the data-point processing happens mainly in our api, making much of the benefits not weigh up against the disadvantages. small spoiler, we didn't choose dataflow as our main processor. we still use dataflow within the company currently, but for fairly specific and limited jobs that require very high scalability. none of the services we researched were an exact match, each service lacked certain properties. each service lacks something that is a hard requirement to scale the engineering effort with the pace the company is and keeps growing with. at this point, we reached product-market fit and were ready to invest in building the pipelines of the future. our requirements were mainly keeping the development efficiency high, keeping the structure open enough for new flows to be added, while also keeping the running costs low. as our core business is software, keeping an eye on how much resources this software burns through is definitely a necessity. taking into account the cost of running your software on servers can make the difference between a profit and a loss and this balance can change very quickly. we have processes in place to keep our bare-metal waste as low as possible without hindering new developments, which in turn gives us ways to optimise our bottomline. being good custodians of resources helps us keep our profit margins high on the software we provide. after investigating pricing of different services and machine types, we had a fairly good idea of how we could combine different services such that we had the perfect balance between maintainability and running costs. at this point, we made the decision to, for the majority of our pipelines, combine cloud pub/sub & kubernetes containers. sometimes, the best solution is the simplest. the reasoning behind using kubernetes was quite simple. kubernetes had been around a couple of years and had been used to host most of our backend microservices as well as frontend apps. as such, we had extensive knowledge on how to automate most of the manual management away from the engineers and into kubernetes and our ci/cd. secondly, as we already had other services using kubernetes, this knowledge was quickly transferable to the pipelines, which made for a unified landscape between our different workloads. the ease of scaling of kubernetes is its main selling point. pair this with the managed autoscaling the kubernetes engine gives and you have a match made in heaven. it might come as a surprise, but bare-metal kubernetes containers are quite cheap on most cloud platforms, especially if your nodes can be pre-emptible. as all our data was stored in persistent storages or in message queues in between pipeline steps, our workloads could be exited at any time and we would still keep our consistent state. combine the cost of kubernetes with the very low costs of pub/sub as a message bus and we have our winner. building around simplicity both kubernetes and pub/sub are quite barebones, without a lot of bells & whistles empowering developers. as such, we needed a simple framework to build new pipelines fast . we dedicated some engineering effort into building this pipeline framework to the right level of abstraction, where a pipeline had an input, a processor and an output. with this simple framework, we've been able to build the entire ota insight platform at a rapid scale, while not constricting ourselves to the boundaries of certain services or frameworks. secondly, as most of our product-level aggregations are done in our go apis, which are optimised for speed and concurrency, we can replace spark with our own business logic which is calculated on the fly. this helps us move fast within this business logic and helps keep our ingestion simple. the combination of both the framework and the aggregations in our apis, create an environment where spark becomes unnecessary and complexity of business logic is spread evenly across teams. summary during our growth path, from our initial spark environment (dataproc) to our own custom pipelines, we've learned a lot about costs, engineering effort, engineering experience and growth & scale limitations. spark is a great tool for many big data applications that deserves to be the most common name in data engineering, but we found it limiting in our day-to-day development as well as financials. secondly, it did not fit entirely in the architecture we envisioned for our business. currently, we know and own our pipelines like no framework could ever provide. this has led to rapid growth in new pipelines, new integrations and more data ingestion than ever before without having to lay awake at night pondering if this new integration would be one too many. all in all, we are glad we took the time to investigate the entire domain of services and we encourage others to be critical in choosing their infrastructure and aligning it with their business requirements, as it can make or break your software solutions, either now, or when scaling. want to know more? come talk to us ! top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse lighthouse follow bringing  travel &amp; hospitality  insights to light trending on dev community hot what was your win this week?! # weeklyretro # discuss kleos cli: mindsdb knowledge base supercharged # ai # python # programming # agentaichallenge 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/kinyungu_denis/apache-pyspark-for-data-engineering-3phi,,,"apache pyspark for data engineering - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse kinyungu denis posted on sep 9, 2022 apache pyspark for data engineering # beginners # python # dataengineering # sql greetings to my dear readers. i wrote an article about installing apache pyspark in ubuntu and explained about about apache spark. read it here . now lets go take a deep dive into pyspark and know what it is. this article covers about apache pyspark a tool that is used in data engineering, understand all details about pyspark and how to use it. one should have basic knowledge in python, sql to understand this article well. what is apache spark? let first understand about apache spark, then we proceed to pyspark. apache spark is an open-source, cluster computing framework which is used for processing, querying and analyzing big data. it lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data. what is apache pyspark? originally it was written in scala programming language, the open source community developed a tool to support python for apache spark called pyspark. pyspark provides py4j library, with the help of this library, python can be easily integrated with apache spark. it not only allows you to write spark applications using python apis, but also provides the pyspark shell for interactively analyzing vast data in a distributed environment. pyspark is a very demanding tool among data engineers. features of pyspark speed - pyspark allows us to achieve a high data processing speed, which is about 100 times faster in memory and 10 times faster on the disk. caching - pyspark framework provides powerful caching and good disk constancy. real-time - pyspark provides real-time computation on a large amount of data because it focuses on in-memory processing. it shows the low latency. deployment - we have local mode and cluster mode. in local mode it is a single machine fox example my laptop, convenient for testing and debugging. cluster mode there is set of predefined machines and its good for production. pyspark works well with resilient distributed datasets (rdds) running our cluster locally to start any spark application on a local cluster or a dataset, we use sparkconf to set some configuration and parameters. commonly used features of the sparkconf when working with pyspark: set(key, value)- setmastervalue(value) - setappname(value)- get(key,defaultvalue=none) - setsparkhome(value) - the following example shows some attributes of sparkconf: a spark program first creates a sparkcontext object which tells the application how to access a cluster. to accomplish the task, you need to implement sparkconf so that the sparkcontext object contains the configuration information about the application. sparkcontext sparkcontext is the first and essential thing that gets initiated when we run any spark application. it is an entry gate for any spark derived application or functionality. it is available as sc by default in pyspark. *know that creating any other variable instead of sc will give an error. inspecting our sparkcontext: master - the url of the cluster connects to spark. appname - the name of your task. the master and appname are the most widely used sparkcontext parameters. pyspark sql pyspark supports integrated relational processing with spark's functional programming. to extract the data by using an sql query language and use the queries same as the sql language. pyspark sql establishes the connection between the rdd and relational table.it supports wide range of data sources and algorithms in big-data. features of pyspark sql: incorporation with spark - pyspark sql queries are integrated with spark programs, queries are used inside the spark programs. developers do not have to manually manage state failure or keep the application in sync with batch jobs. consistence data access - pyspark sql supports a shared way to access a variety of data sources like parquet, json, avro, hive and jdbc. user-defined functions - pyspark sql has a language combined user-defined function (udfs). udf is used to define a new column-based function that extends the vocabulary of spark sql's dsl for transforming dataframe. hive compatibility - pyspark sql runs unmodified hive queries and allow full compatibility with current hive data. standard connectivity - it provides a connection through jdbc or odbc, the industry standards for connectivity for business intelligence tools. important classes of spark sql and dataframes are the following: pyspark.sql.sparksession: represents the main entry point for dataframe and sql functionality. pyspark.sql.dataframe: represents a distributed collection of data grouped into named columns. pyspark.sql.row: represents a row of data in a dataframe. pyspark.sql.column: represents a column expression in a dataframe. pyspark.sql.dataframestatfunctions: represents methods for statistics functionality. pyspark.sql.dataframenafunctions: represents methods for handling missing data (null values). pyspark.sql.groupeddata: aggregation methods, returned by dataframe.groupby(). pyspark.sql.types: represents a list of available data types. pysark.sql.functions: represents a list of built-in functions available for dataframe. pyspark.sql.window: used to work with window functions. import pyspark 
from pyspark.sql import sparksession   
spark = sparksession.builder.getorcreate() enter fullscreen mode exit fullscreen mode from pyspark.sql import sparksession enter fullscreen mode exit fullscreen mode a spark session can be used to create the dataset and dataframe api. a sparksession can also be used to create dataframe, register dataframe as a table, execute sql over tables, cache table, and read parquet file. class builder enter fullscreen mode exit fullscreen mode it is a builder of spark session. getorcreate() enter fullscreen mode exit fullscreen mode it is used to get an existing sparksession, or if there is no existing one, create a new one based on the options set in the builder. pyspark.sql.dataframe a distributed collection of data grouped into named columns. a dataframe is similar as the relational table in spark sql, can be created using various function in sqlcontext. then manipulate it using the several domain-specific-languages (dsl) which are pre-defined functions of dataframe. querying using pyspark sql this displays my file where sql queries are executed: the groupby() function collects the similar category data. pyspark udf the pyspark udf (user define function) is used to define a new column-based function. using user-defined functions (udfs), you can write functions in python and use them when writing spark sql queries. you can declare a user-defined function just like any other python function. the trick comes later when you register a python function with spark. to use functions in  pyspark, first register them through the spark.udf.register() function. it accepts two parameters: name - a string, function name you'll use in sql queries. f - a python function that contains the programming logic. spark.udf.register() enter fullscreen mode exit fullscreen mode py4jjavaerror, the most common exception while working with the udf. it comes from a mismatched data type between python and spark. an example of user define functions: pyspark rdd(resilient distributed dataset) resilient distributed datasets (rdds) are essential part of the pyspark, handles both structured and unstructured data and helps to perform in-memory computations on large cluster. rdd divides data into smaller parts based on a key. dividing data into smaller chunks helps that if one executor node fails, another node will still process the data. in-memory computation - computed results are stored in distributed memory (ram) instead of stable storage (disk) providing very fast computation immutability - the created data can be retrieved anytime but its value can't be changed. rdds can only be created through deterministic operations. fault tolerant - rdds track data lineage information to reconstruct lost data automatically. if failure occurs in any partition of rdds, then that partition can be re-computed from the original fault tolerant input dataset to create it. coarse-gained operation - coarse grained operation means that we can transform the whole dataset but not individual element on the dataset. on the other hand, fine grained mean we can transform individual element on the dataset. partitioning - rdds are the collection of various data items that are so huge in size, they cannot fit into a single node and must be partitioned across various nodes. persistence - optimization technique where we can save the result of rdd evaluation. it stores the intermediate result so that we can use it further if required and reduces the computation complexity. lazy evolution - it doesn't compute the result immediately means that execution does not start until an action is triggered. when we call some operation in rdd for transformation, it does not execute immediately. pyspark provides two methods to create rdds: loading an external dataset, or distributing a set of collection of objects. using parallelize create rdds using the parallelize() function which accepts an already existing collection in program and pass the same to the spark context. using createdataframe() function. we have already sparksession, so we will create our dataframe. external data read either one text file from hdfs, a local file system or any hadoop-supported file system uri with textfile() , or read in a directory of text files with wholetextfiles() . using read_csv() the output of using scores_file.show() and scores_file.printschema() rdd operations in pyspark rdd supports two types of operations: transformations - the process used to create a new rdd. it follows the principle of lazy evaluations (the execution will not start until an action is triggered). for example : map, flatmap, filter, distinct, reducebykey, mappartitions, sortby actions - the processes which are applied on an rdd to initiate apache spark to apply calculation and pass the result back to driver. for example : collect, collectasmap, reduce, countbykey/countbyvalue, take, first map() transformation takes in a function and applies it to each element in the rdd collect() action returns the entire elements in the rdd the rdd transformation filter() returns a new rdd containing only the elements that satisfy a particular function. it is useful for filtering large datasets based on a keyword. count() action returns the number of element available in rdd. rdd transformation reducebykey() operates on key, value (key,value) pairs and merges the values for each key. join() returns rdd with the matching keys with their values in paired form. dataframe from rdd pyspark provides two methods to convert a rdd to dataframe. these methods are: todf(), createdataframe(rdd, schema) dataframes also have two operations: transformations and actions. dataframe transformations include: select, filter, groupby, orderby, dropduplicates, withcolumnrenamed. select() - subsets the columns in a dataframe filter() - filters out rows based on a condition groupby() - used to group based on a column orderby() - sorts the dataframe based on one or more columns dropduplicates() - removes duplicate rows from a dataframe. withcolumnrenamed() - renames a columnn in the dataframe dataframe actions include: head, show, count, describe, columns. describe() - compute the summary statistics of numerical columns in a dataframe printschema() - prints the types of columns in a dataframe column - prints all the columns in dataframe. inspecting data in pyspark # print the first 10 observations
people_df.show(10)

# count the number of rows
print(""there are {} rows in the people_df dataframe."".format(people_df.count()))

# count the number of columns and their names
print(""there are {} columns in the people_df dataframe and their names are {}"".format(len(people_df.columns), people_df.columns)) enter fullscreen mode exit fullscreen mode pyspark dataframe sub-setting and cleaning # select name, sex and date of birth columns
people_df_sub = people_df.select('name', 'sex', 'date of birth')

# print the first 10 observations from people_df_sub
people_df_sub.show(10)

# remove duplicate entries from people_df_sub
people_df_sub_nodup = people_df_sub.dropduplicates()

# count the number of rows
print(""there were {} rows before removing duplicates, and {} rows after removing duplicates"".format(people_df_sub.count(), people_df_sub_nodup.count())) enter fullscreen mode exit fullscreen mode filtering your dataframe # filter people_df to select females
people_df_female = people_df.filter(people_df.sex == ""female"")

# filter people_df to select males
people_df_male = people_df.filter(people_df.sex == ""male"")

# count the number of rows
print(""there are {} rows in the people_df_female dataframe and {} rows in the people_df_male dataframe"".format(people_df_female.count(), people_df_male.count())) enter fullscreen mode exit fullscreen mode stopping sparkcontext to stop a sparkcontext: sc.stop() enter fullscreen mode exit fullscreen mode it's very crucial to understand resilient distributed datasets(rdds) and sql since they will be extensively used in data engineering. conclusion this article covers the key areas of apache pyspark you should understand as you learn to become data engineer.you should be able to initialize spark, use user defined functions (udfs), load data, work with rdds: apply actions and transformations. soon i will write an article on a practical use case of apache pyspark in a project. feel free to drop your comments about the article. top comments (4) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand sngvfx sngvfx sngvfx follow cloud - dataops - data engineering - web&app design and development joined feb 4, 2021 • oct 25 '22 dropdown menu copy link hide asante denis. good read! like comment: like comment: 1 like like comment button reply collapse expand kinyungu denis kinyungu denis kinyungu denis follow let us learn together. email deniskinyungu@gmail.com location kenya, nairobi joined may 11, 2022 • nov 2 '22 dropdown menu copy link hide you are welcome, i hope you learnt a new concept like comment: like comment: 2 likes like comment button reply collapse expand dendi handian dendi handian dendi handian follow data engineer - building data lake & modern data architecture email contact.dendi@yahoo.com location jakarta education b.s. computer science work data engineer at media production company joined oct 13, 2019 • oct 13 '22 dropdown menu copy link hide hello, kinyungu denis do you have end-to-end project resource using pyspark that we could follow and learn? because it's kind of boring if we only reading documentation only. like comment: like comment: 1 like like comment button reply collapse expand kinyungu denis kinyungu denis kinyungu denis follow let us learn together. email deniskinyungu@gmail.com location kenya, nairobi joined may 11, 2022 • nov 2 '22 dropdown menu copy link hide hello, i am working on an end-to-end project that one can follow through, kindly be patient. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse kinyungu denis follow let us learn together. location kenya, nairobi joined may 11, 2022 more from kinyungu denis how to create and use a virtual environment in python in ubuntu 22.04 # python # tutorial # beginners learn ansible and how to install it in ubuntu 22.04. # tutorial # install # dataengineering learning boto3 and aws services the right way in data engineering. # aws # python # beginners 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/kevinwallimann/how-to-recover-from-a-kafka-topic-reset-in-spark-structured-streaming-3phd,,,"how to recover from a kafka topic reset in spark structured streaming - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse kevin wallimann posted on jul 13, 2022 • edited on jul 21, 2022 how to recover from a kafka topic reset in spark structured streaming # kafka # spark kafka topics should not routinely be deleted and recreated or offsets reset. should it be necessary, care must be taken how and when to update the offsets in spark structured streaming's checkpoints, in order to avoid data loss. since such an offset reset happens outside of spark, the offsets in the checkpoints are obviously not automatically updated to reflect the change. this may cause unexpected behaviour, because the offsets are not in the expected range. when and how data loss may occur let's assume a spark structured streaming query with a once-trigger consumed 500 records from kafka topic test-topic . the checkpointed offset for microbatch 0 contains this {checkpoint-dir}/offsets/0: {""test-topic"":{""0"":500}} enter fullscreen mode exit fullscreen mode on the kafka topic, the beginning offset is 0 and the end offset is 500. so far, so good. new end offsets < checkpointed offset now, let's assume the kafka topic has been deleted and recreated and 300 messages produced, such that the new end offset on the topic is 300. if the spark query is restarted, it will fail with the following error message: java.lang.illegalstateexception: partition test-topic-0's offset was changed from 500 to 300, some data may have been missed.
some data may have been lost because they are not available in kafka any more; either the
 data was aged out by kafka or the topic may have been deleted before all the data in the
 topic was processed. if you don't want your streaming query to fail on such cases, set the
 source option ""failondataloss"" to ""false"". enter fullscreen mode exit fullscreen mode based on its checkpoint from batch 0, spark would have expected messages with offsets 500 and higher, but only found lower offsets. spark does not know if all messages have been consumed before the offset reset and by default assumes that messages could have been lost, failing with the above exception. what happens if the query is rerun with failondataloss=false ? this time, spark only prints a warning warn kafkamicrobatchreader: partition test-topic-0's offset was changed from 500 to 300, some data may have been missed.

some data may have been lost because they are not available in kafka any more; either the data was aged out by kafka or the topic may have been deleted before all the data in the topic was processed. if you want your streaming query to fail on such cases, set the source option ""failondataloss"" to ""true"". enter fullscreen mode exit fullscreen mode in the checkpoint folder, a new file for batchid 1 is created, which contains {checkpoint-dir}/offsets/1: {""test-topic"":{""0"":300}} enter fullscreen mode exit fullscreen mode but how many records were consumed? zero. 22/07/13 10:29:59 info microbatchexecution: streaming query made progress: {
  ""id"" : ""5762b467-2d58-4b62-937b-427f99b38659"",
  ""runid"" : ""fc98564a-f2f0-42be-91e2-6d1f97446372"",
  ""name"" : null,
  ""timestamp"" : ""2022-07-13t08:29:57.863z"",
  ""batchid"" : 1,
  ""numinputrows"" : 0,
  ""processedrowspersecond"" : 0.0,
  ""durationms"" : {
    ""addbatch"" : 709,
    ""getbatch"" : 45,
    ""queryplanning"" : 301,
    ""triggerexecution"" : 1372
  },
  ""stateoperators"" : [ ],
  ""sources"" : [ {
    ""description"" : ""kafkav2[subscribe[test]]"",
    ""startoffset"" : {
      ""test-topic"" : {
        ""0"" : 500
      }
    },
    ""endoffset"" : {
      ""test-topic"" : {
        ""0"" : 300
      }
    },
    ""numinputrows"" : 0,
    ""processedrowspersecond"" : 0.0
  } ],
  ""sink"" : {
    ""description"" : ""org.apache.spark.sql.kafka010.kafkasourceprovider@43bbf133""
  }
} enter fullscreen mode exit fullscreen mode as you can see from the log, spark takes 500 as the start offset for this microbatch. so it takes the offset of the previous checkpoint and not the current beginning offset of the topic-partition. from that point of view, it makes sense that no record is ingested, but messages 0-300 are lost nonetheless. new end offsets > checkpointed offset it's also possible that the new end offset is greater than the offset of the latest checkpoint. for example, let's assume that the new end offset is 800. in this case, the user probably expects to ingest the 800 new records. however, if the spark query is restarted, it will succeed, but only ingest the 300 records from offsets 500-800. the log may look like this 22/07/13 16:39:46 info microbatchexecution: streaming query made progress: {
  ""id"" : ""5762b467-2d58-4b62-937b-427f99b38659"",
  ""runid"" : ""1822bd34-269c-4df0-8dbe-b63e19df0e77"",
  ""timestamp"" : ""2022-07-13t14:39:43.074z"",
  ""batchid"" : 2,
  ""numinputrows"" : 300,
  ""processedrowspersecond"" : 96.32674030310815,
  ""durationms"" : {
    ""addbatch"" : 2747,
    ""getbatch"" : 7,
    ""getendoffset"" : 0,
    ""queryplanning"" : 315,
    ""setoffsetrange"" : 331,
    ""triggerexecution"" : 3892,
    ""walcommit"" : 213
  },
  ""stateoperators"" : [ ],
  ""sources"" : [ {
    ""description"" : ""kafkav2[subscribe[test]]"",
    ""startoffset"" : {
      ""test-topic"" : {
        ""0"" : 500
      }
    },
    ""endoffset"" : {
      ""test-topic"" : {
        ""0"" : 800
      }
    },
    ""numinputrows"" : 300,
    ""processedrowspersecond"" : 96.32674030310815
  } ],
  ""sink"" : {
    ""description"" : ""org.apache.spark.sql.kafka010.kafkasourceprovider@4554d7c4""
  }
} enter fullscreen mode exit fullscreen mode spark will not even print a warning, because from spark's perspective, it does not know that offsets have been reset, and it looks just like 300 new records have been added to the topic, without any offset reset happening in between. how to avoid data loss we have seen that in both cases, whether the new end offsets are smaller or larger than the checkpointed offset, data loss may occur. fundamentally, spark cannot distinguish between offsets before or after a recreation of a topic, so especially for the latter case, where the new end offset on the topic-partition is larger than the latest offset in the checkpoint, there is no general solution. one could try to manually modify the latest offset in the checkpoint or delete the checkpoints altogether, however this may require deleting the _spark_metadata folder in case of a file sink. a special case is if the new end offset is 0. in that case, there can be no data loss, because there is no new data on the topic yet. a possible strategy to perform a topic deletion and recreation could therefore be: make sure all data has been ingested from the topic. delete and recreate the topic. restart the spark structured streaming query that consumes from the topic. spark will write a new checkpoint with offset 0. only now start producing to the recreated topic. in the next microbatch, spark will consume from offset 0. conclusion the offsets in spark structured streaming's checkpoints are not automatically updated when a offset reset happens on a kafka topic. queries with once-triggers that are restarted periodically may be oblivious to an offset reset the best way to keep spark's offsets up-to-date is to restart the query before any new data has been published on the reset topic. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse kevin wallimann follow work big data engineer joined jul 11, 2020 more from kevin wallimann how to recover from a deleted _spark_metadata folder in spark structured streaming # spark is structured streaming exactly-once? well, it depends... # spark how to make a column non-nullable in spark structured streaming # spark 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/aws-builders/introduction-to-apache-spark-sparkql-and-spark-mlib-da1,,,"introduction to apache spark, sparkql, and spark mlib. - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse hridyesh bisht for aws community builders posted on jan 15, 2022 introduction to apache spark, sparkql, and spark mlib. # database # bigdata in this blog, i focus on apache spark , features and limitations of apache spark , architecture of apache spark,  architecture of sparkql, and architecture of spark mlib .  let's start by understanding what is apache spark, q.what is apache spark? apache spark is a unified computing engine and a set of libraries for parallel data processing on computer clusters. why  apache spark? fast processing: spark contains resilient distributed datasets (rdd) which saves time taken in reading and writing opertions and hence, hence it runs almost ten to hundred times faster than hadoop. in-memory computing: in spark, data is stored in the ram, so it can access the data quickly and accelerate the speed of analytics. flexible: spark supports multiple languages and allows the developers to write applications in java, scala, r or python. fault tolerance: spark contains resillent distributed datasets(rdd) that are designed to handle the failure of any worker node in the cluster. thus, it ensures that the loss of data reduces to zero. better analytics: spark has a rich set of sql queries, machine learning algorithms, complex analytics. with all these functionalities can be performed better. shortcoming of mapreduce: forces your data processing into map and reduce other workflows missing include join, filter, flatmap, groupbykey, union, intersection, … based on “acyclic data flow” from disk to disk (hdfs) read and write to disk before and after map and reduce (stateless machine) not efficient for iterative tasks, i.e. machine learning only java natively supported only for batch processing interactivity, streaming data how to does apache spark solve these shortcomings? capable of leveraging the hadoop ecosystem, e.g. hdfs, yarn, hbase, s3, … has many other workflows, i.e. join, filter, flatmapdistinct, groupbykey, reducebykey, sortbykey, collect, count, first… in-memory caching of data (for iterative, graph, and machine learning algorithms, etc.) native scala, java, python, and r support supports interactive shells for exploratory data analysis spark api is extremely simple to use credits: https://cdn.ttgtmedia.com/rms/onlineimages/business_analytics-hadoop_vs_spark-f_mobile.png architecture of spark spark is accessible, intense, powerful and proficient big data tool for handling different enormous information challenges. apache spark takes after an ace/slave engineering with two primary daemons and a cluster manager master daemon – (master/driver process) worker daemon – (slave process) credits: https://spark.apache.org/docs/latest/img/cluster-overview.png a spark cluster has a solitary master and many numbers of slaves/workers. the driver and the agents run their individual java procedures and users can execute them on individual machines. below are the three methods of building spark with hadoop components  - standalone - the arrangement implies spark possesses the place on the top of hdfs(hadoop distributed file system) and space is allotted for hdfs, unequivocally. here, spark and mapreduce will run one next to the other to covering all in the form of cluster. hadoop yarn - hadoop yarn arrangement implies, basically, spark keeps running on yarn with no pre-establishment or root get to required. it incorporates spark into the hadoop environment or the hadoop stack. it enables different parts to keep running on the top of the stack having an explicit allocation for hdfs. spark in mapreduce - spark in mapreduce is utilized to dispatch start work notwithstanding independent arrangement. with simr, the client can begin spark and uses its shell with no regulatory access. credits: https://databricks.com/wp-content/uploads/2014/01/sparkhadoop.png abstractions in spark architecture in this architecture, all the components and layers are loosely coupled. these components are integrated with several extensions as well as libraries. there are mainly two abstractions on which spark architecture is based. they are: resilient distributed datasets (rdd): these are the collection of object which is logically partitioned. it supports in-memory computation over spark cluster. spark rdds are immutable in nature. it supports hadoop datasets and parallelized collections. hadoop datasets are created from the files stored on hdfs. parallelized collections are based on existing scala collections. as rdds are immutable, it offers two operations transformations and actions. directed acyclic graph (dag): directed- graph which is directly connected from one node to another. this creates a sequence. acyclic – it defines that there is no cycle or loop available. graph – it is a combination of vertices and edges, with all the connections in a sequence we can call it a sequence of computations, performed on data. in this graph, edge refers to transformation on top of data. while vertices refer to an rdd partition. this helps to eliminate the hadoop mapreduce multistage execution model. it also provides efficient performance over hadoop. credits: https://hxquangnhat.files.wordpress.com/2015/03/scheduling.jpeg spark execution flow : application jar: user program and its dependencies except hadoop & spark jars bundled into a jar file. driver program: the process to start the execution (main() function) main process co-ordinated by the sparkcontext object. allows to configure any spark process wth specific parameters spark actions are executed in the driver cluster manager: an external service to manage resources on the cluster ( yarn) external services for acquring resources on the cluster variety of cluster managers such as local, standalone, and yarn deploy mode cluster: driver inside the cluster, framework launches the driver inside of the cluster. client: driver outside of cluster worker node: any node that run the application program in cluster. executor: a process launched for an application on a worker node that run tasks and keeps data in memory or disk storage across them. each application has its own executors. task: a unit of work that will be sent to executor job: a parallel computation consisting of multiple tasks that gets spawned in response to a spark action. stage: each job is divided into smaller set of tasks called stages that is sequential and depend on each other spark context: represents the connection to a spark cluster, and can be used to create rdds accumulator and broadcast variables on that cluster. main entry point for spark functionality represents the connection to a spark cluster tells spark how & where to access a cluster can be used to create rdds, accumulators and broadcast variables on that cluster. resillient distributed dataset  (rdd) resilient distributed dataset  (rdd) is a basic abstraction in spark. immutable, partitioned collection of elements that can be operated in parallel. main characteristics of rdd a list of partitions a function for computing each split a list of dependencies an other rdds optionally,a partioner for key-value rdds. a list of preferred locations to compute each split on. custom rdd can be also implemented (by overriding functions) credits: https://cdn2.hubspot.net/hubfs/5041972/imported_blog_media/fig-1-apache-spark-process-flow-4.png transformations: these are functions that accept the existing rdds as input and outputs one or more rdds. however, the data in the existing rd in spark does not change as it is immutable. these transformation are executed when they are invoked or called. every time transformation are applied, a new rdd is created. actions: these are functions that return the end result of rdd computations. it uses a lineage graph to load data onto the rdd in particular order. after all of the transformations are done, actions return the final result to the spark driver. actions are operations that provide non-rdd values. features of rdd rdd in apache spark is an immutable collection of objects which computes on the different node of the cluster. resilient, i.e. fault-tolerant with the help of rdd lineage graph(dag) and so able to recompute missing or damaged partitions due to node failures. distributed, since data resides on multiple nodes. dataset represents records of the data you work with. the user can load the data set externally which can be either json file, csv file, text file or database via jdbc with no specific data structure. each and every dataset in rdd is logically partitioned across many servers so that they can be computed on different nodes of the cluster. rdds are fault tolerant i.e. it posses self-recovery in the case of failure. dependencies of rdd narrow dependencies:  each parition of the parent rdd is used by at most one parition of the child rdd. task can be executed locally and we don't have to shuffle. pipelined execution efficient recovery no shuffling wide dependencies: multiple child partitions may depend on one partition of the parent rdd. his means we have to shuffle data unless the parents are hash-partitioned. requires shuffling unless parents are hash-partitioned expensive to recover credits: https://lh4.googleusercontent.com/proxy/ioyxdouny6j8kpljrx11bf_422ewcgwdpu4ghkizdhbnhadfqic8rmjvfogqkpvjkihouyannhfsfuf5pao4651xzwrvpl2gjobfjebezxmtftcooi-smwtweq4=w1200-h630-p-k-no-nu how to create a rdd? by loading an external dataset: you can load an external file onto an rdd. the types of you can load are csv, txt  , json, etc. here is the example of loading a text file onto an rdd. by paralleling the collection of objects: when spark's parallelize method is applied to a group of elements, a new distributed dataset is created. this dateset is an rdd. by performing transformation on the existing rdds: one or more rdds can be created by performing transformations on the existing rdds as mentioned earlier in this tutorial page. spark sql spark sql integrates relational processing with spark’s functional programming. it provides support for various data sources and makes it possible to weave sql queries with code transformations thus resulting in a very powerful tool. spark sql originated as apache hive to run on top of spark and is now integrated with the spark stack. apache hive had certain limitations spark sql was built to overcome these drawbacks and replace apache hive. spark sql is faster than hive when it comes to processing speed. spark sql is an apache spark module used for structured data processing, which: acts as a distributed sql query engine provides dataframes for programming abstraction allows to query structured data in spark programs can be used with platforms such as scala, java, r, and python. features of spark sql: integrated − seamlessly mix sql queries with spark programs. spark sql lets you query structured data as a distributed dataset (rdd) in spark, with integrated apis in python, scala and java. this tight integration makes it easy to run sql queries alongside complex analytic algorithms. unified data access − load and query data from a variety of sources. schema-rdds provide a single interface for efficiently working with structured data, including apache hive tables, parquet files and json files. hive compatibility − run unmodified hive queries on existing warehouses. spark sql reuses the hive frontend and metastore, giving you full compatibility with existing hive data, queries, and udfs. simply install it alongside hive. standard connectivity − connect through jdbc or odbc. spark sql includes a server mode with industry standard jdbc and odbc connectivity. scalability − use the same engine for both interactive and long queries. spark sql takes advantage of the rdd model to support mid-query fault tolerance, letting it scale to large jobs too. do not worry about using a different engine for historical data. limitations of a apache hive: hive uses mapreduce which lags in performance with medium and small sized datasets. no resume capability hive cannot drop encrypted database spark sql was built to overcome the limitations of apache hive running on top of spark. spark sql uses the metastore services of hive to query the data stored and manged by hive. credits: http://programmerprodigycode.files.wordpress.com/2022/01/9309e-18gptmzkg5riiktdb2zimyg.png spark sql architecture language api: spark is very compatible as it supports languages like python, scala and java. schema rdd: as spark sql works on schema, tables and records you can use schema rdd or dataframe as a temporary table. data sources: spark sql supports multiple data sources like json, cassandra database, hive tables. data source api is used to read and store structure and semi structured data into spark sql: structured/semi-structure data multiple formats 3 rd party integrations dataframe api converts the data that is read through data source api into tabular colmns to help perform sql operations. distributed collection of data organized into named columns equivalent to a relational table in sql lazily evaluated sql interpreter and optimised handles the functional programming part of spark sql. it transforms the data frames rdds to get the required results in the required formats. functional programming transforming trees faster than rdds processes all size data credits: https://www.edureka.co/blog/wp-content/uploads/2017/04/spark-sql-spark-tutorial-edureka.png sql service is the entry point for working along structured data in spark, and is used to fetch the result from the interpreted and optimised data. spark mlib mlib stands for machine learnign library in spark. the goal of this library is to make practical machine learning scalable and easy to implement. it contains fast and scalable implementations of standard machine learning algorithms. through spark mllib, data engineers and data scientists have access to different types of statistical analysis, linear algebra and various optimization primitives. spark machine learning library mllib contains the following applications collaborative filtering for recommendations – alternating least squares regression for predictions – logistic regression, lasso regression, ridge regression, linear regression and support vector machines (svm). clustering – linear discriminant analysis, k-mean and gaussian, classification algorithms – naïve bayes, ensemble methods, and decision trees. dimensionality reduction –pca (principal component analysis) and singular value decomposition (svd) benefits of spark mlib: spark mllib is tightly integrated on top of spark which eases the development of efficient large-scale machine learning algorithms as are usually iterative in nature. mllib is easy to deploy and does not require any pre-installation, if hadoop 2 cluster is already installed and running. spark mllib’s scalability, simplicity, and language compatibility helps data scientists solve iterative data problems faster. mllib provides ultimate performance gains (about 10 to 100 times faster than hadoop and apache mahout). credits: https://spark.apache.org/docs/latest/img/ml-pipelinemodel.png features of spark mllib library : mllib provides algorithmic optimisations for accurate predictions and efficient distributed learning. for instance, the alternating least squares machine learning algorithms for making recommendations effectively uses blocking to reduce jvm garbage collection overhead. mllib benefits from its tight integration with various spark components. mllib provides a package called spark.ml to simplify the development and performance tuning of multi-stage machine learning pipelines. mllib provides high-level api’s which help data scientists swap out a standard learning approach instead of using their own specialized machine learning algorithms. mllib provides fast and distributed implementations of common machine learning algorithms along with a number of low-level primitives and various utilities for statistical analysis, feature extraction, convex optimizations, and distributed linear algebra. spark mllib library has extensive documentation which describes all the supported utilities and methods with several spark machine learning example codes and the api docs for all the supported languages. spark mlib tools: ml algorithms: classfication, regrssion, clustering and collaborative filtering featurization: feature extraction, transformation, dimensionality reduction, and selection pipelines: tools for constructing, evaluating and tunnignml pipelines persistence: saving and loading algorithms, models and pipelines utilities: linear algebra, statistics, data handling credits: https://docs.microsoft.com/en-us/sql/big-data-cluster/media/big-data-cluster-overview/ai-ml-spark.png?view=sql-server-ver15 machine learning pipelines componenets : data frame: the ml api uses dataframe from spark sql as a dataset, which can be used to hold a variety of datatypes transformer: this is used to transform one dataframe to another dataframe. examples are hashing term frequency: this calculates how wordoccurs logistic regression model: the model which resultsfrom trying logistic regressions on a dataset binarizer: this changes a given threshold value to 1or 0 estimator: it is an algorithm which can be used on a dataframe to produce transformer. examples are: logistic regression: it is used to determine the weights for the resulting logistic regression model by processing the dataframe standard scaler: it is used to calculate the standard deviation pipeline: calling fit on a pipeline produces pipeline model, and the pipeline contains only transformers and not the estimators pipeline: a pipeline chains multiple transformers and estimators together to specify the ml workflow parameters: to specify the parameters a common api is used by the transformers and estimators for more information about hadoop and hdfs,  hive https://programmerprodigy.code.blog/2021/06/28/introduction-to-hadoophdfs-and-hbase/ https://programmerprodigy.code.blog/2022/01/08/hive-sql-layer-above-hadoop/ top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse aws community builders follow build on! would you like to become an aws community builder? learn more about the program and apply to join when applications are open next. learn more more from aws community builders connecting to a private rds instance via an ec2 proxy # aws # security # database do devops need to know databases? absolutely! mongodb, postgresql, mysql, and redis # devops # aws # database how to crater your database, part one - an introduction # serverless # architecture # database # dynamodb 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/aws-builders/spark-as-function-containerize-pyspark-code-for-aws-lambda-and-amazon-kubernetes-1bka,,,"spark as function - containerize pyspark code for aws lambda and amazon kubernetes - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse prasanth mathesh for aws community builders posted on sep 29, 2021 • edited on nov 1, 2021 spark as function - containerize pyspark code for aws lambda and amazon kubernetes # kubernetes # aws # serverless # docker introduction the python, java etc applications can be containerized as a docker image for deployment in aws lambda and aws eks using the aws ecr as container registry. the spark framework commonly used for distributed big data processing applications supports various deployment modes like local, cluster, yarn, etc. i have discussed serverless data processing architecture patterns in my other articles and in this, we will see how one can build and run a spark data processing application using aws eks and also serverless  lambda runtime. the working version code used for this article is kept in github requirements the following are the set of client tools that should be already installed in the working dev environment. aws cli, kubectl, eksctl, docker one should ensure the right version for each set of tools including the spark, aws sdk and delta.io dependencies. kubernetes deployment aws eks anywhere which was launched recently can enable organizations to create and operate kubernetes clusters on customer-managed infrastructure. this new service by aws is going to change the way of scalability, disaster plan and recovery option that are being followed for kubernetes currently. the following are the few native kubernetes deployments since containerized applications will run in the same manner in different hosts. 1.build and test application on-premise and deploy on the cloud for availability and scalability 2.build, test and run applications on-premise and use the cloud environment for disaster recovery 3.build, test and run application on-premise, burst salves on the cloud for on-demand scaling 4.build and test application on-premise and deploy master on a primary cloud and create slaves on secondary cloud for ever-growing,data-intensive applications that process and store terabytes of data, rpo is critical and its better to use on-premise dev and cloud for prod spark on server local first, let's containerize the application and test it in the local environment. the pyspark code used in this article reads a s3 csv file and writes it into a delta table in append mode. after the write operation is complete, spark code displays the delta table records. build the image with dependencies and push the docker image to aws ecr using the below command. ./build_and_push.sh cda-spark-kubernetes enter fullscreen mode exit fullscreen mode after the build, the docker image is available in the local dev host too which can be tested locally using docker cli docker run cda-spark-kubernetes driver local:///opt/application/cda_spark_kubernetes.py {args} enter fullscreen mode exit fullscreen mode the above image shows the output of the delta read operation aws eks build aws eks cluster using eksctl.yaml and apply rbac role for spark user using below cli. eksctl create cluster -f ./eksctl.yaml
kubectl apply -f ./spark-rbac.yaml enter fullscreen mode exit fullscreen mode once the cluster is cluster is created, verify nodes and cluster ip. the above is a plain cluster that is ready without any application and its dependencies. install spark-operator spark operator is an open-source kubernetes operator to deploy spark applications. helm is similar to yum, apt for k8s and using helm, spark operator can be installed. install spark-operator using below helm cli helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator
helm install spark-operator spark-operator/spark-operator --set webhook.enable=true
kubectl get pods enter fullscreen mode exit fullscreen mode the containerized spark code can be submitted from a client in cluster mode using spark operator and its status can be checked using kubectl cli. run the spark-job.yaml that contains config parameters required for the spark operator in the command line. kubectl apply -f ./spark-job.yaml enter fullscreen mode exit fullscreen mode the cli to get application is given below kubectl get sparkapplication enter fullscreen mode exit fullscreen mode the cli to get logs of the spark driver at the client side is given below. kubectl logs spark-job-driver enter fullscreen mode exit fullscreen mode the delta operation has done the append to the delta table and it's displayed on driver logs as given below. additionally, the driver spark-ui can be forwarded to the localhost port too. the kubernetes deployment requests driver and executor pods on demand and shuts them down once processing is complete. this pod level resource sharing and isolation is a key difference between spark on yarn and kubernetes spark on serverless spark is a distributed data processing framework that thrives on ram and cpu. spark on aws lambda function is suitable for all kinds of workload that can complete within 15 mins. for the workloads that take more than 15 mins, by leveraging continuous/event-driven pipelines with proper cdc, partition and storage techniques, the same code can be run in parallel to achieve the latency of the data pipeline. the base spark image used for aws eks deployment is taken from the docker hub and  it is pre-built with aws sdk and delta.io dependencies. for aws lambda deployment, aws supported python base image is used to build code along with its dependencies using the dockerfile and then it is pushed to the aws ecr. from public.ecr.aws/lambda/python:3.8

arg hadoop_version=3.2.0
arg aws_sdk_version=1.11.375

run yum -y install java-1.8.0-openjdk

run pip install pyspark

env spark_home=""/var/lang/lib/python3.8/site-packages/pyspark""
env path=$path:$spark_home/bin
env path=$path:$spark_home/sbin
env pythonpath=$spark_home/python:$spark_home/python/lib/py4j-0.10.9-src.zip:$pythonpath
env path=$spark_home/python:$path

run mkdir $spark_home/conf

run echo ""spark_local_ip=127.0.0.1"" > $spark_home/conf/spark-env.sh

#env pyspark_submit_args=""--master local pyspark-shell""
env java_home=""/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.amzn2.0.1.x86_64/jre""
env path=${path}:${java_home}/bin

# set up the env vars for code
env aws_access_key_id=""""
env aws_secret_access_key=""""
env aws_region=""""
env aws_session_token=""""
env s3_bucket=""""
env inp_prefix=""""
env out_prefix=""""

run yum install wget
# copy hadoop-aws and aws-sdk
run wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${hadoop_version}/hadoop-aws-${hadoop_version}.jar -p ${spark_home}/jars/ && \ 
    wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${aws_sdk_version}/aws-java-sdk-bundle-${aws_sdk_version}.jar -p ${spark_home}jars/

copy spark-class $spark_home/bin/
copy delta-core_2.12-0.8.0.jar ${spark_home}/jars/
copy cda_spark_lambda.py ${lambda_task_root}

cmd [ ""cda_spark_lambda.lambda_handler"" ] enter fullscreen mode exit fullscreen mode local test the code using a local machine using docker cli as given below. docker run -e s3_bucket=referencedata01 -e inp_prefix=delta/input/students.csv -e out_prefix=/delta/output/students_table -e aws_region=ap-south-1 -e aws_access_key_id=$(aws configure get default.aws_access_key_id) -e aws_secret_access_key=$(aws configure get default.aws_secret_access_key) -e aws_session_token=$(aws configure get default.aws_session_token) -p 9000:8080 kite-collect-data-hist:latest cda-spark-lambda enter fullscreen mode exit fullscreen mode the local mode testing will require an event to be triggered and aws lambda will be in wait mode. trigger an event for lambda function using below cli in another terminal curl -xpost ""http://localhost:9000/2015-03-31/functions/function/invocations"" -d '{}' enter fullscreen mode exit fullscreen mode once aws lambda is completed, we can see the output as given below in the local machine. aws lambda deploy a lambda function using the ecr image and set necessary env variables for the lambda handler. once lambda is triggered and completed successfully we can see the logs in cloud watch. aws lambda currently supports 6 vcpu cores and 10 gb memory and it is billed for the elapsed run time and memory consumption as shown below. the aws pricing is based on the number of requests and gb-sec. the same code is run for various configurations and it is evident from the below table that even if memory is overprovisioned, aws lambda pricing methodology saves the cost. conclusion going forward, wider adoption to use containerized data pipelines for spark will be the need of the hour since sources like web apps, saas products that are built on top of kubernetes generates a lot of data in a continuous manner  for the big data platforms. the most common operations like data extraction and ingestion in the s3 data lake, loading processed data into the data stores and pushing down sql workloads on aws redshift can be done easily using aws lambda spark. thus by leveraging aws lambda along with kubernetes, one can bring down tco along with  build planet-scale data pipelines. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse aws community builders follow build on! would you like to become an aws community builder? learn more about the program and apply to join when applications are open next. learn more more from aws community builders amazon emr on eks now supports service quotas # aws # eks # cloud filter and normalize data using aws glue databrew # aws # awsgluedatabrew # s3bucket # recipe generative ai meets edge: deploying foundation models with aws iot greengrass # aws # rag # ai # genai 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/siddhantpatro/spark-mllib-for-big-data-and-machine-learning-330j,,,"spark mllib for big data and machine learning - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse d siddhant patro posted on feb 9, 2021 spark mllib for big data and machine learning # bigdata # machinelearning # apachespark in this world, full of data, there’s a good chance that you might know what big data and apache spark is. if you don’t, that’s ok! i’ll tell you what it is but before knowing about big data and spark, you need to understand, what is data . data :- the quantities, characters, or symbols containing some kind of information on which operations are performed by a computer, which may be stored and transmitted in the form of electrical signals and recorded on magnetic, optical, or mechanical recording media. since you all got an idea about what data is, now it will be easy for you to understand what big data is. big data :- it is a collection of data that is huge in volume and having more complexity, especially obtained from new data sources, and it is growing exponentially with time. these data sets are so voluminous that traditional data processing software just can’t manage them. it consists of 3 types of data, they are structured , semi-structured and unstructured . machine learning :- it is an application of artificial intelligence (ai) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. apache spark :- with an immense amount of data, we need a tool to digest it and the tool is apache spark . it is a fast, unified computing and open source data-processing engine for parallel data processing on computer clusters. it is designed to deliver the computational speed and scalability required for big data — specifically for streaming data, graph data, machine learning applications. spark provides an unified data processing engine known as the spark stack . this stack is built on top of a strong foundation called spark core , which provides all the necessary functionalities to manage and run distributed applications such as scheduling, coordination, and fault tolerance. available libraries of spark are spark sql , spark streaming , graphx , spark mllib and spark r . spark sql is for batch as well as interactive data processing. spark streaming is for real-time stream data processing. spark graphx is for graph processing. spark mllib is for machine learning. spark r is for running machine learning tasks using the r shell. spark mllib is nothing but a library that helps in managing and simplifying many of the machine learning models for building tasks, such as featurization, pipeline for constructing, evaluating and tuning of the model. machine learning algorithms are iterative in nature, meaning they run through many iterations until a desired objective is achieved. spark makes it extremely easy to implement those algorithms and run them in a scalable manner through a cluster of machines. spark mllib tools are given below:- ml algorithms featurization pipelines model tuning persistence ml algorithms :- ml algorithms form the core of mllib. these include common learning algorithms such as classification, regression, clustering, and collaborative filtering. mllib standardizes apis to make it easier to combine multiple algorithms into a single pipeline. featurization :- featurization includes feature extraction, transformation, dimensionality reduction, and selection. feature extraction is extracting features from raw data. feature transformation includes scaling, and modifying features feature selection involves selecting a subset of necessary features from a huge set of features. pipelines :- in machine learning, it is common to run a sequence of steps to clean and transform data, then train one or more ml algorithms to learn from the data. mllib has a class called pipeline, which consists of a sequence of pipeline stages (transformers and estimators) to be run in a specific order. model tuning :- the goal of the model tuning is to train a model with the right set of parameters to achieve the best performance to meet the object defined in the first step of the ml development process. persistence :- persistence helps in saving and loading ml algorithms, models, and pipelines. this helps in reducing time and efforts as the model is persistence, it can be loaded or reused any time when needed. the above are the tools via which one can learn to use machine learning algorithms on apache spark framework for better and faster processing of massive and voluminous data. conclusion :- in the python world, scikit-learn is one of the most popular open source machine learning libraries. it provides a set of supervised and unsupervised learning algorithms. it is designed to be simple and efficient and therefore, it is a perfect tool to learn and practice machine learning on a single machine. but the moment the size of the data exceeds the storage capacity of a single machine, that’s when it is time to switch to spark mllib. thank you. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse d siddhant patro follow deep learning enthusiast location bhubaneswar, odisha education college of engineering and technology joined may 29, 2020 trending on dev community hot where is the vibe in ""vibe coding"" and what happened to music monday? # watercooler # ai # webdev # productivity human learning is dead—long live human meaning # ai # learning # machinelearning # computerscience to index or not to index: which coding agent to choose? # javascript # discuss # devops # ai 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/kevinwallimann/how-to-recover-from-a-deleted-sparkmetadata-folder-546j,,,"how to recover from a deleted _spark_metadata folder in spark structured streaming
 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse kevin wallimann posted on mar 11, 2021 how to recover from a deleted _spark_metadata folder in spark structured streaming # spark warning: the described procedures have been tested on spark 2.4.3 and 3.0.1, but otherwise not on all possible environments. be mindful of what you're doing on your system. having said that, i'd be grateful for any feedback if you find caveats. introduction spark structured streaming guarantees exactly-once processing for file outputs. one element to maintain that guarantee is a folder called _spark_metadata which is located in the output folder. the folder _spark_metadata is also known as the ""metadata log"" and its files ""metadata log files"". it may look like this: /tmp/destination/_spark_metadata/0
/tmp/destination/_spark_metadata/1
/tmp/destination/_spark_metadata/2
/tmp/destination/_spark_metadata/3 enter fullscreen mode exit fullscreen mode a metadata log file may look like this: v1
{""path"":""file:///tmp/destination/part-00000-5ee05bb5-3c65-4028-9c9e-dbc99f5fdbca.c000.snappy.parquet"",""size"":3919,""isdir"":false,""modificationtime"":1615462080000,""blockreplication"":1,""blocksize"":33554432,""action"":""add""} enter fullscreen mode exit fullscreen mode when spark writes a file to the output folder, it writes the absolute path of the added file to the metadata log file of the current micro-batch. if a partial write occurs, that filename will not be added to the metadata log, and that's how spark can maintain exactly-once semantics. when spark reads a file from the output folder, it only reads from files that are referenced in the metadata log. at least that's the idea. for more details on that topic, see https://dev.to/kevinwallimann/is-structured-streaming-exactly-once-well-it-depends-noe deleting the _spark_metadata folder i hope it's clear by now that this folder should not be deleted. it should not be deleted! anyway, let's see what happens if we delete it nonetheless. for this scenario, let's assume we have a structured streaming query, writing to a folder called /tmp/destination and a checkpoint folder called /tmp/checkpoint-location . after two micro-batches, the folder structure for the checkpoint-folder and the _spark_metadata folder looks like this: /tmp/checkpoint-location/commits
/tmp/checkpoint-location/commits/0
/tmp/checkpoint-location/commits/1
/tmp/checkpoint-location/metadata
/tmp/checkpoint-location/offsets
/tmp/checkpoint-location/offsets/0
/tmp/checkpoint-location/offsets/1
/tmp/checkpoint-location/sources
/tmp/checkpoint-location/sources/0
/tmp/checkpoint-location/sources/0/0

/tmp/destination/_spark_metadata/0
/tmp/destination/_spark_metadata/1 enter fullscreen mode exit fullscreen mode now for some reason, the _spark_metadata folder in the destination is deleted or moved, but not the corresponding checkpoints folder. the following exception will be thrown sooner or later: caused by: java.lang.illegalstateexception: /tmp/destination/_spark_metadata/0 doesn't exist when compacting batch 9 (compactinterval: 10)
    at org.apache.spark.sql.execution.streaming.compactiblefilestreamlog.$anonfun$compact$3(compactiblefilestreamlog.scala:187)
    at scala.option.getorelse(option.scala:189)
    at org.apache.spark.sql.execution.streaming.compactiblefilestreamlog.$anonfun$compact$2(compactiblefilestreamlog.scala:185)
    at org.apache.spark.sql.execution.streaming.compactiblefilestreamlog.$anonfun$compact$2$adapted(compactiblefilestreamlog.scala:183)
    at scala.collection.traversablelike.$anonfun$flatmap$1(traversablelike.scala:245)
    at scala.collection.immutable.numericrange.foreach(numericrange.scala:74)
    at scala.collection.traversablelike.flatmap(traversablelike.scala:245)
    at scala.collection.traversablelike.flatmap$(traversablelike.scala:242)
    at scala.collection.abstracttraversable.flatmap(traversable.scala:108)
    at org.apache.spark.sql.execution.streaming.compactiblefilestreamlog.$anonfun$compact$1(compactiblefilestreamlog.scala:183)
    at org.apache.spark.util.utils$.timetakenms(utils.scala:561)
    at org.apache.spark.sql.execution.streaming.compactiblefilestreamlog.compact(compactiblefilestreamlog.scala:181)
    at org.apache.spark.sql.execution.streaming.compactiblefilestreamlog.add(compactiblefilestreamlog.scala:156)
    at org.apache.spark.sql.execution.streaming.manifestfilecommitprotocol.commitjob(manifestfilecommitprotocol.scala:75)
    at org.apache.spark.sql.execution.datasources.fileformatwriter$.write(fileformatwriter.scala:215) enter fullscreen mode exit fullscreen mode looking at the checkpoint folder, we see the following files /tmp/checkpoint-location/commits
/tmp/checkpoint-location/commits/0
/tmp/checkpoint-location/commits/1
/tmp/checkpoint-location/commits/2
/tmp/checkpoint-location/commits/3
/tmp/checkpoint-location/commits/4
/tmp/checkpoint-location/commits/5
/tmp/checkpoint-location/commits/6
/tmp/checkpoint-location/commits/7
/tmp/checkpoint-location/commits/8
/tmp/checkpoint-location/metadata
/tmp/checkpoint-location/offsets
/tmp/checkpoint-location/offsets/0
/tmp/checkpoint-location/offsets/1
/tmp/checkpoint-location/offsets/2
/tmp/checkpoint-location/offsets/3
/tmp/checkpoint-location/offsets/4
/tmp/checkpoint-location/offsets/5
/tmp/checkpoint-location/offsets/6
/tmp/checkpoint-location/offsets/7
/tmp/checkpoint-location/offsets/8
/tmp/checkpoint-location/offsets/9
/tmp/checkpoint-location/sources
/tmp/checkpoint-location/sources/0
/tmp/checkpoint-location/sources/0/0 enter fullscreen mode exit fullscreen mode meanwhile, the destination folder contains /tmp/destination/_spark_metadata/2
/tmp/destination/_spark_metadata/3
/tmp/destination/_spark_metadata/4
/tmp/destination/_spark_metadata/5
/tmp/destination/_spark_metadata/6
/tmp/destination/_spark_metadata/7
/tmp/destination/_spark_metadata/8 enter fullscreen mode exit fullscreen mode as we can see, the _spark_metadata folder is missing the files 0 and 1 , that were previously deleted. instead of simply writing /tmp/destination/_spark_metadata/9 , spark tries to concatenate the files 0 , 1 , ..., 8 to a file called 9.compact to improve reading efficiency and to avoid the small files problem. this process is called log compaction. that's when the exception is thrown because the files 0 and 1 unexpectedly don't exist. log compaction doesn't happen in every micro-batch, but the frequency is determined by the compactinterval which is 10 by default. how to fix the problem 1. restore the files of the removed _spark_metadata folder if the deleted _spark_metadata folder has only been moved and can be restored, its files should be restored. the files of the deleted _spark_metadata folder should be moved into the new _spark_metadata folder. there should be no overlapping filenames. after restoring the files, the _spark_metadata folder should look like this /tmp/destination/_spark_metadata/0
/tmp/destination/_spark_metadata/1
/tmp/destination/_spark_metadata/2
/tmp/destination/_spark_metadata/3
/tmp/destination/_spark_metadata/4
/tmp/destination/_spark_metadata/5
/tmp/destination/_spark_metadata/6
/tmp/destination/_spark_metadata/7
/tmp/destination/_spark_metadata/8 enter fullscreen mode exit fullscreen mode now, the query can be restarted and should finish without errors. 2. create dummy log files if the metadata log files are irrecoverable, we could create dummy log files for the missing micro-batches. in our example, this could be done like this for i in { 0..1 } ; do echo v1 > ""/tmp/destination/_spark_metadata/ $i "" ; done enter fullscreen mode exit fullscreen mode or on hdfs for i in { 0..1 } ; do echo v1 > ""/tmp/ $i "" ; hdfs dfs -copyfromlocal ""/tmp/ $i "" ""/tmp/destination/_spark_metadata/ $i "" ; done enter fullscreen mode exit fullscreen mode this will create the files /tmp/destination/_spark_metadata/0
/tmp/destination/_spark_metadata/1 enter fullscreen mode exit fullscreen mode now, the query can be restarted and should finish without errors. note that the information from the metadata log files 0 and 1 will definitely be lost, hence the exactly-once guarantee is lost for micro-batches 0 and 1, and you need to address this problem separately, but at least the query can continue. 3. deferring compaction if it's the middle of the night and you simply need that query to continue, or you have no write access to the filesystem, you can buy yourself some time by deferring the compaction. however, this solution does not solve the root cause. by default, the compactinterval is 10. you can increase it to e.g. 100 by restarting the query with this additional config spark-submit --conf spark.sql.streaming.filesink.log.compactinterval = 100 enter fullscreen mode exit fullscreen mode the same exception will be thrown in 100 micro-batches, so this is really just a very temporary fix to keep the query running for a few more micro-batches. eventually, the missing log files have to be recreated. top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand ashish ashish ashish follow joined apr 6, 2021 • apr 6 '21 • edited on apr 6 • edited dropdown menu copy link hide great article.. two cents i would like to add: in any of the methods mentioned here, it only removes/defers the error for the spark producer job (one writing data on s3). but any consumer job who want to read the data already written on s3, will still face one of the issues mentioned below: 1. if you create the blank 0 file error:
exception in thread ""main"" java.lang.illegalstateexception: failed to read log file /spark-warehouse/_spark_metadata/0. incomplete log file enter fullscreen mode exit fullscreen mode 2. if you don't create the blank file: a. if only 1 batch was present
error: exception in thread ""main"" org.apache.spark.sql.analysisexception: unable to infer schema for parquet at . it must be specified manually
b. if multiple batches were present and you deleted only 1 metadata file
error: exception in thread ""main"" java.lang.illegalstateexception: /documents/spark-warehouse/_spark_metadata/0 doesn't exist (latestid: 1, compactinterval: 10) enter fullscreen mode exit fullscreen mode like comment: like comment: 2 likes like comment button reply collapse expand kevin wallimann kevin wallimann kevin wallimann follow work big data engineer joined jul 11, 2020 • may 12 '21 dropdown menu copy link hide hi @gupash thanks for your comment. indeed if you create a blank 0 file, it will throw the error that you posted. however, the dummy log file that i described in the article contains the string ""v1"". in that case, no error should be thrown on the reader's side. maybe i could have pointed out this fact more clearly. like comment: like comment: 1 like like comment button reply collapse expand jmagana2000 jmagana2000 jmagana2000 follow joined jun 14, 2022 • jun 14 '22 dropdown menu copy link hide i was missing files 0 through 5 and i just copied 6 and renamed to 0 to 5 and that worked. like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse kevin wallimann follow work big data engineer joined jul 11, 2020 more from kevin wallimann how to recover from a kafka topic reset in spark structured streaming # kafka # spark is structured streaming exactly-once? well, it depends... # spark how to make a column non-nullable in spark structured streaming # spark 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/adevintaspain/spark-unit-integration-and-end-to-end-tests-f52,,,"spark: unit, integration and end-to-end tests. - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse gustavo martin morcuende for adevinta spain posted on oct 15, 2020 spark: unit, integration and end-to-end tests. # scala # spark # testing introduction at adevinta spain we are building a data platform where we use multiple applications and frameworks some of them based on spark. in order to increase the quality of our spark applications we wanted to run tests in the same way as we did with other frameworks. what means, we wanted to be able to run unit, integration and end-to-end tests. this article explains the way spark tests are run at adevinta spain. hopefully, it will be useful for other big data developers searching ways to improve the quality of their code and at the same time their ci pipelines. unit, integration and end-to-end tests. when working with spark, developers usually will be facing the need of implementing these kinds of tests. other tests like smoke tests, acceptance tests, etc, etc are outside the scope of this article so i will not be mentioning them. unit tests: at this level we will be dealing with code that does not require a spark session in order to work. also, this kind of code does not talk with the outside world. integration tests: at some point we will need to use a spark session. at this level we will be testing spark transformations and in many cases we will have to deal with external systems such as databases, kafka clusters, etc, etc. end-to-end tests: our application probably will be composed of several spark transformations working together in order to implement some feature required by some user. here, we will be testing the whole application. spark project layout this is a typical scala project layout. i think this layout should work under any use case but if it does not work for you, at least i hope, it will bring some inspiration or ideas to your testing implementation. src/
├── main
│   └── scala
│       └── example
│           ├── app
│           │   └── awesomeapp.scala
│           ├── job
│           │   └── awesomejob.scala
│           └── service
│               └── awesomeservice.scala
└── test
    ├── resources
    │   ├── awesomejob
    │   │   └── sourcepath
    │   │       └── awesome.json
    │   └── log4j.properties
    └── scala
        └── example
            ├── app
            │   └── awesomeappendtoendtest.scala
            ├── job
            │   └── awesomejobintegrationtest.scala
            ├── service
            │   └── awesomeservicetest.scala
            └── sharedsparksessionhelper.scala enter fullscreen mode exit fullscreen mode application layout app package under this package we will find the classes in charge of running our spark applications. typically we will have only one spark application. job package a spark application should implement some kind of transformations. modules under this package run spark jobs that require a spark session. service package sometimes business logic does not require a spark session in order to work. in such cases, we can implement the logic in a different module. shared spark session one of the biggest problems to be solved when running spark tests is the isolation of these tests. running a test should not affect the results of another. in order to achieve this goal we are going to need a spark session for each set of tests, in this way, the results of these tests will not affect others that will also require a spark session. so, we need to implement a system that will enable us to run, clear and stop a spark session whenever we need it (before and after a set of related spark tests) the details of the implementation are explained down below: beforeall: beforeall is a scala test function that runs before any other test in our class under test. we will be using this function for starting our spark session. sparkconf: sparkconf function enables us to load different spark sessions with different spark configurations. embedded hive: spark-warehouse and metastore_db are folders used by spark when enabling the hive support. different spark sessions in the same process can not use the same folders. because of that, we need to create random folders in every spark session. beforeeach : scala test function that creates a temporary path which is useful when our spark tests end up writing results in some location. aftereach : clears and resets the spark session at the end of every test. also, it removes the temporary path. afterall : stops the current spark session after the set of tests are run. in this way we will be able to run a new spark session if it is needed (if there is another set of tests requiring the use of spark) how it works the basic idea behind sharedsparksessionhelper lies in the fact that there is one spark session per java process and it is stored in an inheritablethreadlocal . when calling getorcreate method from sparksession.builder we end up either creating a new spark session (and storing it in the inheritablethreadlocal) or using an existing one. so, for example, when running an end-to-end test, because sharedsparksessionhelper is loaded before anything else (by means of the beforeall method), the application under test will be using the spark session launched by sharedsparksessionhelper . once the test class is finished, the afterall method stops the spark session and removes it from the inheritablethreadlocal leaving our test environment ready for a new spark session. in this way, tests using spark can run in an isolated way. awesome project this article would be nothing without a real example. just following this link you will find a project with sbt , scalatest , scalastyle , sbt-coverage and scalafmt where i use the sharedsparksessionhelper trait. this application can be run in any of the available clusters that currently exist such as kubernetes , apache hadoop yarn , spak running in cluster mode or any other of your choice. conclusion testing spark applications can seem more complicated than with other frameworks not only because of the need of preparing a data set but also because of the lack of tools that allow us to automate such tests. by means of the sharedsparksessionhelper trait we can automate our tests in an easy way. i hope this article was useful. if you enjoy messing around with big data, microservices, reverse engineering or any other computer stuff and want to share your experiences with me, just follow me . top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse adevinta spain follow opensource projects of adevinta spain contribute more from adevinta spain testeando una spring boot app dockerizada # showdev # springboot # docker # testing usando aws s3 en local con localstack # backend # aws # microservices # testing integrando testcontainers en el contexto de spring en nuestros tests # testing # microservices # springboot # docker 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/mh_shifat/getting-start-with-hadoop-for-linux-debian-based-2ib8,,,"install hadoop in linux (debian) for big data analysis - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse 5hft posted on feb 21, 2020 • edited on jul 29, 2020 install hadoop in linux (debian) for big data analysis # hadoop # bigdata # industry the apache hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. it is designed to scale up from single servers to thousands of machines, each offering local computation and storage. there are two ways to install hadoop , i.e. single node and multi node . single node cluster means only one datanode running and setting up all the namenode, datanode, resourcemanager and nodemanager on a single machine. this is used for studying and testing purposes. install hadoop: setting up a single node hadoop cluster prerequirments : step 0) install java open-jdk-8 : add repository : sudo add-apt-repository ppa:openjdk-r/ppa update : sudo apt update install : sudo apt install openjdk-8-jdk note : incase of kali-linux just install jdk step 1) install ssh : sudo apt install ssh step 2) install rsync : sudo apt install rsync step 3) ssh without passphase setup : ssh-keygen -t rsa step 4) append : cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys step 5) now ssh localhost issue-1 : ssh: connect to host localhost port 22: connection refused restart ssh : service ssh restart issue-2 : this could be a permission issue so try using chmod : chmod -r 700 ~/.ssh chmod -r 700 ~/.ssh chmod 644 ~/.ssh/authorized_keys chmod 644 ~/.ssh/known_hosts chmod 644 ~/.ssh/config chmod 600 ~/.ssh/id_rsa chmod 644 ~/.ssh/id_rsa.pub then again run : ssh localhost main install process : step 6) hadoop download link (stable) : apache hadoop i have installed hadoop-3.2.1 and i prefer to downlaod this one. step 7) extract the file using tar -xzf hadoop-3.2.1.tar.gz step 8) copy the hadoop-3.2.1 folder to your desired place and rename it hadoop (such as dir looks like /home/username/hadoop) step 9) edit .bashrc file [location : ~ (home directory)] and insert (add) the code given below into .bashrc #for hadoop

export java_home=/usr/lib/jvm/java-8-openjdk-amd64 #java_jdk directory

export hadoop_home=/home/username/hadoop #location of your hadoop file directory

export hadoop_mapred_home=$hadoop_home
export hadoop_conf_dir=$hadoop_home/etc/hadoop
export hadoop_common_home=$hadoop_home
export hadoop_hdfs=$hadoop_home
export yarn_home=$hadoop_home
export hadoop_user_classpath_first=true

alias hadoop=$hadoop_home/bin/./hadoop #for convenience
alias hdfs=$hadoop_home/bin/./hdfs #for convenience

#done note : change username in hadoop_home according to your username. to get the java_jdk path command : readlink -f \$(which java) step 10) reload .bashrc file to effect the changes : source .bashrc step 11) edit the files in hadoop/etc/hadoop/ : core-site.xml (append/add the given code below) : <configuration> <property> <name> fs.defaultfs </name> <value> hdfs://localhost:9000 </value> </property> </configuration> hdfs-site.xml (append/add the given code below) : note : change username according to your username. <configuration> <property> <name> dfs.name.dir </name> <value> file:///home/username/pseudo/dfs/name </value> <!-- username = use `whoami` command in terminal to know your username in machine  --> </property> <property> <name> dfs.data.dir </name> <value> file:///home/username/pseudo/dfs/data </value> <!-- username = use `whoami` command in terminal to know your username in machine  --> </property> <property> <name> dfs.replication </name> <value> 1 </value> </property> </configuration> mapred-site.xml (append/add the given code below) : <configuration> <property> <name> mapred.job.tracker </name> <value> localhost:8021 </value> </property> </configuration> hadoop-env.sh (append/add the given code below) : export java_home = /usr/lib/jvm/java-8-openjdk-amd64 #java_jdk directory to get the java_jdk path run : readlink -f \$(which java) after everything done without any error... step 12) format hadoop file system by running the command:
> hadoop namenode -format step 13) to run hadoop : $hadoop_home/sbin/start-all.sh now open your browser and go to http://localhost:50070 you will get your hadoop working ! :d since hadoop 3.0.0 - alpha 1 there was a change in the port configuration: http://localhost:50070 was moved to http://localhost:9870 to check the process and port: jps stop hadoop : $hadoop_home/sbin/stop-all.sh after machine (pc) started enable hadoop using $hadoop_home/sbin/start-all.sh` the default port number to access all applications of cluster is 8088 http://localhost:8088/ top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand philip kumah jr philip kumah jr philip kumah jr follow a complete curious developer education udacity work wolke sec inc joined jun 28, 2022 • feb 9 '23 • edited on feb 9 • edited dropdown menu copy link hide this was so helpful and very straight to the point. thanks like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse 5hft follow love to learn core & fundamental concepts, think & write code, accept challenges, solve real world problems with cutting edge technologies. location dhaka education b.sc in software engineering work associate software engineer at dynamic solution innovators ltd. joined dec 15, 2019 trending on dev community hot 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development meme monday # discuss # watercooler # jokes introducing dev education tracks: expert-guided tutorials for learning new skills and earning badges # deved # career # ai # gemini 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/maninekkalapudi/intoduction-to-apache-spark-3k5m,,,"intoduction to apache spark - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse maninekkalapudi posted on sep 14, 2020 intoduction to apache spark # dataengineering # apachespark # bigdata # spark hello! it's been 6 months that i started my new role as a data engineer and i'm excited to work with the best tools to build etl pipelines at work. one of them is apache spark . i'll share my knowledge on spark and this post the we'll discuss the origins of big data processing hadoop mapreduce hadoop's shortcomings what is spark? spark's way of doing things spark ecosystem spark components use cases references #100daysofdataeng and beyond 1. the origins of big data processing the origin of the internet in 1989 enabled us to produce and consume data in all forms of life. this ensured a huge boom in data and in the following years essentially every organization and app became data reliant regardless of their scale. big data/data engineering, which deals with the collection, processing and storage of huge volumes of data became a matter of prime importance for business applications. for operational efficiency, demand prediction, user behavior, and other use cases like fraud detection, we need data and also in correct format. this is also know as etl or extract(collection), transform(processing) and load(storage). at this inflection point of data growth traditional tools (ex: relational databases) fell short this kind of new tasks which required processing petabytes scale datasets and providing the results quickly. this was very pronounced at companies like google, which essentially crawls the whole internet and indexes the webpages for search results at planetary scale. google then came up with a very clever solution to their ever growing data problems called mapreduce , which later became apache hadoop ; and later on it went on to became the watershed moment in big data processing and analytics. 2. hadoop mapreduce a mapreduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). the ""mapreduce system"" (also called ""infrastructure"" or ""framework"") orchestrates the processing on distributed servers, running the various tasks in parallel, managing all communications and data transfers between different parts of the system, and providing for redundancy and fault tolerance. this initially solved the problems like building a large clusters of data and processing it. but this solution had a huge efficiency problem over time and as the data grew exponentially. 3. hadoop's shortcomings the verbose batch-processing mapreduce api, cumbersome operational complexity, brittle fault tolerance became evident while using hadoop. with large batches of data jobs with many pairs of mr tasks, each pair’s intermediate computed result is written to the disk. this repeated disk i/o took large mr jobs hours on end, or even days. intermittent iteration of reads and writes between map and reduce computation 4. what is apache spark? spark is a unified engine designed for large-scale distributed data processing, on premise data centers or in the cloud. spark provides in-memory storage for intermediate computations, making it much faster than hadoop mapreduce. 5. the spark way of doing things spark achieves simplicity by providing a fundamental abstraction of a simple logical data structure called a resilient distributed dataset (rdd) upon which all other higher-level structured data abstractions such as dataframes and datasets are constructed. by providing a set of transformations and actions as operations, spark offers a simple programming model that you can use to build big data applications in familiar languages. once an application is built using transformations and actions spark offers, it builds its query computations as a directed acyclic graph (dag) ; its dag scheduler and query optimizer construct an efficient computational graph that defines the path of execution and can usually be decomposed into tasks that are executed in parallel across workers on the cluster. a typical direct acyclic graph(dag) in spark 6. spark ecosystem spark supports the following traditional rdbms like mysql, postgresql and etc., nosql relational databases like mysql and postgresql nosql databases like apache hbase, apache cassandra and mongodb apache hadoop distributed event streaming platform like apache kafka delta lake elastic search engine redis caching database and many more... apache spark’s ecosystem of connectors 7. spark components and apis spark components and api stack spark supports languages like scala sql python - pyspark java r 7.1. spark sql spark sql works well with structured data and you can read data stored in an rdbms table or file formats with structured data like csv, text, json, avro, orc, parquet and etc. we can write the sql queries and access the resulting data as a spark dataframe. // spark sql example // in scala // read data off amazon s3 bucket into a spark dataframe spark . read . json ( ""s3://apache_spark/data/committers.json"" ) . createorreplacetempview ( ""committers"" ) // issue a sql query and return the result as a spark dataframe val results = spark . sql ( """"""select name, org, module, release, num_commits
from committers where module = 'mllib' and num_commits > 10
order by num_commits desc"""""" ) //note: you can write similar code snippets in python, r, or java, //and the generated bytecode will be identical, //resulting in the same performance. enter fullscreen mode exit fullscreen mode 7.2. spark mllib spark comes common machine learning (ml) algorithms library called mllib. it provides many popular machine learning algorithms built on top of high-level dataframe-based apis to build models. these apis allow you to extract or transform features, build pipelines (for training and evaluating), and persist models (for saving and reloading them) during deployment. additional utilities include the use of common linear algebra operations and statistics. note: starting with apache spark 1.6, the mllib project is split between two packages: spark.mllib and spark.ml. spark.mllib is currently under maintenance mode and all new features go into spark.ml. # spark ml library example
# in python from pyspark.ml.classification import logisticregression ... training = spark . read . csv ( ""s3://..."" ) test = spark . read . csv ( ""s3://..."" ) # load training data lr = logisticregression ( maxiter = 10 , regparam = 0.3 , elasticnetparam = 0.8 ) # fit the model lrmodel = lr . fit ( training ) # predict lrmodel . transform ( test ) ... enter fullscreen mode exit fullscreen mode 7.3. spark structured streaming spark continuous streaming model and structured streaming apis, built atop the spark sql engine and dataframe-based apis. it is necessary for big data developers to combine and react in real time to both static data and streaming data from engines like apache kafka and other streaming sources, the new model views a stream as a continually growing table, with new rows of data appended at the end. developers can merely treat this as a structured table and issue queries against it as they would a static table. underneath the structured streaming model, the spark sql core engine handles all aspects of fault tolerance and late-data semantics, allowing developers to focus on writing streaming applications with relative ease. 7.4. graphx graphx is a library for manipulating graphs (e.g., social network graphs, routes and connection points, or network topology graphs) and performing graph-parallel computations. it offers the standard graph algorithms for analysis, connections, and traversals, contributed by users in the community: the available algorithms include pagerank, connected components, and triangle counting. 8. spark use cases processing in parallel large data sets distributed across a cluster performing ad hoc or interactive queries to explore and visualize data sets building, training, and evaluating machine learning models using mllib implementing end-to-end data pipelines from myriad streams of data analyzing graph data sets and social networks 9. references i'm using learning spark, 2nd edition by jules s. damji, brooke wenig, tathagata das and denny lee for gaining my spark knowledge. this article includes examples and images from the same. 10. #100daysofdataeng and beyond in the upcoming post we'll discuss about how spark distributed execution works and later on we'll explore standalone spark application example, how spark is used to build and maintain etl pipelines at scale. recently, i've taken up #100daysofdataeng challenge on twitter! 🙂 please like, share and follow for more updates. thanks you! 🙂 day 1/100: 📊started with basic concepts on #apachespark 📊 installed spark on my local windows machine in local mode 📊 read about building blocks of spark such as spark driver, sparksession, spark executor n cluster manager #100daysofdataeng #100daysofcloud #codenewbie — mani #clouddog☁️#datadog📊 ( @maninekkalapudi ) september 7, 2020 top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse maninekkalapudi follow book worm📚 programmer👨‍💻 cloud enthusiast☁️
data engineer📊 pythonista⌨️ occasional scribbler✍️ location hyderabad, india education bachelor's degree in automation and robotics work data engineer joined jul 23, 2018 trending on dev community hot meme monday # discuss # watercooler # jokes 💻 10 genius technical projects that can 10x your resume in 2025 💼 # programming # webdev # career # development it's 2025 - why is offline file sharing still so broken? # privacy # opensource # productivity # discuss 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/zejnilovic/how-to-compare-your-data-in-with-spark-3m7c,,,"how to compare your data in/with spark - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse saša zejnilović posted on may 1, 2020 how to compare your data in/with spark # spark # scala # testing # bigdata table of contents intro the problem the solution who exactly is behind this project hermes dataset comparison features usage - spark application summing-up intro apache spark, as is, provides quite a lot of different capabilities and features, but it is missing one that i, as a self-proclaimed sdet, find pretty valuable. the comparison of data. i'm talking about the comparison of complex data, complex structures and generating a report that can be used to see where the problem lies; more than just a normal true/false comparison. the problem the main problem that we are trying to solve is that when using standard solutions, running a comparison of sorts on a large dataset returns a basic true or false result, after which you then need to comb through all of the data and try to find the root cause. there is no fast response. fast feedback loops are essential, but that's for a different article. you also need some basic metrics about the dataset to be provided. testing without proper results is putting your trust in hope, and hope alone cannot build your big data solutions. the solution for these reasons, my teammates from absaoss and i have written a tool called hermes . hermes consists of three modules, and one of its modules is a data comparison tool which works either as a spark application or as a library, and it can compare whichever format is supported by apache spark. this tool is written in scala, so it should be possible to use within any jvm application of your own. (i have even seen people using py-spark use our libraries, so it's not only jvm compatible. i am, however, not an expert on that, and i am not sure how ""clean"" of a solution that is.) in this article, i would like to give a brief overview of the features of this spark comparison tool and how to use it as a spark app. usage as a library is a bit more complex, and i believe it deserves a full article of its own. let me first explain who we are. who exactly is behind this project absaoss is an initiative of absa group limited , a south african bank that wants to go open source. you want to standardize your data, move it from cobol to something current or track what and how your data is handled? we do that ( enceladus ), that ( cobrix ) and that ( spline ). and some other interesting stuff. hermes and all other projects are under the apache license. meaning, feel free to use it and contribute. the projects are active, and we spend almost our whole days on github, so we are usually quite fast to respond. all of our projects are in some way currently used by absa in production. hermes' significant advantage is that even though it is used in production, it is quite young and still looking for ideas. it is still growing. current real-world usages are: a testing framework for the enceladus project. a data check tool that gives us an assurance that new tools work as well as the old ones that are being decommissioned. hermes dataset comparison features this feature list should be the same for the people who use it as a library as for those that use it as a spark app. the features are as follows: can be used as an apache spark application or spark library compares virtually any data type if you provide the needed library for the source type on the classpath. spark already supports a lot of source types, but you might need to read oracle, hive or avro. just provide the application with proper packages, and you are good to go jdbc, spark and other packages are not packaged together with the application. they have a provided dependency. this allows us to keep the jar to 150 kb and provide users with more flexibility can compare two different source types writes output as parquet (this is planned to be configurable. issue #72 ) only compares data sets with the same schema. we have a complex schema comparison so the schema does not have to be aligned, but it has to be the same. (we have a plan for selective comparisons in the future) will write _metrics file at the end (this will be written next to the parquet ) if you passed or failed how many rows were processed if any duplicate rows were found number of differences found provides a precise path to what was wrong in the datasets. even if the structure is complex (arrays of structs and the likes). this is written to the final parquet final parquet holds only the rows that were different prints summary to stdout usage - spark application disclaimer: i will try to cover all of the tool's functionalities, but i will be skipping over spark-submit configurations. that is beyond the scope of this text. i will also not cover how to set up your hadoop and apache spark. in this use case, i will try to show possibilities of hermes's dataset comparison. this use case covers usage as a spark application. for usage as a library, look forward to a second article. to use hermes's dataset comparison, you just need to know how to run spark-submit , your data types, their properties/options and where it is. let's start with an easy example: example 1 spark-submit \ <spark-options> \ dataset-comparison-0.2.0.jar \ --new-format csv \ --new-header true \ --new-path /new/path \ --ref-format xml \ --ref-rowtag alfa \ --ref-path /ref/path \ --out-path /out/path \ --keys id enter fullscreen mode exit fullscreen mode example 2 spark-submit \ <spark-options> \ dataset-comparison-0.2.0.jar \ --format xml \ --rowtag alfa \ --new-path /new/path \ --ref-path /ref/path \ --out-path /out/path \ --keys id enter fullscreen mode exit fullscreen mode now, let's go over what these are. the job has one independent parameter, and that is --keys . keys refers to the set of primary keys. you can provide either a single primary key or a number of keys as a comma-delimited list in the form id1,id2,id3 . next up is --out-path . for now, out-path can only be configured to specify the destination path for the parquet file which will contain the output differences and metrics. this is planned to change ( #72 ), and it will have the same rules as --ref and --new prefixes. last and (probably) hardest to grasp are the --ref and --new parameters. these are only prefixes to the spark source type's standard options. just add -format to specify the source format (type). add -path to get the input or output path, unless you are using jdbc connector, then use -dbtable and then any other options prepended with the correct prefix ( --ref or --new ) depending on if it is reference data or the new data that you are testing. these options can also be generalized. taking a look at example 2 , it has only --format ; no --new-format or --ref-format . this is because both source types are xml and both have the same rowtag . in this case, there is no need to specify this twice. if both source types were xml but had different rowtag s, then the --ref-rowtag and --new-rowtag options would need to be specified. after running this, just run hdfs dfs -ls /out/path and take a look at the results. if there were any differences, you should find a parquet file that has a new column added called err_col . this error column will be filled with paths highlighting differences in your structure. its schema is (pretty simple): root |-- errcol : array ( nullable = true ) | |-- element : string ( containsnull = true ) enter fullscreen mode exit fullscreen mode summing-up hermes should be easy to use testing tool and framework. its dataset comparison module currently holds the most value, even outside of absaoss, and i hope it can help you solve an issue or two. if you have any question about this or any of our projects, just send us a message or create a question issue on github. i am looking forward to your comments and see you in the next article - usage as a library. top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 more from saša zejnilović 5 things to watch out for in automated regression tests # testing # regression # management working with nested structures in spark # spark # bigdata # scala # library black box testing misconceptions # beginners # testing 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/anandp86/using-aws-glue-and-pyspark-56fi,,,"guide - aws glue and pyspark  - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse anand posted on sep 19, 2020 guide - aws glue and pyspark # aws # cloud # bigdata # pyspark in this post, i have penned down aws glue and pyspark functionalities which can be helpful when thinking of creating aws pipeline and writing aws glue pyspark scripts. aws glue is a fully managed extract, transform, and load (etl) service  to process large amount of datasets from various sources for analytics and data processing. while creating the aws glue job, you can select between spark, spark streaming and python shell. these job can run proposed script generated by aws glue, or an existing script that you provide or a new script authored by you. along with this you can select different monitoring options, job execution capacity, timeouts, delayed notification threshold and non-overridable and overridable parameters. glue job type and glue version script file name and other available options aws recently launched glue version 2.0 which features 10x faster spark etl job start times and reducing the billing duration from a 10 minute minimum to 1 minute minimum. https://aws.amazon.com/blogs/aws/aws-glue-version-2-0-featuring-10x-faster-job-start-times-and-1-minute-minimum-billing-duration with aws glue you can create development endpoint and configure sagemaker or zeppelin notebooks to develop and test your glue etl scripts. i create a sagemaker notebook connected to the dev endpoint to author and test the etl scripts. depending on the language you are comfortable with, you can spin up the notebook. now, lets talk about some specific features and functionalities in aws glue and pyspark which can be helpful. 1. spark dataframes spark dataframe is a distributed collection of data organized into named columns. it is conceptually equivalent to a table in a relational database. you can create dataframe from rdd, from file formats like csv, json, parquet. with sagemaker sparkmagic(pyspark) kernel notebook, spark session is automatically created. to create dataframe - # from csv files 
s3_in = ""s3://mybucket/train/training.csv""

csv_df = (
    spark.read.format(""org.apache.spark.csv"")
    .option(""header"", true)
    .option(""quote"", '""')
    .option(""escape"", '""')
    .option(""inferschema"", true)
    .option(""ignoreleadingwhitespace"", true)
    .option(""ignoretrailingwhitespace"", true)
    .csv(s3_in, multiline=false)
)

# from parquet files 
s3_parquet=""s3://mybucket/folder1/dt=2020-08-24-19-28/""

df = spark.read.parquet(s3_parquet)

# from json files
df = spark.read.json(s3_json)

# from multiline json file 
df = spark.read.json(s3_json, multiline=true) 2. gluecontext gluecontext is the entry point for reading and writing dynamicframes in aws glue. it wraps the apache sparksql sqlcontext object providing mechanisms for interacting with the apache spark platform. from awsglue.job import job
from awsglue.transforms import *
from awsglue.context import gluecontext
from pyspark.context import sparkcontext
from awsglue.utils import getresolvedoptions
from awsglue.dynamicframe import dynamicframe

gluecontext = gluecontext(sparkcontext.getorcreate()) 3. dynamicframe aws glue dynamicframes are similar to sparksql dataframes. it represent a distributed collection of data without requiring you to specify a schema.it can also be used to read and transform data that contains inconsistent values and types. dynamicframe can be created using the below options – create_dynamic_frame_from_rdd – created from an apache spark resilient distributed dataset (rdd) create_dynamic_frame_from_catalog – created using a glue catalog database and table name create_dynamic_frame_from_options – created with the specified connection and format. example – the connection type, such as amazon s3, amazon redshift, and jdbc dynamicframes can be converted to and from dataframes using .todf() and fromdf(). #create dynamicfame from s3 parquet files
datasource0 = gluecontext.create_dynamic_frame_from_options(
            connection_type=""s3"",
            connection_options = {
                ""paths"": [s3_location]
            },
            format=""parquet"",
            transformation_ctx=""datasource0"")

#create dynamicfame from glue catalog 
datasource0 = gluecontext.create_dynamic_frame.from_catalog(
           database = ""demo"",
           table_name = ""testtable"",
           transformation_ctx = ""datasource0"")

#convert to spark dataframe 
df1 = datasource0.todf()

#convert to glue dynamicframe
df2 = dynamicframe.fromdf(df1, gluecontext , ""df2"") further read - https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html#aws-glue-api-crawler-pyspark-extensions-glue-context-create_dynamic_frame_from_catalog 4. aws glue job bookmark aws glue job bookmark helps process incremental data when rerunning the job on a scheduled interval, preventing reprocessing of old data. further read  - https://aprakash.wordpress.com/2020/05/07/implementing-glue-etl-job-with-job-bookmarks/ https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html 5. write out data the dynamicframe of transformed dataset can be written out to s3 as non-partitioned (default) or partitioned. "" partitionkeys "" parameter can be specified in connection_option to write out the data to s3 as partitioned. aws glue organizes these dataset in hive-style partition. in the below code example, aws glue dynamicframe is partitioned by year, month, day, hour and written in parquet format in hive-style partition on to s3. s3://bucket_name/table_name/year=2020/month=7/day=13/hour=14/part-000-671c.c000.snappy.parquet s3_location = ""s3://bucket_name/table_name""

datasink = gluecontext.write_dynamic_frame_from_options(
    frame= data,
    connection_type=""s3"",
    connection_options={
        ""path"": s3_location,
        ""partitionkeys"": [""year"", ""month"", ""day"", ""hour""]
    },
    format=""parquet"",
    transformation_ctx =""datasink"") further read - https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html#aws-glue-api-crawler-pyspark-extensions-glue-context-write_dynamic_frame_from_options 6. ""glueparquet"" format option glueparquet is a performance optimized apache parquet writer type for writing dynamicframes. it computes and modifies the schema dynamically. datasink = gluecontext.write_dynamic_frame_from_options(
               frame=dynamicframe,
               connection_type=""s3"",
               connection_options={
                  ""path"": s3_location,
                  ""partitionkeys"": [""year"", ""month"", ""day"", ""hour""]
               },
               format=""glueparquet"",
               format_options = {""compression"": ""snappy""},
               transformation_ctx =""datasink"") further read - https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format.html 7. s3 lister and other options for optimizing memory management aws glue provides an optimized mechanism to list files on s3 while reading data into dynamicframe which can be enabled using additional_options parameter "" uses3listimplementation "" to true. further read - https://aws.amazon.com/blogs/big-data/optimize-memory-management-in-aws-glue/ 8. purge s3 path purge_s3_path is a nice option available to delete files from specified s3 path recursively based on retention period or other available filters. as an example, suppose you are running aws glue job to fully refresh the table per day writing the data to s3 with naming convention of s3://bucket-name/table-name/dt=<data-time>. based on the defined retention period using the glue job itself you can delete the dt=<date-time> s3 folders. another option is to set s3 bucket lifecycle policy with prefix. #purge locations older than 3 days
print(""attempting to purge s3 path with retention set to 3 days."")
gluecontext.purge_s3_path(
    s3_path=output_loc, 
    options={""retentionperiod"": 72}) you have other options like purge_table, transition_table and transition_s3_path also available. the transition_table option transitions the storage class of the files stored on amazon s3 for the specified catalog's database and table. further read - https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-glue-context.html#aws-glue-api-crawler-pyspark-extensions-glue-context-purge_s3_path 9. relationalize class relationalize class can help flatten nested json outermost level. further read - https://aprakash.wordpress.com/2020/02/26/aws-glue-querying-nested-json-with-relationalize-transform/ 10. unbox class the unbox class helps unbox string field in dynamicframe to specified format type(optional). further read - https://aprakash.wordpress.com/2020/02/26/aws-glue-querying-nested-json-with-relationalize-transform/ 11. unnest class the unnest class flattens nested objects to top-level elements in a dynamicframe. root
|-- id: string
|-- type: string
|-- content: map
|    |-- keytype: string
|    |-- valuetype: string with content attribute/column being map type, we can use unnest class to unnest each key elements. unnested = unnestframe.apply(frame=data_dynamic_dframe)
unnested.printschema() root
|-- id: string
|-- type: string
|-- content.datelastupdated: string
|-- content.creator: string
|-- content.datecreated: string
|-- content.title: string 12. printschema() to print the spark or glue dynamicframe schema in tree format use printschema() . datasource0.printschema()

root
|-- id: int
|-- name: string
|-- identity: string
|-- alignment: string
|-- eyecolor: string
|-- haircolor: string
|-- gender: string
|-- status: string
|-- appearances: int
|-- firstappearance: choice
|    |-- int
|    |-- long
|    |-- string
|-- year: int
|-- universe: string 13. fields selection select_fields can be used to select fields from glue dynamicframe. # from dynamicframe

datasource0.select_fields([""status"",""haircolor""]).todf().distinct().show() to select fields from spark dataframe use "" select "" - # from dataframe

datasource0_df.select([""status"",""haircolor""]).distinct().show() 14. timestamp suppose the application writes data into dynamodb and has last_updated attribute/column.  dynamodb does not natively support date/timestamp data type. so, you could either store it as string or number. if stored as number, its usually done as epoch time - the number of seconds since 00:00:00 utc on 1 january 1970. you could see something like ""1598331963"" which is 2020-08-25t05:06:03+00:00 in iso 8601. https://www.unixtimestamp.com/index.php how can you convert it to timestamp? when you read the data using aws glue dynamicframe and view the schema, it will show it as ""long"" data type. root
|-- version: string
|-- item_id: string
|-- status: string
|-- event_type: string
|-- last_updated: long to convert the last_updated long data type into timestamp data type, you can use the below - import pyspark.sql.functions as f
import pyspark.sql.types as t

new_df = (
    df
        .withcolumn(""last_updated"", f.from_unixtime(f.col(""last_updated"")/1000).cast(t.timestamptype()))
) 15. temporary view from spark dataframe if you want to store the spark dataframe as table and query it using spark sql, you can convert the dataframe into temporary view that is available for only that spark session using createorreplacetempview . df = spark.createdataframe(
    [
        (1, ['a', 'b', 'c'], 90.00),
        (2, ['x', 'y'], 99.99),
    ],
    ['id', 'event', 'score'] 
)

df.printschema()
root
 |-- id: long (nullable = true)
 |-- event: array (nullable = true)
 |    |-- element: string (containsnull = true)
 |-- score: double (nullable = true)

df.createorreplacetempview(""example"")

spark.sql(""select * from example"").show()

+---+---------+-----+
| id|    event|score|
+---+---------+-----+
|  1|[a, b, c]| 90.0|
|  2|   [x, y]|99.99|
+---+---------+-----+ 16. extract element from arraytype suppose from the above example, you want to create a new attribute/column to store only the last event. how would you do it? using element_at function. it returns element of array at given index in extraction if col is array. it can also be used to extract given key in extraction if col is map. import pyspark.sql.functions as element_at

newdf = df.withcolumn(""last_event"", element_at(""event"", -1))

newdf.printschema()
root
 |-- id: long (nullable = true)
 |-- event: array (nullable = true)
 |    |-- element: string (containsnull = true)
 |-- score: double (nullable = true)
 |-- last_event: string (nullable = true)

newdf.show()
+---+---------+-----+----------+
| id|    event|score|last_event|
+---+---------+-----+----------+
|  1|[a, b, c]| 90.0|         c|
|  2|   [x, y]|99.99|         y|
+---+---------+-----+----------+ 17. explode the explode function in pyspark is used to explode array or map columns in rows. taking an example, lets try to explode ""event"" column from the above example from pyspark.sql.functions import explode

df1 = df.select(df.id,explode(df.event))

df1.printschema()
root
 |-- id: long (nullable = true)
 |-- col: string (nullable = true)

df1.show()
+---+---+
| id|col|
+---+---+
|  1|  a|
|  1|  b|
|  1|  c|
|  2|  x|
|  2|  y|
+---+---+ 18. getfield in a struct type, if you want to get a field by name, you can use "" getfield "". import pyspark.sql.functions as f
from pyspark.sql import row

from pyspark.sql import row
df = spark.createdataframe([row(attributes=row(name='scott', height=6.0, hair='black')),
                            row(attributes=row(name='kevin', height=6.1, hair='brown'))]
)

df.printschema()
root
 |-- attributes: struct (nullable = true)
 |    |-- hair: string (nullable = true)
 |    |-- height: double (nullable = true)
 |    |-- name: string (nullable = true)

df.show()
+-------------------+
|         attributes|
+-------------------+
|[black, 6.0, scott]|
|[brown, 6.1, kevin]|
+-------------------+

df1 = (df
      .withcolumn(""name"", f.col(""attributes"").getfield(""name""))
      .withcolumn(""height"", f.col(""attributes"").getfield(""height""))
      .drop(""attributes"")
      )

df1.show()
+-----+------+
| name|height|
+-----+------+
|scott|   6.0|
|kevin|   5.1|
+-----+------+ 19. startswith if you want to find records based on string match you can use "" startswith "". in the below example i am searching for all records where value for description column starts with ""[{"". import pyspark.sql.functions as f

df.filter(f.col(""description"").startswith(""[{"")).show() 20. extract year,  month,  day,  hour one of the common use case is to write the aws glue dynamicframe or spark dataframe to s3 in hive-style partition. to do so you can extract year, month, day, hour and use it as partitionkeys to write the dynamicframe/dataframe to s3. import pyspark.sql.functions as f

df2 = (raw_df
        .withcolumn('year', f.year(f.col('last_updated')))
        .withcolumn('month', f.month(f.col('last_updated')))
        .withcolumn('day', f.dayofmonth(f.col('last_updated')))
        .withcolumn('hour', f.hour(f.col('last_updated')))            
        ) top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse anand follow avid learner of technology solutions around databases, big-data, machine learning. 5x aws certified | 5x oracle certified. connect on twitter @anandp86 location seattle work aws data lab sa joined jan 9, 2019 more from anand connect to aws aurora postgresql/amazon redshift database from aws lambda # aws # database # datascience new features in amazon dynamodb - partiql, export to s3, integration with kinesis data streams # aws # dynamodb # database # dataengineering transform aws cloudtrail data using aws data wrangler # aws # bigdata # cloud # dataengineering 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/adipolak/pyspark-and-apache-spark-broadcast-mechanism-41bd,,,"pyspark and apache spark broadcast mechanism - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse adi polak posted on apr 6, 2020 pyspark and apache spark broadcast mechanism # todayilearned # beginners # distributedsystems # python apache spark in bitesize (4 part series) 1 apache spark bitesize series 2 apache spark basics 3 apache spark accumulators, simplified ! 4 pyspark and apache spark broadcast mechanism 🐦 follow me on twitter , happy to take your suggestions on topics. apache spark is based on distributed computation and distributed data concepts. each machine/task gets a piece of the data to process. many times, we will need something like a lookup table or parameters to base our calculations. those parameters will be static and won't change during the calculation, they will be read-only params. broadcast variables are used when static(read-only) variables need to be shared across executers. why should we use it? without broadcast variables, these variables would be shipped to each executor for every transformation and action; this can cause network overhead. however, with broadcast variables, they are shipped once to all executors and are cached for future reference. see the example next. python code sample with pyspark : here, we create a broadcast from a list of strings. loading a parquet file to spark dataframe and filter the dataframe based on the broadcast value. the broadcast is being shipped to the executers only once(network call for each executor). if we used a list without the broadcasting mechanism, for every row/entry in the dataframe, we would send the whole list, which will result in many networking requests. the number of requests will be equal or greater than the number of rows in the dataframe. since we talk about big data computation, the number of executors is necessarily smaller than the number of rows. and will clutter our cluster. in the end, we release the executor dedicated memory by calling broadcastvar.unpersist() . from pyspark import sparkcontext , broadcast from pyspark.sql import sparksession import pyspark.sql.functions as func spark = sparksession . builder \ . master ( ""local[*]"" ) \ . appname ( ""app name"" ) \ . getorcreate () # starts spark session locally sc = spark . sparkcontext words_new = sc . broadcast ([ ""list of values to broadcast"" , ""spark"" , ""python"" ]) data = words_new . value # accessing the value stored in the broadcast in master df = spark . read . parquet ( 'some_file_with_header.parquet' ) # loading parquet file into spark dataframe filtered = df . filter ( func . col ( 'name' ). isin ( words_new . value )) # filtering dataframe based on broadcast list with isin functionality words_new . unpersist () # sending requests to each executer to release static variables from dedicated memory enter fullscreen mode exit fullscreen mode that was apache spark broadcast with pyspark in under 3 min! which is part of apache spark bitesize series . apache spark in bitesize (4 part series) 1 apache spark bitesize series 2 apache spark basics 3 apache spark accumulators, simplified ! 4 pyspark and apache spark broadcast mechanism top comments (2) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand shir matishevski shir matishevski shir matishevski follow senior fullstack developer, who loves coding, research, and learning new things. i strive to put curiosity first in every task and keep an open mind 🌈 joined apr 6, 2020 • apr 6 '20 dropdown menu copy link hide good one, thanks adi! 👸 what's the benefit (if any) in using the broadcasting over standard spark caching? like comment: like comment: 2 likes like comment button reply collapse expand adi polak adi polak adi polak follow 1 out of 25 influential women in software development according to apiumhub.

i am a software developer who would like to learn more! location global education m.sc in computer science work software engineer at microsoft joined dec 11, 2018 • apr 6 '20 dropdown menu copy link hide thank you, shir! 💖 spark caching built for caching spark dataframes or rdd in memory. spark is a distributed computing framework that works on distributed data. each executor gets a chunk of the data to process, load data into memory, process it, and remove it from memory ( unless there are optimization or more known actions on the data ). we can ask spark to explicitly cache that chunk of data in the executors' memory. caching action means that it stays in dedicated memory until we call unpersist on it. we can cache many rdds or dataframes, but we won't have legal access from one to the other in the executors. if we exceed the memory space, the executor will write it to disk. with broadcast, we broadcast variables that we need, usually small size, shortlist, dictionary, and such that we are used together with the dataframes or rdds in computation. for example: countries data map and financial transactions, countries data, and location do not change - that means static data. transactions are new and are coming in streaming or batching. we can broadcast the countries with the static data map ( assuming it fits into memory) and in dataframe load the transaction either in batch or streaming. in each transaction computation, we can enrich and make the decisions based on the static data we have - the countries map. with cache - we can cache the transaction data and re-process it again but not the static data that is no an rdd or dataframe. however, we might get blocked, and all other transaction will take longer or if we out of memory will be paused. we can turn the static data into an rdd or dataframe if we would like to take action on it specifically. still, since it is relatively small data, it's better to do the calculations on the driver node and not 'pay' the overhead of distributed computing. is that make sense? like comment: like comment: 3 likes like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse adi polak follow 1 out of 25 influential women in software development according to apiumhub.

i am a software developer who would like to learn more! location global education m.sc in computer science work software engineer at microsoft joined dec 11, 2018 more from adi polak delta lake essential fundamentals: part 2 - the deltalog # database # tutorial # beginners # datascience delta lake essential fundamentals: part 1 - acid # database # tutorial # beginners # datascience free workshop on sql serverless # database # news # beginners # tutorial 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/aloneguid/tips-and-tricks-for-using-python-with-databricks-connect-593k,,,"tips and tricks for using python with databricks connect - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse ivan g posted on oct 6, 2020 • edited on mar 2, 2021 tips and tricks for using python with databricks connect # apachespark # databricks # pyspark # jupyter databricks connect is awesome. it makes development for apache spark a joy. without it i'd probably never be any effective and abandon spark a long time ago. why databricks connect is a magical local instance of spark. your machine will think it's using local installation of spark, however in reality it will use a remote databricks instance. why would you want it? because often you need access to large datasets that don't fit on your machine, and often you need bigger compute clusters to work with data. traditionally, it is possible to achieve it by having some small subsets of data locally and crunching through them, then deploying remotely to execute on a different dataset, but that's not always possible, depending on the data you work with. installing i assume you have already installed miniconda (or anaconda if you don't care about your pc) and you can follow basic steps in the official databricks connect docs . i also assume you've installed jupyter. if not, install from conda - conda install -c conda-forge notebook . i recommend installing this in your base environment. download and install jdk 8 if you haven't already. don't use higher versions as spark is running under java 8. also make sure that you are installing x64 version of the sdk. add bin to path , and create java_home environment variable. on windows this usually looks like: path=c:\program files (x86)\java\jdk1.8.0_261\bin
java_home=c:\program files (x86)\java\jdk1.8.0_261 enter fullscreen mode exit fullscreen mode create conda environment with python version 3.7 and not 3.5 like in the original article (it's probably outdated): conda create --name dbconnect python = 3.7 enter fullscreen mode exit fullscreen mode activate the environment conda activate dbconnect enter fullscreen mode exit fullscreen mode and install tools v6.6: pip install -u databricks-connect == 6.6. * enter fullscreen mode exit fullscreen mode your cluster needs to have two variable configured in order for databricks connect to work: spark.databricks.service.server.enabled needs to be set to true spark.databricks.service.port needs to be set to a port (you need this later). you should be able to use databricks connect now from ide you want. jupyter weirdness i like to have jupyter installed in my base conda environment once, and not duplicated across all the environments i'm creating. if this is the case, when you run jupyter server, it won't display the newly created environment for databricks connect, as it doesn't pick up new environments automatically. to solve this, install ipykernel (jupyter kernel integration) into databricks connect environment: conda install ipykernel enter fullscreen mode exit fullscreen mode instruct jupyter that current environment needs to be added as a kernel: python -m ipykernel install --user --name dbconnect --display-name ""databricks connect (dbconnect)"" enter fullscreen mode exit fullscreen mode go back to the base environment where you have installed jupyter and start again: conda activate base
jupyter kernel enter fullscreen mode exit fullscreen mode the kernel will be displayed in the list. jupyter hints you can enable automatic dataframe display functionality by setting spark.conf.set(""spark.sql.repl.eagereval.enabled"", true) . there are extensions available you can install to extend jupyter functionality. to customise jupyter look&feel, use this repository . using multiple databricks clusters once databricks-connect is configured from the command line by specifying all the parameters, including cluster id, you are tied to that cluster, unless it's reconfigured again. to use a different cluster, a new conda environment can be created, and configured again. however, if all of your clusters are in the same databricks workspace, you can use the following trick to switch between clusters. first, create a cluster map somewhere in code: clusters = { "" dev "" : { "" id "" : "" cluster id "" , "" port "" : "" port "" }, "" prod "" : { "" id "" : "" cluster id "" , "" port "" : "" port "" } } enter fullscreen mode exit fullscreen mode write a function that you can call from your notebook: def use_cluster ( cluster_name : str ): """""" when running via databricks connect, specify to which cluster to connect instead of the default cluster.
    this call is ignored when running in databricks environment.
    :param cluster_name: name of the cluster as defined in the beginning of this file. """""" real_cluster_name = spark . conf . get ( "" spark.databricks.clusterusagetags.clustername "" , none ) # do not configure if we are already running in databricks if not real_cluster_name : cluster_config = clusters . get ( cluster_name ) log . info ( f "" attaching to cluster ' { cluster_name } ' (id: { cluster_config [ ' id ' ] } , port: { cluster_config [ ' port ' ] } ) "" ) spark . conf . set ( "" spark.driver.host "" , "" 127.0.0.1 "" ) spark . conf . set ( "" spark.databricks.service.clusterid "" , cluster_config [ "" id "" ]) spark . conf . set ( "" spark.databricks.service.port "" , cluster_config [ "" port "" ]) enter fullscreen mode exit fullscreen mode pass cluster name from the map to use_cluster - this will select an appropriate cluster before executing the code. the good thing about it is you can leave the call in databricks notebook, as it will be ignored when running in their environment. local vs remote checking if notebook is running locally or in databricks the trick here is to check if one of the databricks-specific functions (like displayhtml) is in the ipython user namespace: def _check_is_databricks () -> bool : user_ns = ip . get_ipython (). user_ns return "" displayhtml "" in user_ns enter fullscreen mode exit fullscreen mode getting spark session databricks notebooks initialise spark variable automatically, therefore you can decide whether to return it or create a new local session: def _get_spark () -> sparksession : user_ns = ip . get_ipython (). user_ns if "" spark "" in user_ns : return user_ns [ "" spark "" ] else : spark = sparksession . builder . getorcreate () user_ns [ "" spark "" ] = spark return spark enter fullscreen mode exit fullscreen mode faking display function databricks has a nice display() function that renders dataframes. we don't have that locally, but we can fake it: def _get_display () -> callable [[ dataframe ], none ]: fn = ip . get_ipython (). user_ns . get ( "" display "" ) return fn or _display_with_json enter fullscreen mode exit fullscreen mode dbutils depending where you run, you can create a local instance of dbutils or get pre-initialised one when running in databricks: def _get_dbutils ( spark : sparksession ): try : from pyspark.dbutils import dbutils dbutils = dbutils ( spark ) except importerror : import ipython dbutils = ipython . get_ipython (). user_ns . get ( "" dbutils "" ) if not dbutils : log . warning ( "" could not initialise dbutils! "" ) return dbutils enter fullscreen mode exit fullscreen mode putting it all together you can put all of the above in a single python file, reference it from every notebook, and they will work regardless where you run: dbconnect.py : from typing import any , tuple , callable from pyspark.sql import sparksession , dataframe import logging import ipython as ip from pyspark.sql.types import structtype , arraytype import pyspark.sql.functions as f clusters = { "" dev "" : { "" id "" : "" cluster id "" , "" port "" : "" port "" }, "" prod "" : { "" id "" : "" cluster id "" , "" port "" : "" port "" } } # logging class silencefilter ( logging . filter ): def filter ( self , record : logging . logrecord ) -> int : return false logging . basicconfig ( format = "" %(asctime)s|%(levelname)s|%(name)s|%(message)s "" , level = logging . info ) logging . getlogger ( "" py4j.java_gateway "" ). addfilter ( silencefilter ()) log = logging . getlogger ( "" dbconnect "" ) def _check_is_databricks () -> bool : user_ns = ip . get_ipython (). user_ns return "" displayhtml "" in user_ns def _get_spark () -> sparksession : user_ns = ip . get_ipython (). user_ns if "" spark "" in user_ns : return user_ns [ "" spark "" ] else : spark = sparksession . builder . getorcreate () user_ns [ "" spark "" ] = spark return spark def _display ( df : dataframe ) -> none : df . show ( truncate = false ) def _display_with_json ( df : dataframe ) -> none : for column in df . schema : t = type ( column . datatype ) if t == structtype or t == arraytype : df = df . withcolumn ( column . name , f . to_json ( column . name )) df . show ( truncate = false ) def _get_display () -> callable [[ dataframe ], none ]: fn = ip . get_ipython (). user_ns . get ( "" display "" ) return fn or _display_with_json def _get_dbutils ( spark : sparksession ): try : from pyspark.dbutils import dbutils dbutils = dbutils ( spark ) except importerror : import ipython dbutils = ipython . get_ipython (). user_ns . get ( "" dbutils "" ) if not dbutils : log . warning ( "" could not initialise dbutils! "" ) return dbutils # initialise spark variables is_databricks : bool = _check_is_databricks () spark : sparksession = _get_spark () display = _get_display () dbutils = _get_dbutils ( spark ) def use_cluster ( cluster_name : str ): """""" when running via databricks connect, specify to which cluster to connect instead of the default cluster.
    this call is ignored when running in databricks environment.
    :param cluster_name: name of the cluster as defined in the beginning of this file. """""" real_cluster_name = spark . conf . get ( "" spark.databricks.clusterusagetags.clustername "" , none ) # do not configure if we are already running in databricks if not real_cluster_name : cluster_config = clusters . get ( cluster_name ) log . info ( f "" attaching to cluster ' { cluster_name } ' (id: { cluster_config [ ' id ' ] } , port: { cluster_config [ ' port ' ] } ) "" ) spark . conf . set ( "" spark.driver.host "" , "" 127.0.0.1 "" ) spark . conf . set ( "" spark.databricks.service.clusterid "" , cluster_config [ "" id "" ]) spark . conf . set ( "" spark.databricks.service.port "" , cluster_config [ "" port "" ]) enter fullscreen mode exit fullscreen mode in your notebook: from dbconnect import spark , dbutils , use_cluster , display use_cluster ( "" dev "" ) # ... df = spark . table ( "" .... "" ) # use spark variable display ( df ) # display dataframes # etc... enter fullscreen mode exit fullscreen mode fixing out-of-memory issues often when using databricks connect you might encounter an error like java heap space etc. etc. etc. . this simply means your local spark node (driver) is running out of memory, which by default is 2gb. if you need more memory, it's easy to increase it. first, find out where pyspark's home directory is: ❯ databricks-connect get-spark-home
c: \u sers \i vang \m iniconda3 \e nvs \h ospark \l ib \s ite-packages \p yspark enter fullscreen mode exit fullscreen mode this should have a subfolder conf (create it if it doesn't exist). and a file spark-defaults.conf (again, create if doesn't exist). full file path would be c:\users\ivang\miniconda3\envs\hospark\lib\site-packages\pyspark\conf\spark-defaults.conf . add a line spark.driver.memory 8g enter fullscreen mode exit fullscreen mode which increases driver memory to 8 gigabytes. monitoring jobs unfortunately, i couldn't find a good way to monitor jobs from dbc environment. there's big data tools plugin for intellij, that in theory supports spark job monitoring, and considering dbc runs a virtual local cluster, i though it would work. however, no luck in any configuration. the best way to monitor jobs i found is to use spark ui from the cluster on databricks you're connected to. this article was originally published on my blog . top comments (0) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse ivan g follow software and data architect, independent consultant and cloud computing professional location london, uk work software professional joined apr 24, 2019 more from ivan g flatten map spark python # spark # pyspark databricks client running on android # databricks # spark # android getting info about spark partitions # spark # databricks # python 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/danvoyce/pyspark-and-parquet-analysis-4ki4,,,"pyspark and parquet - analysis - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse dan voyce posted on jul 16, 2019 pyspark and parquet - analysis # pyspark # parquet # bigdata # analysis pandas is known to be a data scientist’s tool box, because of the amazing portfolio of the functionalities provided by the pandas library. but pandas performs really bad with big data and data which you cannot hold in memory. that is where spark comes into picture. spark is a cluster computing system build for large scale data processing. why use spark? spark is a framework that supports both batch processing (eg. hadoop and mapreduce), and real time processing (eg. apache storm). lot of high level api’s available for spark. you can code in scala, java, python and r. abstractions and concepts dag (direct acyclic graph) - directed graph which controls the flow of execution. sparkcontext - orchestration within the spark cluster. resilient distributed dataset - immutable, any activity (like a filter or a map) on the rdd created a new rdd. transformations - activities on rdd’s actions - lazy loading, processing as and when needed based on the dag. collecting data, get counts are all actions. what is pyspark? pandas, gives you the best tools for data analysis, and spark gives you the capability to work with big data. pyspark gives you the best of both worlds. you can work with big data, without having to really know the underlying mechanisms of cluster computing. pyspark gives you pandas like syntax for working with data frames. for example, reading a csv in pandas: df = pd . read_csv ( ' sample.csv ' ) #whereas in pyspark, its very similar syntax as shown below. df = spark . read . csv ( ' sample.csv ' ) enter fullscreen mode exit fullscreen mode although there are couple of differences in the syntax between both the languages, the learning curve is quite less between the two and you can focus more on building the applications. working with parquet parquet is a high performance columnar format database. to read more about the benefits of storing the data in columnar storage, please visit: https://blog.openbridge.com/how-to-be-a-hero-with-powerful-parquet-google-and-amazon-f2ae0f35ee04 . to read a parquet file from s3, we can use the following command: df = spark . read . parquet ( "" s3://path/to/parquet/file.parquet "" ) #converting compressed csv files to parquet, with snappy compression, 
#takes approximately 40 minutes for each day’s worth of poi_events data. 
#this can be done easily with pyspark using the following code. for i in range ( 1 , 31 ): print ( "" processsing day %02d "" % i ) csv_path = "" s3://test-data/2018/05/%02d/* "" % i p_path = "" s3://locally-sanjay/2018/05/%02d/ "" % i df = sqlcontext . read . option ( "" header "" , "" false "" ). csv ( csv_path ) df . write . parquet ( p_path , mode = ' overwrite ' ) enter fullscreen mode exit fullscreen mode downsides of using pyspark the main downside of using pyspark is that visualisation not supported yet, and you need to convert the data frame to pandas to do visualisation, it is not recommended because converting a pyspark dataframe to a pandas dataframe loads all the data into memory. however, you can use the “sample” method to convert parts of the pyspark dataframe to pandas and then visualise it. best practices using pyspark pyspark.sql.functions library provide built in functions for most of the transformation work. however you can write your own python udf’s for transformation, but its not recommended. the built-ins work inside of the jvm which spark uses, and running python udf’s will make the jvm spin up a python instance and pip all the data to that instance and pip all the output back from the instance to the jvm, which will affect the performance significantly: do not iterate row by row in dataframes. do not convert the entire pyspark dataframe to pandas, always sample and convert to pandas. technology stack the following technology stack was used in the testing of the products at locally: amazon spark cluster with 1 master and 2 slave nodes (standard ec2 instances) s3 buckets for storing parquet files. zeppelin notebook to run the scripts. the performance and cost on the google cloud platform needs to be tested. ultimately we went with orc as our main storage format at locally, but depending on your specific use-case parquet is also a solid choice. orc is the main driving force behind locally's location intelligence platform sanjay kumar - data engineer graduate student in data science, skills in spark, presto, bigquery and big data. top comments (1) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand raj aryan raj aryan raj aryan follow cloud data engineer at deloitte usi | ex-sap labs | aws & python enthusiast! location bangalore, karnataka, india. education ms ramaiah college, bangalore pronouns he/him work deloitte usi, ex- sap labs. joined may 28, 2024 • jun 1 '24 dropdown menu copy link hide thanks for you article:) like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse dan voyce follow technical solutions director at demystdata, formally cto at locally: big data, php frameworks, open source and location / geospatial data location melbourne, au work director, technology solutions (apac) at demystdata joined jul 11, 2019 more from dan voyce converting csv to orc/parquet fast without a cluster! # bigdata # orc # parquet # cudf spatial big data systems - a retrospective # bigdata # analytics # presto # mpp creating a proof of concept for spatial joins # qgis # spatial # geospatial # bigdata 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/awwsmm/installing-and-running-hadoop-and-spark-on-ubuntu-18-393h,,,"installing and running hadoop and spark on ubuntu 18 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse andrew (he/him) posted on dec 13, 2019 • edited on dec 18, 2019 installing and running hadoop and spark on ubuntu 18 # hadoop # spark # java # scala hadoop & spark (4 part series) 1 installing and running hadoop and spark on windows 2 big data analysis with hadoop, spark, and r shiny 3 building a raspberry pi hadoop / spark cluster 4 installing and running hadoop and spark on ubuntu 18 installing and running hadoop and spark on ubuntu 18 this is a short guide (updated from my previous guides) on how to install hadoop and spark on ubuntu linux. roughly this same procedure should work on most debian-based linux distros, at least, though i've only tested it on ubuntu. no prior knowledge of hadoop, spark, or java is assumed. i'll be setting all of this up on a virtual machine (vm) using oracle's virtualbox , so i first need to get an iso file to install the ubuntu operating system (os). i'll download the most recent long-term support (lts) version of ubuntu from their website (as of this writing, that's 18.04.3). setting up a virtual machine is fairly straightforward and since it's not directly relevant, i won't be replicating those linked instructions here. instead, let's just start with a clean ubuntu installation... installing java hadoop requires java to be installed, and my minimal-installation ubuntu doesn't have java by default. you can check this with the command: $ java -version command 'java' not found, but can be installed with: sudo apt install default-jre sudo apt install openjdk-11-jre-headless sudo apt install openjdk-8-jre-headless enter fullscreen mode exit fullscreen mode note: we're going to ignore these suggestions and install java in a different way. hadoop runs smoothly with java 8, but may encounter bugs with newer versions of java. so i'd like to install java 8 specifically. to manage multiple java versions, i install sdkman! (but first i need to install curl): $ sudo apt install curl -y enter fullscreen mode exit fullscreen mode ...enter your password, and then install sdkman! with $ curl -s ""https://get.sdkman.io"" | bash enter fullscreen mode exit fullscreen mode sdkman! is a great piece of software that allows you to install multiple versions of all sorts of different packages, languages, and more. you can see a huge list of available software with: $ sdk ls # or sdk list enter fullscreen mode exit fullscreen mode to make sure you can use sdkman! in every new terminal, run the following command to append a line which sources the sdkman! initialisation script whenever a new terminal is opened: $ echo ""source ~/.sdkman/bin/sdkman-init.sh"" >> ~/.bashrc enter fullscreen mode exit fullscreen mode we're only going to use sdkman! to install one thing -- java. you can list all available versions of a particular installation candidate with: $ sdk list <software> enter fullscreen mode exit fullscreen mode so in our case, that's $ sdk list java enter fullscreen mode exit fullscreen mode ...we can see all of the different available java versions: to install a specific version, we use the identifier in the column all the way on the right with: $ sdk install <software> <identifier> enter fullscreen mode exit fullscreen mode i'm going to install adoptopenjdk's java 8.0.232 (hotspot), so this command, for me, is: $ sdk install java 8.0.232.hs-adpt enter fullscreen mode exit fullscreen mode sdkman! candidates are installed, by default, at ~/.sdkman/candidates : $ ls ~/.sdkman/candidates/java
8.0.232.hs-adpt  current enter fullscreen mode exit fullscreen mode the current symlink always points to whichever java version sdkman! thinks is the version you're currently using, and this is reflected in the java -version command. after the last step, this command returns: $ java -version openjdk version ""1.8.0_232"" openjdk runtime environment ( adoptopenjdk )( build 1.8.0_232-b09 ) openjdk 64-bit server vm ( adoptopenjdk )( build 25.232-b09, mixed mode ) enter fullscreen mode exit fullscreen mode if you install multiple java versions, you can easily switch between them with sdk use : $ sdk install java 13.0.1.hs-adpt

... $ sdk use java 13.0.1.hs-adpt

using java version 13.0.1.hs-adpt in this shell. enter fullscreen mode exit fullscreen mode $ java -version
openjdk version ""13.0.1"" 2019-10-15
openjdk runtime environment (adoptopenjdk)(build 13.0.1+9)
openjdk 64-bit server vm (adoptopenjdk)(build 13.0.1+9, mixed mode, sharing) enter fullscreen mode exit fullscreen mode we also need to explicitly define the java_home environment variable by adding it to the ~/.bashrc file: $ echo ""export java_home= \$ (readlink -f \$ (which java) | sed 's:bin/java::')"" >> ~/.bashrc enter fullscreen mode exit fullscreen mode echo -ing java_home should now give us the path to the sdkman! directory: $ echo $java_home /home/andrew/.sdkman/candidates/java/13.0.1.hs-adpt enter fullscreen mode exit fullscreen mode make sure you switch back to java 8 before continuing with this tutorial: $ sdk use java 8.0.232.hs-adpt

using java version 8.0.232.hs-adpt in this shell. enter fullscreen mode exit fullscreen mode installing hadoop with java installed, the next step is to install hadoop. you can get the most recent version of hadoop from apache's website . as of this writing, that version is hadoop 3.2.1 (released 22 sep 2019). if you click on the link on that webpage, it may redirect you. click until a *.tar.gz file is downloaded. the link i ended up using was http://mirrors.whoishostingthis.com/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz enter fullscreen mode exit fullscreen mode you can download that in the browser, or by using wget in the terminal: $ wget http://mirrors.whoishostingthis.com/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz enter fullscreen mode exit fullscreen mode unpack the archive with tar , and redirect the output to the /opt/ directory: $ sudo tar -xvf hadoop-3.2.1.tar.gz -c /opt/ enter fullscreen mode exit fullscreen mode remove the archive file and move to the /opt/ directory: $ rm hadoop-3.2.1.tar.gz && cd /opt enter fullscreen mode exit fullscreen mode rename the hadoop directory and change its permissions so that its owned by you (my username is andrew ) and not root or 1001 : $ sudo mv hadoop-3.2.1 hadoop && sudo chown andrew:andrew -r hadoop enter fullscreen mode exit fullscreen mode finally, define the hadoop_home environment variable and add the correct hadoop binaries to your path by echoing the following lines and concatenating them to your ~/.bashrc file: $ echo ""export hadoop_home=/opt/hadoop"" >> ~/.bashrc $ echo ""export path= \$ path: \$ hadoop_home/bin: \$ hadoop_home/sbin"" >> ~/.bashrc enter fullscreen mode exit fullscreen mode now, when you source your ~/.bashrc (or open any new shell), you should be able to check that hadoop has been installed correctly: $ hadoop version
hadoop 3.2.1
source code repository...
compiled by ...
... enter fullscreen mode exit fullscreen mode in order for hdfs to run correctly later, we also need to define java_home in the file /opt/hadoop/etc/hadoop/hadoop-env.sh . find the line in that file which begins with: # export java_home= enter fullscreen mode exit fullscreen mode and edit it to match the java_home variable we defined earlier: export java_home = /home/<username>/.sdkman/candidates/java/8.0.232.hs-adpt enter fullscreen mode exit fullscreen mode make sure you change the <username> above to the appropriate username for your setup. in my case, i replace <username> with andrew . installing spark the last bit of software we want to install is apache spark . we'll install this in a similar manner to how we installed hadoop, above. first, get the most recent *.tgz file from spark's website . i downloaded the spark 3.0.0-preview (6 nov 2019) pre-built for apache hadoop 3.2 and later with the command: $ wget http://mirrors.whoishostingthis.com/apache/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop3.2.tgz enter fullscreen mode exit fullscreen mode as with hadoop, unpack the archive with tar , and redirect the output to the /opt/ directory: $ sudo tar -xvf spark-3.0.0-preview-bin-hadoop3.2.tgz -c /opt/ enter fullscreen mode exit fullscreen mode remove the archive file and move to the /opt/ directory: $ rm spark-3.0.0-preview-bin-hadoop3.2.tgz && cd /opt enter fullscreen mode exit fullscreen mode rename the spark directory and change its permissions so that its owned by you (my username is andrew ) and not root or 1001 : $ sudo mv spark-3.0.0-preview-bin-hadoop3.2 spark && sudo chown andrew:andrew -r spark enter fullscreen mode exit fullscreen mode finally, define the spark_home environment variable and add the correct spark binaries to your path by echoing the following lines and concatenating them to your ~/.bashrc file: $ echo ""export spark_home=/opt/spark"" >> ~/.bashrc $ echo ""export path= \$ path: \$ spark_home/bin"" >> ~/.bashrc enter fullscreen mode exit fullscreen mode now, when you source your ~/.bashrc (or open any new shell), you should be able to check that spark has been installed correctly: $ spark-shell --version ...
...version 3.0.0-preview
... enter fullscreen mode exit fullscreen mode configuring hdfs at this point, hadoop and spark are installed and running correctly, but we haven't yet set up the hadoop distributed file system (hdfs). as its name suggests, hdfs is usually distributed across many machines. if you want to build a hadoop cluster, i've previously written instructions for doing that across a small cluster of raspberry pis . but for simplicity's sake, we'll just set up a standalone, local installation here. to configure hdfs, we need to edit several files located at /opt/hadoop/etc/hadoop/ . the first such file is core-site.xml . edit that file so it has the following xml structure: <configuration> <property> <name> fs.defaultfs </name> <value> hdfs://localhost:9000 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode the second file is hdfs-site.xml , which gives the locations of the the namenode and datanode directories. edit that file so it looks like: <configuration> <property> <name> dfs.datanode.data.dir </name> <value> file:///opt/hadoop_tmp/hdfs/datanode </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> file:///opt/hadoop_tmp/hdfs/namenode </value> </property> <property> <name> dfs.replication </name> <value> 1 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode we set dfs.replication to 1 because this is a one-machine cluster -- we can't replicate files any more than once here. read more about data replication in hdfs here. the directories given above ( /opt/hadoop_tmp/hdfs/datanode and /opt/hadoop_tmp/hdfs/namenode ) must exist and be read/write-able by the current user. so create them now, and adjust their permissions, with: $ sudo mkdir -p /opt/hadoop_tmp/hdfs/datanode $ sudo mkdir -p /opt/hadoop_tmp/hdfs/namenode $ sudo chown andrew:andrew -r /opt/hadoop_tmp enter fullscreen mode exit fullscreen mode the next configuration file is mapred-site.xml , which you should edit to look like: <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> enter fullscreen mode exit fullscreen mode ...and finally yarn-site.xml , which you should edit to look like: <configuration> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.auxservices.mapreduce.shuffle.class </name> <value> org.apache.hadoop.mapred.shufflehandler </value> </property> </configuration> enter fullscreen mode exit fullscreen mode configuring ssh if you started with a minimal ubuntu installation like i did, you may need to first set up your ssh connection (as hdfs connects to localhost:9000 ). to check if the ssh server is running, enter the command $ which sshd enter fullscreen mode exit fullscreen mode if nothing is returned, then the ssh server is not installed (this is the case with the minimal ubuntu installation). to get this up and running, install openssh-server , which will start the ssh service automatically: $ sudo apt install openssh-server enter fullscreen mode exit fullscreen mode $ sudo systemctl status ssh
● ssh.service - openbsd secure shell server
  loaded: loaded ...
  actve: active...
  ... enter fullscreen mode exit fullscreen mode to check that this worked, try ssh -ing into localhost : $ ssh localhost
...
are you sure you want to continue connecting ( yes /no ) ? yes ...
welcome to ubuntu 18.04.3 lts...
... enter fullscreen mode exit fullscreen mode you can exit to escape this superfluous self-connection. then, create a public-private keypair (if you haven't already): $ ssh-keygen
generating public/private rsa key pair.
... enter fullscreen mode exit fullscreen mode hit 'enter' / 'return' over and over to create a key in the default location with no passphrase. when you're back to the normal shell prompt, append the public key to your ~/.ssh/authorized_keys file: $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys enter fullscreen mode exit fullscreen mode you should now be able to boot hdfs. continue to the next section. formatting and booting hdfs at this point, we can format the distributed filesystem. be careful and do not run the following command unless you are sure there is no important data currently stored in the hdfs because it will be lost . but if you're setting up hdfs for the first time on this computer, you've got nothing to worry about: format the hdfs with $ hdfs namenode -format -force enter fullscreen mode exit fullscreen mode you should get a bunch of output and then a shutdown_msg : we can then boot the hdfs with the following two commands: $ start-dfs.sh && start-yarn.sh enter fullscreen mode exit fullscreen mode note: if you performed a minimal installation, you may need to install openssh-server by following the instructions given here. you can check that hdfs is running correctly with the command jps : $ jps
10384 datanode
11009 nodemanager
4113 resourcemanager
11143 jps
10218 namenode
10620 secondarynamenode enter fullscreen mode exit fullscreen mode you should see a namenode and a datanode , at minimum, in that list. check that hdfs is behaving correctly by trying to create a directory, then listing the contents of the hdfs: $ hdfs dfs -mkdir /test $ hdfs dfs -ls /
found 1 items
drwxr-xr-x   - andrew supergroup          0 2019-12-13 13:56 /test enter fullscreen mode exit fullscreen mode if you can see your directory, you've correctly configured the hdfs! monitoring hadoop and spark come with built-in web-based monitors that you can access by going to http://localhost:8088 : ...and http://localhost:9870 in your browser: working with spark and hdfs one of the benefits of working with spark and hadoop is that they're both apache products, so they work very nicely with each other. it's easy to read a file from hdfs into spark to analyse it. to test this, let's copy a small file to hdfs and analyse it with spark. spark comes with some example resource files. with the above configuration, they can be found at /opt/spark/examples/src/main/resources . let's copy the file users.parquet to hdfs: $ hdfs dfs -put /opt/spark/examples/src/main/resources/users.parquet /users.parquet enter fullscreen mode exit fullscreen mode parquet files are another apache creation , designed for fast data access and analysis. next, open the spark shell and read in the file with read.parquet : $ spark-shell
...
welcome to
... version-3.0.0-preview
... enter fullscreen mode exit fullscreen mode scala > val df = spark . read . parquet ( ""hdfs://localhost:9000/users.parquet"" ) df : org.apache.spark.sql.dataframe = [ name: string , favorite_color: string ... 1 more field ] scala > df . collect . foreach ( println ) [ alyssa , null , wrappedarray ( 3 , 9 , 15 , 20 )] [ ben , red , wrappedarray ()] enter fullscreen mode exit fullscreen mode this is just a small example, but it shows how spark and hdfs can work closely together. you can easily read files from hdfs and analyse them with spark! if you want to stop the hdfs , you can run the commands: $ stop-dfs.sh enter fullscreen mode exit fullscreen mode and $ stop-yarn.sh enter fullscreen mode exit fullscreen mode conclusion i hope this guide will be useful for anyone trying to set up a small hadoop / spark installation for testing or education. if you're interested in learning more about hadoop and spark, please check out my other articles in this series on dev ! thanks for reading! hadoop & spark (4 part series) 1 installing and running hadoop and spark on windows 2 big data analysis with hadoop, spark, and r shiny 3 building a raspberry pi hadoop / spark cluster 4 installing and running hadoop and spark on ubuntu 18 top comments (5) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand divas gupta divas gupta divas gupta follow joined apr 15, 2020 • apr 15 '20 dropdown menu copy link hide hi andrew, many thanks for creating such a wonderful installation guide. i am following your instruction but am facing one issue in between, it will be great if you can help. i am getting blank line when i am running below two commands - echo $java_home echo $hadoop_home pfb my .bashrc file - ~/.bashrc: executed by bash(1) for non-login shells. see /usr/share/doc/bash/examples/startup-files (in the package bash-doc) for examples if not running interactively, don't do anything case $- in i ) ;; *) return;; esac don't put duplicate lines or lines starting with space in the history. see bash(1) for more options histcontrol=ignoreboth append to the history file, don't overwrite it shopt -s histappend for setting history length see histsize and histfilesize in bash(1) histsize=1000 histfilesize=2000 check the window size after each command and, if necessary, update the values of lines and columns. shopt -s checkwinsize if set, the pattern ""**"" used in a pathname expansion context will match all files and zero or more directories and subdirectories. shopt -s globstar make less more friendly for non-text input files, see lesspipe(1) [ -x /usr/bin/lesspipe ] && eval ""$(shell=/bin/sh lesspipe)"" set variable identifying the chroot you work in (used in the prompt below) if [ -z ""${debian_chroot:-}"" ] && [ -r /etc/debian_chroot ]; then debian_chroot=$(cat /etc/debian_chroot) fi set a fancy prompt (non-color, unless we know we ""want"" color) case ""$term"" in xterm-color|*-256color) color_prompt=yes;; esac uncomment for a colored prompt, if the terminal has the capability; turned off by default to not distract the user: the focus in a terminal window should be on the output of commands, not on the prompt force_color_prompt=yes if [ -n ""$force_color_prompt"" ]; then if [ -x /usr/bin/tput ] && tput setaf 1 >&/dev/null; then # we have color support; assume it's compliant with ecma-48 # (iso/iec-6429). (lack of such support is extremely rare, and such # a case would tend to support setf rather than setaf.) color_prompt=yes else color_prompt= fi fi if [ ""$color_prompt"" = yes ]; then ps1='${debian_chroot:+($debian_chroot)}[\033[01;32m]\u@\h[\033[00m]:[\033[01;34m]\w[\033[00m]\$ ' else ps1='${debian_chroot:+($debian_chroot)}\u@\h:\w\$ ' fi unset color_prompt force_color_prompt if this is an xterm set the title to user@host:dir case ""$term"" in xterm*|rxvt*) ps1=""[\e]0;${debian_chroot:+($debian_chroot)}\u@\h: \w\a]$ps1"" ;; *) ;; esac enable color support of ls and also add handy aliases if [ -x /usr/bin/dircolors ]; then test -r ~/.dircolors && eval ""$(dircolors -b ~/.dircolors)"" || eval ""$(dircolors -b)"" alias ls='ls --color=auto' #alias dir='dir --color=auto' #alias vdir='vdir --color=auto' alias grep='grep --color=auto'
alias fgrep='fgrep --color=auto'
alias egrep='egrep --color=auto' fi colored gcc warnings and errors export gcc_colors='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01' some more ls aliases alias ll='ls -alf' alias la='ls -a' alias l='ls -cf' add an ""alert"" alias for long running commands.  use like so: sleep 10; alert alias alert='notify-send --urgency=low -i ""$([ $? = 0 ] && echo terminal || echo error)"" ""$(history|tail -n1|sed -e '\''s/^\s*[0-9]+\s*//;s/[;&|]\s*alert$//'\'')""' alias definitions. you may want to put all your additions into a separate file like ~/.bash_aliases, instead of adding them here directly. see /usr/share/doc/bash-doc/examples in the bash-doc package. if [ -f ~/.bash_aliases ]; then . ~/.bash_aliases fi enable programmable completion features (you don't need to enable this, if it's already enabled in /etc/bash.bashrc and /etc/profile sources /etc/bash.bashrc). if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi this must be at the end of the file for sdkman to work!!! export sdkman_dir=""/home/divas/.sdkman"" [[ -s ""/home/divas/.sdkman/bin/sdkman-init.sh"" ]] && source ""/home/divas/.sdkman/bin/sdkman-init.sh"" source ~/.sdkman/bin/sdkman-init.sh export hadoop_home=/opt/hadoop export path=$path:$hadoop_home/bin:$hadoop_home/sbin export java_home=$(readlink -f $(which java) | sed 's:bin/java::') thanks, divas than like comment: like comment: 1 like like comment button reply collapse expand nergiz ünal nergiz ünal nergiz ünal follow location turkey work software project manager at turkey joined mar 12, 2020 • mar 12 '20 dropdown menu copy link hide i am very thankful for your good explained entry. i have done whole instructions but i could not handle jps command part :( i cannot see namenode and datanome even i had perform things before that. can you help me for this? teşekkür ederim :) like comment: like comment: 1 like like comment button reply collapse expand sudhakar daggubati sudhakar daggubati sudhakar daggubati follow joined apr 30, 2019 • dec 23 '19 • edited on dec 23 • edited dropdown menu copy link hide hi andrew, came across your blog as i am exploring to setup a pi cluster. nice crisp write-ups; did you gave any thoughts about ansible & , vagrant to avoid manual provisions. as you are trying different aspects, ansible playbooks helps you with speed n flexibility. anyway going to keep an eye😊 thanks like comment: like comment: 2 likes like comment button reply collapse expand funso iyaju funso iyaju funso iyaju follow location abidjan work big data developer at african development bank group joined apr 18, 2020 • apr 21 '20 dropdown menu copy link hide thanks andrew for this post. like comment: like comment: 2 likes like comment button reply collapse expand kbor72 kbor72 kbor72 follow app dba location tel-aviv work mssql soulutions / consultant  at epam joined sep 22, 2020 • sep 22 '20 dropdown menu copy link hide hi andrew, grete tutorial ! thank you a lot, very useful, even today in 2020 09 21 ! like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 more from andrew (he/him) make invalid states unrepresentable # scala # programming # architecture what's the difference between nominal, structural, and duck typing? # java # typescript # javascript # computerscience eliminate unnecessary builds with git hooks in bash, java, and scala # bash # java # scala # git 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ reading list podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/awwsmm/building-a-raspberry-pi-hadoop-spark-cluster-8b2,,,"building a raspberry pi hadoop / spark cluster - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse andrew (he/him) posted on jul 8, 2019 • edited on jul 22, 2019 building a raspberry pi hadoop / spark cluster # raspberrypi # showdev # hadoop # java hadoop & spark (4 part series) 1 installing and running hadoop and spark on windows 2 big data analysis with hadoop, spark, and r shiny 3 building a raspberry pi hadoop / spark cluster 4 installing and running hadoop and spark on ubuntu 18 pro tip: if you're only looking for how to configure hadoop and spark to run on a cluster, start here . table of contents motivation and background what is apache hadoop? map-reduce and parallelisation what is apache spark? hardware about raspberry pi choosing a pi model power over ethernet (poe) and networking disk space cost and overall specs installing the poe hats software operating system networking securing the cluster hadoop & spark single-node setup cluster setup conclusion motivation and background ""big data"" has been an industry buzzword for nearly a decade now, though agreeing on what that term means and what the field of big data analytics encompasses have been points of contention. usage of big data tools like the apache software foundation's hadoop and spark (h&s) software has been met with scorn and praise alike. the truth is that, like any other tools, h&s are only helpful if you know when and how to use them appropriately. many companies use h&s for data storage and analysis. developers with these skills are in demand, but until recently, it was difficult to get the necessary experience with cluster computing without a big investment of time and/or money. the wide availability of small, cheap single-board computers like the brand-new raspberry pi 4 ($35 starting price) has all but eliminated these barriers to developing your own cluster. in this guide, i will teach you how to build a networked cluster of raspberry pis . we'll get them communicating with each other through a network switch, install hdfs, and have spark running distributed processing jobs via yarn across the whole cluster. this guide is a full-featured introduction to the hardware and software involved in setting up an h&s cluster, and can scale (just like h&s!) to any number or size of machines. what is apache hadoop? the apache hadoop distributed filesystem (hdfs) is a distributed (networked) filesystem which can be run on off-the-shelf, consumer-grade hardware. running on networked commodity hardware means that hdfs is both scalable and cost-effective. hdfs splits files into ""blocks"" (typically 64mb in size) and stores multiple copies of these blocks across different ""datanodes"" on the same rack, different racks in the same data storage centre, or across multiple data storage centres. hdfs hardware architecture diagram data split and stored in this way has several advantages: fault tolerance. redundant copies are made of each block and stored across multiple physical locations. this means that if a disk fails (as is to be expected with large volumes of commodity hardware), all of the blocks on that node can be sourced from other locations. hdfs can also make additional copies of the lost blocks so that the desired number of redundant blocks is always maintained. in older versions of hadoop, the single namenode was a potential vulnerability for the entire system. since the namenode holds all of the information about the filesystem and changes made to it, a failed namenode compromises the whole cluster. in newer releases, a cluster can have multiple namenodes , eliminating this single point of failure. parallelisation. data split into blocks is ripe for parallel processing with a map-reduce analysis pipeline. a file split into 100 blocks across 100 similarly-powered machines can be processed in roughly 1/100th of the time it would take to process the same file on a single machine. data locality. in-situ processing of blocks also eliminates the need to transfer large data files across the network. this can drastically reduce network bandwidth requirements. time and computing power that would have otherwise been spent copying files from a repository can instead be used to process the data locally: ""moving computation is cheaper than moving data"" . large datasets. because hdfs breaks files into blocks, the size of any individual file is limited only by the total amount of storage available across the network. hdfs is posix-based but relaxes some posix requirements to allow fast streaming of data , among other benefits. hdfs can support hundreds of networked nodes and tens of millions of files. hdfs speeds up data processing by distributing the i/o (read/write) disk latency across all disks on the network. instead of reading a file from a single location on a single disk, it's read simultaneously from multiple points by multiple machines. those same machines can then operate on that same file in parallel, increasing processing speeds by orders of magnitude. one point to note for hdfs is that, to facilitate data coherency, files written to hdfs are immutable . this means that, for a file to be edited, it must be downloaded from hdfs to a local file system, changed, and uploaded back to hdfs. this workflow is a bit different from what most users are accustomed to, but it is necessary for the high-throughput data access provided by hdfs. hdfs is not meant to replace your normal, interactive filesystem. it should be used as a repository for your big data, which won't change regularly, but which needs to be processed quickly and easily. it's a ""write one time, read many times"" file system. you can read much more about the nitty-gritty architectural details of hdfs here , if you're interested. map-reduce and parallelisation the distributed nature of the data stored on hdfs makes it ideal for processing with a map-reduce analysis framework. map-reduce (also ""mapreduce"", ""map-reduce"", etc.) is a programming technique where, as much as possible, parallelisable tasks are performed concurrently, followed by any non-parallelisable ""bottlenecks"". map-reduce is a general framework for analysis and is not a particular algorithm. (in big data circles, however, it is sometimes synonymous with apache's hadoop mapreduce , which is discussed in detail below.) an example map-reduce pipeline some data analysis tasks are parallelisable . for instance, if we wanted to find the most common letters among all of the words in a particular database, we might first want to count the number of letters in each word. as the frequency of letters in one word don't affect the freqency of letters in another, the two words can be counted separately. if you have 300 words of roughly equal length and 3 computers to count them, you can divvy up the database, giving 100 words to each machine. this approach is very roughly 3x as fast as having a single computer count all 300 words. note that tasks can also be parallelised across cpu cores. note: there is some overhead associated with splitting data up into chunks for parallel analysis. so if those chunks cannot be processed in parallel (if only one cpu core on one machine is available), a parallelised version of an algorithm will usually run more slowly than its non-parallelised counterpart. once each machine in the above example has analysed all of its 100 words, we need to synthesise the results. this is a non-parallelisable task. a single computer needs to add up all of the results from all of the other machines so that the results can be analysed. non-parallelisable tasks are bottlenecks , because no further analysis can even be started until they are complete. common parallelisable tasks include: filtering (ex: remove invalid or incomplete data) transformation (ex: format string data, interpret strings as numeric) streaming calculations (ex: sum, average, standard deviation, etc.) binning (ex: frequency, histogramming) common non-parallelisable tasks include: aggregation (ex: collecting partial results into a single global result) text parsing (ex: regular expressions, syntax analysis) visualisation (ex: creating summary plots) mathematical modelling (ex: linear regressions, machine learning) sorting data is an example of an algorithm which doesn't fit nicely into either of the above categories. although the entire dataset necessarily needs to be collected into one location for complete global sorting, sorting small collections of data which themselves are already locally sorted is much faster and easier than sorting the equivalent amount of unsorted data. sorting data in this way is essentially both a map and a reduce task. parallelisation is not appropriate for all tasks. some algorithms are inherently sequential (aka. p-complete ). these include n-body problems , the circuit value problem , newton's method for numerically approximating the roots of a polynomial function, and hash-chaining , which is widely used in cryptography. what is apache spark? when hdfs was first released in 2006, it was coupled with a map-reduce analysis framework called -- creatively enough -- hadoop mapreduce (usually just ""mapreduce""). both hdfs and mapreduce were inspired by research at google, and are apache counterparts to google's ""google file system"" and ""mapreduce"" , the latter of which google was granted a patent for (which has been criticised). hadoop mapreduce is the original analysis framework for working with data stored on hdfs. mapreduce executes map-reduce analysis pipelines (described above), reading data from hdfs before the ""map"" tasks, and writing the result back to hdfs after the ""reduce"" task. this behaviour in particular is one of the reason's why apache spark , widely seen as a successor to mapreduce, offers a speedup of 10-100x , relative to mapreduce. hadoop mapreduce works with the hdfs to process “process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.” [ source ] hadoop vs. spark performance on a logistic regression spark offers at least four primary advantages over mapreduce: spark minimises unnecessary disk i/o. spark offers several improvements over mapreduce in an effort to read from and write to disk as little as possible.* while mapreduce writes every intermediate result to disk , spark tries to pipeline results as much as possible, only writing to disk when the user demands it, or at the end of an analysis pipeline. spark will also cache data which is used for multiple operations in memory, so it doesn't need to be read from the disk multiple times. for these reasons, spark is sometimes said to have ""in-memory processing"". (this is a bit of a misleading term, though, as both mapreduce and spark necessarily process data in ram.) * in general, reading from and writing to a cpu's cache is an order of magnitude faster than ram, and ram is a few orders of magnitude faster than ssd (which is faster still than a conventional hard disk). spark provides abstractions for fault-tolerant processing. the primary data structure provided by spark is the resilient distributed dataset ( rdd ) : resilient -- spark keeps a lineage of how a given rdd is constructed from any ""parent"" rdds. if any rdd is corrupted or lost, it can easily be recreated from its lineage graph (aka. its logical execution plan ). distributed -- an rdd may physically exist in different pieces over several machines. spark cleanly abstracts away the distributed nature of the files stored on hdfs. the same code which reads and processes a single file stored on a single machine can be used to process a distributed file, broken into chunks and stored over many different physical locations. dataset -- rdds can store simple objects like string s and float s, or more complex objects like tuples, records, custom object s, and so on. these datasets are inherently parallelisable. rdds are immutable collections of data, so they are thread safe : they can be processed in parallel without the programmer having to worry about race conditions or other multithreading pitfalls. (the logic for making files stored on hdfs immutable is similar.) rdds are lazily evaluated . the sequence of calculations which must be traced to construct an rdd are not performed until the rdd needs to be used -- printed to the terminal, plotted, written to a file, etc. this reduces the amount of unnecessary processing that's performed by spark. spark has a different processing model. spark uses a directed acyclic graph ( dag ) processing model, rather than a simple, two-step map-reduce pipeline: diagram of a directed acyclic graph (dag) what this means is that spark takes a holistic view of the entire processing pipeline and attempts to optimise the process globally. while mapreduce will (1) read data from disk, (2) perform a ""map"" operation, (3) perform a ""reduce"" operation, (4) write data to disk, spark is more flexible about what is completed when. as long as forward progress is made toward the final result, maps and reduces may be performed in parallel or at different times on different chunks of data. the dag is a more general version of mapreduce's map-reduce pipeline -- it can also be viewed as the implementation in code of the idealised lineage graph of an rdd. spark eliminates jvm boot times. by booting up a java virtual machine (jvm) on each datanode at startup (as opposed to when a new job is executed), spark eliminates the time required to load *.jar s, parse any configuration files, and so on. mapreduce opens a new jvm each time a new task is run, and all of this startup overhead is incurred on every job. it may only take a few seconds each time, but it adds up. additionally, spark has built-in support for complex analytics like machine learning ( mllib ) and graph processing ( graphx ) and can be used interactively in the python, r, scala, java, and sql shells the above features make spark much faster, more fault-tolerant, and more feature-rich than mapreduce. with all of the benefits spark has over mapreduce, essentially the only time you should prefer to use the latter is when you have legacy mapreduce code to maintain. or if you don't care about processing time. or if you don't want to spend time learning spark. there are other distributed data storage / distributed analysis pipelines, but apache hadoop and spark are free, fast, fault-tolerant, and relatively easy to install and understand. equipped with this foundational understanding of h&s, we can now move on to constructing our own raspberry pi h&s cluster. [ back to top ] hardware about raspberry pi the raspberry pi foundation have been developing and releasing single-board computers since early 2012. originally conceived of as a way to inspire kids to get into electronics and programming , like the diy ""kit"" computers of the 1970s and 80s, the popularity of the raspberry pi has far exceeded the expectations of the foundation's creators , selling 12.5 million units to become the third most popular personal computer ever , beating the commodore 64, but still trailing the apple macintosh and the windows pc. yes, raspberry pis have been used to introduce a new generation to computing, but they're also used for home automation, manufacturing, robotics, digital cameras, and more . raspberry pi comparison chart (click to open pdf) there are many different models of raspberry pi available today, and they fall into four families: model a, model b, zero, and compute. the model b family are the flagship, full-featured raspberry pis. model bs all have ethernet connectivity, which no other family of pi has. every model b has an msrp (manufacturer's suggested retail price) of $35 us. model bs all have the exact same form factor: 85.6mm x 56.5mm x 17.0mm, weighing in at only 17g. model b pis are literally credit-card-sized! and about as ""thick"" as a keyboard key is wide. they're roughly the size and weight of a deck of cards. raspberry pi 3 b+ diagram there are different versions of raspberry pi, as well. version 1 included a model b as well as a model b+, which saw the inclusion of two additional usb ports, a switch from sd to microsd storage, and a change in the gpio (general-purpose input/output) header on the board which allowed for the addition of pi hats (hardware attached on top). hats connect to the gpio pins and add additional features to the pi: allowing poe (power over ethernet), cooling fans, lcd screens, and more. version 1 also included a model a and a+. the a series are physically smaller, cheaper ($20-$25), and have reduced specs (only 1 usb port, no ethernet, less ram). model zero pis are even more stripped-down and even cheaper ($5-$10). details on all of these models of pi can be seen in the table above, or found in various comparison tables online. choosing a pi model we want to network our pis to build a hadoop cluster, so we are restricted to models which have either wi-fi and/or ethernet hardware. of the pis which are currently available, this limits us to the pi 3 model b, pi 3 model b+, pi 3 model a+, or the pi zero wireless. all of these pis have wifi connectivity, but only the model b and b+ have ethernet. note that the version 4 pis were just released a few weeks ago, but they're selling out as quickly as stores can stock them. if you can, i recommend using the version 4 pis over the version 3 ones. they have faster processors and the ability to add more ram (up to 4gb). raspberry pi models if we network the pis using wifi, we'll need a wireless router. we'll also need to buy power supplies for each pi. i'm going to build an eight-pi cluster. eight power supplies will be bulky. i'll have to carry around all of these, plus a multi-outlet, if i want to show off my cluster to anyone. to eliminate this bulk, we can instead power them through poe. this takes care of both the networking and the power supply for each pi, but it is more expensive. this choice restricts me to only the model b or the model b+. finally, to eliminate any possible network latency with the pis (and since all models b/b+ are the same price) i'm going to chose the pi 3 model b+, which has the largest bandwidth ethernet support (~300mbit/s). for my 8-pi cluster, the total cost of the pis is about $280 . power over ethernet (poe) and networking raspberry pis model b+ can be powered via a traditional dc power supply (micro-usb). these  cost about $8 each (so $64 total for my 8-pi cluster) and would require each pi to have access to its own electrical outlet. you can buy a multi-outlet (""power strip"") online for about $9 . alternatively, pis b+ can also be powered via ethernet, through a technology known, creatively, as power over ethernet, or poe. poe requires a network switch which supports poe; these are generally more expensive than those which don't support poe. non-poe, 8-port network switches can be bought for as little as $10 on amazon , and gigabit (1000mbps) switches are available for as little as $15. an 8-port network switch with poe support on all ports runs about $75 . networking via ethernet requires 8 short ethernet cables ( about $14 for a 10-pack ). if you opt for the non-poe route, you could also network your pis with a good, cheap wireless router ( about $23 on amazon). at minimum, then, we're looking at $64 + $9 + $10 + $14 = $97 in networking and power supply costs for a wired (ethernet) cluster with dc power supplies, or $64 + $9 + $23 = $96 in networking and supply costs for a wifi cluster with dc power supplies. raspberry pi cluster using power over ethernet the cluster i'm building will be used for demonstrations, so it needs to be portable. eight power supplies plus a multi-outlet is not ideal, so i opted for the poe option. this eliminates all of that extra bulk, but it comes at a cost. a poe pi cluster requires 8 poe hats ( about $21 each ), plus the ethernet cables (10 for $14), plus a poe-enabled, 8-port network switch (about $75). this means that a poe pi cluster will incur at least $257 in networking and power supply costs, or over 2.5x as much as a wireless cluster. c4labs cloudlet cluster case, holding a raspberry pi cluster i have one more small complication, too. i bought a special carrying case from c4labs for my pi cluster ($55), so the only network switch i could order from amazon uk (due to space restrictions) is the trendnet v1.1r , at a cost of about $92. so my total networking and power supply cost (plus this case) is $329 . note that none of the above quoted prices include shipping. also, the c4labs case comes with cooling fans which are meant to draw their power from the gpio 5v pins on the pis. as these will be occupied by the poe hat, i bought some usb jacks and a soldering iron so i can solder the cooling fan wires to the usb jacks. the fans can then draw their power from the pis via usb. these two purchases added about another $32 to the cluster cost. disk space raspberry pis don't come with on-board storage space. they do come with microsd slots, though, so microsd cards can be used to hold the os and data for the pi. high-rated micro sd cards tend to cost around $17.50 for 128gb models , though most of these limit read speed to about 80mb/s. as of this writing, the cheapest 128gb sd cards with read speeds of up to 100mb/s cost around $21 each or $168 for the entire cluster. cost and overall specs in summary, the total cost of my 8-pi cluster is roughly $280 for all eight raspberry pis model b+ $274 for all of the power supply and networking paraphernalia $168 for all of the microsd cards $55 for a nice carrying case $32 for usb jacks and a soldering iron ...or about $810 total. shipping all of this stuff to ireland cost about another ~$125, bringing the total cost of my cluster to about $935. note that this doesn't include the cost of a keyboard, mouse, monitor, or hdmi cable, all of which i borrowed from other projects. selected specs of this 8-machine cluster are as follows: cpu: 8 x 1.4ghz 64-bit quad-core arm cortex-a53 processors ram: 8 x 1gb lpddr2 sdram (max 1066 mbps data rate) ethernet: gigabit ethernet (max 300 mbps), poe support storage: 8 x 128gb microsd storage (max 80mb/s read speed) ports: 8 x hdmi ports, 8 x 4 x usb 2.0 ports not too bad for under $1000! installing the poe hats for the most part, raspberry pis are plug-and-play. but since we're installing a hat, we have to do a tiny bit of hardware work. the raspberry pi website gives instructions here for how to install the poe hats on top of a raspberry pi -- it's pretty easy: locate the 40-pin gpio header and the nearby 4-pin poe header on the top of the raspberry pi, and the corresponding 40-pin and 4-pin slots on the underside of the poe hat, but don't put anything together yet . when the poe hat is flipped right-side-up, the pins on the pi should align with the slots on the hat: locate the spacers and screws that came packaged with the poe hat. there should be at least 4 spacers and at least 8 screws. screw the spacers to the hat from the top. at this point, if you are only assembling a single pi, you'll  attach the pi to the hat by aligning the spacers to the holes on the pi and screwing into the spacers from the underside of the pi, while making sure to carefully insert the pi gpio and poe pins into the gpio and poe slots on the hat. for my cluster, however, i'm assembling all of the pis within a c4labs cloudlet / cluster case, so the pis need to be attached to the plexiglas mounts. i remove the protective paper from the plexiglas, and add the metal spacers and screws as shown in the following photograph: make sure to use the longer screws, because they need to make it through the plexiglas, past the metal spacers, through the pi circuit board, and into the poe hat spacers: once everything is screwed together, the pi should be securely attached to the plexiglas mount: ...that's it! poe hat successfully installed. be very careful if you attempt to take the poe hat off after you've pushed it down onto the pi pins. it is very easy to wrench those pins off of the circuit board while trying to remove the poe hat. if that happens, you'll have to solder the pins back onto the pi or switch to using a microusb power supply instead (or buy a new pi). [ back to top ] software operating system there are dozens of operating systems available for the raspberry pi . some of the more notable ones include: raspbian -- a debian-based os developed and maintained by the raspberry pi foundation (the foundation's recommended os for the system) windows iot core -- windows' internet of things (iot) operating system, for small devices; looks and feels similar to windows 10 ubuntu core -- a stripped-down version of one of the most popular linux distributions, ubuntu android -- the most popular operating system in the world; installed on over 2 billion devices, mostly mobile phones raspbsd -- an offshoot of the berkeley software distribution (bsd) of unix for the raspberry pi; macos is also bsd-based chromium os -- google's browser-based operating system retropie -- an os for turning your raspberry pi into a retro-gaming machine (no copyrighted games allowed!) risc os pi -- a pi-optimised version of risc os , an operating system developed specifically for reduced instruction set chips (risc), like the amd series used in raspberry pi as fun as it would be to install retropie, i think i'll stick with raspbian, as it's the os that's recommended by the raspberry pi foundation, and has been specially optimised for the low-performance hardware of the raspberry pi. raspbian's noobs (new out of the box software) installer is the easiest way to get raspbian onto a raspberry pi. just download it, and follow the instructions listed on raspberry pi's website. you'll need to format the microsd card as fat ( not exfat !) , extract the noobs *.zip archive, and copy the files within it to your newly-formatted microsd card. then, insert the microsd card into the pi; there's a slot on the underside of the pi, on the side opposite the usb ports: power the pi by plugging an ethernet cable into it, with the other end of the cable in the poe-enabled network switch. plug in the network switch and turn it on. after a second or two, the pi should power up: the yellow light next to the ethernet port, and the red light near the microsd card should both light up. then, connect the pi to a monitor using the hdmi port on the side opposite the gpio header. connect a mouse and a keyboard to the pi via usb, and follow the on-screen instructions in the os install wizard (it should look similar to the screenshot below). noobs installer on a raspberry pi once you've successfully installed your os of choice on a single pi, you can simply clone the microsd card to install the same os on the other pis. there's no need to go through the install wizard multiple times. later on in this tutorial, i'll explain how to easily run a particular command (including installing software with apt ) simultaneously across all pis on the cluster, so you don't need to repeat the same manual tasks over and over like a monkey at a typewriter -- this is why we have technology, isn't it? networking configuring static ip addresses to facilitate easy networking of the pis, i'm going to set static ip addresses for each pi on the network switch. i'll number the pis 1-8 (inclusive) according to their positions on the network switch and in the carrying case. when looking at the ports on the pis (the ""front"" of the case), #1 is the rightmost pi and #8 is the leftmost pi. to enable user-defined, static ip addresses, i edit the file /etc/dhcpcd.conf on each pi and uncomment / edit the lines: interface eth0
static ip_address=192.168.0.10x/24 enter fullscreen mode exit fullscreen mode ...where x should be replaced by 1 for pi #1, 2 for pi #2, etc. after this change has been made on a particular pi, i reboot the machine. once this is done for all eight pis, they should all be able to ping each other at those addresses. i've also installed nmap on pi #1 so that the status of all eight pis can be easily checked: $ sudo nmap –sp 192.168.0.0/24 enter fullscreen mode exit fullscreen mode ...will show the status of all seven other pis, not including the one on which the command is run. the "" n hosts up "" shows how many pis are properly configured according to the above, currently connected, and powered on. enabling ssh to enable ssh on each pi, we need to follow these instructions (reproduced here to avoid link rot): as of the november 2016 release, raspbian has the ssh server disabled by default. it can be enabled manually from the desktop: launch raspberry pi configuration from the preferences menu navigate to the interfaces tab select enabled next to ssh click ok alternatively, raspi-config can be used in the terminal: enter sudo raspi-config in a terminal window select interfacing options navigate to and select ssh choose yes select ok choose finish alternatively, use systemctl to start the service $ sudo systemctl enable ssh $ sudo systemctl start ssh when enabling ssh on a pi that may be connected to the internet, you should change its default password to ensure that it remains secure. see the security page for more details. hostnames initially, all of the pis are known as raspberrypi , and have a single user, pi : $ hostname raspberrypi $ whoami pi enter fullscreen mode exit fullscreen mode this has the potential to become very confusing if we're constantly moving back and forth between the different pis on the network. to simplify this, we're going to assign each pi a hostname based on its position in the case / on the network switch. pi #1 will be known as pi1 , pi #2 as pi2 , and so on. to accomplish this, two files must be edited: /etc/hosts and /etc/hostname . within those files, there should be only one occurrence each of raspberrypi , which is the default hostname. we change each of those to pix where x is the appropriate number 1-8. finally, in /etc/hosts only, we also add the ips for all the other pis at the end of the file, like: 192.168.0.101 pi1
192.168.0.102 pi2
...
192.168.0.108 pi8 enter fullscreen mode exit fullscreen mode this must be done manually on each pi. once the above tasks are finished for the appropriate pi, we reboot that particular pi. now, when the terminal is opened, instead of: pi@raspberrypi:~ $ enter fullscreen mode exit fullscreen mode ...you should see pi@pix:~ $ enter fullscreen mode exit fullscreen mode ...where x is the index of that pi on the cluster. we do this on each pi and reboot each of them after it's been done. from now on in these instructions, the command prompt will be abbreviated to simply $ . any other code is output or the text of a file. simplifying ssh to connect from one pi to another, having followed only the above instructions, would require the following series of commands: $ ssh pi@192.168.0.10x
pi@192.168.0.10x 's password: <enter password – ' raspberry ' default> enter fullscreen mode exit fullscreen mode granted, this is not too much typing, but if we have to do it very often, it could become cumbersome. to avoid this, we can set up ssh aliases and passwordless ssh connections with public/private key pairs. ssh aliases to set up an ssh alias, we edit the ~/.ssh/config file on a particular pi and add the following lines: host pix
user pi
hostname 192.168.0.10x enter fullscreen mode exit fullscreen mode ...replacing x with 1-8 for each of the eight pis. note that this is done on a single pi, so that one pi should have eight chunks of code within ~/.ssh/config , which look identical to the above except for the x character, which should change for each pi on the network. then, the ssh command sequence becomes just: $ ssh pix
pi@192.168.0.10x 's password: <enter password> enter fullscreen mode exit fullscreen mode this can be simplified further by setting up public/private key pairs. public/private key pairs on each pi, run the following command: $ ssh-keygen –t ed25519 enter fullscreen mode exit fullscreen mode this will generate a public / private key pair within the directory ~/.ssh/ which can be used to securely ssh without entering a password. one of these files will be called id_ed25519 , this is the private key . the other, id_ed25519.pub is the public key . no passphrase is necessary to protect access to the key pair. the public key is used to communicate with the other pis, and the private key never leaves its host machine and should never be moved or copied to any other device. each public key will need to be concatenated to the ~/.ssh/authorized_keys file on every other pi. it's easiest to do this once, for a single pi, then simply copy the authorized_keys file to the other pis. let's assume that pi #1 will contain the ""master"" record, which is then copied to the other pis. on pi #2 (and #3, #4, etc.), run the following command: $ cat ~/.ssh/id_ed25519.pub | ssh pi@192.168.0.101 'cat >> .ssh/authorized_keys' enter fullscreen mode exit fullscreen mode this concatenates pi #2's public key file to pi #1's list of authorized keys, giving pi #2 permission to ssh into pi #1 without a password (the public and private keys are instead used to validate the connection). we need to do this for each machine, concatenating each public key file to pi #1's list of authorized keys. we should also do this for pi #1, so that when we copy the completed authorized_keys file to the other pis, they all have permission to ssh into pi #1, as well. run the following command on pi #1: $ cat .ssh/id_ed25519.pub >> .ssh/authorized_keys enter fullscreen mode exit fullscreen mode once this is done, as well as the previous section, ssh -ing is as easy as: $ ssh pi1 enter fullscreen mode exit fullscreen mode ...and that's it! additional aliases can be configured in the ~/.bashrc file to shorten this further (see below) though this is not configured on our system: alias p1 = ""ssh pi1"" # etc. enter fullscreen mode exit fullscreen mode replicate the configuration finally, to replicate the passwordless ssh across all pis, simply copy the two files mentioned above from pi #1 to each other pi using scp : $ scp ~/.ssh/authorized_keys pix:~/.ssh/authorized_keys $ scp ~/.ssh/config pix:~/.ssh/config enter fullscreen mode exit fullscreen mode you should now be able to ssh into any pi on the cluster from any other pi with just ssh pix . ease of use finally, a few miscellaneous ease-of-use-type enhancements for the cluster. get the hostname of every pi except this one to get the hostname of a pi, you can just use: $ hostname pi1 enter fullscreen mode exit fullscreen mode ...to get the hostname of all other pis on the cluster, define the function: function otherpis { grep ""pi"" /etc/hosts | awk '{print $2}' | grep -v $( hostname ) } enter fullscreen mode exit fullscreen mode the -v flag tells grep to invert the selection. only lines which don't match the hostname are returned. ...in your ~/.bashrc , then, source your ~/.bashrc file with: $ source ~/.bashrc enter fullscreen mode exit fullscreen mode (note that, whenever you edit ~/.bashrc , for those changes to take effect, you must source the file or log out and log back in.) then, you can call the new function on the command line with: $ otherpis
pi2
pi3
pi4
... enter fullscreen mode exit fullscreen mode note that this function relies on you having listed all of the ips and hostnames of the pis within the /etc/hosts file. send the same command to all pis to send the same command to each pi, add the following function to the ~/.bashrc of the pi from which you want to dictate commands (i add this to pi #1's ~/.bashrc and then copy the ~/.bashrc file to all other pis using the instructions below): function clustercmd { for pi in $( otherpis ) ; do ssh $pi "" $@ "" ; done $@ } enter fullscreen mode exit fullscreen mode this will run the given command on each other pi, and then on this pi: $ clustercmd date tue apr  9 00:32:41 ist 2019
tue apr  9 05:58:07 ist 2019
tue apr  9 06:23:51 ist 2019
tue apr  9 23:51:00 ist 2019
tue apr  9 05:58:57 ist 2019
tue apr  9 07:36:13 ist 2019
mon apr  8 15:19:32 ist 2019
wed apr 10 03:48:11 ist 2019 enter fullscreen mode exit fullscreen mode ...we can see above that all of our pis have different system times. let's fix that. synchronise time across cluster simply tell each pi to install the package htpdate and the date will be updated. that's it: $ clustercmd ""sudo apt install htpdate -y > /dev/null 2>&1"" $ clustercmd date wed jun 19 16:04:22 ist 2019 
wed jun 19 16:04:18 ist 2019 
wed jun 19 16:04:19 ist 2019
wed jun 19 16:04:20 ist 2019 
wed jun 19 16:04:48 ist 2019 
wed jun 19 16:04:12 ist 2019 
wed jun 19 16:04:49 ist 2019
wed jun 19 16:04:25 ist 2019 enter fullscreen mode exit fullscreen mode notice how all the dates and times are accurate to within a minute now. if we reboot all of the pis, they'll be even more closely aligned: $ clustercmd date wed jun 19 16:09:28 ist 2019 
wed jun 19 16:09:27 ist 2019 
wed jun 19 16:09:29 ist 2019
wed jun 19 16:09:31 ist 2019 
wed jun 19 16:09:28 ist 2019 
wed jun 19 16:09:30 ist 2019 
wed jun 19 16:09:24 ist 2019
wed jun 19 16:09:28 ist 2019 enter fullscreen mode exit fullscreen mode they're now aligned to within 10 seconds. note that it takes a few seconds to run the command across the cluster. to precisely synchronise all clocks to a remote server (a respected standard like time.nist.gov or google.com ) you can do: $ clustercmd sudo htpdate -a -l time.nist.gov enter fullscreen mode exit fullscreen mode ...this will take a few minutes, because the program slowly adjusts the clock in intervals of < 30ms so there are no ""jumps"" in any system file timestamps. once this command finishes, the clocks will be synchronised (remember, it takes a second or two to communicate across the network, so they're still ""off"" by two or three seconds): $ clustercmd date wed jun 19 16:36:46 ist 2019 
wed jun 19 16:36:47 ist 2019 
wed jun 19 16:36:47 ist 2019
wed jun 19 16:36:48 ist 2019 
wed jun 19 16:36:49 ist 2019 
wed jun 19 16:36:49 ist 2019 
wed jun 19 16:36:49 ist 2019
wed jun 19 16:36:49 ist 2019 enter fullscreen mode exit fullscreen mode reboot the cluster i add the following function to my ~/.bashrc on pi #1: function clusterreboot { clustercmd sudo shutdown -r now } enter fullscreen mode exit fullscreen mode this function makes it easy to reboot the entire cluster. another one lets me shut down the entire cluster without rebooting: function clustershutdown { clustercmd sudo shutdown now } enter fullscreen mode exit fullscreen mode send the same file to all pis to copy a file from one pi to all others, let's add a function called clusterscp (cluster secure copy) to the ~/.bashrc file of any particular pi: function clusterscp { for pi in $( otherpis ) ; do cat $1 | ssh $pi ""sudo tee $1 "" > /dev/null 2>&1 done } enter fullscreen mode exit fullscreen mode then we can copy all of the ease-of-use functions we've defined in ~/.bashrc to every other pi on the cluster: $ source ~/.bashrc && clusterscp ~/.bashrc enter fullscreen mode exit fullscreen mode securing the cluster you might only be building a hadoop cluster for fun, but i'm building one to do some data analysis for work. since my cluster is going to hold proprietary data, cluster security is paramount. there is a lot we can do to secure a computer against unauthorised access. the easiest (and most effective) thing that can be done (after setting up passwordless ssh following the instructions in the above section ""simplifying ssh "") is to disable password-based authentication. this means that users can only log into the system if they have a public key corresponding to one of the private keys on one of the pis. while it is possible (in theory) to crack these keys, ed25519 is the most secure public/private key pair algorithm to date and will likely not be breakable within the foreseeable future. disable password-based authentication & root login to disable password-based authentication, we edit the file /etc/ssh/sshd_config . there are 6 options we want to change in particular. search for the following keys in this file and change them so that they match the examples below. make sure that none of the below lines begins with a ' # ', this comments out the line so that it is ignored by the system. permitrootlogin no
passwordauthentication no
challengeresponseauthentication no
usepam no
x11forwarding no
printmotd no enter fullscreen mode exit fullscreen mode the above commands will, respectively: prevent hackers from trying to login as root (so they'll need to know that the username is pi ); disallow password-based authentication (so they'll need the corresponding ed25519 public key to the particular private key on a given pi); disable x11 (gui) forwarding among / from / to the pis (everything must be done command-line); and disable the ""message of the day"" (motd), which can sometimes include compromising information. edit /etc/ssh/sshd_config and make the above changes on a particular pi, then copy those changes to all other pis with... warning: be extremely careful with the following steps. if you make a typo in sshd_config , then restart the ssh service on a pi, it will throw an error and you will not be able to ssh into that pi any more. you will need to manually (connect the hdmi cable to the video output) connect to a pi to reconfigure! $ clusterscp /etc/ssh/sshd_config enter fullscreen mode exit fullscreen mode ...and restart the ssh service (reload the configuration) across all pis with: $ clustercmd sudo service ssh restart enter fullscreen mode exit fullscreen mode to remove the motd entirely, delete the file /etc/motd : $ clustercmd sudo rm /etc/motd enter fullscreen mode exit fullscreen mode monitor ssh activity with fail2ban another big thing we can do to monitor and protect the integrity of the cluster is install a program called fail2ban . fail2ban ""logs and temporarily ban[s] ips based on possible malicious activity"". to install this program on every machine in the cluster, run the command: $ clustercmd sudo apt install fail2ban –y enter fullscreen mode exit fullscreen mode then, copy the configuration file on a single pi: $ cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local enter fullscreen mode exit fullscreen mode ...and edit the lines within jail.local near the spot that starts with [sshd] . note that [sshd] appears near the top of the file in a commented block – ignore that. [sshd] should be configured as follows: [sshd]
enabled = true
port    = ssh
logpath = %(sshd_log)s
backend = %(sshd_backend)s enter fullscreen mode exit fullscreen mode once you've finished editing jail.local , copy it to all other pis on the cluster with: $ clusterscp /etc/fail2ban/jail.local enter fullscreen mode exit fullscreen mode ...and restart the fail2ban service on all pis with: $ clustercmd sudo service fail2ban restart enter fullscreen mode exit fullscreen mode [ back to top ] hadoop & spark single-node setup hadoop apache hadoop v2.7+ requires java 7+ to run. when i installed raspbian on my pis, i used noobs v3.0.1 (2019-04-08). this installs raspbian and also java 1.8.0_65 (java 8 by hotspot aka. oracle). so all pis have an acceptable version of java installed by default. next, we can download and install hadoop. i'll start by building a single-node setup on the master node (pi #1), then i'll the worker nodes to create a multi-node cluster. on pi #1, get hadoop with the commands: $ cd && wget https://bit.ly/2wa3hty enter fullscreen mode exit fullscreen mode (this is a shortened link to hadoop-3.2.0.tar.gz ) $ sudo tar -xvf 2wa3hty -c /opt/ $ rm 2wa3hty && cd /opt $ sudo mv hadoop-3.2.0 hadoop enter fullscreen mode exit fullscreen mode then, make sure to change the permissions on this directory: $ sudo chown pi:pi -r /opt/hadoop enter fullscreen mode exit fullscreen mode finally, add this directory to the $path by editing ~/.bashrc and putting the following lines at the end of the file: export java_home = $( readlink –f /usr/bin/java | sed ""s:bin/java::"" ) export hadoop_home = /opt/hadoop export path = $path : $hadoop_home /bin: $hadoop_home /sbin enter fullscreen mode exit fullscreen mode ...and edit /opt/hadoop/etc/hadoop/hadoop-env.sh to add the following line: export java_home = $( readlink –f /usr/bin/java | sed ""s:bin/java::"" ) enter fullscreen mode exit fullscreen mode you can verify that hadoop has been installed correctly by checking the version: $ cd && hadoop version | grep hadoop
hadoop 3.2.0 enter fullscreen mode exit fullscreen mode spark we will download spark in a similar manner to how we downloaded hadoop, above: $ cd && wget https://bit.ly/2hk6ntw enter fullscreen mode exit fullscreen mode (this is a shortened link to spark-2.4.3-bin-hadoop2.7.tgz ) $ sudo tar –xvf 2hk6ntw –c /opt/ $ rm 2hk6ntw && cd /opt $ sudo mv spark-2.4.3-bin-hadoop2.7 spark enter fullscreen mode exit fullscreen mode then, make sure to change the permissions on this directory: $ sudo chown pi:pi -r /opt/spark enter fullscreen mode exit fullscreen mode finally, add this directory to your $path by editing ~/.bashrc and putting the following lines at the end of the file: export spark_home = /opt/spark export path = $path : $spark_home /bin enter fullscreen mode exit fullscreen mode you can verify that spark has been installed correctly by checking the version: $ cd && spark-shell --version ... version 2.4.3 ... using scala version 2.11.12 ... enter fullscreen mode exit fullscreen mode hdfs to get the hadoop distributed file system (hdfs) up and running, we need to modify some configuration files. all of these files are within /opt/hadoop/etc/hadoop . the first is core-site.xml . edit it so it looks like the following: <configuration> <property> <name> fs.defaultfs </name> <value> hdfs://pi1:9000 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode the next is hdfs-site.xml , which should look like: <configuration> <property> <name> dfs.datanode.data.dir </name> <value> file:///opt/hadoop_tmp/hdfs/datanode </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> file:///opt/hadoop_tmp/hdfs/namenode </value> </property> <property> <name> dfs.replication </name> <value> 1 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode ...this configures where the datanode and namenode information is stored and also sets the replication (the number of times a block is copied across the cluster) to 1 (we will change this later). make sure that you also create these directories with the commands: $ sudo mkdir -p /opt/hadoop_tmp/hdfs/datanode $ sudo mkdir -p /opt/hadoop_tmp/hdfs/namenode enter fullscreen mode exit fullscreen mode ...and adjust the owner of these directories: $ sudo chown pi:pi -r /opt/hadoop_tmp enter fullscreen mode exit fullscreen mode the next file is mapred-site.xml , which should look like: <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> enter fullscreen mode exit fullscreen mode ...and finally yarn-site.xml , which should look like: <configuration> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.auxservices.mapreduce.shuffle.class </name> <value> org.apache.hadoop.mapred.shufflehandler </value> </property> </configuration> enter fullscreen mode exit fullscreen mode once these four files are edited, we can format the hdfs ( warning: do not do this if you already have data in the hdfs! it will be lost! ): $ hdfs namenode -format -force enter fullscreen mode exit fullscreen mode ...we then boot the hdfs with the following two commands: $ start-dfs.sh $ start-yarn.sh enter fullscreen mode exit fullscreen mode ...and test that it's working by creating a temporary directory: $ hadoop fs -mkdir /tmp $ hadoop fs -ls /
found 1 items
drwzr-xr-x   - pi supergroup          0 2019-04-09 16:51 /tmp enter fullscreen mode exit fullscreen mode ...or by running the command jps : $ jps
2736 namenode
2850 datanode
3430 nodemanager
3318 resourcemanager
3020 secondarynamenode
3935 jps enter fullscreen mode exit fullscreen mode this shows that the hdfs is up and running, at least on pi #1. to check that spark and hadoop are working together, we can do: $ hadoop fs -put $spark_home /readme.md / $ spark-shell enter fullscreen mode exit fullscreen mode ...this will open the spark shell, with prompt scala> : scala > val textfile = sc . textfile ( ""hdfs://pi1:9000/readme.md"" ) ... scala > textfile . first () res0 : string = # apache spark enter fullscreen mode exit fullscreen mode hide execstack warning while running the hadoop commands above, you may get a warning like... ""you have loaded library... which might have disabled stack guard."" ...the reason this happens on our raspberry pis is because of a mismatch between the 32-bit runtime the hadoop binaries were built for, and the 64-bit raspbian version we're running. to ignore these warnings, change the line # export hadoop_opts=""-djava.net.preferipv4stack=true"" enter fullscreen mode exit fullscreen mode ...in /opt/hadoop/etc/hadoop/hadoop-env.sh to export hadoop_opts = ""-xx:-printwarnings –djava.net.preferipv4stack=true"" enter fullscreen mode exit fullscreen mode this will hide the warnings in the future. (alternatively, you could also download the hadoop source code and build from scratch.) hide nativecodeloader warning another warning you may see is that util.nativecodeloader is "" unable to load [the] native-hadoop library for your platform "". this warning cannot be resolved easily. it's due to the fact that hadoop is compiled for 32-bit architectures and the version of raspbian we have is 64-bit. we could recompile the library from scratch on the 64-bit machine, but i'd rather just not see the warning. to do this, we can add the following lines to the bottom of our ~/.bashrc file: export hadoop_home_warn_suppress = 1 export hadoop_root_logger = ""warn,drfa"" enter fullscreen mode exit fullscreen mode this will prevent those nativecodeloader warnings from being printed. cluster setup at this point, you should have a single-node cluster and that single node acts as both a master and a worker node. to set up the worker nodes (and distribute computing to the entire cluster), we must take the following steps... create the directories create the required directories on all other pis using: $ clustercmd sudo mkdir -p /opt/hadoop_tmp/hdfs $ clustercmd sudo chown pi:pi –r /opt/hadoop_tmp $ clustercmd sudo mkdir -p /opt/hadoop $ clustercmd sudo chown pi:pi /opt/hadoop enter fullscreen mode exit fullscreen mode copy the configuration copy the files in /opt/hadoop to each other pi using: $ for pi in $( otherpis ) ; do rsync –avxp $hadoop_home $pi :/opt ; done enter fullscreen mode exit fullscreen mode this will take quite a long time, so go grab lunch. when you're back, verify that the files copied correctly by querying the hadoop version on each node with the following command: $ clustercmd hadoop version | grep hadoop
hadoop 3.2.0
hadoop 3.2.0
hadoop 3.2.0
... enter fullscreen mode exit fullscreen mode configuring hadoop on the cluster it's difficult to find a good guide for installing hadoop across a networked cluster of machines. this link points to the one i followed, mostly without incident. to get hdfs running across the cluster, we need to modify the configuration files that we edited earlier. all of these files are within /opt/hadoop/etc/hadoop . the first is core-site.xml . edit it so it looks like the following: <configuration> <property> <name> fs.default.name </name> <value> hdfs://pi1:9000 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode the next is hdfs-site.xml , which should look like: <configuration> <property> <name> dfs.datanode.data.dir </name> <value> /opt/hadoop_tmp/hdfs/datanode </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/hadoop_tmp/hdfs/namenode </value> </property> <property> <name> dfs.replication </name> <value> 4 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode the next file is mapred-site.xml , which should look like: <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> <property> <name> yarn.app.mapreduce.am.resource.mb </name> <value> 256 </value> </property> <property> <name> mapreduce.map.memory.mb </name> <value> 128 </value> </property> <property> <name> mapreduce.reduce.memory.mb </name> <value> 128 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode ...and finally yarn-site.xml , which should look like: <configuration> <property> <name> yarn.acl.enable </name> <value> 0 </value> </property> <property> <name> yarn.resourcemanager.hostname </name> <value> pi1 </value> </property> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.auxservices.mapreduce.shuffle.class </name> <value> org.apache.hadoop.mapred.shufflehandler </value> </property> <property> <name> yarn.nodemanager.resource.memory-mb </name> <value> 900 </value> </property> <property> <name> yarn.scheduler.maximum-allocation-mb </name> <value> 900 </value> </property> <property> <name> yarn.scheduler.minimum-allocation-mb </name> <value> 64 </value> </property> <property> <name> yarn.nodemanager.vmem-check-enabled </name> <value> false </value> </property> </configuration> enter fullscreen mode exit fullscreen mode make these changes to these files, then remove all old files from all pis. i'm assuming you're working your way through this tutorial step-by-step and haven't yet added any big, important data to the single-node pi #1 cluster. you can clean up all the pis with: $ clustercmd rm –rf /opt/hadoop_tmp/hdfs/datanode/ * $ clustercmd rm –rf /opt/hadoop_tmp/hdfs/namenode/ * enter fullscreen mode exit fullscreen mode if you don't do this , you may have errors with the pis not recognizing each other. if you can't get the datanodes (pi #2-#8) to start, or communicate with the namenode (pi #1), cleaning the files above might be the solution (it was for me). next, we need to create two files in $hadoop_home/etc/hadoop/ which tell hadoop which pis to use as worker nodes and which pi should be the master (namenode) node. create a file named master in the aforementioned directory and add only a single line: pi1 enter fullscreen mode exit fullscreen mode then, create a file named workers ( not slaves , as was the case in previous versions of hadoop) in the same directory and add all of the other pis: pi2
pi3
pi4
... enter fullscreen mode exit fullscreen mode then, you'll need to edit /etc/hosts again. on any pi, remove the line which looks like 127.0.0.1 pix enter fullscreen mode exit fullscreen mode ...where x is the index of that particular pi. then, copy this file to all other pis with: $ clusterscp /etc/hosts enter fullscreen mode exit fullscreen mode we can do this now because this file is no longer pi-specific. finally, reboot the cluster for these changes to take effect. when all pis have rebooted, on pi #1, run the command: warning: do not do this if you already have data in the hdfs! it will be lost! $ hdfs namenode -format -force enter fullscreen mode exit fullscreen mode ...we then boot the hdfs with the following two commands: $ start-dfs.sh && start-yarn.sh enter fullscreen mode exit fullscreen mode we can test the cluster by putting files in hdfs from any pi (using hadoop fs -put ) and making sure they show up on other pis (using hadoop fs -ls ). you can also check that the cluster is up and running by opening a web browser and navigating to http://pi1:9870 . this web interface gives you a file explorer as well as information about the health of the cluster. hadoop web ui running on port 9870. hadoop web ui showing datanode statistics. file browser in hadoop web ui. configuring spark on the cluster spark will run fine on a single machine, so we may trick ourselves into thinking we're using the full power of the hadoop cluster when in reality we're not. some of the configuration we performed above was for hadoop yarn (yet another resource negotiator). this is a ""resource negotiator"" for hdfs, which orchestrates how files are moved and analysed around the cluster. for spark to be able to communicate with yarn, we need to configure two more environment variables in pi #1's ~/.bashrc . previously, we defined export spark_home = /opt/spark export path = $path : $spark_home /bin enter fullscreen mode exit fullscreen mode ...in ~/.bashrc . just beneath this, we will now add two more environment variables: export hadoop_conf_dir = $hadoop_home /etc/hadoop export ld_library_path = $hadoop_home /lib/native: $ld_library_path enter fullscreen mode exit fullscreen mode $hadoop_conf_dir is the directory which contains all of the *-site.xml configuration files that we edited above. next, we create the spark configuration file: $ cd $spark_home /conf $ sudo mv spark-defaults.conf.template spark-defaults.conf enter fullscreen mode exit fullscreen mode ...and we add the following lines to the end of this file: spark.master            yarn
spark.driver.memory     465m
spark.yarn.am.memory    356m
spark.executor.memory   465m
spark.executor.cores    4 enter fullscreen mode exit fullscreen mode the meaning of these values is explained at this link . but note that the above is very machine-specific . the configuration above works fine for me with the raspberry pi 3 model b+, but it may not work for you (or be optimal) for a less- or more-powerful machine. spark enforces hard lower bounds on how much memory can be allocated, as well. where you see 465m above, that's the minimum configurable value for that entry -- any less and spark will refuse to run. a raspberry pi 3 model b+ uses between 9-25\% of its ram while idling. since they have 926mb ram in total, hadoop and spark will have access to at most about 840mb of ram per pi. once all of this has been configured, reboot the cluster. note that, when you reboot, you should not format the hdfs namenode again. instead, simply stop and restart the hdfs service with: $ stop-dfs.sh && stop-yarn.sh $ start-dfs.sh && start-yarn.sh enter fullscreen mode exit fullscreen mode now, you can submit a job to spark on the command line: pi@pi1:~ $ spark-submit --deploy-mode client --class org.apache.spark.examples.sparkpi $spark_home /examples/jars/spark-examples_2.11-2.4.3.jar 7
openjdk client vm warning: you have loaded library /opt/hadoop/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. the vm will try to fix the stack guard now.
it 's highly recommended that you fix the library with ' execstack -c <libfile> ', or link it with ' -z noexecstack '.
2019-07-08 14:01:24,408 warn util.nativecodeloader: unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-07-08 14:01:25,514 info spark.sparkcontext: running spark version 2.4.3
2019-07-08 14:01:25,684 info spark.sparkcontext: submitted application: spark pi
2019-07-08 14:01:25,980 info spark.securitymanager: changing view acls to: pi
2019-07-08 14:01:25,980 info spark.securitymanager: changing modify acls to: pi
2019-07-08 14:01:25,981 info spark.securitymanager: changing view acls groups to: 
2019-07-08 14:01:25,981 info spark.securitymanager: changing modify acls groups to: 
2019-07-08 14:01:25,982 info spark.securitymanager: securitymanager: authentication disabled; ui acls disabled; users  with view permissions: set(pi); groups with view permissions: set(); users  with modify permissions: set(pi); groups with modify permissions: set()
2019-07-08 14:01:27,360 info util.utils: successfully started service ' sparkdriver ' on port 46027.
2019-07-08 14:01:27,491 info spark.sparkenv: registering mapoutputtracker
2019-07-08 14:01:27,583 info spark.sparkenv: registering blockmanagermaster
2019-07-08 14:01:27,594 info storage.blockmanagermasterendpoint: using org.apache.spark.storage.defaulttopologymapper for getting topology information
2019-07-08 14:01:27,596 info storage.blockmanagermasterendpoint: blockmanagermasterendpoint up
2019-07-08 14:01:27,644 info storage.diskblockmanager: created local directory at /tmp/blockmgr-e5479834-d1e4-48fa-9f5c-cbeb65531c31
2019-07-08 14:01:27,763 info memory.memorystore: memorystore started with capacity 90.3 mb
2019-07-08 14:01:28,062 info spark.sparkenv: registering outputcommitcoordinator
2019-07-08 14:01:28,556 info util.log: logging initialized @10419ms
2019-07-08 14:01:28,830 info server.server: jetty-9.3.z-snapshot, build timestamp: unknown, git hash: unknown
2019-07-08 14:01:28,903 info server.server: started @10770ms
2019-07-08 14:01:28,997 info server.abstractconnector: started serverconnector@89f072{http/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-08 14:01:28,997 info util.utils: successfully started service ' sparkui ' on port 4040.
2019-07-08 14:01:29,135 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1b325b3{/jobs,null,available,@spark}
2019-07-08 14:01:29,137 info handler.contexthandler: started o.s.j.s.servletcontexthandler@b72664{/jobs/json,null,available,@spark}
2019-07-08 14:01:29,140 info handler.contexthandler: started o.s.j.s.servletcontexthandler@34b7b8{/jobs/job,null,available,@spark}
2019-07-08 14:01:29,144 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1e8821f{/jobs/job/json,null,available,@spark}
2019-07-08 14:01:29,147 info handler.contexthandler: started o.s.j.s.servletcontexthandler@b31700{/stages,null,available,@spark}
2019-07-08 14:01:29,150 info handler.contexthandler: started o.s.j.s.servletcontexthandler@165e559{/stages/json,null,available,@spark}
2019-07-08 14:01:29,153 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1ae47a0{/stages/stage,null,available,@spark}
2019-07-08 14:01:29,158 info handler.contexthandler: started o.s.j.s.servletcontexthandler@5a54d{/stages/stage/json,null,available,@spark}
2019-07-08 14:01:29,161 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1ef722a{/stages/pool,null,available,@spark}
2019-07-08 14:01:29,165 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1d9b663{/stages/pool/json,null,available,@spark}
2019-07-08 14:01:29,168 info handler.contexthandler: started o.s.j.s.servletcontexthandler@14894fc{/storage,null,available,@spark}
2019-07-08 14:01:29,179 info handler.contexthandler: started o.s.j.s.servletcontexthandler@567255{/storage/json,null,available,@spark}
2019-07-08 14:01:29,186 info handler.contexthandler: started o.s.j.s.servletcontexthandler@362c57{/storage/rdd,null,available,@spark}
2019-07-08 14:01:29,191 info handler.contexthandler: started o.s.j.s.servletcontexthandler@4ee95c{/storage/rdd/json,null,available,@spark}
2019-07-08 14:01:29,195 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1c4715d{/environment,null,available,@spark}
2019-07-08 14:01:29,200 info handler.contexthandler: started o.s.j.s.servletcontexthandler@a360ea{/environment/json,null,available,@spark}
2019-07-08 14:01:29,204 info handler.contexthandler: started o.s.j.s.servletcontexthandler@148bb7d{/executors,null,available,@spark}
2019-07-08 14:01:29,209 info handler.contexthandler: started o.s.j.s.servletcontexthandler@27ba81{/executors/json,null,available,@spark}
2019-07-08 14:01:29,214 info handler.contexthandler: started o.s.j.s.servletcontexthandler@336c81{/executors/threaddump,null,available,@spark}
2019-07-08 14:01:29,217 info handler.contexthandler: started o.s.j.s.servletcontexthandler@156f2dd{/executors/threaddump/json,null,available,@spark}
2019-07-08 14:01:29,260 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1e52059{/static,null,available,@spark}
2019-07-08 14:01:29,265 info handler.contexthandler: started o.s.j.s.servletcontexthandler@159e366{/,null,available,@spark}
2019-07-08 14:01:29,283 info handler.contexthandler: started o.s.j.s.servletcontexthandler@1dc9128{/api,null,available,@spark}
2019-07-08 14:01:29,288 info handler.contexthandler: started o.s.j.s.servletcontexthandler@c4944a{/jobs/job/kill,null,available,@spark}
2019-07-08 14:01:29,292 info handler.contexthandler: started o.s.j.s.servletcontexthandler@772895{/stages/stage/kill,null,available,@spark}
2019-07-08 14:01:29,304 info ui.sparkui: bound sparkui to 0.0.0.0, and started at http://pi1:4040
2019-07-08 14:01:29,452 info spark.sparkcontext: added jar file:/opt/spark/examples/jars/spark-examples_2.11-2.4.3.jar at spark://pi1:46027/jars/spark-examples_2.11-2.4.3.jar with timestamp 1562590889451
2019-07-08 14:01:33,070 info client.rmproxy: connecting to resourcemanager at pi1/192.168.0.101:8032
2019-07-08 14:01:33,840 info yarn.client: requesting a new application from cluster with 7 nodemanagers
2019-07-08 14:01:34,082 info yarn.client: verifying our application has not requested more than the maximum memory capability of the cluster (900 mb per container)
2019-07-08 14:01:34,086 info yarn.client: will allocate am container, with 740 mb memory including 384 mb overhead
2019-07-08 14:01:34,089 info yarn.client: setting up container launch context for our am
2019-07-08 14:01:34,101 info yarn.client: setting up the launch environment for our am container
2019-07-08 14:01:34,164 info yarn.client: preparing resources for our am container
2019-07-08 14:01:35,577 warn yarn.client: neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under spark_home.
2019-07-08 14:02:51,027 info yarn.client: uploading resource file:/tmp/spark-ca0b9022-2ba9-45ff-8d63-50545ef98e55/__spark_libs__7928629488171799934.zip -> hdfs://pi1:9000/user/pi/.sparkstaging/application_1562589758436_0002/__spark_libs__7928629488171799934.zip
2019-07-08 14:04:09,654 info yarn.client: uploading resource file:/tmp/spark-ca0b9022-2ba9-45ff-8d63-50545ef98e55/__spark_conf__4579290782490197871.zip -> hdfs://pi1:9000/user/pi/.sparkstaging/application_1562589758436_0002/__spark_conf__.zip
2019-07-08 14:04:13,226 info spark.securitymanager: changing view acls to: pi
2019-07-08 14:04:13,227 info spark.securitymanager: changing modify acls to: pi
2019-07-08 14:04:13,227 info spark.securitymanager: changing view acls groups to: 
2019-07-08 14:04:13,228 info spark.securitymanager: changing modify acls groups to: 
2019-07-08 14:04:13,228 info spark.securitymanager: securitymanager: authentication disabled; ui acls disabled; users  with view permissions: set(pi); groups with view permissions: set(); users  with modify permissions: set(pi); groups with modify permissions: set()
2019-07-08 14:04:20,235 info yarn.client: submitting application application_1562589758436_0002 to resourcemanager
2019-07-08 14:04:20,558 info impl.yarnclientimpl: submitted application application_1562589758436_0002
2019-07-08 14:04:20,577 info cluster.schedulerextensionservices: starting yarn extension services with app application_1562589758436_0002 and attemptid none
2019-07-08 14:04:21,625 info yarn.client: application report for application_1562589758436_0002 (state: accepted)
2019-07-08 14:04:21,680 info yarn.client: 
     client token: n/a
     diagnostics: [mon jul 08 14:04:20 +0100 2019] scheduler has assigned a container for am, waiting for am container to be launched
     applicationmaster host: n/a
     applicationmaster rpc port: -1
     queue: default
     start time: 1562591060331
     final status: undefined
     tracking url: http://pi1:8088/proxy/application_1562589758436_0002/
     user: pi
2019-07-08 14:04:22,696 info yarn.client: application report for application_1562589758436_0002 (state: accepted)
2019-07-08 14:04:23,711 info yarn.client: application report for application_1562589758436_0002 (state: accepted)
2019-07-08 14:04:24,725 info yarn.client: application report for application_1562589758436_0002 (state: accepted)
...
2019-07-08 14:05:45,863 info yarn.client: application report for application_1562589758436_0002 (state: accepted)
2019-07-08 14:05:46,875 info yarn.client: application report for application_1562589758436_0002 (state: accepted)
2019-07-08 14:05:47,883 info yarn.client: application report for application_1562589758436_0002 (state: running)
2019-07-08 14:05:47,884 info yarn.client: 
     client token: n/a
     diagnostics: n/a
     applicationmaster host: 192.168.0.103
     applicationmaster rpc port: -1
     queue: default
     start time: 1562591060331
     final status: undefined
     tracking url: http://pi1:8088/proxy/application_1562589758436_0002/
     user: pi
2019-07-08 14:05:47,891 info cluster.yarnclientschedulerbackend: application application_1562589758436_0002 has started running.
2019-07-08 14:05:47,937 info util.utils: successfully started service ' org.apache.spark.network.netty.nettyblocktransferservice ' on port 46437.
2019-07-08 14:05:47,941 info netty.nettyblocktransferservice: server created on pi1:46437
2019-07-08 14:05:47,955 info storage.blockmanager: using org.apache.spark.storage.randomblockreplicationpolicy for block replication policy
2019-07-08 14:05:48,178 info storage.blockmanagermaster: registering blockmanager blockmanagerid(driver, pi1, 46437, none)
2019-07-08 14:05:48,214 info storage.blockmanagermasterendpoint: registering block manager pi1:46437 with 90.3 mb ram, blockmanagerid(driver, pi1, 46437, none)
2019-07-08 14:05:48,265 info storage.blockmanagermaster: registered blockmanager blockmanagerid(driver, pi1, 46437, none)
2019-07-08 14:05:48,269 info storage.blockmanager: initialized blockmanager: blockmanagerid(driver, pi1, 46437, none)
2019-07-08 14:05:49,426 info cluster.yarnclientschedulerbackend: add webui filter. org.apache.hadoop.yarn.server.webproxy.amfilter.amipfilter, map(proxy_hosts -> pi1, proxy_uri_bases -> http://pi1:8088/proxy/application_1562589758436_0002), /proxy/application_1562589758436_0002
2019-07-08 14:05:49,441 info ui.jettyutils: adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.amipfilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threaddump, /executors/threaddump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
2019-07-08 14:05:49,816 info ui.jettyutils: adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.amipfilter to /metrics/json.
2019-07-08 14:05:49,829 info handler.contexthandler: started o.s.j.s.servletcontexthandler@136bd1{/metrics/json,null,available,@spark}
2019-07-08 14:05:49,935 info cluster.yarnclientschedulerbackend: schedulerbackend is ready for scheduling beginning after waiting maxregisteredresourceswaitingtime: 30000(ms)
2019-07-08 14:05:50,076 info cluster.yarnschedulerbackend$yarnschedulerendpoint: applicationmaster registered as nettyrpcendpointref(spark-client://yarnam)
2019-07-08 14:05:52,074 info spark.sparkcontext: starting job: reduce at sparkpi.scala:38
2019-07-08 14:05:52,479 info scheduler.dagscheduler: got job 0 (reduce at sparkpi.scala:38) with 7 output partitions
2019-07-08 14:05:52,481 info scheduler.dagscheduler: final stage: resultstage 0 (reduce at sparkpi.scala:38)
2019-07-08 14:05:52,485 info scheduler.dagscheduler: parents of final stage: list()
2019-07-08 14:05:52,492 info scheduler.dagscheduler: missing parents: list()
2019-07-08 14:05:52,596 info scheduler.dagscheduler: submitting resultstage 0 (mappartitionsrdd[1] at map at sparkpi.scala:34), which has no missing parents
2019-07-08 14:05:53,314 warn util.sizeestimator: failed to check whether usecompressedoops is set; assuming yes
2019-07-08 14:05:53,404 info memory.memorystore: block broadcast_0 stored as values in memory (estimated size 1936.0 b, free 90.3 mb)
2019-07-08 14:05:53,607 info memory.memorystore: block broadcast_0_piece0 stored as bytes in memory (estimated size 1256.0 b, free 90.3 mb)
2019-07-08 14:05:53,625 info storage.blockmanagerinfo: added broadcast_0_piece0 in memory on pi1:46437 (size: 1256.0 b, free: 90.3 mb)
2019-07-08 14:05:53,639 info spark.sparkcontext: created broadcast 0 from broadcast at dagscheduler.scala:1161
2019-07-08 14:05:53,793 info scheduler.dagscheduler: submitting 7 missing tasks from resultstage 0 (mappartitionsrdd[1] at map at sparkpi.scala:34) (first 15 tasks are for partitions vector(0, 1, 2, 3, 4, 5, 6))
2019-07-08 14:05:53,801 info cluster.yarnscheduler: adding task set 0.0 with 7 tasks
2019-07-08 14:06:08,910 warn cluster.yarnscheduler: initial job has not accepted any resources; check your cluster ui to ensure that workers are registered and have sufficient resources
2019-07-08 14:06:23,907 warn cluster.yarnscheduler: initial job has not accepted any resources; check your cluster ui to ensure that workers are registered and have sufficient resources
2019-07-08 14:06:38,907 warn cluster.yarnscheduler: initial job has not accepted any resources; check your cluster ui to ensure that workers are registered and have sufficient resources
2019-07-08 14:06:47,677 info cluster.yarnschedulerbackend$yarndriverendpoint: registered executor nettyrpcendpointref(spark-client://executor) (192.168.0.106:44936) with id 1
2019-07-08 14:06:48,266 info storage.blockmanagermasterendpoint: registering block manager pi6:39443 with 90.3 mb ram, blockmanagerid(1, pi6, 39443, none)
2019-07-08 14:06:48,361 info scheduler.tasksetmanager: starting task 0.0 in stage 0.0 (tid 0, pi6, executor 1, partition 0, process_local, 7877 bytes)
2019-07-08 14:06:48,371 info scheduler.tasksetmanager: starting task 1.0 in stage 0.0 (tid 1, pi6, executor 1, partition 1, process_local, 7877 bytes)
2019-07-08 14:06:48,375 info scheduler.tasksetmanager: starting task 2.0 in stage 0.0 (tid 2, pi6, executor 1, partition 2, process_local, 7877 bytes)
2019-07-08 14:06:48,379 info scheduler.tasksetmanager: starting task 3.0 in stage 0.0 (tid 3, pi6, executor 1, partition 3, process_local, 7877 bytes)
2019-07-08 14:06:50,877 info storage.blockmanagerinfo: added broadcast_0_piece0 in memory on pi6:39443 (size: 1256.0 b, free: 90.3 mb)
2019-07-08 14:06:52,001 info scheduler.tasksetmanager: starting task 4.0 in stage 0.0 (tid 4, pi6, executor 1, partition 4, process_local, 7877 bytes)
2019-07-08 14:06:52,024 info scheduler.tasksetmanager: starting task 5.0 in stage 0.0 (tid 5, pi6, executor 1, partition 5, process_local, 7877 bytes)
2019-07-08 14:06:52,039 info scheduler.tasksetmanager: starting task 6.0 in stage 0.0 (tid 6, pi6, executor 1, partition 6, process_local, 7877 bytes)
2019-07-08 14:06:52,115 info scheduler.tasksetmanager: finished task 2.0 in stage 0.0 (tid 2) in 3733 ms on pi6 (executor 1) (1/7)
2019-07-08 14:06:52,143 info scheduler.tasksetmanager: finished task 0.0 in stage 0.0 (tid 0) in 3891 ms on pi6 (executor 1) (2/7)
2019-07-08 14:06:52,144 info scheduler.tasksetmanager: finished task 1.0 in stage 0.0 (tid 1) in 3776 ms on pi6 (executor 1) (3/7)
2019-07-08 14:06:52,156 info scheduler.tasksetmanager: finished task 3.0 in stage 0.0 (tid 3) in 3780 ms on pi6 (executor 1) (4/7)
2019-07-08 14:06:52,217 info scheduler.tasksetmanager: finished task 4.0 in stage 0.0 (tid 4) in 222 ms on pi6 (executor 1) (5/7)
2019-07-08 14:06:52,249 info scheduler.tasksetmanager: finished task 6.0 in stage 0.0 (tid 6) in 215 ms on pi6 (executor 1) (6/7)
2019-07-08 14:06:52,262 info scheduler.tasksetmanager: finished task 5.0 in stage 0.0 (tid 5) in 247 ms on pi6 (executor 1) (7/7)
2019-07-08 14:06:52,270 info cluster.yarnscheduler: removed taskset 0.0, whose tasks have all completed, from pool 
2019-07-08 14:06:52,288 info scheduler.dagscheduler: resultstage 0 (reduce at sparkpi.scala:38) finished in 59.521 s
2019-07-08 14:06:52,323 info scheduler.dagscheduler: job 0 finished: reduce at sparkpi.scala:38, took 60.246659 s
pi is roughly 3.1389587699411
2019-07-08 14:06:52,419 info server.abstractconnector: stopped spark@89f072{http/1.1,[http/1.1]}{0.0.0.0:4040}
2019-07-08 14:06:52,432 info ui.sparkui: stopped spark web ui at http://pi1:4040
2019-07-08 14:06:52,473 info cluster.yarnclientschedulerbackend: interrupting monitor thread
2019-07-08 14:06:52,602 info cluster.yarnclientschedulerbackend: shutting down all executors
2019-07-08 14:06:52,605 info cluster.yarnschedulerbackend$yarndriverendpoint: asking each executor to shut down
2019-07-08 14:06:52,640 info cluster.schedulerextensionservices: stopping schedulerextensionservices
(serviceoption=none,
 services=list(),
 started=false)
2019-07-08 14:06:52,649 info cluster.yarnclientschedulerbackend: stopped
2019-07-08 14:06:52,692 info spark.mapoutputtrackermasterendpoint: mapoutputtrackermasterendpoint stopped!
2019-07-08 14:06:52,766 info memory.memorystore: memorystore cleared
2019-07-08 14:06:52,769 info storage.blockmanager: blockmanager stopped
2019-07-08 14:06:52,825 info storage.blockmanagermaster: blockmanagermaster stopped
2019-07-08 14:06:52,851 info scheduler.outputcommitcoordinator$outputcommitcoordinatorendpoint: outputcommitcoordinator stopped!
2019-07-08 14:06:52,902 info spark.sparkcontext: successfully stopped sparkcontext
2019-07-08 14:06:52,927 info util.shutdownhookmanager: shutdown hook called
2019-07-08 14:06:52,935 info util.shutdownhookmanager: deleting directory /tmp/spark-1d1b5b79-679d-4ffe-b8aa-7e84e6be10f2
2019-07-08 14:06:52,957 info util.shutdownhookmanager: deleting directory /tmp/spark-ca0b9022-2ba9-45ff-8d63-50545ef98e55 enter fullscreen mode exit fullscreen mode if you sift through the output of the above command, you can see the the next result is: took 60.246659 s
pi is roughly 3.1389587699411 enter fullscreen mode exit fullscreen mode the very last thing i'd like to advise you to do is to add stop-dfs.sh and stop-yarn.sh to your clusterreboot and clustershutdown functions, if you've defined them. you should shut down the hadoop cluster before powering off all of the machines, then reboot it when the machines boot back up: function clusterreboot { stop-yarn.sh && stop-dfs.sh && \ clustercmd sudo shutdown -r now } enter fullscreen mode exit fullscreen mode function clustershutdown { stop-yarn.sh && stop-dfs.sh && \ clustercmd sudo shutdown now } enter fullscreen mode exit fullscreen mode [ back to top ] conclusion so that's it! hopefully this guide was helpful to you, whether you're setting up a raspberry pi, building your own hadoop and spark cluster, or you just want to learn about some big data technologies. please let me know if you find any issues with the above guide; i'd be happy to amend it or make it clearer. if you want more information about commands you can run on hdfs or how to submit a job to spark , please click on those links. thanks for making it to the end! if you like content like this, be sure to follow me on twitter and here on dev.to . i post about java, hadoop and spark, r, and more. [ back to top ] hadoop & spark (4 part series) 1 installing and running hadoop and spark on windows 2 big data analysis with hadoop, spark, and r shiny 3 building a raspberry pi hadoop / spark cluster 4 installing and running hadoop and spark on ubuntu 18 top comments (38) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand sshu2017 sshu2017 sshu2017 follow joined jun 18, 2020 • jun 19 '20 • edited on jun 19 • edited dropdown menu copy link hide hi andrew, this is an excellent tutorial so thank you very much! i had a quite strange problem though related to the clustercmd function. at the moment, i have installed java and hadoop on both workers (which i call rpi102 and rpi103 ). as you can see from the terminal on the right, i can ssh from the master ( rpi101 ) into rpi102 and run the hadoop version command and got what i expected. however, as you can see from the terminal on the left, when i did ssh rpi102 hadoop version , the hadoop command is not found. but if i try something else, like ssh rpi102 whoami , it worked fine. this seems so odd and really puzzles me. do you have any idea what the issue could be? thanks in advance! like comment: like comment: 1 like like comment button reply collapse expand sshu2017 sshu2017 sshu2017 follow joined jun 18, 2020 • jun 19 '20 dropdown menu copy link hide never mind. this answer helped me out. superuser.com/a/896454/955273 like comment: like comment: 3 likes like comment button reply collapse expand yui chun leung yui chun leung yui chun leung follow joined aug 21, 2021 • aug 21 '21 dropdown menu copy link hide do you solve it by this command? clustermod sudo -e env ""path=$path"" hadoop version | grep hadoop enter fullscreen mode exit fullscreen mode like comment: like comment: like comment button reply collapse expand rohit das rohit das rohit das follow email rohit.das950@gmail.com location india education master of technology (computer science and engineering) at iit bhilai work student joined sep 23, 2019 • sep 23 '19 dropdown menu copy link hide hi. i am not able to get the hadoop version or java version for the nodes over ssh using clustercmd. i have set up a cluster of 5 pis (model b+) with 32 gb micro sd cards in all of them. on running clustercmd hadoop version | grep hadoop enter fullscreen mode exit fullscreen mode i get the following error: bash:hadoop:command not found
bash:hadoop:command not found
bash:hadoop:command not found
bash:hadoop:command not found
hadoop 3.2.0 enter fullscreen mode exit fullscreen mode i am attaching the .bashrc here. please help. thanks. # ~/.bashrc: executed by bash(1) for non-login shells.
# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)
# for examples

# if not running interactively, don't do anything
case $- in
    *i*) ;;
      *) return;;
esac

# don't put duplicate lines or lines starting with space in the history.
# see bash(1) for more options
histcontrol=ignoreboth

# append to the history file, don't overwrite it
shopt -s histappend

# for setting history length see histsize and histfilesize in bash(1)
histsize=1000
histfilesize=2000

# check the window size after each command and, if necessary,
# update the values of lines and columns.
shopt -s checkwinsize

# if set, the pattern ""**"" used in a pathname expansion context will
# match all files and zero or more directories and subdirectories.
#shopt -s globstar

# make less more friendly for non-text input files, see lesspipe(1)
#[ -x /usr/bin/lesspipe ] && eval ""$(shell=/bin/sh lesspipe)""

# set variable identifying the chroot you work in (used in the prompt below)
if [ -z ""${debian_chroot:-}"" ] && [ -r /etc/debian_chroot ]; then
    debian_chroot=$(cat /etc/debian_chroot)
fi

# set a fancy prompt (non-color, unless we know we ""want"" color)
case ""$term"" in
    xterm-color|*-256color) color_prompt=yes;;
esac

# uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
force_color_prompt=yes

if [ -n ""$force_color_prompt"" ]; then
    if [ -x /usr/bin/tput ] && tput setaf 1 >&/dev/null; then
    # we have color support; assume it's compliant with ecma-48
    # (iso/iec-6429). (lack of such support is extremely rare, and such
    # a case would tend to support setf rather than setaf.)
    color_prompt=yes
    else
    color_prompt=
    fi
fi

if [ ""$color_prompt"" = yes ]; then
    ps1='${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w \$\[\033[00m\] '
else
    ps1='${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '
fi
unset color_prompt force_color_prompt

# if this is an xterm set the title to user@host:dir
case ""$term"" in
xterm*|rxvt*)
    ps1=""\[\e]0;${debian_chroot:+($debian_chroot)}\u@\h: \w\a\]$ps1""
    ;;
*)
    ;;
esac

# enable color support of ls and also add handy aliases
if [ -x /usr/bin/dircolors ]; then
    test -r ~/.dircolors && eval ""$(dircolors -b ~/.dircolors)"" || eval ""$(dircolors -b)""
    alias ls='ls --color=auto'
    #alias dir='dir --color=auto'
    #alias vdir='vdir --color=auto'

    alias grep='grep --color=auto'
    alias fgrep='fgrep --color=auto'
    alias egrep='egrep --color=auto'
fi

# colored gcc warnings and errors
#export gcc_colors='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'

# some more ls aliases
#alias ll='ls -l'
#alias la='ls -a'
#alias l='ls -cf'

# alias definitions.
# you may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# see /usr/share/doc/bash-doc/examples in the bash-doc package.

if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi

# get hostname of other pis
function otherpis {
    grep ""pi"" /etc/hosts | awk '{print $2}' | grep -v $(hostname)
}

# send commands to other pis
function clustercmd {
    for pi in $(otherpis); do ssh $pi ""$@""; done
    $@
}

# restart all pis
function clusterreboot {
    for pi in $(otherpis);do
        ssh $pi ""sudo shutdown -r 0""
    done
}

# shutdown all pis
function clustershutdown {
    for pi in $(otherpis);do
        ssh $pi ""sudo shutdown 0""
    done
}

# send files to all pis
function clusterscp {
    for pi in $(otherpis); do
        cat $1 | ssh $pi ""sudo tee $1"" > /dev/null 2>&1
    done
}

function clusterssh {
    for pi in $(otherpis); do
        ssh $pi $1
    done
}

# sudo htpdate -a -l time.nist.gov

export java_home=/usr/local/jdk1.8.0/
export hadoop_home=/opt/hadoop
export spark_home=/opt/spark
export path=/usr/local/jdk1.8.0/bin:$hadoop_home/bin:$hadoop_home/sbin:$spark_home/bin:$path
export hadoop_home_warn_supress=1
export hadoop_root_logger=""warn,drfa"" enter fullscreen mode exit fullscreen mode like comment: like comment: 1 like like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 23 '19 dropdown menu copy link hide did you follow these steps? create the directories create the required directories on all other pis using: $ clustercmd sudo mkdir -p /opt/hadoop_tmp/hdfs $ clustercmd sudo chown pi:pi –r /opt/hadoop_tmp $ clustercmd sudo mkdir -p /opt/hadoop $ clustercmd sudo chown pi:pi /opt/hadoop copy the configuration copy the files in /opt/hadoop to each other pi using: $ for pi in $( otherpis ) ; do rsync –avxp $hadoop_home $pi :/opt ; done this will take quite a long time, so go grab lunch. when you're back, verify that the files copied correctly by querying the hadoop version on each node with the following command: $ clustercmd hadoop version | grep hadoop
hadoop 3.2.0
hadoop 3.2.0
hadoop 3.2.0
... you can't run the hadoop command on the other pis until you've copied over those hadoop directories. if you have done that, you also need to make sure that that directory is on the $path of the other pis by including the following lines in each of their .bashrc files (sorry, i don't think i included this step in the instructions): export java_home = $( readlink –f /usr/bin/java | sed ""s:bin/java::"" ) export hadoop_home = /opt/hadoop export path = $path : $hadoop_home /bin: $hadoop_home /sbin you could also simply clusterscp the .bashrc file from pi #1 to each of the other pis. like comment: like comment: 3 likes like comment button reply collapse expand rohit das rohit das rohit das follow email rohit.das950@gmail.com location india education master of technology (computer science and engineering) at iit bhilai work student joined sep 23, 2019 • sep 23 '19 dropdown menu copy link hide hi. thanks for the reply. yes, i did the steps you mentioned. since java wasn't pre-installed, i installed it manually in each pi, and checked them individually to see if they are working. as you can see below, the env variables are configured as you have suggested. export java_home=/usr/local/jdk1.8.0/
export hadoop_home=/opt/hadoop
export spark_home=/opt/spark
export path=/usr/local/jdk1.8.0/bin:$hadoop_home/bin:$hadoop_home/sbin:$spark_home/bin:$path
export hadoop_home_warn_supress=1
export hadoop_root_logger=""warn,drfa"" like comment: like comment: 1 like like thread thread rohit das rohit das rohit das follow email rohit.das950@gmail.com location india education master of technology (computer science and engineering) at iit bhilai work student joined sep 23, 2019 • sep 24 '19 dropdown menu copy link hide thanks. i resolved the issue by putting the path exports above the following part in .bashrc: # if not running interactively, don't do anything
case $- in
    *i*) ;;
      *) return;;
esac enter fullscreen mode exit fullscreen mode i also put the export path commands in /etc/profile of each pi. thanks. like comment: like comment: 5 likes like comment button reply collapse expand pidevi pidevi pidevi follow joined nov 9, 2019 • nov 9 '19 dropdown menu copy link hide hi andrew, thanks for sharing this with us! following your instruction i have managed to run hdfs and yarn on my cluster using openjdk-8 instead of oracle java (is not pre-installed on raspbian buster and can't be installed via apt). however, when running the wordcount example. > yarn jar hadoop-mapreduce-examples-3.2.0.jar wordcount ""/books/alice.txt"" output enter fullscreen mode exit fullscreen mode following error occurs error: java heap space
error: java heap space
error: java heap space enter fullscreen mode exit fullscreen mode and the job fails. do you have any idea what the reason might be? like comment: like comment: 2 likes like comment button reply collapse expand razvan t. coloja razvan t. coloja razvan t. coloja follow joined nov 4, 2019 • nov 4 '19 • edited on nov 4 • edited dropdown menu copy link hide try this for a more colourful clustercmd prompt put this in your ~/.bashrc: # clustercmd
function clustercmd {
        for i in 1 2 3 4 5 6 7 8; do echo -e ""\e[40;38;5;82m cluster node \e[30;48;5;82m $i \e[0m \e[38;5;4m --------------""; ssh rpi$i ""$@""; done
        $@
} then do a $ source ~/.bashrc looks like this: like comment: like comment: 4 likes like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • nov 4 '19 dropdown menu copy link hide looks nice! thanks, razvan! like comment: like comment: 1 like like comment button reply collapse expand andreas komninos andreas komninos andreas komninos follow joined aug 1, 2019 • aug 1 '19 dropdown menu copy link hide thank you for this superb article. i have been following it to deploy a hadoop/spark cluster using the latest raspberry pi 4 (4gb). i encountered one problem, which was that after completing the tutorial, the spark job was not being assigned. i got a warning: info yarn.client: requesting a new application from cluster with 0 nodemanagers and then it sort of got stuck on info yarn.client: application report for application_1564655698306_0001 (state: accepted). i will describe later how i solved this. first, i want to note that the latest raspbian version (buster) does not include oracle java 8 which is required by hadoop 3.2.0. there is no easy way to get it set-up, but it can be done. first you need to manually download the tar.gz file from oracle's site (this requires a registration). i put it up on a personal webserver so it can be easily downloaded from the pis. then, on each pi: download java package cd ~/downloads wget /jdk8.tar.gz extract package contents sudo mkdir /usr/java cd /usr/java sudo tar xf ~/downloads/jdk8.tar.gz update alternative configurations sudo update-alternatives --install /usr/bin/java java /usr/java/jdk1.8.0_221/bin/java 1000 sudo update-alternatives --install /usr/bin/javac javac /usr/java/jdk1.8.0_221/bin/javac 1000 select desired java version sudo update-alternatives --config java check the java version changes java -version next, here is how i solved the yarn problem. in your tutorial section ""configuring hadoop on the cluster"", after the modifications to the xml files have been made on pi1, two files need to be copied across to the other pis: these are yarn-site.xml and mapred-site.xml. after copying, yarn needs to be restarted on pi1. to set appropriate values for the memory settings, i found a useful tool which is described on this thread stackoverflow.com/questions/495791... copy-pasting the instructions: get the tool wget public-repo-1.hortonworks.com/hdp/... tar zxvf hdp_manual_install_rpm_helper_files-2.6.0.3.8.tar.gz rm hdp_manual_install_rpm_helper_files-2.6.0.3.8.tar.gz mv hdp_manual_install_rpm_helper_files-2.6.0.3.8/ hdp_conf_files run the tool python hdp_conf_files/scripts/yarn-utils.py -c 4 -m 8 -d 1 false -c number of cores you have for each node -m amount of memory you have for each node (giga) -d number of disk you have for each node -bool ""true"" if hbase is installed; ""false"" if not this should provide appropriate settings to use. after the xml files have been edited and yarn has been restarted, you can try this command to check that all the worker nodes are active. yarn node -list like comment: like comment: 3 likes like comment button reply collapse expand pidevi pidevi pidevi follow joined nov 9, 2019 • nov 9 '19 • edited on nov 9 • edited dropdown menu copy link hide hi andreas, i am running raspbian buster on my pis, too. i have downloaded the ""linux arm 64 hard float abi"" (jdk-8u231-linux-arm64-vfp-hflt.tar.gz) and followed your instructions and i get following error bu running java -version -bash: /usr/bin/java: cannot execute binary file: exec format error i guess this java-product is not compatible with the pi. which exact file have you downloaded from the orcale site? like comment: like comment: 2 likes like comment button reply collapse expand sliver88 sliver88 sliver88 follow joined jul 25, 2020 • jul 25 '20 • edited on jul 26 • edited dropdown menu copy link hide first of all i d like to thank andrew for a superb tutorial. besides some minor alternation i had to make, i was able to set up the hdfs etc. but i am running now on the same problem as you andreas. the first thing i d like to add to your recommendations is that downloading the java is easier. sudo apt-get install openjdk-8-jdk and then change the default (as you suggested already): sudo update-alternatives --config java sudo update-alternatives --config javac then change export java_home=$(readlink –f /usr/bin/java | sed ""s:bin/java::"") to export java_home=/usr/lib/jvm/java-8-openjdk-armhf both in ~/.bashrc and in  /opt/hadoop/etc/hadoop/hadoop-env.sh. the part i have been stuck for a while though is that yarn node -list command stucks and if i try to run a spark job then i also get stuck on the accepted part. i haven't yet tried your proposition. ps i know it is a year-old article but still is the best i ve seen so far in my research. ευχαριστώ πολύ και τους 2 (i would like to thank you both) like comment: like comment: 1 like like comment button reply collapse expand scurveedog scurveedog scurveedog follow location alaska, us work ? at home joined jan 20, 2020 • jan 20 '20 dropdown menu copy link hide hello andrew fantastic article/tutorial that made my deployment of a 4 node pi4 cluster almost trivial! but... i also think along the way, your use of bash shell scripting is brilliantly applied to the project. a model i will seek to emulate. like comment: like comment: 2 likes like comment button reply collapse expand sunnymo sunnymo sunnymo follow joined jul 16, 2019 • jul 16 '19 dropdown menu copy link hide excellent article, i have a pi 3b+ and pi model b(the oldest 256m ver.), is it possible to run a spark cluster? just for study. like comment: like comment: like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • jul 16 '19 dropdown menu copy link hide my pis are struggling as-is. i wouldn't recommend any lower specs than the model 3 b+ (1 gb ram) like comment: like comment: 1 like like comment button reply collapse expand sunnymo sunnymo sunnymo follow joined jul 16, 2019 • jul 17 '19 dropdown menu copy link hide i do want to make use of the old pi, even if it is used as a namenode, i think it doesn't need much computation resource. i'm new to spark, so the question might be silly, sorry about that. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • jul 17 '19 • edited on jul 17 • edited dropdown menu copy link hide even though namenodes aren't processing data, they still have some cpu and memory requirements (they have to orchestrate the data processing, maintain records of the filesystem, etc.). i saw somewhere that 4gb per node was the recommended minimum. all i know from experience is that 1gb seems to barely work. spark sets minimum memory limits and i don't think 256mb is enough to do anything. like comment: like comment: 1 like like thread thread sunnymo sunnymo sunnymo follow joined jul 16, 2019 • jul 19 '19 dropdown menu copy link hide okay, the only thing that 256m can do may be running an nginx reverse proxy in my private cloud or rip, thanks for that. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • jul 19 '19 dropdown menu copy link hide maybe you could turn it into a pi-hole? like comment: like comment: 1 like like thread thread sunnymo sunnymo sunnymo follow joined jul 16, 2019 • jul 25 '19 dropdown menu copy link hide unfortunately, pi-hole project requires at least 512m memory. my old pi should r.i.p right now, i'll leave it to my children as the first gift form elder generation. like comment: like comment: 2 likes like comment button reply collapse expand jangbae jangbae jangbae follow joined jul 6, 2020 • jul 6 '20 • edited on jul 6 • edited dropdown menu copy link hide it was a great tutorial! i could successfully build a hadoop system with four raspberry pi 4s. there were a few hiccups while i was following this tutorial but could be solved by following others postings. thanks a lot! like comment: like comment: 1 like like comment button reply collapse expand jangbae jangbae jangbae follow joined jul 6, 2020 • jul 8 '20 dropdown menu copy link hide it was a bit early for me to declare my cluster is all set. the problem i'm facing is that the worker nodes are not showing up. i have a master node and three worker nodes in the cluster. ""hdfs dfsadmin -report"" only shows the master node. double checked the xml files to make sure if there is any typo and they have been copied to the worker nodes correctly but found nothing. do you have any suggestions to figure this out? like comment: like comment: 1 like like comment button reply collapse expand scott sims scott sims scott sims follow joined oct 4, 2019 • oct 4 '19 dropdown menu copy link hide hi andrew, first off, great stuff. thank you so much for being as thorough as you are. this was a well put together #howto. secondly, i'd like to highlight some issues i came across as following your post. this is to be expected, but the links to the installations of hadoop and spark are no longer valid. but i just simply followed your link down the rabbit hole to find the most up to date available installer. next, i did run into the same issue as rohit das did as he noted below. i simply followed his fix and it worked perfectly. thank you rohit for figuring this out. thirdly, i ran into an issue of the namenodes and datanodes being registered, but the datanodes weren't able to connect to pi1. looking in the ui, everything was 0 and 0 datanodes were registered. after chasing so post after post, i finally got it to work by changing in core-site.xml hdfs://pi1:9000 to hdfs://{ip address}:9000 where {ip address} is the fully typed ip address of pi 1. lastly, i mounted 250gb ssd hard drives to my workers. once i get the optimal resource limits worked out for this in the configuration xml's i'll be sure to post here what they were. i should note my name node is a raspberry pi 3 but my workers are raspberry pi 4 with 4gb ram. like comment: like comment: 1 like like comment button reply view full discussion (38 comments) some comments may only be visible to logged-in visitors. sign in to view all comments. code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 more from andrew (he/him) what this senior developer learned from his first big rust project # rust # iot # showdev elegant multi-line shell strings # shell # bash # zsh # showdev what's the difference between nominal, structural, and duck typing? # java # typescript # javascript # computerscience 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/zejnilovic/building-hadoop-native-libraries-on-mac-in-2019-1iee,,,"building hadoop native libraries on mac in 2019 - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse saša zejnilović posted on may 20, 2019 • edited on dec 3, 2019 building hadoop native libraries on mac in 2019 # bigdata # hadoop # macos tl;dr to be found at the end recently i came into a situation that i ""needed"" hadoop native libraries. well, when i say ""needed"", i mean i was just getting fed up by the constant warnings like this one: warn util.nativecodeloader: unable to load native-hadoop library for your platform... using builtin-java classes where applicable enter fullscreen mode exit fullscreen mode so i thought i would build my own hadoop native libraries. how hard can it be, right? honest answer? less than an hour if you don't have a tutorial. fifteen minutes if you do and most of that is compilation time. in my search, i found out a lot of tutorials and guides were either outdated or didn't offer everything needed for a full compilation and installation and that is why i wrote my own which i tested on two independent macs, thus it should be ""tested enough"". why do it there was no real world issue i was hoping to solve. i just had a few minutes on my hands and i used them to learn something new. but i did read that there are cases of speed improvements which is good if you are developing or testing something locally because local machines tend to be slow and any improvement is more than welcome. another thing is i did see two random articles a while back saying they did have some issues with the java libraries, but chances of some of you having the same issues are really small. dependencies first of all, we need to install the dependencies for the build and i am including links so you can check what you are going to install exactly: gcc autoconf automake libtool cmake snappy gzip bzip2 zlib wget openssl 1.0 1.1 on brew has an issue. more in the comments section. thanks to @imasli protobuf 2.5.0 (please note i am skipping maven, java and others that i think you would already have. if i am wrong, tell me and let's update the article. as well as hadoop installation. there is a beautiful article about hadoop installation on mac by zhang hao here .) for the installation of most of these, i will be using homebrew . it's a good tool, has a one-liner installation and a very short average time to be productive with it. as the link provides everything you need i am skipping the installation here. if you are not using homebrew for the first time, update and upgrade your tools. if you are using it for some time already and would like to keep some things with the current version, use brew pin like this . # update brew update
brew upgrade # then the installation brew install wget gcc autoconf automake libtool cmake snappy gzip bzip2 zlib openssl enter fullscreen mode exit fullscreen mode as you could have noticed one of those dependencies listed is missing from the list above. yes! it is a protobuf that has been deprecated and can't be easily installed from homebrew. so let's build our own. it's cleaner that way and much more fun then it sounds. we will first need to get it from github and unarchive it somewhere. you can delete it right after, so you don't need a special folder structure. wget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.gz tar -xzf protobuf-2.5.0.tar.gz cd protobuf-2.5.0 enter fullscreen mode exit fullscreen mode then comes the process of building and making sure everything went smoothly. it takes some time and i advise you to run it step by step to see and know what is happening. some warnings here and there are normal so you can skip those. ./configure
make
make check
make install # and just to check if everything is ok. # this should print libprotoc 2.5.0 protoc --version enter fullscreen mode exit fullscreen mode openssl setup now, linking openssl libraries by hand as homebrew refuses to link openssl and the compiler needs them. this is a known feature and needs to be done by running ln . cd /usr/local/include ln -s ../opt/openssl/include/openssl . enter fullscreen mode exit fullscreen mode this will solve an error that looks something like the caption below. [ exec ] -- configuring incomplete, errors occurred! [ exec ] see also /users/user/github/hadoop/hadoop-tools/hadoop-pipes/target/native/cmakecmake error at /usr/local/cellar/cmake/3.14.3/share/cmake/modules/findpackagehandlestandardargs.cmake:137 ( message ) : [ exec ] could not find openssl, try to set the path to openssl root folder in the [ exec ] system variable openssl_root_dir ( missing: openssl_include_dir ) [ exec ] call stack ( most recent call first ) : [ exec ] /usr/local/cellar/cmake/3.14.3/share/cmake/modules/findpackagehandlestandardargs.cmake:378 ( _fphsa_failure_message ) [ exec ] /usr/local/cellar/cmake/3.14.3/share/cmake/modules/findopenssl.cmake:413 ( find_package_handle_stfiles/cmakeoutput.log. [ exec ] andard_args ) [ exec ] cmakelists.txt:20 ( find_package ) [ exec ] [ exec ] enter fullscreen mode exit fullscreen mode building native libraries and finally! the building of the libraries. again, this will create a folder that you can delete in the end. here is probably the first place you will need to modify something and that is the version of hadoop you will be using. git clone https://github.com/apache/hadoop.git cd hadoop # change the version as needed git checkout branch-<version> # and just package. mvn package -pdist ,native -dskiptests -dtar # after build, move your newly created libraries. cp -r hadoop-dist/target/hadoop-<version>/lib $hadoop_home enter fullscreen mode exit fullscreen mode setting up environment variables now the critical part, making your shell see the libraries. i don't know what kind of shell you are using, nevertheless, put this into your shell profile ( .bashrc , .zshrc , etc.): export hadoop_opts = ""-djava.library.path= ${ hadoop_home } /lib/native"" export ld_library_path = $ld_library_path : ${ hadoop_home } /lib/native export java_library_path = $java_library_path : ${ hadoop_home } /lib/native enter fullscreen mode exit fullscreen mode this will point all the libraries to the right path and will make everything fall right into place. the last thing that we need is just to check if everything is ok (and by everything i mean almost everything, because bzip is acting up and i still have not found a way to solve, when i do i will update this). hadoop checknative -a #the output should be something like this. 19/05/17 19:00:14 warn bzip2.bzip2factory: failed to load/initialize native-bzip2 library system-native, will use pure-java version
19/05/17 19:00:14 info zlib.zlibfactory: successfully loaded & initialized native-zlib library
native library checking:
hadoop: true /usr/local/cellar/hadoop/2.7.5/lib/native/libhadoop.dylib
zlib: true /usr/lib/libz.1.dylib
snappy: true /usr/local/lib/libsnappy.1.dylib
lz4: true revision:99
bzip2: false openssl: true /usr/lib/libcrypto.35.dylib
19/05/17 19:00:14 info util.exitutil: exiting with status 1 enter fullscreen mode exit fullscreen mode afterword hopefully, everything is running smoothly and you no longer get those warnings and if i helped even one person with this i am glad. because if there is no added value for the reader, then it is just me talking to my wall. on the other hand, if you did find some issues in the code or the article, please do tell me and i will fix everything i am capable of. tl;dr this is just a step by step shell script extracted from the upper text. top comments (18) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand cameron hudson cameron hudson cameron hudson follow joined sep 14, 2019 • sep 14 '19 dropdown menu copy link hide depending on which hadoop version you want to install, you may need to use an earlier java version to package it. this can be done by temporarily changing the java_home environment variable before running mvn package . in my case, instead of mvn package -pdist,native -dskiptests -dtar i had to run java_home=/library/java/javavirtualmachines/jdk1.8.0_221.jdk/contents/home mvn package -pdist,native -dskiptests -dtar like comment: like comment: 3 likes like comment button reply collapse expand 001027261_shunfang lan 001027261_shunfang lan 001027261_shunfang lan follow joined jun 5, 2020 • jun 5 '20 dropdown menu copy link hide jdk11 is still in progress cwiki.apache.org/confluence/displa... like comment: like comment: 1 like like comment button reply collapse expand saša zejnilović saša zejnilović saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 • sep 25 '19 dropdown menu copy link hide yes, you are right. i didn't think of that use case, i just assumed people would have a compliant java version already installed or set as the main one. like comment: like comment: like comment button reply collapse expand ajndesai ajndesai ajndesai follow joined may 3, 2020 • may 3 '20 • edited on may 3 • edited dropdown menu copy link hide thanks for this article. i get this error when compiling to generate package using this below command: mvn package -pdist,native -dskiptests -dtar branch: branch-3.2 [info] apache hadoop mapreduce nativetask ................. failure [  1.766 s] [error] failed to execute goal org.apache.hadoop:hadoop-maven-plugins:3.2.2-snapshot:cmake-compile (cmake-compile) on project hadoop-mapreduce-client-nativetask: make failed with error code 2 -> [help 1] like comment: like comment: 1 like like comment button reply collapse expand 001027261_shunfang lan 001027261_shunfang lan 001027261_shunfang lan follow joined jun 5, 2020 • jun 5 '20 dropdown menu copy link hide it's because hadoop doesn't support macos native building now. see github.com/apache/hadoop/blob/trun... note that building hadoop 3.1.1/3.1.2/3.2.0 native code from source is broken
on macos. for 3.1.1/3.1.2, you need to manually backport yarn-8622. for 3.2.0,
you need to backport both yarn-8622 and yarn-9487 in order to build native code. like comment: like comment: 5 likes like comment button reply collapse expand saša zejnilović saša zejnilović saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 • may 3 '20 dropdown menu copy link hide hello, i don't see enough of the message.  do you have java 8?  or maybe more of the error (line with cause by )? i have tried building this now and it works for me. like comment: like comment: 1 like like comment button reply collapse expand ajndesai ajndesai ajndesai follow joined may 3, 2020 • may 3 '20 • edited on may 3 • edited dropdown menu copy link hide yes i have java 8 and that's my java home. hadoop, hive setup and working though. i read some of the compression codecs works only with native libraries and jobs will fail with java libraries. i tried this on mac mojave os 10.14.6 like comment: like comment: 1 like like comment button reply collapse expand imasli imasli imasli follow location berlin work data engineer joined dec 3, 2019 • dec 3 '19 dropdown menu copy link hide what version of openssl did you use? i used openssl 1.1 from brew and i have this error error: variable has incomplete type 'hmac_ctx' (aka 'hmac_ctx_st') like comment: like comment: 1 like like comment button reply collapse expand saša zejnilović saša zejnilović saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 • dec 3 '19 dropdown menu copy link hide this seems more like an issue with the openssl installation than the version you are using. anyway, this is my current pc (not sure with which i have built it, but libs work) ╰─ $ openssl version
libressl 2.8.3
╰─ $ brew list --versions | grep ssl
openssl 1.0.2t
openssl@1.1 1.1.1d like comment: like comment: 2 likes like comment button reply collapse expand imasli imasli imasli follow location berlin work data engineer joined dec 3, 2019 • dec 3 '19 • edited on dec 3 • edited dropdown menu copy link hide according to issues.apache.org/jira/browse/hado... openssl 1.1 broke the compilation. they patched it but didn't include it to the version 2 build. your tutorial used openssl 1.0 (open ssl 1.1 will have openssl@1.1 on the path) too bad that homebrew already deprecated openssl 1.0 like comment: like comment: 2 likes like thread thread saša zejnilović saša zejnilović saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 • dec 3 '19 dropdown menu copy link hide thank you very much! will update the post. like comment: like comment: 2 likes like thread thread imasli imasli imasli follow location berlin work data engineer joined dec 3, 2019 • dec 4 '19 dropdown menu copy link hide this issue discusses how to forcefully install openssl 1.0 using homebrew github.com/homebrew/homebrew-core/... like comment: like comment: 1 like like comment button reply collapse expand krikoon73 krikoon73 krikoon73 follow joined may 3, 2020 • may 3 '20 • edited on may 3 • edited dropdown menu copy link hide hi sasa, thank you for this ! i'm a bit new on hadoop and i'm trying to fix the ""native library"" thing based on your article. everything is ok until ""apache hadoop common"" build : mac osx highsierra hadoop version 2.9.2 ~/hadoop/tmp/hadoop   branch-2.9.2  brew list autoconf        cmake           gettext         go          libidn2         libunistring        openjdk         pcre2           pyenv-virtualenv    sshpass         wget automake        direnv          git         gzip            libmpc          maven           openssl@1.1     pkg-config      readline        telnet          zlib bzip2           gcc         gmp         isl         libtool         mpfr            pandoc          pyenv           snappy          tree openssl : ok protoc : 2.5.0 ok error message : [info] apache hadoop common ............................... failure [02:08 min] i have : [warning] /users/ccompain/hadoop/tmp/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/opensslcipher.c:256:14: error: incomplete definition of type 'struct evp_cipher_ctx_st' [warning]   if (context->flags & evp_ciph_no_padding) { [warning]       ~~~~~~~^ [warning] /usr/local/include/openssl/ossl_typ.h:90:16: note: forward declaration of 'struct evp_cipher_ctx_st' [warning] typedef struct evp_cipher_ctx_st evp_cipher_ctx; [warning]                ^ [warning] /users/ccompain/hadoop/tmp/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/opensslcipher.c:262:20: error: incomplete definition of type 'struct evp_cipher_ctx_st' [warning]     int b = context->cipher->block_size; [warning]             ~~~~~~~^ [warning] /usr/local/include/openssl/ossl_typ.h:90:16: note: forward declaration of 'struct evp_cipher_ctx_st' [warning] typedef struct evp_cipher_ctx_st evp_cipher_ctx; [warning]                ^ [warning] /users/ccompain/hadoop/tmp/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/opensslcipher.c:263:16: error: incomplete definition of type 'struct evp_cipher_ctx_st' [warning]     if (context->encrypt) { [warning]         ~~~~~~~^ [warning] /usr/local/include/openssl/ossl_typ.h:90:16: note: forward declaration of 'struct evp_cipher_ctx_st' [warning] typedef struct evp_cipher_ctx_st evp_cipher_ctx; [warning]                ^ [warning] /users/ccompain/hadoop/tmp/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/opensslcipher.c:310:14: error: incomplete definition of type 'struct evp_cipher_ctx_st' [warning]   if (context->flags & evp_ciph_no_padding) { [warning]       ~~~~~~~^ [warning] /usr/local/include/openssl/ossl_typ.h:90:16: note: forward declaration of 'struct evp_cipher_ctx_st' [warning] typedef struct evp_cipher_ctx_st evp_cipher_ctx; [warning]                ^ [warning] /users/ccompain/hadoop/tmp/hadoop/hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/crypto/opensslcipher.c:313:20: error: incomplete definition of type 'struct evp_cipher_ctx_st' [warning]     int b = context->cipher->block_size; [warning]             ~~~~~~~^ [warning] /usr/local/include/openssl/ossl_typ.h:90:16: note: forward declaration of 'struct evp_cipher_ctx_st' [warning] typedef struct evp_cipher_ctx_st evp_cipher_ctx; [warning]                ^ [warning] 5 errors generated. [warning] 5 errors generated. [warning] make[2]: *** [cmakefiles/hadoop.dir/main/native/src/org/apache/hadoop/crypto/opensslcipher.c.o] error 1 [warning] make[2]: *** waiting for unfinished jobs.... [warning] make[1]: *** [cmakefiles/hadoop_static.dir/all] error 2 [warning] make[1]: *** waiting for unfinished jobs.... [warning] make[1]: *** [cmakefiles/hadoop.dir/all] error 2 [warning] make: *** [all] error 2 any idea ? thks, christophe like comment: like comment: 1 like like comment button reply collapse expand saša zejnilović saša zejnilović saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 • may 3 '20 dropdown menu copy link hide hello, in all honesty, i don't remember having this problem, but seems like you aren't alone. apparently it is an openssl feature. this apache.org jira seems to be talking precisely about your problem. seems like they even provide places you need to fix in the c code. like comment: like comment: 1 like like comment button reply collapse expand dale schaffer dale schaffer dale schaffer follow location israel joined sep 24, 2019 • sep 24 '19 dropdown menu copy link hide when is $hadoop_home defined? when i run line 29 in script it fails and prints out the usage text for cp like comment: like comment: 1 like like comment button reply collapse expand saša zejnilović saša zejnilović saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 • sep 25 '19 dropdown menu copy link hide $hadoop_home is not defined by me. it is part of proper hadoop installation and i am not doing that in this article. like comment: like comment: like comment button reply collapse expand sahir azeddin sahir azeddin sahir azeddin follow data science student education fsr work student joined nov 28, 2022 • nov 28 '22 dropdown menu copy link hide hello, i am trying to install hadoop native library on mac but i have some problems here i tried to solve it but i can’t. please if u have any idea. my problem looks like: like comment: like comment: 1 like like comment button reply collapse expand arulselvan madhavan arulselvan madhavan arulselvan madhavan follow joined aug 7, 2020 • aug 7 '20 dropdown menu copy link hide thanks for writing this. helped me a lot like comment: like comment: 1 like like comment button reply view full discussion (18 comments) code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse saša zejnilović follow location prague work big data lead at absa group limited joined jul 2, 2018 more from saša zejnilović working with nested structures in spark # spark # bigdata # scala # library how to compare your data in/with spark # spark # scala # testing # bigdata 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home tags about contact code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/awwsmm/installing-and-running-hadoop-and-spark-on-windows-33kc,,,"installing and running hadoop and spark on windows - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse andrew (he/him) posted on nov 5, 2018 • edited on dec 18, 2019 installing and running hadoop and spark on windows # hadoop # spark # windows # tutorial hadoop & spark (4 part series) 1 installing and running hadoop and spark on windows 2 big data analysis with hadoop, spark, and r shiny 3 building a raspberry pi hadoop / spark cluster 4 installing and running hadoop and spark on ubuntu 18 installing and running hadoop and spark on windows we recently got a big new server at work to run hadoop and spark (h/s) on for a proof-of-concept test of some software we're writing for the biopharmaceutical industry and i hit a few snags while trying to get h/s up and running on windows server 2016 / windows 10. i've documented here, step-by-step, how i managed to install and run this pair of apache products directly in the windows cmd prompt, without any need for linux emulation. update 16 dec 2019: software version numbers have been updated and the text has been clarified. get the software the first step is to download java, hadoop, and spark. spark seems to have trouble working with newer versions of java, so i'm sticking with java 8 for now: java (using version: 8u230+) hadoop (using version: 3.1.3) spark (using version: 3.0.0 preview) i can't guarantee that this guide works with newer versions of java. please try with java 8 if you're having issues. also, with the new oracle licensing structure (2019+), you may need to create an oracle account to download java 8. to avoid this, simply download from adoptopenjdk instead . for java, i download the ""windows x64"" version of the adoptopenjdk hotspot jvm ( jdk8u232-b09 ); for hadoop, the binary of v3.1.3 ( hadoop-3.1.3.tar.gz ); for spark, v3.0.0 ""pre-built for apache hadoop 2.7 and later"" ( spark-3.0.0-preview-bin-hadoop2.7.tgz ). from this point on, i'll refer generally to these versions as hadoop-<version> and spark-<version> ; please replace these with your version number throughout the rest of this tutorial. even though newer versions of hadoop and spark are currently available, there is a bug with hadoop 3.2.1 on windows that causes installation to fail. until that patched version is available (3.3.0 or 3.1.4 or 3.2.2), you must use an earlier version of hadoop on windows. next, download 7-zip to extract the *gz archives. note that you may need to extract twice (once to move from *gz to *.tar files, then a second time to ""untar""). once they're extracted (hadoop takes a while), you can delete all of the *.tar and *gz files. you should now have two directories and the jdk installer in your downloads directory: note that -- as shown above -- the ""hadoop"" directory and ""spark"" directory each contain a license , notice , and readme file. with particular versions of hadoop, you may extract and get a directory structure like c:\users\<username>\downloads\hadoop-<version>\hadoop-<version>\... enter fullscreen mode exit fullscreen mode ...if this is the case, move the contents of the inner hadoop-<version> directory to the outer hadoop-<version> directory by copying-and-pasting, then delete the inner hadoop-<version> directory. the path to the license file, for example, should then be: c:\users\<username>\downloads\hadoop-<version>\license enter fullscreen mode exit fullscreen mode ...and similar for the ""spark"" directory. warning : if you see a message like ""can not create symbolic link : a required privilege is not held by the client"" in 7-zip, you must run 7-zip in administrator mode , then unzip the directories. if you skip these files, you may end up with a broken hadoop installation. move the spark and hadoop directories into the c:\ directory (you may need administrator privileges on your machine to do this). then, run the java installer but change the destination folder from the default c:\program files\adoptopenjdk\jdk-<version>\ to just c:\java . (h/s can have trouble with directories with spaces in their names.) once the installation is finished, you can delete the java *.msi installer. make two new directories called c:\hadoop and c:\spark and copy the hadoop-<version> and spark-<version> directories into those directories, respectively: if you get ""name too long""-type warnings, skip those files. these are only *.html files and aren't critical to running h/s. set up your environment variables next, we need to set some environment variables. go to control panel > system and security > system > advanced system settings > environment variables... : ...and add new system variables (bottom box) called: java_home --> c:\java hadoop_home --> c:\hadoop\hadoop-<version> spark_home --> c:\spark\spark-<version> (adjust according to the versions of hadoop and spark that you've downloaded.) then, edit the path (again, in the system variables box at the bottom) and add those variables with \bin appended (also \sbin for hadoop): if you echo %path% in cmd you should now see these three directories somewhere in the middle of the path, because the user path is appended to the system path for the %path variable. you should check now that java -version , hdfs -version , and spark-shell --version return version numbers, as shown below. this means that they were correctly installed and added to your %path% : please note that if you try to run the above commands from a location with any spaces in the path , the commands may fail. for example, if your username is ""firstname lastname"" and you try to check the hadoop version, you may see an error message like: c:\users\firstname lastname>hdfs -version
error: could not find or load main class lastname enter fullscreen mode exit fullscreen mode to fix this, simply move to a working directory without any spaces in the path (as i did in the screenshot above): c:\users\firstname lastname>cd ..

c:\users>hdfs -version
openjdk version ""1.8.0_232""
openjdk runtime environment (adoptopenjdk)(build 1.8.0_232-b09)
openjdk 64-bit server vm (adoptopenjdk)(build 25.232-b09, mixed mode) enter fullscreen mode exit fullscreen mode configure hadoop next, go to %hadoop_home%\etc\hadoop and edit (or create) the file core-site.xml so it looks like the following: core-site.xml <?xml version=""1.0"" encoding=""utf-8""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> <configuration> <property> <name> fs.defaultfs </name> <value> hdfs://localhost:9000 </value> </property> </configuration> enter fullscreen mode exit fullscreen mode in the same directory, edit (or create) mapred-site.xml with the following contents: mapred-site.xml <?xml version=""1.0"" encoding=""utf-8""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> <configuration> <property> <name> mapreduce.framework.name </name> <value> yarn </value> </property> </configuration> enter fullscreen mode exit fullscreen mode next, edit (or create) hdfs-site.xml : hdfs-site.xml <?xml version=""1.0"" encoding=""utf-8""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> <configuration> <property> <name> dfs.replication </name> <value> 1 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> file:///c:/hadoop/hadoop- <version> /namenode </value> </property> <property> <name> dfs.datanode.data.dir </name> <value> file:///c:/hadoop/hadoop- <version> /datanode </value> </property> </configuration> enter fullscreen mode exit fullscreen mode ...yes, they should be forward slashes, even though windows uses backslashes. this is due to the way that hadoop interprets these file paths. also, be sure to replace <version> with the appropriate hadoop version number. finally, edit yarn-site.xml so it reads: yarn-site.xml <?xml version=""1.0"" encoding=""utf-8""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> <configuration> <property> <name> yarn.nodemanager.aux-services </name> <value> mapreduce_shuffle </value> </property> <property> <name> yarn.nodemanager.auxservices.mapreduce.shuffle.class </name> <value> org.apache.hadoop.mapred.shufflehandler </value> </property> </configuration> enter fullscreen mode exit fullscreen mode the last thing we need to do is create the directories that we referenced in hdfs-site.xml : patch hadoop now, you need to apply a patch created by and posted to github by user cdarlint . ( note that this patch is specific to the version of hadoop that you're installing, but if the exact version isn't available, try to use the one just before the desired version... that works sometimes.) make a backup of your %hadoop_home%\bin directory (copy it to \bin.old or similar), then copy the patched files (specific to your hadoop version, downloaded from the above git repo) to the old %hadoop_home%\bin directory, replacing the old files with the new ones. now, if you run hdfs namenode -format in cmd , you should see: one more thing to do: copy hadoop-yarn-server-timelineservice-<version> from c:\hadoop\hadoop-<version>\share\hadoop\yarn\timelineservice to c:\hadoop\hadoop-<version>\share\hadoop\yarn (the parent directory). (these are short version numbers, like 3.1.3 , and may not match between the jar file name and the directory name.) boot hdfs finally, you can boot hdfs by running start-dfs.cmd and start-yarn.cmd in cmd : you should verify that the namenode , datanode , resourcemanager , and nodemanager are all running using the jps command: you can also open localhost:8088 and localhost:9870 in your browser to monitor your shiny, new hadoop distributed file system: finally, test that you can edit the filesystem by running hadoop fs -mkdir /test , which will make a directory called test in the root directory: testing hadoop and spark we know now how to create directories ( fs -mkdir ) and list their contents ( fs -ls ) in hdfs, what about creating and editing files? well, files can be copied from the local file system to hdfs with fs -put . we can then read files in the spark-shell with sc.textfile(...) : note that you read a file from hdfs on hdfs://localhost:9000/ and not just hdfs:// . this is because this is the defaultfs we defined in core-site.xml . if you want to stop the hdfs , you can run the commands: c:\users> stop-dfs.cmd enter fullscreen mode exit fullscreen mode and c:\users> stop-yarn.cmd enter fullscreen mode exit fullscreen mode so there you have it! spark running on windows, reading files stored in hdfs. this took a bit of work to get going and i owe a lot to people who previously encountered the same bugs as me, or previously wrote tutorials which i used as a framework for this walkthrough. here are the blogs, github repos, and so posts i used to build this tutorial: muhammad bilal yar's hadoop 2.8.0 walkthrough java.net.urisyntaxexception java.lang.unsatisfiedlinkerror fatal resourcemanager.resourcemanager localhost:50070 error kuldeep singh's walkthrough and troubleshooting guide jacek laskowski's gitbook java.io.ioexception: incompatible clusterids hdfs basic commands spark basic commands hadoop & spark (4 part series) 1 installing and running hadoop and spark on windows 2 big data analysis with hadoop, spark, and r shiny 3 building a raspberry pi hadoop / spark cluster 4 installing and running hadoop and spark on ubuntu 18 top comments (60) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide hi andrew, i am getting this error when i try to execute start-yarn.cmd: this file does not have an app associated with it for performing this action. please install an app or, if one is already installed, create an association in the defaul apps settings page. it seems like yarn is not a command known by windows. here is my environment variables: thepracticaldev.s3.amazonaws.com/i... like comment: like comment: 1 like like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide hi david, it sounds like you're trying to run this program by double-clicking on it. you should run it in the cmd prompt like: c:\> start-yarn.cmd let me know if that works for you. like comment: like comment: 1 like like comment button reply collapse expand david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide no. i am using the cmd console. for example if i try to type just hadoop in console, it shows me some options. but if i try to type yarn it says: 'yarn' is not recognized as an internal or external command, operable program or batch file. i am attaching the images where it can be seen: thepracticaldev.s3.amazonaws.com/i... like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide these error messages are giving you hints about what's going wrong. it looks like your %path% is set up correctly and hadoop is on it, but you can't run the hadoop command by itself. that's what the error message is telling you. you need to include additional command-line arguments. try running hadoop version and see if you get any output. like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide when i execute this command ""hadoop version""  i get this: hadoop 2.9.1 subversion github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e compiled by root on 2018-04-16t09:33z compiled with protoc 2.5.0 from source with checksum 7d6d2b655115c6cc336d662cc2b919bd this command was run using /c:/bigdata/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar but if i try execute just ""yarn"" i get: 'yarn' is not recognized as an internal or external command, operable program or batch file. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide right, so hadoop is working fine. yarn isn't a command that you run, it's just the resource negotiator that the hdfs (hadoop distributed file system) uses behind the scenes to manage everything. if you successfully ran start-yarn.cmd and start-dfs.cmd , you're good to go! try uploading a file to hdfs with: c:\> hadoop fs -put <file name here> / ...and checking that it's been uploaded with c:\> hadoop fs -ls / like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide hi, thanks for your answer. but the problem is exactly that. when a i run the command start-yarn.cmd i get: this file does not have an app associated with it for performing this action. please install an app or, if one is already installed, create an association in the defaul apps settings page. so, i tried to see what the content is for the file start-yarn.cmd and it has a call to yarn command. so i tried to call it in a independent console and i get the same error. that is the reason why i think the problem is yarn, the command as is. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide okay, i think we're getting close. can you echo %path% and share the result? start-yarn.cmd should be within the hadoop /sbin directory. if you haven't added it to your path correctly, maybe that's why you can't access it. like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide thanks for the answer. here it is: echo %path% result: c:\program files (x86)\common files\oracle\java\javapath;c:\programdata\oracle\java\javapath;e:\app\dserranoa\product\11.2.0\client_1;e:\app\dserranoa\product\11.2.0\client_1\bin;c:\oraclexe\app\oracle\product\11.2.0\server\bin;;c:\windows\system32;c:\windows;c:\windows\system32\wbem;c:\windows\system32\windowspowershell\v1.0\;c:\program files\tortoisegit\bin;c:\program files\putty\;c:\program files\microsoft sql server\130\tools\binn\;c:\program files\microsoft\web platform installer\;c:\program files (x86)\microsoft sdks\azure\cli\wbin;c:\program files (x86)\microsoft sql server\110\dts\binn\;c:\program files (x86)\microsoft sql server\120\dts\binn\;c:\program files (x86)\microsoft sql server\130\dts\binn\;c:\program files\microsoft sql server\110\tools\binn\;c:\program files (x86)\microsoft sql server\110\tools\binn\managementstudio\;c:\program files (x86)\microsoft sql server\110\tools\binn\;c:\program files\nodejs\;c:\program files\microsoft sql server\110\dts\binn\;c:\program files (x86)\microsoft visual studio 10.0\common7\ide\privateassemblies\;c:\program files (x86)\bitvise ssh client;c:\program files\dotnet\;c:\program files\microsoft service fabric\bin\fabric\fabric.code;c:\program files\microsoft sdks\service fabric\tools\servicefabriclocalclustermanager;c:\program files (x86)\brackets\command;c:\windows\system32\openssh\;c:\program files\microsoft sql server\client sdk\odbc\130\tools\binn\;c:\program files (x86)\microsoft sql server\140\tools\binn\;c:\program files\microsoft sql server\140\tools\binn\;c:\program files\microsoft sql server\140\dts\binn\;c:\program files (x86)\microsoft sql server\140\dts\binn\;c:\program files (x86)\microsoft sql server\150\dts\binn\;c:\program files\java\jdk1.8.0_121\bin;c:\program files\mysql\mysql shell 8.0\bin;c:\users\dserranoa\appdata\local\microsoft\windowsapps;c:\progra~1\java\jdk1.8.0_121;c:\bigdata\hadoop-2.9.1;c:\bigdata\hadoop-2.9.1\bin;c:\bigdata\hadoop-2.9.1\sbin i have attached the image of my environment variables. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide huh. can you run: c:\> dir c:\bigdata\hadoop-2.9.1\sbin ...and give the result? like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide sure, here it is: volume in drive c has no label. volume serial number is 8276-d962 directory of c:\bigdata\hadoop-2.9.1\sbin 11/09/2019  09:55 a.m. . 11/09/2019  09:55 a.m.              .. 16/04/2018  06:52 a.m.             2.752 distribute-exclude.sh 11/09/2019  09:55 a.m.              federationstatestore 16/04/2018  06:52 a.m.             6.475 hadoop-daemon.sh 16/04/2018  06:52 a.m.             1.360 hadoop-daemons.sh 16/04/2018  06:52 a.m.             1.640 hdfs-config.cmd 16/04/2018  06:52 a.m.             1.427 hdfs-config.sh 16/04/2018  06:52 a.m.             3.148 httpfs.sh 16/04/2018  06:52 a.m.             3.677 kms.sh 16/04/2018  06:52 a.m.             4.134 mr-jobhistory-daemon.sh 16/04/2018  06:52 a.m.             1.648 refresh-namenodes.sh 16/04/2018  06:52 a.m.             2.145 slaves.sh 16/04/2018  06:52 a.m.             1.779 start-all.cmd 16/04/2018  06:52 a.m.             1.471 start-all.sh 16/04/2018  06:52 a.m.             1.128 start-balancer.sh 16/04/2018  06:52 a.m.             1.401 start-dfs.cmd 16/04/2018  06:52 a.m.             3.734 start-dfs.sh 16/04/2018  06:52 a.m.             1.357 start-secure-dns.sh 16/04/2018  06:52 a.m.             1.571 start-yarn.cmd 16/04/2018  06:52 a.m.             1.347 start-yarn.sh 16/04/2018  06:52 a.m.             1.770 stop-all.cmd 16/04/2018  06:52 a.m.             1.462 stop-all.sh 16/04/2018  06:52 a.m.             1.179 stop-balancer.sh 16/04/2018  06:52 a.m.             1.455 stop-dfs.cmd 16/04/2018  06:52 a.m.             3.206 stop-dfs.sh 16/04/2018  06:52 a.m.             1.340 stop-secure-dns.sh 16/04/2018  06:52 a.m.             1.642 stop-yarn.cmd 16/04/2018  06:52 a.m.             1.340 stop-yarn.sh 16/04/2018  06:52 a.m.             4.295 yarn-daemon.sh 16/04/2018  06:52 a.m.             1.353 yarn-daemons.sh 28 file(s)         61.236 bytes 3 dir(s)  101.757.034.496 bytes free like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide so start-dfs.cmd works, but start-yarn.cmd doesn't? weird. they're both in the same directory. that doesn't make much sense. i'm not sure how i can help further without being at your terminal. i'd say maybe try starting from scratch? sometimes, it's easy to miss a small step or two. like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide mmm well, i tried to do the same process in another machine and it happened again. the same error. the yarn daemons are not running. i have checked different options but i have not could find any solution yet. i don't know if yarn needs some additional installation or something like that or if there is another environment variable that i am not setting up. i am really lost here. what kind of command would you use in my console? like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 11 '19 dropdown menu copy link hide i would start from scratch, and make sure the correct version (8) of java is installed, and re-install hadoop. then, i would double-check all of the environment variables. can you try adding the environment variables as system environment variables, rather than user environment variables? you may need to be an administrator to do this. if all of that checks out, and the %path% is correct, and all of the .cmd files are on the path, i'm not sure what else i would do. there's no reason why those commands shouldn't work if they're on the %path% . like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 11 '19 dropdown menu copy link hide i appreciate your help. i have already added the variables to the system but the problem is still there. i would really appreciate that you tell me  if you have another ideas to solve this issue. i think it is also weird but it seems something related to yarn. i will look for more info , more tricks and if i solve it i will post here. thanks so much. like comment: like comment: 1 like like comment button reply collapse expand david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 12 '19 dropdown menu copy link hide hi andrew , it is me again. now i am testing in my personal machine. but now i ma having another problem. in my local machine my user is ""david serrano"". as you can see it has one space in it. when i try to format the namenode with ""hdfs namenode -format"" i am getting this error: error: could not find or load main class serrano caused by: java.lang.classnotfoundexception: serrano so, i guess the problem is the space in my user name. what can i do in this case? thanks in advance! like comment: like comment: 1 like like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 14 '19 dropdown menu copy link hide hadoop doesn't like spaces in paths. i think the only thing you can do is put java, hadoop, and spark in locations where there are no spaces in the path. i usually use: c:\java
c:\hadoop
c:\spark like comment: like comment: 1 like like comment button reply collapse expand david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 16 '19 dropdown menu copy link hide hi, well all the files are in paths without spaces. however, hadoop is executing something using my user ""david serrano"" and it is generating the problem. i have not found the root cause of this. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 16 '19 dropdown menu copy link hide are there any spaces on your %path% at all? like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 16 '19 dropdown menu copy link hide hi, here it is my path: c:\users\david serrano>echo %path% c:\program files (x86)\common files\oracle\java\javapath;c:\program files\microsoft mpi\bin\;c:\program files (x86)\intel\icls client\;c:\program files\intel\icls client\;c:\windows\system32;c:\windows;c:\windows\system32\wbem;c:\windows\system32\windowspowershell\v1.0\;c:\program files (x86)\intel\intel(r) management engine components\dal;c:\program files\intel\intel(r) management engine components\dal;c:\program files (x86)\intel\intel(r) management engine components\ipt;c:\program files\intel\intel(r) management engine components\ipt;c:\program files\dotnet\;c:\program files\microsoft sql server\130\tools\binn\;c:\program files (x86)\microsoft sql server\client sdk\odbc\130\tools\binn\;c:\program files (x86)\microsoft sql server\140\tools\binn\;c:\program files (x86)\microsoft sql server\140\dts\binn\;c:\program files (x86)\microsoft sql server\140\tools\binn\managementstudio\;c:\windows\system32\openssh\;c:\program files\git\cmd;c:\program files\microsoft sql server\client sdk\odbc\130\tools\binn\;c:\program files\microsoft sql server\140\tools\binn\;c:\program files\microsoft sql server\140\dts\binn\;c:\program files\java\jdk-12.0.1\bin;c:\program files\mysql\mysql shell 8.0\bin\;c:\progra~1\java\jdk-12.0.1;c:\bigdata\hadoop-3.1.2;c:\bigdata\hadoop-3.1.2\bin;c:\bigdata\hadoop-3.1.2\sbin; as you can see, there are a lot of spaces, however, in the cofngiuration f the variables i am using--> c:\progr~1 ... in order to avoid spaces problems. but, the problem is with my user ""david serrano"". the error says: error: could not find or load main class serrano caused by: java.lang.classnotfoundexception: serrano as you can see in the path there is not ""serrano"" word. so, my conclusion is that the problem is in my user. but i don't know how i can to avoid this. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 16 '19 dropdown menu copy link hide maybe it's doing something with your working directory path? try cd -ing to c:\ first, then running hadoop. i'm really not sure, though. like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 16 '19 dropdown menu copy link hide i already did that: c:>hadoop version error: could not find or load main class serrano caused by: java.lang.classnotfoundexception: serrano do you know which script of hadoop call the user profile? do you know if hadoop has some way to set up the name of the user profile in the scripts? like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 16 '19 dropdown menu copy link hide i don't, sorry, david. i'm not sure why that should be hard-coded anywhere, if it's not in your %path% and you're not in that directory. like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 16 '19 dropdown menu copy link hide well, here are some info (although it is a little bit old) that could give some clue about the problem: blog.benhall.me.uk/2011/01/install... i think i can do something similar to the advice in the above blog. however i need to know which is the variable that hadoop is using to call java in order to change it in the config files. if you have some info about it, please post here in order to try to solve the problem. thanks in advance. like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • sep 16 '19 dropdown menu copy link hide hadoop uses java_home to determine where your java distribution is installed. in a linux installation, there's a file called hadoop/etc/hadoop/hadoop-env.sh . it might be .cmd instead of .sh on windows, but i'm not sure. check out my other article on installing hadoop on linux . (search for ""java_home"" to find the relevant bit.) like comment: like comment: 1 like like thread thread david camilo serrano david camilo serrano david camilo serrano follow my name is david serrano.
i am .net developer, scrum master. i am working in software industry since 2009. location bogota, colombia work ing. software. at psl joined sep 11, 2019 • sep 16 '19 dropdown menu copy link hide yes, the java_home variable is fine in my laptop. however, hadoop must use in another part of its code the variable %username% or %userprofile%. those variables are the problematic thing. i need to locate that part in hadoop and try to change in some config file (if it is possible). actually i have another machine with ubuntu and hadoop works normally. the idea was installing on windows to do some specific work in both systems. i appreciate your attention, and if you get some new info about this kind of problems (user name with spaces and yarn problems in windows) please don't hesitate in posting it here. thanks a lot. like comment: like comment: 1 like like thread thread pranay pranay pranay follow joined mar 6, 2021 • mar 6 '21 dropdown menu copy link hide hey guys, i am also the same problem in my system due to space in my system user name >did you find any solution thanx in advance like comment: like comment: 1 like like comment button reply collapse expand پنوں پاکستانی پنوں پاکستانی پنوں پاکستانی follow data scientist location manchester work software engineer at job seeker joined mar 28, 2019 • mar 28 '19 dropdown menu copy link hide hi andrew when i run start-dfs.cmd and start-yarn.cmd command it gives me an error c:\java\jdk1.8.0_201\bin\java -xmx32m -classpath ""c:\hadoop\hadoop-3.1.2\etc\hadoop;c:\hadoop\hadoop-3.1.2\share\hadoop\common;c:\hadoop\hadoop-3.1.2\share\hadoop\common\lib*;c:\hadoop\hadoop-3.1.2\share\hadoop\common*"" org.apache.hadoop.util.platformname' is not recognized as an internal or external command, operable program or batch file. the system cannot find the file c:\windows\system32\cmd.exe\bin. the system cannot find the file c:\windows\system32\cmd.exe\bin. please help me like comment: like comment: 1 like like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • mar 28 '19 dropdown menu copy link hide hi پنوں, it looks like your system variables are mis-configured. the path c:\windows\system32\cmd.exe\bin doesn't make any sense, as cmd.exe is an executable, not a directory. double-check that you have the environment variables set correctly and let me know if you continue to have issues. like comment: like comment: 2 likes like comment button reply collapse expand پنوں پاکستانی پنوں پاکستانی پنوں پاکستانی follow data scientist location manchester work software engineer at job seeker joined mar 28, 2019 • mar 28 '19 dropdown menu copy link hide there was a problem with environment variables i was trying c:\windows\system32\cmd.exe\bin but that was prompting an error. but when i changed the system variable with c:\windows\system32\cmd.exe it was running fine. thank you boss for your help stay blessed. like comment: like comment: 4 likes like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • mar 28 '19 dropdown menu copy link hide happy to help! like comment: like comment: 1 like like comment button reply collapse expand chinanu chinanu chinanu follow joined oct 22, 2019 • oct 22 '19 dropdown menu copy link hide thanks andrew for this tutorial, it was very helpful. how does one address this encryption error: info sasl.sasldatatransferclient: sasl encryption trust check: localhosttrusted = false, remotehosttrusted = false i get it after i hadoop put a file. like comment: like comment: 1 like like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • oct 22 '19 dropdown menu copy link hide hi chinanu. i haven't encountered an error like this before, so unfortunately your guess is as good as mine. this site seems to suggest that it might be an issue with missing jar files? i'm not sure. like comment: like comment: 2 likes like comment button reply collapse expand michèle michèle michèle follow joined jul 20, 2019 • jul 20 '19 dropdown menu copy link hide thank you so much, after 4 tutorials and 3 days of trying it finally worked! yay!!! for those who might have the same problem as i did: when i used start-dfs.cmd and start-yarn.cmd it said the command couldn't be found. after a quick internet search i figured out that i needed to go to the sbin directory because it's in there and start it from there. worked fine then. like comment: like comment: 2 likes like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • jul 20 '19 dropdown menu copy link hide glad it worked! i actually went back to follow this guide again recently and skipped over the part where i say to add \sbin to the path , too. no worries! like comment: like comment: 1 like like comment button reply collapse expand parixitodedara parixitodedara parixitodedara follow joined feb 24, 2019 • feb 24 '19 dropdown menu copy link hide thanks for putting this together and sharing knowledge. i tried to get hadoop up and running on my windows machine last year, and it was painful! anywho, it encouraged me to put together a blog just like you - exitcondition.com/install-hadoop-w... keep exploring! like comment: like comment: 2 likes like comment button reply collapse expand nebrod666 nebrod666 nebrod666 follow joined aug 2, 2019 • aug 2 '19 dropdown menu copy link hide just signed to thank you for this tutorial. well explained and very clear. also, thanks for the link to the patch for bin files. i was only able to work with older versions of hadoop and almost tempted to try to build the bins on my own. cheers! like comment: like comment: 2 likes like comment button reply collapse expand anhle16 anhle16 anhle16 follow joined jan 27, 2020 • jan 27 '20 dropdown menu copy link hide hello andrew, thank you so much for your tutorial. i just have some questions hope you can help: after running start-dfs.cmd and start-yarn.cmd in cmd (boot hdfs step) i noticed that the yarn is working fine but namenode and datanode started for a few secs and then both stopped working for some reasons. any idea what might cause this issue? during the setting path process, i couldn't run the command: hdfs -version (i cd out to c:/user but i still have the same error error: could not find or load main class last name) so i edit /etc/hadoop/hadoop-env.cmd and change this line: set hadoop_ident_string=%username% to set hadoop_ident_string=myuser this allows me to do hdfs -version but i don't know this change will affect anything or not could you please clarify? is this change make my namenode and datanode not working like comment: like comment: 1 like like comment button reply collapse expand felicitas pojtinger felicitas pojtinger felicitas pojtinger follow pronouns she/her joined aug 26, 2018 • nov 6 '18 dropdown menu copy link hide but ... why? just get fedora and done ;) like comment: like comment: 1 like like comment button reply collapse expand andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • nov 6 '18 dropdown menu copy link hide client-specified software that only runs on windows server :/ like comment: like comment: 1 like like comment button reply collapse expand felicitas pojtinger felicitas pojtinger felicitas pojtinger follow pronouns she/her joined aug 26, 2018 • nov 6 '18 dropdown menu copy link hide well, that's sad. have you thought about using smth. like an iis container for those proprietary blobs? like comment: like comment: 1 like like thread thread andrew (he/him) andrew (he/him) andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. email aww@awwsmm.com location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 • nov 6 '18 dropdown menu copy link hide i haven't, no... how would that work? can you point me to any good resources? like comment: like comment: 1 like like thread thread felicitas pojtinger felicitas pojtinger felicitas pojtinger follow pronouns she/her joined aug 26, 2018 • nov 7 '18 dropdown menu copy link hide see the docker hub for more info, although i don't use it personally (i use & write floss exclusivly) like comment: like comment: 2 likes like comment button reply collapse expand karthik karthik karthik follow joined nov 1, 2019 • nov 1 '19 dropdown menu copy link hide hi andrew, try syncfusion bigdata studio and syncfusion cluster manager products. it has builtin hadoop ecosystems for windows platform. much easier to install and configure hadoop ecosystems in windows. like comment: like comment: 1 like like comment button reply view full discussion (60 comments) code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse andrew (he/him) follow got a ph.d. looking for dark matter, but not finding any. now i code full-time. je parle un peu français. location ottawa, canada education ph.d. in [astroparticle] physics pronouns he / him work principal consultant at improving joined sep 15, 2018 more from andrew (he/him) 5 tips for writing articles people will want to read # writing # beginners # tutorial super simple markdown # markdown # writing # beginners # tutorial installing and running hadoop and spark on ubuntu 18 # hadoop # spark # java # scala 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://dev.to/willvelida/streaming-data-in-databricks-delta-tables-4n9p,,,"streaming data in databricks delta tables - dev community skip to content navigation menu search powered by search algolia log in create account dev community close add reaction like unicorn exploding head raised hands fire jump to comments save boost more... moderate copy link copy link copied to clipboard share to x share to linkedin share to facebook share to mastodon report abuse will velida posted on jul 23, 2018 • originally published at medium on jul 23, 2018 streaming data in databricks delta tables # data # azure # streaming # bigdata databricks delta uses both apache spark and databricks file system (dbfs) to provide a transactional storage layer that can do incredible things for us as data engineers. databricks delta is a optimized spark table that stores data in parquet file format in dbfs and it uses a transaction log that efficiently tracks changes to a table. in this tutorial, we’re going to stream some tweets from twitter that contains the #azure string, send that to azure event hubs and then writes and appends those tweets to a table. this tutorial builds on the sentiment analysis on streaming data using azure databricks tutorial that’s on the azure docs. if you’re just starting out with streaming in databricks, i recommend you check it out as you get to work with some pretty cool azure services. in this tutorial, instead of printing out to the console, we will append the data as it comes to a delta table that we can store inside a database. if you wish to follow along with this tutorial, make sure you complete the azure one first so you know what i’m on about. also bear in mind that at the time of writing, databricks delta requires a databricks runtime of 4.1 or above. in order to write our data to a delta table, we’ll do the following things: save the streamed data to parquet files as they come in to a sink in dbfs. read the parquet files and then append each file to a table called ‘tweets’ let’s crack on! save the streamed data in a sink we can write data to a databricks delta table using structured streaming. in this tutorial, we’ll write our stream to a path, which we’ll use to add new records to the table we’re about to create as it comes. write the following scala code at the end of analyzetweetsfromeventhubs: streamingdataframe.writestream
  .format(""delta"")
  .outputmode(""append"")
  .option(""checkpointlocation"", ""/delta/events/checkpoints/tweets"")
  .start(""/delta/tweets"") in this block of code, we’re appending our stream in delta format to the location /delta/tweets. you might be asking ‘what the hell is delta format?’ the answer is read the parquet files and append to tweets table now that we’ve got our sink, let’s create a table for these files to be written to. underneath your sink code, write the following scala code: val tweets = spark.read.parquet(""/delta/tweets"")
tweets.write.format(""delta"").mode(""append"").saveastable(""tweets"") here, we create a value called tweets that reads our streamed parquet files, then we write those formats to a table called tweets. we can view this table by clicking on the data icon in the ui. we can have a look inside our table and we can see that rather than having the content and sentiment printed to the console, we have it inside a table within databricks we can also view the contents inside our notebook using display: val tweetstable = spark.table(""tweets"")
display(tweetstable) your output might be a little different depending on when you run it, but here’s what it could look like: in conclusion in this tutorial, we used the power of databricks delta to take our streaming data and put it into a table that we can run queries against and store in a database within databricks. we could extend this example further by joining it with some static data or in spark version 2.3.0+, join it to another streaming dataset. this example used azure event hubs, but for structured streaming, you could easily use something like apache kafka on hdinsight clusters . top comments (3) subscribe personal trusted user create template templates let you quickly answer faqs or store snippets for re-use. submit preview dismiss collapse expand swati arora swati arora swati arora follow joined aug 7, 2018 • aug 7 '18 • edited on aug 7 • edited dropdown menu copy link hide hi will, thanks for amazing write up. but i am facing an issue while executing cmd: tweets.write.format(""delta"").mode(""append"").saveastable(""tweets"") for the first time the data is stored in delta table, but executing it again gives me the error: ""org.apache.spark.sql.analysisexception: cannot create table (' default . tweets '). the associated location ('dbfs:/user/hive/warehouse/tweets') is not empty.; "" how can i make sure the data is continuously getting stored in table format as well. thanks in advance like comment: like comment: 1 like like comment button reply collapse expand muhammad bilal shafqat muhammad bilal shafqat muhammad bilal shafqat follow location pakistan work big data engineer at bentley systems joined apr 10, 2020 • apr 10 '20 dropdown menu copy link hide hey will nice post, well i think, i would directly write data to delta table instead of writing it first to parquet files because if i will write them as parquet and then read them in delta table then only first time row present in parquet files on dbfs will get ingested into table, and rows coming after that they will not get ingested into table and i would have to manually run read.saveastable() to get them ingested into delta table, try it, please share your thoughts. thanks. like comment: like comment: 2 likes like comment button reply collapse expand parath kumar sabesan parath kumar sabesan parath kumar sabesan follow joined feb 16, 2021 • feb 16 '21 dropdown menu copy link hide hi will, when i try to write a streaming data to a partitioned managed delta table it's not loading data into it and also it's not showing any error, but the same thing working fine with non partitioned managed delta table what i'm missing here ?? dfwrite.writestream\ .partitionby(""submitted_yyyy_mm"")\ .format(""delta"")\ .outputmode(""append"")\ .queryname(""orders"")\ .option(""checkpointlocation"", orders_checkpoint_path)\ .table(user_db+"".""+orders_table) like comment: like comment: 1 like like comment button reply code of conduct • report abuse are you sure you want to hide this comment? it will become hidden in your post, but will still be visible via the comment's permalink . hide child comments as well confirm for further actions, you may consider blocking this person and/or reporting abuse will velida follow senior software engineer at microsoft location australia education university of auckland work senior software engineer at microsoft joined sep 30, 2017 more from will velida building remote mcp servers with .net and azure container apps # dotnet # csharp # azure # ai how tracing works in azure ai foundry agents # azure # ai # python creating an aks automatic cluster with your own custom vnet in bicep # azure # kubernetes # devops # tutorial 💎 dev diamond sponsors thank you to our diamond sponsors for supporting the dev community google ai is the official ai model and platform partner of dev neon is the official database partner of dev algolia is the official search partner of dev dev community — a space to discuss and keep up software development and manage your software career home dev++ podcasts videos tags dev help forem shop advertise on dev dev challenges dev showcase about contact free postgres database software comparisons code of conduct privacy policy terms of use built on forem — the open source software that powers dev and other inclusive communities. made with love and ruby on rails . dev community © 2016 - 2025. we're a place where coders share, stay up-to-date and grow their careers. log in create account",12
https://www.linkedin.com/pulse/testing-strategies-apache-spark-applications-ensuring-reliability-npvxf,JUnit,Integration Testing; Regression Testing; Unit Testing,"testing strategies for apache spark applications: ensuring reliability at scale agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in testing strategies for apache spark applications: ensuring reliability at scale report this article infomover technologies infomover technologies hire developers backed by our - workforce productivity platform that provides automated ""timesheets & logged hours"" published jan 2, 2024 + follow in the dynamic world of big data processing, apache spark has emerged as a powerful framework for handling large-scale data analytics and processing tasks. as organizations increasingly rely on spark applications to extract valuable insights from vast datasets, the importance of rigorous testing cannot be overstated. in this article, we'll explore key strategies for testing spark applications to ensure reliability and performance at scale. 1. unit testing: laying the foundation begin your testing journey by establishing a robust foundation with unit testing. utilize tools like junit or scalatest to test individual components and functions within your spark application. by isolating and validating each unit of code, you can identify and address issues early in the development cycle. 2. integration testing: ensuring harmony conduct comprehensive integration testing to validate the seamless interaction between different components of your spark application. focus on testing the connections and data flows between rdds or dataframes to guarantee that the overall system functions as intended. 3. local testing: early identification of issues before deploying your spark application on a large cluster, conduct testing on a local machine or a small cluster. this allows for the early identification of issues and ensures a smoother transition to a larger production environment. 4. mocking: simulating real-world scenarios employ mocking frameworks to simulate the behavior of external systems or services. this enables you to test your spark application in isolation, providing a controlled environment to identify and rectify potential issues. 5. spark testing base: leveraging testing libraries consider integrating testing libraries like ""spark-testing-base"" (available for scala) into your testing toolkit. these libraries offer utility classes and methods specifically designed for testing spark applications, streamlining the testing process. 6. cluster testing: simulating production realities conduct testing on a real cluster that simulates the production environment. utilize tools such as the standalone cluster or apache mesos to assess how your spark application performs in a distributed setting, identifying and addressing any cluster-specific challenges. 7. data validation: ensuring correct transformations verify the correctness of data transformations and processing within your spark application. implement thorough data validation checks to compare input and output data, ensuring that transformations are applied accurately. 8. performance testing: evaluating scalability assess the performance of your spark application by simulating various workloads and data sizes. tools like apache jmeter or gatling can be adapted to evaluate scalability, helping you identify potential bottlenecks and optimize performance. 9. fault tolerance testing: ensuring resilience test the fault tolerance of your spark application by introducing faults or failures in the cluster. validate that the application can recover gracefully and continue processing without data loss, ensuring robustness in the face of unexpected challenges. 10. scalability testing: adapting to growing workloads evaluate the scalability of your spark application by gradually increasing data volume and cluster size. this testing strategy helps you identify and address scalability issues, ensuring that your application can handle growing workloads without compromising performance. 11. security testing: safeguarding sensitive data if your spark application deals with sensitive data, prioritize security testing. ensure that data is protected during processing and storage, implementing rigorous security measures to safeguard against potential vulnerabilities. 12. logging and monitoring: insightful diagnostics implement comprehensive logging and monitoring in your spark application. these practices provide valuable insights into issues, bottlenecks, and performance anomalies during testing and in production, enabling proactive troubleshooting. 13. data quality testing: verifying data integrity verify the quality of the data processed by your spark application. establish checks for data completeness, accuracy, and consistency throughout the processing pipeline to maintain high data integrity. 14. regression testing: preserving functionality establish a suite of regression tests to ensure that new features or changes do not introduce defects in existing functionality. this helps maintain the reliability of your spark application over time, preserving its core capabilities. 15. documentation and knowledge transfer: sharing insights document your testing processes, methodologies, and results. this documentation serves as a valuable resource for knowledge transfer among team members, fostering consistency in testing practices and ensuring collective understanding. in conclusion, testing apache spark applications demands a holistic approach that addresses the unique challenges posed by distributed and large-scale processing environments. by incorporating these testing strategies into your development lifecycle, you can ensure the reliability, scalability, and performance of your spark applications, paving the way for data-driven insights that propel your organization forward. #bigdata #spark #testingstrategies #dataanalytics links with this icon were created by linkedin and links without it were added by the author. like comment copy linkedin facebook twitter share 7 to view or add a comment, sign in sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/distributed-big-data-processing-pysparkpandas-pros-cons-joshi,,,"distributed big data processing with pyspark.pandas - pros and cons agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in distributed big data processing with pyspark.pandas - pros and cons report this article yash mahendra joshi yash mahendra joshi no titles, just learning and relearning constantly published aug 10, 2022 + follow pandas is one of world's most loved open-source data analysis and manipulation library built on top of the python programming language. cherished by data scientists and data engineers alike, pandas acts as a wrapper over two core python libraries—matplotlib for data visualization and numpy for mathematical operations. yet it cannot always hold upto the challenges of the big data world where we have to load sometimes gigabytes, terabytes or even exabytes of data for solving key customer problems. since apache spark 3.2, databricks has brought about a wonderful innovation to distribute pandas over many machines using pyspark pandas. yes, you read that right : to distribute pandas computation over many machines means we can scale much more easily , and therefore read & write much bigger amounts of data. it also means the speed of transformation can be vastly increased. the pandas api on spark project (a replacement for koalas) makes data scientists more productive when interacting with big data, by implementing the pandas dataframe api on top of apache spark. by unifying the two ecosystems with a familiar api, pandas api on spark offers a seamless transition between small and large data. when your dataset is less than 1000 rows, pandas by default is applied on pyspark.pandas datasets . else, you simply get your scaling benefits: look at the results of this 31 gb test. we've had dask and other solutions that have tried this in the past, with varying levels of success, but not normally that simple to execute on a simple jupyter notebook-like interface. #databricks on the other hand, seems to have come up with an enticingly simple solution : simply replacing an 'import pandas as pd' by an 'import pyspark.pandas as ps' in your notebook, and we can scale up effortlessly. plus , we can read #delta tables , that were not possible with pure pandas, and make queries using spark sql on delta tables in a #lakehouse . wow! that's bananas! but is it really that simple ? is it really that simple ? not quite so in reality. firstly we must know that the class changes : pandas.core.frame.dataframe for pure pandas and pyspark.pandas.frame.dataframe for pandas on spark.  hence, your topandas() must be replaced with to_pandas_on_spark() in your code when working with this new class. the concat and merge function in pandas on spark does not allow lists, but only a ps.dataframe or ps.series. so you have to make a list into a dataframe using ps.dataframe(list) before trying to use ps.concat or ps.merge. when reading excel files on pandas on spark, we must precise the data type, other read_excel function does not work. for example;  df_mag_reference = spd.read_excel(path, dtype=str ) normally in a pure pandas dataframe, we can make a list directly from a dataframe column. but in pandas on spark, we are obliged to add to_numpy() before making this treatment. when a pandas-on-spark dataframe is converted from a spark dataframe, it loses the index information, which results in using the default index in pandas api on spark dataframe. the default index is inefficient in general comparing to explicitly specifying the index column, from a performance perspective. therfore, we should to specify the index column whenever possible when doing conversions. finally, there are issues with dates when working with pandas_on_spark. if your key has a date in it, be warned !! all that being said, i have worked on two real-life use cases where using pandas on spark has been a life saver.  one of these is when we've had to write in delta lake format for big data use cases, and clearly that is one enormous advantage of pyspark.pandas. to summarize, there are some differences with pandas and some bugs that we can encounter, but the first version of the pyspark pandas api looks to be a winner for big data use cases. keep in mind, it's still not as fast as implementing your code in pure pyspark or scala. ;) for more information and the docs: https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html like comment copy linkedin facebook twitter share 32 to view or add a comment, sign in more articles by yash mahendra joshi unlocking data freedom and governance: why and how you should move to unity catalog apr 28, 2025 unlocking data freedom and governance: why and how you should move to unity catalog unity catalog has been around for a while now. it's even been famously open-sourced at last year's databricks data&ai… 30 apache spark 1.0 to 4.0 : key advancements of this big-data framework's 14 year journey. jun 27, 2024 apache spark 1.0 to 4.0 : key advancements of this big-data framework's 14 year journey. apache spark: a milestone in data engineering apache spark is an open-source distributed computing system which… 16 2 comments embracing excellence: my journey to becoming a databricks champion at accenture jun 17, 2024 embracing excellence: my journey to becoming a databricks champion at accenture constructing a house takes time, and databricks is a bit like masonry – you have different building blocks (from… 37 2 comments data lineage on databricks: what's good and what could be better dec 1, 2023 data lineage on databricks: what's good and what could be better finally been able to get my hands on #unitycatalog set up on #databricks, and play around with #datalineage, which is a… 26 1 comment delta live tables on databricks. when should you use them and when should you not ? jul 15, 2023 delta live tables on databricks. when should you use them and when should you not ? delta live tables (dlts) with #databricks are great to use but come with their own constraints. i have had the pleasure… 84 5 comments what tools does a good data engineer in 2022 need to know : part 1 jan 22, 2022 what tools does a good data engineer in 2022 need to know : part 1 it's been a while since i last wrote an article on linkedin. during the last 6 months or so, my competencies in the… 37 1 comment learning to tackle big data problems the right way nov 17, 2021 learning to tackle big data problems the right way we live in a world of exponentially increasing data. data that is growing in volumetry, in velocity, and in variety. 36 3 comments making machine learning forecasts work  : a use case of bitcoin forecasting oct 8, 2021 making machine learning forecasts work  : a use case of bitcoin forecasting forecasting is a complex domain, but not one that is impossible to do with the help of machine learning. firstly, let's… 31 4 comments what to expect when you are a developer in the ai field: some key lessons i have learnt. jun 9, 2021 what to expect when you are a developer in the ai field: some key lessons i have learnt. functional knowledge is key to success in ml/ai projects. having worked in the supply chain, it's very important to… 34 1 comment from zero to hero: how to learn power bi quickly - excerpts from my learning journey. mar 28, 2021 from zero to hero: how to learn power bi quickly - excerpts from my learning journey. “learning never exhausts the mind.” – leonardo da vinci let me be very honest with you, power bi wasn't at all on my… 80 5 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/tidy-production-pandas-hamilton-stefan-krawczyk,,Integration Testing; Unit Testing,"tidy production pandas with hamilton agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in your pandas code with hamilton. image from pixabay. tidy production pandas with hamilton report this article stefan krawczyk stefan krawczyk agentforce. | co-creator of apache hamilton & apache burr | pipelines & agents: data, data science, machine learning, & llms published jul 26, 2022 + follow subtitle: writing production-grade pandas code with hamilton. note: this post originally appeared in towards data science . “ tidy ” & “ pandas ” are not two words one would often associate together, let alone with the word “ production ”. in this post i’ll argue that if you’re using pandas in production, you should be using hamilton , an open source micro-framework, as it enables you to write tidy and production grade code by default. you might be thinking? is this post about the r tidyverse equivalent but for pandas? yes, in the spirit of tidy code, but no in terms of how it’s achieved. also, just to ground our terms before we get started: production : by production we mean that this code needs to be run in order for a business process to function. e.g. creating features for a machine learning model, or transforming data for ingestion into a dash boarding tool. tidy: by tidy we mean code that is readable, maintainable and testable. we over index a little on the ability for your code to live a long time; your production code generally lives a longer life that you intended it to, so let’s make it easy to maintain. another way to think about it, is that we want your pandas code to easily facilitate software engineering best practices. pandas production problems there is a lot of disagreement in the industry about whether pandas code should ever be run in production. while it’s clearly great for quick prototyping and research, pandas heavy codebases often end up tripping over themselves; software engineering best practices are hard to follow. this next section should hopefully be all head nods; common paint points that are felt when using pandas in production. integration testing & unit testing is a challenge pandas code is commonly written as a linear python script. that’s most likely because it first started as code in various cells from within a jupyterhub notebook, that are themselves linearly executed. this approach makes it difficult to test over time. sure, as you’re developing the script, you’re “testing” it as you go. but once it’s in production, time can pass and data & context can change. for example, if you need to adjust production pandas code that’s running, how do you gain confidence that the change you’re making won’t have adverse effects? unit testing is likely non-existent, or the test coverage is spotty; writing inline pandas manipulations is easy to do and hard to programmatically test. while integration testing generally involves running your entire script. if it’s doing a lot of computation, this means a slow iteration cycle to test everything, or perhaps skipping testing altogether. documentation is non-existent documentation is critical to collaboration and code maintenance. if you’re used to writing pandas code that creates a bunch of columns inline (because it’s easy to do) you invariably sacrifice documentation. documentation should be easily surface-able and in sync with the code. once you start to place documentation outside the code, it’s very easy for it to get out of date… the code is hard to reuse it’s common to see a single python file that contains all your pandas code: logic to pull the data, logic to perform transforms, and logic to save the output. this means that a transform function in this script, isn’t accessible or reusable in an easy manner. some people retort with “i’ll just refactor”, but how many times does that happen? people end up cutting & pasting code, or reimplementing the same logic over and over. no one understands your pandas code but you as a former data scientist, i know firsthand the horror of inheriting someone else’s code. pandas is a powerful tool, but everyone wields it differently. with the above concerns of testing, documentation, and reusability, taking ownerships of someone else’s pandas code is daunting, to say the least. hamilton hamilton was built to solve the exact problems elicited above — turning a messy pandas codebase into something tidy . no, not exactly the same as the r tidyverse that you might know and love, but in the same spirit… what is hamilton? hamilton is a declarative paradigm for specifying dataflows . that’s just an academic way of saying: you write python functions that declare what they output and what they depend on, by encoding it directly in the function definition. you write these python functions to specify how data and computation should flow, i.e. a dataflow (aka pipeline/workflow). in code this means that, rather than writing:
df['age_mean'] = df['age'].mean()
df['age_zero_mean'] = df['age'] - df['age_mean'] you write # a_function_module.py
def age_mean(age: pd.series) -> float:
    """"""average of age""""""
    return age.mean()

def age_zero_mean(age: pd.series, age_mean: float) -> pd.series:
    """"""zero mean of age""""""
    return age - age_mean you then have to write a bit of “ driver ” code to actually perform computation. the “ driver ” code’s responsibility is to instantiate what functions can and should be computed, and how. import pandas as pd
from hamilton import driver
import a_function_module # where your transforms live

config_and_inputs = {'age': pd.series([...])}
dr = driver.driver(config_and_inputs, a_function_module)
# here we're constructing a data frame with only two columns
df = dr.execute(['age', 'age_zero_mean']) we’ll skip an extensive introduction to hamilton here in lieu of links to prior introductions: how to use hamilton with pandas in 5 minutes general backstory & introduction on hamilton otherwise you just need pip install sf-hamilton to get started. why does using hamilton result in tidy production pandas code? here are the four main reasons: testable code, always . documentation friendly code, always . reusable logic, always . runtime data quality checks, always . let’s use the following function to discuss these points: tag(owner='data-science', pii='false')
@check_output(data_type=np.float64, range=(-5.0, 5.0), allow_nans=false)
def height_zero_mean_unit_variance(height_zero_mean: pd.series,
	                               height_std_dev: pd.series) -> pd.series:
	   """"""zero mean unit variance value of height""""""
	   return height_zero_mean / height_std_dev) testable code, always hamilton forces you to write functions decoupled from specifying how data gets to the function. this means that it is inherently straightforward to provide inputs for unit testing , always. in the function above, providing height_zero_mean and height_std_dev is just a matter of coming up with a representative pandas series for each one to test this function; in defining the function, we did not specify how the inputs are going to be provided. similarly one can easily test the above function end to end with hamilton. you only need to specify computing height_zero_mean_unit_variance to the “ driver ” code, and it will execute the functions required to produce height_zero_mean_unit_variance. that way even integration testing cycles can be relatively quick. you do not need to run your entire script and compute everything to test a single change. i.e.: df = dr.execute(['height_zero_mean_unit_variance']) documentation friendly code, always. hamilton has four features that help with documentation: functions . by using functions as the abstraction, one can naturally insert documentation through the function’s docstring . this can then connect with tooling such as sphinx to surface these more broadly. naming . hamilton forces naming to be front and center in your mind. as the name of a column requested by the driver .execute() function, corresponds with a function written by you (or a colleague), descriptive, concise names evolve to be the norm. furthermore, the code reads naturally and intuitively, from function name to function arguments; it’s very hard to name anything important ` foobar ` when using hamilton. visualization . hamilton can produce a graphviz file & image that can produce a graphical representation of how functions tie together. this is a key tool to help someone grok the big picture. for example, see the example visualization below. tags . hamilton enables one to assign tags (key value pairs) by annotating a function. as the above example demonstrates, @tag(owner='data-science', pii='false') provides extra metadata to help code readers understand, for example, who owns the code and whether it contains personal identifying information. example rendering that hamilton can produce. taken with permission from https://outerbounds.com/blog/developing-scalable-feature-engineering-dags . reusable logic, always. to be useful, the function above needs to be curated in a python module. because that module is not coupled to the “driver” code, it is very easy to refer to that function in a variety of contexts: multiple drivers can use that same function. those drivers could construct the dag differently, e.g. by loading data from different locations. so you have code reuse from day one. if you have a python repl , it’s easy to import the function and run it. you can publish the function’s module and version it for reuse and reusability. in addition, as hamilton forces all core logic to be in functions that are decoupled from “driver code”, it is easy for hamilton to provide out of the box ways to scale computation. frameworks like ray and dask are straightforward to switch on and integrate. all you need to do is change a few lines in your “driver” code to do so. see hamilton’s ray & dask examples for more information. runtime data quality checks, always. unit testing is valuable, but it does not replace validating assumptions in production. rather than employing separate tasks (or even a separate function) to check your data. hamilton enables the use of a simple decorator to run validations over the output of a function at runtime: @check_output(data_type=np.float64, range=(-5.0, 5.0), allow_nans=false) this makes it easy for someone who does not understand the context of the code to grok a few basic properties of the output. as the decorator lives adjacent to the transform function definition, it is much simpler to maintain. there is no separate system to update — you can do it all in a single pull request! when the function is executed and validation fails, current options are to log a warning or throw an exception. this is a very quick and easy way to ensure what’s running in production matches your expectations. for those who prefer the power of pandera , rejoice! data validation in hamilton comes out of the box with a full integration — you can pass a pandera schema to the decorator . additional benefits aside from making your pandas code base tidy, hamilton also helps in these more macro aspects of your development workflow with pandas. faster iteration cycles. once you have hamilton up and running, the flexibility of adding, changing, and adjusting what your code does is straightforward. you can: develop in a test driven manner . easily test your changes by requesting only what is required to compute them debug methodically by tracing computational data lineage. you start with the function, debug that logic, and if the problem lies elsewhere, you can iteratively recurse through the inputs of the function. create drivers for multiple contexts very easily, leveraging the power of your prior defined functions. faster onboarding. as a consequence of writing functions with various hooks for documentation, ramping up new hires becomes a much simpler task. exploring the code base can be done graphically, and running and testing code is straightforward to explain. less time spent on code maintenance and upkeep by design, hamilton makes it easy to follow software engineering best practices. this means maintaining, inheriting, or even handing off code is very manageable. it also means that it’s simple to make all your transform logic appear uniform and aesthetically pleasing (for example see links in the next section) and keep it that way. a realistic example i’ve touted the benefits, but what does the code actually look like? here are some examples: > combining hamilton with metaflow : see normalized_features.py and feature_logic.py . 🤔 rhetorical question: how would you feel inheriting this code? accompanying blog post > an example in the hamilton repository: data quality (based on the example above, but includes @check_output annotations). to conclude code lives for much longer than you generally anticipate. making sure it is easy to write, maintain, and accessible to whomever comes after you, is a key ingredient in making pandas work in a production environment. hamilton with pandas helps you do all that. using hamilton with pandas results in tidy production code that, no matter the author, can be maintained and scaled, both computationally (e.g. ray , dask ) and organizationally. we’d love for you to: 💪 try hamilton if you have not yet. justpip install sf-hamilton to get started. ⭐️ us on github , 📝 leave us an issue if you find something, 📣 join our community on slack — we’re more than happy to help answer questions you might have or get you started. other hamilton posts you might be interested in: how to use hamilton with pandas in 5 minutes how to use hamilton in a notebook environment general backstory & introduction on hamilton developing scalable feature engineering dags (hamilton with metaflow) links with this icon were created by linkedin and links without it were added by the author. like comment copy linkedin facebook twitter share 29 to view or add a comment, sign in more articles by stefan krawczyk february updates feb 20, 2025 february updates tl;dr: #hamilton highlights: crossed 2000 github stars, released multithreading based dag parallelism, richprogressbar… 40 3 comments last week of 2024 / first week of 2025 jan 2, 2025 last week of 2024 / first week of 2025 tl;dr: #hamilton + #burr 2024 stats: 35m+ telemetry events (10x), 100k+ unique ips (10x) from 1000+ companies, 1m+… 33 3 comments week of december 9th dec 13, 2024 week of december 9th tl;dr: #hamilton release highlights: better typeddict support and modular subdag example office hours & meet ups for… 11 week of december 2nd dec 5, 2024 week of december 2nd tl;dr: #hamilton release highlights: async datadog integration, polars & pandas with_columns support. #burr release… 13 week of november 18th nov 22, 2024 week of november 18th tl;dr: #hamilton release highlights: sdk configurability #burr release highlights: parallelism ui modifications, video… 8 week of november 11th nov 15, 2024 week of november 11th tl;dr: #hamilton release highlights: async support for @pipe + various small fixes #burr release highlights:… 8 week of november 4th nov 8, 2024 week of november 4th tl;dr: #hamilton release highlights: @with_columns decorator for pandas by jernej frank & module overrides for async… 5 week of october 28th oct 31, 2024 week of october 28th tl;dr: #hamilton release highlights: in-memory cache store. #burr release highlights: release candidate for a first… 13 week of october 21st oct 24, 2024 week of october 21st tl;dr: #hamilton release highlights: some minor fixes and docs updates from five different os contributors! also… 12 week of october 14th oct 17, 2024 week of october 14th tl;dr: announcing shreya shankar as an advisor. #hamilton release highlights: tweaks to pipe_input, new… 32 3 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/role-apache-pig-big-data-analytics-priyanka-thota,,,"role of apache pig in big data analytics agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in role of apache pig in big data analytics report this article priyanka thota priyanka thota associate software engineer at bosch published apr 23, 2022 + follow introduction to apache pig pig was developed by yahoo in the year 2006 so that it could be used for software testing performed without planning and documentation (but can be applied to early scientific experimental studies) while creating and executing mapreduce jobs on huge data sets. pig is a high-level platform or tool which is used to process large datasets. it provides a high level of abstraction for processing over mapreduce. it provides a high-level scripting language , known as pig latin which is used to develop the data analysis codes. the two main components of pig are pig latin and pig engine a good example of a pig application is the etl transaction model that describes how a process will extract data from a source, transform it according to a rule set and then load it into a datastore. pig is used to process the data stored in hdfs, where the programmers first write the scripts using pig latin language, and then internally pig engine converts all these scripts into a specific map and reduces the task. and at last, the result gets stored in hdfs. as pig is a high-level scripting language that is used with apache hadoop , it enables data workers to write complex data transformations without knowing java. pig is used to perform all kinds of data manipulation operations in hadoop. it provides the pig-latin language that contains many inbuilt functions like join, filter, sort, load, group, etc data types: 1) simple data types - int, long, float, double, bytearray, chararray, boolean, datetime, biginteger, bigdecimal. 2) complex data types - tuple, bag, map. processing data with apache pig step 1: you first need to first have the path which you need the data to be downloaded from. step 2: upload the data so that you can process step 3: open pig to create your script step 4: define relation step 5: click execute to run the script view the data after the process is complete the dump command is used to view the data, and you should add it to the pig script, save and execute it. you can sort it using the order by option. both inline comments '--' and block comments /* */ are supported in apachepig pig commands load - reads data from the system --load the content of a file into a pig bag named ‘input_lines'

input_lines = load 'changes.txt' as (line:chararray); store - writes data to file system -- store the results ( executes the pig script )

store ordered_word_count into 'output’; foreach - applies expressions to each record and outputs one or more records -- basic syntax

alias1 generate alias2 = foreach expression; filter - applies predicate and removes records that do not return true --basic syntax

alias2 = filter alias1 by expression; group/cogroup - collects records with the same key from one or more inputs --basic syntax

alias2 = group alias1 by colummn; join - joins two or more inputs based on a key --basic syntax (inner join/outer join)

alias3 = join alias1 by col1, alias2 by col1; order - sorts records based on a key -- order the records by count

ordered_word_count = order word_count by count desc; distinct - removes duplicate records --basic syntax:

alias2 = distinct alias1; union - merges data sets --basic syntax

relation_name3 = union relation_name1, relation_name2; split - splits data into two or more sets based on filter conditions --basic syntax

split relation1_name into relation2_name if (condition1), relation2_name (condition2), stream - sends all records through a user-provided binary dump - writes output to stdout --basic syntax

dump name; limit - limits the number of records --basic syntax

result = limit relation_name required number of tuples; these pig commands can be written either in interactive mode or batch mode. interactive mode – by using the grunt shell by invoking pig on the command line $ pig

-- grunt shell gets opened

grunt> batch mode – run pig in batch mode using pig scripts and the ""pig"" command $ pig –f id.pig –p <param>=<value> ... a dvantages of apache pig less code - the pig consumes fewer lines of code to perform any operation. using apachepig is very easy for non-java programmers as well. reusability - the pig code is flexible enough to reuse again. nested data types - the pig provides a useful concept of nested data types like tuple, bag, and map.eferred from - referred from - conclusion allows the user to create custom user-defined functions that analyze larger sets of data representing them as data flows. as pig is generally used with hadoop; we can perform all the data manipulation operations in hadoop using pig at ease. like comment copy linkedin facebook twitter share 27 1 comment prudhvi raj krosuru working as research analyst at sandspace technologies 
aws certified developer-associate | aviatrix certified engineer|student at kluniversitydata science & business analytics intern  at the sparks foundation 3y report this comment this is too good like reply 1 reaction to view or add a comment, sign in more articles by priyanka thota association rules in data mining nov 12, 2021 association rules in data mining hello, connections :) i am priyanka thota, and i'm here to share my view regarding the association rules in data mining… 25 1 comment a glimpse on our ongoing project, projeclick - a project management system mar 18, 2021 a glimpse on our ongoing project, projeclick - a project management system skill development project - 2 hello connections!! me, yaswanth and haswanth as a team are working on a skill… 27 sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/learning/apache-spark-essential-training-big-data-engineering-2021,,,"apache spark essential training: big data engineering online class | linkedin learning, formerly lynda.com skip to main content learning linkedin learning search skills, subjects, or software expand search this button displays the currently selected search type. when expanded it provides a list of search options that will switch the search inputs to match the current selection. jobs people learning clear text clear text clear text clear text clear text start free trial sign in solutions for: business higher education government buy for my team all topics business business software and tools data analysis preview apache spark essential training: big data engineering with kumaran ponnambalam liked by 116 users duration: 1h 4m skill level: intermediate released: 10/1/2024 start my 1-month free trial buy for my team course details data engineering is the foundation for building analytics and data science applications in the new big data world. data engineering requires combining multiple big data technologies to construct data pipelines and networks to stream, process, and store data. this course focuses on building full-fledged solutions that combine apache spark with other big data tools to create end-to-end data pipelines. instructor kumaran ponnambalam begins by defining data engineering, its functions, and its concepts. next, kumaran goes over how spark capabilities such as parallel processing, execution plans, state management options, and machine learning work with extract, transform, load (etl). he introduces you to batch processing use cases and processes, as well as real-time processing pipelines. after taking you through several useful best practices, kumaran concludes with an end-to-end exercise project. show more show less skills you’ll gain data pipelines data engineering apache spark big data earn a sharable certificate share what you’ve learned, and be a standout professional in your desired industry with a certificate showcasing your knowledge gained from the course. learning linkedin learning certificate of completion showcase on your linkedin profile under “licenses and certificate” section download or print out as pdf to share with others share as image online to demonstrate your skill meet the instructor kumaran ponnambalam ai / ml leader & author learner reviews 4.7 out of 5 24 ratings how are ratings calculated? the overall rating is calculated using the average of submitted ratings. ratings and reviews can only be submitted when non-anonymous learners complete at least 40% of the course. this helps us avoid fake reviews and spam. 5 star current value: 18 75% 4 star current value: 5 21% 3 star current value: 1 4% 2 star current value: 0 0% 1 star current value: 0 0% contents introduction introduction driving big data engineering with apache spark 48s (locked) course prerequisites 1m 18s (locked) setting up the exercise files 6m 2s 1. data engineering concepts 1. data engineering concepts what is data engineering? 1m 34s (locked) data engineering vs. data analytics vs. data science 1m 19s (locked) data engineering functions 3m 9s (locked) batch vs. real-time processing 2m 15s (locked) data engineering with spark 1m 7s 2. spark capabilities for etl 2. spark capabilities for etl spark architecture review 2m 10s (locked) parallel processing with spark 3m 9s (locked) spark execution plan 1m 11s (locked) stateful stream processing 2m 18s (locked) spark analytics and ml 1m 58s 3. batch processing pipelines 3. batch processing pipelines batch processing use case: problem statement 1m 43s (locked) batch processing use case: design 1m 44s (locked) setting up the local db 1m 53s (locked) uploading stock to a central store 3m 51s (locked) aggregating stock across warehouses 2m 46s 4. real-time processing pipelines 4. real-time processing pipelines (locked) real-time use case: problem 1m 50s (locked) real-time use case: design 1m 41s (locked) generating a visits data stream 1m 48s (locked) building a website analytics job 2m 50s (locked) executing the real-time pipeline 2m 18s 5. data engineering with spark: best practices 5. data engineering with spark: best practices (locked) batch vs. real-time options 2m 18s (locked) scaling extraction and loading operations 2m 18s (locked) scaling processing operations 1m 1s (locked) building resiliency 1m 19s 6. end-to-end exercise project 6. end-to-end exercise project (locked) project exercise requirements 1m 56s (locked) solution design 56s (locked) extracting long last actions 1m 40s (locked) building a scorecard 1m 40s conclusion conclusion (locked) more about apache spark 43s what’s included practice while you learn 1 exercise file learn on the go access on tablet and phone similar courses 1h 13m hands-on data science using sql, tableau, python, and spark download courses use your ios or android linkedin learning app, and watch courses on your mobile device without an internet connection. * price may change based on profile and billing country information entered during sign in or registration explore business topics artificial intelligence for business business analysis and strategy business software and tools career development customer service diversity, equity, and inclusion (dei) finance and accounting human resources leadership and management marketing professional development project management sales small business and entrepreneurship training and education see all explore creative topics aec animation and illustration audio and music graphic design motion graphics and vfx photography product and manufacturing user experience video visualization and real-time web design see all explore technology topics artificial intelligence (ai) cloud computing cybersecurity data science database management devops hardware it help desk mobile development network and system administration software development web development see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/installing-big-data-technologies-windows-hadi-fadlallah,,,"installing big data technologies on windows agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in big data (reference: pngegg.com) installing big data technologies on windows report this article hadi fadlallah hadi fadlallah data professional | assistant professor published jan 9, 2021 + follow even if big data technologies may not be suitable for microsoft windows, they are sometimes needed for testing or educational purposes. last year i published a series of articles on towardsdatascience , providing step-by-step guides to install some of the hadoop ecosystem technologies on the microsoft windows 10 operating system. installing hadoop 3.2.1 single node cluster on windows 10 apache hadoop software library is a framework that allows the distributed processing of large data sets across clusters of computers using simple programming models. installing apache hive 3.1.2 on windows 10 apache hive is an open-source data warehousing software developed by facebook and built on top of hadoop. it allows querying data using a sql-like language called hiveql or using apache spark sql. it can store data within a separate repository and allows the building of external tables on top of data stored outside hive repositories. installing apache pig 0.17.0 on windows 10 apache pig is an open-source framework developed by yahoo used to write and execute hadoop mapreduce jobs. it is designed to facilitate writing mapreduce programs with a high-level language called piglatin instead of using complicated java code. it also can be extended with user-defined functions. apache pig converts the piglatin scripts into mapreduce using a wrapper layer in an optimized way, which decreases the need to optimize scripts manually to improve their efficiency. like comment copy linkedin facebook twitter share 13 1 comment aveek das 💻 sr. data engineer | 🎓 msc data science and analytics | scala | python | aws | 📝 technical author | 💛 open source | aws community builder 4y report this comment this was very informative, hadi . thank you! like reply 2 reactions 3 reactions to view or add a comment, sign in more articles by hadi fadlallah data integration between nosql and sql server aug 5, 2023 data integration between nosql and sql server in the ever-evolving landscape of #datamanagement, the fusion of #nosql technologies with traditional… 13 learn biml feb 19, 2023 learn biml #biml is the abbreviation for business intelligence markup language, an xml dialect used to create and configure… 14 getting started with sphinx search feb 14, 2023 getting started with sphinx search #sphinx (sql phrase index) is a standalone full-text #searchengine developed in 2001. it provides efficient search… 11 ssis features face-to-face feb 13, 2023 ssis features face-to-face #sqlserver integration services provides a wide variety of features that helps developers to build a robust extract… 10 write for tech blog jan 29, 2023 write for tech blog tech blog is a non-profit medium publication created by munchy bytes that tends to publish technical articles, ideas… 15 ssis bad habits to kick dec 17, 2022 ssis bad habits to kick #sqlserver integration services (#ssis) is one of the most popular and widely used #etl and data integration tools… 17 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/big-data-cluster-environment-powered-raspberry-pi-4-hadoop-mishra-,,,"big data – cluster environment: powered by raspberry pi-4, hadoop, and spark agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in big data – cluster environment: powered by raspberry pi-4, hadoop, and spark report this article brijesh kumar mishra brijesh kumar mishra iot | digital transformation | pmp | pmi-acp | pmi-rmp | cobit-5 | itil expert | ccskv4 certified | microsoft certified: azure solution architect, devops expert, administrator & iot specialty published aug 15, 2021 + follow introduction: this write-up contains detailed instructions to create a low-cost, high-performance big data cluster using raspberry pi – 4. the buzzword in the industry nowadays is “big data” and got inspired by the knowledge-sharing sessions of prof. saurabh in our epdt program, i decided to initiate this project. this project aims to get familiar with hadoop, spark, and cluster computing, without a big investment of time or money. this project will use raspberry pi-4 to build a networked cluster of 3 nodes, communicating with each other through a network switch, install hdfs, and have spark running in distributed processing jobs via yarn across the whole cluster. this write-up will be the full-featured introduction to the hardware and software involved in setting up the hadoop & spark cluster and can scale to any number or size of machines. we will cover: individual raspberry pi-4 setup with ubuntu server lts 20.04 installation physical cluster setup cluster setup - public key ssh authentication, static ip, host/hostnames configuration hadoop installation - single node and multi-node; hadoop 3.2.1 spark installation - spark jobs via yarn and the spark shell; spark 3.0.1 hardware components: qty	item				configuration


 3	raspberry pi – 4 	    :8gb ram
 4	cat 6 lan cable		    :1 foot each
 3	power adaptor               
         or 
    multiport usb adaptor    :4 usb port – 5.2v min. 2.5a 

 3	sd cards				:class 10, high speed
 1	4 port ethernet switch	:4 port gigabit switch hub cluster configuration: cpu each node: quad-core cortex-a72 (arm v8) 64-bit soc @1.5ghz
         ram : 8 x 3 = 24 gb lpddr4
      storage: 64 x 3 = 192 gb source: a data science/big data laboratory by pier taranti build raspberry pi hadoop/spark cluster from scratch by henry liang how to install and set up a 3-node hadoop cluster by linode (contributions from florent houbart) install, configure, and run spark on top of a hadoop yarn cluster apache hadoop documentation apache spark documentation physical cluster setup: 1. cost-effective server rack to start the setup, install your raspberry pi on the stack of spacers. you can find these spacers on any e-commerce site. 2. network connection i used a wired ethernet connection from my router connected via a 4-port tp-link switch for my setup. further, each rpi-4 is connected to the switch via a 1-foot cat-6 lan cable. 3. power supply there are several options to configure the power supply – use a multi-port high power usb power adaptor, capable of 5.2v 2.5a to 3a on each port. official raspberry pi foundation’s power supply to power the nodes individually is my preferred way as rpi-4 is power-sensitive when it comes to high performance. 4. individual raspberry pi setup 4.1.        ubuntu server lts 20.04 installation use the raspberry pi imager to write the ubuntu server lts 20.04 64 bit to each pi. 4.1.  pi configuration we will do an ssh into each rpi and setup some basic configurations. if you are finding it difficult to get the ip addresses of each pi, try the following method. 1. install the fing mobile application and connect to your wifi network, this will show the list of connected devices with the name ubuntu and their ip address. 2. log in to your router and check the connected devices list, with the name ubuntu. 3. please download and install an ssh client at your convenience, i prefer putty for its simplicity and versatility in usages. note: plug-in one pi at a time; finish the setup configuration before moving to the next pi. default - user name: ubuntu

 password: ubuntu once you're connected, you'll be prompted to change the default password. make sure the same updated password on each rpi, should be something secure and easy to recall. ensure that the pi has the time-synchronized using the following command: timedatectl status – this should return last login: tue aug 3 23:55:14 2021 from 192.168.0.xxx last login: tue aug  3 23:55:14 2021 from 192.168.0.xxx
ubuntu@pi01:~$ timedatectl status
               local time: fri 2021-08-06 23:26:52 ist
           universal time: fri 2021-08-06 17:56:52 utc
                 rtc time: n/a
                time zone: asia/kolkata (ist, +0530)
system clock synchronized: yes
              ntp service: active
          rtc in local tz: no if the system clock is synchronized and the ntp service is active, you're good to go. else, use the following commands to configure. timedatectl list-timezones – will show the list of timezones available. sudo timedatectl set-timezone asia/kolkata - if you are in india, otherwise select the timezone accordingly. to update the latest configuration of ubuntu, run the following commands to finish the individual configuration: sudo apt update
sudo apt upgrade
sudo reboot if you get a cache lock error after the update command, reboot the pi try again by - ctrl+c sudo reboot cluster setup if you can connect to all your rpi’s with the new password, we are good to proceed with the cluster setup. here we will setup static ips, hosts/hostname, and public-key ssh authentication to facilitate each pi to communicate without a password using encrypted keys. 1. static ip setup the following steps will need to be done on each pi. ubuntu server lts 20.04 requires netplan for network configuration. specifically, editing a few yaml files. ssh into the pi and find the name of your network interface by running: ip a the output information should look like this: ubuntu@pi01:~$ ip a

1: lo: <loopback,up,lower_up> mtu 65536 qdisc noqueue state unknown group default qlen 1000

    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever

2: eth0: <broadcast,multicast,up,lower_up> mtu 1500 qdisc mq state up group default qlen 1000
    link/ether dc:a6:32:f8:a5:ce brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.xx/24 brd 192.168.0.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::dea6:32ff:fef8:a5ce/64 scope link
       valid_lft forever preferred_lft forever

3: wlan0: <broadcast,multicast> mtu 1500 qdisc noop state down group default qlen 1000

    link/ether dc:a6:32:f8:a5:cf brd ff:ff:ff:ff:ff:ff the network interface name is eth0 tag. keep this in mind or create a notepad to tabulate these details and save them for future reference. next, will nano to edit the configuration files. use the following commands to edit the configuration file to disable automatic network configuration. sudo nano /etc/cloud/cloud.cfg.d/99-disable-config.cfg all you need to add in the newly created file: network: {config: disabled} now we will, setup the static ip by editing the 50-cloud-init.yaml file. use the following command: sudo nano /etc/netplan/50-cloud-init.yaml the basic template to set a static ip is: network:
    ethernets:
        {network interface name}:
            dhcp4: false
            addresses: [{specifc ip adress}/24]
            gateway4: {gateway address}
            nameservers:
                addresses: [{gateway address}, 8.8.8.8]
    version: 2 my configuration file looked like so: (x being the last digit of the specific ip address for each pi; 192.168.0.116, 192.168.0.115, 192.168.0.114, etc.) gnu nano 4.8            /etc/netplan/50-cloud-init.yaml

# this file is generated from information provided by the datasource.  changes
# to it will not persist across an instance reboot.  to disable cloud-init's
# network configuration capabilities, write a file
# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:
# network: {config: disabled}
network:
    ethernets:
        eth0:
            dhcp4: false
            addresses: [192.168.0.xx/24]
            gateway4: 192.168.0.1
            nameservers:
                addresses: [192.168.0.xx, 8.8.8.8]
    version: 2 after editing the file, apply the settings by using the following commands: sudo netplan apply this command will make your ssh session hang, create a new terminal session, and ssh into the pi using the new ip address as per the above configuration. then reboot the pi and confirm the static ip address is set correctly by using : ip a 2. hosts/hostname configuration the following steps will need to be done on each pi. to get all the clusters to talk to each other flawlessly, we need to configure the hosts and hostnames files to the specific rpi information. first, we'll ssh into the rpi and update the hostname file by using: sudo nano /etc/hostname the hostname file should like so (x being the last digit of the specific ip address for each pi): pi0x for example, the hostname file for pi01 will look like this: pi01 next, we'll have to update the hosts' file using the following command: sudo nano /etc/hosts the host's file should look like so after editing: # the following lines are desirable for ipv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts
 
192.168.0.xxx pi01
192.168.0.xxx pi02
192.168.0.xxx pi03 make sure to delete the localhost 127.0.0.1 line from the file. addressing is simple, {ip address} {hostname} please note, the hostname file will be different for each rpi, but the hosts' file should be the same . once all are configured, reboot the rpis and move on to ssh key authentication. 3. ssh key authentication configuration perform these steps on the master pi until only until directed to do otherwise. first, edit the ssh config file on the master rpi using the following command: nano ~/.ssh/config add all nodes to the config file, including the host, user, and hostname for each pi. the template for adding nodes to the config file is: host pixx
user ubuntu

hostname {ip address} my config file looked like this after adding all of my nodes to the file: host pi01
user ubuntu
hostname 192.168.0.xxx
 
host pi02
user ubuntu
hostname 192.168.0.xxx
 
host pi03
user ubuntu
hostname 192.168.0.xxx
now, create an ssh key pair on the pi using: now, create an ssh key pair on the pi using: ssh-keygen -t rsa -b 4096 press enter through all the prompts because the key pair needs to be saved in the .ssh directory, and the key pair should be passwordless. the output should look similar to this: your identification has been saved in /your_home/.ssh/id_rsa
your public key has been saved in /your_home/.ssh/id_rsa.pub
the key fingerprint is:
sha256:/hk7mj5n5aiqdftvuzr+2qt+qcis7bim5iv0dxrc3ks user@host
the key's randomart image is:
+---[rsa 4096]----+
|                .|
|               + |
|              +  |
| .           o . |
|o       s   . o  |
| + o. .oo. ..  .o|
|o = oooooeo+ ...o|
|.. o *o+=.*+o....|
|    =+=oob=o.... |
+----[sha256]-----+ repeat the ssh keygen on your nodes rpi 2 and rpi 3. then use the following command on all pis (including master rpi 1) to copy the public keys into pi 1's authorized key list: ssh-copy-id pi01 finally, you can copy pi 1's configuration files to the rest of the pi's using the following commands: note, below commands needs to run only on master rpi01 and xx stands for the specific digit identifier for the rpi (rpi02, rpi03). scp ~/.ssh/authorized_keys pixx:~/.ssh/authorized_keys

scp ~/.ssh/config pixx:~/.ssh/config now you should be able to ssh into any node from any rpis without providing a password. if not, please refer to the configuration setup again and resolve it before you proceed further. 4. cluster setup using bashrc this step starts with editing the .bashrc file to create some custom functions for ease of use. on the master pi, we'll first edit the ~/.bashrc file: nano ~/.bashrc within this file, add the following code to the bottom of the file: # hadoop cluster management functions
 
#   list what other nodes are in the cluster
function cluster-other-nodes {
    grep ""pi"" /etc/hosts | awk '{print $2}' | grep -v $(hostname)
}
 
#   execute a command on all nodes in the cluster
function cluster-cmd {
    for node in $(cluster-other-nodes);
    do
        echo $node;
        ssh $node ""$@"";
    done
    cat /etc/hostname; $@
}
 
#   reboot all nodes in the cluster
function cluster-reboot {
    cluster-cmd sudo reboot now
}
 
#   shutdown all nodes in the cluster
function cluster-shutdown {
    cluster-cmd sudo shutdown now
}
 
function cluster-scp {
    for node in $(cluster-other-nodes);
    do
        echo ""${node} copying..."";
        cat $1 | ssh $node ""sudo tee $1"" > /dev/null 2>&1;
    done
    echo 'all files copied successfully'
}
 
#   start yarn and dfs on cluster
function cluster-start-hadoop {
    start-dfs.sh; start-yarn.sh
}
 
#   stop yarn and dfs on cluster
function cluster-stop-hadoop {
    stop-dfs.sh; stop-yarn.sh

} source the .bashrc file on the master pi: source ~/.bashrc now use the following command to copy the .bashrc to all the worker nodes: cluster-scp ~/.bashrc lastly, run the following command to source the .bashrc file on all nodes: cluster-cmd source ~/.bashrc you now have a functioning cluster computer. to start running parallel processing tasks, we'll install hadoop first and then spark. hadoop 3.2.2 installation 1. install java 8 to install java 8 on each node, use the following command: cluster-cmd sudo apt install openjdk-8-jdk then, because we haven't done it in a while, use the following command to reboot the pis: cluster-reboot after everything is rebooted, ssh into the master pi and run the following command to verify java was installed correctly: cluster-cmd java -version 2. hadoop single node installation steps only for the master pi (rpi 01) until directed to do otherwise. next, use wget to download it onto the master rpi. wget https://downloads.apache.org/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz next, extract the tar file and move the binary to the /opt directory by using the following command: sudo tar -xvf hadoop-3.2.2.tar.gz -c /opt/ && cd /opt change the name of the directory from hadoop-3.2.2 to hadoop: sudo mv hadoop-3.2.2 hadoop change the permissions on the directory. sudo chown ubuntu:ubuntu -r /opt/hadoop setup .profile, .bashrc, and hadoop-env.sh environment variables edit .profile to add hadoop binaries to path: nano /opt/hadoop/.profile add the following line: path=/opt/hadoop/hadoop/bin:/opt/hadoop/hadoop/sbin:$path edit the .bashrc file by nano ~/.bashrc append the following environmental variables at the bottom of the file. # path and options for java
export java_home=/usr/lib/jvm/java-8-openjdk-arm64
 
# path and options for hadoop
export hadoop_home=/opt/hadoop
export path=$path:$hadoop_home/bin:$hadoop_home/sbin
export hadoop_install=$hadoop_home
export hadoop_mapred_home=$hadoop_home
export hadoop_common_home=$hadoop_home
export hadoop_hdfs_home=$hadoop_home
export hadoop_install=$hadoop_home
export yarn_home=$hadoop_home
export hadoop_common_lib_native_dir=$hadoop_home/lib/native
export hadoop_common_lib_native_dir=$hadoop_home/lib/native
export hadoop_conf_dir=$hadoop_home/etc/hadoop
export hadoop_opts=""-djava.library.path=$hadoop_home/lib/native"" then source the .bashrc file to ensure it updates. source ~/.bashrc next, set the value of java_home in /opt/hadoop/etc/hadoop/hadoop-env.sh. you'll have to scroll down to find the correct line. the line will be commented out. uncomment the line and add the correct path to the variable. it should look like this: ...
# the java implementation to use. by default, this environment
# variable is required on all platforms except os x!
export java_home=/usr/lib/jvm/java-8-openjdk-arm64
... setup the core-site.xml and hdfs-site.xml use the following command to edit the core-site.xml file. nano /opt/hadoop/etc/hadoop/core-site.xml it should look like so after editing: <configuration>
    <property>
        <name>fs.defaultfs</name>
        <value>hdfs://pi01:9000</value>
    </property>
</configuration> then edit the hdfs-site.xml file: nano /opt/hadoop/etc/hadoop/hdfs-site.xml after editing: <configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration> test mapreduce format the namenode caution; all data will be deleted! : hdfs namenode –format start the namenode and datanode: start-dfs.sh make the required directories to run mapreduce jobs: hdfs dfs -mkdir /user
hdfs dfs -mkdir /user/ubuntu copy input files into the distributed filesystem: hdfs dfs -mkdir input
hdfs dfs -put /opt/hadoop/etc/hadoop/*.xml input run the test example: hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output 'dfs[a-z.]+' view the output on the distributed file system: hdfs dfs -cat output/* when done, stop the namenode and datanode: stop-dfs.sh test yarn configure the following parameters in the configuration files: /opt/hadoop/etc/hadoop/mapred-site.xml: <configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.application.classpath</name>
        <value>$hadoop_mapred_home/share/hadoop/mapreduce/*:$hadoop_mapred_home/share/hadoop/mapreduce/lib/*</value>
    </property>
</configuration> /opt/hadoop/etc/hadoop/yarn-site.xml: <configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>        <value>java_home,hadoop_common_home,hadoop_hdfs_home,hadoop_conf_dir,classpath_prepend_distcache,hadoop_yarn_home,hadoop_mapred_home</value>
    </property>
</configuration> start resourcemanager and nodemanager: start-yarn.sh start namenode and datanode: start-dfs.sh test if all daemons are running: jps you should see this output after running the jps command: 5616 secondarynamenode
5760 jps
5233 namenode
4674 nodemanager
5387 datanode
4524 resourcemanager if you see all six daemons, then it's time to move on to the multi-node installation of hadoop. stop all daemons: stop-yarn.sh
stop-dfs.sh 3. hadoop multi-node installation edit site configuration files update the following configuration files: nano /opt/hadoop/etc/hadoop/core-site.xml: nano /opt/hadoop/etc/hadoop/core-site.xml: nano /opt/hadoop/etc/hadoop/core-site.xml:
    <configuration>
        <property>
            <name>fs.default.name</name>
            <value>hdfs://pi01:9000</value>
        </property>
    </configuration> nano /opt/hadoop/etc/hadoop/hdfs-site.xml: nano /opt/hadoop/etc/hadoop/hdfs-site.xml: <configuration>
    <property>
            <name>dfs.namenode.name.dir</name>
            <value>/opt/hadoop/data/namenode</value>
    </property>


    <property>
            <name>dfs.datanode.data.dir</name>
            <value>/opt/hadoop/data/datanode</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>/opt/hadoop/hdfs/namenodesecondary</value>
    </property>
    <property>
            <name>dfs.replication</name>
            <value>1</value>
    </property>
</configuration> nano /opt/hadoop/etc/hadoop/mapred-site.xml: nano /opt/hadoop/etc/hadoop/mapred-site.xml: <configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>hadoop_mapred_home=$hadoop_home</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>hadoop_mapred_home=$hadoop_home</value>
    </property>   
    <property>
        <name>mapreduce.reduce.env</name>
        <value>hadoop_mapred_home=$hadoop_home</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.resource.memory-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>mapreduce.map.resource.memory-mb</name>
        <value>256</value>
    </property>
    <property>
        <name>mapreduce.reduce.resource.memory-mb</name>
        <value>256</value>
    </property>
</configuration> nano /opt/hadoop/etc/hadoop/yarn-site.xml: nano /opt/hadoop/etc/hadoop/yarn-site.xml: <configuration>
    <property>
        <name>yarn.acl.enable</name>
        <value>0</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>pi01</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1536</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>1536</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>128</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration> set the master and workers for the master: nano /opt/hadoop/etc/hadoop/master it should look like this: pi01 for the workers: nano /opt/hadoop/etc/hadoop/workers it should look like this: pi02
pi03 copy hadoop configuration to the rest of nodes ssh into pi 2 and then pi 3 and run the following commands: sudo mkdir /opt/hadoop
 
sudo chown ubuntu:ubuntu -r /opt/hadoop after creating the /opt/hadoop directory in rpi 2 and rpi 3 and changing the directory owner, ssh back into rpi 1. use the following command to copy all files from the master node to the worker nodes: for pi in $(cluster-other-nodes); do rsync -avxp /opt/hadoop $pi:/opt; done then copy the .bashrc file from the master node to the worker nodes and source it on all nodes: cluster-scp ~/.bashrc
 
cluster-cmd source ~/.bashrc after everything is copied, you can verify that hadoop was installed correctly using the following command on each node: hadoop version | grep hadoop format the hdfs run the following commands only on the master node unless directed otherwise. to run for the first time, the hdfs needs to be formatted: hdfs namenode -format run hdfs and yarn to start the hdfs, run the following command on the master node: start-dfs.sh to start yarn, run the following command: start-yarn.sh ensure that everything is working by running the following command on all nodes: jps on the master node, jps should return a similar output: 7202 secondarynamenode
6954 namenode
7498 jps
7389 resourcemanager on the worker nodes, jps should return a similar output: 3889 jps
3684 nodemanager
3514 datanode get/put data to the hdfs first, create the user home directory in the hdfs: hdfs dfs -mkdir -p /user/ubuntu create a directory called books in the hdfs: hdfs dfs -mkdir books grab a few books from the gutenberg project: cd ~
wget -o alice.txt https://www.gutenberg.org/files/11/11-0.txt
wget -o holmes.txt https://www.gutenberg.org/files/1661/1661-0.txt
wget -o irondoor.txt https://www.gutenberg.org/files/65995/65995-0.txt put the books on the hdfs in the books directory: hdfs dfs -put alice.txt holmes.txt frankenstein.txt books list the contents of the books directory: hdfs dfs -ls books move one of the books back into the local filesystem: hdfs dfs -get books/holmes.txt list the current directory to ensure that you successfully retrieved the file: ls mapreduce job - wordcount via yarn let's test with ""hello, world!"" to mapreduce; the wordcount. first, submit a job with the sample wordcount jar to yarn: yarn jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount ""books/*"" output after the job is finished (it could take a few minutes), get the results with the following command: hdfs dfs -ls output this will return the following information: found 2 items
-rw-r--r--   1 ubuntu supergroup          0 2020-09-10 18:09 output/_success
-rw-r--r--   1 ubuntu supergroup     272490 2020-09-10 18:09 output/part-r-00000 then, print the result with the following command (use ctrl-z to exit): hdfs dfs -cat output/part-r-00000 | less congratulations, you now have a working yarn cluster! spark installation 1. download apache spark only needs to be performed on the master pi. first, download the spark 3.0.3 (pre-built for hadoop 3.2 and later) binary onto the master pi. wget https://downloads.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz next, extract the tar and move the binary to the /opt/hadoop/ directory using the following command: sudo tar -xvf spark-3.0.3-bin-hadoop3.2.tgz -c /opt/hadoop && cd /opt/hadoop change the name of the directory in /opt/hadoop/ to spark: sudo mv spark-3.0.3-bin-hadoop3.2 spark change the permissions on the directory. sudo chown ubuntu:ubuntu -r /opt/hadoop/spark 2. configure yarn for spark integration add the following code to the end of the .bashrc file: # path and options for spark
export spark_home=/opt/hadoop/spark
export path=$path:$spark_home/bin
export ld_library_path=/opt/hadoop/lib/native:$ld_library_path source the .bashrc file: source ~/.bashrc add the spark directory to the path by editing the hadoop user profile: nano /opt/hadoop/.profile add the following lines: path=/opt/hadoop/spark/bin:$path
export hadoop_conf_dir=/opt/hadoop/etc/hadoop
export spark_home=/opt/hadoop/spark
export ld_library_path=/opt/hadoop/lib/native:$ld_library_path exit the terminal session and ssh back again to restart the session. rename the spark default template config file: mv /opt/hadoop/spark/conf/spark-defaults.conf.template /opt/hadoop/spark/conf/spark-defaults.conf edit $spark_home/conf/spark-defaults.conf and set spark.master to yarn: sudo nano /opt/hadoop/spark/conf/spark-defaults.conf set the following configurations: spark.master yarn
spark.driver.memory 512m
spark.yarn.am.memory 512m
spark.executor.memory 512m submit a spark application to the yarn cluster run the following command using the spark-submit command to test the parallel processing calculation of pi : spark-submit --deploy-mode client --class org.apache.spark.examples.sparkpi /opt/hadoop/spark/examples/jars/spark-examples_2.12-3.0.3.jar 10 you should see the following line in the output after submitting the job: ...
pi is roughly 3.1401351401351403
... monitor your spark applications enable the web ui of spark to track your submitted jobs by editing the spark-defaults.conf file: sudo nano /opt/hadoop/spark/conf/spark-defaults.conf add the following lines to the configuration file: spark.eventlog.enabled            true
spark.eventlog.dir                hdfs://pi01:9000/spark-logs
spark.history.provider            org.apache.spark.deploy.history.fshistoryprovider
spark.history.fs.logdirectory     hdfs://pi01:9000/spark-logs
spark.history.fs.update.interval  10s
spark.history.ui.port             18080 create the log directory in the hdfs: hdfs dfs -mkdir /spark-logs run the history server: /opt/hadoop/spark/sbin/start-history-server.sh rerun the pi parallel processing job to generate logs in the hdfs. spark-submit --deploy-mode client --class org.apache.spark.examples.sparkpi /opt/hadoop/spark/examples/jars/spark-examples_2.12-3.0.3.jar 10 access the spark history server through the web ui at http://{master node ip}:18080 in a web browser. using the spark shell let's take a look at some of the existing data on our cluster interactively; via the spark shell: to start the spark shell: spark-shell then let's do a small scala example to show the power of the spark shell. first, let's create a string variable named input from the alice.txt file. var input = spark.read.textfile(""books/alice.txt"") then, let's count the number of non-blank lines using the filter function: input.filter(line => line.length()>0).count() the returned output should be so: res0: long = 2813 to exit the spark shell, use. :q congratulations, now you have a working cluster computing environment at home :). like comment copy linkedin facebook twitter share 34 4 comments gabriel alejandro florit polanco ingeniero en informática // software engineer // data engineer 1y report this comment excellent, just what i was looking for. thanks like reply 1 reaction 2 reactions mohan shenoy an aspiring solution architect | passionate about bringing software engineering solutions to business problems 3y report this comment very nice article brijesh, thanks for sharing. 
i am planning to build this setup for my learning purposes. 
may i know, how much total it costed in indian currency inr.  and let me know what are the issues you faced like reply 1 reaction 2 reactions saurabh kumar associate professor at iim-indore 3y report this comment nicely articulated brijesh! keep up the good work. like reply 2 reactions 3 reactions see more comments to view or add a comment, sign in more articles by brijesh kumar mishra iot portfolio, impact on telecoms jul 21, 2021 iot portfolio, impact on telecoms telecom services are largely commoditized now, yet they constitute the core of the telecommunications provider’s… 43 2 comments rapid poc, with vi iot sim card mar 14, 2021 rapid poc, with vi iot sim card this article is a high-level overview of using industrial grade vi™ iot sim card to execute… 25 3 comments iot (iiot) product/project management in industry 4.0 jan 31, 2021 iot (iiot) product/project management in industry 4.0 in industry 4.0, managing the disruptive innovation is the key of successful product/project management and especially… 21 iot projects: cost estimation technique that worked for me jan 2, 2021 iot projects: cost estimation technique that worked for me the most critical phase of iot project planning is - accurate project budget estimation, of its complete solution… 8 life @ startup... jun 2, 2020 life @ startup... generational trends reveal the different priorities in job satisfaction and employee’s belief system, working in… 5 qubes os, brings secure xen-hypervisor to desktop computing may 26, 2020 qubes os, brings secure xen-hypervisor to desktop computing in jan, 2018, during a key brainstorming session with the team for a secure iiot enabled kiosk system for hospitals, we… 13 1 comment ansible on the raspberry pi-4 may 19, 2020 ansible on the raspberry pi-4 we have prepared a lab environment for our new trainees to get their hands dirty on configuration management on cloud… 7 unio's smart case for raspberry pi-4 with fan and shutdown button for graceful shutdown may 18, 2020 unio's smart case for raspberry pi-4 with fan and shutdown button for graceful shutdown glad to share, unio’s csr initiative to realize the power of innovation, learning, iot and low cost problem solving… 11 2 comments design for failure : pre-mortem may 8, 2020 design for failure : pre-mortem a good leader, gives the team a logical reason to think and emotional support to act - relentlessly to achieve… 11 1 comment as per uk based globaldata, apac iot service industry revenue to hit $96 billion by 2023 dec 10, 2019 as per uk based globaldata, apac iot service industry revenue to hit $96 billion by 2023 uk based a data analytics and consulting company globaldata is forecasting that, iot products sales growth will be at… 3 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/apache-spark-hivewarehousesession-crud-hive-3-managed-weichberger,,,"apache spark :: hivewarehousesession (crud) with hive 3 managed tables agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in apache spark :: hivewarehousesession (crud) with hive 3 managed tables report this article laurent weichberger laurent weichberger principal customer engineer published dec 8, 2020 + follow at a ""fortune 500"" client, i had the opportunity to help them with an in-place upgrade from hortonworks data platform (hdp) 2.6.5 to cloudera data platform (cdp) 7.x. one of my roles was to ensure that the apache spark jobs would all work against hive 3 ""managed tables,"" since hive 3 without refactoring breaks the sparksession because of the new hive metastore. i had heard about this issue, but never tried to solve it, so this was my chance to get dirty in the spark trenches... i went about it using the scientific method of changing only what i needed to change to make it work. first i tried to read a hive 3 managed table with a vanilla scala org.apache.spark.sql.sparksession and naturally it failed, as expected. next i attempted to do what everyone said i must do, build a hivewarehousesession object to read the hive 3 managed tables. luckily this was well documented and there is a hortonworks implemented factory pattern [gof] of this api. for this all of this to work, you need to make sure the jar is available: for hortonworks data platform (hdp) it requires a hortonworks ""hive-warehouse-connector-assembly"" jar to be present: spark-shell --jars /usr/hdp/3.0.1.0-183/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.0.1.0-183.jar for cloudera data platform (cdp), same idea (remember the two companies merged): spark-shell --jars /opt/cloudera/parcels/cdh/lib/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.7.1.1.0-565.jar make sure there is a hive 3 managed table present to test this spark code. i wrote a hive table named ""spirit"" with two columns, ""born"" an int, and ""name"" a string, into which i wrote the names of spiritual masters, and the year they were born. assuming that the table is present and has some data, try this to make sure we can at least reach the table and read it. our first step on our full crud support from apache spark on hive 3 managed tables. let's read: // hortonworks api imports
import com.hortonworks.hwc.hivewarehousesession
import com.hortonworks.hwc.hivewarehousesession._ next we need to use that factory pattern to get a hivewarehousesession. i read the source code and this is just a decorator pattern [gof] around the old sparksession to enable hive support of hive 3 managed tables. it is not much code, but it works, so we are happy: // factory pattern with build()
val hive = hivewarehousesession.session(spark).build()

// simple select to make sure we can reach the hive 3 managed table
val df = hive.executequery(""select * from spirit"") i even tried the old sparksession trick of spark.table(""table_name"") but that doesn't work here, worth a try though: // val df2 = hive.table(""spirit"") :: this failed!

// these worked:
hive.showtables().show()
hive.showdatabases().show() we can ""describe"" the hive 3 managed table nicely like this: // hive ql describe command worked
val descriptiondf = hive.describetable(""spirit"") now for the real fun, i was told the hivewarehousesession would enable full crud capabilities, like update and delete, and create table. wow, i definitely wanted to see this in action. here we go, hive ql insert is available on the executeupdate(/* hql */): // testing hql insert for hive 3
hive.executeupdate(""insert into spirit values ('st. francis', 1226), ('hafiz',1315)"") this worked. now i wanted to test the update and delete hql functionality. for this we continue to use the executeupdate() function: // there is also st. francis xavier born 1506 let's not confuse them
hive.executeupdate(""update spirit set name='st. francis of assisi' where name='st. francis'"")

// and we can delete a row from hive 3 like this

hive.executeupdate(""delete from spirit where name='hafiz'"") but that's not all! i feel like i am a game show host here for a moment, singing the praises of this hivewarehousesession. next i wanted to see if i could really create a new hive 3 managed table from spark. so i tried it like this: // similar to ""spirit"" table
hive.createtable(""masters"").column(""born"",""int"").column(""name"",""string"").ifnotexists().create()

// then i added a new row into my new table, immediately
hive.executeupdate(""insert into masters values (1838,'shirdi sai baba')"")

// then i prove that it works by showing the table and showing the contents
hive.showtables().show()
hive.executequery(""select * from masters"").show() this all worked! yay. next i showed all my work to cloudera before using this solution at the client site, because i wanted validation (even though it ran perfectly). i was surprised at the response from cloudera. i was told, ""yes, and you don't need to do that any longer. just give the right configuration settings and the sparksession will work fine. i was suspicious of this response. i asked for the configuration and tried it. i had mixed results, as follows: the configuration settings to give spark-shell, or spark-submit, are basically this: spark-shell 
--conf spark.sql.extensions=com.qubole.spark.hiveacid.hiveacidautoconvertextension --conf spark.kryo.registrator=""com.qubole.spark.hiveacid.util.hiveacidkyroregistrator"" 
--conf spark.sql.hive.hwc.execution.mode=spark 
--conf spark.sql.hive.hiveserver2.jdbc.url=""jdbc:hive2://<some_ip>:10000/"" 
--conf spark.sql.hive.hiveserver2.jdbc.url.principal=hive/<ip>@something.local 
--jars /opt/cloudera/parcels/cdh/lib/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.7.1.1.0-565.jar let's take this one at a time, as there is automagical behavior here, and we need to understand this: as we can see from the first two --conf settings, qubole has been busy. they created a hiveacidautoconvertextension to give the sparksession super powers. great. they also created a hiveacidkyroregistrator , but the dev team spelled ""kryo"" wrong in their classname, which added grey hairs to my head when implementing this solution. whew! beware of this when you start development. next we set the execution.mode=spark. this is weird because before this ""fix"" it was set to spark2. not sure what this is about, just do this. next we give hivewarehousesession the jdbc.url, and the jdbc.url.principal so that it can reach hive 3 managed tables. this is a long conversation, but the long and short of it is that there are two ""modes"" which the hivewarehousesession can use, and one is jdbc mode. so if it goes into jdbc mode, it needs these. i am not going to discuss the details of this here, it is outside the scope of this blog. lastly, none of this works without the required jar (which we saw previously) so we need that still, so the dependencies can be reached. one last point is that i tested this many ways, and had some success and some failure, as follows: when using spark-submit, this works for read only sparksession work against hive 3 managed tables. when using spark-shell, this works for read only sparksession work against hive 3 managed tables. when using apache oozie spark-action, this fails no matter how many ways i tried to configure the spark-action. i brought this to the attention of cloudera support (as of november 2020) and they admitted that this oozie spark-action doesn't support hivewarehousesession. lastly i tried to use the sparksession, after the above fancy --conf work, to perform a spark.sql(/* hql */) and fool the spark.sql into thinking it could run my hql: insert, update, and delete, and none of that worked. at the end of the day, i learned a lot about apache spark running against hive 3 managed tables. i hope this helps you as well. if you need help, please let me know, i am available to help you. may all beings be happy, and may all beings be free as we enter the holidays. om. laurent weichbeger big data bear ompoint (at) gmail (dot) com like comment copy linkedin facebook twitter share 36 16 comments chawin chokchaimongkolkij -- 1y report this comment how to create temporary table with hivewarehousesession? like reply 1 reaction 2 reactions raashad bashir data engineer | cloudera | pyspark | python | informatica powercenter | greenplum | etl & automation | shell scripting 3y report this comment was struggling a bit with accessing hive acid tables from spark and then came across this post. great post. like reply 1 reaction 2 reactions lester martin 🥑 dev advocate, trainer, blogger, data engineer 4y report this comment it is your recommendation to stick with using the hivewarehousesession for now then? like reply 1 reaction gurdeep singh 4y report this comment syed waseemulla like reply 1 reaction 2 reactions see more comments to view or add a comment, sign in more articles by laurent weichberger collibra data quality capabilities for basel committee on banking supervision’s standard 239 (“bcbs 239”) by by  k. haslbeck & l. weichberger mar 29, 2025 collibra data quality capabilities for basel committee on banking supervision’s standard 239 (“bcbs 239”) by by  k. haslbeck & l. weichberger preamble during 2024 i met with one of our multinational banking clients at one of their offices in india, and we… 34 1 comment python code to grab two letter state codes jan 23, 2025 python code to grab two letter state codes i needed to have just a list of two letter state codes for a collibra data quality custom rule i was creating for a… 17 3 comments anatomy of a healthy data quality project team, part iii: dq information and sharing with project management, by l. weichberger & c. schmidt (2023). aug 10, 2023 anatomy of a healthy data quality project team, part iii: dq information and sharing with project management, by l. weichberger & c. schmidt (2023). in our previous blog post we shared about dq team culture. we will end this series with part 3: dq required information… 23 4 comments anatomy of a healthy data quality project team, part ii: dq team culture, by laurent weichberger (august 2023). aug 7, 2023 anatomy of a healthy data quality project team, part ii: dq team culture, by laurent weichberger (august 2023). in part one of this blog we discuss data quality roles and responsibilities. beyond roles and responsibilities, we have… 15 5 comments anatomy of a healthy data quality project team :: part i, dq roles and responsibilities aug 5, 2023 anatomy of a healthy data quality project team :: part i, dq roles and responsibilities one of our most successful dq customers has asked me to compose something related to the “anatomy of a successful dq… 51 7 comments dq outlier detection with interquartile range (iqr) in python apr 4, 2022 dq outlier detection with interquartile range (iqr) in python i recently created a presentation on getting started with data quality outlier detection for my company collibra… 34 10 comments data quality without software, towards a dq culture oct 16, 2021 data quality without software, towards a dq culture preamble: the purpose of this document is to discuss data quality in isolation from any software product which solved… 23 1 comment snowflake stored procedure with javascript mar 8, 2021 snowflake stored procedure with javascript i wrote my first snowflake stored procedure using the public citibikes data from citigroup inc. we need to use… 7 2 comments python snowflake connector implementation dec 21, 2020 python snowflake connector implementation last week i added data to my brand new hashmap snowflake database, and this morning i was able to rapidly query that… 19 4 comments write some nifi flows in under an hour jul 28, 2020 write some nifi flows in under an hour i had to write some nifi ""flows"" for a hortonworks dataflow test. my recent work has been around the hdp 2. 6 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/creating-random-test-data-spark-python-code-mich-talebzadeh-ph-d-,,,"creating random test data in spark using pyspark agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in creating random test data in spark using pyspark report this article mich talebzadeh (ph.d.) mich talebzadeh (ph.d.) published dec 26, 2020 + follow background in one of my assignments, i was asked to provide a script to create random data in spark/pyspark for stress testing. years ago i developed such script for oracle and adapted it for sap ase. later on, i developed the same generator for scala. it is always good to use some numerical functions to create such random data. one can optionally store data in a database of choice such as hive. anyway this is the gist of it, hopefully some will find it useful. the tools used pycharm version 2020.3, community edition python interpreter 3.7, 64-bit spark-3.0.1-bin-hadoop2.7 package used by pycharm hive dw version 3.1.1 and hadoop 3.1.1 on rhes 7.6 n.b you need to create a virtual environment for each python project in pycharm initialisation of variables under project directory in pycharm, create a package called conf and within conf create a python file and call it variables.py. just put all your frequently accessed variable in that file # hive variables
db = ""pycharm""
tablename = ""randomdatapy""
fullyqualifiedtablename = db + '.' + tablename
tempview = ""tmp""
settings = [
      (""hive.exec.dynamic.partition"", ""true""),
      (""hive.exec.dynamic.partition.mode"", ""nonstrict""),
      (""spark.sql.orc.filterpushdown"", ""true""),
      (""hive.msck.path.validation"", ""ignore""),
      (""spark.sql.casesensitive"", ""true""),
      (""spark.speculation"", ""false""),
      (""hive.metastore.authorization.storage.checks"", ""false""),
      (""hive.metastore.client.connect.retry.delay"", ""5s""),
      (""hive.metastore.client.socket.timeout"", ""1800s""),
      (""hive.metastore.connect.retries"", ""12""),
      (""hive.metastore.execute.setugi"", ""false""),
      (""hive.metastore.failure.retries"", ""12""),
      (""hive.metastore.schema.verification"", ""false""),
      (""hive.metastore.schema.verification.record.version"", ""false""),
      (""hive.metastore.server.max.threads"", ""100000""),
      (""hive.metastore.authorization.storage.checks"", ""/apps/hive/warehouse""),
      (""hive.stats.autogather"", ""true"")
]
rowstogenerate = 10 assuming that you have your main python modules under src package, you can import your variables in variables.py as import conf.variables as v under src package, create a python file called usedfunctions.py and create your functions used for generating data there. import random
import string
import math

def randomstring(length):
    letters = string.ascii_letters
    result_str = ''.join(random.choice(letters) for i in range(length))
    return result_str

def clustered(x,numrows):
    return math.floor(x -1)/numrows

def scattered(x,numrows):
    return abs((x -1 % numrows))* 1.0

def randomised(seed,numrows):
    random.seed(seed)
    return abs(random.randint(0, numrows) % numrows) * 1.0

def padstring(x,chars,length):
    n = int(math.log10(x) + 1)
    result_str = ''.join(random.choice(chars) for i in range(length-n)) + str(x)
    return result_str

def padsinglechar(chars,length):
    result_str = ''.join(chars for i in range(length))
    return result_str

def println(lst):
    for ll in lst:
      print(ll[0]) you can of course add your own functions there if you need to expand it calling spark functions you need to create a folder that can be shared by multiple pycharm projects and put your spark stuff there. i call this folder packages (you create it as a standalone project called packages in pycharm). under packages folder, create a package called sparkutils, for spark specific stuff as shown below under sparkutils package create a python file called sparkstuff.py and put all spark contexts there: from pyspark.sql import sparksession
from pyspark import sparkcontext
from pyspark.sql import hivecontext


def spark_session(appname):
  return sparksession.builder \
        .appname(appname) \
        .enablehivesupport() \
        .getorcreate()

def sparkcontext():
  return sparkcontext.getorcreate()

def hivecontext():
  return hivecontext(sparkcontext())

def spark_session_local(appname):
    return sparksession.builder \
        .master('local[1]') \
        .appname(appname) \
        .enablehivesupport() \
        .getorcreate() how to load your packages folder in pycharm you can add your packages folder as content root as shown below you can then import the spark functions as follows: from sparkutils import sparkstuff as s putting it in all together first start by creating a python file under src package called randomdata.py start by importing what modules you need import usedfunctions as uf
import conf.variables as v
from sparkutils import sparkstuff as s the first line will import the usedfunctions as explained before, the second one will import all the needed variables and the third one all spark functions that are used in the python module. next start creating spark session and call the appname etc. it also shows when the job stared appname = ""app1""
spark = s.spark_session(appname)
spark.sparkcontext._conf.setall(v.settings)
sc = s.sparkcontext()
lst = (spark.sql(""select from_unixtime(unix_timestamp(), 'dd/mm/yyyy hh:mm:ss.ss') "")).collect()
print(""\nstarted at"");uf.println(lst) the next stage is to check if hive table already exists, otherwise create it. in my case hive database and table are called: # hive variables
db = ""pycharm""
tablename = ""randomdatapy""
fullyqualifiedtablename = db + '.' + tablename #
## check if table exist otherwise create it
rows = 0
sqltext  = """"
if (spark.sql(f""""""show tables in {v.db} like '{v.tablename}'"""""").count() == 1):
    spark.sql(f""""""analyze table {v.fullyqualifiedtablename} compute statistics"""""")
    rows = spark.sql(f""""""select count(1) from {v.fullyqualifiedtablename}"""""").collect()[0][0]
    print (""number of rows is "",rows)
else:
    print(f""\ntable {v.fullyqualifiedtablename} does not exist, creating table "")
    sqltext = f""""""
    create table {v.db}.{v.tablename}(
      id int
    , clustered int
    , scattered int
    , randomised int
    , random_string varchar(50)
    , small_vc varchar(50)
    , padding  varchar(4000)
    )
    stored as parquet
    """"""
    spark.sql(sqltext) so it creates a basic parquet table now check if table already exists. if so start with new rows added. note that maxid is a monotonically increasing number like a sequence. it is unique start = 0
if (rows == 0):
      start = 1
      maxid= 0
else:
      maxid = spark.sql(f""select max(id) from {v.fullyqualifiedtablename}"").collect()[0][0]
start = maxid + 1
end = start + v.rowstogenerate - 1
print (""starting at id = "",start, "",ending on = "",end)
range = range(start, end+1) that range is important. a spark rdd is produced as explained below by calling lambda function s in python ## this traverses through the range and increment ""x"" by one unit each time, and that x value is used in the code to generate random data through python lambda function

rdd = sc.parallelize(range). \
         map(lambda x: (x, uf.clustered(x,v.rowstogenerate), \
                           uf.scattered(x,v.rowstogenerate), \
                           uf.randomised(x, v.rowstogenerate), \
                           uf.randomstring(50), \
                           uf.padstring(x,"" "",50), \
                           uf.padsinglechar(""x"",4000))) next stage is to create a dataframe on the rdd generated df = rdd.todf(). \
         withcolumnrenamed(""_1"",""id""). \
         withcolumnrenamed(""_2"", ""clustered""). \
         withcolumnrenamed(""_3"", ""scattered""). \
         withcolumnrenamed(""_4"", ""randomised""). \
         withcolumnrenamed(""_5"", ""random_string""). \
         withcolumnrenamed(""_6"", ""small_vc""). \
         withcolumnrenamed(""_7"", ""padding"") then just see if all is created ok df.printschema() now create a temporary view in spark df.createorreplacetempview(f""""""{v.tempview}"""""") and insert data from the temporary view into the underlying hive table sqltext = f""""""
      insert into table {v.fullyqualifiedtablename}
      select
              id
            , clustered
            , scattered
            , randomised
            , random_string
            , small_vc
            , padding
      from tmp
""""""
spark.sql(sqltext) check that all data added ok spark.sql(f""""""select min(id) as minid, max(id) as maxid from {v.fullyqualifiedtablename}"""""").show(n=20,truncate=false,vertical=false) and finally timestamp it lst = (spark.sql(""select from_unixtime(unix_timestamp(), 'dd/mm/yyyy hh:mm:ss.ss') "")).collect()
print(""\nfinished at"");uf.println(lst) the sample run output is shown from pycharm console below c:\users\admin\pycharmprojects\pythonproject2\pilot\venv\scripts\python.exe c:/users/admin/pycharmprojects/pythonproject2/pilot/src/randomdata.py
setting default log level to ""warn"".
to adjust logging level use sc.setloglevel(newlevel). for sparkr, use setloglevel(newlevel).
                                                                                
started at
26/12/2020 13:37:46.46
number of rows is  1200
starting at id =  1201 ,ending on =  1210
root
 |-- id: long (nullable = true)
 |-- clustered: double (nullable = true)
 |-- scattered: double (nullable = true)
 |-- randomised: double (nullable = true)
 |-- random_string: string (nullable = true)
 |-- small_vc: string (nullable = true)
 |-- padding: string (nullable = true)


+-----+-----+
|minid|maxid|
+-----+-----+
|1    |1210 |
+-----+-----+




finished at
26/12/2020 13:38:33.33


process finished with exit code 0 full code from randondata.py the main code is shown below. also you can access all in the github import usedfunctions as uf
import conf.variables as v
from sparkutils import sparkstuff as s

appname = ""app1""
spark = s.spark_session(appname)
spark.sparkcontext._conf.setall(v.settings)
sc = s.sparkcontext()
lst = (spark.sql(""select from_unixtime(unix_timestamp(), 'dd/mm/yyyy hh:mm:ss.ss') "")).collect()
print(""\nstarted at"");uf.println(lst)

#
## check if table exist otherwise create it
rows = 0
sqltext  = """"
if (spark.sql(f""""""show tables in {v.db} like '{v.tablename}'"""""").count() == 1):
    spark.sql(f""""""analyze table {v.fullyqualifiedtablename} compute statistics"""""")
    rows = spark.sql(f""""""select count(1) from {v.fullyqualifiedtablename}"""""").collect()[0][0]
    print (""number of rows is "",rows)
else:
    print(f""\ntable {v.fullyqualifiedtablename} does not exist, creating table "")
    sqltext = f""""""
    create table {v.db}.{v.tablename}(
      id int
    , clustered int
    , scattered int
    , randomised int
    , random_string varchar(50)
    , small_vc varchar(50)
    , padding  varchar(4000)
    )
    stored as parquet
    """"""
    spark.sql(sqltext)

start = 0
if (rows == 0):
      start = 1
      maxid= 0
else:
      maxid = spark.sql(f""select max(id) from {v.fullyqualifiedtablename}"").collect()[0][0]
start = maxid + 1
end = start + v.rowstogenerate - 1
print (""starting at id = "",start, "",ending on = "",end)
range = range(start, end+1)
## this traverses through the range and increment ""x"" by one unit each time, and that x value is used in the code to generate random data through python lambda function

rdd = sc.parallelize(range). \
         map(lambda x: (x, uf.clustered(x,v.rowstogenerate), \
                           uf.scattered(x,v.rowstogenerate), \
                           uf.randomised(x, v.rowstogenerate), \
                           uf.randomstring(50), \
                           uf.padstring(x,"" "",50), \
                           uf.padsinglechar(""x"",4000)))
df = rdd.todf(). \
         withcolumnrenamed(""_1"",""id""). \
         withcolumnrenamed(""_2"", ""clustered""). \
         withcolumnrenamed(""_3"", ""scattered""). \
         withcolumnrenamed(""_4"", ""randomised""). \
         withcolumnrenamed(""_5"", ""random_string""). \
         withcolumnrenamed(""_6"", ""small_vc""). \
         withcolumnrenamed(""_7"", ""padding"")
df.printschema()
df.write.mode(""overwrite"").saveastable(f""""""{v.db}.abcd"""""")
df.createorreplacetempview(f""""""{v.tempview}"""""")
sqltext = f""""""
      insert into table {v.fullyqualifiedtablename}
      select
              id
            , clustered
            , scattered
            , randomised
            , random_string
            , small_vc
            , padding
      from tmp
""""""
spark.sql(sqltext)
spark.sql(f""""""select min(id) as minid, max(id) as maxid from {v.fullyqualifiedtablename}"""""").show(n=20,truncate=false,vertical=false)
lst = (spark.sql(""select from_unixtime(unix_timestamp(), 'dd/mm/yyyy hh:mm:ss.ss') "")).collect()
print(""\nfinished at"");uf.println(lst) conclusion in this article, i described how to generate some random data in spark using python code. you can of course do the same in scala. although the generated random columns are only seven, it will be a good starting point. you can of course add your preferences there. i have kept the main code brief without putting it in a class or main method. however, it does what it says on the tin. those interested can contact me through mich.talebzadeh@gmail.com. for those new to spark there is an ebook published by databricks that some especially data scientists will find it useful. it is called a gentle introduction to apache spark . happy reading! disclaimer: great care has been taken to make sure that the technical information presented in this article is accurate, but any and all responsibility for any loss, damage or destruction of data or any other property which may arise from relying on its content is explicitly disclaimed. the author will in no case be liable for any monetary damages arising from such loss, damage or destruction. like comment copy linkedin facebook twitter share 9 3 comments mich talebzadeh (ph.d.) 4y report this comment as stated in the article, for the benefit of rdbms dbas, i originally developed this script for oracle  and sybase 8 years ago. https://talebzadehmich.wordpress.com/category/databases/oracle-and-sybase /

the oracle code was like below:

execute dbms_random.seed (0)
drop table dummy2;
create table dummy2
tablespace app1
as
with generator as (
  select      rownum      id
  from    dual
  connect by
        rownum <= 1000
)
select
    rownum                     id
   , trunc((rownum - 1) / 1000)        clustered
   , mod(rownum - 1,1000)           scattered
   , trunc( dbms_random.value (0,1000))     randomised
   , substr( dbms_random.string ('a',15),1,15)  random_string
   , lpad(rownum,10)              rownumchar
   , rpad('x',10,'x')             xchar
   , trunc( dbms_random.value (0,5))       n1
from
  generator  g1,
  generator  g2
;
alter table dummy2 add constraint dummy2_pk primary key(id);
select count(1) from dummy2;
exit

also the code is available in sap ase (transact sql) and scala like reply 1 reaction 2 reactions mich talebzadeh (ph.d.) 4y report this comment someone mentioned why not use pandas (python) for this purpose.

why spark here. generally, parallel architecture comes into play when the data size is significantly large which cannot be handled on a single machine, hence, the use of spark becomes meaningful. in cases where (the generated) data size is going to be very large (which is often norm rather than the exception these days), the data cannot be processed and stored in pandas data frames as these data frames  store data in ram. then, the whole dataset from a storage like hdfs or cloud storage cannot be collected, because it will take significant time and space and probably won't fit in a single machine ram. like reply 1 reaction 2 reactions see more comments to view or add a comment, sign in more articles by mich talebzadeh (ph.d.) a first-hand account of fraud and document tampering apr 15, 2025 a first-hand account of fraud and document tampering what happens when a clumsy attempt is made to fabricate official records? it creates confusion, exposes deeper… 4 metadata analysis: indications of tampering with records feb 7, 2025 metadata analysis: indications of tampering with records what is metadata and why it matters in forensic analysis metadata is the hidden digital footprint embedded within files… 3 1 comment the rise of fake reviews: how to spot online deception with data science jan 31, 2025 the rise of fake reviews: how to spot online deception with data science introduction fake online reviews have become a widespread problem, deceiving consumers and manipulating brand… 2 the dark side of the self storage industry: why regulation is long overdue jan 29, 2025 the dark side of the self storage industry: why regulation is long overdue introduction the self-storage industry operates in a legal grey area, where companies market themselves as secure and… 3 beyond digital: the overlooked gdpr risks in handling physical data oct 30, 2024 beyond digital: the overlooked gdpr risks in handling physical data introduction: —understanding the full scope of gdpr when people think about the general data protection regulation… 4 2 comments recovering from critical data loss: my experience with oracle 12c, rman, and spark aug 21, 2024 recovering from critical data loss: my experience with oracle 12c, rman, and spark introduction data loss incidents are among the most challenging scenarios for it professionals. this article details my… 5 feature engineering for data engineers: building blocks for ml success aug 3, 2024 feature engineering for data engineers: building blocks for ml success introduction to features in machine learning features are akin to the individual columns in a dataset, representing… 7 2 comments review of the first 20 pages of ""the big book of generative ai from databricks"" jul 24, 2024 review of the first 20 pages of ""the big book of generative ai from databricks"" ""the big book of generative ai from databricks"" provides a comprehensive guide to understanding and implementing… 3 1 comment the role of artificial intelligence in combating financial fraud jul 21, 2024 the role of artificial intelligence in combating financial fraud the financial industry, often seen as traditional (like high street banks) and resistant to change, is undergoing a… 2 1 comment a comprehensive study in financial fraud jul 19, 2024 a comprehensive study in financial fraud financial crime refers to non-violent crime that involves the illegal acquisition, manipulation, or misuse of financial… 2 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/test-data-generation-using-spark-simple-json-columns-load-ghadiyaram,,,"test data generation using spark by using simple json data descriptor with columns and datatypes to load in dwh like hive. agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in test data generation using spark by using simple json data descriptor with columns and datatypes to load in dwh like hive. report this article ram ghadiyaram ram ghadiyaram vice president cloud big data analytics , ai/ml published apr 21, 2019 + follow often there are requirements to generate test data in formats like parquet avro csv json xml etc... for testing purpose. there are several ways to do it.... this can be achived using spark json(as sample schema descriptor) will try to demonstrate it and is 10 minutes read. sample json descriptor saved as file which has column name order, length datatype etc.... you can change this file according to your business needs. {
  ""columns"": [
    {
      ""name"": ""col1"",
      ""businesscolumnname"": ""id1"",
      ""orderofcolumntobeloadedintodatabase"": 0,
      ""desc"": ""columndescription"",
      ""type"": ""int"",
      ""length"": 2
    },
    {
      ""name"": ""label"",
      ""businesscolumnname"": ""label"",
      ""orderofcolumntobeloadedintodatabase"": 1,
      ""descriptivename"": ""label"",
      ""type"": ""string"",
      ""length"": 50
    },
    {
      ""name"": ""status_description"",
      ""businesscolumnname"": ""status"",
      ""orderofcolumntobeloadedintodatabase"": 2,
      ""type"": ""string"",
      ""length"": 50
    },
    {
      ""name"": ""orderofdisplay"",
      ""businesscolumnname"": ""orderofdisplay"",
      ""orderofcolumntobeloadedintodatabase"": 3,
      ""type"": ""int"",
      ""length"": 2
    },
    {
      ""name"": ""isactive"",
      ""businesscolumnname"": ""isactive"",
      ""orderofcolumntobeloadedintodatabase"": 4,
      ""type"": ""int"",
      ""length"": 1
    },
    {
      ""name"": ""tobedisplayed"",
      ""businesscolumnname"": ""todisplay"",
      ""orderofcolumntobeloadedintodatabase"": 5,
      ""type"": ""int"",
      ""length"": 1
    },
    {
      ""name"": ""notes"",
      ""businesscolumnname"": ""notes"",
      ""orderofcolumntobeloadedintodatabase"": 6,
      ""type"": ""string"",
      ""length"": 300
    },
    {
      ""name"": ""created_date"",
      ""businesscolumnname"": ""created_date"",
      ""orderofcolumntobeloadedintodatabase"": 7,
      ""descriptivename"": ""date of which it was created"",
      ""type"": ""long"",
      ""length"": 18
    }
  ]
} datasetgenerator/ dataframeutil - which is responsible for test data is as follows... test data formats which are generated from this program are parquet,avro,csv,json,xml input arguments for this program are : <positive percent> <num rows to be generated> <fileformat[s]> <number of files to be generated>. package com.examples


import java.io.{file, filereader}

import com.examples.dataframeutil._
 
import com.google.gson.jsonparser
import org.apache.log4j.{level, logger}
import org.apache.spark.rdd.rdd
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{integertype, stringtype, _}

import scala.util.{random, success, try}


/**
  *
  * @author : ram.ghadiyaram
  *
  */

object datasetgenerator {


  val intarray = (1 to 999)

  logger.getlogger(""org"").setlevel(level.warn)
  val longarray = (1l to 999l)
  val rangelist = intarray.mkstring("":"")
  val myrandomstringarray = array(""long live     "" + rangelist

    , ""long live testing 1 "" + rangelist
    , ""long live testing 2 "" + rangelist, null)
  private val logger = logger.getlogger(this.getclass.getname)

  /** *
    * <pre>
    * </pre>
    *
    * @param args
    *
    */

  def main(args: array[string]): unit = {

    val spark: sparksession = sparksession.builder().appname(this.getclass.getname).master(""local[*]"").getorcreate()
    require(args.length <= 4,
      s""""""
         | usage : <positive percent>  <num rows to be generated> <fileformat[s]> <number of files to be generated
         | current values are : ${args.foreach(println)}
   """""".stripmargin)
    val positiverecordpercent = if (args.length > 0) args(0).todouble else .75require(positiverecordpercent <= 1,
      s""""""
         |

    | if its 75% then 0.75 need to be passed remaining 25% i.e. 0.25 will be negetive cases.
         | current value passed : $positiverecordpercent

   """""".stripmargin)


    val numberofrowstobegenerated = if (args.length > 0) args(1).toint else 10
    val format = if (args.length > 0) args(2).tostring.touppercase else ""all""
    val numberofpartfilestobegenerated = if (args.length > 0) args(3).toint else 1

    val sc = spark.sparkcontext
    import com.google.gson.stream.jsonreader
    val reader = new jsonreader(new filereader(""c:\\codebase\\spark-general-examples\\src\\main\\resources\\testdataset.json""))
    val o = new jsonparser().parse(reader).getasjsonobject

    val myjsonasstring = o.getasjsonarray(""columns"").tostring
    import spark.implicits._
    logger.info(myjsonasstring)

    /** if we are using > =spark 2.2 **/

    val dataframefromjson = spark.read.json(seq(myjsonasstring).tods())

    /** if we are using < spark 2.2 *  val rdd = sc.parallelize(seq(b))  val df = spark.read.json(rdd) **/


    val dataframeofrequiredcolumns = dataframefromjson.selectexpr(""businesscolumnname"", ""orderofcolumntobeloadedintodatabase"", ""type"", ""length"")

    dataframeofrequiredcolumns.show

    dataframeofrequiredcolumns.printschema()


    val keyedby = dataframeofrequiredcolumns.rdd.keyby(_.getas[long](""orderofcolumntobeloadedintodatabase""))
    keyedby.foreach(x => logger.info(""using keyby-->>"" + x))
    val mysortedmapbycolorder = scala.collection.immutable.treemap(keyedby.collectasmap().toarray: _*)
    val list = scala.collection.mutable.listbuffer.empty[structfield]
    for ((key, value) <- mysortedmapbycolorder) {
      list += structfield(value.get(0).tostring,
        createstruct(value.get(2).tostring), false)
    }
    logger.debug(""list is "" + list.tolist)
    val schema = structtype(list.tolist)

    logger.info(""schema as treestring -----"" + schema.treestring)

    //val replicateddf1 = replicatedf(100, testdf)// logger.info(""-----"" + schema.treestring)// creating data from sample values
    val mytestdatalist = scala.collection.mutable.listbuffer.empty[any]
    //val schema1 = list1.tolistfor ((key1, value1) <- mysortedmapbycolorder) {
      logger.info(value1)
      val value = if (value1.get(2) == null) """" else value1
      logger.info(value)
      logger.info(""value1.getas[long](\""length\"")"" + value1.getas[long](""length""))
      val valueoflength: int = try(value1.getas[long](""length"")) match {
        case success(s) => s.tostring.toint
        case _ => logger.info(""error :value1.getas[long](\""length\"") is null""); 0

      }
      mytestdatalist += createtestdata(value.tostring, valueoflength)
    }

    var row = row.fromseq(mytestdatalist.toseq)
    row = row.fromseq(mytestdatalist.toseq)
    logger.info(""list is "" + mytestdatalist)
    val testdatadf: rdd[row] = spark.sparkcontext.parallelize(seq(row))
    logger.info(""printing each row "")
    testdatadf.foreach(logger.info)
    val testdf: dataframe = spark.createdataframe(testdatadf, schema)
    logger.info(""row is "" + row.getclass.getname + ""-----> "" + row)
    testdf.show
    testdf.printschema()
    logger.info("":::::::experiment to replicate data!!!"")
    val dfreplicated = replicateonerowmanytimes(numberofrowstobegenerated, testdf)
    //val df2 = arrayofreplicated(1).withcolumn("""","""");
    val ided = addcolumnindex(dfreplicated, ""index"")

    movecolumntofirstpos(ided, ""index"")
    val (training, test1) = splitdf(dfreplicated, positiverecordpercent, 1 - positiverecordpercent)
    val mycols = test1.schema.fields
    var test = test1;
    mycols.foreach(x => {
      test =
        x.datatype match {
          case floattype => test.withcolumn(x.name, lit(random.nextfloat()))
          case doubletype => test.withcolumn(x.name, lit(random.nextdouble()))
          case integertype => test.withcolumn(x.name, lit(intarray(random.nextint(intarray.size))))
          case stringtype => test.withcolumn(x.name, lit(myrandomstringarray(random.nextint(myrandomstringarray.size))))
          case longtype => test.withcolumn(x.name, lit(random.nextlong()))
// any data types you want to add thats upto your data requirements//case timestamptype => test //.withcolumn(x.name, timestamp.valueof(""2017-12-02 03:04:00""))case _ => test
        }

    })
    val finaldf = training.union(test)
    logger.info(finaldf.count())
    test.show(false)
    finaldf.printschema()
    finaldf.show(false)

    val dftowrite = finaldf.coalesce(numberofpartfilestobegenerated).write.mode(savemode.overwrite)
    val allformats = array(""parquet"", ""avro"", ""csv"", ""json"", ""xml"")
    format match {

      case ""all"" =>
        logger.info(s""writing all formats ${allformats.mkstring(""\n"")} "");
        allformats.foreach(formattype => writetofileformats(formattype, dftowrite));
      case _ =>
        logger.info(s""writing $format format } "");
        writetofileformats(format, dftowrite)
  }
} dataframeutil is the support class for the above... package com.examples


import java.sql.timestamp
import java.text.decimalformat

import org.apache.log4j.{level, logger}
import org.apache.spark.internal.logging
import org.apache.spark.sql
import org.apache.spark.sql.types._
import org.apache.spark.sql.{dataframe, dataframewriter, row, sparksession}

/**
  * dataframeutil - reusable functions across this project
  *
  * @author ram ghadiyaram
  */
object dataframeutil {
  val logger = logger.getlogger(this.getclass.getname)
  logger.getlogger(""org"").setlevel(level.warn)
  //  def main(args: array[string]): unit = {//    val conf = new sparkconf()//    conf.setappname(""datasets test"")//    conf.setmaster(""local[2]"")//    val sc = new sparkcontext(conf)//    logger.info(sc)//  }val decimalformat = new decimalformat(""###.###"")

  /**
    * replicateonerowmanytimes : replicates same row many ines so that test data is created
    *
    * @param n  int
    * @param df dataframe
    * @return dataframe
    */def replicateonerowmanytimes(n: int, df: dataframe): dataframe = df.sqlcontext.createdataframe(
    df.sparksession.sparkcontext.parallelize(list.fill(n)(df.take(1)(0)).toseq),
    df.schema)

  /**
    * createstruct.
    *
    * @param myobject
    * @return
    */def createstruct(myobject: string): datatype = {

    myobject match {
      case t if t.touppercase.contains(""string"") => stringtypecase t if t.touppercase.contains(""long"") => longtypecase t if t.touppercase.contains(""int"") => integertypecase t if t.touppercase.contains(""float"") => floattypecase t if t.touppercase.contains(""double"") => doubletypecase t if t.touppercase.contains(""timestamp"") => timestamptypecase t if t.touppercase.contains(""json"") => stringtypecase t if t.touppercase.contains(""short"") => shorttypecase t if t.touppercase.contains(""byte"") => bytetypecase t if t.touppercase.contains(""boolean"") => booleantypecase t if t.touppercase.contains(""binary"") => binarytypecase t if t.touppercase.contains(""calendar"") => calendarintervaltypecase t if t.touppercase.contains(""decimal"") => decimaltype.system_defaultcase _ => stringtype
    }
  }

  /**
    * createtestdata.
    *
    * @param myobject
    * @return
    */def createtestdata(myobject: string, length: int): any = {
    val r = scala.util.random
    logger.info(""myobject"" + myobject)
    myobject.touppercase() match {
      case t if t.contains(""string"") => {
        randomstring(length)
      }
      case t if t.touppercase.contains(""long"") => r.nextlong()
      case t if t.touppercase.contains(""int"") => {
        val full = r.nextint(integer.max_value).tostring
        val len = (if (full.length > length) length else full.length).toint
        logger.info(s""full$full $length"")
        val int = full.substring(0, len).toint;
        logger.info(""next int value -->>"" + int)
        int
      }
      case t if t.touppercase.contains(""float"") => r.nextfloat()
      case t if t.touppercase.contains(""double"") => r.nextdouble()
      case t if t.contains(""timestamp"") => timestamp.valueof(""2017-12-02 03:04:00"")
      case t if t.contains(""json"") => ""{}""case t if t.touppercase.contains(""short"") => r.nextint(length).toshort
      case t if t.touppercase.contains(""byte"") => r.nextlong().tobyte
      case t if t.touppercase.contains(""boolean"") => r.nextboolean()
      case t if t.touppercase.contains(""binary"") => 4.tobinarystring
      case t if t.touppercase.contains(""calendar"") => null
    }
  }

  /**
    * randomstring - generates new randomstring
    *
    * @param length
    * @return
    */def randomstring(length: int) = {
    val r = new scala.util.randomval sb = new stringbuilderfor (i <- 1 to length) {
      sb.append(r.nextprintablechar)
    }
    sb.tostring
  }

  /**
    * add column index to dataframe
    *
    * @param spark      sparksession
    * @param df         dataframe
    * @param columnname string
    * @return
    */def addcolumnindex(df: dataframe, columnname: string): sql.dataframe = {
    logger.info(""original df"")
    df.show
    val finaldf = df.sparksession.createdataframe(
      df.rdd.zipwithindex.map {
        case (row, index) => row.fromseq(row.toseq :+ index)
      },
      // create schema for index columnstructtype(df.schema.fields :+ structfield(columnname, longtype, false)))
    logger.info(""after transformation "")
    finaldf.show(false)
    finaldf
  }

  /**
    * randomstring - generates new randomstring
    *
    * @param length
    * @return
    */def randomint(length: int) = {
    logger.info(""length is "" + length)
    val r = new scala.util.randomval sb = new stringbuilderfor (i <- 1 to length) {
      val value = r.nextint(length)
      if (value > 0)
        sb.append(value)
    }
    logger.info(sb)
    sb.tostring.substring(0, length).toint
  }

  /**
    * movecolumntofirstpos : this function can be used after withcolumn applied to a data frame
    * which will add column to the right most position
    * if you want to move column to first position in the dataframe selection and drop duplicate as well
    *
    * @param dataframecolumnorderchanged
    */def movecolumntofirstpos(dataframecolumnorderchanged: dataframe, colname: string) = {
    val xs = array(colname) ++ dataframecolumnorderchanged.columns.dropright(1)
    val fistposcoldf = dataframecolumnorderchanged.selectexpr(xs: _*)
    fistposcoldf.show(false)
    fistposcoldf
  }

  /**
    * splitdf - splits in to 2 dataframe one is training and another one is test data
    *
    * @param dfreplicated
    * @param percent1
    * @param percent2
    */def splitdf(dfreplicated: dataframe, percent1: double, percent2: double): tuple2[dataframe, dataframe] = {
    dfreplicated.randomsplit(array[double](percent1, percent2: double)) match {
      case array(training, test) => (training, test)
    }
  }

  /**
    * writetofileformats.
    *
    * @param format
    * @param dftowrite
    */def writetofileformats(format: string, dftowrite: dataframewriter[row]) = {
    format match {
      case ""avro"" => dftowrite.format(""com.databricks.spark.avro"").save(s""src/test/resources/testdatasetgen/test.${format.tostring.tolowercase}"")
      case ""csv"" => dftowrite.csv(s""src/test/resources/testdatasetgen/test.${format.tostring.tolowercase}"")
      case ""json"" => dftowrite.json(s""src/test/resources/testdatasetgen/test.${format.tostring.tolowercase}"")
      case ""xml"" => dftowrite.format(""com.databricks.spark.xml"").save(s""src/test/resources/testdatasetgen/test.${format.tostring.tolowercase}"")
      case _ | ""parquet"" => dftowrite.parquet(s""src/test/resources/testdatasetgen/test.${format.tostring.tolowercase}"")
    }
  }

  /** lazily instantiated singleton instance of sparksession */

  object sparksessionsingleton extends logging {
    val logger = logger.getlogger(this.getclass.getname)
    @transient private var instance: sparksession = _

    def getinstance(app: option[string]): sparksession = {
      logdebug("" instance "" + instance)
      if (instance == null) {
        instance = sparksession
          .builder
          .config(""spark.master"", ""local"") //.config(""spark.eventlog.enabled"", ""true"")
          .appname(""apptestdataplaygroundwithspark"")
          .getorcreate()
      }
      instance
    }
  }

} note : i used ""local"" and its saved my test resources directory. some one want to generate huge data in hdfs then run master with ""yarn"" result : like comment copy linkedin facebook twitter share 21 to view or add a comment, sign in more articles by ram ghadiyaram my thought to find sql and durations (generic python module) and  without touching the pyspark business code which is already deployed nov 17, 2024 my thought to find sql and durations (generic python module) and  without touching the pyspark business code which is already deployed problem : traditionally, tracking sql, execution durations in pyspark meant adding timing code to each query. this… 29 how to add custom spark listener logs to the aws emr ui oct 19, 2024 how to add custom spark listener logs to the aws emr ui introduction aws emr (elastic mapreduce) provides a powerful platform for processing large datasets using popular big… 18 how to generate csv from impala/hive console output to csv using pyspark oct 12, 2023 how to generate csv from impala/hive console output to csv using pyspark i have employee table in hive, i will execute a query in hive employee table which will give console output like this i… 15 delta lake insights oct 4, 2023 delta lake insights delta lake, as an open-source storage layer that brings acid transactions to apache spark and big data workloads… 17 spark sql/hive.. - interview questions for big data engineers dec 27, 2020 spark sql/hive.. - interview questions for big data engineers note : gave an attempt to cover some of the use cases/concepts here. will keep on adding in the future. 44 spark sql window functions using plain sql. nov 29, 2020 spark sql window functions using plain sql. spark got several window functions, which are.. 21 apache spark - advanced aggregations nov 17, 2020 apache spark - advanced aggregations group by operation to perform aggregations in our queries. consider the case where we have data with retail store… 27 how to do  simple reporting with excel sheets using apache spark, scala ? aug 31, 2019 how to do  simple reporting with excel sheets using apache spark, scala ? spark data can be published as excel sheet question: can spark data be published as an excel sheet? answer: yes, spark… 37 hadoop yarn fair scheduler advantages.. explained...                   
  configuration part2 jul 30, 2019 hadoop yarn fair scheduler advantages.. explained...                   
  configuration part2 this is continuation to my previous post i used aws emr cluster to configure this and is 10 minutes read.. 10 hadoop yarn fair scheduler advantages.. explained... part1 jul 22, 2019 hadoop yarn fair scheduler advantages.. explained... part1 what is fair : keywords: hadoop, mapreduce, task scheduling, yet another resource negotiator, yarn, hadoop distributed… 17 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/sentiment-analysis-pyspark-ricky-kim,,,"sentiment analysis with pyspark agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in sentiment analysis with pyspark report this article ricky kim ricky kim data engineer at moo published mar 13, 2018 + follow one of the tools i’m deeply interested but haven’t had many chances to explore is apache spark. most of the time, pandas and scikit-learn is enough to handle the size of data i’m trying to build a model on. but that also means that i haven’t had a chance to deal with petabytes of data yet, and i want to be prepared for the case i’m faced with a real big-data. i have tried some basic data manipulation with pyspark before, but only to a very basic level. i want to learn more and be more comfortable in using pyspark. this post is my endeavour to have a better understanding of pyspark. python is great for data science modelling, thanks to its numerous modules and packages that help achieve data science goals. but what if the data you are dealing with cannot be fit into a single machine? maybe you can implement careful sampling to do your analysis on a single machine, but with distributed computing framework like pyspark, you can efficiently implement the task for large datasets. spark api is available in multiple programming languages (scala, java, python and r). there are debates about how spark performance varies depending on which language you run it on, but since the main language i have been using is python, i will focus on pyspark without going into too much detail of what language should i choose for apache spark. spark has three different data structures available through its apis: rdd, dataframe (this is different from pandas data frame), dataset. for this post, i will work with dataframe, and the corresponding machine learning library sparkml. i first decided on the data structure i would like to use based on the advice from the post in analytics vidhya. “dataframe is much faster than rdd because it has metadata (some information about data) associated with it, which allows spark to optimize query plan.” you can find a comprehensive introduction from the original post . and there’s also an informative post on databricks comparing different data structures of apache spark: “a tale of three apache spark apis: rdds, dataframes, and datasets”. then i figured out that i need to use sparkml instead sparkmllib if i want to deal with dataframe. sparkmllib is used with rdd, while sparkml supports dataframe. one more thing to note is that i will work in local mode with my laptop. the local mode is often used for prototyping, development, debugging, and testing. however, as spark’s local mode is fully compatible with the cluster mode, codes written locally can be run on a cluster with just a few additional steps. in order to use pyspark in jupyter notebook, you should either configure pyspark driver or use a package called findspark to make a spark context available in your jupyter notebook. you can easily install findspark by “pip install findspark” on your command line. let’s first load some of the basic dependencies we need. import findspark
findspark. init () import pyspark as ps import warnings
from pyspark.sql import sqlcontext first step in any apache programming is to create a sparkcontext. sparkcontext is needed when we want to execute operations in a cluster. sparkcontext tells spark how and where to access a cluster. it is first step to connect with apache cluster. try : # create sparkcontext on all cpus available: in my case i have 4 cpus on my laptop sc = ps.sparkcontext( 'local[4]' )
    sqlcontext = sqlcontext(sc)
    print( ""just created a sparkcontext"" ) except valueerror:
    warnings.warn( ""sparkcontext already exists in this scope"" ) the dataset i’ll use for this post is annotated tweets from “ sentiment140 ”. it originated from a stanford research project, and i used this dataset for my previous series of twitter sentiment analysis. since i already cleaned the tweets during the process of my previous project, i will use pre-cleaned tweets. if you want to know more in detail about the cleaning process i took, you can check my previous post: “ another twitter sentiment analysis with python-part 2 ” . df = sqlcontext.read.format( 'com .databricks.spark.csv').options(header= 'tru e', inferschema= 'tru e').load( 'project -capstone/ twitter_sentiment_analysis /clean_tweet.csv') type ( df ) df.show(5) df = df.dropna()
df. count () after successfully loading the data as spark dataframe, we can take a peek at the data by calling .show(), which is equivalent to pandas .head(). after dropping na, we have a bit less than 1.6 million tweets. i will split this into three parts; training, validation, test. since i have around 1.6 million entries, 1% each for validation and test set will be enough to test the models. (train_ set , val_ set , test _ set ) = df.randomsplit([0.98, 0.01, 0.01], seed = 2000) hashingtf + idf + logistic regression through my previous attempt at sentiment analysis with pandas and scikit-learn, i learned that tf-idf with logistic regression is quite a strong combination, and showed robust performance, as high as word2vec + convolutional neural network model. so in this post, i will try to implement tf-idf + logistic regression model with pyspark. by the way, if you want to know more in detail about how tf-idf is calculated, please check my previous post: “ another twitter sentiment analysis with python — part 5 (tfidf vectorizer, model comparison, lexical approach) ” from pyspark.ml.feature import hashingtf, idf, tokenizer from pyspark.ml.feature import stringindexer from pyspark.ml import pipeline

tokenizer = tokenizer(inputcol= ""text"" , outputcol= ""words"" )
hashtf = hashingtf(numfeatures= 2 ** 16 , inputcol= ""words"" , outputcol= 'tf' )
idf = idf(inputcol= 'tf' , outputcol= ""features"" , mindocfreq= 5 ) #mindocfreq: remove sparse terms label_stringidx = stringindexer(inputcol = ""target"" , outputcol = ""label"" )
pipeline = pipeline(stages=[tokenizer, hashtf, idf, label_stringidx])

pipelinefit = pipeline.fit(train_set)
train_df = pipelinefit.transform(train_set)
val_df = pipelinefit.transform(val_set)
train_df.show( 5 ) from pyspark.ml.classification import logisticregression
lr = logisticregression(maxiter= 100 )
lrmodel = lr.fit(train_df)
predictions = lrmodel.transform(val_df) from pyspark.ml.evaluation import binaryclassificationevaluator
evaluator = binaryclassificationevaluator(rawpredictioncol= ""rawprediction"" )
evaluator.evaluate(predictions) 0.86! that looks good, maybe too good. because i already tried the same combination of techniques with the same data in pandas and sklearn, i know that the result for unigram tf-idf with logistic regression is around 80% accuracy. there can be some slight difference due to the detailed model parameters, but still, this looks too good. and by looking at the spark documentation i realised that what binaryclassificationevaluator evaluates is by default areaunderroc. and for binary classification, spark doesn’t support accuracy as a metric. but i can still calculate accuracy by counting the number of predictions matching the label and dividing it by the total entries. accuracy = predictions. filter (predictions.label == predictions.prediction). count () / float(val_set. count ())
accuracy now it looks more plausible, actually, the accuracy is slightly lower than what i have seen from sklearn's result. countvectorizer + idf + logistic regression there’s another way that you can get term frequency for idf (inverse document frequency) calculation. it is countvectorizer in sparkml. apart from the reversibility of the features (vocabularies), there is an important difference in how each of them filters top features. in case of hashingtf it is dimensionality reduction with possible collisions. countvectorizer discards infrequent tokens. let’s see if performance changes if we use countvectorizer instead of hashingtf. %%time
from pyspark.ml.feature import countvectorizer

tokenizer = tokenizer(inputcol= ""text"" , outputcol= ""words"" )
cv = countvectorizer(vocabsize=2**16, inputcol= ""words"" , outputcol= 'cv' )
idf = idf(inputcol= 'cv' , outputcol= ""features"" , mindocfreq=5) #mindocfreq: remove sparse terms label_stringidx = stringindexer(inputcol = ""target"" , outputcol = ""label"" )
lr = logisticregression(maxiter=100)
pipeline = pipeline(stages=[tokenizer, cv, idf, label_stringidx, lr])

pipelinefit = pipeline.fit(train_ set )
predictions = pipelinefit.transform(val_ set )
accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float (val_set.count())
roc_auc = evaluator.evaluate(predictions) print ""accuracy score: {0:.4f}"" .format(accuracy) print ""roc-auc: {0:.4f}"" .format(roc_auc) it looks like using countvectorizer has improved the performance a little bit. n-gram implementation in scikit-learn, n-gram implementation is fairly easy. you can define a range of n-grams when you call tfidf vectorizer. but with spark, it is a bit more complicated. it does not automatically combine features from different n-grams, so i had to use vectorassembler in the pipeline, to combine the features i get from each n-gram. i first tried to extract around 16,000 features from unigram, bigram, trigram. this means i will get around 48,000 features in total. then i implemented chi-squared feature selection to reduce the number of features to 16,000 in total. from pyspark.ml.feature import ngram, vectorassembler from pyspark.ml.feature import chisqselector def build_trigrams (inputcol=[ ""text"" , ""target"" ], n= 3 ) : tokenizer = [tokenizer(inputcol= ""text"" , outputcol= ""words"" )]
    ngrams = [
        ngram(n=i, inputcol= ""words"" , outputcol= ""{0}_grams"" .format(i)) for i in range( 1 , n + 1 )
    ]

    cv = [
        countvectorizer(vocabsize= 2 ** 14 ,inputcol= ""{0}_grams"" .format(i),
            outputcol= ""{0}_tf"" .format(i)) for i in range( 1 , n + 1 )
    ]
    idf = [idf(inputcol= ""{0}_tf"" .format(i), outputcol= ""{0}_tfidf"" .format(i), mindocfreq= 5 ) for i in range( 1 , n + 1 )]

    assembler = [vectorassembler(
        inputcols=[ ""{0}_tfidf"" .format(i) for i in range( 1 , n + 1 )],
        outputcol= ""rawfeatures"" )]
    label_stringidx = [stringindexer(inputcol = ""target"" , outputcol = ""label"" )]
    selector = [chisqselector(numtopfeatures= 2 ** 14 ,featurescol= 'rawfeatures' , outputcol= ""features"" )]
    lr = [logisticregression(maxiter= 100 )] return pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringidx+selector+lr) and now i’m ready to run the function i defined above. %%time
trigram_pipelinefit = build_trigrams().fit(train_ set )
predictions = trigram_pipelinefit.transform(val_ set )
accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float (dev_set.count())
roc_auc = evaluator.evaluate(predictions) # print accuracy, roc_auc print ""accuracy score: {0:.4f}"" .format(accuracy) print ""roc-auc: {0:.4f}"" .format(roc_auc) accuracy has improved, but as you might have noticed, fitting the model took 4 hours! and this is mainly because of chisqselector. what if i extract 5,460 features each from unigram, bigram, trigram in the first place, to have around 16,000 features in total in the end, without chi-squared feature selection? def build_ngrams_wocs (inputcol=[ ""text"" , ""target"" ], n= 3 ) : tokenizer = [tokenizer(inputcol= ""text"" , outputcol= ""words"" )]
    ngrams = [
        ngram(n=i, inputcol= ""words"" , outputcol= ""{0}_grams"" .format(i)) for i in range( 1 , n + 1 )
    ]

    cv = [
        countvectorizer(vocabsize= 5460 ,inputcol= ""{0}_grams"" .format(i),
            outputcol= ""{0}_tf"" .format(i)) for i in range( 1 , n + 1 )
    ]
    idf = [idf(inputcol= ""{0}_tf"" .format(i), outputcol= ""{0}_tfidf"" .format(i), mindocfreq= 5 ) for i in range( 1 , n + 1 )]

    assembler = [vectorassembler(
        inputcols=[ ""{0}_tfidf"" .format(i) for i in range( 1 , n + 1 )],
        outputcol= ""features"" )]
    label_stringidx = [stringindexer(inputcol = ""target"" , outputcol = ""label"" )]
    lr = [logisticregression(maxiter= 100 )] return pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringidx+lr) let's run this newly defined n-gram function. %%time
trigramwocs_pipelinefit = build_ngrams_wocs().fit(train_ set )
predictions_wocs = trigramwocs_pipelinefit.transform(val_ set )
accuracy_wocs = predictions_wocs.filter(predictions_wocs.label == predictions_wocs.prediction).count() / float (val_set.count())
roc_auc_wocs = evaluator.evaluate(predictions_wocs) # print accuracy, roc_auc print ""accuracy score: {0:.4f}"" .format(accuracy_wocs) print ""roc-auc: {0:.4f}"" .format(roc_auc_wocs) this has given me almost same result, marginally lower, but the difference is in the fourth digit. considering it takes only 6 mins without chisqselector, i definitely choose the model without chisqselector. and finally, let’s try this model on the final test set. test _predictions = trigramwocs_pipelinefit.transform( test _ set ) test _accuracy = test _predictions.filter( test _predictions.label == test _predictions.prediction).count() / float ( test _set.count()) test _roc_auc = evaluator.evaluate( test _predictions) # print accuracy, roc_auc print ""accuracy score: {0:.4f}"" .format( test _accuracy) print ""roc-auc: {0:.4f}"" .format( test _roc_auc) final test set accuracy is 81.22% with roc-auc 0.8862. through this post, i have implemented a simple sentiment analysis model with pyspark. even though it might not be an advanced level use of pyspark, but i believe it is important to keep expose myself to new environment and new challenges. exploring some basic functions of pyspark really sparked (no pun intended) my interest. i am attending spark london meetup tomorrow (13/03/2018) for “apache spark: deep learning pipelines, pyspark mllib and models in streams”. i can’t wait to explore deeper into pyspark world!! thank you for reading and you can find the jupyter notebook from the below link: https://github.com/tthustla/setiment_analysis_pyspark/blob/master/sentiment%20analysis%20with%20pyspark.ipynb and medium blog post: like comment copy linkedin facebook twitter share 11 1 comment oliver steadman 💚💚💚💚💚💚💚💚💚💚💚 6y report this comment looking forward to the next article ricky! like reply 1 reaction to view or add a comment, sign in more articles by ricky kim bayesball: bayesian analysis of batting average mar 4, 2018 bayesball: bayesian analysis of batting average *in addition to short code blocks i will attach, you can find the link for the whole jupyter notebook at the end of… 4 another twitter sentiment analysis with python — part 11 (cnn + word2vec) feb 23, 2018 another twitter sentiment analysis with python — part 11 (cnn + word2vec) this is the 11th and the last part of my twitter sentiment analysis project. it has been a long journey, and through… 9 2 comments efficient frontier portfolio optimisation in python feb 17, 2018 efficient frontier portfolio optimisation in python my personal interest in finance has led me to take an online course on investment management in coursera. it is a… 9 another twitter sentiment analysis with python — part 10 (neural network with doc2vec/word2vec/glove) feb 9, 2018 another twitter sentiment analysis with python — part 10 (neural network with doc2vec/word2vec/glove) this is the 10th part of my ongoing twitter sentiment analysis project. you can find the previous posts from the below… 6 another twitter sentiment analysis - part 9 (neural networks with tfidf vectors using keras) jan 31, 2018 another twitter sentiment analysis - part 9 (neural networks with tfidf vectors using keras) this is the 9th part of my ongoing twitter sentiment analysis project. you can find the previous posts from the below… 4 another twitter sentiment analysis with python - part 8 (dimensionality reduction: chi2, pca) jan 25, 2018 another twitter sentiment analysis with python - part 8 (dimensionality reduction: chi2, pca) this is the 8th part of my ongoing twitter sentiment analysis project. you can find the previous posts from the below… 7 1 comment another twitter sentiment analysis with python - part 7 (phrase model + doc2vec) jan 20, 2018 another twitter sentiment analysis with python - part 7 (phrase model + doc2vec) this is the 7th part of my ongoing twitter sentiment analysis project. you can find the previous posts from the below… 5 another twitter sentiment analysis with python - part 6 (doc2vec) jan 18, 2018 another twitter sentiment analysis with python - part 6 (doc2vec) this is the 6th part of my ongoing twitter sentiment analysis project. you can find the previous posts from the below… 5 another twitter sentiment analysis with python - part 5 (tfidf vectorizer, model comparison, lexical approach) jan 13, 2018 another twitter sentiment analysis with python - part 5 (tfidf vectorizer, model comparison, lexical approach) this is the 5th part of my ongoing twitter sentiment analysis project. you can find the previous posts from the below… 3 another twitter sentiment analysis with python - part 4 (count vectorizer, confusion matrix) jan 10, 2018 another twitter sentiment analysis with python - part 4 (count vectorizer, confusion matrix) this is the part 4 of my ongoing twitter sentiment analysis project. in part 3, i mainly focused on eda and data… 2 1 comment show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/unit-testing-apache-spark-applications-scala-python-choudhary,,Unit Testing,"unit testing apache spark applications in scala or python agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in hops hadoop unit testing apache spark applications in scala or python report this article abhishek choudhary abhishek choudhary data infrastructure engineering in rwe/rwd | healthtech  dhanvantriai published jul 12, 2017 + follow i saw a trend that developers usually find it very complicated to test spark application, may be no good library available or something. but not doing unit testing spark application is actually making developer life much tough. since apache spark is a distributed framework, its extremely hard and annoying to always deploy your jar in server and test the output. that’s where spark testing saves you. in a good testing framework, you can skip this entire process of sending jars everytime to server and simply run in the test in your workstation/laptop or whatever. moreover, testing spark applications will help you to test your functions/udfs with any unknown, weird data which you can just anticipate. if you have a streaming data where you just don’t know when a particular set of data will come, testing will solve this problem. making it a habit will surely help you and your development. apache spark scala testing https://github.com/holdenk/spark-testing-base how to use it add the following in you sbt file – librarydependencies += ""com.holdenkarau"" %% ""spark-testing-base"" % ""2.1.0_0.6.0"" test( ""testfilterandsum_udf of network utils"" ) { val agentconndf = spark.createdataframe( seq (

    ( 10 , ""a"" , 1000 ),

    ( 11 , ""a"" , 5000 ),

    ( 12 , ""a"" , 6000 ),

    ( 12 , ""a"" , 7000 ),

    ( 14 , ""a"" , 6000 ),

    ( 10 , ""a"" , 90000 )

  )).todf( ""remotehostname"" , ""name"" , ""age"" ) val resultrdd = 

  assertdataframeequals(output.orderby( ""remotehostname"" ),result)

} now simply run test, testonly commands tested in windows/linux both apache spark python testing https://pypi.python.org/pypi/pytest-spark pip install pytest-spark add pytest.ini file in your project and simply add your spark directory, can be a local build [pytest]

spark_home = def test_create_df (spark_session) : df = create_mock_df(spark_session) assert df.count() == 2 just run pytest file name tested in linux only like comment copy linkedin facebook twitter share 54 to view or add a comment, sign in more articles by abhishek choudhary slack new architecture jan 1, 2020 slack new architecture this article presented the architecture/engineering decisions and changes brought in slack to scale it massively but by… 34 spark on yarn cluster, some observations apr 24, 2017 spark on yarn cluster, some observations 1. number of partitions in spark basic => n number of cores = n partitions = number of executors good => 2-3 times of… 45 4 comments apache spark (big data) cache - something nice to know jan 17, 2017 apache spark (big data) cache - something nice to know spark caching is one of the most important aspect of in-memory computing technology. spark rdd caching is required when… 15 apache airflow - if you are bored of oozie & style dec 12, 2016 apache airflow - if you are bored of oozie & style apache airflow is an incubator apache project for workflow or job scheduler. dag is the backbone of airflow. 16 1 comment apache spark serialization issue nov 13, 2016 apache spark serialization issue its bit common to face spark serialization issue while working with streaming or basic spark job org.apache. 5 3 comments few points on apache spark 2.0 streaming over cluster aug 23, 2016 few points on apache spark 2.0 streaming over cluster experience on apache spark 2.0 streaming over cluster apache spark streaming documentation has enough details about its… 8 facebook architecture (technical) nov 19, 2015 facebook architecture (technical) facebook's current architecture is: web front-end written in php. facebook's hiphop compiler [1] then converts it to… 12 apache flink ,from a developer point of view oct 26, 2015 apache flink ,from a developer point of view what is apache flink ? apache flink is an open source platform for distributed stream and batch data processing flink’s… 9 2 comments apache spark (big data) dataframe  - things to know oct 12, 2015 apache spark (big data) dataframe  - things to know what is the architecture of apache spark now? what is the point of interaction in spark? previously it was rdd but… 50 6 comments apache spark 1.5 released ... sep 10, 2015 apache spark 1.5 released ... apache spark 1.5 is released and now available to download http://spark. 1 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/unit-testing-spark-spring-boot-applications-neeraj-malhotra,JUnit,Unit Testing,"unit testing spark spring boot applications agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in unit testing spark spring boot applications report this article neeraj malhotra neeraj malhotra published dec 9, 2017 + follow unit testing is a critical part of any production-grade application and big data applications are no exception. however, its challenging (hence more fun) to write unit tests in the case of big data applications due to their complex ecosystem. you need to mock large number of dependencies (or use similar framework) to ensure your application logic can run in the test environment. spark applications are relatively easy to unit test as it has ""local mode"" in which application can run inside single jvm. but if you use spring boot framework then it makes it even more easy to unit test your application. we will see this in a ""hello world"" spring booted spark application. source code the complete application sample code is available at github . i will just paste snippets of the code to explain the concepts. our application this simple application has a method process() that we want to test. here is the code: public int process () {

    system. out .println( ""processing started..."" );
    dataset<row> scoresdf = sparksession.read().table( ""scores"" ); int result = scoresdf.agg(max( ""score"" )).collectaslist(). get ( 0 ).getint( 0 );
    system. out .println( ""result is: "" + result);
    system. out .println( ""processing completed..."" ); return result;
 } application configuration there is only one bean that we create in our application configuration class. this is for providing sparksession object, as shown below: @ bean public sparksession sparksession() {

      return sparksession

            .builder()

            .appname(""sparkwithspring"")

            .master(""yarn"")

            .enablehivesupport()

            .getorcreate();

   } test configuration we need similar sparksession bean for our test cases to run. since, we need to run our test cases on build machine, we use ""local"" as master instead of ""yarn"" as needed for production application. @ bean public sparksession sparksession() {

      return sparksession

            .builder()       

            .appname(""sparkwithspringtest"")

            .master(""local"")

            .enablehivesupport()

            .getorcreate();

   } to ensure that your test cases use test configuration, we use below annotation to test class: @ springboottest ( classes ={testconfig.class, springsampleapplication.class}) unit test setup our application reads a ""scores"" table, so we will create one similar table with test data: @before public void setup () { if (!setupdone) {

         structtype schema = new structtype().add(datatypes.createstructfield( ""name"" , datatypes.stringtype, true ))

               .add(datatypes.createstructfield( ""score"" , datatypes.integertype, true ));

         list<row> data = new arraylist<>();

         data.add(rowfactory.create( ""jerry"" , new integer( 90 )));

         data.add(rowfactory.create( ""tom"" , new integer( 50 )));

         data.add(rowfactory.create( ""pikachu"" , new integer( 95 )));

         dataset<row> df = sparksession.createdataframe(data, schema);

         df.show();

         df.write().mode(savemode.overwrite).saveastable( ""scores"" );

         setupdone = true ;

      }

   } you would have noticed that we use "" setupdone "" flag to ensure this table is created only once rather than for every test case. though, junit has @beforeclass annotation but that needs our setup method to be marked static and then dependency injection for sparksession won't work. unit test case the springsampleapplication object is autowired in the test class already. @test public void testsum () assertequals ( 95 , application.process() ) ;
} test results like comment copy linkedin facebook twitter share 5 1 comment arapkering kirwa software engineer | java developer | computational nlu | alumnus in waiting jkuat. 1y report this comment caused by: java.lang.reflect.inaccessibleobjectexception : unable to make private java.nio.directbytebuffer (long,int) accessible: module java.base does not ""opens java.nio "" to unnamed module @3745e5c6 is what am  getting . like reply 1 reaction to view or add a comment, sign in more articles by neeraj malhotra integrating spark with spring boot may 31, 2017 integrating spark with spring boot source: http://relishcode.com off late, i have started relying more and more on spring boot for increased productivity. 72 28 comments rising cost of technical debt feb 26, 2017 rising cost of technical debt technical debt is the fallout of preferring an inferior short term solution instead of a superior long term solution… 8 introduction to threat modeling jan 8, 2017 introduction to threat modeling threat modeling is one of the key activity in the design phase of security development lifecycle (sdl) that is promoted… 4 xss prevention using input validation nov 27, 2016 xss prevention using input validation this is my last blog in the xss prevention series that i started after my deep dive into this area few months back… 6 xss prevention using html sanitization oct 9, 2016 xss prevention using html sanitization in the last blog, we discussed about preventing xss attacks by encoding the user supplied input before displaying it… 10 2 comments xss prevention using output encoding sep 5, 2016 xss prevention using output encoding as mentioned in the earlier blog, output encoding is the best defense against xss. output encoding depends on the… 21 xss prevention aug 16, 2016 xss prevention recently, i got an excellent opportunity to take deep dive into xss (cross site scripting) vulnerability. as part of my… 7 spring rest for http conditional updates jul 19, 2016 spring rest for http conditional updates since many of us use spring framework and have code written with spring web mvc, it is interesting to see how it can… 1 http conditional updates with jax-rs jun 12, 2016 http conditional updates with jax-rs finally i got the time to write this blog and conclude my previous blog on concurrency using http conditional updates… 3 concurrency using http conditional updates may 22, 2016 concurrency using http conditional updates let us assume we have multiple clients that are concurrently getting and updating shared resource like customer… 2 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/apache-spark-big-data-cache-something-nice-know-abhishek-choudhary,,Unit Testing,"apache spark (big data) cache - something nice to know agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in apache spark (big data) cache - something nice to know report this article abhishek choudhary abhishek choudhary data infrastructure engineering in rwe/rwd | healthtech  dhanvantriai published jan 17, 2017 + follow spark caching is one of the most important aspect of in-memory computing technology. spark rdd caching is required when rdd branches out or when rdd is used multiple times. cache is as well known as break the lineage remember spark cache is lazy, so simply creating an rdd and call rdd.cache() will do nothing. what happens if your parent rdd dies but you assumed that child rdd is already cached ? parentrdd
val childrdd = parentrdd. map (...)
childrdd.first() // execute action childrdd.cache()
parentrdd.unpersist() // running the program in spark 2.1.0 cluster mode storage view says, cache happened only 50%. fine lets try count the childrdd // this should be finished within 35 ms childrdd. count () for above sample, i was expecting an execution time of about 30 ms but its more than that (60 ms), exploring further the dag tree, i realised its doing more work than i expected. caching is serialization & storing in shared memory of cluster, so it has de-serialization associated to confirm again, i re-run the count operation and this time actually finished within expected time. why ? because first time count operation read through entire rdd and entire rdd was cached this time and next time when i ran the count again, it ran much faster because there was no need of rdd recreation. caching, partial or full, is based on the first action call made on rdd right after cache calling. notice the difference in executor computing time, the green bar from the previous image. so now computing time is much lesser than previous partial cache status. few more important points about spark caching - spark caching is not at all necessary, and it fits mostly with iterative style of computing or rdd re-creation is expensive. spark cache uses lru for releasing memory but its better to use unpersist manually if you are sure about rdd lifecycle. spark cache leads to overhead in jvm which is extremly important for spark application, so always be sure about caching. tuning spark memory fraction will change the behavior of caching and data splill, so if you are changing, read and understand the concept properly. spark.memory.fraction

spark.memory.storagefraction like comment copy linkedin facebook twitter share 15 to view or add a comment, sign in more articles by abhishek choudhary slack new architecture jan 1, 2020 slack new architecture this article presented the architecture/engineering decisions and changes brought in slack to scale it massively but by… 34 unit testing apache spark applications in scala or python jul 12, 2017 unit testing apache spark applications in scala or python i saw a trend that developers usually find it very complicated to test spark application, may be no good library… 54 spark on yarn cluster, some observations apr 24, 2017 spark on yarn cluster, some observations 1. number of partitions in spark basic => n number of cores = n partitions = number of executors good => 2-3 times of… 45 4 comments apache airflow - if you are bored of oozie & style dec 12, 2016 apache airflow - if you are bored of oozie & style apache airflow is an incubator apache project for workflow or job scheduler. dag is the backbone of airflow. 16 1 comment apache spark serialization issue nov 13, 2016 apache spark serialization issue its bit common to face spark serialization issue while working with streaming or basic spark job org.apache. 5 3 comments few points on apache spark 2.0 streaming over cluster aug 23, 2016 few points on apache spark 2.0 streaming over cluster experience on apache spark 2.0 streaming over cluster apache spark streaming documentation has enough details about its… 8 facebook architecture (technical) nov 19, 2015 facebook architecture (technical) facebook's current architecture is: web front-end written in php. facebook's hiphop compiler [1] then converts it to… 12 apache flink ,from a developer point of view oct 26, 2015 apache flink ,from a developer point of view what is apache flink ? apache flink is an open source platform for distributed stream and batch data processing flink’s… 9 2 comments apache spark (big data) dataframe  - things to know oct 12, 2015 apache spark (big data) dataframe  - things to know what is the architecture of apache spark now? what is the point of interaction in spark? previously it was rdd but… 50 6 comments apache spark 1.5 released ... sep 10, 2015 apache spark 1.5 released ... apache spark 1.5 is released and now available to download http://spark. 1 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/file-format-identification-hive-tables-amol-kale,,,"file format identification for hive tables agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in file format identification for hive tables report this article amol kale amol kale sr. architect at transunion llc | ms data science at ucsd published jul 6, 2017 + follow there are many a times when we start working in hadoop ecosystem and use hive as data warehouse software for data summarisation, query, and analysis. right from the word go we are presented with the question as to which file format to choose for creating the hive tables from performance and compatibility perspective. i would like to share few results, i have come across while testing various file formats for hive tables. environment used for the test: hadoop cloudera cluster with cdh version 5.9.0. (4 node cluster)
hive version: hive 1.1.0-cdh5.9.0 as part of this test below listed file formats were analysed: 1.textfile format.

2.orc file format.

3.rcfile format.

4.parquet file format.

5.sequencefile format.

6.avro file format. as part of this test below datasets were used: all the file formats to store tables were tested for below performance parameters. time taken to write data. time taken for reading or querying the data. cpu time taken for loading the data. size of the table files created in each file format. data evolution supported or not. read performance below is the snippet which consists of comparison results of tables(with 3 million records) with different file formats for varied query and column types. the tables were tested for query time for:- • where clause having one condition. • where clause having two conditions. • where clause having three conditions. • group by column 'category’ with aggregate function on column ‘amount’. • group by column custno’ with aggregate function on column ‘amount’. • join between tables transactions and customers using custno as the join column(buckets created clustered by custno on both the tables). • join between tables transactions and customers using custno as the join column(buckets created, clustered by ‘state’ on transactions table). • where clause on a struct column location consisting of fields ‘city’ and ‘state’ in transactions table. • distinct keyword with state field of struct column in transactions table. write performance below is the table capturing key figures for data loading into table stored with different file formats. clearly from the table below, parquet or orc may optimise partial and full read performance, but they do so at the expense of write performance. also uncompressed files are fast to write due to the lack of compression. storage performance below table shows the file size of the external tables created when stored as different file formats. parquet gives the provision of default compression to 1/10th of the uncompressed version. benchmarks indicate that orc files compress to be the smallest of all file formats in hadoop with explicitly giving any compression codec. sequence files and avro file format can support limited compression with different codecs. observation benefits of choosing orc: considering all the parameters to be tested for a file format to create tables, orc file format comes out as the best format for below reasons. best with resect to the compression, can save lot of space; if that is a constraint. good with read performance giving results quickly for most of the queries. provides good writing performance. orc stores collections of rows in one file and within the collection the row data is stored in a columnar format. this allows parallel procession of row collections across a cluster. benefits of choosing parquet: amongst the other column-oriented binary file formats, parquet file format is a better option. parquet is good for queries scanning particular columns within a table, for example querying “wide” tables with many columns or performing aggregation operations like avg() or sum() for the values of a single column. parquet provides compression closer to what orc file format achieves. unlike rc and orc files parquet serdes also support limited schema evolution. in parquet, new columns can be added at the end of the structure. hopefully, these observation would enable you to decide the file formats to be used for hive tables in your project. below are few notes to keep in mind before choosing any file format: date datatype not supported in parquet. avro files are supported in hive 0.14.0 and later. parquet columnar storage format supported in hive 0.13.0 and later. for rc file: in order to add a column to your data you must rewrite every pre-existing rc file. in hive, it is very important that parquet column names are lowercase. if your parquet file contains mixed case column names, hive will not be able to read the column and will return queries on the column with null values and not log any errors. if you are storing intermediate data between mapreduce jobs, then sequence files are preferred. avro is great if your schema is going to change over time, but query performance will be slower than orc or parquet. csv/text files are excellent if you are going to extract data from hadoop to bulk load into a database. like comment copy linkedin facebook twitter share 11 1 comment souvik ghosh big data analytics & machine learning platform 7y report this comment does the test consider other execution engines for hive ? like reply 1 reaction to view or add a comment, sign in more articles by amol kale spark launcher jan 28, 2018 spark launcher developers working with spark framework often come across scenarios where they want to launch multiple spark jobs in a… 27 4 comments resource allocation for spark jobs jul 12, 2017 resource allocation for spark jobs for spark users, often the task cut out for them is to learn how to squeeze every last bit of juice out of their… 17 6 comments spark with scala over java jul 8, 2017 spark with scala over java while working with parallel, distributed and in-memory processing framework like spark, we have to choose between the… 14 1 comment sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/apache-spark-ruchi-soni,,,"apache spark agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in apache spark report this article ruchi soni ruchi soni ❄️snowflake data superhero❄️|author| data & ai lead| master data architect-4x snowflake/aws/gcp/azure| cloud consultant published apr 2, 2017 + follow spark is an apache product.it is a fast and general engine for large scale data processing . it runs programs up to 100x faster than hadoop mapreduce in memory, or 10x faster on disk. it is a unified platform for big data and is written in scala language but it also supports other languages like java, python and r. history of spark spark comes with multiple benefits like readability,expressiveness,fast,testability,interactive,fault tolerant,unify big data spark architecture spark libraries spark sql :- it enables wider audiences beyond big data engineers to use spark and provides ability to work with different sources and brings multiple optimizations like predicate push down , column pruning. spark streaming :- it is used for streaming data at the rate of gigabytes per second. additionally recovery time is in a matter of seconds. spark mllib library :- it is a machine learning library and makes machine learning easy and scalable. it also allows tuning, testing and early failure detection. spark graphx :- it is used to bring table like structure to graph and is used for graph analytics. like comment copy linkedin facebook twitter share 7 to view or add a comment, sign in more articles by ruchi soni making a difference, one tree at a time!!! jun 5, 2022 making a difference, one tree at a time!!! realizing organization sustainability promise working towards contributing to the organization’s sustainability… 75 4 comments time to mess with ""data mesh"" jan 22, 2022 time to mess with ""data mesh"" since quite some time i was intrigued to understand the concept of data mesh. is it ""old wine in new bottle"" or we are… 35 how to crack snowpro advanced architect exam nov 9, 2021 how to crack snowpro advanced architect exam i recently passed snowpro advanced: architect exam in nov 21 and as promised coming up with list of resources to help… 151 4 comments microservices vs monolithic architecture-use cases jan 9, 2020 microservices vs monolithic architecture-use cases monolithic as the name suggests refers to a single unit. this architecture consists of a client side user interface… 20 what makes a good architect jan 8, 2020 what makes a good architect architecture is an abstraction of a system that selects some details and suppresses others. design is important since… 10 4 comments "" data visualization-the buzzword"" feb 14, 2018 "" data visualization-the buzzword"" data visualization is the presentation of data in a pictorial or graphical format. it enables decision makers to see… 2 unstructured data and hadoop jan 7, 2018 unstructured data and hadoop the ways in which big data will change our lives is significant. to enable big data to deliver we need to ensure that… 16 apache kafka mar 25, 2017 apache kafka apache kafka is a high throughput distributed streaming messaging system which manages data movement, building… 4 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/simple-apache-spark-pid-masking-dataframe-sqlcontext-hive-laurent,,,"simple apache spark pid masking with dataframe, sqlcontext, regexp_replace, hive, and oracle. agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in on-site spark training in georgia simple apache spark pid masking with dataframe, sqlcontext, regexp_replace, hive, and oracle. report this article laurent weichberger laurent weichberger principal customer engineer published aug 21, 2017 + follow recently i taught our standard apache spark training at an on-site client. rapidly they realized the power of spark and asked me if we could please substitute a regular (canned) lab-exercise with one of their own use cases which urgently needed to be solved. i said -- ""sure, which one?"" without getting into the details of their use case, let me just say they wanted to use spark to read in production data, mask pid (""data elements, which identify a unique individual""), and write that masked data back out to a new apache hive table, and also back out to an oracle table, all from a spark dataframe. i said, let's do it as a project on the last day. we all came to an agreement about the use case on wednesday before the end of class, this left us one day (thursday) to solve it. we started our work at 9:00am. i had them decompose the use case into user stories (ala agile-scrum), and we created 30 minute sprints. everyone was working on a story, either alone or in a team. it was amazingly productive. we had a one hour lunch break and then solved this and tested it by 2:00pm. we moved a demo version of the production code into safe location for the project, and loaded it into a dataframe for starters: val dataframea = sqlcontext.sql( ""select * from data_worker.demo"" ) 2.a. we used the withcolumn() method to add a column to the dataframe to hold the masked version of the data, and named it ""text_masked."" the value of the new column is the old column ""text"" with the regexp_replace function applied to it. val dataframeb = dataframea
  .withcolumn( ""text_masked"" , 
     regexp_replace(dataframea( ""text"" ), ""\\d{2,}"" , "" * "" )) //see note 1 below . drop ( ""text"" ) 2.b. the regexp_replace takes three parameters, the column you wish to transform, the pattern to apply, and the new value for that which is found using the pattern. here we are only concerned with numbers, so we say in the text column, whenever we find two or more numbers consecutively, replace that collection of numbers with an asterisk surrounded by white space on each side. note we drop() the old column ""text"" so that we don't continue to propagate the pid data around the systems. so, a ssn value of 789-01-2345 would become * - * - * and a telephone number (212) 345-6789 would become ( * ) * - *. this was good enough for our poc lab exercise solution. 3.a. to write out to hive with this data, we used a simple dataframe method registertemptable followed by the sqlcontext ""create table"" invocation: dataframeb.registertemptable(""demotemp"")

sqlcontext.sql("" create table demohive as select * from demotemp "") 3.b. to write out to oracle we had a more complex invocation from dataframe: import java.util.properties import java.io._

val prop = new properties() 

prop.setproperty( ""user"" , “user”)
prop.setproperty( ""password"" , “password”)
prop.setproperty( ""driver"" , ""oracle.jdbc.driver.oracledriver"" )

dataframeb.write.mode(org.apache.spark.sql.savemode.append)
  .jdbc( ""jdbc:oracle:thin:@(description=
     ﻿(address=(protocol=tcp)(host=<your-host>)(port=<your-port))
    ﻿ (connect_data=(server=dedicated)))"" , ""call.<oracle-table-name>"" , prop) notes: 1. for regex_replace see: enjoy! for more information: laurent weichberger, big data bear, hortonworks :: lweichberger@hortonworks.com like comment copy linkedin facebook twitter share 21 2 comments kanika saxena associate at jp morgan chase & co. | big data | data engineer | hadoop | spark | aws 6y report this comment it's a nice approach. but what can be done if we want to mask the pids with something more relevant, say numbers with numbers and alphabets with alphabets? like reply 1 reaction 2 reactions see more comments to view or add a comment, sign in more articles by laurent weichberger collibra data quality capabilities for basel committee on banking supervision’s standard 239 (“bcbs 239”) by by  k. haslbeck & l. weichberger mar 29, 2025 collibra data quality capabilities for basel committee on banking supervision’s standard 239 (“bcbs 239”) by by  k. haslbeck & l. weichberger preamble during 2024 i met with one of our multinational banking clients at one of their offices in india, and we… 34 1 comment python code to grab two letter state codes jan 23, 2025 python code to grab two letter state codes i needed to have just a list of two letter state codes for a collibra data quality custom rule i was creating for a… 17 3 comments anatomy of a healthy data quality project team, part iii: dq information and sharing with project management, by l. weichberger & c. schmidt (2023). aug 10, 2023 anatomy of a healthy data quality project team, part iii: dq information and sharing with project management, by l. weichberger & c. schmidt (2023). in our previous blog post we shared about dq team culture. we will end this series with part 3: dq required information… 23 4 comments anatomy of a healthy data quality project team, part ii: dq team culture, by laurent weichberger (august 2023). aug 7, 2023 anatomy of a healthy data quality project team, part ii: dq team culture, by laurent weichberger (august 2023). in part one of this blog we discuss data quality roles and responsibilities. beyond roles and responsibilities, we have… 15 5 comments anatomy of a healthy data quality project team :: part i, dq roles and responsibilities aug 5, 2023 anatomy of a healthy data quality project team :: part i, dq roles and responsibilities one of our most successful dq customers has asked me to compose something related to the “anatomy of a successful dq… 51 7 comments dq outlier detection with interquartile range (iqr) in python apr 4, 2022 dq outlier detection with interquartile range (iqr) in python i recently created a presentation on getting started with data quality outlier detection for my company collibra… 34 10 comments data quality without software, towards a dq culture oct 16, 2021 data quality without software, towards a dq culture preamble: the purpose of this document is to discuss data quality in isolation from any software product which solved… 23 1 comment snowflake stored procedure with javascript mar 8, 2021 snowflake stored procedure with javascript i wrote my first snowflake stored procedure using the public citibikes data from citigroup inc. we need to use… 7 2 comments python snowflake connector implementation dec 21, 2020 python snowflake connector implementation last week i added data to my brand new hashmap snowflake database, and this morning i was able to rapidly query that… 19 4 comments apache spark :: hivewarehousesession (crud) with hive 3 managed tables dec 8, 2020 apache spark :: hivewarehousesession (crud) with hive 3 managed tables at a ""fortune 500"" client, i had the opportunity to help them with an in-place upgrade from hortonworks data platform… 36 16 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/comprehensive-analysis-data-processing-part-deux-apache-fazelat,,,"a comprehensive analysis - data processing part deux: apache spark vs apache storm agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in a comprehensive analysis - data processing part deux: apache spark vs apache storm report this article rassul fazelat rassul fazelat president & ceo @ data talent advisors | data, analytics, rag & gen ai recruiting published jan 23, 2016 + follow disclaimer: this post is a combination of original content and facts gathered from reputable sources sited below. i've been compelled to write these posts due so many tech writers putting out articles that are not technically sound, these posts are meant to be factoid for a ""one-stop"" reference. also please keep in mind many of these topics are so new they are evolving as i type this post, so your inputs are greatly appreciated & welcomed. those of you that follow my big data posts on linkedin may have read my post on data processing engines, ""a comprehensive analysis apache flink and how it compares to apache spark"" the reason i am re-visiting data processing again is to help clarify some misunderstandings about apache storm, another popular (also fast) data processing engine that deserves it's time in the limelight. big data enthusiast's interests has been piqued by the recent news of the yahoo! big data team recently performing some bench marking tests comparing apache flink, storm and spark which you can read about here: ( please keep in mind, much of the apache flink results are pending and updated results are in process ): link to yahoo data processing benchmarks test many big data experts evaluating these data processing tools, look not only at performance but also other very important elements such as security, and overall integration capabilities. in this post, i would like to establish the following: how did apache storm come about and who created it? where does it fall into the big data eco-system? what are the different components of apache storm? how does apache storm compare to apache spark streaming ? so in order for us to get a little bit of background on apache storm and how it came about let's get the info directly from the horse's mouth, in this case the creator, nathan marz's blog site who writes: "" apache storm recently (2014) became a top-level project , marking a huge milestone for the project and for me personally. it's crazy to think that four years ago storm was nothing more than an idea in my head, and now it's a thriving project with a large community used by a ton of companies . in this post i want to look back at how storm got to this point and the lessons i learned along the way. the topics i will cover through storm's history naturally follow whatever key challenges i had to deal with at those points in time. the first 25% of this post is about how storm was conceived and initially created, so the main topics covered there are the technical issues i had to figure out to enable the project to exist. the rest of the post is about releasing storm and establishing it as a widely used project with active user and developer communities. the main topics discussed there are marketing, communication, and community development. any successful project requires two things: it solves a useful problem you are able to convince a significant number of people that your project is the best solution to their problem what i think many developers fail to understand is that achieving that second condition is as hard and as interesting as building the project itself. i hope this becomes apparent as you read through storm's history. before storm storm originated out of my work at backtype . at backtype we built analytics products to help businesses understand their impact on social media both historically and in realtime. before storm, the realtime portions of our implementation were done using a standard queues and workers approach. for example, we would write the twitter firehose to a set of queues, and then python workers would read those tweets and process them. oftentimes these workers would send messages through another set of queues to another set of workers for further processing. we were very unsatisfied with this approach. it was brittle – we had to make sure the queues and workers all stayed up – and it was very cumbersome to build apps. most of the logic we were writing had to do with where to send/receive messages, how to serialize/deserialize messages, and so on. the actual business logic was a small portion of the codebase. plus, it didn't feel right – the logic for one application would be spread across many workers, all of which were deployed separately. it felt like all that logic should be self-contained in one application. the first insight in december of 2010, i had my first big realization. that's when i came up with the idea of a ""stream"" as a distributed abstraction. streams would be produced and processed in parallel, but they could be represented in a single program as a single abstraction. that led me to the idea of ""spouts"" and ""bolts"" – a spout produces brand new streams, and a bolt takes in streams as input and produces streams as output. they key insight was that spouts and bolts were inherently parallel, similar to how mappers and reducers are inherently parallel in hadoop. bolts would simply subscribe to whatever streams they need to process and indicate how the incoming stream should be partitioned to the bolt. finally, the top-level abstraction i came up with was the ""topology"", a network of spouts and bolts. i tested these abstractions against our use cases at backtype and everything fit together very nicely. i especially liked the fact that all the grunt work we were dealing with before – sending/receiving messages, serialization, deployment, etc. would be automated by these new abstractions. before embarking on building storm, i wanted to validate my ideas against a wider set of use cases. so i sent out this tweet:"" the initial stable release was on september 17, 2011. wikipedia does a great job describing the topology here: ""a storm application is designed as a ""topology"" in the shape of a directed acyclic graph (dag) with spouts and bolts acting as the graph vertices. edges on the graph are named streams and direct data from one node to another. together, the topology acts as a data transformation pipeline. at a superficial level the general topology structure is similar to a mapreduce job, with the main difference being that data is processed in real-time as opposed to in individual batches. additionally, storm topologies run indefinitely until killed, while a mapreduce job dag must eventually end. [5] storm became an apache top-level project in september 2014 [6] and was previously in incubation since september 2013. [7] [8] "" when comparing apache storm to other data processing engines it's important to note, that apache storm comes in (2) ""flavors"" apache storm core & ""trident,"" which actually runs on top of storm. apache storm trident is essentially a high level abstraction of apache storm. here's what i've gathered are some differences that trident offers versus core storm: trident adds complexity to a storm topology, lowers performance and generates state. trident processes messages in batches, so throughput time could be longer. trident is not yet able to process loops in topologies. (this may have changed with newer releases). the diagram below (by trivadis) does a great job comparing core & trident storm vs apache spark streaming. stream processing architectures – the old and the new at a high level, modern distributed stream processing pipelines execute as following the data lifecycle as we discussed in another previous post, ""a comprehensive analysis - apache kafka"" the data stream life cycle consists of these (3) key components: 1. create - creating data from a multitude of event sources (machines, social media, sensors, logs, databases, click stream, email, html, images, location, etc) 2. collect - make data streams available for consumption (activemq, rabbitmq, apache kafka, etc.) 3. process - processing streams and possibly creating derived streams (apache mapreduce, tez, flink, spark, storm, etc.) this illustration below from dataartisans' apache flink presentation really demonstrates where these data processing engines sit in relation to each other and in relation to other technologies. with this note and overview i would like to dip into apache spark streaming (directly from databricks blog site) and previewing it's various components. most of the specifics within apache spark 1.5 were covered in a previous post here , the changes released for apache spark 1.6 will be examined in a future post. source: dataartisans presentation the following is an excerpt from databricks blog site about apache spark streaming: ""to process the data, most traditional stream processing systems are designed with a continuous operator model , which works as follows: there is a set of worker nodes, each of which run one or more continuous operators . each continuous operator processes the streaming data one record at a time and forwards the records to other operators in the pipeline. there are “source” operators for receiving data from ingestion systems, and “sink” operators that output to downstream systems. continuous operators are a simple and natural model. however, with today’s trend towards larger scale and more complex real-time analytics, this traditional architecture has also met some challenges. we designed spark streaming to satisfy the following requirements: fast failure and straggler recovery – with greater scale, there is a higher likelihood of a cluster node failing or unpredictably slowing down (i.e. stragglers). the system must be able to automatically recover from failures and stragglers to provide results in real time. unfortunately, the static allocation of continuous operators to worker nodes makes it challenging for traditional systems to recover quickly from faults and stragglers. load balancing – uneven allocation of the processing load between the workers can cause bottlenecks in a continuous operator system. this is more likely to occur in large clusters and dynamically varying workloads. the system needs to be able to dynamically adapt the resource allocation based on the workload. unification of streaming, batch and interactive workloads – in many use cases, it is also attractive to query the streaming data interactively (after all, the streaming system has it all in memory), or to combine it with static datasets (e.g. pre-computed models). this is hard in continuous operator systems as they are not designed to the dynamically introduce new operators for ad-hoc queries. this requires a single engine that can combine batch, streaming and interactive queries. advanced analytics like machine learning and sql queries – more complex workloads require continuously learning and updating data models, or even querying the “latest” view of streaming data with sql queries. again, having a common abstraction across these analytic tasks makes the developer’s job much easier. to address these requirements, spark streaming uses a new architecture called discretized streams that directly leverages the rich libraries and fault tolerance of the spark engine. architecture of spark streaming: discretized streams instead of processing the streaming data one record at a time, spark streaming discretizes the streaming data into tiny, sub-second micro-batches. in other words, spark streaming’s receivers accept data in parallel and buffer it in the memory of spark’s workers nodes. then the latency-optimized spark engine runs short tasks (tens of milliseconds) to process the batches and output the results to other systems. note that unlike the traditional continuous operator model, where the computation is statically allocated to a node, spark tasks are assigned dynamically to the workers based on the locality of the data and available resources. this enables both better load balancing and faster fault recovery, as we will illustrate next. in addition, each batch of data is a resilient distributed dataset (rdd), which is the basic abstraction of a fault-tolerant dataset in spark. this allows the streaming data to be processed using any spark code or library. benefits of discretized stream processing let’s see how this architecture allows spark streaming to achieve the goals we set earlier. dynamic load balancing dividing the data into small micro-batches allows for fine-grained allocation of computations to resources. for example, consider a simple workload where the input data stream needs to partitioned by a key and processed. in the traditional record-at-a-time approach taken by most other systems, if one of the partitions is more computationally intensive than the others, the node statically assigned to process that partition will become a bottleneck and slow down the pipeline. in spark streaming, the job’s tasks will be naturally load balanced across the workers — some workers will process a few longer tasks, others will process more of the shorter tasks. fast failure and straggler recovery in case of node failures, traditional systems have to restart the failed continuous operator on another node and replay some part of the data stream to recompute the lost information. note that only one node is handling the recomputation, and the pipeline cannot proceed until the new node has caught up after the replay. in spark, the computation is already discretized into small, deterministic tasks that can run anywhere without affecting correctness. so failed tasks can be relaunched in parallel on all the other nodes in the cluster, thus evenly distributing all the recomputations across many nodes, and recovering from the failure faster than the traditional approach. unification of batch, streaming and interactive analytics the key programming abstraction in spark streaming is a dstream, or distributed stream. each batch of streaming data is represented by an rdd, which is spark’s concept for a distributed dataset. therefore a dstream is just a series of rdds. this common representation allows batch and streaming workloads to interoperate seamlessly. users can apply arbitrary spark functions on each batch of streaming data: for example, it’s easy to join a dstream with a precomputed static dataset (as an rdd). since the batches of streaming data are stored in the spark’s worker memory, it can be interactively queried on demand. for example, you can expose all the streaming state through the spark sql jdbc server, as we will show in the next section. this kind of unification of batch, streaming and interactive workloads is very simple in spark, but hard to achieve in systems without a common abstraction for these workloads. advanced analytics like machine learning and interactive sql spark interoperability extends to rich libraries like mllib (machine learning), sql, dataframes, and graphx. let’s explore a few use cases: streaming + sql and dataframes rdds generated by dstreams can be converted to dataframes (the programmatic interface to spark sql), and queried with sql. for example, using spark sql’s jdbc server , you can expose the state of the stream to any external application that talks sql. then you can interactively query the continuously updated “word_counts” table through the jdbc server, using the beeline client that ships with spark, or tools like tableau. streaming + mllib machine learning models generated offline with mllib can applied on streaming data. for example, the following code trains a kmeans clustering model with some static data and then uses the model to classify events in a kafka data stream. we demonstrated this offline-learning-online-prediction at our spark summit 2014 databricks demo. since then, we have also added streaming machine learning algorithms in mllib that can continuously train from a labelled data stream. other spark libraries can also easily be called from spark streaming. performance given the unique design of spark streaming, how fast does it run? in practice, spark streaming’s ability to batch data and leverage the spark engine leads to comparable or higher throughput to other streaming systems. in terms of latency, spark streaming can achieve latencies as low as a few hundred milliseconds. developers sometimes ask whether the micro-batching inherently adds too much latency. in practice, batching latency is only a small component of end-to-end pipeline latency. for example, many applications compute results over a sliding window, and even in continuous operator systems, this window is only updated periodically (e.g. a 20 second window that slides every 2 seconds). many pipelines collect records from multiple sources and wait for a short period to process delayed or out-of-order data. finally, any automatic triggering algorithm tends to wait for some time period to fire a trigger. therefore, compared to the end-to-end latency, batching rarely adds significant overheads. in fact, the throughput gains from dstreams often means that you need fewer machines to handle the same workload. future directions for spark streaming spark streaming is one of the most widely used components in spark, and there is a lot more coming for streaming users down the road. some of the highest priority items our team is working on are discussed below. you can expect these in the next few releases of spark: backpressure – streaming workloads can often have bursts of data (e.g. sudden spike in tweets during the oscars) and the processing system must be able to handle them gracefully. in the upcoming spark 1.5 release (next month), spark will be adding better backpressure mechanisms that allow spark streaming dynamically control the ingestion rate for such bursts. this feature represents joint work between us at databricks and engineers at typesafe. dynamic scaling – controlling the ingestion rate may not be sufficient to handle longer terms variations in data rates (e.g. sustained higher tweet rate during the day than night). such variations can be handled by dynamically scaling the cluster resource based on the processing demands. this is very easy to do within the spark streaming architecture — since the computation is already divided into small tasks, they can be dynamically redistributed to a larger cluster if more nodes are acquired from the cluster manager (yarn, mesos, amazon ec2, etc). we plan to add support for automatic dynamic scaling. event time and out-of-order data – in practice, users sometimes have records that are delivered out of order, or with a timestamp that differs from the time of ingestion. spark streaming will support “event time” by allowing user-defined time extraction function. this will include a slack duration for late or out-of-order data. ui enhancements – finally, we want to make it easy for developers to debug their streaming applications. for this purpose, in spark 1.4, we added new visualizations to the streaming spark ui that let developers closely monitor the performance of their application. in spark 1.5, we are further improving this by showing more input information like kafka offsets processed in each batch. to learn more about spark streaming, read the official programming guide , or the spark streaming research paper that introduces its execution and fault tolerance model."" i hope this post has further highlighted (2) popular data processing engines that many big data professional are comparing. it is important to note that these data processing engines are not replacements for each other but they should be be used in conjunction with each other. feel free to post your comments below, these posts are meant to be as much collaborative as informative, would love to hear from everyone. rassul fazelat (follow me here @bigdatavision ), is managing partner - founder of data talent advisors , a boutique data & analytics talent advisory & headhunting firm, organizer of nyc big data visionaries meetup , co-organizer of nyc marketing analytics forum & co-organizer of nyc advanced analytics meetup . other posts in the comprehensive analysis (big data) series: deconstructing ai - a closer look 3 reason why hadoop as a service is making sense for business analytics a comprehensive analysis: blockchain technology beyond bitcoin a comprehensive analysis: big data security a comprehensive analysis: dataflow technolog y a comprehensive analysis: data processing part deux: apache spark vs apache storm a comprehensive analysis - nosql vs rdbms a comprehensive analysis: apache kafka a comprehensive analysis: java vs scala a comprehensive analysis: apache flink and how it compares to apache spark a comprehensive analysis: apache spark vs mapreduce big data career series: why are enterprise big data architects not data scientists? why data engineers are not data scientists? top 5 majors for a data science career like comment copy linkedin facebook twitter share 26 1 comment george vassis data engineer technical lead 9y report this comment performance section in spark has twice the same paragraph. the comparison between spark and storm is almost inexisting besides the table. feels a bit like a sequential copy/paste of a vague shoort describtion of storm followed by spark separated by a table. like reply 2 reactions 3 reactions to view or add a comment, sign in more articles by rassul fazelat a comparative analysis:  cloud edw oct 25, 2017 a comparative analysis:  cloud edw disclaimer: this post is a combination of original content and facts gathered from reputable sources due to so many… 29 5 comments deconstructing ai – a closer look feb 1, 2017 deconstructing ai – a closer look by clive marshall & rassul fazelat at a recent nyc meetup that we co-hosted, it was evident that the attendees were… 24 1 comment a comprehensive overview: containers as a service (caas), next generation of virtualization sep 24, 2016 a comprehensive overview: containers as a service (caas), next generation of virtualization disclaimer: this post is a combination of original content and information gathered from reputable websites to provide… 54 4 comments 3 reasons why ""hadoop as a service"" is making sense for business analytics? jul 24, 2016 3 reasons why ""hadoop as a service"" is making sense for business analytics? disclaimer: this post is based on my own experience, conversations with big data professionals the last 4 years, up… 35 2 comments a comprehensive analysis: blockchain beyond bitcoin jun 4, 2016 a comprehensive analysis: blockchain beyond bitcoin disclaimer: this post is a combination of original content and facts gathered from reputable sources sited below. i've… 26 1 comment top 5 college majors for a data science career may 9, 2016 top 5 college majors for a data science career those of you that read my post on why data engineers are not data scientists?, have motivated me to write this next… 73 5 comments a comprehensive analysis:  big data security mar 18, 2016 a comprehensive analysis:  big data security disclaimer: this post is a combination of original content and facts gathered from reputable sources sited below. i've… 24 3 comments a comprehensive analysis: dataflow technology feb 27, 2016 a comprehensive analysis: dataflow technology disclaimer: this post is a combination of original content and facts gathered from reputable sources sited below. i've… 73 2 comments a comprehensive analysis - nosql vs rdbms dec 26, 2015 a comprehensive analysis - nosql vs rdbms disclaimer: this post is a combination of original content and facts gathered from reputable sources sited below. i've… 147 26 comments a comprehensive analysis:  apache kafka nov 29, 2015 a comprehensive analysis:  apache kafka disclaimer: this post is a combination of original content and facts gathered from reputable sources sited below. i've… 69 12 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/data-science-technology-choice-case-study-harry-powell,,,"scala/spark for data science: a case study agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in scala/spark for data science: a case study report this article harry powell harry powell data science leader with track record of innovation and value creation published jan 28, 2016 + follow a couple of years ago, my data science team at barclays chose to work exclusively using functional programming in scala and spark. this has been described as "" brave "". scala was just a programming language. spark 1.0 had not been released. would that work for data science? i thought it might be interesting to reflect on why we made that choice, and how we have found it in practice. i think it raises some useful points even if you are using python, r or sas. there are a load of blogs on scala adoption from a technology angle, but we chose scala from a purely business and data science point of view . we thought about what we needed to do and about which technologies we should invest in. there are always trade-offs in any technology choice. there is no one answer that fits all situations. you need to think about what matters to your use case (from a business angle) and prioritise. the starting point for our thinking was the tension in data science between exploration (finding out interesting patterns) and production (building products which makes money from those patterns). we wanted to strike the right balance. production when we build a data science application we know we will need to deploy it to production and our customers will use it. our data science must be reliable. we must be completely confident in our results. we must be able to develop it further without compromising reliability. because of this we like immutability and static typing. if your code complies, it will run. if your tests pass, it probably works. we tried building an application in python but had real problems. it was quick to write, but hard to debug and even harder to refactor. initially we found scala a bit cumbersome, but it paid dividends in the end. as an example, we released a large and sophisticated spark application without encountering a single serious problem in testing. etl we want our environment to excel at what we spend most of our time doing. while data scientists talk a lot about machine learning and pattern recognition technology, ask any data scientist what he spends most of his time doing and you’ll be told “data wrangling”, “munging”, “feature extraction” etc. we like the way that creating data pipelines is really natural in scala. because scala and spark are lazy, you can create your pipeline before optimising and executing it. and because everything is a function, you can compose filters and transformations easily. this turns out to be really fast and flexible. even better, you can then test your etl in a way that is impossible with sql. we tried sql a few times for etl, but once the queries were complicated it became impractical to review and a number of times we only came across logic problems late in development. initially we found etl into statically typed structures a pain, but the recent inclusion of dynamically-typed dataframes in spark has proved a useful half-way house, greatly easing the transformation into statically-typed, testable data structures. scaling to big data there is no point building toy models that don’t work on real world data . at barclays we don’t have the largest datasets in the world (terabytes not petabytes) but we still find scale is non-trivial problem. we have found that big data add-ons to small data applications tend to disappoint so we often code our own solutions directly in spark. one of the reasons for choosing scala was the close integration with spark so that the same code developed for “small data” can pretty much be run in big data by converting a list to an rdd. (you can do something similar for streaming). spark itself is not as mature as we would like, but scala has been great here. for example we were able to ensure that a spark big data function was correct by writing a logically identical scala small data function, and by running the same tests on both. try that in r or python. business logic often data science code is quite complicated and abstract. you want to do your analysis focussing on what you want to achieve, not how you are going to compute it . we like the directness of scala. there is little boilerplate and if you use sensible style guides it is very readable. this cuts down documentation and ensures that code can be shared easily as people are moved on and off the project. and this is because you can see what each function is doing because there are no loops, counters, state and other accounting stuff to keep in your head: you can just think about the business logic. machine learning i guess you might be thinking that these are all big data and software engineering considerations. what about analytics? isn’t this the elephant in the room? is scala any good at that? the straight answer is that we have faced some analytical challenges, but we have largely overcome them and are mostly comfortable where we have come out. scala does not have a strong set of analytical libraries compared to r and python etc. you can of course use java libraries, which work fine, but the scala libs breeze and mllib are immature. if you are able to aggregate or sample your data you can train the model using a python/r library, and this works fine. for big data there are few if any useful ml libraries, perhaps because you need to optimise carefully depending upon the domain. it turns out that our use cases are seldom of the “extract features and dump into random forests” variety, and we often have to develop our code directly anyway. in this case, scala has significant advantages over r and python because of the elegant way it represents and composes functions. there is no doubt however that our life would be easier if there was a coherent and performant ml framework for both scala & spark and this would be an area of focus for the data science community. interactive analytics another challenge is interactivity. interpreted languages like r and python are ideal for interactive analyses, and we do occasionally use them. the scala compiler is slow and the code runs in the shell, so you have to get pretty good at reading unformatted output on the command line. there is a pretty cool set of applications such as  ispark notebook (based on ipython notebook) which allow you to write code, execute it and inspect the results. these are not mature and the coding environment is not nearly as good as an ide (like intellij idea) but they are ok for trying out ideas. alternatively we have found the console of intellij to be quite useful to play with data, but the output is still less easy to read than in, say r. finally, production of charts, plots and diagrams is less well developed in scala than in python(matplotlib), r(ggplot) or javascript(d3 etc). we have tried scala-bokeh and wisp, both of which are fine and can be used from the spark notebook but anyone familiar with tableau would find it tough to use. conclusion we are pretty happy with our choice of scala and spark, though it may not be for everyone. it suits use cases that are big data or result in data science applications. it is not ideal for quick add-hoc analyses of small data. like comment copy linkedin facebook twitter share 69 11 comments oulharj abdlhadi dévelopeur web et mobile(ios, android) 9y report this comment great post harry like reply 1 reaction harry powell data science leader with track record of innovation and value creation 9y report this comment replying to jeremy alexander below: you very rarely need to use recursion. any tail recursive function can be implemented with a higher-order function like map, fold etc. the key to writing elegant functional code is to get your head around the higher order functions. like reply 1 reaction jeremy alexander product manager | tourradar 9y report this comment from a business point of view, scala is definitely my favourite language and i'm glad to hear there is yet another reason for larger companies to adopt it. 

what are your thoughts about loops? is being forced to used recursion as big as a disadvantage as i've been told? like reply 1 reaction kingsley davies architecture, identity, platforms and strategy. 
driving instructor @ sidecar, starter @ goldenpaths.io, founder @ goodtechconf. 
security cleared. busy technical adulting. 9y report this comment nice post harry! be good to catch up at some stage too (i run a meetup group for fp and finance, and sounds like there might be a great talk off that back of that post :-) !) like reply 1 reaction see more comments to view or add a comment, sign in more articles by harry powell why your single source of data truth project is failing (and will keep failing), and how to succeed in a different way feb 28, 2025 why your single source of data truth project is failing (and will keep failing), and how to succeed in a different way many organizations try to create a “single source of data truth.” they want a single place where you can see all… 23 5 comments how to get sponsorship for your ai project feb 14, 2025 how to get sponsorship for your ai project ai projects are likely to be complex, expensive and may impact a customer’s business model. we should expect that, to… 15 2 comments graph use-case archetypes may 5, 2023 graph use-case archetypes this note is to try to help you think about use cases for graph data analytics and machine learning in your… 35 1 comment driving sustainable growth in banks by connecting customer data using a graph database may 4, 2023 driving sustainable growth in banks by connecting customer data using a graph database growing a banking business requires you to make good decisions at each stage of the value cycle from acquiring new… 47 8 comments what questions should you ask of chat-gpt based analytics platforms? mar 31, 2023 what questions should you ask of chat-gpt based analytics platforms? you know the scenario. you are flooded with sales guys showing you amazing software applications based on some ai… 37 4 comments how to think differently jan 22, 2022 how to think differently last year, i wrote a series of micro-blogs about how to think differently about data science and analytics questions… 74 5 comments a business leader’s short guide to graph databases: what they are and why you need them. dec 19, 2021 a business leader’s short guide to graph databases: what they are and why you need them. the word “graph” is very fashionable in it circles right now, but graphs have actually been around for a while (graph… 107 11 comments a tribute to my indigital colleagues at jlr dec 17, 2021 a tribute to my indigital colleagues at jlr dear jlr indigital colleagues, in my final few days at jaguar land rover i have been thinking quite a lot about what we… 359 22 comments thinking differently: avoiding optimisation 1/2 nov 8, 2021 thinking differently: avoiding optimisation 1/2 a lot of business people and analysts are obsessed with optimisation; the best possible business strategy; a single… 38 10 comments bayesian a-b testing may 23, 2021 bayesian a-b testing … or how we were able to make decisions about price tests in half the time.. 64 5 comments show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://www.linkedin.com/pulse/map-reduce-file-compression-processing-cost-sunil-ranka,,,"map reduce: file compression and processing cost agree & join linkedin by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . skip to main content linkedin articles people learning jobs games join now sign in map reduce: file compression and processing cost report this article sunil s. ranka sunil s. ranka published aug 19, 2015 + follow recently while working with a customer we ran into an interesting situation concerning file compression and processing time. for a system like hadoop, file compression has been always a good way to save on space, especially when hadoop replicates the data multiple times. all hadoop compression algorithms exhibit a space/time trade-off: faster compression and decompression speeds usually come at the expense of space savings.  for more details about how compression is used, see https://documentation.altiscale.com/when-and-why-to-use-compression . there are many file compression formats, but below we only mention some of the commonly used compression methods in hadoop. the type of compression plays an important role -- the true power of mapreduce is realized when input can be split, and not all compression formats are splittable, resulting in an unexpected number of map tasks. in the case of splittable formats, the number of mappers will correspond to number of block-sized chunks into which the file has been stored, whereas in case of a non-splittable format a single map task will process all the blocks. although the table above shows that lzo is not splittable, in fact it is possible to index lzo files so that performance can be greatly improved. at altiscale, our experience has shown that by indexing lzo format files, it will make lzo-compressed files splittable and you can gain in performance. for more information, see https://documentation.altiscale.com/compressing-and-indexing-your-data-with-lzo on how to do it. the test results can be viewed  at https://sranka.wordpress.com/2015/08/19/map-reduce-file-compression-and-processing-cost/ conclusion in our test #1 scenario the uncompressed file size was 5.9 gb when stored in hdfs. with a hdfs block size of 256 mb, the file was stored as ~24 blocks, and a mapreduce job using this file as input created 24 input splits, each processed independently as input to a separate map task taking only 1 min 16 sec. in the rest of the test scenarios, due to gzip, the file could not be split, resulting in a single input split and taking an average time of approximately 11 min. even the gzip -1 option, meant for optimize speed, or  -9 option, meant for optimize space, did not  help much. gzip compression is an important aspect of the hadoop ecosystem; it helps save space at a trade off of processing time. if the data processing is time sensitive, then a splittable compression format, or even uncompressed files would be recommended. like comment copy linkedin facebook twitter share 21 1 comment manoj kumar building nuragi | world's first hybrid workforces orchestrator. built for results. 9y report this comment nice article and knowledge sharing like reply 1 reaction to view or add a comment, sign in more articles by sunil s. ranka five archetypes that will win in the age of ai jul 23, 2024 five archetypes that will win in the age of ai whether we’re a ceo trying to plan for the next decade or a front-line worker questioning our q4 job security… most of… 10 2 comments workforce of the future: a closer look at robotic process automation jan 22, 2019 workforce of the future: a closer look at robotic process automation we all have heard the word automation, and we have seen an automation wave crash through our personal and professional… 27 1 comment cloud allergy – clouds security and changing notion jun 30, 2016 cloud allergy – clouds security and changing notion with my recent role as cto/advisor with www.analytos. 18 big data – tez, mr, spark execution engine : performance comparison feb 25, 2016 big data – tez, mr, spark execution engine : performance comparison there is no question that massive data is being generated in greater volumes than ever before. along with the… 16 accessing hdfs files on local file system using mountablehdfs – fuse apr 9, 2015 accessing hdfs files on local file system using mountablehdfs – fuse hi all recently we had one requirement wherein we had to merge the files post map and reducer job. since the file… 9 1 comment need of reference architecture for big data analytic application apr 7, 2015 need of reference architecture for big data analytic application with big data and analytics playing an influential role helping organizations achieve a competitive advantage, it… 15 show more see all articles sign in stay updated on your professional world sign in by clicking continue to join or sign in, you agree to linkedin’s user agreement , privacy policy , and cookie policy . new to linkedin? join now explore topics sales marketing it services business administration hr management engineering soft skills see all linkedin © 2025 about accessibility user agreement privacy policy cookie policy copyright policy brand policy guest controls community guidelines العربية (arabic) বাংলা (bangla) čeština (czech) dansk (danish) deutsch (german) ελληνικά (greek) english (english) español (spanish) فارسی (persian) suomi (finnish) français (french) हिंदी (hindi) magyar (hungarian) bahasa indonesia (indonesian) italiano (italian) עברית (hebrew) 日本語 (japanese) 한국어 (korean) मराठी (marathi) bahasa malaysia (malay) nederlands (dutch) norsk (norwegian) ਪੰਜਾਬੀ (punjabi) polski (polish) português (portuguese) română (romanian) русский (russian) svenska (swedish) తెలుగు (telugu) ภาษาไทย (thai) tagalog (tagalog) türkçe (turkish) українська (ukrainian) tiếng việt (vietnamese) 简体中文 (chinese (simplified)) 正體中文 (chinese (traditional)) language",12
https://medium.com/@geekfrosty/pydeequ-testing-data-quality-at-scale-209b674a4259,,,"how to use pydeequ for testing data quality at scale | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story pydeequ — testing data quality at scale akashdeep gupta 11 min read · dec 24, 2023 -- share this blog post will cover the different components of pydeequ and how to use pydeequ to test data quality in depth. 💡all the code present in this post is present on my github here . ⚠️ currently (dec’23), pydeequ isn’t compatible with spark version > 3.3, but the community is working on it. all the details around it can be seen here . what is pydeequ? pydeequ is an open-source python wrapper around deequ (an open-source tool developed and used in amazon). deequ allows you to calculate data quality metrics on your dataset, define and verify data quality constraints, and be informed about changes in the data distribution. it’s built on top of apache spark so it can scale with large datasets (billions of rows). why testing data quality is important? the quality of data within a system can make or break your application. incorrect, missing, or malformed data can have a large impact on production systems. this bad data quality in the system can result in failures in production, unexpected output from ml models, wrong business decisions, and much more. environment and data preparation: -- -- written by akashdeep gupta 220 followers · 22 following i am a professional data engineer with over 10 years of experience. i write deep dives on data engineering tech. newsletter @ https://www.guptaakashdeep.com no responses yet help status about careers press blog privacy rules terms text to speech",12
https://anupamchand.medium.com/pyspark-data-frame-quality-validation-framework-in-databricks-using-great-expectations-hands-on-5fd0b070ea09,,,"pyspark data frame quality validation framework in databricks using great expectations(hands on) | by anupam chand | medium sitemap open in app sign up sign in medium logo write sign up sign in pyspark data frame quality validation framework in databricks using great expectations(hands on) anupam chand 7 min read · dec 21, 2023 -- 2 listen share we all know how important data quality for a data platform and data analysis. databricks in one of popular platforms used to conduct etl, transformations and analysis. today we are going to look a way on how to achieve data quality testing on databricks using great expectations. there are multiple articles on the internet but this article will try to show how to make it configuration friendly so that we quickly have a single framework which can be used for multiple pipelines using different configurations. i will also share the full code so you can literally fork the repo and tailor it to your use. great expectations is one of popular open source libraries used to test data quality of data. there are multiple inbuilt expectations that can be used which cover most of the needed checks. however it also has capacity of handling custom expectations but this article will not be focusing on these. we will be used delta tables to store our config. when using this in your pipeline, you can use a separate configuration pipeline to build ad populate the necessary delta tables. if you prefer to use a sql(like mssql or postgres) or nosql(cosmos or mongodb) data store, feel free to do so. for those of you in a hurry, you can dive into the code right away. github - anuthereaper/data_quality_checks_with_databricks: data quality checking frame work for… data quality checking frame work for pyspark in databricks based on great expectations framework - github … github.com first create a notebook and call it common_funcs.py. this will hold all common functions to be used each time you need to validate some data. for this example, i’ve added some of the popular built in functions but there are many more which you can get from the documenation. you will need to alter the below code to add those. once they are added and tested, they can be driven from config. !pip install great_expectations !pip  install simplejson import json class jsonfilereader: def __init__(self, filename): self.filename = filename def read(self): with open(self.filename) as f: return json.load(f) from abc import abc, abstractmethod class expectation(abc): def __init__(self, column, dimension, add_info = {}): self.column = column self.dimension = dimension self.add_info = add_info @abstractmethod def test(self, ge_df): pass class notnullexpectation(expectation): def __init__(self, column, dimension, add_info = {}): super().__init__(column, dimension, add_info) def test(self, ge_df): ge_df.expect_column_values_to_not_be_null(column=self.column, mostly=self.add_info[""mostly""], meta = {""dimension"": self.dimension}) class uniqueexpectation(expectation): def __init__(self, column, dimension, add_info = {}): super().__init__(column, dimension, add_info) def test(self, ge_df): ge_df.expect_column_values_to_be_unique(column=self.column, mostly=self.add_info[""mostly""], meta = {""dimension"": self.dimension}) class valuesinlistexpectation(expectation): def __init__(self, column, dimension, add_info = {}): super().__init__(column, dimension, add_info) def test(self, ge_df): ge_df.expect_column_values_to_be_in_set(column=self.column, value_set=self.add_info[""value_set""], mostly=self.add_info[""mostly""], meta = {""dimension"": self.dimension}) class valuesinbetweenexpectation(expectation): # work in progress def __init__(self, column, dimension, add_info = {}): super().__init__(column, dimension, add_info) def test(self, ge_df): ge_df.expect_column_values_to_be_between(column=self.column, min_value=self.add_info[""min_value""],max_value=self.add_info[""max_value""], mostly=self.add_info[""mostly""], meta = {""dimension"": self.dimension}) class valuestomatchregex(expectation): # work in progress def __init__(self, column, dimension, add_info = {}): super().__init__(column, dimension, add_info) def test(self, ge_df): ge_df.expect_column_values_to_match_regex(column=self.column, regex=self.add_info[""regex""], mostly=self.add_info[""mostly""], meta = {""dimension"": self.dimension}) class valuesintypelist(expectation): # work in progress def __init__(self, column, dimension, add_info = {}): super().__init__(column, dimension, add_info) def test(self, ge_df): ge_df.expect_column_values_to_be_in_type_list(column=self.column, type_list=self.add_info[""type_list""], mostly=self.add_info[""mostly""], meta = {""dimension"": self.dimension}) from great_expectations.dataset.sparkdf_dataset import sparkdfdataset class dataquality: def __init__(self, pyspark_df, config_path): self.pyspark_df = pyspark_df self.config_path = config_path def rule_mapping(self, dq_rule): return{""check_if_not_null"" : ""notnullexpectation"", ""check_if_unique"" : ""uniqueexpectation"", ""check_if_values_in_list"" : ""valuesinlistexpectation"",""check_if_values_inbetween"":""valuesinbetweenexpectation"", ""check_if_values_match_regex"":""valuestomatchregex"",""check_if_values_in_typelist"":""valuesintypelist""}[dq_rule] def _get_expectation(self): class_obj = globals()[self.rule_mapping()] return class_obj(self.extractor_args) def convert_to_ge_df(self): return sparkdfdataset(self.pyspark_df) def read_config(self): json_reader = jsonfilereader(self.config_path) return json_reader.read() def run_test(self): ge_df = self.convert_to_ge_df() config = self.read_config() #print(config) for column in config[""columns""]: if column[""dq_rule(s)""] is none: continue for dq_rule in column[""dq_rule(s)""]: expectation_obj = globals()[self.rule_mapping(dq_rule[""rule_name""])] expectation_instance = expectation_obj(column[""column_name""], dq_rule[""rule_dimension""], dq_rule[""add_info""]) expectation_instance.test(ge_df) dq_results = ge_df.validate() return dq_results # function to display a formatted summary of the results def create_df_from_dq_results(spark, dq_results): dq_data = [] for result in dq_results[""results""]: if result[""success""] == true: status = 'passed' else: status = 'failed' dq_data.append(( result[""expectation_config""][""kwargs""][""column""], result[""expectation_config""][""meta""][""dimension""], status, result[""expectation_config""][""expectation_type""], result[""result""][""unexpected_count""], result[""result""][""element_count""], result[""result""][""unexpected_percent""], float(100-result[""result""][""unexpected_percent""]), result[""expectation_config""][""kwargs""][""mostly""])) dq_columns = [""column"", ""dimension"", ""status"", ""expectation_type"", ""unexpected_count"", ""element_count"", ""unexpected_percent"", ""expected_percent"",""mostly""] dq_df = spark.createdataframe(data=dq_data,schema=dq_columns) return dq_df,dq_results[""success""] # this will create a json file required for great expectations from timeit import default_timer as timer import simplejson as json from pyspark.sql.functions import col def format_json_file(interface_id,outfile): t0 = timer() config_mstr_df = spark.sql(f""select * from ge_config_table where interface_id = \""{interface_id}\"""") #distinct spark.conf.set(""spark.sql.execution.arrow.enabled"", ""true"") config_mstr_cols_df = config_mstr_df.dropduplicates([""interface_id"",""column_name""]).select(col(""interface_id""),col(""column_name"")) config_json_org = {} config_json_org[""data_product_name""] = interface_id columns = [] for row in config_mstr_cols_df.rdd.collect(): interface_id = row['interface_id'] colm_name = row['column_name'] #print(interface_id, colm_name) config1 = {} config1[""column_name""] = colm_name rules = [] col_name = ""'"" + colm_name + ""'"" config_mstr_df_i = config_mstr_df.filter((config_mstr_df.column_name == colm_name) & (config_mstr_df.interface_id == interface_id)) for row2 in config_mstr_df_i.rdd.collect(): config2 = {} config2[""rule_name""] = row2[""rule_name""] config2[""rule_dimension""] = row2[""rule_dimension""] add_info = {} if row2[""mostly""] is none: add_info[""mostly""] = 1.0 else: add_info[""mostly""] = row2[""mostly""] if row2[""value_set""] != """": value_set = row2[""value_set""].split('|') if config2[""rule_name""] == ""check_if_values_inbetween"": add_info[""min_value""] = int(value_set[0]) add_info[""max_value""] = int(value_set[1]) else: if config2[""rule_name""] == ""check_if_values_in_list"": add_info[""value_set""] = value_set config2[""add_info""] = add_info rules.append(config2) config1[""dq_rule(s)""] = rules columns.append(config1) config_json_org[""columns""] = columns with open(outfile, ""w"") as f: json.dump(config_json_org, f) we will use the below sql to create a configuration table and insert some dummy config into it. you can assume “org_001” to be some kind of interface indicator so the framework knows which data validations to apply. so you can have different set of validations for different interfaces. i would like to check the following : “roll_col” is not null and must be unique. the data should meet this check 100%, meaning that even if a single data fails this test, we should show that test as a failure and the overall test is a failure as well. “name” column should not be null. the data should meet this check 100% “subject” should have values within the list [maths,science,english,hindi,sanskrit]. a pass rate of 70% is deemed successful. anything lower is deemed a failure. “marks” column should be between 10 and 100 and again this should be a 100% pass rate. create or replace table ge_config_table (interface_id varchar(20), column_name varchar(50) , rule_name varchar(50), rule_dimension varchar(50), value_set varchar(50), mostly decimal(2,1)); insert into ge_config_table values ('org_001', 'roll_no', 'check_if_not_null', 'completeness','', 1.0), ('org_001', 'roll_no', 'check_if_unique', 'uniqueness','', 1.0), ('org_001', 'name', 'check_if_not_null', 'completeness','', 1.0 ), ('org_001', 'subject', 'check_if_values_in_list', 'validity','maths|science|english|hindi|sanskrit',0.7 ), ('org_001', 'marks', 'check_if_values_inbetween', 'inbetween','10|100',1.0 ); finally we come to our actual data validation code. data_validation_with_config.py. this notebook accepts 2 parameters interface_id → the interface which you are executing the checks for. we will pass org_001 to retrieve the config which we stored in our delta table earlier. output_json_filename → this will be the file name with which the json will be stored. this should have some kind of unique name which includes the interface id and the timestamp so we can refer back to check exactly what validations were done. we will set this to ge_json.json for our example. you can pass these in via a datafactory pipeline or workflow or restapi call (however you plan to orchestrate the notebook). %run ./common_funcs dbutils.widgets.text(""interface_id"",""org_001"") dbutils.widgets.text(""output_json_filename"",""ge_json.json"") interface_id = dbutils.widgets.get(""interface_id"") outfile = dbutils.widgets.get(""output_json_filename"") interface_id = ""'"" + interface_id + ""'"" # create a formatted json file containing the test suite to be used as input to the great expectation tests format_json_file(interface_id,outfile) # create sample data frame and execute tests student_df = spark.createdataframe([ (1,""ram"",""maths"",50), (2,""shyam"",""history"",45), (3,""mohan"", ""science"",29), (4,""sohan"", ""maths"",60), (5,""rohini"", ""science"",32), (6,""raj"", ""maths"",10), (7,""meena"", ""hindi"",5), (8,""rani"", ""sanskrit"",20)], [""roll_no"", ""name"", ""subject"",""marks""]) dq = dataquality(student_df, outfile) dq_results = dq.run_test() dq_df,test_outcome = create_df_from_dq_results(spark, dq_results) print(f""overall test outcome pass?: {test_outcome}"") print(""summary of results"") dq_df.show() # show complete detailed results print(dq_results) # fails the notebook if the overall testing outcome was false (failed) if test_outcome == false: raise exception(""data quality checks failed"") when we execute this set of code (please note : i’m using dbr 13.3) , we get the following summary output. we can see the marks check failed. why? because meena had a marks of 5. we can also see that the subject check passed even though we have 1 student “shyam” who studied history. since the pass rate was 87.5% which was greater than the 70% we had setup, this test has been marked as “passed”. since one of the tests was failed, the overall test outcome is marked as “false”. let’s tweak the dataframe a bit and see the results. student_df = spark.createdataframe([ (1,""ram"",""maths"",50), (2,""shyam"",""history"",45), (3,""mohan"", ""science"",29), (4,""sohan"", ""maths"",60), (5,""rohini"", ""science"",32), (6,""raj"", ""maths"",10), (7,""meena"", ""hindi"",15), (8,""rani"", ""sanskrit"",20)], [""roll_no"", ""name"", ""subject"",""marks""]) dq = dataquality(student_df, outfile) dq_results = dq.run_test() dq_df,test_outcome = create_df_from_dq_results(spark, dq_results) print(f""overall test outcome pass?: {test_outcome}"") print(""summary of results"") dq_df.show() now meena has a score of 15. so we shoud have a pass for that particular test as well as the overall test. running the cell, we get the following summary. so now, all the tests are passed and the overall test outcome has been set to true. to show the detailed results, we are printing the results using : print(dq_results) we can either let these results remain in the output for future reference or better still, we can write these results to storage so it can be kept for an even long period and referred to for any debugging or checks. hope this was helpful. if you have any questions please feel free to put it in the comments. if you would like to add to the existing repo with change please let me know the changes in the comments. if you liked this article and it helped you, please add a clap just a “good job” in the comments. special thanks to pallavi sinha for her article containing her code base. i based my code on hers. databricks great expectations pyspark data validation data quality -- -- 2 written by anupam chand 94 followers · 10 following it solution architect with an interest in cloud computing and anything geeky and techy. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@lgsoliveira/polars-vs-pyspark-lazy-evaluation-and-big-data-fbc933cc11af,,,"polars vs pyspark: lazy evaluation and big data | by luís oliveira | level up coding sitemap open in app sign up sign in medium logo write sign up sign in level up coding · follow publication coding tutorials and news. the developer homepage gitconnected.com && skilled.dev && levelup.dev follow publication polars vs pyspark: lazy evaluation and big data luís oliveira 6 min read · may 10, 2023 -- 2 listen share the real tests!!!! source to fast process of data in python you may use several libraries however apache spark and polars are excellent choices for it. polars is a rust-based data processing library that provides a dataframe api similar to pandas (but faster). it is designed to handle large data sets efficiently, thanks to its use of multi-threading and simd optimization. pyspark , on the other hand, is a python-based data processing framework that provides a distributed computing engine based on apache spark . i recently published one article comparing the time performance between polars with pyspark polars vs pyspark: testing with middle size data checking execution time levelup.gitconnected.com if you saw my latest article, updates i made, and all the comments (including from the author of polars ritchie vink … 😮) you may have noticed that i was: not doing tests to “big data” but middle-size data (1.2 gb) and testing polars with eager transformation and therefore way slower. then i decided to change my polars code because both spark and polars have optimizers using lazy evaluation. in this article, i will show you: new polars code to allow lazy evaluation; new time results with the 1.2 gb parquet file; new test results with a folder only with parquet files and with a total of 6.2 gb. 1. new polars code to allow lazy evaluation on polar i had to make some changes. namely, on the extraction part i had to extract with a scan_parquet() that will create a lazyframe based on the parquet file. def pl_read_parquet(path, ): """""" converting parquet file into polars dataframe """""" df= pl.scan_parquet(path,) return df then, on the transformation process, i had to make some changes to allow the transformation of lazyframes. def transformation(df_trips, df_zone): df_trips= mean_test_speed_pl(df_trips, ) df = df_trips.join(df_zone,how=""inner"", left_on=""pulocationid"", right_on=""locationid"",) df = df.select([""borough"",""zone"",""trip_distance"",""passenger_count""]) df = get_queens_test_speed_pd(df) df = round_column(df, ""passenger_count"",0) df = round_column(df, ""trip_distance"",2) df = rename_column(df, ""passenger_count"",""mean_passenger_count"") df = rename_column(df, ""trip_distance"",""mean_trip_distance"") df = sort_by_columns_desc(df, ""mean_trip_distance"") return df  # return lazy dataframe def rename_column(df, column_old, column_new): """""" renaming columns """""" df = df.rename({column_old: column_new}) return df def mean_test_speed_pl(df_pl,): """""" getting mean per pulocationid """""" df_pl = df_pl.groupby('pulocationid').agg(pl.col([""trip_distance"", ""passenger_count""]).mean()) return df_pl def sort_by_columns_desc(df, column): """""" sort by column """""" df = df.sort(column, descending=true) return df def round_column(df, column,to_round): """""" round numbers on columns """""" df = df.with_columns(pl.col(column).round(to_round)) return df def get_queens_test_speed_pd(df_pl): """""" only getting borough in queens """""" df_pl = df_pl.filter(pl.col(""borough"")=='queens') return df_pl finally, on the load process, i had to add the collect() function to retrieve all the elements of the dataset and then write in a new parquet. def loading_into_parquet(df_pl): """""" save dataframe in parquet """""" df_pl.collect(streaming=true).write_parquet(f'yellow_tripdata_pl.parquet') the full code is available here: polars_lazy_performance.py if you are enjoying this article please don’t forget to clap or comment to support me!!!! please 😊 2. results 2.1 new results for middle-size data after the changes, i made a new run with lazy evaluation. wow!!!!! 😮😮 now polars worked as expected with the lazy evaluation. in the table below you can see the results for the five tests and average result. now you can see that polars took an average of 4.97 sec (!!!) to do the etl of 1.2 gb parquet file. let’s redo the pyspark tests because, since i am using my laptop, the available memory can change according to the software i am currently using. pyspark with the current status of my laptop pyspark is way faster but… for 1.2 gb it is still slower than polars!! i set one test of polars in the middle of the pyspark tests so you can see i am using the current status for both libraries. now, with the optimizer, polars win! 🙂 for 1.2 gb parquet file: lazy evaluation polars: 4.97 sec pyspark : 14.26 sec as you may see in the pyspark result we have ~5 seconds on the extraction process and that is weird because we are doing a lazy evaluation but… this time may be the time to start the java virtual machine. check the article below: how does pyspark work? — step by step (with pictures) do you find yourself talking about spark without really understanding all the words you’re using? do you feel like you… medium.com so… if we set the tests to bigger size data this “start time” would be relatively smaller? if you are enjoying this article please don’t forget to clap or comment to support me!!!! please 😊 2.2. results with bigger size data if you saw my previous article then you know that i build a 1,2 gb parquet file based on yellow taxi trip data from nyc for all the years 2019, 2020, and 2021. it was “dumb” from my side to create a single file because i could read a full path with all the parquet files… 😕 for the new tests, i created a folder and then inserted all the files from 2015 to 2021 in a total of 6.2 gb. i made some tests with polars and pyspark to see if now we have the “big data” effect. polars result of polars for 6.2 gb data ok! now we are starting to see the limitation of my laptop memory or the limitation of running polars locally. even so… running in lazy evaluation with data around 6.2 gb (!!!) it has a performance ~40 % better than eager evaluation with 1.2 gb. use the optimizer! let us test pyspark now. pyspark result of pyspark for 6.2 gb data ok! now we are talking! for bigger data, spark optimizer looks to work better than the polars optimizer. i probably get out of memory with polars and that is not a blocker in spark. again, i set one polars run in the middle so you can see i am using the same resources final result for 6.2 gb then spark takes longer to start but then (for big data) it is faster! i want to see biggeeeeeerrrr ok, let’s try to push my laptop a little bit: 8.23 gb ! ohhh!! 😥 polars was unable to handle this amount of data! on the other hand, pyspark only got 2 seconds slower with more than 2 gb !!! 😎 i feel great potential in polars 😎 but spark is still the big data tool! other resources comparing pandas, polars, and spark also in terms of memory usage pandas, spark, and polars — when to use which? betterprogramming.pub dataset for polars and spark part 4. handling large datasets with polars and spark: creating a 100 million row fake dataset in… a powerful combination for efficient data processing and manipulation medium.com polar vs pandas pandas vs polars vs pandas 2.0 …. fight testing an etl process levelup.gitconnected.com pandas vs polars vs pandas 2.0 … round 2 an article developed after some ideas from the first article levelup.gitconnected.com from pandas to polars - how to extract, transform, and load in python cheatsheet for polars levelup.gitconnected.com read every story from luís oliveira (and thousands of other writers on medium) for a low price !!! spark polars python data engineering data science -- -- 2 follow published in level up coding 250k followers · last published 1 day ago coding tutorials and news. the developer homepage gitconnected.com && skilled.dev && levelup.dev follow written by luís oliveira 1.3k followers · 193 following with me you will learn about data engineering, analytics engineering, and data career tip. see more in https://newbie2proficient.substack.com responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@bewin4u/fuzzy-matching-for-million-row-address-dataset-with-rapidfuzz-and-splink-b704eaf1fda9,,,"fuzzy matching for million row address dataset (large dataset) with rapidfuzz and splink | by michael david robinston | medium sitemap open in app sign up sign in medium logo write sign up sign in fuzzymatch at scale fuzzy matching for million row address dataset (large dataset) with rapidfuzz and splink michael david robinston 7 min read · aug 20, 2022 -- 1 listen share hello, medium readers ! welcome to my article in medium. without much fanfare and bla bla bla i’ll hit the bulls eye. have you ever had a huge dataset (don’t ask me how much kg !) without proper primary keys or identifers, where you have to map this particular column to another dataset or another column which has related data but arranged without order..? yes i can hear you tell we have the famous fuzzywuzzy library, fuzzymatcher and the corresponding ratios associated with it in python for matching anything. (if you wanna have a quick read, click here https://towardsdatascience.com/string-matching-with-fuzzywuzzy-e982c61f8a84 ) or copy paste this link. fuzzy wuzzy is a great invention in data science history and the efficacy is also impeccable. but wait.. then why am i writing this article..? yes. the reason is, when it comes to things at scale (say you have a 100k rows in your dataset) fuzzy wuzzy is not at all a good option, unless you want to sit before your computer for days. so here’s the immediate alternative for that. yes. rapidfuzz by maxbachmann solves this problem to a greater extent. i know you are yelling at me ‘man, give us the code and show some demo !’. but bear with me for two mins. the main advantage of this library is, it doesn’t have to translate everything from python to c++ each time you run the code + 16x faster than the normal fuzzywuzzy library. our dataset so here i’m gonna do a totally simple code and approach for my fictitious address dataset (that’s a disclaimer. the code could be improved for efficiency and i have just used as a test case. but simple is always the best :) test case 1 — rapidfuzz in a small dataset i’m using my colab as usual for this test case. we start with the installation first as follows: installing rapidfuzz with adding -q you won’t see each and every line of the installation process in colab and the visual space is optimum (no need for fuzzymatcher here. we need only rapidfuzz). first i’m gonna demonstrate the efficiency of the library using a 4 row dataset as follows : rapidfuzz in a small dataset note that, while using this library two things are very important. a) the passed input should be of type string. b) there should be no ‘null’ values. if null is there, map everything to a string and it’ll solve the problem mostly. now you may ask me a question, ‘why do you use a choices_dict here and do index mapping for the list items’. the answer is simple. when i use a ratio to compare a cell in the first dataframe to the other in the next dataframe it does the job perfectly. but how do you know what’s the row number matched..? and in future if you have to make a join between both the datasets based on the match, how would you do that? that’s why we need the index of the matching row from the second dataset here. you’ll understand my logic from the image below : result df with index position can you see in row 1 in second column the third object is an index position of the row in the second dataset. now you’ll have another question. ‘man you told that this is a million row mapping use case and you are doing a demo with a baby dataset of 4 rows…?’. patience my friend. i’ll take you there now. test case 2 — using rapidfuzz on a large dataset so here we have two huge datasets. df1 has one column of addresses and df2 has another column of addresses called ‘crashlocation2’ and both the datasets are of shape 758,757 rows * 1 column (so to check for match from df1 in df2 for each row my code has to run 758757 times. so overall 758757 * 758757 times) and df2[‘crashlocation2’] is nothing but a sample from df1 and hence i have collapsed the row position to make it difficult for rapidfuzz to match, purposely for this use case. note that i have done the proper preprocessing of mapping the whole column to strings and checking for null values too (it took me 5 hours to learn this lesson after the code failed again and again. so i’m saving your time here. buy me a coffee .. lol) df1[‘address’] df2[‘crashlocation2’] i have made it more tough for rapidfuzz purposely by mixing many things in the first dataframe (df1) to get that fictitious address column. now, lets jump into the pool. before jumping, make sure you run this in your colab console (press ctrl + shift + i) and you’ll get the following screen : in filter space — enter the following code → function clickconnect(){ console.log(“working”); document.queryselector(“colab-toolbar-button#connect”).click() } setinterval(clickconnect,60000) and then hit enter. why we need to do this ? the run time of colab is 90 mins and it’ll get disconnected after that automatically ( unless you wanna sit near your screen and keep on clicking for every minute like mr bean..lol) . so you can run this first and then our rapidfuzz, go to gym or shopping and when you are back you have you million dollar dataset..oops million row dataset ready with the matched pair. (this snippet will keep the run time connected by creating new sections once in a while in colab, so don’t blame me if you have to delete 100s of new sections after the job is done). our code for this dataset rapidfuzz for large datasets step 1 our results from rapidfuzz for large dataset bingo. i’m back from gym after 2 hours and this is what i’ve got. can you see that the library has done a good job within 6347 seconds which is almost 105 mins and that’s remarkable for a dataset of 758,757 rows (around a million rows). now the results anyway aren’t that good because i told you that i purposely made it more complex by mixing different things here and there (while you see in test case 1 the results are exactly what i wanted). moreover it depends on what ratio you use. either a partial ratio or token sort or token set, make sure to use in a small test case as we did in case 1 and then select your best match. now before we gloat don’t forget to run this in your colab console again ctrl + shift + i and then in console stopclickconnect(); hit enter. otherwise your laptop will be working forever creating new sections in your notebook until google finds and busts you (lol) improvements → there are many improvements which could be made on top of this. for example there is a cdist function in rapidfuzz which calculates the results in arrays and much faster than the methods used here and the extractone used in our case is also much efficient in terms of computation speed when compared with normal fuzzy wuzzy. also you can do some tinkering to my function used here for more optimization. wait.. i’m not done yet. read the next paragraph and then bid adieu. fuzzy match for very large datasets of one million plus rows of records — splink by robin linacre the author of this library called splink claims that it can be used to match millions of records in minutes using probabilistic record linkage algorithms from the splink library. his article also claims that it can be used at backend with big data workstreams like apache spark etc and from his article the results look promising. (i haven’t tested this yet and you are free to play with splink and let me know the results). so if you have a really big data problem with greater than a million records, splink might be an option. to know more about splink, click here : fuzzy matching and deduplicating hundreds of millions of records with splink originally posted: 2022-08-04. view source code for this page here. a fast, accurate and scalable record linkage… www.robinlinacre.com to know more about rapidfuzz and maxbachmann, click here : github - maxbachmann/rapidfuzz: rapid fuzzy string matching in python using various string metrics rapid fuzzy string matching in python and c++ using the levenshtein distance rapidfuzz is a fast string matching… github.com alright. it’s time to bid good bye. thanks to maxbachmann and robin linacre for their wonderful inventions and thanks to you guys for your patience. let’s ufyzz tamch (fuzzy match lol .. :) my linkedin page www.linkedin.com/in/micdavid4u the author of this article (i am writing about myself here) is a product owner — heading a revenue assurance squad (building automated tools with ai) — spark new zealand, new zealand’s largest telecom operator and an alumnus of university of canterbury. fuzzy matching fuzzy match big data fuzzy search rapidfuzz -- -- 1 written by michael david robinston 11 followers · 2 following i lead and help a team build meaningful and automated solutions with data (and sometimes out of box :) ) responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://miyake-akio.medium.com/big-data-with-rust-part-1-3-yes-we-can-fd396410e35,,,"big data with rust part 1/3 — yes we can!!! | by diogo miyake | medium sitemap open in app sign up sign in medium logo write sign up sign in big data with rust part 1/3 — yes we can!!! diogo miyake 10 min read · oct 10, 2022 -- 1 listen share a series of post to show rust frameworks that allows we work in big data area. in total there are 3 posts: this with introduction and part 1/3 using datafusion , part 2/3 showing polars and part 3/3 with fluvio. photo by cookie the pom on unsplash after a few months without time to write everything, i decide to write some things about tests that i am doing using rust. purpose of this article the purpose that i wrote this article is to share my little experience with these frameworks for big data and show that rust can be a light alternative to another computational libraries or frameworks for big data computing … after all, moore’s law is there to guide us about the evolution of hardware, and not everything is solved by increasing computational power. other parts of this series big data with rust part 2/3 — yes we can!!! a series of posts to show rust frameworks that allows we work in big data area. in total there are 3 posts: the… miyake-akio.medium.com big data with rust part 3/3 — yes we can!!! a series of posts to show rust frameworks that allows we work in big data area. in total there are 3 posts: the… miyake-akio.medium.com why rust? first i love studying and test new technologies, second rust is increasing the number of users, is safety, and a good language to know, you can know more about rust here: what is rust and why is it so popular? rust has been stack overflow's most loved language for four years in a row, indicating that many of those who have had… stackoverflow.blog in rust we trust: microsoft azure cto shuns c and c++ updated microsoft azure cto mark russinovich has had it with c and c++, time-tested programming languages commonly used… www.theregister.com linus torvalds: rust will go into linux 6.1 the rust in linux debate is over. the implementation has begun. in an email conversation, linux's creator linus… www.zdnet.com stack overflow: rust remains most loved, but clojure pays the best the 2022 results of the annual stack overflow developer survey are in! over 73,000 developers from over 180 countries … thenewstack.io my big data context i have 4+ years of experience with data being 3+ in big data and today i work with data architecture, data pipelines, data security and cloud computing technologies, and when i discover rust i think.. this is great why i don't try it in big data context? and after some time, i be here. frameworks for big data processing today exists a lot of framework for big data processing, i will cite common and most used today: spark : apache spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters. kafka : apache kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. flink : apache flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams. flink has been designed to run in all common cluster environments, perform computations at in-memory speed and at any scale. beam : apache beam is an open-source, unified programming model for batch and streaming data processing pipelines that simplifies large-scale data processing dynamics. thousands of organizations around the world choose apache beam due to its unique data processing features, proven scale, and powerful yet extensible capabilities. cloud options : have a lot of cloud options that uses in core some of above frameworks example: aws emr cluster uses spark or another tool, databricks uses spark, gcp dataflow uses apache beam. etc.. ps. : datafusion at this moment is only for batch processing in another parts as described above i will show another for streaming. ps2.: some references are got from projects documentation. what is datafusion ? datafusion is an extensible query execution framework, written in rust, that uses apache arrow as its in-memory format. datafusion supports both an sql and a dataframe api for building logical query plans as well as a query optimizer and execution engine capable of parallel execution against partitioned data sources (csv and parquet) using threads. arrow apache arrow is a project that some tools like spark, parquet, polars, dremio, dask and pandas uses it. for another use cases see this page . why datafusion? ps.: infos from main page… high performance : leveraging rust and arrow’s memory model, datafusion achieves very high performance easy to connect : being part of the apache arrow ecosystem (arrow, parquet and flight), datafusion works well with the rest of the big data ecosystem easy to embed : allowing extension at almost any point in its design, datafusion can be tailored for your specific use case high quality : extensively tested, both by itself and with the rest of the arrow ecosystem, datafusion can be used as the foundation for production systems. come on and get start with code… https://media.giphy.com/media/l1j9nrpoes7i54xnw/giphy.gif first: all of code are in this repository in gitlab: https://gitlab.com/miyake-diogo/rust-big-data-playground second: install rust third: clone repo or make tou new code with cargo new <any name of your project` fourth: see dataset details here: https://www.stats.govt.nz/large-datasets/csv-files-for-download/ info about datasets: https://www.stats.govt.nz/information-releases/statistical-area-1-dataset-for-2018-census-updated-march-2020 click on this link and download file after extract and renamed files are look like this: after download add on folder like structure below: data └── minilake ├── curated │   ├── aggregated_tables │   │   ├── part-0.parquet │   │   ├── part-1.parquet │   │   ├── part-2.parquet │   │   ├── part-3.parquet │   │   ├── part-4.parquet │   │   ├── part-5.parquet │   │   ├── part-6.parquet │   │   ├── part-7.parquet │   │   ├── part-8.parquet │   │   └── part-9.parquet │   └── final_obt │       ├── part-0.parquet │       ├── part-1.parquet │       ├── part-2.parquet │       ├── part-3.parquet │       ├── part-4.parquet │       ├── part-5.parquet │       ├── part-6.parquet │       ├── part-7.parquet │       ├── part-8.parquet │       └── part-9.parquet ├── raw │   ├── age_census │   │   └── dimenlookupage8277.csv │   ├── area_census │   │   └── dimenlookuparea8277.csv │   ├── covid │   │   └── owid-covid-data.csv │   ├── ethnic_census │   │   └── dimenlookupethnic8277.csv │   ├── fato_census │   │   └── data8277.csv │   ├── sex_census │   │   └── dimenlookupsex8277.csv │   └── year_census │       └── dimenlookupyear8277.csv └── stage ├── age_census │   └── part-0.parquet ├── area_census │   └── part-0.parquet ├── covid │   └── part-0.parquet ├── ethnic_census │   └── part-0.parquet ├── fato_census │   └── part-0.parquet ├── sex_census │   └── part-0.parquet └── year_census └── part-0.parquet below image of folders: don't forget download data and create folder structure let's start adding on cargo.toml dependencies: run cargo build to install libraries. **note:** if you have some problems with parquet or arrow you need to update rustup. overview of commands first create a context you can create session context to run your processing queries or methods. let ctx: sessioncontext = sessioncontext::new(); read a csv to read csv we can use the 2 different methods: dataframe api // read csv to dataframe let df = ctx.read_csv(“file.csv”, csvreadoptions::new()).await?; if you want to pass methods in csvreadoptions you can pass after `.new()` // read csv read csv to dataframe passing method delimiter to enum csvreadoptions let df = ctx.read_csv(“file.csv”, csvreadoptions::new().delimiter(b’;’)).await?; // execute and print results df.show_limit(5).await?; ok(()) sql api // register table from csv ctx.register_csv(“table_name”, “file.csv”, csvreadoptions::new()).await?; // create a plan to run a sql query let df = ctx.sql(“select * from table_name”).await?; let's show our data and schema to we show the data we can write code below: // execute and print results df.show().await?; // show entirely df to show n lines use show_limit ok(()) hint: note that show of data is likely apache spark.. some methods are like too. we have difference is in show method for show a determined number of rows you need use show_limit. save dataframe as parquet we can save dataframe as parquet, and method also are similar to spark. df.write_parquet(""folder_to_save/{}"",none).await?; transforming data go to transformation and make some exploratory of data to we select only columns that we need we can use select we can see more in this link . select let df = df.select(vec![col(""a""), col(""b"")])?; df.show_limit(5).await?; filter df.filter(col(""column_name"").eq(lit(0_i32)))?.show_limit(5).await?; distinct let df = df.distinct()?; df.show().await?; others: you can try a lot of another functions like union, union_distinct, sort, join, collect, aggregate, etc… i will show join and aggregate . case, when, then //use a auxiliar function let function = when(col(""column_name"").eq(lit(""stringtoget"")), lit(0_u32)).otherwise(col(""column_name""))?; let dataframe = dataframe.with_column(""column_name"",cast(function, int64))?; join let join_dataframe = left_dataframe.join(right_dataframe, jointype::inner, &[""col_a"", ""col_b""], &[""col_a2"", ""col_b2""], none)?; aggregate let agg_df = dataframe.aggregate( vec![col(""col1""),col(""col2""),col(""col3""),col(""col4""),col(""col5"")], vec![sum(col(""total_count""))])?; my pipeline for test i wrote a simple pipeline to get data on local folder and save intermediate like a data lake approach and made some transformations in data to test and check functionality of framework. i resume all of things that i do because in code have some info and i explain above some methods. pipeline first importing libs. after open a main function. creating of context and set of files path as variables. reading files as dataframe api. note that i use a loop to print schema and first 5 lines of dataframe. renaming columns and printing to check. saving files as staged area and after it i made a join and some aggregate, and to finish i save these files on local storage. conclusion i like to write some code for data in rust and i think datafusion is a good library but i found some problems: when i try to save large file in parquet the file is not saved and is corrupted i try parse and change column schema but when i try to save, file is not saved (i describe on issue with link below) i try filter also and the problem is same. i try to save parquet with partition and the same problem occurs. i needed to use a limit size (100k) to finish my tests and… yes i have success to join and aggregate dataframe (i said this because when i try it with all columns of dataframe i receive error of compiler) my concerns: write code in rust is love.. i don't think rust is a hard language to learn ( and i'm not a master blaster expert on software engineering) and datafusion is likely spark but with particularities of rust. although rust is awesome and datafusion is good, i think need more maturity to change your spark code to datafusion. i understand that is a project in ascension, the community of users are not greater than spark or pyspark and i hope that in the future we will have more projects in rust. issues i opened: write csv not save all lines of dataframe · issue #3783 · apache/arrow-datafusion describe the bug when i try to save dataframe as csv, only around 400k of lines are saved.. data has more than 1m of… github.com problem to get value in filter after change datatype of column · issue #3701 ·… describe the bug when i try to use filter method to get a specific value occur this error: *error… github.com [rust][datafusion] what causes ""error: execution(""file size of 4 is less than footer"")"" error? ·… when i try to read a csv and write as parquet, compiler raises this errror: error: execution(""file size of 4 is less… github.com thanks thanks for reading and feel free to reach me on linkedin . and in some weeks i write part 2 of this series. references all codes are in my repo: https://gitlab.com/miyake-diogo/rust-big-data-playground csv files for download | stats nz find csv files with the latest data from infoshare and our information releases. www.stats.govt.nz https://www3.stats.govt.nz/2018census/age-sex-by-ethnic-group-grouped-total-responses-census-usually-resident-population-counts-2006-2013-2018-censuses-rc-ta-sa2-dhb.zip?_ga=2.20958136.102282402.1663639578-985979153.1663098055 example usage - arrow datafusion documentation edit description arrow.apache.org datafusion - rust datafusion is an extensible query execution framework that uses apache arrow as its in-memory format. datafusion… docs.rs dataframe in datafusion::dataframe - rust pub struct dataframe { /* private fields */ } expand description dataframe represents a logical set of rows with the… docs.rs data engineering with rust and apache arrow datafusion 1/4 — introduction welcome to the introduction of my article series “data engineering with rust and apache arrow datafusion.” medium.com writing a data pipeline in rust with datafusion (vs pyspark) most of the data pipelines i have written so far are written in python: pandas or spark: pyspark. recently in one of my… towardsdev.com expr in datafusion::prelude - rust expand description expr is a central struct of datafusion's query api, and represent logical expressions such as a + 1… docs.rs rust big data data engineering technology data -- -- 1 written by diogo miyake 270 followers · 108 following big data platform security engineer with knowledge in architecture. enthusiast in technology, software, trying to help people to live better. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/towards-data-science/introducing-fugue-reducing-pyspark-developer-friction-a702230455de,,Test-Driven Development,"introducing fugue — reducing pyspark developer friction | by kevin kho | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. introducing fugue — reducing pyspark developer friction increase developer productivity and decrease costs for big data projects kevin kho 15 min read · feb 14, 2022 -- 2 listen share an initial version of this article was published on james le’s blog here . it has been updated to include new fugue features. photo by cesar carlevarino aragon on unsplash fugue’s motivation data practitioners often start out by working with pandas or sql. sooner or later, the size of data being processed outgrows what pandas can handle efficiently, and distributed compute becomes necessary. one such tool is spark, a popular distributed computing framework that enables processing large amounts of data in-memory on a cluster of machines. while the spark engine is very powerful in scaling data pipelines, there are many pitfalls that new users, and even experienced ones, face when using spark. the expected initial difficulty is having to learn an entirely new framework. the syntax and usage of spark and pandas are very different. users who migrate projects from pandas to spark often find themselves re-writing most of the code, even for the exact same application logic. even worse, some operations that are trivial to do in pandas become a lot harder in spark and take a while to implement. a simple example of this disparity is getting the median for each group of data. in pandas, there is no need to think twice about getting the median of each group. on spark though, the operation isn’t as straightforward. we compare the syntax of the two frameworks in the code snippet below: groupby-median in pandas versus spark this syntax disparity is because calculating the median is expensive in a distributed setting. all of the data belonging to one group needs to be moved to the same machine. as such, data needs to be shuffled and sorted before the median can be obtained. to reduce computation cost, an approximate median can be obtained with the specified tolerance. in the snippet above, 20 is the accuracy meaning the relative error could be 1/20, or 5%. specifying the tolerance allows users to balance accuracy and speed. beyond the syntax difference, there are important concepts in a distributed setting (such as partitioning, shuffling, persisting, and lazy evaluation) that pandas users are not aware of initially. these concepts take a significant amount of time to learn and master, making it hard to fully utilize the spark engine. fugue , an open-source abstraction layer, provides a seamless transition from a single machine to a distributed computing setting. with fugue, users can code their logic in native python, pandas, or sql, and then bring it to the spark (or dask) engine for execution. this means that users don’t even need to learn the spark syntax to use spark. image by author: fugue logo this article will discuss the pain points spark users face and how fugue addresses them. fugue is the direct result of several years of questioning how the spark developer experience can be improved. beyond providing an easier interface to coding in spark, there are more tangible benefits introduced by using an abstraction layer. here, we’ll show how fugue: handles inconsistent behavior between different compute frameworks (pandas, spark, and dask) allows reusability of code across pandas-sized and spark-sized data dramatically speeds up testing and lowers total project cost enables new users to be productive with spark much faster provides a sql interface capable of handling end-to-end workflows inconsistencies between pandas and spark can we have a unified interface for big and small data? pandas users transitioning to spark often encounter inconsistent behaviors. first off, pandas allows for mixed column types. this means strings and numbers can be mixed in the same column. in spark, the schema is strictly enforced, and mixed-type columns are not allowed. this is because pandas has the luxury of seeing all the data as it performs operations, while spark performs operations across several machines that hold different parts of data. this means spark can easily have different partitions behave differently if the schema is not strictly enforced. nulls are also handled differently by pandas and spark. the table below summarizes the default handling of null values null handling in pandas versus spark this is the first benefit of using fugue as an abstraction layer. getting pandas code to run on spark is one thing, but it’s a very tedious process to have the code give consistent results between the computation engines. in a lot of cases, extra code has to be written to get the same results. fugue takes care of the consistency to create a consistent bridge between pandas and spark. fugue was designed to be consistent with spark and sql because that guarantees code will work as expected in the distributed setting. users should not have to spend their time worrying about framework-specific behavior. decoupling of logic and execution why do i need to choose a framework before i start a data project? one of the pain points with using pandas and spark is that the logic is tightly coupled with the interface. this is impractical because it requires data practitioners to choose what they’ll code with at the project onset. here are two scenarios, which are two sides of the same problem. a user codes in pandas, and then the data becomes too big. to solve this, the underlying hardware has to be upgraded to support execution (vertical scaling). a user codes in spark expecting data to be big, but it never grows to the size that demands spark. the code and tests run slower than it has to because of the spark overhead. in both scenarios, the user ends up using the wrong tool for the job. these scenarios could be avoided if logic and execution are decoupled. using fugue as an abstraction layer allows users to write one code base that is compatible with both pandas and spark . execution can then be specified during runtime by passing an execution engine. to demonstrate this, let’s look at the easiest way to use fugue, the transform() function. for this example, we have a dataframe with columns id and value . we want to create a column called food by mapping value to the corresponding food in mapping . simple setup for our problem pandas has a simple method for this. we can create a pandas function that invokes it. using pandas to perform the map operation without editing the pandas function, we can bring it to spark using fugue’s transform() function. this function can take a pandas dataframe or spark dataframe, and it will return a spark dataframe if using the spark engine. using fugue transform to port a function to spark note we need to call .show() because spark evaluates lazily. the output is seen below. +---+-----+------+ | id|value|  food| +---+-----+------+ |  0|    a| apple| |  1|    b|banana| |  2|    c|carrot| +---+-----+------+ in this scenario, we did not have to edit the original pandas-based function. the transform() function took care of porting execution over to spark because we supplied a spark_session as the engine. if the engine is not specified, the default pandas-based execution engine is used. pandas users may not be used to defining schema explicitly, but it is a requirement for distributed computing. but actually, pandas will not always be the easiest way to express logic. thus, fugue also supports using native python functions by being flexible in handling different input and output types . below are three different implementations for our map_letter_to_food() function. all of them are compatible with the fugue transform() function and can be used on pandas, spark, and dask engines with the same syntax. different implementations of our function, all compatible with fugue notice all of the logic is defined in the map_letter_to_food() function. execution is then deferred to the transform() call where we specify the engine. users only need to be concerned with defining their logic in their preferred way. fugue will then do the work of bringing it to the specified execution engine. while spark provides pandas_udf as a way to execute pandas functions on spark, fugue provides a simpler interface around the schema. schema management ends up producing a lot of boilerplate code in spark . here, the schema is passed to transform() in a minimal string, leaving the original function definition untouched. also, fugue can use pandas_udf under the hood if a user specifies, and the overhead for using fugue in this scenario is less than one second as can be seen in this benchmark . on a more practical level, it is very common for data science teams to have shared libraries that contain specific business logic for cleaning and transforming data. currently, the logic has to be implemented twice — once for pandas-sized projects, and once again for spark-sized projects). by using fugue, the same function could be used on both pandas and spark engines without any code change. this also future-proofs the code . what if one day, you decide you want to use the dask engine? what if you wanted to use a ray engine? using fugue as an abstraction layer would let you migrate seamlessly, as it would just be a matter of specifying the execution engine during runtime. on the other hand, writing code using the spark api would automatically lock in the codebase to that framework. fugue’s minimalistic interface intentionally makes it easy to offboard if a user wants to. improving testability of spark how can we accelerate the development iterations and testing on big data projects? testing code in spark is tedious. there are currently two approaches that are used in developing spark applications. users on databricks may use the databricks-connect python library, which replaces the local installation of pyspark. whenever pyspark is called, the execution plan is compiled locally and then executed on the configured cluster. this means that simple tests and code changes require the backend cluster to be spun up . it takes a while and is also extremely expensive. the second approach is to develop locally and then use the spark-submit tool to package the code and run it on the cluster through ssh. this process takes a bit more work and is time-consuming. for teams doing test-driven development, the whole test suite can take a very long time to test. even if all the testing is done locally, spark is still slow to spin up compared to pandas because the jvm environment needs to be set up. assertions on values on dataframe operations require either a collect() or topandas() call, which would take a lot of time compared to pandas-based evaluation. because selecting an execution engine at runtime, we can use the pandas-based engine on smaller data during testing, and then use the spark engine for production. testing becomes faster and cheaper because code is decoupled from spark , meaning that the spark runtime does not have to be spun up for every little code test. after testing locally with pandas, the same code can be brought to the spark execution engine to scale. the consistency guaranteed by fugue ensures that running on the default engine and running on the spark execution engine provides the same results. having this separation dramatically speeds up development cycles , and makes big data projects significantly cheaper as expensive mistakes could be avoided. testing time is often reduced from minutes to seconds. users of fugue also benefit from having to write fewer tests. in our transform() example above, only the original function needs to be tested. users can also test transform() , but it has already been heavily tested on the fugue level. in comparison, using a pyspark approach will require 1 or 2 helper functions that then have to be tested also. an equivalent pyspark code snippet to transform() can be found here . reducing expensive mistakes how can we reduce the friction distributed computing beginners face? a lot of spark users are not aware that it’s very easy for data to be recomputed in spark. distributed computing frameworks lazily evaluate code, meaning the computation graph (or dag) is constructed and then executed when an action is performed to materialize a result. actions are operations like printing or saving the dataframe. in the computation graph below, b is recomputed for the actions that run c, d, and e. this means it is computed three times. if one run of b takes one hour, we unnecessarily add two hours to our workflow. image by author: sample dag experienced spark users will know that b should be persisted to avoid recomputation. however, people less familiar with lazy evaluation often suffer from unnecessary recomputation . in extreme situations, this lazy evaluation and recomputation can lead to unexpected behavior when operations are not deterministic. the clearest example would be a column with random numbers in b. the random number column will be recalculated with different results for c, d, and e if b was not persisted. to solve this, fugue also has optimizations at the workflow level. previously, we showed fugue’s transform() interface for a single function. fugue also has support for constructing full workflows by using fugueworkflow() as seen below. this is an engine-agnostic dag representation of an entire workflow. fugueworkflow() can take in an engine like the previously shown transform() function to port it to spark or dask. fugueworkflow example by analyzing the dependencies of the computation graph (dag) constructed, fugue can smartly persist dataframes that will be reused by multiple actions. for more control, fugue also provides an interface for users to persist the dataframes themselves. through this dag, fugue can also perform validations (such as schema and partitioning) that allow code to fail quickly. fugue will recognize if the schema is mismatched for future operations, and error out immediately. a lot of spark users often spend a lot of money and time running code on a cluster, only to find out hours later that it failed. having fugue’s dag compiling process helps users avoid expensive mistakes . sql interface how can sql be elevated to a first-class grammar for compute workflows? one of spark’s innovations is the sql interface in sparksql. the sparksql interface is great for allowing people who prefer sql to describe their computation logic. unfortunately, it does not let users take advantage of everything spark has to offer because it is tightly based on ansi sql. it is also a second-class interface, often invoked in-between predominantly python-based code. fugue has a sql interface based on sparksql’s implementation but with added enhancements. first, there are additional keywords like broadcast, persist, prepartition, and presort that allow users to explicitly take advantage of spark’s distributed computing operations. there is also support to use python functions with fuguesql through the transform keyword (among others). more keywords such as load and save have been added to support end-to-end workflows. below is the equivalent of our previous fugueworkflow . fuguesql example now a heavy sql user can load data, perform transformations, and save results all using fuguesql on top of the spark engine. sql lovers can express their end-to-end computation logic in a sql-like interface. one weakness is that ansi sql only allows one select statement, while fuguesql allows multiple. fuguesql allows variable assignments as temporary tables, which is a friendlier syntax than common table expressions (ctes). for more information, check the fuguesql docs . this fuguesql interface builds on top of the abstraction layer, making it compatible with pandas, spark, dask, and blazingsql. it is a first-class citizen that offers the same flexibility and benefits as the fugue python api . there is also a notebook extension with syntax highlighting that allows users to just invoke the %%fsql cell magic. for more information, see this article . note that the syntax highlighting is only available for the classic jupyter notebook at the moment, and not for jupyterlab. it also works well in the kaggle environment , taking advantage of the multiple cores in a kaggle kernel. image by author: demo of jupyter notebook extension for fugue partitioning are there better ways to partition data for certain use cases? spark uses hash partitions by default. for a small number of keys, this could easily lead to uneven partitions. this may not seem like a big deal, but if each key takes one hour to run, having uneven partitions could make a job take several more hours to run. the tricky thing is partitions on spark cannot be made even without writing a significant amount of code. fugue allows users to choose between the default hash partition, random partition, or an even partition. each of these partitioning strategies lends itself well to different use cases . below is a table summary of when to use each one. image by author: different partitioning strategies available with fugue even partitioning is particularly useful for smaller data that require large computations. when data is skewed, some partitions end up containing more data than others. execution time is then dependent on the completion time of the partition with the largest amount of data. by enforcing an equal number of elements for each partition, the execution time can be reduced. for more information, check the partition documentation . in the code below, we get the five rows that contain the highest values of col2 . the presort is applied as the data is partitioned. the transform() function can also take in a partition strategy. example of partition operation on fugue fugue vs koalas vs modin image by author: koalas, modin, and fugue fugue often gets compared with koalas and modin as a bridge between single-core computing to distributed computing. koalas is a pandas interface for spark, and modin is a pandas interface for dask and ray. it’s hard to compare the projects because the objectives are different, but the main difference is that these two frameworks believe pandas can be the grammar for distributed computing, while fugue believes native python and sql should be, but supports pandas usage as well . at the onset, switching to koalas or modin may seem a lot easier when coming from pandas. some users mistakenly expect that the pandas import statement can be changed, and the code will work perfectly on the distributed setting. in a lot of cases, this promise is too good to be true because this requires the interfaces of the libraries to be perfectly in sync with the pandas api, which is nearly impossible. for example, the koalas implementation of the rolling operation does not have the window types that the pandas api provides. but having complete parity with the pandas api does not always make sense in the distributed setting. for example, a transpose operation works in pandas but is very expensive when the data is spread on different machines. in extreme cases, the application has to make extreme compromises to get this import statement magic to work. if an operation doesn’t exist in the modin api, the architecture defaults to using pandas , which collects all of the data to a single machine. this can easily overload the machine collecting all the data that was previously spread across multiple workers. there are also philosophical reasons why fugue avoids using pandas as the grammar for distributed compute operations. koalas and modin add vocabulary to that grammar, such as persist and broadcast operations to control data movement between workers. but the misalignment here is that the base grammar of pandas does not translate well to distributed scenarios. the index is very core to pandas workflows. in a typical script, a lot of reset_index() and set_index() calls will be used. when performing groupby operations, the index is automatically set. the index preserves a global order, allowing for the iloc method to be used. some operations even use index in join conditions. in a distributed setting, order is not guaranteed, as it’s often unnecessarily computationally expensive to keep track of it. the performance-productivity tradeoff and fugue there is always a tradeoff between code performance and developer productivity. optimizing for performance requires deep engine-specific tricks that are hard to code and maintain. on the other hand, optimizing for developer productivity means churning out solutions as fast as possible without worrying about code performance. fugue sacrifices a bit of performance for significant increases in iteration speed and maintainability. by focusing on defining the logic on a partition level, users often find their code becomes clearer and big data problems become small and manageable. while using pandas and custom functions on spark used to be slower, it is getting more performant due to improvements on the spark engine (the use of apache arrow). the efficiency lost by fugue applying conversions is very minimal and users often see speedups in their code gained from more efficient handling of data in the distributed setting. in fact, fugue transcribes a lot of the code into spark code, meaning that the only thing changing is the interface in a lot of cases. conclusion in this article, we talked about the pain points of using spark, including testability, the inconsistencies with pandas, and the lack of a robust sql interface. we presented fugue as a friendlier interface to work with spark. fugue does not compete with the spark engine; fugue makes it easier to use. by using fugue, users often see quicker iterations of big data projects, reducing time-to-delivery and project cost. using fugue is non-invasive and free of any dependencies. logic can be defined in native python code or pandas, and then ported to spark. fugue believes in adapting to the user , so they can focus on defining their logic rather than worrying about its execution. though not covered in this article, fugue also provides ways to use native spark code or spark configurations. it does not restrict access to the underlying framework. contact us if you want to learn more about fugue, discuss your spark pain points, or even correct something wrong mentioned in this article, we’d love to hear from you! also, feel free to reach out if you want us to give a presentation to your team, meetup, or conference. email: hello@fugue.ai slack: join here resources additional resources for fugue: fugue tutorials fugue repo list of fugue conferences presentations (pycon, pydata, kubecon, etc.) there are a lot more specific applications opened by the abstraction layer. so far, we have presented validation , tuning , and the sql interface . spark pandas data science fugue data engineering -- -- 2 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by kevin kho 200 followers · 5 following working on fugue. prev. at prefect. https://github.com/fugue-project/fugue/ responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/selectfrom/building-your-first-dbt-project-on-databricks-3bfba2b95a63,,,"building your first dbt project on databricks | by mendelsohn neil chan | selectfrom sitemap open in app sign up sign in medium logo write sign up sign in selectfrom · follow publication a vocal community of enthusiastic developers. we speak all things data, code and engineering. follow publication building your first dbt project on databricks a hands-on tutorial complete with sample code snippets and screenshots to help you build, test, and deploy your first dbt project on databricks mendelsohn neil chan 7 min read · jul 14, 2022 -- 2 listen share source: getdbt.com why databricks and dbt? the winning combination of databricks and dbt dramatically simplifies your overall data architecture by providing a centralized and familiar sql-based environment for collaborative data transformation. built on top of open source technologies with widespread community support — dbt, delta lake, and apache spark — this ensures ongoing innovation and eliminates the risk of any vendor lock-in. goals and objectives in this blog post / tutorial, you will accomplish 5 things: install dbt and use it inside a python virtual environment create a dbt project and connect to a databricks sql warehouse develop dbt models to transform and shape your data analyze your results in the databricks sql ui run data quality validations to ensure data quality step 1: python virtual environment setup in this preliminary step, we will instantiate a python virtual environment . the purpose of a virtual environment is to ensure proper dependency management and project isolation. this helps reduce unexpected package version conflicts and code dependency collisions. 1.1: run the commands below to create an empty directory named dbt_project in the root of your home directory: mkdir ~/dbt_project cd ~/dbt_project 1.2: once you’re in this empty directory, create a file named pipfile using the code snippet below. pipfile is a dedicated file used by the python virtual environment to manage project and library dependencies ( note: if you use a different version of python, replace 3.9.12 with your version number. run the command, python --version to check your version ): [[source]] url = ""https://pypi.org/simple"" verify_ssl = true name = ""pypi"" [packages] dbt-databricks = ""*"" [requires] python_version = ""3.9.12"" 1.3: let’s now instantiate the python virtual environment and install all the packages defined in the pipfile containing the dbt databricks adapter package, dbt-databricks , which automatically installs dbt core and other library dependencies required: pipenv install instantiating the virtual environment 1.4: once the virtual environment is successfully created, activate the virtual environment ( note: notice how the terminal now displays (dbt_project) before the command prompt ) pipenv shell this is an indication that the virtual environment is running in a self-contained, isolated environment with all necessary packages installed inside 1.5: confirm that your virtual environment is running the expected versions of dbt and the dbt databricks adapter by running the command below: dbt — version step 2: connect to a databricks sql warehouse after the initial setup and configuration of the virtual environment, we will now create a dbt project, which is a collection of directories and artifacts that serve as the building blocks of using dbt. we will also connect to a databricks sql warehouse which will serve as the fundamental compute resource to run dbt models. 2.1: within the activated virtual environment, run the dbt init command followed by the name of your dbt project. this tutorial will use a project named my_first_dbt_project : dbt init my_first_dbt_project be prepared to enter the following when prompted: enter the number 1 to select databricks server hostname of your databricks sql warehouse http path personal access token default schema name ( this is where your tables and views will be stored in ) enter the number 4 when prompted for the number of threads 2.2: your configurations will be saved to a profiles.yml file. open this file now to examine and verify the connection settings to databricks: dbt debug --config-dir open /users/<your-username>/.dbt connection params in the profiles.yml file (personal access token value has been redacted) 2.3: to close out this step, verify that the connection details are correct. make sure that you are in the correct project directory (my_first_dbt_project) before running the command below: dbt debug screenshot indicating successful connectivity between dbt and databricks step 3: building dbt models we now arrive at one of the most important steps in this tutorial, where we finally create dbt models. in a nutshell, dbt models are select statements defined as .sql files, with the name of the file serving as the model’s name. one model within the context of dbt is conceptually equivalent to either a table or view in databricks sql. this tutorial uses visual studio code to create the dbt model scripts, but you may use any text editor that you prefer. model scripts can also be nested within subdirectories within the models directory 3.1: within the project’s model folder, create the first model named bronze_orders.sql with the sql statement below. we will be utilizing the medallion architecture data design pattern in this project, with the goal of incrementally improving the structure and quality of data as it flows through each layer of the architecture ( from bronze ⇒ silver ⇒ gold tables ): {{ config( materialized = “table”, file_format = “delta” ) }} {% for order_id in range(1, 101)%} select {{ order_id }} as order_id, current_timestamp() as order_date_time {% if not loop.last %} union all {% endif %} {% endfor %} if this is your first time creating a dbt model, let us deconstruct and understand the code block above: the config block defines how the model will be materialized. in the above example, our dbt model will be materialized as a managed delta table in this tutorial, we will be generating mock data simulating sales orders for a fictional company. we achieve this through the help of the jinja templating language, where we generate a for loop to generate orders with a unique id ranging from one (1) to one hundred (100) the syntax is similar to how you would loop over an iterable in python 3.2. in the same model directory, create a second and third file named silver_orders.sql and gold_orders.sql respectively. note that because we have omitted the config block in these two models, the default materialization strategy would be for dbt to create a view in the database based on these statements: with silver_orders as ( select order_id, date(order_date_time) as order_date, year(order_date_time as order_year, month(order_date_time) as order_month, day(order_date_time) as order_day_of_month from {{ref(‘bronze_orders’)}} ) select * from silver_orders with gold_orders as ( select count(order_id) as order_count from {{ref(‘silver_orders’)}} ) select * from gold_orders your project directory should look similar to the image above after building the three models 3.3: finally, it’s time to execute! inside the virtual environment, run the command dbt run .this step lets dbt connect to the target database we defined earlier ( dbt_project_db ) and executes the three .sql model files to materialize them as tables or views: dbt run the correct, sequential execution of the three models is achieved through the jinja function {{ ref }} , which allows dbt to run the bronze_orders model first, followed by silver_orders and gold_orders subsequently. 3.4: navigate to the databricks sql ui to validate that the three dbt models have been materialized correctly in the target database: step 4: run data validation tests in this step, we will create data quality tests to ensure that our data is clean and conforms to our expectations. there are two ways of defining tests in dbt: generic tests : there are four built-in tests in dbt that are expressed as yaml files (unique, not_null, relationships, accepted_values) singular tests : these are similar to models in that they are expressed as sql queries that assert a data quality expectation about your data. 4.1: inside the project’s model directory, create a file named schema.yml using the code block below. this is an example of a generic test: version: 2 models: - name: bronze_orders columns: - name: order_id tests: - unique - not_null - name: silver_orders columns: - name: order_id tests: - unique - not_null - relationships: to: ref('bronze_orders') field: order_id - name: gold_orders columns: - name: order_count tests: - accepted_values: values: [100] 4.2: inside the project’s test folder, create a file named dq_even_order_ids.sql with the following sql query. our goal here is to isolate and quarantine records where the order id is divisible by two . this is an example of a singular test: {{ config(store_failures = true) }} select * from {{ ref('silver_orders') }} where order_id % 2 = 0 4.3: within the virtual environment terminal, run the command dbt test . as expected, fifty records will fail our “divisible by two” test: dbt test 4.4: because we have set the config block to store records that failed the singular test named dq_even_order_ids , we can query it in a separate table for examination: step 5: managing environments (optional) one of the ways in which databricks and dbt implement software and data engineering best practices is through proper environment separation. this is done to enable engineers to develop and test code without impacting the users of their downstream data products. within the context of dbt and databricks, this can be achieved through the use of targets within the profiles.yml file which we configured in step 1. environment management through the use of targets within a profile to switch your connection to the prod environment for instance, add the -t flag signifying the target connection after the dbt debug command. this target flag is the key for how we can dynamically run in different environments: dbt debug -t prod conclusion congratulations! after completing the five steps above, you have just built your first dbt project on top of databricks. i hope you found this hands-on tutorial interesting and useful. databricks dbt analytics engineering data engineering data analytics -- -- 2 follow published in selectfrom 355 followers · last published sep 26, 2023 a vocal community of enthusiastic developers. we speak all things data, code and engineering. follow written by mendelsohn neil chan 94 followers · 51 following 📊 data engineering x technical presales ☕ certified third-wave coffee snob 📚 udemy course creator 👨‍💻 all views are my own. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/better-programming/how-to-integrate-great-expectations-with-databricks-e17740e2a97a,,,"how to integrate great expectations with databricks | steve russo | better programming sitemap open in app sign up sign in medium logo write sign up sign in better programming · advice for programmers. member-only story how to integrate great expectations with databricks get better data quality metrics with one change to great expectations steve russo 5 min read · jul 7, 2022 -- 3 share great expectations and databricks are a perfect match! image made the author. a common challenge data engineering teams face is how to best measure data quality. poor data quality leads to wrong insights and potentially bad business decisions. an integrated data quality framework reduces the team’s workload when assessing data quality issues. great expectations (ge) is a great python library for data quality. it comes with integrations for apache spark and dozens of preconfigured data expectations. databricks is a top-tier data platform built on spark. so you’d expect them to integrate seamlessly, but that is not quite the case. so in this article, i’ll walk through a simple change you can make to one ge class that allows for a more integrated solution between ge and databricks. all the code for this article is available in the repo here . the problem i was hoping for a simple way to integrate ge with databricks without switching between pyspark and configuration files. i’ve found that using ge in a hosted environment is challenging . ge does offer a step-by-step guide on ‘ how to use great expectations in databricks .’ if you follow the guide step-by-step, you end up with a mountain of… -- -- 3 published in better programming 221k followers · last published nov 10, 2023 advice for programmers. written by steve russo 940 followers · 38 following senior data engineer • ai/ml • https://www.linkedin.com/in/stevenjosephrusso/ responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@ttungl/big-data-with-pyspark-feature-engineering-12c7e2086fb1,,,"big data with pyspark: feature engineering | by tung thanh le | medium sitemap open in app sign up sign in medium logo write sign up sign in big data with pyspark: feature engineering tung thanh le 9 min read · mar 22, 2022 -- listen share notes: this is my note from the datacamp.com course with john hogue general notes: define the goals of the analysis: what types of problems? i.e. classification or regression or unsupervised learning or sth else. find independent and dependent features. understand the context and limitations of the data, i.e., seasons impact of a year, type of products, areas, etc. find if attributions are available in the datasets. validating the data load, such as count(), .columns, check data types .dtypes or .info() or .printschema, check min max std with .describe(); eda df.describe(['listprice']).show() df.cov('salescloseprice','yearbuilt') # sample pyspark dataframe before converting to pandas for viz. df.sample(false, 0.5, 42) # (withreplacement, fraction, seed) # plotting a distribution import seaborn as sns sample_df = df.select(['salescloseprice']).sample(false, 0.5, 42) pandas_df = sample_df.topandas() sns.distplot(pandas_df) # relationship plotting import seaborn as sns pandas2_df = df.select(['salescloseprice','sqftaboveground']).sample(false, 0.5, 42).topandas() sns.lmplot(x='sqftaboveground', y='salescloseprice', data=pandas2_df) # compute the skewness from pyspark.sql.functions import skewness print(df.agg({'listprice': 'skewness'}).collect()) # drop data: could be recorded incorrect, unique events, incorrect format, duplications, missing values, not relevant. # dropping columns cols_to_drop = ['no_idx', 'class', 'unitnumber'] df = df.drop(*cols_to_drop) # text filtering df = df.where(~df['potentialshortsale'].like('not disclosed')) # outlier filtering: within three stddev of the mean. ## value filtering std_val = df.agg({'salescloseprice':'stddev'}).collect()[0][0] mean_val = df.agg({'salescloseprice':'mean'}).collect()[0][0] ## create three stddev upper and lower bounds hi_bound = mean_val + 3*std_val lo_bound = mean_val - 3*std_val ## filter in-between df = df.where((df['listprice'] < hi_bound) & (df['listprice'] > lo_bound)) # dropping missing values df = df.dropna() # drop any records with null values ## drop records if all columns in subset are null. df = df.dropna(how='all', subset['listprice', 'salescloseprice']) ## drop records where at least two columns have null values. df = df.dropna(thresh=2) ## drop duplicates df.dropduplicates() # entire df df.dropduplicates(['col1']) # only columns list # list of possible values containing 'yes' yes_values = ['yes w/ qualifying', 'yes w/no qualifying'] # filter the text values out of df but keep null values text_filter = ~df['assumablemortgage'].isin(yes_values) | df['assumablemortgage'].isnull() df = df.where(text_filter) # adjusting data: scaling for regression or knn algorithms. ## minmaxscaling max_days = df.agg({'daysonmarket':'max'}).collect()[0][0] min_days = df.agg({'daysonmarket':'min'}).collect()[0][0] ## create a new column based off the scaled data df = df.withcolumn('scaled_days', (df['daysonmarket'] - min_days)/(max_days - min_days)) ## standardization: transform data to standard normal distribution of mean of zero and stddev of 1 mean_days = df.agg({'daysonmarket':'mean'}).collect()[0][0] stddev_days = df.agg({'daysonmarket':'stddev'}).collect()[0][0] ## create a new column with the scaled data df = df.withcolumn('ztrans_days', (df['daysonmarket'] - mean_days) / stddev_days ) ## log scaling from pyspark.sql.functions import log df = df.withcolumn('log_salescloseprice', log(df['salescloseprice'])) ## inverse log (1/log) for skewness. ### calculate the max year max_year = df.agg({'yearbuilt': 'max'}).collect()[0][0] ### create a new column of reflected data df = df.withcolumn('reflect_yearbuilt', (max_year + 1) - df['yearbuilt']) ### create a new column based reflected data df = df.withcolumn('adj_yearbuilt', 1 / log(df['reflect_yearbuilt'])) # missing data df.isnull() # .isnull() for pyspark df; .isnull() for pandas df; # plotting missing data import seaborn as sns pandas_df = df.select(['roomarea1']).sample(false, 0.5, 42).topandas() sns.heatmap(data=pandas_df.isnull()) # imputation df.fillna(value, subset=none) # value to replace; subset of cols to replace missings. ## example col_mean = df.agg({'daysonmarket':'mean'}).collect()[0][0] df.fillna(col_mean, subset=['daysonmarket]) ## imputation threshold def column_dropper(df, threshold): total_records = df.count() for col in df.columns: # calculate the percentage of missing values missing = df.where(df[col].isnull()).count() missing_percent = missing / total_records # drop column if percent of missing is more than threshold if missing_percent > threshold: df = df.drop(col) return df # drop columns that are more than 60% missing df = column_dropper(df, threshold=.6) # about joins ## orienting our data directions, i.e., left join. outdf = df1.join(df2,        # other data merge on  = none, # keys to join how = none) # type of join; default='inner' ## example condition = [df['offmarketdate']==df1['datetime']] df = df.join(df1, on=condition, 'left') df.where(~df['nm'].isnull()).count() # how many sales occurred on bank holidays ## note that we can also use spark.sql() to apply sql query. ## careful joins # cast data types walk_df = df.withcolumn('longitude', df['longitude'].cast('double')) walk_df = df.withcolumn('latitude', df['latitude'].cast('double')) # round precision df = df.withcolumn('longitude', round(df['longitude'], 5)) df = df.withcolumn('latitude', round(df['latitude'], 5)) # create join condition condition = [(walk_df['latitude'] == df['latitude']), (walk_df['longitude'] == df['longitude'])] # join the dataframes together join_df = df.join(walk_df, on=condition, how='left') # count non-null records from new field print(join_df.where(~join_df['walkscore'].isnull()).count()) # sql query in pyspark df.createorreplacetempview('df') sql_query = ""select * from df where longitude > 0 and latitude > 0"" out_df = spark.sql(sql_query) feature generation generating new features: multiplying, summing, differencing, dividing, combining two features, etc. use libraries: featuretools, tsfresh. # create a new feature by combining two features df = df.withcolumn('total_sqft', (df['width'] * df['length'])) # new feature by dividing two features df = df.withcolumn('price_per_sqft', (df['listprice'] / df['total_sqft'])) # new feature by differencing two features df = df.withcolumn('daysonmarket', datediff('offmarketdate','listdate')) # create new feature by adding two features together df = df.withcolumn('total_sqft', df['sqftbelowground'] + df['sqftaboveground']) # create additional new feature using previously created feature df = df.withcolumn('baths_per_1000sqft', df['bathstotal'] / (df['total_sqft'] / 1000)) df[['baths_per_1000sqft']].describe().show() # sample and create pandas dataframe pandas_df = df.sample(false, 0.5, 0).topandas() # linear model plots sns.jointplot(x='total_sqft', y='salescloseprice', data=pandas_df, kind=""reg"", stat_func=r2) plt.show() sns.jointplot(x='baths_per_1000sqft', y='salescloseprice', data=pandas_df, kind=""reg"", stat_func=r2) plt.show() time feature choose the right level (daily, monthly, so on.). from pyspark.sql.functions import to_date, year, month, dayofmonth, weekofyear # cast the data type to date df = df.withcolumn('listdate', to_date('list_date')) # year, month components df = df.withcolumn('list_year', year('listdate')) df = df.withcolumn('list_year', month('listdate')) # dayofmonth, weekofyear components df = df.withcolumn('list_dayofmonth', dayofmonth('listdate')) df = df.withcolumn('list_weekofyear', weekofyear('listdate')) # time based metrics from pyspark.sql.functions import datediff ## calculate difference btw two date fields df = df.withcolumn('daysonmarket', datediff('offmarketdate', 'listdate')) # create lagging features ## window(): returns a record based off a group of records ## lag(col, count=1): returns the value that is offset by rows before the current row. from pyspark.sql.functions import lag from pyspark.sql.window import window # create window w = window().orderby(df['date']) # create lagged column df = df.withcolumn('mortgage-1wk', lag('mortgage', count=1).over(w)) # examples # import needed functions from pyspark.sql.functions import to_date, dayofweek # convert to date type df = df.withcolumn('listdate', to_date('listdate')) # get the day of the week df = df.withcolumn('list_day_of_week', dayofweek('listdate')) # sample and convert to pandas dataframe sample_df = df.sample(false, 0.5, 42).topandas() # plot count plot of day of week sns.countplot(x=""list_day_of_week"", data=sample_df) plt.show() # extracting features from pyspark.sql.functions import when # create boolean filters find_under_8 = df['roof'].like('%age 8 years or less%') find_over_8 = df['roof'].like('%age over 8 years%') # apply filters using when() and otherwise() df = df.withcolumn('old_roof', (when(find_over_8, 1) .when(find_under_8, 0) .otherwise(none))) # splitting columns from pyspark.sql.functions import split split_col = split(df['roof'], ',') # put the first value of the list into a new column df = df.withcolumn('roof_material', split_col.getitem(0)) # explode the list to rows and pivot # from index1 | [a, b, c] -> index1 | a ; index1 | b; index1 | c # then one-hot-encoding a, b, c. from pyspark.sql.functions import split, explode, lit, coalesce, first ## split the column on commas into a list df = df.withcolumn('roof_list', split(df['roof'], ',')) ## explode list into new records for each item ex_df = df.withcolumn('ex_roof_list', explode(df['roof_list'])) ## get dummy column of constant value ex_df = ex_df.withcolumn('const_val', lit(1)) ## pivot the values into boolean columns pivot_df = ex_df.groupby('no').pivot('ex_roof_list')\      .agg(coalesce(first('const_val'))) #ignore nulls, takes first value binarizing, bucketing, encoding # binarizing from pyspark.ml.feature import binarizer ## cast the data type to double df = df.withcolumn('fireplaces', df['fireplaces'].cast('double')) ## create binarizing transformer bin = binarizer(threshold=0.0, inputcol='fireplaces', outputcol='fireplacet') ## apply the transformer df = bin.transform(df) # bucketing from pyspark.ml.feature import bucketizer ## define how to split data splits = [0, 1, 2, 3, 4, float('inf')] ## create bucketing transformer buck = bucketizer(splits=splits, inputcol='bathstotal', outputcol='baths') ## apply transformer df = buck.transform(df) # one hot encoding from pyspark.ml.feature import onehotencoder, stringindexer ## create indexer transformer stringindexer = stringindexer(inputcol='city', outputcol='city_index') ## fit apply transformer model = stringindexer.fit(df) indexed_df = model.transform(df) ## create encoder transformer encoder = onehotencoder(inputcol='city_index', outputcol='city_vec') ## apply transformer encoded_df = encoder.transform(indexed_df) choosing the algorithm spark ml landscape: predict quantity: regression (ml.regression) predict category: for unlabeled data, use ml.clustering; for labeled data, use ml.classification. predict similarity: for basket based, use association rules (ml.fpm); for user based, use collaborative filtering (ml.recommendation). ml.regression: generalizedlinearregression, isotonicregression, linearregression, decisiontreeregression, gbtregression , randomforestregression . note that, splitting randomly in train and test for a time series component would leak information about what happens in the future. to prevent this, you can split your data sequentially over the time series, and train your model on the first sequences and test it with the last chunk, then incremental testing called step-forward optimization. the size of your sets depends on how far out you need to forecast. train and test splits for time series # create vars for max and min dates in our data max_date = df.agg({'offmktdate':'max'}).collect()[0][0] min_date = df.agg({'offmktdate':'min'}).collect()[0][0] # find how many days our data spans from pyspark.sql.functions import datediff, date_add range_in_days = datediff(max_date, min_date) # find the date to split the data on split_in_days = round(range_in_days * 0.8) split_date = date_add(min_date, split_in_days) # split the data into 80% train, 20% test train_df = df.where(df['offmktdate'] < split_date) test_df = df.where(df['offmktdate'] >= split_date)\ .where(df['listdate'] >= split_date) def train_test_split_date(df, split_col, test_days=45): """"""calculate the date to split test and training sets"""""" # find how many days our data spans max_date = df.agg({split_col: 'max'}).collect()[0][0] min_date = df.agg({split_col: 'min'}).collect()[0][0] # subtract an integer #days from the last date in dataset split_date = max_date - timedelta(days=test_days) return split_date # find the date to use in spitting test and train split_date = train_test_split_date(df, 'offmktdate') # create sequential test and training sets train_df = df.where(df['offmktdate'] < split_date) test_df = df.where(df['offmktdate'] >= split_date).where(df['listdate'] <= split_date) adjusting time feature from pyspark.sql.functions import datediff, to_date, lit split_date = to_date(lit('2017-12-10')) # create sequential test set test_df = df.where(df['offmktdate'] >= split_date).where(df['listdate'] <= split_date) # create a copy of daysonmarket to review later test_df = test_df.withcolumn('daysonmarket_original', test_df['daysonmarket']) # recalculate daysonmarket from what we know on our split date test_df = test_df.withcolumn('daysonmarket', datediff(split_date, df['listdate'])) random forest regression rf can handle skewed/non-normal data, missing values, both numeric and categorical values, unscaled data. appended features: economic (mortgate_rate); governmental (median home price for city, home age percentages for city, home size percentages for city); social (walk score, bike score); seasonal (bank holidays); temporal features (limited value with one year of data, holiday weeks); rates, ratios, sums (business, personal levels); etc. note that missing values are handled by rf internally where they partition on missing values, as long as you replace these missing values by something outside of the range of normal values, they will be handled correctly. likewise, categorical features need to be mapped to numbers, they are okay to stay all in one column by using a stringindexer. transform data frame columns to feature vectors . from pyspark.ml.feature import vectorassembler # replace missing values df = df.fillna(-1) # define cols to be converted to vectors features_cols = list(df.columns) # remove dependent var from the list features_cols.remove('salescloseprice') # create the vector assembler transformer vec = vectorassembler(inputcols=features_cols, outputcol='features') # apply the vector transformer to data df = vec.transform(df) # select only the feature vectors and dependent variable (target) ml_ready_df = df.select(['salescloseprice','features']) dropping columns with low observations obs_threshold = 30 cols_to_remove = list() # inspect first 10 binary columns in list for col in binary_cols[0:10]: # count the number of 1 values in the binary column obs_count = df.agg({col: 'sum'}).collect()[0][0] # if less than our observation threshold, remove if obs_count <= obs_threshold: cols_to_remove.append(col) # drop columns and print starting and ending dataframe shapes new_df = df.drop(*cols_to_remove) handle missing and categorical values # replace missing values df = df.fillna(-1, subset=['walkscore', 'bikescore']) # create list of stringindexers using list comprehension indexers = [stringindexer(inputcol=col, outputcol = col + ""_idx"").sethandleinvalid(""keep"") for col in categorical_cols] # create pipeline of indexers indexer_pipeline = pipeline(stages=indexers) # fit and transform the pipeline to the original data df_indexed = indexer_pipeline.fit(df).transform(df) # clean up redundant columns df_indexed = df_indexed.drop(*categorical_cols) building a model training a rf, predicting it on new data, and evaluate it. from pyspark.ml.regression import randomforestregressor # initialize model with columns to utilize rf = randomforestregressor(featurescol=""features"", labelcol=""salescloseprice"", predictioncol=""prediction_price"", seed=42 ) # train model model = rf.fit(train_df) # make predictions predictions = model.transform(test_df) # evaluate model from pyspark.ml.evaluation import regressionevaluator # select cols to compute test error evaluator = regressionevaluator(labelcol=""salescloseprice"", predictioncol=""prediction_price"") # evaluation metrics rmse = evaluator.evaluate(predictions, {evaluator.metricname:""rmse""}) r2 = evaluator.evaluate(predictions, {evaluator.metricname:""r2""}) build a gbtregressor model and evaluate it. from pyspark.ml.regression import gbtregressor from pyspark.ml.evaluation import regressionevaluator # train a gradient boosted trees (gbt) model. gbt = gbtregressor(featurescol=""features"", labelcol=""salescloseprice"", predictioncol=""prediction_price"", seed=42 ) # train model. model = gbt.fit(train_df) # select columns to compute test error evaluator = regressionevaluator(labelcol=""salescloseprice"", predictioncol=""prediction_price"") # dictionary of model predictions to loop over models = {'gradient boosted trees': gbt_predictions, 'random forest regression': rfr_predictions} for key, preds in models.items(): # create evaluation metrics rmse = evaluator.evaluate(preds, {evaluator.metricname: ""rmse""}) r2 = evaluator.evaluate(preds, {evaluator.metricname: ""r2""}) interpreting, saving, and loading model import pandas as pd # convert feature importances to a pandas column fi_df = pd.dataframe(model.featureimportances.toarray(), columns=['importance']) # convert list of feature names to pandas column fi_df[""feature""] = pd.series(feature_cols) # sort the data based on feature importance fi_df.sort_values(by=['importance'], ascending=false, inplace=true) # save model model.save(""rfr_final_model"") # load model from pyspark.ml.regression import randomforestregressionmodel model2 = randomforestregressionmodel.load('rfr_final_model') big data pyspark python feature engineering -- -- written by tung thanh le 28 followers · 125 following computer scientist no responses yet help status about careers press blog privacy rules terms text to speech",12
https://fithis2001.medium.com/adding-some-minio-to-your-standalone-apache-spark-cluster-d068b4f4fdab,,,"adding some minio to your standalone apache spark cluster | by vasileios anagnostopoulos | medium sitemap open in app sign up sign in medium logo write sign up sign in adding some minio to your standalone apache spark cluster disaggregated compute and storage for the apprentice data engineer vasileios anagnostopoulos 9 min read · aug 26, 2022 -- 1 listen share background one of the typical problems, a beginner in apache spark encounters is to have an apache spark cluster and an apache hadoop cluster to practice the locality of compute and storage through yarn or follow apache spark examples by interacting with an hdfs system. this typically entails connecting your apache spark to an already running hdfs system, or just run your apache spark as a yarn job in an existing hadoop cluster. the approach in most of the cases amounts to some kind of the following options: download distribution for apache hadoop and apache spark and run them locally. use a docker distribution that is all-inclusive and preset like hortonworks sandbox use a more modularized docker distribution like big data europe . pay for a preset distribution in various cloud providers. of course, (1) does not qualify as clustered if you need more than 1 instances and takes some steps to set up. on the other extreme, (4) may cost you $$$. solution (3) downloads a lot and since so many things are preset, it does not qualify as educational, especially when maintenance is left on volunteers and can be behind the current versions of the packages. solution (2) is too fat for my taste. still, a lot is preset. however, if you have a fairly capable laptop, you can create your own setup, and learn on the way. one of my main requirements was to have something that can be setup easily. this rules out hadoop/yarn since it has too many moving parts and knobs to make it work as a cluster. making my own docker image was a no-go at that time (but could take an existing one and upgrade it, of course). using some other’s distribution was also a no-go because i saw a lot of them that quickly become unmaintained or get updates sporadically. coming to spark, by ruling out yarn or paying $$$ for educational purposes, i see the standalone cluster mode as the best solution to my constraints since i was not interested at this point in learning kubernetes. instead i should focus on apache spark. but still, the standalone mode needs some storage to pull data. while a relational database is a very viable (and overkill if you do not already use it for something else) option, i wanted something like hadoop-like. minio to the rescue one possible solution was to use something like s3, but without having to use amazon web services. leofs is an option, but there is a lot of hype about minio and good documentation … and docker images. minio promises disaggregated compute and storage. the value proposition is that storage and compute should scale independently . this is in accord with cloud computing. contrary to hadoop it is not a file storage. in hdfs a file is split and replicated across various nodes. but minio is an object storage. you have files+metadata which both make an object. after a couple of minutes of thought, it is obvious that hdfs can be emulated easily. after all, it is a game of path names. but you lose the co-location of compute and storage, which is exactly the same if your apache spark cluster and apache hadoop cluster live in different servers. in this particular case, minio has broken the speed barrier against hdfs. this is actually what i was looking since i jumped outside the hadoop ecosystem. the only remaining question is how to use it with apache spark? it turns out that i’m not the only one having this question. fortunately, there are answers like this and this (from medium): big data without hadoop/hdfs? minio tested on jupter + pyspark the takeover of hortonworks by cloudera ended the free distribution of hadoop. therefore, a lot of people are looking… python.plainenglish.io unfortunately, these are either outdated or they use apache spark from the sdk and not through a standalone cluster. they can also be complicated, and in some cases the answers are scattered across many sites. sometimes they lead to dead ends. after a lot of trial and error and posts and blogs, this is the documentation of my setup. the purpose is to be simple, maintainable and oriented towards beginners. let’s start. setup minio and first sanity check we will use the minio docker image from minio . the bitnami one is not maintained anymore. i provide a docker-compose.yml for the code of this article. you can start minio as docker compose up minio minio takes some time to start and here you are if you access it at http://127.0.0.1:9001 the minio console we will use the data from here . it is a csv file with addresses for demo purposes, which you need to download. log in, create a bucket as “mybucket” and upload your csv file. this is the end result bucket with files you can also join the docker network and list the contents without problems by using the outdated bitnami client docker run -it — rm — name minio-client \ — env minio_server_host=”my-minio-server” \ — env minio_server_access_key=”theroot” \ — env minio_server_secret_key=”theroot123"" \ — network app-tier — volume $home/mcconf:/.mc \ bitnami/minio-client ls minio/mybucket the output is shown in the next screenshot accessing minio programmatically through python for sanity check, we will access through python our minio server from our pc. first, we need to install python minio support . pip install minio open your favorite editor, ide or cli environment and execute access minio through python you can also join the docker network through jupyter . create in your current folder a sub-folder, e.g. jupyter-workspace . now you can run docker run -it --rm --network app-tier  -p 10000:8888 -v ""${pwd}""/jupyter-workspace:/home/jovyan/ jupyter/scipy-notebook:latest connect to http://localhost:10000 take the token from the console output and login with it. create a python3 notebook and execute a modified version of the above code. first in a cell run !pip install minio in case you have any problems, look here . in the next cell run the modified code success!!! accessing minio through local hadoop client now it is time for our first encounter with the apache ecosystem. it came to me as a pleasant surprise that hadoop can “see” an s3 file system as another distributed file store. it is not only for hdfs. there is extensive documentation on this . minio is s3 compatible, so we are in business. hdfs is so advertised that it is easy to miss the point that hadoop actually is both an implementation and a driver for hdfs. let’s get to work! first, we need to download the hadoop distribution, since the plan is to access our “distributed” minio from hadoop and apply its commands . download the latest release from here (3.3.4). i keep everything in my downloads/apache folder. so the next reasonable step now is to have export hadoop_home=$home/downloads/apache/hadoop-3.3.4 export path=$hadoop_home/bin:$path the idea is now to naively access it as a s3 file system with the s3a scheme which call the appropriate driver. hadoop fs -ls s3a://mybucket/addresses.csv of course, it fails. we have not configured where s3a is !!!. head over to $hadoop_home/etc/hadoop backup core-site.xml (e.g. make it _core-site.xml) and “steal” some of the contents from here . a more up-to-date list can be found here . we put the bare minimum necessary (in my repo it is this file ): core-site.xml for minio access our attempt now fails again however now we are in a better position. it cannot find a library. the real hint is in overview section . in summary: export hadoop_optional_tools=”hadoop-aws” this time we succeed, yay!!!! so we can use apache hadoop to interact with another distributed filesystem. a huge deal. now our hopes for spark access get better and better since it uses hadoop as its jdbc for file systems. accessing minio through local spark we will first make sure that we can access the cluster through spark locally. head over to the downloads section of apache spark and download the latest release, without any built-in hadoop. you can always use the all-inclusive. but here for tutorial reasons we do the bare minimum. after this article you will have the knowledge to handle that situation. download it, unzip it and set it up export hadoop_home=$home/downloads/apache/hadoop-3.3.4 export spark_home=$home/downloads/apache/spark-3.3.0-bin-without-hadoop export path=$spark_home/bin:$hadoop_home/bin:$path export hadoop_optional_tools=”hadoop-aws” we have a script for accessing minio through spark. it reads while the python distribution has the python packages we need, i will also use it through pip (python 3.10.6), for autocompletion reasons and easy setup. pip3 install pyspark let’s give it a try in thonny. oooops!!!! what went wrong? hadoop can talk to minio and spark talks to hadoop. it cannot talk though, since we have a spark without hadoop. the fix is usually the documentation . export spark_dist_classpath=$(hadoop classpath) we succeed this time. the file is there (you can also verify through ui) or with hadoop!!! now we are ready to provide our apache spark standalone cluster with some storage. running our spark standalone cluster our cluster is based on this repo . there are some differences though. latest dependencies spark is without bundled hadoop i add a latest hadoop distribution enable s3 in hadoop so as to be able to run commands from workers or master i also have added a docker-compose.yml that sets everything up. the other thing i would like to mention is that since hadoop comes with an empty core-site.xml , it should be provided either directly, by linking an external one to the hadoop folder, or link a spark-defaults.conf which puts the same variables in a non-xml format with a hadoop prefix. i included both. feel free to connect to the container and execute hadoop file system commands. let's spin our full cluster with minio and spark (master url is in 127.0.0.1:8080). docker compose up we delete the parquet file hadoop fs -rmr s3a://mybucket/output_addresses.parquet and now we will try to re-create it from our spark cluster!!! we definitely need our env variables before submitting in a new terminal export hadoop_home=$home/downloads/apache/hadoop-3.3.4 export spark_home=$home/downloads/apache/spark-3.3.0-bin-without-hadoop export path=$spark_home/bin:$hadoop_home/bin:$path export hadoop_optional_tools=”hadoop-aws” export spark_dist_classpath=$(hadoop classpath) now we can submit spark-submit — master spark://127.0.0.1:7077 spark-access-minio.py we were too optimistic, sigh!!!! crash and burn! not really obvious what is happening here. some months ago i opened this bug . months passed, day to day work took over. at some point in mid-july i decided to track it down. i had a very hard time. the hadoop in the container resolves just fine. eventually i added a magic line in my /etc/hosts to resolve it to my localhost in case it comes from the local spark client. guess what! it worked this time and the file is there, though it is a mystery to me why. i suspect driver is validating data as it is described by others ! i am very interested in an explanation. conclusion i decided to deviate from the typical hadoop/spark combo because of complexity. i opted instead for a different solution that is easier and equally cloud native. that path needed a lot of experimentation and studying. the gains in knowledge were non-trivial. i re-discovered that it can pay off to do things differently. i shared my journey to make the life of others easier and as a means of self-documentation. i used an x64 mac with mac os monterey, python 3.10.6 and latest openjdk11. code is in this repository . as usually i am very happy for corrections and suggestions for improvement. spark minio hadoop data engineering -- -- 1 written by vasileios anagnostopoulos 65 followers · 38 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@sergey.kotlov/unit-testing-of-spark-applications-cf0162a5cb3c,,Unit Testing,"unit testing of spark applications | by sergey kotlov | medium sitemap open in app sign up sign in medium logo write sign up sign in unit testing of spark applications sergey kotlov 8 min read · mar 29, 2022 -- listen share in this post, we’ll look at one of the ways to unit test spark applications and prepare test datasets. the motivation for using the described approach at joom is the large number of constantly evolving data sources (including more than 1000 different spark tables) used in our spark applications. and as our spark infrastructure developed, we began to create some tools to simplify unit testing. photo by ildefonso polo on unsplash introduction usually, production spark code performs operations on spark datasets . you can cover it with tests using a local sparksession and creating spark datasets of the appropriate structure with test data. there are the following basic ways to create the test datasets: creating test data manually. typically, these will be small datasets, which allow you to check the correctness of your code in various situations. creating arbitrary datasets using generators. using samples from production data (this post does not cover this approach). in my opinion, the easiest and most effective way to fully test application logic is to create the necessary test data manually for each test case. you can easily create a test spark dataset/dataframe using scala case classes that match the required data structure (we call them “test data classes”). for example, if a spark application uses a table with orders, you could define the case class with its structure: next, you create test data suitable for your test scenario and turn it into dataset or dataframe. it is convenient to have all fields optional because production tables (and their corresponding classes) may consist of several dozens or even hundreds of fields, but specific spark applications often use only a subset of those fields. optional fields allow you to set only the required data in your tests. repository of test data classes as previously mentioned, we have a large number of different data tables. we have a separate project in our code repository that contains all the test data classes (like order described earlier) for all spark tables and other data structures that can be used as input for our spark applications. this project is a centralized repository of these test data classes, and it also contains our test api. all spark projects that have unit tests have this project as a dependency. the requirements for this test data classes repository: our developers should have ready-to-use test data classes for all data structures that can be used as a data source in their spark applications. this simplifies the creation of test data. the definition of these test data classes must match the real tables and be updated when they change. thus, the code of all applications will be tested against the real data structures with which it will have to deal in the production environment. if some tables change in the future and become incompatible with the existing applications code, we will know about it at the test launch stage. these test data classes should be created and updated automatically. to achieve these goals, we use automatic code generation of the test data classes based on the structure of existing spark tables. we use custom gradle tasks to perform the code generation. these gradle tasks are located in the build.gradle file of this project with the test infrastructure. we have implemented two alternative options for automatically adding or updating the test data classes. 1. automatic generation based on the description of spark tables in hive metastore. it works as follows: we run a custom gradle task to perform this code generation. in the parameters of this gradle task, you can set a list of databases and tables for which you want to generate these classes since often, not all of the existing tables may be used in your spark apps. this gradle task, through a special custom service, gets the actual schemas (in json format) for all required tables from hive metastore and converts them into structtype objects. then, we pass the structtype objects to our custom code generator, which for each of them creates a <yourtablename>.scala file containing the code of the test data class. these classes have structures identical to the corresponding tables. the generated files are saved to the desired location of the test infrastructure project. all these files are organized by packages corresponding to databases. an example of using such gradle task in the build.gradle file of the project: 2. automatic generation based on production classes containing table creation logic. for some spark tables, there are special classes in our projects that define the logic for creating these tables. also, these classes specify the spark schemas of the resulting tables (primarily for documenting fields) and implement a special trait: trait joomtable { val tablename: string val database: string val schema: structtype } it is possible to generate the test data classes based on such production classes. in this case, the code generator discovers the classes that implement the joomtable trait and extracts the structtype objects from them using reflection . the advantage of this approach is that when you edit or add any such table, you don’t need to run your spark app to save the new version of the table schema into hive metastore in order to get the updated test data class. the disadvantage is that you have to describe the resulting table schema in your code, which you may consider as additional overhead. it is worth mentioning that spark tables are not the only source of input for our spark apps. we also have data in the form of so-called events . events can be of different types (for example, productpurchase — the fact of purchasing a product, productopen — the fact of viewing a product card), and each type has its own structure. spark apps access the event dataframe through a special custom api. our test framework also supports the automatic generation of test data classes for different types of events so that they can also be used in unit tests. these gradle tasks for creating/updating the test data classes can be run manually, and you could also set them to run automatically on a schedule together with unit tests. you can find the source code of the code generator here . structure of the test data classes in theory, the table/event structure can be quite complex, with any nesting depth, with various combinations of arraytype, maptype, structtype. our code generator supports structures of any complexity. the code generator creates a root class. and all nested structures are wrapped in case classes named after the corresponding fields and placed in a root object to avoid name conflicts in the case of the same field names in different tables/events. for example, for the product table with the structure: root |-- product_id: string |-- merchant_id: string |-- category_id: string |-- name: string |-- price: decimal(38,18) |-- images: array |    |-- element: struct |    |    |-- url: string |    |    |-- height: integer |    |    |-- width: integer |    |    |-- properties: map |    |    |    |-- key: string |    |    |    |-- value: string |-- attributes: map |    |-- key: string |    |-- value: array |    |    |-- element: string the code generator will create the product.scala file. all fields of such case classes are optional. because often, a particular spark app uses only a small subset of the fields available in the tables. and their optionality allows you to set in the test code only those fields which you need for each specific test scenario. test api in the production code of all our spark applications, we use a special class called platformapi . this class provides the following functionality: determines an appropriate configuration for each spark application. creates a sparksession with the necessary configuration, spark listeners, etc. provides an instance of a class called warehousecontext , which has getters that return dataframes for the most commonly used tables and data structures. in most cases, these getters use sparksession to read some spark tables and return its dataframes, but there may be more complex logic (for example, in the case of our events api). the project containing the test data classes also contains the test version of all these infrastructure classes so they can be used in unit tests instead of the production version of platformapi. for example, testplatformapi : provides a local sparksession. we use dataframesuitebase from spark-testing-base for this. specifies the local version of the configuration for tests (for example, in the production, we use s3 to store spark tables data, but in tests, we replace the s3 paths with local ones). returns the test version of warehousecontext, which in your test code can be configured to return the test dataframes you created instead of reading the production tables. finally, in our tests, we use a custom extender for funsuite called platformapisuite ( class platformapisuite extends funsuite with dataframesuitebase ). platformapisuite provides testplatformapi, contains a set of helper methods to facilitate test writing, clears temporary data after test completion. all of our unit tests use this platformapisuite class. an example of what such a test might look like: you can also write any additional helper methods for the needs of specific tests. for example, in some situations, it may be convenient to set test data not in the form of tables but in the form of higher-level business entities. for example, some applications may work with the concept of user data, which includes user events and records in various tables (user, order, etc.). in this case, you may define the test data using the high-level userdata class (it may be much more convenient), and some utility code will add the necessary records to the test tables/events with which the application works. it may not be necessary to cover all existing spark applications with full-fledged tests. but it is recommended that for each spark application, there be at least a minimal test on empty tables so that you can easily verify that the application does not crash on the table structures available in the production environment. this reduces the time needed to detect many errors. for example, if using the spark sql api we mistyped any field name or used an operation that is incorrect for the type of field, then it will be revealed during the next local test run. this is some kind of protection against incorrect changes in existing tables that make them incompatible with existing applications. in this case, after regenerating the test data classes, the unit tests will report which applications are broken. testing on automatically generated arbitrary datasets this method of creating test data is not used by us often. but in some cases, it can be useful to check the correctness of some applications on a large number of different input values. using scala case classes is convenient because there are enough libraries to work with them. below is an example in which we generate collections of test data classes with arbitrary data, which we then pass to the code under test. we use scalacheck-shapeless to create an instance of the specified class with arbitrary data. you can override the rules for generating arbitrary values (see trait arbitrarytypes). for some fields, it is desirable to replace completely arbitrary values with values from the provided list (for example, for a productive sql join). it is convenient to use quicklens for such modification of fields in deeply nested classes. conclusion in this post, i described our approach to writing unit tests for spark applications. we looked at the general approach to testing spark applications, the automatic creation of the test data classes, our api for writing unit tests, and a way to generate arbitrary test data automatically. spark unit testing scala big data -- -- written by sergey kotlov 132 followers · 27 following big data engineer at joom no responses yet help status about careers press blog privacy rules terms text to speech",12
https://aoyilmaz.medium.com/a-solution-of-big-data-apache-hadoop-33b705d42a0e,,,"a solution of big data: apache hadoop | by ahmet okan yilmaz | medium sitemap open in app sign up sign in medium logo write sign up sign in a solution of big data: apache hadoop ahmet okan yilmaz 4 min read · apr 27, 2022 -- listen share in my previous article , i talked about big data. there was a description about big data: big data isn’t a concept. it’s a problem to solve. and now, we have a solution: apache hadoop! source: https://hadoop.apache.org/ what is apache hadoop apache hadoop is a framework that allows us for the distributed processing of large data sets across clusters of computers using simple programming models. instead of using one large computer to store and process the data, allows clustering multiple computers to analyze massive datasets in parallel more quickly. hadoop systems can handle various forms of structured and unstructured data, giving users more flexibility for collecting, processing, analyzing and managing data than relational databases and data warehouses provide. hadoop was created by doug cutting and mike cafarella in 2005. doug cutting named it after his son’s toy elephant. ecosystem hadoop ecosystem is a platform or a suite which provides various services to solve the big data problems. it includes apache projects and various commercial tools and solutions. source: https://www.edureka.co/blog/wp-content/uploads/2016/10/hadoop-ecosystem-edureka.png modules hadoop consists of four main modules: hdfs (hadoop distributed file system): as the primary component of the hadoop ecosystem, hdfs is a distributed file system that provides high-throughput access to application data with no need for schemas to be defined up front. yarn (yet another resource negotiator): yarn is a resource-management platform responsible for managing compute resources in clusters and using them to schedule users’ applications. it performs scheduling and resource allocation across the hadoop system. mapreduce: mapreduce is a programming model for large-scale data processing. using distributed and parallel computation algorithms, mapreduce makes it possible to carry over processing logic and helps to write applications that transform big datasets into one manageable set. hadoop common: hadoop common includes the libraries and utilities used and shared by other hadoop modules. applications hbase: hbase is a non-relational distributed database running on the big data hadoop cluster that stores large amounts of structured data. cassandra: cassandra is a wide-column store nosql database management system. pig: pig is comprised of a high-level language for expressing data analysis programs, and infrastructure for evaluating these programs. pig was designed for performing a long series of data operations, making it ideal for etl data pipelines, research on raw data, and iterative processing of data. hive: hive allows for easy reading, writing, and managing files on hdfs. it has its own querying language for the purpose known as hql (hive querying language) which is very similar to sql. this makes it very easy for programmers to write mapreduce functions using simple hql queries. sqoop: sqoop plays an important part in bringing data from relational databases into hdfs. the commands written in sqoop internally converts into mapreduce tasks that are executed over hdfs. it works with almost all relational databases. flume: flume aggregates, collects, and moves large amounts of log data. kafka: kafka sits between the applications generating data (producers) and the applications consuming data (consumers). kafka is distributed and has in-built partitioning, replication, and fault-tolerance. it can handle streaming data and also allows businesses to analyze data in real-time. oozie: oozie is a workflow scheduler system that allows users to link jobs written on various platforms. using oozie you can schedule a job in advance and can create a pipeline of individual jobs to be executed sequentially or in parallel to achieve a bigger task. zookeeper: zookeeper is an open-source, distributed, and centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services across the cluster. ambari: ambari is a web-based interface for managing, configuring, and testing big data clusters to support its components. spark: spark is an alternative framework to hadoop built on scala but supports varied applications written in java, python, etc. spark is also responsible for hadoop streaming and supporting sql, machine learning and processing graphs. advantages of hadoop fast: in hdfs the data distributed over the cluster and are mapped which helps in faster retrieval. even the tools to process the data are often on the same servers, thus reducing the processing time. it is able to process terabytes of data in minutes and petabytes in hours. scalable: hadoop cluster can be extended by just adding nodes in the cluster. cost-effective: hadoop is open source and uses commodity hardware to store data so it really cost-effective as compared to the traditional relational database management systems. resilient to failure: hdfs has the property with which it can replicate data over the network, so if one node is down or some other network failure happens, then hadoop takes the other copy of data and use it. normally, data are replicated thrice but the replication factor is configurable. sources apache hadoop this is the second stable release of apache hadoop 3.3 line. it contains 284 bug fixes, improvements and enhancements… hadoop.apache.org data engineering apache hadoop big data -- -- written by ahmet okan yilmaz 187 followers · 80 following industrial engineer | data scientist | factory manager no responses yet help status about careers press blog privacy rules terms text to speech",12
https://ajithshetty28.medium.com/deequ-i-mean-data-quality-a0e6c048469d,,,"deequ, i mean data quality. we say how we can turn the world upside… | by ajith shetty | medium sitemap open in app sign up sign in medium logo write sign up sign in deequ, i mean data quality ajith shetty 5 min read · nov 20, 2021 -- 1 listen share photo by stephen phillips - hostreviews.co.uk on unsplash we say how we can turn the world upside down with the power of data. and it is rightly so. but a small mistake in the data could send the same world for a toss. a comma in an amount column would make a lot of difference. data quality is something which you can never ignore and it should always be a part of your pipeline for the lifetime. at this moment of time, the use of spark is enormous. no pipeline would be complete without having spark in between. we need a framework which can talk to spark directly and it can run run on top of big data. enter, amazon deequ. introduction deequ is a library built on top of apache spark for defining “unit tests for data”, which measure data quality in large datasets. python users may also be interested in pydeequ, a python interface for deequ. you can find pydeequ on github , readthedocs , and pypi . source what does it do amazon deequ would help you in: metrics computation: you can use deequ to get the quality metrics like maximum, minimum, correlation, completeness etc. once the metrics are calculated you can store the data in s3 to analyse at later point. constraint verification: you may define the constraint verification and the deequ will generates the data quality report. constraint suggestion: well deequ is smart enough to generate automated constraints based on the data you define. source: https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/ deequ reads the data directly and runs the constraints verification directly on top of the spark and generates the data quality report. user can provide the constraints to verify or the deequ could suggest it for you. once the metrics or report is generated, the output could be saved in a local file or s3, which can be queried at later point. supported list of analyser deequ/src/main/scala/com/amazon/deequ/analyzers at master · awslabs/deequ deequ is a library built on top of apache spark for defining ""unit tests for data"", which measure data quality in large… github.com how does it help using deequ, you can create a pipeline to validate the completeness or the missing of your data. you can provide the constraints to verify or let the deequ define it for you. you can use deequ for anomaly detection. deequ supports the repository where you can store your metrics results and query later. deequ can profile all your columns. demo setup: we will be using databricks community edition for easy setup and demo. https://community.cloud.databricks.com/ install the library. 2. import the classes 3. setup the dummy data 4. lets create the repository in the local file system. it can be s3 location as well. we need to pass the tag, which under which each of your run metrics will be stored. 5. lets create the verification suite. where we define all the checks we want to perform. in our example, we want to check: a. completeness on id and name b. record count should be greater than 5. c. citizenship should be either yes or no d. the travel_count should not be negative. and here you can see the result. our check has failed for: record count, we expected it to be greater than 5. name column contains null citizenship, contains a value other than yes and no checks have succeeded for: travel count which is not lesser than 0 for any records id column never being null. now let’s save the data and query it. give the userepository method and pass the repository name. now either you can query on top of repository for a given column. or you can convert the data to dataframe. bonus: you can run the profiling for all the columns by just 1 command. you may find the above code example here: github - ajithshetty/deequdemo you can't perform that action at this time. you signed in with another tab or window. you signed out in another tab or… github.com reference: github - awslabs/deequ: deequ is a library built on top of apache spark for defining ""unit tests… deequ is a library built on top of apache spark for defining ""unit tests for data"", which measure data quality in large… github.com deequ/src/main/scala/com/amazon/deequ/examples at master · awslabs/deequ deequ is a library built on top of apache spark for defining ""unit tests for data"", which measure data quality in large… github.com test data quality at scale with deequ | amazon web services in this blog post, we introduce deequ, an open source tool developed and used at amazon. deequ allows you to calculate… aws.amazon.com ajith shetty bigdata engineer — bigdata, analytics, cloud and infrastructure. subscribe ✉️ || more blogs 📝|| linkedin 📊|| profile page 📚|| git repo 👓 interested in getting the weekly newsletter on the big data analytics around the world, do subscribe to my: weekly newsletter just enough data data quality deeque data enginnering -- -- 1 written by ajith shetty 350 followers · 38 following bigdata engineer — love for bigdata, analytics, cloud and infrastructure. want to talk more? ping me in linked in: https://www.linkedin.com/in/ajshetty28/ responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://subpath.medium.com/managing-huge-datasets-with-scala-spark-9840ad760424,,,"managing huge datasets with scala spark | by alexander osipenko | medium sitemap open in app sign up sign in medium logo write sign up sign in managing huge datasets with scala spark some tips and tricks for working with large datasets in scala spark alexander osipenko 3 min read · nov 3, 2021 -- listen share photo by pat whelen on unsplash spark is awesome! it’s scalable and fast, especially when you writing in a “native spark” and avoiding custom udfs. but when you working with large data frames there are some tips that you can use to avoid oom errors and speed up the whole computation. here is a shortlist of the things that i learned from my personal experience. using configuration suited for the task it’s always a good idea to start with proper configuration. in my opinion, spark has awesome documentation , highly recommend starting with it. depending on how you use your spark: inside cluster or in a stand-alone mode your configuration will be different. i’m using spark mostly in standalone mode, so here are my examples: 1. driver memory and driver maxresult : when you are working with a large dataset you need to increase default memory allocation and maxresultsize value. val spark = sparksession.builder .config(""spark.driver.maxresultsize"", ""{your-value}"") .config(""spark.driver.memory"", ""{your-value}"") 2. broadcast timeout, network timeout, and heartbeat when you trying to save a large data frame to the database or some bucket i noticed that sometimes tasks might fail just because default timeouts thresholds are too small. .config(""spark.sql.broadcasttimeout"", ""{your-value}"") .config(""spark.sql.debug.maxtostringfields"", ""{your-value}"") .config(""spark.network.timeout"", ""{your-value}"") .config(""spark.executor.heartbeatinterval"", ""{your-value}"") 3. garbage collection you can use the garbage collector provided by jvm .config(""spark.executor.extrajavaoptions"", ""-xx:+useg1gc -xx:+unlockdiagnosticvmoptions -xx:+g1summarizeconcmark -xx:initiatingheapoccupancypercent=35 -verbose:gc -xx:+printgcdetails -xx:+printgcdatestamps -xx:onoutofmemoryerror='kill -9 %p'"") using caching in the right place: caching is an essential thing in spark and by using it in the correct places you can seriously reduce execution time. sometime you might consider .persist instead of caching. you can read more about it here . // cache you dataframe after expensive operations dataframe .select(...) .filter(...) .groupby(...) .agg(...) .cache repartition before joining: joins are a pretty expensive operation to run, there are several tricks you can use, but the one that i found the most useful is re-partitioning before joining. it helps because awesome by the time you get to join you can already perform some operations on your datasets and partitions might be skewed. and skewed partitions will seriously affect join execution time. val leftrepartitioned = left.repartition(256, col(""your column"")) val rightrepartitioned = right.repartition(256, col(""your column"")) val joined = left.join(right, ...) repartition after groupby: the same goes for groupby, it usually helps a lot val groupeddataset = foo .groupby(""bar"") .// your aggregations and other operations . .repartition(256, col(""your column"")) data skew in order to get the best performance from spark, you need to pay attention to partitions skew. there are many great articles about it ( 1 , 2 ), so i would not repeat them here. but just keep in mind that sometimes repartitions trick my new work if the column that you chose for partitioning is skewed. if you are not sure which column to choose you can always use the salting trick . udfs i should also mention elephant in the room — user defined functions. because they give you so much freedom it’s sometimes tempting to use them more often than you actually need them. every time when you wanna implement something custom i recommend you to double-check collections with default spark functions from the org.apache.spark.sql.functions. maybe you can solve your problem using expr :) . i also recommend you to check out spark-daria . it’s a collection with some useful methods that expand spark capabilities. deleting cached datasets after you are done with them if you cached some data frames using .cache you can call .unpersist to delete it from memory. val dataframecached = dataframe. cache // some more code dataframecached. unpersist or you can flush memory completely using sqlcontext.clearcache() thanks for reading! any feedback is welcome! linkedin — github — medium data science machine learning spark scala data engineering -- -- written by alexander osipenko 501 followers · 13 following leading/coaching/building data science teams from the scratch no responses yet help status about careers press blog privacy rules terms text to speech",12
https://datasciencelogs.medium.com/big-data-fundamentals-with-pyspark-939112ef2281,,,"fundamentals of big data with pyspark | by aruna singh | medium sitemap open in app sign up sign in medium logo write sign up sign in https://www.edureka.co/blog/big-data-analytics/ fundamentals of big data with pyspark aruna singh 9 min read · jun 4, 2021 -- listen share this article introduces the exciting world of big data, as well as the various concepts and different frameworks for processing big data. you will understand why apache spark is considered the best framework for bigdata big data concepts and terminology what exactly is big data? it is a term which refers to the study and applications of data sets that are too complex for traditional data-processing software. there are three vs of big data that are used to describe its characteristics: volume refers to the size of data, variety refers to different sources and formats of data and velocity is the speed at which data is generated and available for processing now, let’s take a look at some of the concepts and terminology of big data clustered computing is the pooling of resources of multiple machines to complete jobs parallel computing is a type of computation in which many calculations are carried out simultaneously distributed computing involves nodes or networked computers that run jobs in parallel batch processing refers to the breaking data into smaller pieces and running each piece on an individual machine real-time processing demands that information is processed and made ready immediately big data processing systems hadoop/mapreduce : an open source and scalable framework for batch data. apache spark: it is also open source and is suited for both batch and real-time data processing. it is a fast and general-purpose framework for big data processing. apache spark provides high-level apis in scala, java, python, and r. it runs most computations in memory and thereby provides better performance for applications such as interactive data mining. it is a powerful alternative to hadoop mapreduce, with rich features like machine learning, real-time stream processing, and graph computations. at the center of the ecosystem is the spark core which contains the basic functionality of spark. the rest of spark’s libraries are built on top of it. s park runs on two modes. the first is the local mode where you can run spark on a single machine such as your laptop. it is very convenient for testing, debugging and demonstration purposes. the second is the cluster mode where spark is run on a cluster. the cluster mode is mainly used for production. spark’s version of python: pyspark apache spark is originally written in scala programming language. to support python with spark, pyspark was developed with similar computation power as scala. apis in pyspark are similar to pandas & scikit-learn python packages. spark comes with an interactive python shell in which pyspark is already installed in it. it is particularly helpful for fast interactive prototyping before running the jobs on clusters. unlike most other shells, spark shell allows you to interact with data that is distributed on disk or in memory across many machines, and spark takes care of automatically distributing this processing. spark provides the shell in three programming languages: spark-shell for scala, pyspark for python and sparkr for r. pyspark. similar to scala shell, pyspark shell has been augmented to support connecting to a cluster. in pyspark shell , a sparkcontext represents the entry point to spark functionality. pyspark automatically creates a sparkcontext for you in the pyspark shell (so you don't have to create it by yourself) and is exposed via a variable sc . you can access the sparkcontext in the pyspark shell as a variable named sc. it’s like a key to your car. without the key you cannot enter the house, similarly, without an entry point, you cannot run any pyspark jobs. now, let’s take a look at some of the important attributes of sparkcontext. sc.version shows the version of spark that you are currently running sc.pythonver shows the version of python that spark is currently using. sc.master shows the url of the cluster or “local” string to run in local mode. the easiest way to demonstrate the power of pyspark’s shell is to start using it. let’s take an example of a simple list containing numbers ranging from 1 to 100 in the pyspark shell. the most important thing to understand here is that we are not creating any sparkcontext object because pyspark automatically creates the sparkcontext object named sc , by default in the pyspark shell. you can load your raw data into pyspark using sparkcontext by two different methods which we will be discussed later: sparkcontext’s parallelize method sparkcontext’s textfile method use of lambda function in python python supports the creation of anonymous functions. that is functions that are not bound to a name at runtime, using a construct called the lambda. it is used in conjunction with typical functional concepts like map and filter functions. like def, the lambda creates a function to be called later in the program. let’s look at some of its uses: use of lambda function in python — map(): the map function is called with all the items in the list and a new list is returned which contains items returned by that function for each item. use of lambda function in python — filter(): the function is called with all the items in the list and a new list is returned which contains items for which the function evaluates to true. introduction to pyspark rdd it is simply a collection of data distributed across the cluster. rdd is the fundamental and backbone data type in pyspark. now, let’s take a look at the different features of rdd. the name rdd captures 3 important properties: resilient , which means the ability to withstand failures and recompute missing or damaged partitions. distributed , which means spanning the jobs across multiple nodes in the cluster for efficient computation. datasets , which is a collection of partitioned data e.g. arrays, tables, tuples or other objects. there are three different methods for creating rdds, out of them you have already seen two methods which is being mentioned before. sparkcontext’s parallelize method: from external datasets ( sparkcontext’s textfile method ): files stored in hdfs or objects in amazon s3 buckets or from lines in a text file stored locally and pass it to sparkcontext’s textfile method. from existing rdds (mutating rdds) : this transformation is the way to create an rdd from already existing rdd. partitioning in pyspark data partitioning is an important concept in spark and understanding how spark deals with partitions allow one to control parallelism. a partition in spark is the division of the large dataset with each part being stored in multiple locations across the cluster. by default, spark partitions the data at the time of creating rdd based on several factors such as available resources, external datasets etc, however, this behavior can be controlled by passing a second argument called minpartitions which defines the minimum number of partitions to be created for an rdd. introduction to rdds in pyspark there are the various operations that support rdds in pyspark. rdds in pyspark supports two different types of operations — transformations and actions. transformations are operations on rdds that return a new rdd. actions are operations that perform some computation on the rdd. lazy evaluation : the most important feature which helps rdds in fault tolerance and optimizing resource use. spark creates a graph from all the operations you perform on an rdd and execution of the graph starts only when an action is performed on rdd. transformations on rdds : the map() takes in a function and applies it to each element in the rdd. the filter() takes in a function and returns an rdd that only has elements that pass the condition. the flatmap() is similar to map transformation except it returns multiple values for each element in the source rdd. the union() returns the union of one rdd with another rdd. actions on rdds: the collect() action returns complete list of elements from the rdd. the take() action print an ’n’ number of elements from the rdd. the count() action returns the total number of rows/elements in the rdd the first() action returns the first element in an rdd introduction to pair rdds in pyspark real world datasets are generally key/value pairs. each row is a key that maps to one or more values. in order to deal with this kind of dataset, pyspark provides a special data structure called pair rdds. in pair rdds, the key refers to the identifier, whereas value refers to the data. there two most common ways of creating pair rdds are as follows: the first step in creating pair rdds is to get the data into key/value form. next, we create a pair rdd using map function which returns tuple with key/value pairs with key being the name and age being the value. transformations on pair rdds : reducebykey: it is the most popular pair rdd transformation which combines values with the same key using a function. reducebykey runs several parallel operations, one for each key in the dataset, returns a new rdd consisting of each key and the reduced value for that key. sortbykey: we can sort pair rdd as long as there is an ordering defined in the key and returns an rdd sorted by key in ascending or descending order. groupbykey: it groups all the values with the same key in the pair rdd join transformation: applying join transformation merge two pair rdds together by grouping elements based on the same key. advanced actions on pair rdds: reduce() operates on two elements of the same type of rdd and returns a new element of the same type. the function should be commutative and associative so that it can be computed correctly in parallel. in many cases, it is not advisable to run collect action on rdds because of the huge size of the data. in these cases, it’s common to write data out to a distributed storage systems. saveastextfile() saves rdd with each partition as a separate file inside a directory by default. however, you can change it to return a new rdd that is reduced into a single partition using the coalesce method. countbykey() is only available on rdds of type (key, value). with the countbykey operation, we can count the number of elements for each key. one thing to note is that countbykey should only be used on a dataset whose size is small enough to fit in memory. collectasmap() returns the key-value pairs in the rdd to the as a dictionary. like sparkcontext’s parallelize method, collectasmap produces the key-value pairs in the rdd as a dictionary which can be used for downstream analysis. similar to countbykey, this action should only be used if the resulting data is expected to be small, as all the data is loaded into the memory. hence, we have covered the overview of big data fundamentals with pyspark, and in the process learned some useful syntax for rdd transformations and actions. there is another article which includes sql, dataframes and machine learning with mlib. to understand it, you can click on this link . stay connected and enjoy reading ! big data machine learning data analysis data science -- -- written by aruna singh 118 followers · 82 following as a data scientist at amazon, i love three things - data, data, data :d no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/data-science/running-spark-on-kubernetes-approaches-and-workflow-75f0485a4333,,,"running spark on kubernetes: approaches and workflow | by yifeng jiang | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. running spark on kubernetes: approaches and workflow best ways to run spark jobs on kubernetes for development, data exploration and in production yifeng jiang 6 min read · oct 18, 2021 -- 1 listen share photo by lazarescu alexandra on unsplash during the years of working on apache spark applications, i have always been swtiching my environment between development and production. i would use an ide such as visual studio code to write the scala or pyspark code, test it locally against a small piece of the data, submit the spark job to hadoop yarn to run in production and hopefully it just works on real big data. i would spend a lot of time dealing with spark dependencies for my local and production environment and make sure they are in sync. this workflow becomes more painful for machine learning applications, where a combination of pyspark, python libraries and jupyter notebook environment is required. even for enterprises with many engineers, it is still challenging to setting up and maintaining environments for spark application development, data exploration and running in production. enter spark on kubernetes. with spark on kubernetes, and ideally a fast object storage like flashblade s3, we can use a single environment to run all these different spark tasks, easily. a jupyter notebook backed by multiple spark pods for quick prototyping and data exploration, for small and big data. declare a spark application in a yaml file and submit it to run in production. apache airflow to orchestrate and schedule pipelines with multiple jobs. this new workflow is much more pleasant comparing to the previous one. all my tasks leverage the same kubernetes environment. dependencies are managed in container images so that they are consistent across development and production. performance issue can be detected in development phase because testing at scale becomes easier. and most importantly, there is no hadoop cluster to manage anymore. i explained how to set up spark to run on kubernetes and access s3 in my previous blog . this time, i will describe my new workflow to run spark on kubernetes for development, data exploration and production. super charging jupyter notebook with pyspark on kubernetes jupyter notebook is handy for quick prototyping and data exploration because developers and data scientists can start coding right the way on its web-based interactive development environment. however, since it runs in a single python kernel, it could be slow processing big data. on the other hand, pyspark allows us to write spark code in python and run in a spark cluster, but its integration with jupyter was not there — until the resent spark 3.1 release, which allows spark jobs to run natively in a kubernetes cluster. this makes it possible to process big data from a jupyter notebook. with a couple of lines of code configuration, we can now write pyspark code in a jupyter notebook, submit the code to run as a spark job in a kubernetes cluster. launch a spark on kubernetes session from jupyter in the example notebook blow, my pyspark code reads 112m records from a csv file stored in flashblade s3, and then performs some feature engineering tasks. due to the huge number of records, if running on a single process, this could be very slow. read 112m records from s3 into pyspark feature engineering in pyspark however, in this case, on the backend, the heavy processing is handled by a spark job running in kubernetes. the below are the spark pods in kubernetes launched by the notebook. spark pods in kubernetes launched by jupyter notebook because the backend is a fully distributed spark job, it is fast. we can now process and explore huge number of records in juypyter notebook, from the browser. streamline spark submission in production next step in the workflow is to submit the spark code to production. there were discussions on whether or not notebook should be treated as production code. some companies such as netflix has been doing this , but i think most are still not there yet. in my workflow, i would copy and refine the notebook code to a python file, put it in s3, declare a pyspark job in yaml file and submit it to kubernetes using the spark on k8s operator. you can find the details in my previous blog . there are two approaches to submit a spark job to kubernetes in spark 3.x: using the traditional spark-submit script using the spark on k8s operator i choose to use the spark on k8s operator, because it is native to kubernetes, therefore can be submitted from anywhere kubernetes client is available. with this approach, submitting a spark job is a standard kubernetes command: kubectl apply -f nyc-taxi.yaml . this helps streamline spark submission. it is also more flexible because spark client is not required on the node. you may have noticed that, this is different from how i launch a spark on kubernetes session from jupyter in the above section, where the traditional spark-submit is used. this is true because i wanted the spark driver to run inside the jupyter kernel for interactive development. whereas in production, we want reproducibility, flexibility and portability. orchestrate and schedule pipelines while the spark on k8s operator is great to submit a single spark job to run on kubernetes, often time, we want to chain multiple spark and other types of jobs into a pipeline, and schedule the pipeline to run periodically. apache airflow is a popular solution for this. apache airflow is a open-source platform programmatically author, schedule and monitor workflows. it can run on kubernetes. it also integrates well with kubernetes. i will skip the details of how to run airflow on kubernetes, and from airflow how to orchestrate spark jobs to run on kubernetes. for now, let’s focus on the behaviours and value it brings in. in the example blow, i define a simple pipeline (called dag in airflow) with two tasks which execute sequentially. the first task submits a spark job called nyc-taxi to kubernetes using the spark on k8s operator, the second checks the final state of the spark job that submitted in the first state. i have also set the dag to run daily. a simple pipeline defined in airflow on the airflow ui, this is what the dag looks like: the nyc taxi analysis dag on airflow ui while running, the first task in the dag will spin up multiple spark pods, as defined in the nyc-taxi.yaml file, on kubernetes through the spark on k8s operator, just like the kubectl apply command does. spark pods spun up by airflow airflow helps manage dependencies and scheduling of the multi-job workflow. since it reuses the jobs and runs in the same kubernetes environment, overhead of introducing airflow is minimum. conclusion with spark on kubernetes, and an external s3 object storage for the data, my data engineering process is dramatically simplified. i would open my browser to start quick prototyping and data exploration. thanks to the power of spark on kubernetes, i don’t have to limit my prototyping and exploration to a small set of sample data. once i am good with the prototype, i put the code in a python file, modify and submit it for running in production with a single kubenetes command. for complex pipelines, i would orchestrate the jobs with airflow to help manage dependencies and scheduling. this is my data engineering workflow. i like that i only need a single environment for all of these. apache spark kubernetes data engineering s3 jupyter notebook -- -- 1 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by yifeng jiang 574 followers · 25 following software & solutions engineer, big data and machine learning, jogger, hiker, traveler, gamer. responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/data-arena/databricks-certified-associate-developer-for-apache-spark-tips-to-get-prepared-for-the-exam-cf947795065b,,,"databricks certified associate developer for apache spark — tips to get prepared for the exam | by thiago cordon | data arena | medium sitemap open in app sign up sign in medium logo write sign up sign in data arena · data arena is a place where you will find the most exciting publications about data in general. here you can exchange ideas with people who are really making things happen with data. join us, share your ideas, concepts, use cases, codes and let’s make the data community grow. databricks certified associate developer for apache spark — tips to get prepared for the exam thiago cordon 8 min read · may 31, 2021 -- listen share photo by nguyen dang hoang nhu on unsplash databricks , founded by the creators of apache spark , is being largely adopted by many companies as a unified analytics engine for big data and machine learning. gartner has classified databricks as a leader in the last quadrant for data science and machine learning platforms. magic quadrant for data science and machine learning platforms — gartner (march 2021). as many companies are using apache spark, there is a high demand for professionals with skills in this framework but a scarce set of available candidates to fill these positions. as we can see below, the big data market will continue increasing meaning that the need for that kind of professionals will continue high. big data market size revenue forecast worldwide from 2011 to 2027 (in billion u.s. dollars) — https://www.statista.com/statistics/254266/global-big-data-market-forecast/ being prepared is crucial to fill the market needs and one way to do that is getting a certification. although there are a lot of resources about spark over the internet and even if you already work with it, getting a certification is a good way to prove your knowledge, and studying to get it is a good strategy to measure how much you know and what are the topics you have more difficulty and you should study to improve. in this post, i’ll share some important tips that i’ve followed to get certified. i hope it can be useful for you too. about the exam image from databricks — https://academy.databricks.com/learning-paths as demonstrated in the image above, the apache spark associate developer is applied for data engineer and data scientist learning paths. this exam will assess you in spark architecture and in the use of spark datafram api to manipulate data. general information ☑️ exam length: the exam consists of 60 multiple-choice questions and you’ll have 120 minutes to complete — it sounds like a lot of time to do that but, believe me, it’s not. programming language: you can choose between python or scala. spark version: although the exam for spark 2.4 is still available, it’s better to get the test in the most recent version of spark which is 3.0. exam language: english. pass score: the minimum passing score for the exam is 70% — you need to correctly answer a minimum of 42 questions. cost: 200.00 usd. no free retakes. resources available in the test environment: you will not be able to run code during the exam. a limited pdf version of the spark api documentation for the language you are taking the exam will be available — it doesn’t help much because it’s extensive and you don’t have the find feature in this pdf. you’ll also have a notepad available to make notes. scheduling the exam 📆 it’s recommended to schedule your exam some time in advance because some times and days of the week may be more sought after. to schedule your exam: 1. access the databricks academy and choose the desired exam. databricks certification page — image from https://academy.databricks.com/category/certifications 2. read the exam details and click on register. 3. you will be redirected to the kryterion webassessor , where you’ll have to register yourself. after registered, you can choose an exam, schedule the date/hour and proceed to checkout. note that here is where you choose the test language between python or scala. kryterion webassessor exam registration. security checks 👮 this is an online proctored exam so, there are some security requirements to be noticed. before your exam day, you’ll have to download and install an application called sentinel (which works only in windows) provided by kriterion, the company responsible for applying the test. after downloading the application, you’ll need to register yourself with facial recognition. it’s an easy process and you’ll see these instructions on kryterion webassessor page after you schedule your exam. 👉 on the test day, the sponsor may require you to complete some security checks before starting your exam: id confirmation 360 degree video review of your test environment 👉 other important requirements to be noticed about the test environment: there is only one active computer, one active monitor, one keyboard, and one mouse. you are not wearing a lanyard, badge, hat, watch, or jewelry. (remove them before the exam starts.) you may not interact with anyone — aside from online support staff — during your exam. you may not use dual monitors. breaks during an exam are only allowed when pre-approved by your test sponsor. if you interrupt your exam for a break, we will inform your test sponsor. do not lean out of the camera view during your exam. a proctor must be able to see you at all times. cell phones are not permitted in the testing area. reading the exam aloud is prohibited. what is covered by the exam? 📋 although the exam covers data manipulation, the sql language is not assessed. all questions related to data manipulation will be asked to solve using spark dataframe api. spark streaming is another topic that the exam doesn’t cover. 👉 the exam questions are distributed into three categories: exam questions categories. spark dataframe api questions represent most of the exam so, it should be your study focus if you have difficulties with that. 👉 here is a list of topics assessed in the exam by each category. use it to assess the topics you have more difficulties with and prioritize them in your study plan. spark architecture — conceptual cluster architecture: nodes, drivers, workers, executors, slots, etc. spark execution hierarchy: applications, jobs, stages, tasks, etc. shuffling partitioning lazy evaluation transformations vs actions narrow vs wide transformations spark architecture — applied execution deployment modes stability storage levels repartitioning coalescing broadcasting dataframes spark dataframe api subsetting dataframes (select, filter, etc.) column manipulation (casting, creating columns, manipulating existing columns, complex column types) string manipulation (splitting strings, regex) performance-based operations (repartitioning, shuffle partitions, caching) combining dataframes (joins, broadcasting, unions, etc) reading/writing dataframes (schemas, overwriting) working with dates (extraction, formatting, etc) aggregations miscellaneous (sorting, missing values, typed udfs, value extraction, sampling) preparation 👨‍🎓👩‍🎓 according to the databricks academy page , the minimally qualified candidate should: have a basic understanding of spark architecture, including adaptive query execution . be able to apply the spark dataframe api to complete individual data manipulation task, including: ➡ selecting, renaming and manipulating columns ➡ filtering, dropping, sorting and aggregating rows ➡ joining, reading, writing and partitioning dataframes ➡ working with udfs and spark sql functions although not explicitly tested in the exam, the candidate must have a working knowledge of either python or scala, depending on the language you have chosen in your test. 👉 study resources i listed here some resources you can use to get prepared for the exam. ➡ training — apache spark programming with databricks → this training is recommended to learn how to work with dataframe api — remember that more than 70% of the exam questions are related to dataframe api practice. ➡ training — quick reference: spark architecture → this is recommended to learn the concepts of spark architecture and the distributed computing. this is one of the trainings inside the pack of self paced courses that databricks sells. ➡ training — databricks certified developer for spark 3.0 practice exams → this is a well-rated udemy course with a comprehensive set of questions for the certification exam that you can practice. most questions come with detailed explanations, giving you a chance to learn from your mistakes. there are also links to the spark documentation and web contents to help you in your study. ➡ book — spark: the definitive guide: big data processing made simple → this book covers spark architecture and dataframe api usage. the recommended sections to study are: i. gentle overview of big data and spark ii. structured apis — dataframes, sql, and datasets iv. production applications ➡ book — learning spark, 2nd edition → this is another book that covers spark architecture and dataframe api usage. it’s lighter than the book “spark: the definitive guide” and covers the exam topics. the recommended sections to study are: 1. introduction to apache spark: a unified analytics engine 2. downloading apache spark and getting started 3. apache spark’s structured apis 4. spark sql and dataframes: introduction to built-in data sources (excluding the spark sql topics) 5. spark sql and dataframes: interacting with external data sources (excluding the spark sql topics) 7. optimizing and tuning spark applications ➡ spark documentation — python api → this is the documentation available in pdf in the exam if you chose python language. i recommend become familiar with this documentation, especially the sections pyspark.sq.module and pyspark package . becoming familiar with this documentation will help you to quickly browse the pdf file on exam day if you need it. ➡ spark documentation — scala api → this is the documentation available in pdf in the exam if you chose the scala language. the same recommendation here: become familiar with this documentation to browse it quickly. i didn’t take the scala exam but i would say that the following packages are important for the test: org.apache.spark.rdd org.apache.spark.sql for the databricks training mentioned above, one tip i give you is to check if your company has a partnership with databricks. if so, depending on the level of partnership, they can offer discount vouchers or even totally free training. final considerations i hope this article can help you to plan and prepare yourself for the exam or, at least, it helps you to study spark 😄. if you already have this certification, share your thoughts and how you got prepared. know someone who is preparing for this exam? share this content. thanks for reading and best of luck in your exam! 🤞 spark databricks data engineering data science certification -- -- published in data arena 355 followers · last published sep 9, 2024 data arena is a place where you will find the most exciting publications about data in general. here you can exchange ideas with people who are really making things happen with data. join us, share your ideas, concepts, use cases, codes and let’s make the data community grow. written by thiago cordon 208 followers · 67 following data practicioner, enabling business with data. editor at https://medium.com/data-arena no responses yet help status about careers press blog privacy rules terms text to speech",12
https://tanajisutar31.medium.com/databricks-spark-3-certification-preparation-guide-dbda5aa7b787,,,"databricks spark 3 certification: preparation guide | by tanaji sutar | medium sitemap open in app sign up sign in medium logo write sign up sign in tanaji sutar 4 min read · may 7, 2021 -- listen share databricks spark 3 certification: preparation guide recently i cleared databricks certificate for apache spark 3 (python) exam with score 86% on the 3rd of may 2021. if you are also planning for the same, hope this will help you to prepare better. this article tries to explain the steps i followed and not the technical concepts of the spark. let’s have some clarity about this certification exam first. what it isn’t: it’s not an assessment of understanding of databricks platform specific things like cluster setup, delta lake, file system commands etc. integration of any big data tool with spark. spark’s graphx, ml, streaming, data science etc. python/scala (but you must be aware of basic syntaxes to understand the questions) design of spark/big data-based project. what it is actually: it’s an assessment of spark’s data frame/dataset api. it more about syntaxes. it’s an assessment of understanding of spark architecture. it’s about what new things are there in spark3. it’s about spark performance tuning. let’s get started!! first, you must have a notepad or word document ready to note down the things. every concept, tricks and points you come across, keep them noted. it’s really useful while revision. many of us uses pen and paper/notebook but i would recommend to have notepad (or google docs) so that you can read it from cellphone, laptop. along with that, whatever you find useful for the exam like paragraphs/images/screenshot/urls from internet/blogs/practice tests etc., you can simply copy and paste in it. go through the webpage of databricks to get details of the exam. understand the syllabus and scope of the exam. https://academy.databricks.com/category/certifications https://academy.databricks.com/exam/databricks-certified-associate-developer 3. before appearing for the exam, you must have some hands-on, so i would recommend you to go through below udemy course that guides you to prepare things specific to certifications only and saves you from diversions. https://www.udemy.com/share/103mc6bumzdvhtth4=/ 4. databricks recommends us to go through, below two books: if you have prior knowledge of spark architecture and you already have gone through these books then no need to repeat , but folks who just started to learn spark, should spend some time to go through only specific sections of the book as mentioned below: sections i, ii, and iv of spark: the definitive guide chapters 1–7 of learning spark 5. once you think you are almost 40 to 50 % prepared, start appearing for the practice tests! many of us keep postponing the practice tests with very common reason that they haven’t completed syllabus but i would say, our actual preparation accelerates when we appear for couple of practice tests and we get an exact idea about what is being asked, what is the pattern of the questions, where we stand right now, and what we need to do next. keep below things in mind while giving practice tests: in the first round of tests, you may get score of 30 to 50% and fail( as passing score is 70%), but don’t worry about it, keep moving ahead. read every question clearly and then look at the options. even though you feel like you know the answer, check all the options and validate them. avoid pausing and resuming the test, try to complete it in one shot. once you complete the test, try to review it on the same day. check the things where you made the mistakes and list them down to rectify them later. before appearing for next practice test, prepare on points/concepts you missed in previous test. i recommend you to go through all the below practice tests from udemy: practice test by dned technologies: https://www.udemy.com/share/103oacbumzdvhtth4=/ practice tests by mert ozer: https://www.udemy.com/share/1040pmaeyfev1uq3wh/ practice test by ivan t: https://www.udemy.com/share/10417qaeyfev1uq3wh/ 6. at this stage, i consider that now, you have covered all the syllabus. you have appeared at least once for all the practice tests. you have better understanding of concepts, syntax and exam format. now, start your second round of attempts for all the practice tests. try to take at least one test per day. this time, since you are being more prepared, you might get around 80 to 90% percent of score. 7. now, you can schedule exam by clicking on “register” button at the bottom of the page below. it needs visa/credit card for payment. https://academy.databricks.com/exam/databricks-certified-associate-developer you can take test from an authorized center as well as from personal laptop. if you are planning to take an exam from your personal laptop, please go through below pages: https://kryterion.force.com/support/s/topic/0to1w000000i5h3wac/online-proctoring?language=en_us https://www.kryteriononline.com/systemcheck/ https://kryterion.force.com/support/s/?language=en_us important note: in between the test, if you lose internet connection or face some issues then immediately contact support team and they will help you. all your answers will be saved and test will be resumed on next available slot. you will get score immediately once you submit the exam, but actual certificate will be delivered to your e-mail within 7 to 10 days. i hope this article gives an overview of preparation for the certification. i will keep sharing more articles on spark’s concepts specific to this certificate. please feel free to connect with me on linkedin for any further questions. certification databricks apache spark -- -- written by tanaji sutar 16 followers · 29 following sr. data engineer at enquero no responses yet help status about careers press blog privacy rules terms text to speech",12
https://antonhaugen.medium.com/feature-selection-with-pyspark-a172d214f0b7,,,"feature selection with pyspark. this past week i was doing my first… | by anton haugen | medium sitemap open in app sign up sign in medium logo write sign up sign in feature selection with pyspark anton haugen 3 min read · mar 29, 2021 -- listen share this past week i was doing my first machine learning project for l ayla ai’s pyspark for data scientist course on udemy. while in the first lessons for classification, the accuracy scores for the models had stellar results, when using the same data cleaning, normalization, and scaling, the accuracy for these models was absolutely dismal. whenever i did machine learning projects with scikit-learn in python, i would do the feature selection and polynomial transformations in a more hands-on manner, that is whenever i wasn’t doing an nlp projects. in some cases for big data projects, you might be working with 7,000 features. fortunately, spark comes with built in feature selection tools. in most pipelines, feature selection should occur just before the modeling stage, after etl, handling imbalance, preprocessing, and importantly, the train-test split. i will be covering the three types of feature selection found in the spark documentation. vectorslicer perhaps the most straightforward of the three methods discussed here, vectorslicer takes your features array and creates a subarray from selected indices. depending on the results of your initial examination of your data, you may find that only certain columns contain meaningful information. for example, for a classification problem of medieval art and artifacts involving the entirety of the metropolitan museum of art’s entire collection, the artist column may be empty since most artifacts had no attribution and would probably cause unnecessary labor for your model algorithm; in this case, you may want to use a vectorslice to not include the index position of the art column. from pyspark.ml.feature import vectorslicer vs= vectorslicer(inputcol= “features”, outputcol=”sliced”, indices=[1,4]) output= vs.transform(df) output.select(‘userfeatures’, ‘features’).show() if your dataframe has an attributegroup, you can also use the names parameter to specify feature names. rformula for those who like to create feature interactions to make novel features, this would be the approach for you. as the name suggests, you use an r style formula to engineer new features since my knowledge of r is limited to using the formula whenever i import the ols library, i had to learn what each symbol meant: · ~ is what separates the target from the features. think of it as your equals sign. · + means to concatenate terms. if you use “+0”, this will remove the intercept. · — removes a term, “- 1” will remove the intercept · : interaction will allow you to create interactions between columns. i.e. y~ a+ b + a:b will correspond to y= w0+w1*a+w2*b +w3*a*b, where the w’s are coefficients because r formulas use feature names and outputs a feature array, you would do this before you creating your feature array. here’s what the code would look like : from pyspark.ml.feature import rformula formula=rformula(formula= “clicked ~ country+ hour”, featurescol= “features”, labelcol= “label”) output = formula.fit(dataset).transform(dataset) output.select(“features”, “label”).show() chisqselector this is the approach that i went with in my initial problem. it uses chisquare to yield the features with the most predictive power. the first of the five selection methods are numtopfeatures, which tells the algorithm the number of features you want. second is percentile, which yields top the features in a selected percent of the features. third, fpr which chooses all features whose p-value are below a inputted threshold. fourth, fdr uses the benjamini-hochberg procedure whose false discovery rate is below a threshold. and lastly, fwe chooses all p-values below threshold using a scale according to the number features. from pyspark.ml.feature import chisqselector selector=chisqselector(percentile=0.9, featurescol=”features”, outputcol=’selectedfeatures’, labelcol= “label”) model=selector.fit(train) result = model.transform(train) train =result.select('label','selectedfeatures').withcolumnrenamed('selectedfeatures', 'features') new_test=model.transform(test) test=new_test.select('label','selectedfeatures').withcolumnrenamed('selectedfeatures', 'features') feature selection can be nearly impossible manually when handling dataframes with thousands of features. mastering these techniques are vital to modeling with big data. sources: https://databricks.com/session/building-custom-ml-pipelinestages-for-feature-selection https://spark.apache.org/docs/2.2.0/ml-features.html#feature-selectors data science spark python machine learning ai -- -- written by anton haugen 34 followers · 19 following data scientist and writer no responses yet help status about careers press blog privacy rules terms text to speech",12
https://ghoshm21.medium.com/how-to-download-really-big-data-sets-for-big-data-testing-ea33b9100f09,,,"how to download really big data sets for big data testing | by sandipan ghosh | medium sitemap open in app sign up sign in medium logo write sign up sign in how to download really big data sets for big data testing sandipan ghosh 3 min read · aug 3, 2020 -- listen share for a long time, i have been working with big data technologies, like mapreduce, spark, hive, and very recently i have started working on ai/ml. for different types of bigdata framework testing and text analysis, i do have to do a large amount of data processing. we have a hadoop cluster, where we usually do this. however recently, i had a situation where i had to crunch 100 gbs of data on my laptop. i didn't have the opportunity to put this data to our cluster, since it would require a lot of approval, working with admin to get space, opening up the firewall, etc. so i took up the challenge to get it done using my laptop. my system only has 16 gb of ram and i5 processor. another challenge was i do not have admin access, so i can not install any required software without approval. however, luckily i had docker installed. for processing the data i can use spark on local mode as spark support parallel processing using cpu cores. as i5 has 4 cores and 4 threads, the spark could run the entire process on 8 parallel processes. how to get the data: yellow cab data now to the real topic, where to get the really big opensource data, which is 100gb in size? we need both structure(csv) and semistructured(json)data source1:- after little research, i found out that we can download entire yellow cab data from the nyc gov data site. here is the link this does need a little bit of effort to download the data, as all the data are split in monthly csv. each csv is 2 gb in size. so i wrote a python program that will download each month csv for the website into a local directory and will also show a little progress bar on the screen. import urllib.request from tqdm import tqdm class downloadprogressbar(tqdm): def update_to(self, b=1, bsize=1, tsize=none): if tsize is not none: self.total = tsize self.update(b * bsize - self.n) def download_url(url, output_path): with downloadprogressbar(unit= 'b' , unit_scale=true, miniters=1, desc=url.split( '/' )[-1]) as t: urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to) year = list(range(2009, 2020)) month = [ '01' , '02' , '03' , '04' , '05' , '06' , '07' , '08' , '09' , '10' , '11' , '12' ] for x, y in [(x, y) for x in year for y in month]: print( ""fetching data for %s, %s"" % (x, y)) link = ""https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_%s-%s.csv"" % (x, y) file_name = '/home/sandipan/documents/yellow_taxi/yellow_taxi_data_%s-%s.csv' % (x, y) print(link, file_name) download_url(link, file_name) json data how about the semi-structured data? well, we can use ‘open library’ data. the open library is an initiative intended to create “one web page for every book ever published.” you can download their dataset which is about 20gb of compressed data. we can download the data very easily using wget. then unzip it using unzip command. wget — continue http://openlibrary.org/data/ol_cdump_latest.txt.gz well, that's all, have fun with all the data. in my next post, i will post how to process the data locally. big data analytics data science large datasets -- -- written by sandipan ghosh 35 followers · 18 following bigdata solution architect & lead data engg with experience in building data-intensive applications,tackling challenging architectural and scalability problems. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://sailajakarra.medium.com/big-data-machine-learning-with-sparkml-part-1-simple-setup-56b2ed2c0ab3,,,"big data machine learning with sparkml — part 1 — simple setup | by sailaja karra | medium sitemap open in app sign up sign in medium logo write sign up sign in big data machine learning with sparkml — part 1 — simple setup sailaja karra 4 min read · nov 11, 2020 -- listen share in this blog i would like to show the basics of setting up spark in google colab and run a simple linear regression model using spark ml. before we jump in i would like to take a minute to explain why another ml library to learn is important. with tensorflow, pytorch & scikit learn frameworks already available why do we need another ml library. one simple answer is “spark”. we all know and love spark for making the whole big data frameworks easy esp. with spark sql wouldn’t it be better if we can somehow use the same distributed framework for ml too. we can do this using spark ml. for most of the simple tasks not having to install anything and sparks ability to run in distributed and parallel way makes it worth looking at. before we dive in, we need to install spark in colab. i specifically am talking about pyspark. here are the steps to do this in colab. install open jdk 8 on colab !sudo apt install openjdk-8-jdk 2. pip install pyspark !pip install pyspark 3. set java home environment variable import os java8_location= '/usr/lib/jvm/java-8-openjdk-amd64' os.environ['java_home'] = java8_location 4. create a sparksql session from pyspark.sql import sparksession from pyspark.sql.functions import array, col, explode, lit, struct from pyspark.sql import dataframe spark = sparksession.builder.appname('final').config(""spark.driver.memory"", ""10g"").config(""spark.executor.memory"",""10g"").getorcreate() 5. import the required sparkml #spark ml imports from pyspark.ml.regression import linearregression from pyspark.ml.linalg import vectors from pyspark.ml.feature import vectorassembler with the above steps we have spark setup on google colab and ready to do our machine learning tasks. for the ml task, i want to keep this simple in the first part and run a simple linear regression model. we are going use the boston dataset from kaggle for this. i have the dataset downloaded as csv files so this is how we load the data into spark. data = spark.read.csv('/content/drive/my drive/train.csv',header=true,inferschema=true) once we have the data in spark we can check if the loading is done correctly by looking at the schema. data.printschema() we see that all of our data is loaded correctly, at least the data types. now coming to the specific spark ml part, it seems we need to append all the feature row values into one column called ‘features’. to do this we use the vectorassembler function we imported before. #all the features columns as a list features_cols = ['crim','zn','indus','chas','nox','rm','age','dis','rad','tax', 'ptratio','b','lstat'] #vector assembler to put these all in one column va = vectorassembler(inputcols= features_cols,outputcol='features') this creates a column called ‘features’ that puts together all the features. va_boston = va.transform(data) va_boston.show() this shows all the columns including the newly created ‘features’ column. since we only need the features column and the final ie ‘mdev’ column we can use the select function to do this. va_boston_final = va_boston.select(['features','medv']) then we do the regular train & test split. i used 70/30 split for this. train,test = va_boston.randomsplit([0.70,0.30]) finally the linear regression model is instantiated with the labelcol as ‘mdev’. then we fit on the train model. lr = linearregression(labelcol='medv') lr_model = lr.fit(train) now we can evaluate the model performance on the test data. test_results = lr_model.evaluate(test) rmse = test_results.rootmeansquarederror r2 = test_results.r2 print(f'rmse = {rmse}, r2 = {r2} ') we get an rmse of 4 & r2 of 73.67%. please do note this is literally out of the box model performance without any eda or scaling of the features. we certainly can improve the model performance and that would be my next blog. happy reading !!! references google colaboratory edit description colab.research.google.com mllib: main guide - spark 3.0.1 documentation mllib is spark's machine learning (ml) library. its goal is to make practical machine learning scalable and easy. at a… spark.apache.org spark and python for big data with pyspark learn the latest big data technology - spark! and learn to use it with one of the most popular programming languages… www.udemy.com [03/24] boston housing dataset predict the price of the house in boston www.kaggle.com spark big data python -- -- written by sailaja karra 78 followers · 3 following no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@gu.martinm/pyspark-unit-integration-and-end-to-end-tests-c2ba71467d85,,Unit Testing,"pyspark: unit, integration and end-to-end tests. | by gustavo martin morcuende | medium sitemap open in app sign up sign in medium logo write sign up sign in pyspark: unit, integration and end-to-end tests. gustavo martin morcuende 6 min read · aug 30, 2020 -- listen share introduction many years ago software developers understood the benefits of testing their code. testing in the big data world is not so extended. through this article i intend to show one way of creating and running pyspark tests. there are many articles out there where it is explained how to write tests and integrate them in the ci pipelines. when working with spark i did not manage to find any good documentation or patterns that could help me to create and automate tests in the same way as i used to do with other frameworks. this article explains the way i run pyspark tests. hopefully, it will be useful for future big data developers searching ways to improve the quality of their code and at the same time their ci pipelines. unit, integration and end-to-end tests. these are the kinds of tests that we can need when working with spark. i am not mentioning others like smoke tests, acceptance tests, etc, etc because i think they are outside the scope of this article. unit tests: at this level we will be dealing with code that does not require a spark session in order to work. also, this kind of code does not talk with the outside world. integration tests: at some point we will need to use a spark session. at this level we will be testing spark transformations and in many cases we will have to deal with external systems such as databases, kafka clusters, etc, etc. end-to-end tests: our application probably will be composed of several spark transformations working together in order to implement some feature required by some user. here, we will be testing the whole application. you must be wondering why i am not drawing the typical test pyramid. well, the answer is simple: most of the time, when working with data we will be dealing with integration tests instead of unit tests so the test pyramid does not make too much sense in our case. pyspark project layout this is a strongly opinionated layout so do not take it as if it was the only and best solution. i think this layout should work under any use case but if it does not work for you, at least i hope, it will bring some inspiration or ideas to your testing implementation. ├── src │   └── awesome │       ├── app │       │   └── awesome_app.py │       ├── job │       │   └── awesome_job.py │       └── service │           └── awesome_service.py └── tests ├── endtoend │   ├── fixtures │   └── test_awesome_app.py ├── integration │   ├── fixtures │   └── test_awesome_job.py ├── shared_spark_session_helper.py └── unit └── test_awesome_service.py it is a src layout using pytest with three different packages: app, job and service for our pyspark application. for pytest we will be using three different folders: endtoend, integration and unit. application layout app package under this folder we will find the modules in charge of running our pyspark applications. typically we will have only one pyspark application. for example, in the above layout, awesome_app.py will contain the __main__ required for running the application. job package a spark application should implement some kind of transformations. modules under this package run spark jobs that require a spark session. for example, awesome_job.py could contain spark code implementing one or several transformations. service package sometimes business logic does not require a spark session in order to work. in such cases, we can implement the logic in a different module. pytest layout unit folder folder where our unit tests will reside. understanding unit tests as those that neither require a spark session in order to work nor talk with the outside world (file systems, databases, etc, etc) integration folder contains tests that require a spark session or deal with the outside world. there is also a fixture folder where we can store the required data sets for running our tests. under this path we can find tests that check spark transformations. endtoend folder this folder includes tests that run a whole pyspark application and check that results are correct. spark applications can be composed of multiple transformations, tests under this path check the application as a whole. as in the integration folder, there is a fixture folder where we can include data sets for testing our applications. shared spark session one of the biggest problems to be solved when running spark tests is the isolation of these tests. running a test should not affect the results of another. in order to achieve this goal we are going to need a spark session for each set of tests, in this way, the results of these tests will not affect others that will also require a spark session. so, we need to implement a system that will enable us to run, clear and stop a spark session whenever we need it (before and after a set of related spark tests) as stated before, we will be using pytest. pytest works with fixtures but also allows us to use a classic xunit style setup . we will be using this xunit style for our shared spark session. the details of the implementation are explained down below: setup_class method: we want to share the same spark session across a set of tests. the boundaries of this set will be represented by a test class, what means, in one test class we can share the same spark session through all the tests located in that test class. spark_conf class method: different set of tests may also need spark sessions with different configurations. the spark_conf method enables us to load a spark session with the required configuration for each set of tests. embedded hive: spark-warehouse and metastore_db are folders used by spark when enabling the hive support . different spark sessions in the same process can not use the same folders. because of that, we need to create random folders in every spark session. setup_method : creates a temporary path which is useful when our spark tests end up writing results in some location. teardown_method : clears and resets the spark session at the end of every test. also, it removes the temporary path. teardown_class method : stops the current spark session after the set of tests are run. in this way we will be able to run a new spark session if it is needed (if there is another set of tests requiring the use of spark) how it works the basic idea behind sharedsparksessionhelper lies in the fact that there is one spark session per java process and it is stored in an inheritablethreadlocal . when calling getorcreate method from sparksession.builder we end up either creating a new spark session (and storing it in the inheritablethreadlocal ) or using an existing one. so, for example, when running an end-to-end test, because sharedsparksessionhelper is loaded before anything else (by means of the setup_class method), the application under test will be using the spark session launched by sharedsparksessionhelper . once the test class is finished, the teardown_class method stops the spark session and removes it from the inheritablethreadlocal leaving our test environment ready for a new spark session. in this way, tests using spark can run in an isolated way. awesome project this article would be nothing without a real example. just following this link you will find a project with tox , pipenv , pytest , pylint and pycodestyle where i use the sharedsparksessionhelper class. the awesome project uses object-oriented programming style but the same could be applied when using procedural style, the only requirement is to write test classes which can be used with the sharedsparksessionhelper class. many times you will end up using either object-oriented or procedural style or both at the same time in your python projects. feel free to use one or another depending on what you need. of course this application can be run in any of the available clusters that currently exist such as kubernetes , apache hadoop yarn , spak running in cluster mode or any other of your choice. conclusion testing spark applications can seem more complicated than with other frameworks not only because of the need of preparing a data set but also because of the lack of tools that allow us to automate such tests. by means of the sharedsparksessionhelper class we can automate our tests in an easy way and it should work smoothly with pytest and unittest . i hope this article was useful. if you enjoy messing around with big data, microservices, reverse engineering or any other computer stuff and want to share your experiences with me, just follow me . spark pyspark pytest unit testing end to end testing no rights reserved by the author. -- -- written by gustavo martin morcuende 116 followers · 185 following jack of all trades, master of none. https://gumartinm.name https://twitter.com/gumartinm https://www.linkedin.com/in/gustavo-martin-morcuende no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/agile-lab-engineering/how-to-create-an-apache-spark-3-0-development-cluster-on-a-single-machine-using-docker-964478c3735b,,,"how to create an apache spark 3.0 development cluster on a single machine using docker | by mario cartia | agile lab engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in agile lab engineering · ideas go software! how to create an apache spark 3.0 development cluster on a single machine using docker mario cartia 3 min read · sep 23, 2020 -- 2 listen share apache spark is the most widely used in-memory parallel distributed processing framework in the field of big data advanced analytics. the main reasons for its success are the simplicity of use of its api and the rich set of features ranging from those for querying the data lake using sql to the distributed training of complex machine learning models through the use the most popular algorithms. given this simplicity of using its api, however, one of the most frequently problem encountered by developers, similar to what happens with most distributed systems, is the creation of a development environment where you can test your applications by simulating the execution of code on multiple nodes of a cluster. although spark provides a local execution mode, it may hide a number of issues due to the distributed mechanism of code execution, making testing ineffective. that’s why the most effective way to create a test environment that is more like a production cluster environment is to use docker containers. an additional benefit of using this approach is that you can test your applications on different versions of the framework by simply changing a few parameters in the configuration file. in our example we will use version 3.0, the test cluster hypothetically could be useful in testing the compatibility of our applications with the recently released major version as well as to test the new features introduced. spark is designed so that you can run on different types of clusters. this is done by supporting several cluster managers such as yarn, the hadoop platform resource manager, mesos or kubernetes. if an existing cluster infrastructure is not available, spark can run on an integrated resource manager/scheduler. this is commonly referred as “ standalone ” cluster mode. in our example, we’ll create a cluster consisting of a master node and 3 worker nodes like the one in the image below. to setup our cluster, we will use the images created by the developers of the open source project “ big data europe ”, whose sources are available on github: https://github.com/big-data-europe/docker-spark . we’ll make small changes to the docker-compose.yml configuration file to size the number of nodes and most importantly to add a persistent volume for reading/writing data during our experiments. this is the configuration file that we’re going to use. you can then start the cluster and run a shell on the master node once it starts. and finally launch our spark-shell. at the time of testing i am using a laptop with a cpu with 8 cores and 16gb of ram. that’s why i allocated 2 cores for each executor (6 in total) and 2.5 gb of ram. our development cluster is ready, have fun! big data spark docker docker compose development -- -- 2 published in agile lab engineering 287 followers · last published jun 20, 2025 ideas go software! written by mario cartia 238 followers · 229 following old school developer, veteran system administrator, technology lover and jazz piano player. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@yesilliali/install-big-data-components-hadoop-sqoop-hive-spark-onto-ubuntu-image-part-1-3c766d90412b,,,"install big data components (hadoop, sqoop, hive, spark) onto ubuntu image — part-1 | by ali yesilli | medium sitemap open in app sign up sign in medium logo write sign up sign in install big data components (hadoop, sqoop, hive, spark) onto ubuntu image — part-1 ali yesilli 9 min read · jun 29, 2020 -- 3 listen share i was needed a big data environment with the latest versions of hadoop and hive for testing a project. i built a big data environment from scratch so i want to share it to help someone who needs a big data environment for testing or training. i am going to try to explain every steps as easy as i can. in this chapter, i will tell prerequisites and installation of hadoop 3. i will explain how to install hive, sqoop and spark in part-2. i will use ubuntu image but it works with both ubuntu desktop and vm, just follow the sequence in order. here the list of the technologies and versions that will be installed; ubuntu 20.04 apache hadoop 3.2.1 mysql 8.0 sqoop 1.4.7 apache hive 3.1.2 apache spark 3.0.0 please be aware of the versions of the components. i am going to install the latest stable versions of the each technologies at the moment. but if you read this article later on, you should check the versions because there will may newer versions and there will may conflict between versions of the technologies or there will may some additional adjustments. so it is very important to check the versions before install them. let’s start! 1- run an ubuntu image in docker as i said i am going to use an ubuntu image but if you have an ubuntu desktop or vm, you can skip this part. and continue from 2. step. if you don’t have any knowledge about docker, no worries. docker is a set of platform as a service products that uses os-level virtualization to deliver software in packages called containers. i am not going to explain docker technology in this article because it is not our focus now. i just give you the docker commands that i will use. if you don’t have docker you can download from its website empowering app development for developers | docker the world's leading service for finding and sharing container images with your team and the docker community. fo www.docker.com it is very easy to download and install docker on your desktop. if you will doubt about installation, you can find a lot of documents on internet. now, i assume that you install docker on your desktop successfully. so it is time to use it. open the terminal docker --version if you can see the version information, it means docker is running and ready to use. before run ubuntu image it is good to create a network for hadoop so we can check hadoop logs through the network on our desktop docker network create — driver=bridge hadoop now check list of networks docker network ls so we can run the ubuntu image docker run -it \ — net=hadoop \ — name big_data \ — hostname bigdata-master \ ubuntu i want the tell about the code above. -it is short for --interactive , it means it will connect to bash whenever it will create the container so we can run the commands directly. in second line we define network as hadoop which we created before. i gave the name of container as big_data and hostname as bigdata-master, you can give any names you want. and the last line we give the image that we want to use. here we will use ubuntu. i didn’t specify the version of the ubuntu so it will download the latest version. as you can see above screenshot, due to ubuntu image didn’t exist, it downloaded it first and build the container. we connected the container with root user and everything is ready to use 2- install hadoop 3 before starting to install hadoop, we should run an update and install some applications which will be needed. apt-get update apt-get install -y openssh-client openssh-server vim sudo ssh unzip and it is better to create a user for hadoop instead of using root. so we create a user and give it sudo rights adduser hadoop after you run adduser command, it will ask you the password. i enter the password as “hadoop” user = hadoop password = hadoop we should give it sudo rights as well for further operations adduser hadoop sudo user is ready, let’s switch to hadoop user su — hadoop ok now, we need to install java 8, it is most stable version of the java for all the big data components. if you have any other installed java version, it would be better if you remove it. it might cause a conflict sudo apt install openjdk-8-jdk openjdk-8-jre check java version java is installed, next step is that setting java_home and jre_home environment variables vim ~/.bashrc add below codes at the end of the lines; export java_home=/usr/lib/jvm/java-8-openjdk-amd64 export jre_home=/usr/lib/jvm/java-8-openjdk-amd64/jre if you are using vi or vim editor, you can enter capital g, g , to go to latest line. then you can use i or a to adjust the text. after you enter variables you must save it. so to quit with save, type : first , then wq and enter. last thing we must run below command to refresh the variables source ~/.bashrc java is ready so we can start to download and configure hadoop. go to apache hadoop website and download the latest version apache hadoop first beta release of apache hadoop ozone with gdpr right to erasure, network topology awareness, o3fs, and improved… hadoop.apache.org in download page you can check the versions, at the moments, latest version is 3.2.1 so we are going to install it. click the binary link that you can see below screenshot copy the download link in the next page. then run below command to download the file. be careful, mirrors may be different so below command may not work. change the download link with the one that you copied from the download page wget http://apache.mirrors.spacedump.net/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz hadoop is downloaded! now we can start to configuration. run below commands to extract the files and move it. tar -xzf hadoop-3.2.1.tar.gz sudo mv hadoop-3.2.1 /usr/local/hadoop sudo chmod 777 /usr/local/hadoop next step is setting up hadoop variables like we did before for java. vim ~/.bashrc insert below variables at the bottom export hadoop_home=/usr/local/hadoop export hadoop_install=$hadoop_home export hadoop_mapred_home=$hadoop_home export hadoop_common_home=$hadoop_home export hadoop_hdfs_home=$hadoop_home export hadoop_yarn_home=$hadoop_home export yarn_home=$hadoop_home export hadoop_common_lib_native_dir=$hadoop_home/lib/native export path=$path:$hadoop_home/sbin:$hadoop_home/bin don’t forget to save it with wq, after save it, run below command again source ~/.bashrc we need to create two folders for namenode and datanode. and also one folder for logs. run below commands to create folders mkdir -p $hadoop_home/hdfs/namenode mkdir -p $hadoop_home/hdfs/datanode mkdir $hadoop_home/logs now we are going to make some adjustments in configuration files below. hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml log4j.properties first change the directory to configuration folder cd $hadoop_home/etc/hadoop add below java home to hadoop-env.sh vim hadoop-env.sh export java_home=/usr/lib/jvm/java-8-openjdk-amd64 add below property to core-site.xml vim core-site.xml <configuration> <property> <name>fs.defaultfs</name> <value>hdfs://bigdata-master:9000/</value> </property> </configuration> add below properties to hdfs-site.xml vim hdfs-site.xml <configuration> <property> <name>dfs.namenode.name.dir</name> <value>file:///usr/local/hadoop/hdfs/namenode</value> <description>namenode directory for namespace and transaction logs storage.</description> </property> <property> <name>dfs.datanode.data.dir</name> <value>file:///usr/local/hadoop/hdfs/datanode</value> <description>datanode directory</description> </property> <property> <name>dfs.replication</name> <value>2</value> </property> </configuration> add below properties to mapred-site.xml vim mapred-site.xml <configuration> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> <property> <name>yarn.app.mapreduce.am.env</name> <value>hadoop_mapred_home=${hadoop_home}</value> </property> <property> <name>mapreduce.map.env</name> <value>hadoop_mapred_home=${hadoop_home}</value> </property> <property> <name>mapreduce.reduce.env</name> <value>hadoop_mapred_home=${hadoop_home}</value> </property> </configuration> add below properties to yarn-site.xml vim yarn-site.xml <configuration> <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> <property> <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name> <value>org.apache.hadoop.mapred.shufflehandler</value> </property> <property> <name>yarn.resourcemanager.hostname</name> <value>bigdata-master</value> </property> </configuration> last adjustment in log4j.properties. i want to change log level to error . so add below script to the file. otherwise there are a lot of logs return when you run hadoop commands. it is not necessary, you can skip it, if you want to see all logs vim log4j.properties log4j.logger.org.apache.hadoop.util.nativecodeloader=error do not forget to save the changes before exit the text editor for all the files above. we did all the needed adjustments for configuration. next step is that creating an ssh keygen. run below commands for it ssh-keygen -t rsa -p ‘’ -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys it is ready to format namenode but before that we should adjust port information if you are using docker container as like me. we need to change port 22. because using port 22 might cause a conflict in docker. so i am going to change port 22 to 2222. for do this, adjust ssh_config and sshd_config files and add the row port 2222. if you are using ubuntu desktop or vm, skip these steps. sudo vim /etc/ssh/ssh_config port 2222 same thing for sshd_config file; sudo vim /etc/ssh/sshd_config port 2222 last thing for ssh config is creating a config file add below command; vim ~/.ssh/config host * stricthostkeychecking no we are ready to run below command to format namenode; hdfs namenode -format now it is time to restart ssh, after that we can start hadoop; sudo /etc/init.d/ssh restart run start-all.sh command to start hadoop; as you can see all namenames, datanodes, secondary namenodes, resourcemanager and nodemanagers are started successfully. let’s check it perfect! hadoop is ready to use. now let’s create a folder into hdfs for test; hadoop fs -mkdir /ali everything looks good. we can also check the hadoop version so we installed hadoop 3 onto ubuntu successfully. now we can install any other big data components. i will explain how to install hive 3 and spark 3 onto ubuntu into next part. i will add the link when part-2 will be ready. hadoop 3 hive spark big data hadoop -- -- 3 written by ali yesilli 65 followers · 3 following https://www.linkedin.com/in/aliyesilli/ responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://vivianamarquez.medium.com/is-koalas-the-new-pandas-843a0f7b9003,,,"is koalas the new pandas?. well… not just yet! | by viviana márquez | medium sitemap open in app sign up sign in medium logo write sign up sign in is koalas the new pandas? viviana márquez 4 min read · jun 29, 2020 -- 1 listen share well… not just yet! last week, during the spark + ai summit 2020, koalas 1.0 was released . here is the demo in case you are curious. koalas is bringing to the data science world the promise of translating the pandas api into pyspark seamlessly. a dream come true! 🤩 right? pandas is one of the most used tools for data wrangling and analysis, however, it does not scale well when dealing with big data sets. at the moment, when faced with big data, most data scientist have to either migrate to pyspark (which has a significantly different syntax) or sample their data to be able to use pandas. therefore, koalas will bring the best of both worlds by allowing data scientists to scale their projects using the good old pandas syntax. does that mean that…? one brilliant twitter user asked if that means that we can just do: import databricks.koalas as pd 🔥🐨🐼 in this post, we will put that to the test. installing koalas first, you have to install koalas using: pip install databricks pip install koalas (note: you also have to have spark installed in your environment.) let’s compare! before i begin, let me clarify that in this post i will test whether or not koalas has the same functionalities as pandas, not its speed. path = ""sample.csv"" 1. test #1: loading a csv file 🐼 pandas 🐨 koalas uh, oh! koalas has trouble reading some of the lines that contain a comma. 2. test#2: sample your dataset 🐼 pandas 🐨 koalas uh, oh! according to koalas documentation, they have an n parameter for the number of items to return but it is still not supported, so you have to use frac instead, which will return a fraction of items on the selected axis. 3. test#3: dropping and renaming columns 🐼 pandas 🐨 koalas uh, oh! in koalas the parameter inplace is not implemented, therefore you have to reassign the result to your variable. 4. test#4: dealing with datetime columns 🐼 pandas 🐨 koalas koalas is not happy about that syntax, but it also got the job done. 5. test#5: lambda time! 🐼 pandas 🐨 koalas seamless! this is really, really nice because using pyspark we would have had to create an ugly udf (user-defined function). 6. test#6: describe 🐼 pandas 🐨 koalas 7. test#7: value counts 🐼 pandas 🐨 koalas koalas cried again, but it did the job! conclusions koalas is still far from being a 100% seamless transition from pandas to spark, however, this is just its first release and i am hopeful that the open-source community will make out of koalas one of the most powerful tools in the python world 🔥 on a side note, koalas is very slow when dealing with small datasets. this is expected to happen as spark is meant to deal with big datasets, nevertheless, i would love to see koalas getting smart enough to switch under the hood between pandas and spark depending on the context. pandas koalas spark pyspark apache spark -- -- 1 written by viviana márquez 182 followers · 65 following 🤖👩‍💻 edtech founder @missfactorial 📊🎞️ making ai & data as captivating as your favorite series 🎤 miss sucre 2021 & tv/radio host responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://mageswaran1989.medium.com/big-data-play-ground-for-engineers-hive-and-metastore-15a977169eb7,,Unit Testing,"big data play ground for engineers : hive and metastore | by mageswaran d | medium sitemap open in app sign up sign in medium logo write sign up sign in big data play ground for engineers : hive and metastore this is part of the series called big data playground for engineers and the content page is here! mageswaran d 9 min read · apr 15, 2020 -- listen share git: a fully functional code base and use case examples are up and running. repo : https://github.com/gyan42/spark-streaming-playground website: https://gyan42.github.io/spark-streaming-playground/build/html/index.html after this you can setup common metastore between spark streaming, spark sql and hive, thus enabling cross tooling query capability. from apache spark you can read hive tables and vice versa! what is hive? apache hive is an open source data warehouse system built on top of hadoop for querying and analyzing large datasets stored in hadoop files. it process structured and semi-structured data in hadoop. in tradition database store, both the computing and data lives in the same machine, forcing people to go with big machine every time they reach the bottleneck with the business requirements for more power and data storage. with hadoop in the picture, people decouples the data and data base query engine to take the advantage of distributed computing. these are similar to traditional sql queries but with extra power and tweaks to handle data in tera bytes! apache hive - in depth hive tutorial for beginners - dataflair apache hive is an open source data warehouse system built on top of hadoop haused for querying and analyzing large… data-flair.training in short, data lives in distributed storage system like hdfs/s3/gfs and the compute power comes from the cluster backed by hadoop mapreduce (vanishing these days), apache spark or some custom build distributed query engines with custom meta store. refer this link as part of the repo to setup hive in your local machine: https://gyan42.github.io/spark-streaming-playground/build/html/setup/apachehive.html what hive meta store? as we know, the database is among the most important and powerful parts of any organization. it is the collection of schema, tables, relationships, queries, and views. it is an organized collection of data. metastore is the central repository of apache hive metadata. it stores metadata for hive tables (like their schema and location) and partitions in a relational database. it provides client access to this information by using metastore service api. all hive implementations need a metastore service, where it stores metadata. it is implemented using tables in a relational database. by default, hive uses a built-in derby sql server. it provides single process storage, so when we use derby, we cannot run instances of hive cli. whenever we want to run hive on a personal machine or for some developer task, then it is good, but when we want to use it in a cluster, then mysql or any other similar relational database is required. hive metastore consists of two fundamental units: 1. a service that provides metastore access to other apache hive services. 2. disk storage for the hive metadata which is separate from hdfs storage. hive metastore modes there are three modes for hive metastore deployment: · embedded metastore · local metastore · remote metastore 1. embedded metastore in hive by default, metastore service runs in the same jvm as the hive service. it uses embedded derby database stored on the local file system in this mode. thus both metastore service and hive service runs in the same jvm by using embedded derby database. but, this mode also has limitation that, as only one embedded derby database can access the database files on disk at any one time, so only one hive session could be open at a time. if we try to start the second session it produces an error when it attempts to open a connection to the metastore. so, to allow many services to connect the metastore, it configures derby as a network server. this mode is good for unit testing. but it is not good for the practical solutions. 2 . local metastore hive is the data-warehousing framework, so hive does not prefer single session. to overcome this limitation of embedded metastore, for local metastore was introduced. this mode allows us to have many hive sessions i.e. many users can use the metastore at the same time. we can achieve by using any jdbc compliant like mysql which runs in a separate jvm or different machines than that of the hive service and metastore service which are running in the same jvm. hive is the data-warehousing framework, so hive does not prefer single session. to overcome this limitation of embedded metastore, for local metastore was introduced. this mode allows us to have many hive sessions i.e. many users can use the metastore at the same time. we can achieve by using any jdbc compliant like mysql which runs in a separate jvm or different machines than that of the hive service and metastore service which are running in the same jvm. 3. remote metastore moving further, another metastore configuration called remote metastore . in this mode, metastore runs on its own separate jvm, not in the hive service jvm. if other processes want to communicate with the metastore server they can communicate using thrift network apis. we can also have one more metastore servers in this case to provide more availability. this also brings better manageability/security because the database tier can be completely firewalled off. and the clients no longer need share database credentials with each hiver user to access the metastore database. to use this remote metastore, you should configure hive service by setting hive.metastore.uris to the metastore server uri(s). metastore server uris are of the form http://thrift://host:port , where the port corresponds to the one set by metastore_port when starting the metastore server. databases supported by hive hive supports 5 backend databases which are as follows: · derby · mysql · ms sql server · oracle · postgres postgresql as backend for hive metastore here is a good comparison: postgresql vs mysql setup: #install ubuntu packages sudo apt-get install postgresql postgresql-contrib # check the version sudo -u postgres psql -c ""select version();"" # test the installation sudo su - postgres psql #to launch the terminal \q #to quit # or to run psql directly sudo -i -u postgres psql to cut short the permission configurations for new users, lets create a ubuntu user with same name. we need log into postgressql to create users before they can use the db. for our case we are going ot create a user called ` hive ` and db called ` hive ` sudo adduser hive #password hive sudo su - postgres psql #to launch the terminal create user hive with password 'hive'; # drop user hive; (if needed) \du create database hive; grant all privileges on database hive to hive; \list # to see the db created \q we need do some changes to connection configs, to enable login from different services: # change the 3rd colum values to ""all"" sudo vim /etc/postgresql/10/main/pg_hba.conf # ""local"" is for unix domain socket connections only local   all   all                                     md5 # ipv4 local connections: host    all   all             127.0.0.1/32            md5 # ipv6 local connections: host    all   all             ::1/128                 md5 if you would like to restart the service: sudo service postgresql restart sudo systemctl restart postgresql to see how many active connections to the db are made, helps you to debug when many connection are made to db. select pid, application_name, state from pg_stat_activity; this is important some times to debug the connections, as it becomes stale and hangs there if connections are not closed properly, making new connection bounce back. use hive provided tool to setup the metastore tables an schema: /path/to/hive/bin/schematool -dbtype postgres -initschema and then try running following commands, you should see bunch of tables there: sudo -i -u hive  psql -d hive #asks for two passwords, one for sudo and other one for db `hive` which is `hive` hive=> \dt fire a python shell and test out the connection pip install psycopg2==2.8.3 python import psycopg2 conn = psycopg2.connect(host=""localhost"", port=5432, database=""hive"", user=""hive"", password=""hive"") sql_command = ""select * from \""cds\"";"" print (sql_command) # load the data data = pd.read_sql(sql_command, conn) print(data) #>>> output empty dataframe columns: [cd_id] index: [] integration with spark part of our work flow we need to read and write to postresql from apache spark. lets test whether we can do it from local ` pyspark ` terminal. #it is mandate to give the postgres maven ids for runtime pyspark --packages postgresql:postgresql:9.1-901-1.jdbc4 #read the table ""authors"" df = spark.read. \ format(""jdbc""). \ option(""url"", ""jdbc:postgresql://localhost:5432/hive""). \ option(""schema"", ""public""). \ option(""dbtable"", ""\""cds\""""). \ option(""user"", ""hive""). \ option(""password"", ""hive""). \ option(""driver"", ""org.postgresql.driver""). \ load() # display the table df.printschema() and now comes the configuration part of hive and spark to use common metastore… following config is important since spark and hive has different versions of metastore schema: hive.metastore.schema.verification is diabled, to make metastore compatible between different services. path/to/hive/cong/hive-site.xml <configurations> <property> <name>javax.jdo.option.connectiondrivername</name> <value>org.postgresql.driver</value> <description>postgresql metastore driver class name</description> </property> <property> <name>javax.jdo.option.connectionurl</name> <value>jdbc:postgresql://localhost:5432/hive</value> </property> <property> <name>javax.jdo.option.connectionusername</name> <value>hive</value> <description>username to use against metastore database</description> </property> <property> <name>javax.jdo.option.connectionpassword</name> <value>hive</value> <description>password to use against metastore database</description> </property> <property> <name>hive.metastore.schema.verification</name> <value>false</value> <description>to disable the schema check, so that spark and hive can work together</description> </property> <property> <name>hive.server2.thrift.port</name> <value>10001</value> <description>tcp port number to listen on, default 10000</description> </property> <property> <name>hive.server2.enable.doas</name> <value>false</value> <description> setting this property to true will have hiveserver2 execute hive operations as the user making the calls to it. </description> </property> </configurations> /path/to/spark/conf/hive-site.xml <?xml version=""1.0""?> <?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?> <!-- licensed to the apache software foundation (asf) under one or more contributor license agreements.  see the notice file distributed with this work for additional information regarding copyright ownership. the asf licenses this file to you under the apache license, version 2.0 (the ""license""); you may not use this file except in compliance with the license.  you may obtain a copy of the license at http://www.apache.org/licenses/license-2.0 unless required by applicable law or agreed to in writing, software distributed under the license is distributed on an ""as is"" basis, without warranties or conditions of any kind, either express or implied. see the license for the specific language governing permissions and limitations under the license. --> <configuration> <property> <name>hive.server2.enable.doas</name> <value>false</value> <description> setting this property to true will have hiveserver2 execute hive operations as the user making the calls to it. </description> </property> <property> <name>hive.metastore.schema.verification</name> <value>false</value> <description>to disable the schema check, so that spark and hive can work together</description> </property> <property> <name>javax.jdo.option.connectiondrivername</name> <value>org.postgresql.driver</value> <description>postgresql metastore driver class name</description> </property> <property> <name>javax.jdo.option.connectionurl</name> <value>jdbc:postgresql://localhost:5432/hive</value> </property> <property> <name>javax.jdo.option.connectionusername</name> <value>hive</value> <description>username to use against metastore database</description> </property> <property> <name>javax.jdo.option.connectionpassword</name> <value>hive</value> <description>password to use against metastore database</description> </property> </configuration> now go a head and create db/tables in spark and see it in hive or vice versa. spark thrift server: thrift server enable rest endpoint to spark, hosting itself as a running application in the spark cluster. it can be thought as a distributed sql engine with an rest end point. spark doc @ https://spark.apache.org/docs/latest/sql-distributed-sql-engine.html sbin/start-thriftserver.sh \ --master spark://imchlt276:7077 \ --hiveconf hive.server2.thrift.bind.host=localhost \ --hiveconf hive.server2.thrift.port=10000 \ --executor-memory 2g \ --conf spark.jars=libs/postgresql-42.2.10.jar \ --conf spark.cores.max=2 test it with `beeline` client, user will be your machine login user and empty password. /path/to/spark/bin/beeline !connect jdbc:hive2://localhost:10000 hive thrift server: use beeline from hive package command to connect to hive thrift server (assuming it is already running), port 10001 is already configured in /path/to/spark/conf/hive-site.xml /path/to/hive/bin/beeline -n ssp -u jdbc:hive2://localhost:10001 references: https://www.quora.com/what-is-hive-metastore https://www.tecmint.com/install-postgresql-on-ubuntu/ https://linuxize.com/post/how-to-install-postgresql-on-ubuntu-18-04/ https://medium.com/@thomaspt748/how-to-upsert-data-into-relational-database-using-spark-7d2d92e05bb9 https://linuxize.com/post/how-to-create-a-sudo-user-on-ubuntu/ https://stackoverflow.com/questions/21898152/why-cant-you-start-postgres-in-docker-using-service-postgres-start https://markheath.net/post/exploring-postgresql-with-docker spark hive metastore database spark streaming -- -- written by mageswaran d 377 followers · 19 following a simple guy in pursuit of of ai and deep learning with big data tools :) @ https://www.linkedin.com/in/mageswaran1989/ no responses yet help status about careers press blog privacy rules terms text to speech",12
https://informationit27.medium.com/what-are-the-steps-for-mapreduce-in-big-data-2572cfb95a8d,,,"what are the steps for mapreduce in big data? | by multitech | medium sitemap open in app sign up sign in medium logo write sign up sign in what are the steps for mapreduce in big data? multitech 7 min read · sep 30, 2020 -- listen share what is mapreduce? a mapreduce is a data processing tool which is used to process the data parallelly in a distributed form. it was developed in 2004, on the basis of paper titled as “mapreduce: simplified data processing on large clusters,” published by google. the mapreduce is a paradigm which has two phases, the mapper phase, and the reducer phase. in the mapper, the input is given in the form of a key-value pair. the output of the mapper is fed to the reducer as input. the reducer runs only after the mapper is over. the reducer too takes input in key-value format, and the output of reducer is the final output.to more info visit: big data online training steps in map reduce the map takes data in the form of pairs and returns a list of <key, value> pairs. the keys will not be unique in this case. using the output of map, sort and shuffle are applied by the hadoop architecture. this sort and shuffle acts on these list of <key, value> pairs and sends out unique keys and a list of values associated with this unique key <key, list(values)>. an output of sort and shuffle sent to the reducer phase. the reducer performs a defined function on a list of values for unique keys, and final output <key, value> will be stored/displayed. sort and shuffle the sort and shuffle occur on the output of mapper and before the reducer. when the mapper task is complete, the results are sorted by key, partitioned if there are multiple reducers, and then written to disk. using the input from each mapper <k2,v2>, we collect all the values for each unique key k2. this output from the shuffle phase in the form of <k2, list(v2)> is sent as input to reducer phase. usage of mapreduce it can be used in various application like document clustering, distributed sorting, and web link-graph reversal. it can be used for distributed pattern-based searching. we can also use mapreduce in machine learning. it was used by google to regenerate google’s index of the world wide web. it can be used in multiple computing environments such as multi-cluster, multi-core, and mobile environment. prerequisite before learning mapreduce, you must have the basic knowledge of big data. audience our mapreduce tutorial is designed to help beginners and professionals. problem we assure that you will not find any problem in this mapreduce tutorial. but if there is any mistake, please post the problem in contact form. data flow in mapreduce mapreduce is used to compute the huge amount of data . to handle the upcoming data in a parallel and distributed form, the data has to flow from various phases. data flow in mapreduce phases of mapreduce data flow input reader the input reader reads the upcoming data and splits it into the data blocks of the appropriate size (64 mb to 128 mb). each data block is associated with a map function. once input reads the data, it generates the corresponding key-value pairs. the input files reside in hdfs. map function the map function process the upcoming key-value pairs and generated the corresponding output key-value pairs. the map input and output type may be different from each other. partition function the partition function assigns the output of each map function to the appropriate reducer. the available key and value provide this function. it returns the index of reducers. shuffling and sorting the data are shuffled between/within nodes so that it moves out from the map and get ready to process for reduce function. sometimes, the shuffling of data can take much computation time. the sorting operation is performed on input data for reduce function. here, the data is compared using comparison function and arranged in a sorted form. reduce function the reduce function is assigned to each unique key. these keys are already arranged in sorted order. the values associated with the keys can iterate the reduce and generates the corresponding output. output writer once the data flow from all the above phases, output writer executes. the role of output writer is to write the reduce output to the stable storage. mapreduce api in this section, we focus on mapreduce apis. here, we learn about the classes and methods used in mapreduce programming. mapreduce mapper class in mapreduce, the role of the mapper class is to map the input key-value pairs to a set of intermediate key-value pairs. it transforms the input records into intermediate records. these intermediate records associated with a given output key and passed to reducer for the final output. mapreduce word count example in mapreduce word count example, we find out the frequency of each word. here, the role of mapper is to map the keys to the existing values and the role of reducer is to aggregate the keys of common values. so, everything is represented in the form of key-value pair.if you are intrested to learn complete course visit itguru’s : big data and hadoop online training pre-requisite java installation — check whether the java is installed or not using the following command. java -version hadoop installation — check whether the hadoop is installed or not using the following command. hadoop version mapreduce word count example in mapreduce word count example, we find out the frequency of each word. here, the role of mapper is to map the keys to the existing values and the role of reducer is to aggregate the keys of common values. so, everything is represented in the form of key-value pair. pre-requisite java installation — check whether the java is installed or not using the following command. java -version hadoop installation — check whether the hadoop is installed or not using the following command. hadoop version steps to execute mapreduce word count example create a text file in your local machine and write some text into it. $ nano data.txt mapreduce word count example check the text written in the data.txt file. $ cat data.txt mapreduce word count example in this example, we find out the frequency of each word exists in this text file. create a directory in hdfs, where to kept text file. $ hdfs dfs -mkdir /test upload the data.txt file on hdfs in the specific directory. $ hdfs dfs -put /home/codegyani/data.txt /test mapreduce word count example write the mapreduce program using eclipse. file: wc_mapper.java package com.javatpoint; import java.io.ioexception; import java.util.stringtokenizer; import org.apache.hadoop.io.intwritable; import org.apache.hadoop.io.longwritable; import org.apache.hadoop.io.text; import org.apache.hadoop.mapred.mapreducebase; import org.apache.hadoop.mapred.mapper; import org.apache.hadoop.mapred.outputcollector; import org.apache.hadoop.mapred.reporter; public class wc_mapper extends mapreducebase implements mapper<longwritable,text,text,intwritable>{ private final static intwritable one = new intwritable(1); private text word = new text(); public void map(longwritable key, text value,outputcollector<text,intwritable> output, reporter reporter) throws ioexception{ string line = value.tostring(); stringtokenizer tokenizer = new stringtokenizer(line); while (tokenizer.hasmoretokens()){ word.set(tokenizer.nexttoken()); output.collect(word, one); } } } file: wc_reducer.java package com.javatpoint; import java.io.ioexception; import java.util.iterator; import org.apache.hadoop.io.intwritable; import org.apache.hadoop.io.text; import org.apache.hadoop.mapred.mapreducebase; import org.apache.hadoop.mapred.outputcollector; import org.apache.hadoop.mapred.reducer; import org.apache.hadoop.mapred.reporter; public class wc_reducer extends mapreducebase implements reducer<text,intwritable,text,intwritable> { public void reduce(text key, iterator<intwritable> values,outputcollector<text,intwritable> output, reporter reporter) throws ioexception { int sum=0; while (values.hasnext()) { sum+=values.next().get(); } output.collect(key,new intwritable(sum)); } } file: wc_runner.java package com.javatpoint; import java.io.ioexception; import org.apache.hadoop.fs.path; import org.apache.hadoop.io.intwritable; import org.apache.hadoop.io.text; import org.apache.hadoop.mapred.fileinputformat; import org.apache.hadoop.mapred.fileoutputformat; import org.apache.hadoop.mapred.jobclient; import org.apache.hadoop.mapred.jobconf; import org.apache.hadoop.mapred.textinputformat; import org.apache.hadoop.mapred.textoutputformat; public class wc_runner { public static void main(string[] args) throws ioexception{ jobconf conf = new jobconf(wc_runner.class); conf.setjobname(“wordcount”); conf.setoutputkeyclass(text.class); conf.setoutputvalueclass(intwritable.class); conf.setmapperclass(wc_mapper.class); conf.setcombinerclass(wc_reducer.class); conf.setreducerclass(wc_reducer.class); conf.setinputformat(textinputformat.class); conf.setoutputformat(textoutputformat.class); fileinputformat.setinputpaths(conf,new path(args[0])); fileoutputformat.setoutputpath(conf,new path(args[1])); jobclient.runjob(conf); } } download the source code. create the jar file of this program and name it countworddemo.jar. run the jar file hadoop jar /home/codegyani/wordcountdemo.jar com.javatpoint.wc_runner /test/data.txt /r_output the output is stored in /r_output/part-00000 mapreduce word count example now execute the command to see the output. hdfs dfs -cat /r_output/part-00000 mapreduce word count example to more info visit: big data hadoop course big data mapreduce hadoop technology technology news -- -- written by multitech 74 followers · 2 following big data,ios,android,spark no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/agile-lab-engineering/spark-remote-debugging-371a1a8c44a8,,,"spark remote debugging. hi everybody! i’m a big data engineer @… | by lorenzo pirazzini | agile lab engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in agile lab engineering · ideas go software! spark remote debugging lorenzo pirazzini 8 min read · dec 15, 2020 -- listen share hi everybody! i’m a big data engineer @ agile lab , a remote-first big data engineering and r&d firm located in italy. our main focus is to build big data and ai systems, in a very challenging — yet awesome — environment. in agile lab we use in various projects, among other technologies, apache spark as a processing engine. spark is fast and it is simple for a developer to write some code that can be run right away, but as for regular programs, it is not always easy to understand what a spark job is doing. this article will focus on how a developer can remotely debug a running spark scala/java application (running on yarn ) using intellij idea , but all the spark and environment configurations hold also for other ides. agent jdwp, licence to debug to perform remote debugging of a spark job, we leverage the jdwp agent (java debug wire protocol) that defines a communication protocol between a debugger and a running jvm. jdwp defines only the format and layout of packets exchanged by the debugger and the target jvm, while the transport protocol can be chosen by the user. usually, the available transport mechanisms are shared memory ( dt_shmem ) and socket ( dt_socket ) but only the latter, which uses a tcp socket connection to communicate, can be used for remote debugging. so in order to enable remote debugging, we must configure the target jvm with the following java property in order to make it acting as a jdwp server to which our ide can connect: -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4747 the property above tells the jvm to load the jdwp agent and wait for a socket connection on the specified port. in particular: transport=dt_socket tells the agent to use socket as the desired transport mechanism. server=y means that the jvm will act a jdwp server: it will listen for a debugger client to attach to it. suspend=y tells the jvm if it must wait for a debugger connection before executing the main function. if this is set to false (n), the main function will start while listening for the debugger connection anyway. address=4747 specifies the port at which the debug socket will listen on. in the example, the target jvm will listen on port 4747 for incoming client connections. we will leverage the jdwp agent for all the following remote debugging scenarios, so remember that you can always adjust the configurations listed above to fit your use case. you must choose your spark deployment …but choose wisely before delving into the debug of your application, here’s a quick recap of how a spark job executes on a cluster; each spark job requires: * a process called driver that performs all the standard java code * one or more executor processes that will perform all the code defined inside the transformations and actions of the rdds/datasets. this means that in a realistic scenario we will have different jvms running at the same time (often on different nodes): one for the driver and one for each executor . a typical spark application will run on multiple nodes, each containing one or more processes spark allows the developer to run a job in different modes depending on the requirement of the desired use case. in particular, we will focus on two configurations of the spark-submit command: deploy-mode and master . those two configurations allow a developer to decide how the spark application will be deployed and run: master : this configuration tells spark which is the master url of the cluster. a complete list of all the allowed values can be found in the official spark documentation . deploy-mode : whether to deploy your driver on one of the worker nodes ( cluster ) or locally as an external client ( client ). in client mode, the driver is started in the same node where the spark-submit is launched. even in one of the allowed value for master is local , which allows running the application on the local machine specifying how many threads should be used, we will not explore it: there is no need for remote debugging if the spark application runs with master local since everything runs on the same local jvm. this article will focus only on spark applications launched with a spark-submit run against a yarn cluster ( master yarn configuration), but the same considerations will hold also for other resource managers. your honour, my client pleads not guilty when the spark-submit command is invoked with client deploy-mode , spark will spawn the driver in the client process that performed the submit command. executors will spawn into nodes of the cluster depending on the resources associated with them. in client mode, the driver is spawned in the same process used to start the spark-submit command if you are performing the spark-submit command from an edge node of your cluster, you can debug the driver code by simply passing the jdwp agent configuration as a driver extra java option: spark-submit --class org.example.myjob \ --master yarn \ --deploy-mode client \ --conf ""spark. driver .extrajavaoptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747"" \ myjob.jar the command above will start the spark job with the driver running on the edge node and listening on port 4747 (this value is arbitrary, you can choose any available port number here). now we can setup our ide to start a remote debug towards the edge node ip and port 4747: define a remote debug configuration with the edge node ip as host we can start debugging our code with the configuration just defined by clicking on the debug icon: always check that the chosen debug configuration is the remote one code execution will stop at each breakpoint we define in our code; remember that since we are debugging the driver we can set up breakpoints anywhere in our code except for the code defined inside rdds/datasets, that will be performed by the executors. you and what nodes army? to debug the executor code we can focus on how a spark job behaves in cluster deploy-mode : the driver is spawned into one of the cluster nodes as well as the executors. so in this configuration connecting to the driver or one of the executors will require us to check where the processes are actually running. in cluster mode the driver will be spawned in one of the cluster nodes, as done for the executors if we need to debug the driver we can pass the same configurations presented above for the client mode, while if we need to debug one of the executors we should pass the agent properties in the executor options instead of the driver options: spark-submit --class org.example.myjob \ --master yarn \ --deploy-mode client \ --conf ""spark. executor .extrajavaoptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747"" \ myjob.jar as discussed, in cluster mode if we want to debug the driver or one of the executors we first need to find out where the actual process is running. to do so we can leverage the spark ui: whenever we start the job we can access its spark ui using the link printed by the spark-submit command. then we can access the executors section of the spark ui , where all the running processes associated to our job are listed. from here we can see the driver node and all the nodes where executors are running, so we can find the executor ip and use it in our ide debug configuration. in this case, the driver was spawned on cluster.node2 and the single executor on cluster.node1 at this point, we need to check the spark ui to find out the ip addresses of the desired nodes. since we defined the port in the agent configuration, we should keep the same debug port, changing only the ip address. if we need to debug the executor code, since in the spark ui we saw that the executor was cluster.node1, we can set that address in the remote debug configuration to simplify the debugging mechanism it is advised to start your spark job with only one executor: debugging a spark job with multiple executors dramatically increases its complexity. for example, if two executors are spawned on the same node, they will have the same ip address and debug port, so it could lead to inconsistencies. for debug purposes, you could scale it down to only one executor. spark-submit --class org.example.myjob \ --master yarn \ --deploy-mode client \ --num-executors 1 \ --conf ""spark.executor.extrajavaoptions=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4747"" \ myjob.jar if we decide to debug one of the executors, code execution will stop at each breakpoint we define inside rdds/datasets transformations and actions; remember that since we are debugging the executor, all breakpoints set up outside rdds/datasets will not be reached since that code will be performed only by the driver process. in a cluster far, far away… in a real scenario, the spark job will run inside a cluster which is not accessible from the outside world: you can only access a cluster edge node, but you are not able to access directly the nodes where the driver and executors processes will run. the edge node can access the nodes since it is co-located in the cluster. in a real scenario, the cluster is not accessible from the outside and you can only communicate with the edge nodes of the cluster in this situation we need to take advantage of a mechanism called port forwarding: we can forward the port of our target node to a port of the edge node. in this way, we can use our edge node (for which we have access) as a proxy for the desired target node (for which we don’t). a very common way to perform the port forwarding is to use ssh : ssh -l 4848:target.node:4747 user@edge.node the command above, run from your local machine, will connect to the edge node ( edge.node ) with the specified username ( user ). obviously, you must provide your identity to the remote machine, the edge node, using one of several methods depending on the protocol version used. the -l specifies that the given port on the local host ( 4848 ) has to be forwarded to the given host ( target.node ) and port ( 4747 ) on the remote side. in summary, in order to perform remote debugging of your code for a spark job running in a cluster like the one described above you will need to: add the desired agent configuration to the spark-submit command start the job open the spark ui and find out where your process is running use the ssh command to forward the port specified in the agent from the target node to your local machine through the edge node start the remote debug from your ide using as ip and port localhost and the forwarded port since we forwarded the remote port 4747 to our local port 4848 we can edit the remote debug configuration to listen for localhost on port 4848 if you made it this far, you may be interested in other spark-related articles that you can find on our blog . stay tuned because other spark articles are coming! spark debug cluster java ssh -- -- published in agile lab engineering 287 followers · last published jun 20, 2025 ideas go software! written by lorenzo pirazzini 36 followers · 4 following senior big data engineer at agile lab no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@haataa/pyspark-basics-6543795fd093,,,"pyspark basics. introduction | by haitian wei | medium sitemap open in app sign up sign in medium logo write sign up sign in pyspark basics haitian wei 3 min read · feb 17, 2020 -- listen share introduction why spark? spark is currently one of the most popular tools for big data analytics. although still used by many companies, hadoop is a slightly older technology compared to spark. in general, hadoop mapreduce is slower than spark because hadoop writes data out to disk during intermediate steps. advantages over mapreduce the goal of the spark project was to keep the benefits of mapreduce’s scalable, distributed, fault-tolerant processing framework, while making it more efficient and easier to use. the advantages of spark over mapreduce are: spark executes much faster by caching data in memory across multiple parallel operations, whereas mapreduce involves more reading and writing from disk. spark runs multi-threaded tasks inside of jvm processes, whereas mapreduce runs as heavier weight jvm processes. this gives spark faster startup, better parallelism, and better cpu utilization. spark provides a richer functional programming model than mapreduce. spark is especially useful for parallel processing of distributed data with iterative algorithms. how a spark application runs on a cluster source a spark application runs as independent processes, coordinated by the sparksession object in the driver program. the resource or cluster manager assigns tasks to workers, one task per partition. a task applies its unit of work to the dataset in its partition and outputs a new partition dataset. because iterative algorithms apply operations repeatedly to data, they benefit from caching datasets across iterations. results are sent back to the driver application or can be saved to disk. spark components core contains the basic functionality of spark. also home to the api that defines rdds, which is spark’s main programming abstraction. sql package for working with structured data. it allows querying data via sql as well as apache hive. it supports various sources of data, like hive tables, parquet, json, csv, etc. streaming enables processing of live streams of data. spark streaming provides an api for manipulating data streams that are similar to spark core’s rdd api. mllib provides multiple types of machine learning algorithms, like classification, regression, clustering, etc. graphx library for manipulating graphs and performing graph-parallel computations. this library is where you can find pagerank and triangle counting algorithms. pyspark pyspark is a spark api that allows you to interact with spark through the python shell. pyspark is a particularly flexible tool for exploratory big data analysis because it integrates with the rest of the python data analysis ecosystem, including pandas , numpy , and matplotlib. read and write data with pyspark import pyspark from pyspark import sparkconf from pyspark.sql import sparksession spark = sparksession \ .builder \ .appname(""test"") \ .getorcreate() spark.sparkcontext.getconf().getall() path = ""data/sparkify_log_small.json"" user_log = spark.read.json(path) user_log.printschema() references spark 101: what is it, what it does, and why it matters in this blog post, we will give an introduction to apache spark and its history and explore some of the areas in which… mapr.com tutorial: using pyspark and the mapr sandbox pyspark is a spark api that allows you to interact with spark through the python shell. if you have a python… mapr.com spark pyspark data science machine learning big data -- -- written by haitian wei 98 followers · 4 following https://github.com/haataa no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/swlh/productionizing-a-spark-job-with-databricks-notebook-dd950a242c7d,,Unit Testing,"productionizing a spark job with databricks notebook | by punchh technology blog | the startup | medium sitemap open in app sign up sign in medium logo write sign up sign in the startup · get smarter at building your thing. follow to join the startup’s +8 million monthly readers & +772k followers. productionizing a spark job with databricks notebook punchh technology blog 7 min read · mar 22, 2020 -- 3 listen share unit testing, automation, udf,… can be made easy author: xin heng (2020-mar-22) punchh p unchh has been on this exciting journey of building out its enterprise data lake by leveraging spark and associated big data technologies. databricks provides high-performing spark runtime and an easy-to-use notebook. while it is straightforward to insert spark etl code into production, i have found it not easy to bring in software developer practices. when spark udf came in the picture, it would become even a bigger problem. in this article, i give a simple demonstration of how we, at punchh, productionize pyspark jobs. — — — — — — — — — — — — — — — — — — databricks runtime is great. those who have worked with spark for a while must appreciate the speed and convenience provided by this product. in addition, there are other products such as notebook, jobs api, databricks connect, and so on. one may wonder: how can i combine all these tools together to productionize my big data jobs. i was not able to find an answer. here are the problems that i saw: i can quickly test out a spark etl logic inside the notebook. if it is working, i can quickly schedule it inside the notebook. when my etl logic becomes increasingly complex, my script would get longer and longer. unfortunately, it is very difficult (or inconvenient) to do unit testing inside a notebook. in other words, i can do fast prototyping with the notebook, but will soon be constrained by not having a well-structured codebase. spark udf is powerful. however, many people like me have struggled with the module not found error when running a udf object. the error persists because spark is not able to locate the function i have imported from another module. databricks connect is a good dev tool. it can help me throw my local dev job into a spark cluster. although the job is run in the cluster, i am able to debug in my local — very powerful. nevertheless, two technical problems arise: problem 1. databricks connect has a much higher cost than jobs api. (but jobs api is very hard to use during code development and debugging.) problem 2. i am allowed to configure a connection with only one cluster. if i have production jobs hitting different clusters, databricks connect would not work. in this article, i use a pyspark codebase (pyspark_demo) to demonstrate how i tackle these issues. in real production at punchh, we have much more stuff around this dev to prod process. having said that, i am hoping this little demo can provide a useful piece of viewpoint in your big data journey. the demo codebase is at https://github.com/punchh/pyspark_demo . step 1: how i start a pyspark project i use a combination of pycharm and databricks notebook, but my goal is always establishing a formal codebase as soon as i can. the codebase should adhere to a coding standard. for example, my application code is inside a folder that is named after my repo name. the tests/ folder contains all my unit test and integration test functions. in addition, the tests/folder structure strictly follows that of the main. the below screenshot demonstrates such an idea. structure of the codebase databricks connect is a cool feature. it has helped me establish a local dev environment and accelerate my software development. “… it allows you to write jobs using spark native apis and have them execute remotely on a databricks cluster... step through and debug code in your ide even when working with a remote cluster.” note that whatever spark code i have tried out and implemented must go through all of my unit tests ($ make test). see my makefile for more detail. i show an example output after running the unit tests in the below table. i have passed the unit tests but failed the 65% test coverage requirement. you get the idea. ------- coverage: platform darwin, python 3.7.4-final-0 -------- name                                     stmts   miss  cover ------------------------------------------------------------ pyspark_demo/__init__.py                     0      0   100% pyspark_demo/apps/__init__.py                0      0   100% pyspark_demo/apps/test_data_query.py        17      0   100% pyspark_demo/commons/__init__.py             0      0   100% pyspark_demo/commons/rate_processor.py       7      2    71% pyspark_demo/commons/spark_utils.py          7      7     0% pyspark_demo/etl_controller.py              36     36     0% ------------------------------------------------------------ total                                       67     45    33% fail required test coverage of 65% not reached. total coverage: 32.84% =============================== 2 passed, 8 warnings in 33.40s step 2: install my project repo as a python package this is an imperative step because of spark udf. why udf? it is utilized when people are dealing with complex logic operations among multiple columns. they cannot be easily described as columnar operations. i am a bit addicted to a modulized and organized codebase. thus, i always need to import a function from another module. if i had not installed my project as a package, udf would usually fail because spark computation inside the worker node could not find the functions i was importing from elsewhere. however, if it were a package, it would be packaged together with other libs and get shipped into the worker node. in this demo codebase, i am using udf to create a rate column from quantity and amount columns. it is not necessary, but i use this calculation for illustration purposes. how do i install a repo: the below bash commands will do the magic for you. it should look very straightforward. $ rm -rf dist/ $ python setup.py sdist bdist_wheel $ cd .. $ pip install -i pyspark_demo/dist/pyspark_demo-*.whl  # must be outside the project root $ cd pyspark_demo the above step will not only install pyspark_demo as a python package but also create a .whl file that i must now upload (and install) into my spark cluster. this screenshot shows where you can do the installation manually. by leveraging jobs api, one can also use a bash script to automate this procedure. “libraries” on databricks clusters tab in addition, there is a dbfs cli tool one can leverage. below is an example. i have hidden my environment variables, but you should be able to figure it out. dbfs rm ${dbfs_dir}/${whl_file} dbfs cp ./dist/${whl_file} $dbfs_dir databricks libraries install --cluster-id=$cluster_id --whl=${dbfs_dir}/${whl_file} step 3: set up a simple production script inside databricks notebook, and automate the job in step one and two, i have utilized a few technologies to establish a local dev environment and run udf successfully. the example python code is working and ready for automation. the below bash commands should be able to run your local environment. $ job=test  # $ date_range=2020-01-01,2020-01-02 $ python pyspark_demo/etl_controller.py --job=$job --date_range=$date_range 2>&1 | tee logs/etl_controller_job_${job}.log at this stage, the databricks notebook finally enters the picture. i convert the above python command lines into a simple notebook script like below: pyspark-demo notebook the last step is simple: i go to the jobs tab to establish the automation job. i can choose the cluster that is dedicated to this job. i set the scheduler at a 2-hour cadence. i prefer to use an auto job cluster, which is less than half of the cost of a regular, all-purpose spark cluster. this is pretty much it. one may also choose to use jenkins for more professional ci/cd. in addition, the ui presents an easy way to study the previous runs. jobs tab by now, i have derived a simple but complete flow that connects code development, deployment, and automation. i have solved the pain points mentioned aforehand. i develop a formal codebase with my ide, e.g., pycharm, sublime. i leverage databricks connect to establish the connection of my local dev tools with my dev spark cluster. i propose that one should package his/her repo as a python lib, and install it on the cluster. by using the databricks notebook i can set up a script for automation. i could configure any type of cluster. in addition, i am able to avoid the high cost of databricks connect. i save two extra steps of dockerize and install (on essentially aws ec2 or azure vm). it is working particularly well for my big data applications that have very few dependencies or interconnectivity. d uring the coronavirus outbreak, i feel myself having a lot more time to investigate a few technologies that i wanted to but was never able to do. i hope you will find this quick article helpful. about the author xin heng is the vp of data at punchh. his team’s mission to build world-class big data technologies to drive business growth. the team is responsible for the company’s machine learning, advanced analytics, and big data engineering. punchh pyspark big data devops programming -- -- 3 published in the startup 853k followers · last published 5 hours ago get smarter at building your thing. follow to join the startup’s +8 million monthly readers & +772k followers. written by punchh technology blog 166 followers · 12 following punchh is a marketing & data platform. in the blog site, we will share our learnings from data and technology. responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@datalackey/unit-integration-testing-kafka-and-spark-f52eabcc579,JUnit; TestNG,Integration Testing; Unit Testing,"unit & integration testing kafka and spark | by chris bedford | medium sitemap open in app sign up sign in medium logo write sign up sign in unit & integration testing kafka and spark overview chris bedford 13 min read · sep 23, 2020 -- listen share kafka is one of the most popular sources for ingesting continuously arriving data into spark structured streaming apps. however, writing useful tests that verify your spark/kafka-based application logic is complicated by the apache kafka project’s current lack of a public testing api (although such api might be ‘coming soon’, as described here ). this post describes two approaches for working around this deficiency and discusses their pros and cons. we first look at bringing up a locally running kafka broker in a docker container, via the helpful gradle-docker-compose plug-in . (side note: although our examples use gradle to build and run tests, translation to maven “should be” (tm) straightforward.) this part of the article assumes basic knowledge of docker and docker-compose . next we look at some third party (i.e., unaffiliated with apache kafka) libraries which enable us to run our tests against an in-memory kafka broker. in the last section we will look at some of the upgrade issues that might arise with the embedded / in-memory kafka broker approach when we migrate from older to newer versions of spark. to start off, we’ll walk you through building and running the sample project. building and running the sample project assuming you have docker, docker-compose, git and java 1.8+ installed on your machine, and assuming you are not running anything locally on the standard ports for zookeeper (2181) and kafka (9092), you should be able to run our example project by doing the following: git clone git@github.com:buildlackey/kafka-spark-testing.git cd kafka-spark-testing git checkout first-article-spark-2.4.1 gradlew clean test integrationtest next, you should see something like the following (abbreviated) output: > task :composeup … creating bb5feece1xxx-spark-testing__zookeeper_1 … … creating spark-master … … creating spark-worker-1 … done … will use 172.22.0.1 (network bb5f-xxx-spark-testing__default) as host of kafka … tcp socket on 172.22.0.1:8081 of service ‘spark-worker-1’ is ready … + — — — — — — — — + — — — — — — — — + — — — — — — — — -+ | name | container port | mapping | + — — — — — — — — + — — — — — — — — + — — — — — — — — -+ | zookeeper_1 | 2181 | 172.22.0.1:2181 | + — — — — — — — — + — — — — — — — — + — — — — — — — — -+ | kafka_1 | 9092 | 172.22.0.1:9092 | + — — — — — — — — + — — — — — — — — + — — — — — — — — -+ | spark-master | 7077 | 172.22.0.1:7077 | | spark-master | 8080 | 172.22.0.1:8080 | + — — — — — — — — + — — — — — — — — + — — — — — — — — -+ | spark-worker-1 | 8081 | 172.22.0.1:8081 | + — — — — — — — — + — — — — — — — — + — — — — — — — — -+ … build successful in 39s 9 actionable tasks: 9 executed note the table showing names and port mappings of the locally running containers brought up by the test. the third column shows how each service can be accessed from your local machine (and, as you might guess, the ip address will likely be different when you run through the set-up). testing against dockerized kafka before we get into the details of how our sample project runs tests against dockerized kafa broker instances, let’s look at the advantages of this approach over using in-memory (embedded) brokers. post-test forensic analysis. one advantage of the docker-based testing approach is the ability to perform post-test inspections on your locally running kafka instance, looking at things such as the topics you created, their partition state, content, and so on. this helps you answer questions like: “was my topic created at all?”, “what are its contents after my test run?”, etc.easily use same kafka as production. for projects whose production deployment environments run kafka in docker containers this approach enables you to synchronize the docker version you use in tests with what runs in your production environment via a relatively easy configuration-based change of the zookeeper and docker versions in your docker-compose file. no dependency hell. due to the absence of ‘official’ test fixture apis from the apache kafka project, if you choose the embedded approach to testing, you need to either (a) write your own test fixture code against internal apis which are complicated and which may be dropped in future kafka releases, or (b) introduce a dependency on a third party test library which could force you into ‘dependency hell’ as a result of that library’s transitive dependencies on artifacts whose versions conflict with dependencies brought in by spark, kafka, or other key third party components you are building on. in the last section of this post, we will relive the torment i suffered in getting the embedded testing approach working in a recent client project. dockerized testing — forensic analysis on running containers let’s see how to perform forensic inspections using out of-the-box kafka command line tools. immediately after you get the sample project tests running (while the containers are up), just run the following commands. (these were tested on linux, and “should work” (tm) on macos): mkdir /tmp/kafka cd /tmp/kafka curl https://downloads.apache.org/kafka/2.2.2/kafka_2.12-2.2.2.tgz -o /tmp/kafka/kafka.tgz tar -xvzf kafka.tgz — strip 1 export path=”/tmp/kafka/bin:$path” kafka-topics.sh — zookeeper localhost:2181 — list you should see output similar to the following: __consumer_offsets some-testtopic-1600734468638 __consumer_offsets is a kafka system topic used to store information about committed offsets for each topic/partition, for each consumer group. the next topic listed — something like some-testtopic-xxx — is the one created by our test. it contains the content: ‘hello, world’. this can be verified by running the following command to see what our test wrote out to the topic: kafka-console-consumer.sh — bootstrap-server localhost:9092 \ — from-beginning — topic some-testtopic-1600734468638 the utility should print “hello, world”, and then suspend as it waits for more input to arrive on the monitored topic. press ^c to kill the utility. dockerized testing — build scaffolding in the listing below we have highlighted the key snippets of gradle configuration that ensure docker and zookeeper containers have been spun up before each test run. lines 1–4 simply declare the plug-in to activate. line 7 ensures that when we run the integrationtest task the dockercompose task will be executed as a prerequisite to ensure required containers are launched. since integration tests — especially those that require docker images to be launched, and potentially downloaded — typically run longer than unit tests, our build script creates a custom dependency configuration to run such tests, and makes them available via the separate integrationtest target (to be discussed more a little further on), as distinct from the typical way to launch tests: gradlew test. lines 10–17 configure docker-compose’s behavior, with line 10 indicating the path to the configuration file that identifies the container images to spin up, environment variables for each, the order of launch, etc. line 13 references the configuration attribute that ensures containers are left running. the removexxx switches that follow are set to false, but if you find that docker cruft is eating up a lot of your disk space you might wish to experiment with settings these to true. plugins { … id “com.avast.gradle.docker-compose” version “0.13.3” } dockercompose.isrequiredby(integrationtest) dockercompose { usecomposefiles = // we leave containers up in case we want to do forensic analysis on results of tests stopcontainers = false removecontainers = false // default is true removeimages = “none” // other accepted values are: “all” and “local” removevolumes = false // default is true removeorphans = false // removes contain } the kafka-related portion of our docker-compose file is reproduced below, mainly to highlight the ease of upgrading your test version of kafka to align with what your production team is using — assuming that your kafka infrastructure is deployed into a docker friendly environment such as kubernetes. if your prod team deploys on something like bare metal this section will not apply. but if you are lucky, your prod team will simply give you the names and versions of the zookeeper and kafka images used in your production deployment. you would replace lines 4 and 8 with those version qualified image names, and as your prod team deploys new versions of kafka (or zookeeper) you simply have those two lines to change to stay in sync. this is much easier than fiddling with versions of test library .jar’s due to the attendant dependency clashes that may result from such fiddling. once the kafka project releases an officially supported test kit this advantage will be less compelling. version: ‘2’ services: zookeeper: image: wurstmeister/zookeeper:3.4.6 ports: — “2181:2181” kafka: image: wurstmeister/kafka:2.13–2.6.0 command: ports: — “9092:9092” environment: kafka_zookeeper_connect: zookeeper:2181 kafka_advertised_host_name: 127.0.0.1 volumes: — /var/run/docker.sock:/var/run/docker.sock depends_on: — zookeeper dockerized testing — build scaffolding: running integration tests separately the listing below highlights the portions of our project’s build.gradle file which enable our integration test code to be stored separately from code for our unit tests, and which enable the integration tests themselves to be run separately. sourcesets { integrationtest { java { compileclasspath += main.output + test.output runtimeclasspath += main.output + test.output srcdir file(“src/integration-test/java”) } resources.srcdirs “src/integration-test/resources”, “src/test/resources” } } configurations { integrationtestcompile.extendsfrom testcompile } task integrationtest(type: test) { testclassesdirs = sourcesets.integrationtest.output.classesdirs classpath = sourcesets.integrationtest.runtimeclasspath } integrationtest { usetestng() { usedefaultlisteners = true } } dockercompose.isrequiredby(integrationtest) lines 1–10 describe the integrationtest ‘source set’. source sets are logical groups of related java (or scala, kotlin, etc.) source and resource files. each logical group typically has its own sets of file dependencies and classpaths (for compilation and runtime). line 6 indicates the top level directory where integration test code is stored. line 8 identifies the locations of directories which hold resources to be made available to the runtime class path. note that we specify both src/test/resources (as well as src/integrationtest/resources) so any configuration we put under the former directory is also available to integration tests at runtime. this is useful for things like logger configuration, which is typically the same for both types of tests, and as a result is a good candiate for sharing. line 12 defines the ‘custom dependency configuration ’ integrationtest. a dependency configuration in gradle is similar to a maven scope — it maps roughly to some build time activity which may require distinct dependencies on specific artifacts to run. line 13 simply states that that the integrationtest dependency configuration should inherit its compile path from the testcompile dependency configuration which is available out-of-the box via the java plugin. if we wanted to add additional dependencies — say on guava — just for our integration tests, we could do so by adding lines like the ones below to the ‘dependencies’ configuration block: dependencies { integrationtestcompile group: ‘com.google.guava’, name: ‘guava’, version: ‘11.0.2’ …. // other dependenacies } line 17–20 define ‘integrationtest’ as an enhanced custom task of type test. tests need a test runner, which for gradle is junit by default. our project uses testng, so we declare this on line 23. (side note: line 4 seems to state the same thing as line 13, but i found both were required for compilation to succeed). we didn’t touch on code so much in this discussion, but as you will see in the next section the test code that runs against dockerized kafka brokers shares many commonalities with the code we wrote to test against embedded brokers. after reading through the next section you should be able to understand what is going on in containerizedkafkasinktest. embedded broker based testing we described some of the advantages of docker-based testing of kafka/spark applications in the sections above, but there are also reasons to prefer the embedded approach: lowered build complexity (no need for separate integration tests and docker-compose plug-in setup).somewhat faster time to run tests, especially when setting up a new environment, as there is no requirement to downloaded images.reduced complexity in getting code coverage metrics if all your tests are run as unit tests, within the same run. (note that if this were a complete no-brainer then there would be not be tons of how-to articles out there like this .)your organization’s build infrastructure might not yet support docker (for example, some companies might block downloads of images from the “docker hub” site). so, now let’s consider how to actually go about testing against an embedded kafka broker. first, we need to choose a third-party library to ease the task of launching said broker . i considered several, with the kafka unit testing project being my second choice, and spring kafka test ending up as my top pick. my main reasoning was that the latter project, being backed by vmware’s spring team is likely to work well into the future, even as the kafka internal apis it is based on may change. also, the top level module for spring-kafka-test only brought in about six other spring dependencies (beans, core, retry, aop, and some others), rather than the kitchen sink, as some might fear. with our test library now chosen let’s look at the code, the overall organization of which we present diagrammatically: our kafkacontext interface declares the getproducer() method to get at a kafka producer which is used simply to send the message ‘hello, world’ to a topic (lines 18–20 of the listing for abstractkafkasinktest shown below). our abstract test class then creates a dataset of rows that return the single key/value pair available from our previous write to the topic (lines 23–28), and finally checks that our single row dataset has the expected content (lines 32–37). public abstract class abstractkafkasinktest { final logger logger = loggerfactory.getlogger(abstractkafkasinktest.class); protected static final string topic = “some-testtopic-” + new date().gettime(); protected static final int kafkabrokerlistenport = 6666; kafkacontext kafkactx; abstract kafkacontext getkafkacontext() throws executionexception, interruptedexception; @beforetest public void setup() throws exception { kafkactx = getkafkacontext(); thread.sleep(1000); // todo — try reducing time, or eliminating } public void testsparkreadingfromkafkatopic() throws exception { kafkaproducer producer = kafkactx.getproducer(); producerrecord producerrecord = new producerrecord(topic, “dummy”, “hello, world”); producer.send(producerrecord); sparksession session = startnewsession(); dataset rows = session.read() .format(“kafka”) .option(“kafka.bootstrap.servers”, “localhost:” + kafkactx.getbrokerlistenport()) .option(“subscribe”, topic) .option(“startingoffsets”, “earliest”) .load(); rows.printschema(); list rowlist = rows.collectaslist(); row row = rowlist.get(0); string strkey = new string(row.getas(“key”), “utf-8”); string strvalue = new string(row.getas(“value”), “utf-8”); assert(strkey.equals(“dummy”)); assert(strvalue.equals(“hello, world”)); } sparksession startnewsession() { sparksession session = sparksession.builder().appname(“test”).master(“local”).getorcreate(); session.sqlcontext().setconf(“spark.sql.shuffle.partitions”, “1”); // cuts a few seconds off execution time return session; } } abstractkafkasinktest delegates to its subclasses the decision of what kind of context (embedded or containerized) will be returned from getkafkacontext() (line 9, above). embeddedkafkasinktest, for example, offers up an embeddedkafkacontext, as shown on line 3 of the listing below. public class embeddedkafkasinktest extends abstractkafkasinktest { kafkacontext getkafkacontext() throws executionexception, interruptedexception { return new embeddedkafkacontext(topic, kafkabrokerlistenport); } @test public void testsparkreadingfromkafkatopic() throws exception { super.testsparkreadingfromkafkatopic(); } } the most interesting thing about embeddedkafkacontext is how it makes use of the embeddedkafkabroker class provided by spring-kafka-test. the associated action happens on lines 10–15 of the listing below. note that we log the current run’s connect strings for both zookeeper and kafka. the reason for this is that — with a bit of extra fiddling — it actually is possible to use kafka command line tools to inspect the kafka topics you are reading/writing in your tests. you would need to introduce a long running (or infinite) loop at the end of your test that continually sleeps for some interval then wakes up. this will ensure that the kafka broker instantiated by your test remains available for ad hoc querying. public class embeddedkafkacontext implements kafkacontext { final logger logger = loggerfactory.getlogger(embeddedkafkacontext.class); private final int kafkabrokerlistenport; private final string bootstrapservers; private final string zookeeperconnectionstring; embeddedkafkacontext(string topic, int kafkabrokerlistenport) { this.kafkabrokerlistenport = kafkabrokerlistenport; embeddedkafkabroker broker = new embeddedkafkabroker(1, false, topic); broker.kafkaports(kafkabrokerlistenport); broker.afterpropertiesset(); zookeeperconnectionstring = broker.getzookeeperconnectionstring(); bootstrapservers = broker.getbrokersasstring(); logger.info(“zookeeper: {}, bootstrapservers: {}”, zookeeperconnectionstring, bootstrapservers); } @override public int getbrokerlistenport() { return kafkabrokerlistenport; } } public class embeddedkafkasinktest extends abstractkafkasinktest { kafkacontext getkafkacontext() throws executionexception, interruptedexception { return new embeddedkafkacontext(topic, kafkabrokerlistenport); } @test public void testsparkreadingfromkafkatopic() throws exception { super.testsparkreadingfromkafkatopic(); } } embedded broker based testing downsides: dependency hell we mentioned earlier that one upside of dockerized testing is the ability to avoid dependency conflicts that otherwise could arise as a result of bringing in a new test library, or upgrading spark (or other third party components) after you finally manage to get things to work. if you pull the sample project code and execute the command git checkout first-article-spark-2.4.1 you will see that for this version of spark we used an earlier version of spring-kafka-test (2.2.7.release versus 2.4.4.release, which worked for spark 3.0.1). you should be able to run gradlew clean test integrationtest just fine with this branch in its pristine state. but note the lines at the end of build.gradle in this version of the project: configurations.all { resolutionstrategy { force ‘com.fasterxml.jackson.core:jackson-databind:2.6.7.1’ } } if you were to remove these lines our tests would fail due to the error “jsonmappingexception: incompatible jackson version: 2.9.7”. after some digging we found that it was actually spring-kafka-test that brought in version 2.9.7 of jackson-databind, so we had to introduce the above directive to force all versions back down to the version of this dependency used by spark. now after putting those lines back (if you removed them), you might want to experiment with what happens when you upgrade the version of spark used in this branch to 3.0.1. try modifying the line spark_version = “2.4.1” to spark_version = “3.0.1” and running gradlew clean test integrationtest again. you will first get the error “could not find org.apache.spark:spark-sql_2.11:3 0.1.”. that is because scala version 2.11 was too old for the spark 3.0.1 team to release their artifacts against. so we need to change the line scala_version = “2.11” to scala_version = “2.12” and try running our tests again. this time we see our tests fail at the very beginning, in the setup phase, with the error “nosuchmethoderror: scala.predef$.refarrayops([ljava/lang/object;)lscala/collection/mutable/arrayops”. this looks like we might be bringing in conflicting versions of the scala runtime libraries. indeed if we look at the dependency report for spring-kafka-test version 2.2.7.release we see (at the bottom of the page reproduced below) that this version of spring-kafka-test brings in org.apache.kafka:kafka_2.11:2.0.1. this is a version of kafka that was compiled with scala 2.11, which results in a runtime clash between that version and the version that spark needs (2.12). this problem was very tricky to resolve because the fully qualified (group/artifact/version) coordinates of spring-kafka-test:2.2.7.release offer no hint of this artifact’s dependency on scala 2.11. once i realized there was a scala version conflict it was a pretty easy decision to hunt around for a version of spring-kafka-test that was compiled against a release of kafa which was, in turn, compiled against scala 2.12. i found that in version 2.4.4, whose abbreviated dependency report is shown below. hopefully this recap of my dependency-related suffering gives you some motivation to give docker-based testing a try ! gradle kafka spark -- -- written by chris bedford 2 followers · 2 following bay area-based s/w developer with broad experience in cloud-based, big data / ai / ml projects, using tools like spark, elastic search, cassandra and kafka. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/data-science/automated-data-quality-testing-at-scale-using-apache-spark-93bb1e2c5cd0,,,"automated data quality testing at scale using apache spark | by tanmay deshpande | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in mastodon tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. member-only story automated data quality testing at scale using apache spark with an open source library from amazon — deequ tanmay deshpande 7 min read · jun 29, 2019 -- 3 share photo by stephen dawson on unsplash i have been working as a technology architect, mainly responsible for the data lake/hub/platform kind of projects. every day we ingest data from 100+ business systems so that the data can be made available to the analytics and bi teams for their projects. problem statement while ingesting data, we avoid any transformations. the data is replicated as it is from the source. the sources can be of type mysql, sql server, oracle, db2, etc. the target systems can be hadoop/hive or big query. even though there is no transformation done on the data since the source and target systems are different, sometimes these simple data ingestions could cause data quality issues. source and target systems can have different data types which might cause more issues. special characters in data might cause row/column shiftings. possible solution in order to solve this problem, most of the developers use a manual approach for data quality testing after they built the data pipelines. this can be done by running some simple tests like sample data comparison between source and target null checks on primary key columns -- -- 3 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by tanmay deshpande 4.5k followers · 2.4k following i write about technology in simple words! responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@srowen/solving-real-life-mysteries-with-big-data-and-apache-spark-7fd760cab2d8,,,"solving real-life mysteries with big data and apache spark | by sean owen | medium sitemap open in app sign up sign in medium logo write sign up sign in solving real-life mysteries with big data and apache spark sean owen 9 min read · aug 13, 2019 -- listen share can using simple statistical techniques in combination with big data help solve the tamam shud mystery? reprinted from https://blog.cloudera.com/blog/2016/09/solving-real-life-mysteries-with-big-data-and-apache-spark/ , now since unpublished. everyone loves a good real-life mystery. that’s why the three most popular tv shows of the 80s and 90s were jack palance’s reboot of ripley’s believe it or not! , unsolved mysteries with robert stack, and beyond belief: fact or fiction hosted by commander riker. (well…they were in my house, anyway.) at cloudera, the highly-skilled support team has gotten good at cracking actual stranger-than-fiction cases like, “why doesn’t this kerberos ticket renew?” or, “who deleted that table?” in this spirit, on a recent random walk through wikipedia links, i found a fascinating 68-year-old unsolved mystery known as the tamam shud case (aka “mystery of the somerton man”). if you enjoyed serial , then the story itself is worth a read, and worth watching . however for anyone who touches data for a living, the most intriguing part of the story will undoubtedly be this: facts of the case a man is found dead on a beach near adelaide, australia, in december 1948. well-dressed and in good shape, he seems to have died from poisoning. his mundane possessions include no identification, but do include a scrap of a paper with the words, “tamám shud”. this phrase turns out to be the closing words (in persian) of the rubaiyat of omar khayyam , meaning “finished.” soon after, the very book from which it was torn is located. inside its cover is scrawled the unlisted phone number of a local woman, along with this mysterious text: wrgoababd mliaoi wtbimpanetp mliaboaiaqc ittmtsamstgab to this day, nobody has conclusively explained its meaning, and the dead man has never been identified. several people have approached these letters as a cryptographic cipher. the odd circumstances of death do sound like something out of a john le carré spy novel. some of the best attempts, however, fail to produce anything but truly convoluted parsings . another possibility may already have occurred to you: are they the first letters of words in a sentence (an initialism )? some suspect this death was a suicide, and that the message is merely some form of final note . with this morbid scenario in mind, it’s easy to imagine many phrases, like “my life is all but over,” that fit the letters because indeed their frequency seems to match that of english text. this lead has been picked up a few times. these writeups ( example ) present indications that the message is indeed an initialism. however, they don’t apply what is arguably the clear statistical tool for this job. and they don’t take advantage of big data. so, let’s do both. the chi-squared test does the frequency of letters in the tamam shud text resemble the frequency of first letters of english text? if so, that would be evidence that the text is an english initialism. but any given english sentence’s frequency of initials won’t exactly match english text as a whole. rather, it will vary a bit depending on what sentences are chosen. the well-known chi-squared tes t (χ2) can help. it takes as input some expected frequencies of discrete things occurring — like letters starting words — and actual observed counts of those things. it can then quantify the probability that a sample actually drawn from the expected frequencies would exhibit deviations from the expected frequencies as large or larger than that of the observed sample: a p-value . although you may hear about degrees of freedom and the chi-squared statistic in this context, these are details that libraries will take care of in a context like this. this usage of the chi-squared test is known as a goodness-of-fit test . this possibility that the observed counts come from the expected distribution is called the null hypothesis . in this hypothesis, the sample is not from a different distribution. a low p-value means that the null hypothesis is unlikely. it’s evidence that the given initials did not come from a source whose frequencies match the expected frequencies. a high p-value is not quite evidence that the initials came from the same source; it just means that actual samples from the expected distribution would regularly show deviation from expected counts that are as large, or larger. although it’s indicative that the initials came from the expected distribution, it’s not the probability that they did. to get to work, fire up a scala spark-shell. it’s easy to count the frequency of letters in the mystery text in scala: def texttocharcounts(text: string) = { // start with 0 count for all letters val countmap = scala.collection.mutable.map(('a' to 'z').map(_ -> 0l):_*) text.foreach(countmap(_) += 1) countmap.toseq.sorted.map { case (_, count) => count }.toarray } val tamamshudtext = ""wrgoababdmliaoiwtbimpanetpmliaboaiaqcittmtsamstga"" val tamamshudcounts = texttocharcounts(tamamshudtext) ... array(9, 4, 1, 1, 1, 0, 2, 0, 6, 0, 0, 2, 5, 1, 3, 2, 1, 1, 2, 6, 0, 0, 2, 0, 0, 0) so, “a” occurs 9 times, and “z” occurs 0 times. these are the observed frequencies. finding the overall expected frequency of initials in english text requires a lot of english text. fortunately, it’s easy to access the entire text of english wikipedia as json dump . obtain the latest content dump, like enwiki-20160808-cirrussearch-content.json.gz . (but be warned: it’s over 22gb in size.) then push it straight into hdfs, to an example directory like /user/ds , with: curl https://dumps.wikimedia.org/other/cirrussearch/current/enwiki-20160808-cirrussearch-content.json.gz | gunzip -c | hdfs dfs -put - /user/ds/cirrus.json being json, the text can be extracted with a bit of apache spark code and jackson , using the dataset api. for example, this can be :paste -ed into spark-shell: val wikipediatext = sqlcontext .read.text(""hdfs:///user/ds/cirrus.json"").as[string] .filter(_.startswith(""{\""namespace\"":0,"")) .mappartitions { jsons => val mapper = new com.fasterxml.jackson.databind.objectmapper() jsons.map(json => mapper.readvalue(json, classof[java.util.map[_,_]]).get(""text"").tostring ) } and some careful filtering and counting with spark reveals the distribution of initials in the entire corpus: def texttoinitials(text: string): array[char] = text.split(""\\w+"").filter(_.nonempty) .map(word => character.touppercase(word.head)) .filter(c => c >= 'a' && c <= 'z') val wikipediacounts = wikipediatext // split into words, map to initials .flatmap(text => texttoinitials(text).map(_.tostring)) .todf // count characters .groupby(""value"").count .as[(string,long)].collect() .map { case (s, count) => (s.head, count) } // sort by character and return counts .toseq.sorted .map { case (_, count) => count }.toarray ... array(282341805, 114250689, 145476474, 84765259, ... now, it’s easy to use an implementation of the chi-squared test to compute a p-value: def pvalue(expected: array[long], observed: array[long]) = { // disregard letters with 0 count in expected val (nzexpected, nzobserved) = expected.zip(observed).filter { case (e, _) => e > 0 }.unzip val cs = new org.apache.commons.math3.stat.inference.chisquaretest() cs.chisquaretest(nzexpected.map(_.todouble).toarray, nzobserved.toarray) } pvalue(wikipediacounts, tamamshudcounts) ... 0.2191200150296464 judging just by letter counts, if a sample of english text was taken repeatedly, then the observed counts would differ as much or more than this mystery text does from the expected frequency about 21.9% of the time. this is not unlikely, and thus not sufficient to rule out the null hypothesis: that the mystery letters are english initials. compare to the result when these counts are compared to a uniform distribution of initials, which is what one would expect if letters were chosen uniformly at random — and also how many ciphered texts would appear: pvalue( array .fill(26)(1l), tamamshudcounts) ... 1.6201309989138934e-6 that’s less than an 0.002% chance, and strong evidence the letters aren’t like randomly-chosen letters. this famous passage, given as initials, appears in wikipedia, and a similar analysis suggests it yields a similar p-value: fsasyaofbfotcannciladttptamace nwaeiagcwtwtnoanscasdcle val passagecounts = texttocharcounts( ""fsasyaofbfotcannciladttptamacenwaeiagcwtwtnoanscasdcle"") pvalue(wikipediacounts, passagecounts) ... 0.2673565500306385 to be honest, this work has been done before at the university of adelaide , with the conclusion also being that the message seems to be an english initialism (if it’s an initialism in any language at all). can we add to the investigation with spark? tracking the source of the “quote” with some strong evidence that these are english initials, and the complete text of wikipedia to hand, it’s natural to simply search for a passage whose initials match part of the mystery text. while it’s rather unlikely that the somerton man was quoting wikipedia, it’s possible he had a famous passage in mind that may appear in wikipedia. it takes just a short passage of text and a few minutes on a cluster to find some answers. for example, looking for substrings like “mliaboaiaqc” entails: import org.apache.spark.sql._ val wikitextinitials = wikipediatext .map(text => (text, new string(texttoinitials(text)))) .rdd.todf(""text"", ""initials"").cache() wikitextinitials .filter(col(""initials"").contains(""mliaboaiaqc"")) .select(""text"").as[string].collect() the bad news is that most any search for significant strings from the mystery text yields no hits. some shorter strings do, but they’re obviously not a match. it’s another dead end. but, if you didn’t recognize the quote above by its initials, you can find it now. among other things, it occurs as part of the gettysburg address . in case you’re wondering, no , the initials do not appear in the rubaiyat of omar khayyam either, in the english translation from which the scrap was torn. fortunately, thanks to project gutenberg , we actually have available the text of many well-known public-domain english texts and can search them, too. although all texts can be downloaded from the site itself, that can take days — instead, gutenberg-tar.com provides a compressed archive that’s easier to obtain. use p7zip to decompress it; if the entire archive is decompressed to one large text file (tip: 7za x -so gutenberg_txt.7z 2> /dev/null | hdfs dfs -put - /user/ds/gutenberg.txt ), it can be searched like the wikipedia json dump above, but no parsing of json is needed this time—it can just be read as a dataset of strings. to cut to the chase: no , nothing emerges as a clear match to any significant substring. another lead turns up nothing. however, we can try to locate texts whose frequency of initials seems more likely to have given rise to the counts observed in the mystery sample, by applying the chi-squared test again. it’s problematic to interpret the p-value as a metric for ranking things. however, for merely hunting for clues about relevant texts, it might be sufficient. this might give some indication of what type of writing it resembles. (poetry? short fiction?) to do this, decompress the archive as files and upload them to hdfs with hdfs dfs -put gutenberg /user/ds/ . this will take a while, and create over 70,000 small files nested three to five levels deep. in this case, it will actually be helpful to use a method from the original rdd api, called wholetextfiles , which reads (filename, contents) pairs. this approach is useful when the input consists of many small files. search for files at all levels of nesting, splitting the larger set that’s nested deeper into more partitions, for balance: val gutenbergbase = ""hdfs:///user/ds/gutenberg"" val alltextsrdd = sc.union( sc.wholetextfiles(s""$gutenbergbase/*/*/*.txt"", 10), sc.wholetextfiles(s""$gutenbergbase/*/*/*/*.txt"", 100), sc.wholetextfiles(s""$gutenbergbase/*/*/*/*/*.txt"", 1000)) the files have a large header and footer that shouldn’t be counted. this splits a text file into lines, and then reassembles the part after the header and before the footer into a string: def stripheaderfooter(text: string) = { val lines = text.split(""\n+"").map(_.trim).filter(_.nonempty) val headerend = lines.indexwhere(_.matches( ""\\*\\*\\*.*start.+project gutenberg.*\\*\\*\\*"")) val footerstart = lines.indexwhere(_.matches( ""\\*\\*\\*.*end.+project gutenberg.*\\*\\*\\*""), headerend) lines.slice(if (headerend < 0) 0 else headerend + 1, if (footerstart < 0) lines.size else footerstart) .mkstring("" "") } finally, it’s possible to consider each text’s initials as a distribution and compare to the mystery text: val topmatches = alltextsrdd .mapvalues(stripheaderfooter) .mapvalues(text => new string(texttoinitials(text))) .mapvalues(texttocharcounts) .filter { case (_, counts) => counts.size > 1 } .mapvalues(pvalue(_, tamamshudcounts)) .sortby { case (_, pvalue) => -pvalue } .take(5).foreach(println) ... (.../gutenberg/3/6/7/3679/3679.txt,0.7984799078362972) (.../gutenberg/3/5/9/3597/3597.txt,0.7920832483908181) (.../gutenberg/2/6/3/2631/2631-8.txt,0.788166446663857) (.../gutenberg/2/6/3/2631/2631.txt,0.788166446663857) (.../gutenberg/9/0/7/9070/9070-8.txt,0.7873012087723055) these texts are: getting gold , by j. c. f. johnson the essays of montaigne , volume 17 , by michel de montaigne mr. gladstone and genesis , by thomas henry huxley (duplicate) the imaginary invalid , by moliére there’s no obvious theme to these books. it’s also possible that, with a relatively short sample, most of these top results have high p-values due to chance. unfortunately, there’s no “smoking gun” here. conclusion there are more leads to follow. why not compare bigrams (pairs of initials)? their order of occurrence is certainly significant. the total number of bigrams in the english language is 26 x 26 = 676, and there are 48 bigrams observed in the mystery text, whereas most buckets would contain 0 or 1. the counts would be so sparse that the chi-squared test becomes difficult to apply. maybe some mysteries are unsolved for a reason: the message could represent a personal message, or may simply be nonsense. when those mysteries involve potential codes, though, statistical techniques (even one as simple as the well-known chi-squared test) can provide valuable supporting clues. and complementing such techniques with big data technologies like spark make it easy to deploy plenty of raw computing resources to search high and low for patterns in vast amounts of business or scientific data. spark statistics text mining -- -- written by sean owen 1.5k followers · 111 following big-data data science personality @ databricks. prev: director data science @ cloudera no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/clairvoyantblog/productionalizing-spark-streaming-applications-4d1c8711c7b0,,,"productionalizing apache spark streaming applications | yarn cluster mode | clairvoyant blog sitemap open in app sign up sign in medium logo write sign up sign in mastodon clairvoyant blog · follow publication clairvoyant is a data and decision engineering company. we design, implement and operate data management platforms with the aim to deliver transformative business value to our customers. follow publication member-only story productionalizing spark streaming applications taking your big data spark streaming job out of the test environment and getting it ready for prime-time in production. robert sanders 15 min read · feb 13, 2019 -- 6 share the apache spark project has become an essential tool in a big data engineers toolkit. it includes many capabilities ranging from a highly performant batch processing engine to a near-real-time streaming engine. spark streaming at clairvoyant , we’ve been working with clients who are interested in building highly performant real-time big data systems for their business. many use cases have come up including alert engines, processing iot data, and much more. we’ve dabbled in several types of technologies including apache nifi , apache flume , apache flink , and more. but one of our favorite technologies to use is spark streaming. spark streaming is an extension to core apache spark that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. source data streams can be any of the following as described in the below image and more. spark streaming — source-link -- -- 6 follow published in clairvoyant blog 1.1k followers · last published feb 12, 2025 clairvoyant is a data and decision engineering company. we design, implement and operate data management platforms with the aim to deliver transformative business value to our customers. follow written by robert sanders 1.7k followers · 392 following senior avp of data management for exl services | marathon runner | triathlete | endurance athlete responses ( 6 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://igorizraylevych.medium.com/why-do-you-need-to-use-apache-spark-for-your-big-data-project-123e187a4d0,,,"why do you need to use apache spark for your big data project | by igor izraylevych | medium sitemap open in app sign up sign in medium logo write sign up sign in why do you need to use apache spark for your big data project igor izraylevych 5 min read · may 7, 2019 -- listen share have you heard of apache spark? it is a leading framework for processing big data. let me introduce it to you! i first heard about spark just over 2 years ago when i started working on the project related to consented tracking of consumer data, building audiences, creating target ad campaigns and aggregating detailed analytics. it’s a dmp platform with unique long-lasting cross-device technology. at the same time, i got acquainted with the scala programming language in which spark was written. all this fascinated me so much that i devoted all the subsequent time to studying these and several related technologies. today i’ll focus your attention on spark. nowadays spark is used in lots of leading companies such as amazon, ebay, nasa, etc. many organizations operate spark in clusters, involving thousands of nodes. according to the spark faq, the largest of these clusters have more than 8,000 nodes. what is apache spark? apache spark is a unified analytics engine for large-scale data processing. the project is being developed by the free community, currently, it is the most active of the apache projects. comparing to hadoop mapreduce, another data processing platform, spark accelerates the programs operating in memory by more than 100 times, and on drive — by more than 10 times. furthermore, the code is written faster because here in spark we have more high-level operators at our disposal. natively spark supports scala, python, and java. it is well integrated with the hadoop ecosystem and data sources. spark main components sql & dataframes is a spark component that supports data querying using either sql or dataframe api. both help to access a variety of data sources, including hive, avro, parquet, orc, json, and jdbc in the usual way. there is even a possibility to join data across the mentioned sources. spark streaming supports real-time streaming processing. such data can be log files of the working web server (for example, processed by apache flume or placed on hdfs / s3), information from social networks (for example, twitter), as well as various message queues such as kafka. mllib is a machine learning library that provides various algorithms designed for horizontal scaling on a cluster for classification, regression, clustering, co-filtering, etc. graphx is a library for manipulating graphs and performing parallel operations with them. the library provides a universal tool for etl, research analysis and graph-based iterative computing. spark core is a basic engine for a large-scale parallel and distributed data processing. the core is responsible for: memory management and recovery after failures planning, distribution, and tracking of tasks in a cluster interaction with storage systems cluster managers are used for the management of the spark work in a cluster of servers. what can you do with spark? spark helps to create reports quickly, perform aggregations of a large amount of both static data and streams. it solves the problem of machine learning and distributed data integration. it is easy enough to do. by the way, data scientists may use spark features through r- and python-connectors. it copes with the problem of “everything with everything” integration. there is a huge amount of spark connectors. spark can be used as a quick filter to reduce the dimension of the input data. for example, transmit, filtering and aggregating a flow from kafka, adding it to mysql, etc. the scope of apache spark application potentially, the coverage of spark is very extensive. here is an indicative (but not exhaustive) selection of some practical situations where a high-speed, diverse and volumetric processing of big data is required for which spark is so well suited: online marketing etl creation of analytical reports profiles classification behavior analysis profiles segmentation targeted advertising semantic search systems media & entertainment recommendation systems schedule optimization expansion and retention of the audience targeted advertising content monetization government intelligence and cybersecurity felony prediction and prevention weather forecasting tax implementation traffic streamlining healthcare pharmaceutical drug assessment scientific research data processing of: patient records crm weather forecasting fitness trackers demographic data research data data from devices and sensors finances employee surveillance predictive modeling financial markets forecasting auto insurance consumer credit operations loans manufacturing behavior analysis creation of analytical reports data processing from devices and sensors targeted advertising crm employee monitoring logistics & mobility etl of sensors data weather forecasting predictive analytics creation of analytical reports agricultural weather forecasting etl from: soil sensors drones monitoring gadgets what challenges do you usually face while working with spark? in fact, challenges are typical for any big data framework. things that work on a small dataset (test methods and some jvm settings) often work differently on big data in production. another possible challenge for a java or python developer is that you need to learn scala. most of the code base and function signatures require reading the scala code with a dictionary. and last but not at least, even a small home project is expensive if you test it in the cluster. where is spark heading? spark is a very dynamic platform. that was relevant a year or two ago, now it has been replaced by more optimal components. if you buy a book about spark, you risk getting outdated knowledge. because while this book was being written, many changes in spark happened. for example, there are three apis for working with data now. they all appeared not immediately, but consistently. each of these apis was better than the previous ones. as a result, you often have to work with several of these interfaces in parallel that is a certain disadvantage. i think in the future there will be a single api for all components. spark is also following the path of strong integration with machine learning and deep learning. conclusion so, spark helps to simplify non-trivial tasks related to the high computational load, the processing of big data (both in real time and archived), both structured and unstructured. spark provides seamless integration of complex features — for example, machine learning and algorithms for working with graphs. spark carries the processing of big data to the masses. try it in your project — and you will not regret! if you liked this, show your support by clapping us to share with other people on medium. follow us on facebook , instagram , linkedin , behance , medium and visit our corporate blog for more news and articles on smart solutions. any questions? feel free to contact us ! originally written for s-pro blog. big data apache spark technology framework data processing -- -- written by igor izraylevych 293 followers · 33 following co-founder and ceo of s-pro, entrepreneur, advisor & expert in mobility & it strategy. custom solutions for enterprise and startups http://s-pro.io/ no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@datalackey/getting-spark-2-4-3-multi-node-stand-alone-cluster-working-with-docker-46ba745054c4,,,"getting spark 2.4.3 multi-node (stand-alone) cluster working with docker | by chris bedford | medium sitemap open in app sign up sign in medium logo write sign up sign in getting spark 2.4.3 multi-node (stand-alone) cluster working with docker chris bedford 3 min read · aug 25, 2019 -- listen share i recently went looking for a good docker recipe to locally launch a spark 2.4.3 cluster in stand-alone mode. running in ‘local’ mode is good for roughing out your business logic and unit tests, but it will not flush out bugs that only surface in a fully distributed environment. that is where integration tests come in, and while some organizations will set up a test cluster for this purpose, you don’t want to be twiddling your thumbs when your network is down, or your admin decides to take down the test cluster you depend on for maintenance. this is where docker comes to our rescue. this project contains a fairly recent (2.2.0) dockerfile and docker-compose.yml that will bring up a multi-node stand-alone spark cluster, but i wanted 2.4.3. so, i forked the project and brought it up to date, plus i added a little example project to make it easy to test out. below i lay out the steps you can follow to get a stand-alone cluster up and running on whatever machine you use (provided you have git, docker and docker-compose already installed). three caveats: (1) the docker-compose.yml is set to version “2” and if you use a later version than me, you might need to set it to “3”, (2) this was tested on linux, but i am very sure that docker commands on mac will work the same — not at all sure about windows, (3) i am assuming you have the proper version of scala for spark 2.4.3 installed (2.12.x) on your machine, and that you have downloaded spark 2.4.3 locally on your machine to run spark-submit. getting the cluster up in your environment open two terminals, in each one cd to /tmp. in the first, type: git clone git@github.com:buildlackey/docker-spark-standalone.git cd docker-spark-standalone docker-compose up you will see logs for the client (client_1), master (master_1), and worker (worker_1) nodes of the cluster. don’t worry if you see failed to connect to master/172.24.0.4:7077 in the worker_1 log at the start of the boot process. the worker is trying to connect to a master which is not fully up. this will work itself out, and in 5 seconds or so you should see: master_1 | 19/08/25 02:56:35 info master: registering worker 172.24.0.2:36655 with 4 cores, 4.0 gb ram worker_1 | 19/08/25 02:56:35 info worker: successfully registered with master spark://master:7077 now, in the second window type: cd /tmp/docker-spark-standalone/spark-example sbt package this will create the .jar file you will submit to spark as follows: spark-submit — master spark://127.0.0.1:7077 — class simpleapp \ — name simple target/scala-2.12/simple-project_2.12–1.0.jar you should then see output that looks something like this: 2019–08–24 20:08:25 warn utils:66 — your hostname, chris-laptop resolves to a loopback address: 127.0.1.1; using 192.168.1.83 instead (on interface wlp4s0) 2019–08–24 20:08:25 warn utils:66 — set spark_local_ip if you need to bind to another address 2019–08–24 20:08:25 warn nativecodeloader:62 — unable to load native-hadoop library for your platform… using builtin-java classes where applicable 2019–08–24 20:08:25 info sparkcontext:54 — running spark version 2.3.1 2019–08–24 20:08:25 info sparkcontext:54 — submitted application: simple application ….. lots more junk… 2019–08–24 20:08:30 info codegenerator:54 — code generated in 12.991607 ms + — — -+ |value| + — — -+ |hi ho| + — — -+ no luck submitting from my ide. after importing the sample project into intellij i thought there would be no problem running it via right click. but regretably, that was not my fate. i am continuing to see the error below when i run locally, which is really irksome. java.io.invalidclassexception: org.apache.spark.rpc.netty.nettyrpcendpointref; local class incompatible: if i figure this out i will update this post. -- -- written by chris bedford 2 followers · 2 following bay area-based s/w developer with broad experience in cloud-based, big data / ai / ml projects, using tools like spark, elastic search, cassandra and kafka. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@rmache/big-data-with-spark-in-google-colab-7c046e24b3,,,"big data with spark in google colab | by rodolfo maciéczyk | medium sitemap open in app sign up sign in medium logo write sign up sign in big data with spark in google colab rodolfo maciéczyk 10 min read · aug 8, 2019 -- 2 listen share building up the desire to extract the most relevant information from huge amounts of data, can lead us to what this fairly new term, “big data”, refers. although that desire always existed, with the generation of massive amounts of data from almost anything we do each day, the stack get’s bigger by the second. but “bigger”, depends from where we look at it. our technology get’s also powerful, maybe not at the same pace as the amount of data we create, but it definitely get’s smarter. hence, the time span of our capacity to extract and understand that data drops, and the cycle starts again. so, if big data is the desire , what are spark and colab ? the latter, are tools that complement a data scientist’s toolbox. the first one, is a framework that simplifies the task when working on distributed computing. it was developed from the beginning to work with machine learning algorithms, meaning that it has been optimized for high performance situations where data is accessed multiple times over many iterations. on the other hand google colaboratory, also known as colab, is a free environment that runs in the cloud and accessed through google drive, where you can run your scripts. combining a dataset with these two tools can lead us to perform big data. for the purpose of learning, we will have the following objective: build a machine learning model that predicts the cancellation of flights, using spark as our framework and colab as our environment. we will be following the next steps: know the dataset setup our colab and spark environment download the dataset directly from a website to our google drive import additional tools and setup constants connect to the spark server and load the data prepare, clean and validate the data setup and run our model in spark evaluate our model our dataset the dataset is hosted at stat-computing.org , however the original comes from rita (research and innovative technology administration) at the us department of transportation. there are several years we can explore, however, in this case we will only focus on two years, 2007 and 2008. only these two years sum up a total of 14'462'943 rows and 29 columns worth of data. setting up colab and spark environment assuming you have a google account, access your google drive, and create two folders, by clicking on: + new > folder. we can name them “colab datasets” and “colab notebooks”. next, we will connect colab to our drive. click on + new > more > + connect more apps , type colab in the search box, click on colaboratory and then on + connect. you can check colaboratory is added by clicking on + new > more. now let’s create a new notebook in our “colab notebook” folder. double click in the latter, and right click inside the folder. next, click on more > colaboratory . you have now created your first jupyter notebook in your google drive thanks to colab. this is the environment where you’ll write and run your code. you can choose to rename your notebook and call it flight_delays, click on the default name in the upper left hand side and rename it : we can now setup and install all the spark dependencies for python so we link our notebook with the spark server. also, we will import the required environment variables. copy and run the following code: # install spark-related dependencies !apt-get install openjdk-8-jdk-headless -qq > /dev/null !wget -q http://apache.osuosl.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz !tar xf spark-2.4.3-bin-hadoop2.7.tgz !pip install -q findspark !pip install pyspark # set up required environment variables import os os.environ[""java_home""] = ""/usr/lib/jvm/java-8-openjdk-amd64"" os.environ[""spark_home""] = ""/content/spark-2.4.3-bin-hadoop2.7"" be aware that depending on the time you are reading this article, you must replace the spark version with the latest. to do this, check the last version at pypi , and replace the version number in the code above. furthermore, we will point colab to our google drive. this will enable us to use the datasets we will download later to our “colab datasets” folder created earlier. copy and run the following code: # point colaboratory to your google drive from google.colab import drive drive.mount('/content/gdrive') you will be prompted to copy and paste an authorization code to allow your drive to get connected to colab. the result should look like this: data download to google drive now that you are set up, you can proceed to download the relevant datasets to your “colab datasets” folder. for this, i used a useful code from geeksforgeeks.org . this enables you to download any dataset from any website to your google drive, avoiding downloading to your local machine and uploading to your drive: # download datasets directly to your google drive ""colab datasets"" folder import requests # 2007 data file_url = ""http://stat-computing.org/dataexpo/2009/2007.csv.bz2"" r = requests.get(file_url, stream = true ) with open(""/content/gdrive/my drive/colab datasets/2007.csv.bz2"", ""wb"") as file: for block in r.iter_content(chunk_size = 1024): if block: file.write(block) # 2008 data file_url = ""http://stat-computing.org/dataexpo/2009/2008.csv.bz2"" r = requests.get(file_url, stream = true ) with open(""/content/gdrive/my drive/colab datasets/2008.csv.bz2"", ""wb"") as file: for block in r.iter_content(chunk_size = 1024): if block: file.write(block) note that you only need to do this once. if you run your code several times, you can comment each of the latter lines and bypass this block. import tools from pyspark and setup constants we need additional tools to connect to the spark server, load our data, clean it and prepare it to run our model. in this case, i’m choosing random forrest as our classifier, which is present in the pyspark machine learning library , so we will also setup the constants we will later need to run our model: # tools we need to connect to the spark server, load our data, # clean it and prepare it from pyspark import sparkcontext from pyspark.sql import sparksession from pyspark.ml import pipeline from pyspark.ml.classification import randomforestclassifier from pyspark.ml.feature import indextostring, stringindexer, vectorindexer, vectorassembler from pyspark.ml.evaluation import multiclassclassificationevaluator from pyspark.sql.functions import isnan, when, count, col # set up constants csv_2007= ""/content/gdrive/my drive/colab datasets/2007.csv.bz2"" csv_2008= ""/content/gdrive/my drive/colab datasets/2008.csv.bz2"" app_name = ""flight delays"" spark_url = ""local[*]"" random_seed = 141109 training_data_ratio = 0.7 rf_num_trees = 8 rf_max_depth = 4 rf_num_bins = 32 check your folder and dataset file names are exactly as the ones in the constants, otherwise, modify the code above to match your path. connect to the server and load the data counting on the constants we set above, we are now ready to connect to the spark server and load our datasets. we will also concatenate row-wise both datasets using pyspark’s .unionall method: # connect to the spark server spark = sparksession.builder.appname(app_name).master(spark_url).getorcreate() # load datasets df_2007 = spark.read.options(header=""true"",inferschema = ""true"").csv(csv_2007) df_2008 = spark.read.options(header=""true"",inferschema = ""true"").csv(csv_2008) # we concatenate both datasets df = df_2007.unionall(df_2008) we have now created a dataframe from two datasets. this actually is a spark dataframe, which is not the same as a pandas dataframe, maybe generating some confusion. although spark dataframes mimics each time closer to pandas, as the spark machine learning library mimics closer to the scikit-learn library, they are close but still not the same. some great advantages of a spark dataframe is that it is distributed, enabling, if the conditions are given, to speed up processes and get the benefits from parallel processing. prepare, clean and validate the data with our dataset loaded we can now check up it’s shape: print(f""the shape is {df.count():d} rows by {len(df.columns):d} columns."") returning: this is a massive amount of data (for now, thinking on what we mentioned in the beginning). later we will build vector variables for our random forrest classifier, and for this, we need that there are no null values, otherwise, spark will fail in building our vectors. we can check if null values are present running the following: null_counts = df.select([count(when(isnan(c) | col(c).isnull(), c)).alias(c)for c in df.columns]).topandas().to_dict(orient='records') print(f""we have {sum(null_counts[0].values()):d} null values in this dataset."") this will return that we have 14'248'147 null values in the dataset. if we run a quick exploration, for example with the .show() method, we can see that a great part of the cancellationcode column has null values. since the purpose of this article is to learn the steps to build a model using spark and colab, and not to make deep analysis of the given dataset, we will drop this column, and any additional cell that contains null values: df = df.drop(df.cancellationcode) df = df.na.drop() running the null_counts code above again, will conclude that no null values are present. and now running the shape code we used earlier to determine our dataset’s shape, will return the following: this reduced our dataset by 83'387 rows and a whole column, just 0.58 % row-wise. feature and label vector as i mentioned before, we will build vector variables which on one hand will be the label vector that we want our classifier to predict, and on the other hand, the feature vector, that is, the collection of columns we want our classifier to use to predict results. for this, we first inspect our dataset’s column types running: df.dtypes and as a return we will obtain: we will determine we want to use our cancelled column to build our label vector. why ? this column has two unique variables as we can check running: df.select('cancelled').distinct().rdd.map(lambda r: r[0]).collect() from the variable description referred in the dataset’s site, “1” indicates that “yes”, the flight has been cancelled, and “0” means “no”, the flight hasn’t been cancelled. this is what we want our model to predict. from the .dtypes method we run before, we can see that there are several “string” variables, these will be a problem when we want to generate our vector labels and run our model. therefore, we either convert them to other type of variables, or remove them. as our final objective in this article is to learn how to make all this work, and not analyze this given dataset, we will choose not to use these columns. hence, we will create a feature column list, with those int type columns we want to use for our feature vector: feature_cols = ['year', 'month', 'dayofmonth', 'dayofweek', 'crsdeptime', 'crsarrtime', 'flightnum', 'distance', 'diverted'] next, we will generate our feature vector and add the corresponding column to the end of our dataframe, which will be undergoing the .transform() method: df = vectorassembler(inputcols=feature_cols, outputcol=""features"").transform(df) now we can isolate both input columns we will use to train and test our model, and view how they look like: df.select(""cancelled"", ""features"").show(5) setup and run our model in spark we are now ready to build our indexers, split our dataset into 70 % for our training set and 30 % for our test set, define the parameters of our model and finally link everything together into a pipeline which we’’ll later use to actually run the model: # generate a labelindexer labelindexer = stringindexer(inputcol=""cancelled"", outputcol=""indexedlabel"").fit(df) # generate the indexed feature vector featureindexer = vectorindexer(inputcol=""features"", outputcol=""indexedfeatures"", maxcategories=4).fit(df) # split the data into training and tests sets (trainingdata, testdata) = df.randomsplit([training_data_ratio, 1 - training_data_ratio]) # train the randomforest model rf = randomforestclassifier(labelcol=""indexedlabel"", featurescol=""indexedfeatures"", numtrees=rf_num_trees) # chain indexers and the forest models in a pipeline pipeline = pipeline(stages=[labelindexer, featureindexer, rf]) having both our training and test sets ready, we can train our model and make predictions running the .transform() method: # train model model = pipeline.fit(trainingdata) # make predictions predictions = model.transform(testdata) evaluation and considerations at this moment we can use the multiclassclassificationevaluator to test our model’s accuracy: evaluator = multiclassclassificationevaluator( labelcol=""indexedlabel"", predictioncol=""prediction"", metricname=""accuracy"") accuracy = evaluator.evaluate(predictions) print(f""test error = {(1.0 - accuracy):g}"") print(f""accuracy = {accuracy:g}"") this looks like a great accuracy result ! for the purpose of this article, the steps we took helped us to chain everything together, and show us how easy you can link a dataset allocated in google drive with spark within google’s colab environment. however, you need to be aware that this result needs further analysis and wrangling. one of the main issues that we have considering this result, is that in previous steps we should have checked how balanced our data is, i.e. , how many “0”’s versus “1”’s are present in the cancelled column, and according to that result, we over or under sample our database. using spark is a great gateway to manipulate really large and multiple datasets, and reduce your processing time, so the benefits of combining it with colaboratory and your google drive are immense! you can check out the complete code at my github repository . python ai machine learning data science big data -- -- 2 written by rodolfo maciéczyk 19 followers · 2 following data scientist & entrepreneur responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/clairvoyantblog/introduction-to-the-databricks-community-cloud-aa8b8adb091,,,"deep dive into the databricks community | data visualization  | clairvoyant blog sitemap open in app sign up sign in medium logo write sign up sign in mastodon clairvoyant blog · follow publication clairvoyant is a data and decision engineering company. we design, implement and operate data management platforms with the aim to deliver transformative business value to our customers. follow publication member-only story introduction to the databricks community cloud explaining what the databricks community cloud is and how you can leverage it robert sanders 7 min read · jan 29, 2019 -- share the databricks community cloud is a free version of databricks’ cloud-based big data platform for business. with this product, users can spin up micro-clusters running configurable versions of apache spark, create and manage notebooks that can execute spark code and much more. in this post, we’ll go over some of the high-level features and provide a step by step example on how you can get started with databricks to showcase some of the main features. why is it useful? learning about spark testing different versions of spark rapid prototyping data analysis code repository and more… getting started first thing you’ll need to do is to create an account and log in. to do so, follow the url below and click “sign up”. https://community.cloud.databricks.com/login.html log in once you create an account and log in, you’ll see the below home page: -- -- follow published in clairvoyant blog 1.1k followers · last published feb 12, 2025 clairvoyant is a data and decision engineering company. we design, implement and operate data management platforms with the aim to deliver transformative business value to our customers. follow written by robert sanders 1.7k followers · 392 following senior avp of data management for exl services | marathon runner | triathlete | endurance athlete no responses yet help status about careers press blog privacy rules terms text to speech",12
https://yousry.medium.com/yet-another-spark-serialization-article-c87b5abe1d38,,,"yet another spark serialization article | by yousry mohamed | medium sitemap open in app sign up sign in medium logo write sign up sign in yet another spark serialization article because seeing is believing yousry mohamed 6 min read · apr 14, 2019 -- listen share https://www.wired.com/2010/04/floppy-disks-a-eulogy/ well, the topic of serialization in spark has been discussed hundred of times and the general advice is to always use kryo instead of the default java serializer. it’s claimed to be faster and saves disk space as well. but why should we worry much about how spark serializes its internal data frames or rdds. there are two main reasons: joins and grouping operations will be hugely impacted by serialization as they will usually involve data shuffling among cluster nodes. the faster and smaller the amount of data to be shuffled, the quicker the join/grouping operation would be. caching also depends on serialization specially when caching to disk or when data spills over from memory to disk and also in the case of memory_only_ser storage level which caches in memory but serializes the data in big data scenarios, a slight improvement might result in huge performance gain due to the massive amount of data being processed, think something like chain reactions. that sounds like good marketing talk but yet to be verified practically. it’s a bit hard to benchmark such things in a quick blog post using a practical application specially using join or grouping use cases. my target is to compare both serialization options in a small spark sandbox application using disk caching to get a feeling if there is really a benefit of using kryo because seeing is believing. setup nothing too complicated is required. a local spark 2.x environment is all that is needed. for sure i have also the tooling to write and compile scala apps. intellij community edition is good enough for that. and by the way, i am running on windows using spark 2.3.2 the code it’s a canonical scala console application with sbt built tool. the first scala file we have is a data generator class to generate some test data. one thing to highlight here, kryo shines when there is a complex object graph to serialize. i started initially with simple classes (flat case class with a bunch of simple fields) but in such case there was no major difference between kryo and java. also i will use rdds and not data frames but i guess same outcome applies to data frame if we have similar schema. anyway, it’s pretty obvious from the below snippet that we can call a singleton object to generate a 100,1000 element array of dummy invoice data. the second and final class is the application class. it’s main job is to call the above generator and wrap the generated array into an rdd and then cache it to disk and then apply a couple of actions on this rdd. the actions are not very relevant but the first one of them is needed to trigger the aching behaviour. also this class parses command line arguments and checks if a certain parameter is sent and if so it switches from the default (java) serializer to kryo. the application class job is to: parse command line arguments and use kryo if application is called with a “kryo” keyword in command line arguments part of kryo registration is to register all classes included in the rdd to be serialized. it’s also crucial to include the next line as it forces spark to use kryo against all downstream classes in object graph to be serialized conf.set(""spark.kryo.registrationrequired"", ""true"") create an rdd out of the dummy data from the generator and cache (persist) it to disk. we could also used storagelevel.memory_only_ser but disk_only is good enough to test stuff from spark ui and locally on the file system as well. running the test let’s give the above code a go and see what we shall get. first we will try the default java serializer. assuming the application is compiled correctly in inetllij and packaged using sbt, we can run the following in a cmd/powershell window: spark-submit --class serializer --master local[*] serializer_2.11-0.1.jar if everything is ok, the shell window will show something like: now let’s see how much disk space is used to cache this dataset. spark ui is hosted on http://localhost:4040/ in the local case. opening this url and heading to storage and we can see more details about cached data in spark cluster. because the data is cached to disk and in application class we specified a custom temp directory, we can also view the local files used to cache this rdd. i have 8 logical cores on my laptop so i got 8 files as i triggered spark application to make use of all cores available. ok, java scores a 20mb for this test app so let’s see if kryo scores better. to force kryo we just need to call same application but pass “kryo” at the end of the command. spark-submit --class serializer --master local[*] serializer_2.11-0.1.jar kryo spark ui shows much less disk space used to store the persisted rdd. also it’s easy to verify if kryo is the current serializer from environment tab. kryo scored 13.8mb while java scored 20mb and this is roughly 31% improvement against this small dataset with not too deep object graph. this comparison is for the disk/memory space used and it does not include cpu/compute factor but i am happy with current finding. in a nutshell, if i have a complex object graph with heaps of data and likelihood of shuffling then kryo is a must. notes kryo is not set as the default serializer because it needs an explicit registration step. kryo can without the request to register all classes but probably there will be no benefit as class names have to be persisted with data. for data frames, spark uses a different encoder neither kryo nor java. so the above discussion doesn’t apply to data frames unless they are converted to pure rdds. testing the same application but tweaking it to use a data frame instead of an rdd yields a 30mb serialized data frame size for both java & kryo so effectively there is no difference between them for data frames. the difference can be spotted easily from the screenshot as well. rdds show as parallelcollectionrdd while data frames show as the logical execution plan which is localtablescan in our case. cached partitioned count is different as i did the data frame example on a different laptop but trust me this is not the root cause behind the 30mb. 😂 source code in case you would like to pull this super complex app and give it a go 😉 ylashin/spark-serialization-test simple spark app to compare java vs kryo serialization - ylashin/spark-serialization-test github.com spark kryo -- -- written by yousry mohamed 421 followers · 13 following yousry is a principal data engineer working for mantel group. he is very passionate about all things data including big data, machine learning and ai. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@xaviergeerinck/creating-a-big-data-cluster-with-sql-server-2019-d333c6b7406a,,,"creating a big data cluster with sql server 2019 | by xavier geerinck | medium sitemap open in app sign up sign in medium logo write sign up sign in creating a big data cluster with sql server 2019 xavier geerinck 6 min read · jan 17, 2019 -- 1 listen share sql server 2019 came out in preview a while ago, and the new features announced are just wonderful! a one-to-go-tool for all your big data needs: unstructured and structured data that can be processed using just one tool! but the most wonderful feature of all in my opinion are the “big data clusters”, allowing you to spin up scalable clusters and deploy sql server, spark and hdfs containers side-by-side. but how does it accomplish all of this? well the architecture below shows us the magic: so what will we do in this post? well here we will do just that! deploy our big data cluster and start running our notebooks and sql queries on it! note: there is documentation available that explains all of this, but i would like to present an easy to understand view for all developers out there. in any case feel free to check this documentation at: https://docs.microsoft.com/en-us/sql/big-data-cluster/quickstart-big-data-cluster-deploy big note: this is a very large deployment and it’s recommended to have a aks cluster size of 64gb ram ! prerequisites before we can get started, it’s required to have mssqlctl and azure-cli installed and have an aks cluster running. az cli # needed on wsl sudo apt-get update sudo apt-get install -y libssl-dev libffi-dev sudo apt-get install -y python-dev # modify sources sudo apt-get install apt-transport-https lsb-release software-properties-common -y az_repo=$(lsb_release -cs) echo ""deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $az_repo main"" | \ sudo tee /etc/apt/sources.list.d/azure-cli.list # microsoft signing key sudo apt-key --keyring /etc/apt/trusted.gpg.d/microsoft.gpg adv \ --keyserver packages.microsoft.com \ --recv-keys bc528686b50d79e339d3721ceb3e94adbe1229cf # install sudo apt-get update sudo apt-get install azure-cli mssqlctl # install python sudo apt-get update && / sudo apt-get install -y python3 && / sudo apt-get install -y python3-pip && / sudo -h pip3 install --upgrade pip # install mssqlctl pip3 install --index-url https://private-repo.microsoft.com/python/ctp-2.2 mssqlctl aks deploy an aks instance quickly: note: we can get the node sizes easily through the command: az vm list-sizes -l <your_location> --query ""[?numberofcores<=`16 ` && numberofcores>= ` 8 ` && memoryinmb>= ` 16384 ` ]"" -o table — see http://jmespath.org/examples.html note 2: we can get our kubernetes versions through az aks get-versions -l <your_location> --query ""orchestrators[].orchestratorversion"" # set our azure context az account set -s <subscription_id> # create our resource group (e.g. az group create -n xavier-sqlbigdata -l westeurope) az group create -n <your_name> -l <region> # create aks in our resource group # e.g. az aks create --name xavier-sqlbigdatacluster --resource-group xavier-sqlbigdata --generate-ssh-keys --node-vm-size ""standard_d8_v3"" --node-count 3 --kubernetes-version ""1.11.5"" az aks create --name <cluster_name> --resource-group <your_name> --generate-ssh-keys --node-vm-size ""standard_d8_v3"" --node-count 3 --kubernetes-version ""1.11.5"" # get the credentials # e.g. az aks get-credentials --name xavier-sqlbigdatacluster --resource-group xavier-sqlbigdata --admin az aks get-credentials --name <cluster_name> --resource-group <your_name> --admin # if needed, view the cluster with # e.g. az aks browse --name xavier-sqlbigdatacluster --resource-group xavier-sqlbigdata az aks browse --name <cluster_name> --resource-group <your_name> create our sql server big data cluster # create sql big data cluster # e.g. mssqlctl create cluster sql-server-cluster export accept_eula=y export cluster_platform=""aks"" export controller_username=""admin"" # change to what you want export controller_password=""test123#"" # change to what you want export mssql_sa_password=""test123#"" export knox_password=""test123#"" export docker_registry=""private-repo.microsoft.com"" export docker_repository=""mssql-private-preview"" export docker_username=""docker-email@something.comm"" export docker_password=""docker-password"" export docker_email=""docker-email@something.com"" export docker_private_registry=""1"" mssqlctl create cluster <cluster_name> let’s start easy by creating our sql server big data cluster, this will deploy the complete cluster on the aks cluster we created earlier. it will look like this: 2019-01-16 09:30:35.0826 utc | info | creating cluster... 2019-01-16 09:30:37.0313 utc | info | deploying controller... 2019-01-16 09:30:39.0764 utc | info | the service account token is ready for controller 2019-01-16 09:30:41.0613 utc | info | waiting for controller pod to be up... ... it takes a while ... 2019-01-16 09:39:40.0930 utc | info | waiting for controller pod to be up... 2019-01-16 09:39:46.0337 utc | info | controller pod is running. 2019-01-16 09:39:46.0542 utc | info | controller endpoint: https://11.22.33.44:30080 2019-01-16 09:43:15.0898 utc | info | deployment progress can be tracked at portal endpoint: https://11.22.33.44:30777/portal/ 2019-01-16 09:43:15.0905 utc | info | deploying cluster... 2019-01-16 09:43:16.0627 utc | info | cluster monitoring is ready. 2019-01-16 09:43:16.0627 utc | info | initiating cluster creation. 2019-01-16 09:43:16.0628 utc | info | creating cluster with name: sql-server-cluster 2019-01-16 09:56:14.0388 utc | info | control plane is ready. 2019-01-16 10:02:38.0710 utc | info | storage pool is ready. 2019-01-16 10:02:38.0711 utc | info | data pool is ready. 2019-01-16 10:03:06.0027 utc | info | master pool is ready. 2019-01-16 10:03:23.0044 utc | info | compute pool is ready. 2019-01-16 10:03:24.0355 utc | info | cluster state: ready 2019-01-16 10:03:24.0355 utc | info | monitor and track your cluster at the portal endpoint: https://11.22.33.44:30777/portal/ 2019-01-16 10:03:24.0356 utc | info | cluster deployed successfully. allowing you to login on the your cluster! display information about our cluster let’s see how we can get the information to login on our cluster through commands for automation: # note: view detailed info through `kubectl get service kubernetes -o json` # get endpoint-master-pool # e.g. kubectl get svc endpoint-master-pool -n sql-server-cluster -o=custom-columns=""""ip:.status.loadbalancer.ingress[0].ip,port:.spec.ports[0].port"""" kubectl get service endpoint-master-pool -n <cluster_name> -o=custom-columns=""""ip:.status.loadbalancer.ingress[0].ip,port:.spec.ports[0].port"""" # get service security loadbalancer # e.g. kubectl get svc service-security-lb -n sql-server-cluster -o=custom-columns=""""ip:.status.loadbalancer.ingress[0].ip,port:.spec.ports[0].port"""" kubectl get service service-security-lb -n <cluster_name> -o=custom-columns=""""ip:status.loadbalancer.ingress[0].ip,port:.spec.ports[0].port"""" # cluster admin portal # e.g. kubectl get svc service-proxy-lb -n sql-server-cluster -o=custom-columns=""""ip:.status.loadbalancer.ingress[0].ip,port:.spec.ports[0].port"""" kubectl get service service-proxy-lb -n <cluster_name> -o=custom-columns=""""ip:status.loadbalancer.ingress[0].ip,port:.spec.ports[0].port"""" we can now open our azure data studio and login on our clusters: sql: ip and port received from endpoint-master-pool (use <ip>,<port> format in data studio) spark/hdfs: ip received from service-security-lb (no port required on connection) note: for spark/hdfs we use username: root , password: <your_password> note: for sql we use username: sa , password: <your_sa_password> big data sql development ai software development -- -- 1 written by xavier geerinck 327 followers · 34 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/data-science/could-julia-replace-scala-77b73c345f6e,,,"could julia replace scala?. let’s be honest: a lot of us would… | by emma boudreau | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. member-only story could julia replace scala? emma boudreau 5 min read · nov 9, 2019 -- 4 share l et’s be honest: a lot of us would really love to remove scala from our data-science workflow. spark is a great way to manage enterprise-level hadoop and coordinate workers for deep learning, however, the effort overhead is significantly heavier than that of traditional statistical languages like r and python. fortunately, there is a language developed at mit that is making its academic rounds from mit that holds a lot of potential to combine all the things we love about spark, python, and r into one convenient package. spark is a multi-functional data-management tool built within scala, and is the most common segment of the scalable language that data-scientists tend to work in. spark’s primary strength comes with big data, as its integration with hadoop as well as fast speed make it extremely ideal for use with large, un-managed datasets that require a-lot of processing in order to access and manipulate. however, spark’s inheritence leaves it dead in the water in a-lot of ways that would traditionally make a language non-viable for machine learning operations. a debatable disadvantage of spark is that spark is far more difficult to use and read than the other languages typically used in data science… -- -- 4 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by emma boudreau 5.8k followers · 26 following i am a computer nerd. i love art, programming, and hiking. https://github.com/emmaccode responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/expedia-group-tech/unit-testing-apache-spark-applications-using-hive-tables-ec653c6f25be,JUnit,Unit Testing,"unit testing apache spark applications using hive tables | by neeraj prithyani | expedia group technology | medium sitemap open in app sign up sign in medium logo write sign up sign in expedia group technology · stories from the expedia group technology teams unit testing apache spark applications using hive tables techniques for creating and managing unit tests of spark batch applications neeraj prithyani 4 min read · feb 25, 2019 -- 2 listen share at homeaway , we have many batch applications that use apache spark to process data from hive tables based on s3 datasets. these applications perform spark sql transformations to generate their final output. the transformations are written similarly to an sql stored procedure in an rdbms. we wanted to bake unit tests into the intermediate steps to ensure our applications are robust and can catch any breaking changes in the future. however, we faced some challenges. challenges hive tables and s3 datasets are not available on developers’ local machines (or on build servers like jenkins) to run tests without additional setup. the code needs a local spark session to run. we needed a way to get a spark session for local (and jenkins) that does not connect to the hive metastore. the team evaluated many options and established best practice to use the spark-testing-base library for unit testing such applications. below are some key techniques that helped us unit test our spark application. toolset spark-testing-base spark-testing-base is a library that simplifies the unit testing of spark applications. it provides utility classes to create out-of-the-box spark sessions and dataframe utility methods that can be used in assert statements. scalatest scalatest is a powerful tool that can be used to unit test scala and java code. it is similar to junit for java. setup add a dependency for spark-testing-base to your pom.xml : <dependency> <groupid>com.holdenkarau</groupid> <artifactid>spark-testing-base_2.11</artifactid> <version>${spark.version}_0.10.0</version> <scope>test</scope> </dependency> add the scalatest maven plugin to your pom.xml : <plugin> <groupid>org.scalatest</groupid> <artifactid>scalatest-maven-plugin</artifactid> <version>2.0.0</version> <configuration> <reportsdirectory>${project.build.directory}/surefire-reports</reportsdirectory> <junitxml>.</junitxml> <filereports>wdf testsuite.txt</filereports> </configuration> <executions> <execution> <id>test</id> <goals> <goal>test</goal> </goals> </execution> </executions> </plugin> key techniques we used using the out-of-the-box local spark session using trait sharedsparkcontext makes a locally generated spark session available to use without doing anything else. out-of-the-box local spark session we refactored our classes to receive this spark session from the main class using dependency injection. in the production environment, this points to the production hive metastore. during local testing and unit tests this points to the metastore in the jvm. test bed (test database) setup all the databases and tables that the application uses can be defined up front pointing to a temporary location. this can be done in a utility class that does this test bed setup. we will then call this utility class before the tests. moreover, any tables that are needed for the code to work, but not necessarily needed for unit testing, can be defined up front as empty tables. we’ll discuss defining tables with specific data for unit testing in the next section. test bed setup using csv files to populate hive tables we found that using csv was pretty simple for defining data that any of our tables needed for unit testing. once set up, csvs were simple to edit for various data scenarios. steps: use structtype to define the table schema. only define columns that your program uses. there is no point in defining all the columns that are in the hive table if they are not used in our app! use spark csv reader to create a dataframe pointing to csv files stored in test/resources store csv with all data combinations in test/resources use spark’s saveastable method to define a hive table from this dataframe defining and loading tables for unit tests create csv files in test/resources dataframe assert method trait dataframesuitebase provides method named assertsdataframeequals that can be used to compare two dataframe s. assertdataframeequals method using scalatest matchers to assert dataframe elements scalatest trait matchers provides easy to read assert checks. we used these to test individual elements in the test result dataframes. scalatest matchers conclusion these were some techniques we used to unit test our spark batch applications. there are some features that spark-testing-base provides like generating test datasets, dataframes, and resilient distributed datasets (rdds) that are useful. the streamingsuitebase also looks very promising and easy to use for spark streaming applications. being new to scala and then looking at all the features scalatest provides was refreshing. we used funsuite, but there are many different styles we can explore and incorporate. this was an important step for our team towards improving our unit testing coverage for spark batch applications. testing scala-based spark code snippets in notebooks and then porting them in a scala app improves time to release. we are excited to continue building more unit testing techniques for other spark based applications! apache spark data science unit testing hive scala -- -- 2 published in expedia group technology 5k followers · last published jun 3, 2025 stories from the expedia group technology teams written by neeraj prithyani 41 followers · 2 following software developer at expedia group. passion for big data processing. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://nehajirafe.medium.com/what-is-spark-javatopython-failure-3a1587e30da5,,,"what is spark “javatopython” failure? | by neha jirafe | medium sitemap open in app sign up sign in medium logo write sign up sign in what is spark “javatopython” failure? neha jirafe 2 min read · sep 17, 2019 -- listen share with a long history of working with java, i was so excited to use python for the new project i was working on. boom, i launched a emr cluster, and started the pyspark console. wow pretty, i could quickly protype my data exploration. i started of writting the data pipeline using python and tested it working perfectly with the test load. working with real load !! time to deploy with the real load. oh no — “container killed by yarn for exceeding memory limits” i was totally clueless when my jobs failed repeatedly at this stage after almost 2 hours of execution. i tried tweaking the spark executors , memory etc as mentioned in this blog , and i had to wait for another 2 hours to see the job fail miserably “javatopython” ? after a nightfull of introspection, the root cause was identified. the spark job was loading huge amount of data from database, since spark uses jdbc , this data was loaded as java objects (javardd) , which were then transformed to python datasets. double serialization eating up all my memory and failing the jobs. solution of course tuning the executors and memory was the base of the solution, however another important aspect was creating appropriate number of partitions while loading the data from database. conclusion python might not be the best tool with spark, its worth investing in scala in case you have some serious loads to work on. and thats all for this story big data spark python emr aws -- -- written by neha jirafe 137 followers · 78 following discovering the world from data lens , lead data engineer https://www.linkedin.com/in/neha-jirafe-16257310/ no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@shanikapathirana7/real-time-big-data-processing-hadoop-vs-apache-spark-278eccb9ca8c,,,"big data : real-time processing — hadoop vs apache spark | by shanika pathirana | medium sitemap open in app sign up sign in medium logo write sign up sign in big data : real-time processing — hadoop vs apache spark shanika pathirana 4 min read · dec 13, 2019 -- listen share big data consists of large number of data sets, which cannot be captured, cleansed, managed, and processed using most commonly used software within a limited time. those large data sets are in the form of unstructured, semi-structured or structured data. data processing is converting data into desired and usable form. considering the current trends, prevailing techniques analyze big data sets at a scale of terabyte and petabyte. like in twitter, stock market and banking, millions of data is generated in each second and those data is processed by real-time data analysis. this research is done considering few big data processing frameworks and their suitability with various user requirements. traditional solution in the early stages, data is stored and processed in a computer located at the organization itself. and databases like oracle and ibm are used for storing big data. here there is an application which handles data storage and analysis, based on the interaction of user. there’s a limitation in this approach. it is suitable for applications where less voluminous data that can be stored in standard database servers is processed or up to the limit of the processing capacity. but it’s a difficult task to process big data through a single database bottleneck. modern solution google introduced an algorithm called mapreduce, as a solution for this problem. mapreduce, which is a parallel programming model, divides the task into small parts and distributes the tasks to many computers, and collects the results from them to form the result data set after integration. there are many big data processing frameworks. and hadoop and spark are well known among them. hadoop this open-source framework lets users to store and process big data in a distributed environment across clusters of computers with the use of simple programming models. hadoop is scalable from single servers to thousands of machines, where each offers computation and storage locally. here the mapreduce algorithm is used to run applications, processing the data in parallel with others. hadoop can be used for developing applications to perform statistical analysis on large amount of data. it works on data locality principle, which is the process of taking computations closer to the place where actual data resides on the node, instead of moving large data to computation. there are two layers at the core of hadoop. first layer is mapreduce, which is used for processing or computation. second layer is hadoop distributed file system (hdfs), which is used for storage. hadoop distributed file system hdfs, which is based on the google file system (gfs), provides a distributed file system that is designed to run on commodity hardware. hdfs can be deployed on low-cost hardware and it is highly fault-tolerant, providing high rate of application data access, being much suitable for applications having large data sets. core tasks that hadoop performs are initially data is divided into directories and files, with uniform sized blocks of 128m or 64m, which in turn distributed across number cluster nodes for processing further, under the processing supervision of hdfs. to withstand hardware failures, all the blocks are replicated. it is checking whether the code is executed successfully. it performs the sorting which takes place between the map and reduce stages and the sorted data is sent to a specific computer. debugging logs for each job is written. advantages of using hadoop framework are allowing users to quickly write and efficiently test distributed systems, and it automatic distributes the data and work across the machines and in turn, utilizes the underlying parallelism of the cpu cores. servers can be added or removed from the cluster dynamically without any interruption in the operation. since hadoop is based on java, it is compatible on all the platforms. however the limitation of hadoop is that it is unfit for large data on network. spark for batch processing large data sets, data processing ability of spark in real time is 100 times faster than hadoop mapreduce. in addition, spark has the capability to support streaming of data along with distributed processing. but there are many disadvantages in using spark as well. since there is no automatic optimization process, the user has to optimize the code manually. and also there presence a limited number of algorithms. when using spark along with hadoop, users confront issues in using small files, since hdfs is not designed to provide a large number of small files, but a fewer number of large files. comparison between hadoop and spark for data processing using hadoop, data is stored over a long period of time, which is based on batch processing of big data. but a real-time data feed can be used for data processing using spark. although mapreduce can also be used to process real-time data, its speed is nowhere close to the speed of spark. big data hadoop spark processing -- -- written by shanika pathirana 3 followers · 1 following no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/yotpoengineering/introducing-metorikku-big-data-pipelines-using-apache-spark-f04456f7d5a8,,,"introducing metorikku — big data pipelines using apache spark | by ofir ventura | yotpo engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in yotpo engineering · we're the engineering department of yotpo, we share our insights and thoughts on developing and running large scale web and data applications. introducing metorikku — big data pipelines using apache spark ofir ventura 4 min read · jan 22, 2018 -- 3 listen share big data solutions and platforms have become a major trend in the tech world. we can better understand our data and get meaningful insights from it. the pain is, how to store, manage and process the data effectively. usually the challenges are to compute real meaningful insights, build personalised recommendation systems, increase engagement, apply fraud detection, churn prevention, reporting, text analysis using nlp, or any other processing. in this blog post, i will be introducing metorikku, and how we use it to build batch data pipelines, combining many data sources efficiently, saving valuable resources and time. what is metorikku? metorikku is a distributed etl engine built on top of apache spark sql. by creating a simple configuration, you can define your input sources, your data manipulation steps and lastly your output sources. metorikku integrates with a wide variety of popular data sources such as cassandra, redshift, redis, segment, json, parquet and so on. data manipulations are performed by a predefined set of metrics which runs using spark’s distributed sql engines. whether you need to support a/b testing, train machine learning models, or pipe transformed data into various data stores, metorikku provides the infrastructure to perform all data preparations. on top of it, you can apply machine learning models, perform graph processing and the organisation’s various applications can consume the results. metorikku came to life after we encountered several similar use cases again and again. from data cleaning and preparation tasks to powering dashboards and email digests in production, metorikku is used by several teams in yotpo including big data, data engineering, data science, bi analysts and full stack developers. by exposing our different data sources and stores and making them easily approachable and query-able, we have definitely made an impact by making our company more “data-driven” than ever before. how it works? all you need to start is a running spark cluster. requires apache spark v2.2 and above. metorikku has easy-to-use instructions and flexible configurations, so you can easily create data processes. we like to schedule metorikku jobs using apache airflow made by airbnb. running metorikku requires defining your inputs , outputs , and metrics . metorikku loads the data, initialises a spark session and registers the input tables. a metric is defined by its sql steps in the metric configuration file, and each step defines a spark dataframe which you can select from on your next steps. after the dataframes are computed, metorikku output writers handles the writing process. basic configuration examples to run metorikku you must first define 2 files. the first is a yaml configuration file which includes your input sources, output destinations and metrics files locations — for further explanation on the different configurations metorikku support please go the project repository on github. a simple movies.yaml file for metorikku could be as follows: a metric file defines the steps and queries of the etl as well as where and what to output. for example, a simple configuration json should be as follows: notice that once we registered our inputs using the yaml configuration, we can now use them inside our queries as a given data source. metorikku also provides a built-in testing framework named “metorikku tester”, which helps writing tests to your data manipulations by defining mock data and the desired outputs. simple mocks file are defined using the jsonl format lastly, we can feed the following json file to metorriku tester in order to run our tests looking ahead it’s impressive how much the big data infrastructures and tools have improved over the past years. we’ve come a long way through hadoop clusters and complex mapreduce implementations, to easy-to-use frameworks, data infrastructures and apis. with few exceptions, you shouldn’t build infrastructures or tools from scratch these days, and you can save a substantial amount of your developers capacity. i expect that big data infrastructures and tools will continue to grow fast, at least as fast as the data itself! the new metorikku tool offers a simple platform to combine your data sources, so you can analyse, test, and write the results to a data source of your choice. metorikku will continue to evolve, adding useful features in the future such as adding kafka or other streaming platforms to enable the creation of lambda applications, adding more types of writers and readers such as jdbc, a web interface for easy creation of metrics and configurations and much more. you are welcome to check out our contributing guide , or comment for any further questions. big data apache spark etl data pipeline distributed systems -- -- 3 published in yotpo engineering 312 followers · last published oct 27, 2024 we're the engineering department of yotpo, we share our insights and thoughts on developing and running large scale web and data applications. written by ofir ventura 75 followers · 2 following ml & data engineering manager @ lemonade responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://garystafford.medium.com/getting-started-with-pyspark-for-big-data-analytics-using-jupyter-notebooks-and-docker-ba39d2e3d6c7,,,"getting started with pyspark for big data analytics using jupyter notebooks and jupyter docker stacks | by gary a. stafford | medium sitemap open in app sign up sign in medium logo write sign up sign in member-only story getting started with pyspark for big data analytics using jupyter notebooks and jupyter docker stacks gary a. stafford 16 min read · nov 22, 2018 -- 9 share an updated version of this popular post is published in towards data science: getting started with data analytics using jupyter notebooks, pyspark, and docker introduction there is little question, big data analytics , data science , artificial intelligence (ai), and machine learning (ml), a subcategory of ai, have all experienced a tremendous surge in popularity over the last few years. behind the hype curves and marketing buzz, these technologies are having a significant influence on many aspects of our modern lives. due to their popularity and potential benefits, academic institutions and commercial enterprises are rushing to train large numbers of data scientists and ml and ai engineers. search results courtesy googletrends ( https://trends.google.com ) learning popular programming paradigms, such as python, scala, r, apache hadoop, apache spark, and apache kafka, requires the use of multiple complex technologies. installing, configuring, and managing these technologies often demands an advanced level of familiarity with linux, distributed systems, cloud- and container-based platforms, databases, and data-streaming applications. these barriers may prove a deterrent to students, mathematicians, statisticians, and data scientists. -- -- 9 written by gary a. stafford 5k followers · 753 following area principal solutions architect @ aws | 14x aws certified gold jacket | polyglot developer | dataops | genai | technology consultant, writer, and speaker responses ( 9 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@saipeddy/setting-up-a-thrift-server-4eb0c55c11f0,,,"setting up a thrift server. run sql queries against your data | by sai peddy | medium sitemap open in app sign up sign in medium logo write sign up sign in setting up a thrift server run sql queries against your data sai peddy 6 min read · feb 20, 2018 -- 1 listen share photo by rob lambert on unsplash this blog post will be the final installment of setting up a big data sql querying system . this post will walk you through setting up a thrift server so that you are able to query the parquet data you had previously generated. whether or not you are following the series of blog post this will still walk you through setting up a spark/hive thrift server. before we get any further, you might be asking what exactly is thrift? this blog helped me tremendously when i was playing with the thrift server to improve its performance. it also has an excellent description of the thrift server and its history. here is just a piece of that information: spark (sql) thrift server is an excellent tool built on the hiveserver2 for allowing multiple remote clients to access spark. it provides a generic jdbc endpoint that lets any client including bi tools connect and access the power of spark. let’s talk about how it came to be and why you should use it. — russell spitzer two notes before starting: this tutorial assumes you have spark downloaded — i downloaded spark-2.2.1 this is not a production deploy, this was all done locally. starting thriftserver starting the thrift server is fairly simple: #spark_path = the download of spark you just downloaded cd ${spark_path}/sbin/ ./start-thriftserver.sh your thriftserver should now have started. however, just running the thrift server like this gets you a pretty bare bone and weak system to run big data queries, but it can work. expanding on this, you have a few options when configuring thrift — but we will cover just the following two: limit the number of resources that the thriftserver application consumes connect to a bigger spark cluster — to increase the allocation of resources configuring thrift: this section is not mandatory but has useful configuration information. the following configurations can be added to ./start-thriftserver.sh command. specify a default port: --hiveconf hive.server2.thrift.port=9999 limit the executor memory: --executor-memory 1g — this gives the thrift server 1gb of memory per executor that it runs on. specify the core count: --conf spark.cores.max=100 so, you just witnessed 3 different ways to set configs for the thrift server. this might be confusing if this is your first time with thrift. if you visited the blog post above, you know that thrift is basically a combination of spark and hive, which leads to these configs. typically --hiveconf refers to configurations in hive, while in this context --conf leads to configurations in spark. everything else is prebuilt configs for the server to take. prebuilt also includes specifying the spark master to connect to. connecting to masters: --master 10.111.111.111:7077,10.111.111.112:7077 when starting up the thrift server, if you specify a master (or multiple) for the server to connect to, then the server is able to use the resources available on that cluster. if you navigate to your spark cluster, you should see an application as such: clicking on that application will take you into the application page, and you should have the following information available to you. the ui can definitely be useful, but also has a few quirks that would be worth knowing. the best way to learn is to start experimenting with queries and seeing what happens (i’ll show you one way to start experimenting in the next section). however, one useful thing to know is that a sql query does not necessarily correlate to just one job — a query can sometimes create multiple jobs. perfect time to show a nice diagram about how the overall system connects and interacts. connecting via beeline: beeline should already be available with the package of spark you downloaded. using the following commands, you can connect to your thrift server using beeline. #assumption: you are running thrift on the default port cd ${spark_path}/bin/beeline !connect jdbc:hive2://localhost:10000 #you will be prompted for username & password-click enter if not set if you are following the series of blogposts you should already have parquet data in this folder structure: | testbucket ---|parquetfiles ---|year=2017 ---|month=12 ---|day=25 ---|part0000 ---|part0001 ---|year=2018 ---|month=02 ---|day=17 ---|part0000 ---|part0001 with the data having a format similar to this: { ""dest_ip"": ""20.123.123.12"", ""dest_port"": 8000, ""isboolean"": true, ""timestamp"": ""2018–07–08t17:48:40z"", ""@diffcultfield1"": ""test"", ""difficult-field2"": ""test"", ""nested_field"": { ""field_1"": ""fieldval"", ""field_2"": ""fieldval"" } a few new fields were added to show some special steps that are required in the creation of tables on top of this data. adjust your table creation according to your data. specifically, the fields @difficultfield1, difficult-field2, & nested_field require slightly different syntax. one more note prior to creating your table — it is entirely fine to have your data saved locally and not in s3 — update the location value in the table below to point to the local location. creating your table: create external table parquetdata(dest_ip string, dest_port int, isboolean boolean, timestamp timestamp, `@difficultfield` string, ` difficult-field2` string, nested_field struct<field_1:string, field_2:string>) partitioned by (year int , month int , day int ) row format serde 'org.apache.hadoop.hive.ql.io.parquet.serde.parquethiveserde' stored as inputformat 'org.apache.hadoop.hive.ql.io.parquet.mapredparquetinputformat' outputformat 'org.apache.hadoop.hive.ql.io.parquet.mapredparquetoutputformat' location 's3a://testbucket/parquetfiles/' tblproperties (""parquet.compress""=""snappy""); msck repair table parquetdata; alter table parquetdata recover partitions; let’s go over the 3 commands above and what they are doing. the result of the create external table command will be an external table with the name parquetdata with the specified schema. two important notes in the schema: 1. fields with special characters are surrounded by “`” 2. nested fields require that you specify the nested schema moving on — you can also specifying the what partitioning exists on top of the data. if you pass partitioning information at the time of query, your queries should get a performance boost. there are a few other pieces of the command that specify the different formats regarding the data. such as the serde as well as the format of the input and output of the data— essentially specifying that the underlying data is parquet and how to handle that. the last two commands go together. if you were to just create your external table then try querying it, you would get 0 results. the reason this happens is that when your data is in s3 — thrift doesn’t automatically pick up all the available partitions in s3. you have to repair the table and recover the partitions. if you perform this, then your external table will pick up all of your latest partitions available in s3. however, any new partitions will not be found until you run the commands again. when you create tables in hive, where partitioned data already exists in s3 or hdfs, you need to run a command to update the hive metastore with the table’s partition structure. — stack overflow user you should now be able to start querying your data with sql commands! try a simple command select * from parquetdata; enjoy the power ~ depending on your scale, you may run into issues with performance. the blog i referred to at the start of the post should be pretty useful. this blog mentions a lot of options for performance improvements. big data thrift data engineering sql system -- -- 1 written by sai peddy 29 followers · 9 following data engineer | love to learn | interested in…too many things responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/free-code-camp/how-to-use-spark-clusters-for-parallel-processing-big-data-86a22e7f8b50,,,"how to use spark clusters for parallel processing big data | by hari santanam | we’ve moved to freecodecamp.org/news | medium sitemap open in app sign up sign in medium logo write sign up sign in we’ve moved to freecodecamp.org/news · we’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. see you there. how to use spark clusters for parallel processing big data use apache spark’s resilient distributed dataset (rdd) with databricks hari santanam 8 min read · dec 3, 2018 -- 2 listen share star clusters-tarantula nebula due to physical limitations, the individual computer processor has largely reached the upper ceiling for speed with current designs. so, hardware makers added more processors to the motherboard (parallel cpu cores, running at the same speed). but… most software applications written over the last few decades were not written for parallel processing. additionally, data collection has gotten exponentially bigger, due to cheap devices that can collect specific data (such as temperature, sound, speed…). to process this data in a more efficient way, newer programming methods were needed. a cluster of computing processes is similar to a group of workers. a team can work better and more efficiently than a single worker. they pool resources. this means they share information, break down the tasks and collect updates and outputs to come up with a single set of results. just as farmers went from working on one field to working with combines and tractors to efficiently produce food from larger and more farms, and agricultural cooperatives made processing easier, the cluster works together to tackle larger and more complex data collection and processing. cluster computing and parallel processing were the answers, and today we have the apache spark framework. databricks is a unified analytics platform used to launch spark cluster computing in a simple and easy way. what is spark? apache spark is a lightning-fast unified analytics engine for big data and machine learning. it was originally developed at uc berkeley. spark is fast. it takes advantage of in-memory computing and other optimizations. it currently holds the record for large-scale on-disk sorting. spark uses resilient distributed datasets (rdd) to perform parallel processing across a cluster or computer processors. it has easy-to-use apis for operating on large datasets, in various programming languages. it also has apis for transforming data, and familiar data frame apis for manipulating semi-structured data. basically, spark uses a cluster manager to coordinate work across a cluster of computers. a cluster is a group of computers that are connected and coordinate with each other to process data and compute. spark applications consist of a driver process and executor processes. briefly put, the driver process runs the main function, and analyzes and distributes work across the executors. the executors actually do the tasks assigned — executing code and reporting to the driver node. in real-world applications in business and emerging ai programming, parallel processing is becoming a necessity for efficiency, speed and complexity. image credit: databricks — apache spark v2.pdf great — so what is databricks? databricks is a unified analytics platform, from the creators of apache spark. it makes it easy to launch cloud-optimized spark clusters in minutes. think of it as an all-in-one package to write your code. you can use spark (without worrying about the underlying details) and produce results. it also includes jupyter notebooks that can be shared, as well as providing github integration, connections to many widely used tools and automation monitoring, scheduling and debugging. see here for more information. you can sign up for free with the community edition. this will allow you to play around with spark clusters. other benefits, depending on plan, include: get clusters up and running in seconds on both aws and azure cpu and gpu instances for maximum flexibility. get started quickly with out-of-the-box integration of tensorflow, keras, and their dependencies on databricks clusters. gif is mine, from spark architecture-image: https://databricks.com/spark/about let’s get started. if you have already used databricks before, skip down to the next part. otherwise, you can sign up here and select ‘community edition’ to try it out for free. main databricks screen-start with the quickstart tutorial, upper left follow the directions there. they are clear, concise and easy: create a cluster attach a notebook to the cluster and run commands in the notebook on the cluster manipulate the data and create a graph operations on python dataframe api; create a dataframe from a databricks dataset manipulate the data and display results now that you have created a data program on cluster, let’s move on to another dataset, with more operations so you can have more data. the dataset is the 2017 world happiness report by country, based on different factors such as gdp, generosity, trust, family, and others. the fields and their descriptions are listed further down in the article. i previously downloaded the dataset, then moved it into databricks’ dbfs (databricks files system) by simply dragging and dropping into the window in databricks. or, you can click on data from left navigation pane, click on add data, then either drag and drop or browse and add. # file location and type #this file was dragged and dropped into databricks from stored #location; https://www.kaggle.com/unsdsn/world-happiness#2017.csv file_location = ""/filestore/tables/2017.csv"" file_type = ""csv"" # csv options # the applied options are for csv files. for other file types, these # will be ignored: schema is inferred; first row is header - i # deleted header row in editor and intentionally left it 'false' to #contrast with later rdd parsing, #delimiter # separated, #file_location; if you don't delete header row, instead of reading #c0, c1, it would read ""country"", ""dystopia"" etc. infer_schema = ""true"" first_row_is_header = ""false"" delimiter = "","" df = spark.read.format(file_type) \ .option(""inferschema"", infer_schema) \ .option(""header"", first_row_is_header) \ .option(""sep"", delimiter) \ .load(file_location) display(df) code output showing schema and content now, let’s load the file into spark’s resilient distributed dataset(rdd) mentioned earlier. rdd performs parallel processing across a cluster or computer processors and makes data operations faster and more efficient. #load the file into spark's resilient distributed dataset(rdd) data_file = ""/filestore/tables/2017.csv"" raw_rdd = sc.textfile(data_file).cache() #show the top 5 lines of the file raw_rdd.take(5) output from rdd. note the spark jobs / stages on top note the “spark jobs” below, just above the output. click on view to see details, as shown in the inset window on the right. databricks and sparks have excellent visualizations of the processes. in spark, a job is associated with a chain of rdd dependencies organized in a direct acyclic graph (dag). in a dag, branches are directed from one node to another, with no loop backs. tasks are submitted to the scheduler, which executes them using pipelining to optimize the work and transform into minimal stages. spark rdd job sequence illustrated by dag graph metrics for tasks processed with rdd don’t worry if the above items seem complicated. there are visual snapshots of processes occurring during the specific stage for which you pressed spark job view button. you may or may not need this information — it is there if you do. rdd entries are separated by commas, which we need to split before parsing and building a dataframe. we will then take specific columns from the dataset to use. #split rdd before parsing and building dataframe csv_rdd = raw_rdd.map(lambda row: row.split("","")) #print 2 rows print(csv_rdd.take(2)) #print types print(type(csv_rdd)) print('potential # of columns: ', len(csv_rdd.take(1)[0])) #use specific columns from dataset from pyspark.sql import row parsed_rdd = csv_rdd.map(lambda r: row( country = r[0],   #country, position 1, type=string happiness_rank = r[1], happiness_score = r[2], gdp_per_capita = r[5], family = r[6], health = r[7], freedom = r[8], generosity = r[9], trust = r[10], dystopia = r[11], label = r[-1] ) ) parsed_rdd.take(5) output from the parsed rdd here are the columns and definitions for the happiness dataset: happiness dataset columns and definitions country — name of the country. region — region the country belongs to. happiness rank — rank of the country based on the happiness score. happiness score — a metric measured in 2015 by asking the sampled people the question: “how would you rate your happiness on a scale of 0 to 10 where 10 is the happiest.” economy (gdp per capita) — the extent to which gdp (gross domestic product) contributes to the calculation of the happiness score family — the extent to which family contributes to the calculation of the happiness score health — (life expectancy)the extent to which life expectancy contributed to the calculation of the happiness score freedom — the extent to which freedom contributed to the calculation of the happiness score. trust — (government corruption)the extent to which perception of corruption contributes to happiness score. generosity — the extent to which generosity contributed to the calculation of the happiness score. dystopia residual — the extent to which dystopia residual contributed to the calculation of the happiness score (dystopia=imagined place or state in which everything is unpleasant or bad, typically a totalitarian or environmentally degraded one. residual — what’s left or remaining after everything is else is accounted for or taken away). # create a view or table temp_table_name = ""2017_csv"" df.createorreplacetempview(temp_table_name) #build dataframe from rdd created earlier df = sqlcontext.createdataframe(parsed_rdd) display(df.head(10) #view the dataframe's schema df.printschema() dataframe output-compare with previous table output in screenshot way above. print df schema output #build temporary table to run sql commands #table only alive for the session #table scoped to the cluster; highly optimized df.registertemptable(""happiness"") #display happiness_score counts using dataframe syntax display(df.groupby('happiness_score') .count() .orderby('count', ascending=false) ) df.registertemptable(""happiness"") #display happiness_score counts using dataframe syntax display(df.groupby('happiness_score') .count() .orderby('count', ascending=false) ) output of running dataframe display command on temp table now, let’s use sql to run a query to do same thing. the purpose is to show you different ways to process data and to compare the methods. #use sql to run query to do same thing as previously done with dataframe (count by happiness_score) happ_query = sqlcontext.sql("""""" select happiness_score, count(*) as freq from happiness group by happiness_score order by 2 desc """""") display(happ_query) sql query output-count by happiness score another sql query to practice our data processing: #another sql query happ_stats = sqlcontext.sql("""""" select country, happiness_rank, dystopia from happiness where happiness_rank > 20 """""") display(happ_stats) output from the last sql query there! you have done it — created a spark-powered cluster and completed a dataset query process using that cluster. you can use this with your own datasets to process and output your big data projects. you can also play around with the charts-click on the chart /graph icon at the bottom of any output, specify the values and type of graph and see what happens. it is fun. the code is posted in a notebook here at databricks public forum and will be available for about 6 months as per databricks. for more information on using sparks with deep learning, read this excellent article by favio vázquez thanks for reading! i hope you have interesting programs with databricks and enjoy it as much as i have. please clap if you found it interesting or useful. for a complete list of my articles, see here . big data spark machine learning python tech -- -- 2 published in we’ve moved to freecodecamp.org/news 587k followers · last published may 23, 2019 we’ve moved to https://freecodecamp.org/news and publish tons of tutorials each week. see you there. written by hari santanam 747 followers · 314 following i am interested in ai, machine learning and its value to business and to people. i occasionally write about life in general as well. responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3,,,"spark 2: how to install it on windows in 5 steps | by doron vainrub | big data engineering | medium sitemap open in app sign up sign in medium logo write sign up sign in big data engineering · best technical posts about data (we love both small and big) spark 2: how to install it on windows in 5 steps doron vainrub 4 min read · mar 21, 2018 -- 18 listen share this is a very easy tutorial that will let you install spark in your windows pc without using docker. by the end of the tutorial you’ll be able to use spark with scala or python. before we begin: it’s important that you replace all the paths that include the folder “program files” or “program files (x86)” as explained below to avoid future problems when running spark. if you have java already installed, you still need to fix the java_home and path variables replace “program files” with “progra~1” replace “program files (x86)” with “progra~2” example: “c:\program files\java\jdk1.8.0_161” --> “c:\progra~1\java\jdk1.8.0_161” 1. prerequisite — java 8 before you start make sure you have java 8 installed and the environment variables correctly defined: download java jdk 8 from java’s official website set the following environment variables: java_home = c:\progra~1\java\jdk1.8.0_161 path += c:\progra~1\java\jdk1.8.0_161\bin optional: _java_options = -xmx512m -xms512m (to avoid common java heap memory problems whith spark) tip : progra~1 is the shortened path for “program files”. 2. spark: download and install download spark from spark’s official website choose the newest release ( 2.3.0 in my case) choose the newest package type ( pre-built for hadoop 2.7 or later in my case) download the . tgz file 2. extract the .tgz file into d:\spark note: in this guide i’ll be using my d drive but obviously you can use the c drive also 3. set the environment variables: spark_home = d:\spark\spark-2.3.0-bin-hadoop2.7 path += d:\spark\spark-2.3.0-bin-hadoop2.7\bin 3. spark: some more stuff (winutils) download winutils.exe from here: https://github.com/steveloughran/winutils choose the same version as the package type you choose for the spark .tgz file you chose in section 2 “spark: download and install” (in my case: hadoop-2.7.1) you need to navigate inside the hadoop-x.x.x folder , and inside the bin folder you will find winutils.exe if you chose the same version as me (hadoop-2.7.1) here is the direct link: https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe 2. move the winutils.exe file to the bin folder inside spark_home , in my case: d:\spark\spark-2.3.0-bin-hadoop2.7\bin 3. set the folowing environment variable to be the same as spark_home : hadoop_home = d:\spark\spark-2.3.0-bin-hadoop2.7 4. optional: some tweaks to avoid future errors this step is optional but i highly recommend you do it . it fixed some bugs i had after installing spark. hive permissions bug create the folder d:\tmp\hive execute the following command in cmd started using the option run as administrator cmd> winutils.exe chmod -r 777 d:\tmp\hive 3. check the permissions cmd> winutils.exe ls -f d:\tmp\hive 5. optional: install scala if you are planning on using scala instead of python for programming in spark, follow this steps: 1. download scala from their official website download the scala binaries for windows ( scala-2.12.4.msi in my case) 2. install scala from the .msi file 3. set the environment variables: scala_home = c:\progra~2\scala path += c:\progra~2\scala\bin tip : progra~2 is the shortened path for “program files (x86)”. 4. check if scala is working by running the following command in the cmd cmd> scala -version testing spark pyspark (spark with python) to test if spark was succesfully installed, run the following code from pyspark’s shell (you can ignore the warn messages): scala-shell to test if scala and spark where succesfully installed, run the following code from spark-shell (only if you installed scala in your computer): pd: the query will not work if you have more than one spark-shell instance open pyspark with pycharm (python 3.x) if you have pycharm installed, you can also write a “hello world” program to test pyspark pyspark with jupyter notebook this tutorial was taken from: get started with pyspark and jupyter notebook in 3 minutes you will need to use the findspark package to make a spark context available in your code. this package is not specific to jupyter notebook, you can use it in you ide too. install and launch the notebook from the cmd: create a new python notebook and write the following at the beginning of the script: now you can add your code to the bottom of the script and run the notebook. spark apache spark scala pyspark python -- -- 18 published in big data engineering 714 followers · last published feb 3, 2020 best technical posts about data (we love both small and big) written by doron vainrub 227 followers · 121 following i write about how to use ai and technology to automate the boring stuff and spend more time on the things you love | data engineer @meta | building sumly.ai responses ( 18 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/quick-code/top-tutorials-to-learn-hadoop-for-big-data-3fa31f399063,,,"10+ best hadoop tutorials for beginners [2025 mar] - learn hadoop online | quick code sitemap open in app sign up sign in medium logo write sign up sign in quick code · find the best tutorials and courses for the web, mobile, chatbot, ar/vr development, database management, data science, web design and cryptocurrency. practice in javascript, java, python, r, android, swift, objective-c, react, node js, ember, c++, sql & more. 10 best hadoop tutorials for beginners [2025 mar]—learn hadoop online learn hadoop to store and process large datasets of data with the best hadoop tutorials for beginners in 2025. quick code 13 min read · feb 5, 2018 -- listen share 1. the ultimate hands-on hadoop — tame your big data! hadoop, mapreduce, hdfs, spark, pig, hive, hbase, mongodb, cassandra, flume — the list goes on! over 25 technologies. the world of hadoop and “big data” can be intimidating — hundreds of different technologies with cryptic names form the hadoop ecosystem. with this course, you’ll not only understand what those systems are and how they fit together — but you’ll go hands-on and learn how to use them to solve real business problems. learn and master the most popular big data technologies in this comprehensive course, taught by a former engineer and senior manager from amazon and imdb . you’ll go way beyond hadoop itself, and dive into all sorts of distributed systems you may need to integrate with. install and work with a real hadoop installation right on your desktop with hortonworks and the ambari ui manage big data on a cluster with hdfs and mapreduce write programs to analyze data on hadoop with pig and spark store and query your data with sqoop , hive , mysql , hbase , cassandra , mongodb , drill , phoenix , and presto design real-world systems using the hadoop ecosystem learn how your cluster is managed with yarn , mesos , zookeeper , oozie , zeppelin , and hue handle streaming data in real time with kafka , flume , spark streaming , flink , and storm understanding hadoop is a highly valuable skill for anyone working at companies with large amounts of data. almost every large company you might want to work at uses hadoop in some way, including amazon, ebay, facebook, google, linkedin, ibm, spotify, twitter, and yahoo. and it’s not just technology companies that need hadoop; even the new york times uses hadoop for processing images. you’ll find a range of activities in this course for people at every level. if you’re a project manager who just wants to learn the buzzwords, there are web ui’s for many of the activities in the course that require no programming knowledge. if you’re comfortable with command lines, we’ll show you how to work with them too. and if you’re a programmer, i’ll challenge you with writing real scripts on a hadoop system using scala, pig latin, and python . you’ll walk away from this course with a real, deep understanding of hadoop and its associated distributed systems, and you can apply hadoop to real-world problems. please note the focus on this course is on application development, not hadoop administration. although you will pick up some administration skills along the way. 2. hadoop platform and application framework this course is for novice programmers or business people who would like to understand the core tools used to wrangle and analyze big data. in this course, you will learn: technologies opportunities and challenges of big data hype. about hadoop stack and tool and technologies associated with big data solutions. basics of hadoop distributed file system (hdfs). the main design goals of hdfs. map/reduce concepts. how to design, implement, and execute tasks in the map/reduce framework. you will an insight into big data hype, its technologies opportunities and challenges. you will take a deeper look into the hadoop stack and tool and technologies associated with big data solutions. you will take a detailed look at the hadoop stack ranging from the basic hdfs components, to application execution frameworks, and languages, services. you will take a detailed look at the hadoop distributed file system (hdfs). you will cover the main design goals of hdfs, understand the read/write process to hdfs, the main configuration parameters that can be tuned to control hdfs performance and robustness, and get an overview of the different ways you can access data on hdfs. you will learn and practice map/reduce concepts. you will learn about the big idea of map/reduce and you will learn how to design, implement, and execute tasks in the map/reduce framework. you will also learn the trade-offs in map/reduce and how that motivates other tools. finally, you will focus on the apache spark cluster computing framework, an important contender of hadoop mapreduce in the big data arena. 3. big data analytics with hadoop and apache spark apache hadoop was a pioneer in the world of big data technologies, and it continues to be a leader in enterprise big data storage. the course includes: introduction and setup hdfs data modeling for analytics data ingestion with spark data extraction with spark optimizing spark processing in this course, learn how to leverage these two technologies to build scalable and optimized data analytics pipelines. the course explores ways to optimize data modeling and storage on hdfs; discusses scalable data ingestion and extraction using spark; and provides tips for optimizing data processing in spark. plus, it provides a use case project that allows you to practice your new techniques. 4. big data and hadoop for beginners — with hands-on! everything you need to know about big data, and learn hadoop, hdfs, mapreduce, hive & pig by designing data pipeline. the main objective of this course is to help you understand complex architectures of hadoop and its components, guide you in the right direction to start with, and quickly start working with hadoop and its components. it covers everything what you need as a big data beginner. learn about big data market, different job roles, technology trends, history of hadoop, hdfs, hadoop ecosystem, hive and pig. in this course, we will see how as a beginner one should start with hadoop. this course comes with a lot of hands-on examples which will help you learn hadoop quickly. the course have 6 sections, and focuses on the following topics: big data at a glance: learn about big data and different job roles required in big data market. know big data salary trends around the globe. learn about hottest technologies and their trends in the market. getting started with hadoop: understand hadoop and its complex architecture. learn hadoop ecosystem with simple examples. know different versions of hadoop (hadoop 1.x vs hadoop 2.x), different hadoop vendors in the market and hadoop on cloud. understand how hadoop uses elt approach. learn installing hadoop on your machine. we will see running hdfs commands from command line to manage hdfs. getting started with hive: understand what kind of problem hive solves in big data. learn its architectural design and working mechanism. know data models in hive, different file formats supported by hive, hive queries etc. we will see running queries in hive. getting started with pig: understand how pig solves problems in big data. learn its architectural design and working mechanism. understand how pig latin works in pig. you will understand the differences between sql and pig latin. demos on running different queries in pig. use cases: real life applications of hadoop is really important to better understand hadoop and its components, hence we will be learning by designing a sample data pipeline in hadoop to process big data. also, understand how companies are adopting modern data architecture i.e. data lake in their data infrastructure. practice: practice with huge data sets. learn design and optimization techniques by designing data models, data pipelines by using real life applications’ data sets. 5. learn big data: the hadoop ecosystem masterclass master the hadoop ecosystem using hdfs, mapreduce, yarn, pig, hive, kafka, hbase, spark, knox, ranger, ambari, zookeeper. in this course you will learn big data using the hadoop ecosystem. why hadoop? it is one of the most sought after skills in the it industry. the average salary in the us is $112,000 per year, up to an average of $160,000 in san fransisco (source: indeed). the course is aimed at software engineers, database administrators, and system administrators that want to learn about big data. other it professionals can also take this course, but might have to do some extra research to understand some of the concepts. you will learn how to use the most popular software in the big data industry at moment, using batch processing as well as realtime processing. this course will give you enough background to be able to talk about real problems and solutions with experts in the industry. updating your linkedin profile with these technologies will make recruiters want you to get interviews at the most prestigious companies in the world. the course is very practical, with more than 6 hours of lectures. you want to try out everything yourself, adding multiple hours of learning. if you get stuck with the technology while trying, there is support available. i will answer your messages on the message boards and we have a facebook group where you can post questions. 6. master apache hadoop — infinite skills hadoop training looking to master apache hadoop, this course from infinite skills shows you how to work with the hadoop framework. this introduction to apache hadoop training course from infinite skills will teach you the tools and functions needed to work within this open-source software framework. this course is designed for the absolute beginner, meaning no prior experience with hadoop is required. you will start out by learning the basics of hadoop, including the hadoop run modes and job types and hadoop in the cloud. you will then learn about the hadoop distributed file system (hdfs), such as the hdfs architecture, secondary name node, and access controls. this video tutorial will also cover topics including mapreduce, debugging basics, hive and pig basics, and impala fundamentals. finally, this course will teach you how to import and export data. once you have completed this computer based training video, you will be fully capable of using the tools and functions you’ve learned to work successfully in hadoop. working files are included, allowing you to follow along with the author throughout the lessons. 7. learn big data : complete hadoop ecosystem with practicals learn complete big data (spark + mongodb + pig + hadoop + hive + cassandra + hbase + redis + beeline) with examples. this course is specially designed for all profile students i.e. developers and testers who wanted to build their career into big data arena in real world. so i have designed this course so they can start working with all big data related tools and technologies i.e. hadoop, hive, pig, hbase, cassandra, mongodb, redis in complete big data. all the users who are working or looking their career in big data profile in big data and wanted to move into testing domain should take this course and go through the complete tutorials which has beginner to advance knowledge. it will give the detailed information for different commands and queries which are used in development and testing all big data related tools and technologies including different databases applications in complete queries/commands which is needed by the tester to move into bigger umbrella i.e. big data ecosystems environment. this course is well structured with all elements of different all big data related tools and technologies databases i.e. haoop , hive , hbase + cassandra + mongodb + redis in complete big data with advance commands in practical manner separated by different topics. students should take this course who wanted to learn end to end big data ecosystem technologies including different databases in complete big data from scratch. 8. hadoop made very easy learn hadoop, pig, hive and mahout with a hands on approach without spending too much time and boost your career. this course teaches you hadoop, pig, hive and apache mahout from scratch with an example based and hands on approach. master the fundamental concepts of big data, hadoop and mahout with ease understand the big data & apache hadoop landscape learn hdfs & mapreduce concepts with examples and hands on labs learn hadoop streaming understand analytics with hadoop using pig and hive machine learning concepts collaborative filtering with apache mahout real world recommender system with mahout and hadoop big data and data science foundation to empower you with the most specialized skills the core concepts are stressed upon and the focus is on building a solid foundation of the key hadoop, map reduce and collaborative filtering concepts upon which you can learn just about every other technology in the same space. preliminary java and unix knowledge is expected. the first few topics will focus on the rise of big data and how apache hadoop fits in. you will focus on the fundamentals of hadoop and its core components: hdfs and map reduce. you will then setup and play around with hadoop and hdfs and then deep dive into mapreduce programming with hands on examples. you will also spend time on combiners and partitioners and how they can help. you will also spend time on hadoop streaming: a tool that helps non-java professionals to leverage the power of hadoop and do pocs on it. once you have a solid foundation of hdfs and mapreduce, in the next couple of topics you will explore higher level components of the hadoop ecosystem: hive and pig. you will go into the details of both hive and pig by installing them and working with examples. hive and pig can make your life easy by shielding you from the complexity of writing mr jobs and yet leveraging the parallel processing ability of the hadoop framework. in the next few lectures you will look at something very interesting: apache mahout and machine learning. apache mahout is a java library that lets you write machine learning applications with ease. youwill learn the basics of machine learning and go deeper into collaborative filtering and recommender systems, something that mahout excels that. you will look at some similarity algorithms, understand their real-life implications and apply them when you will build together a real world movie recommender system using mahout and hadoop. 9. hadoop developer in real world the course covers all the must know topics like hdfs, mapreduce, yarn, apache pig and hive etc. and we go deep in exploring the concepts. you just don’t stop with the easy concepts, we take it a step further and cover important and complex topics like file formats, custom writables, input/output formats, troubleshooting, optimizations etc. all concepts are backed by interesting hands-on projects like analyzing million song dataset to find less familiar artists with hot songs, ranking pages with page dumps from wikipedia, simulating mutual friends functionality in facebook just to name a few. 10. learn by example: hadoop, mapreduce for big data problems a hands-on workout in hadoop, mapreduce and the art of thinking “parallel”. this course is a zoom-in, zoom-out, hands-on workout involving hadoop, mapreduce and the art of thinking parallel. zoom-in, zoom-out: this course is both broad and deep. it covers the individual components of hadoop in great detail, and also gives you a higher level picture of how they interact with each other. hands-on workout involving hadoop, mapreduce : this course will get you hands-on with hadoop very early on. you’ll learn how to set up your own cluster using both vms and the cloud. all the major features of mapreduce are covered — including advanced topics like total sort and secondary sort. the art of thinking parallel: mapreduce completely changed the way people thought about processing big data. breaking down any problem into parallelizable units is an art. the examples in this course will train you to “think parallel”. what’s covered: using mapreduce to recommend friends in a social networking site: generate top 10 friend recommendations using a collaborative filtering algorithm. build an inverted index for search engines: use mapreduce to parallelize the humongous task of building an inverted index for a search engine. generate bigrams from text: generate bigrams and compute their frequency distribution in a corpus of text. build your hadoop cluster: install hadoop in standalone, pseudo-distributed and fully distributed modes set up a hadoop cluster using linux vms. set up a cloud hadoop cluster on aws with cloudera manager. understand hdfs, mapreduce and yarn and their interaction customize your mapreduce jobs: chain multiple mr jobs together write your own customized partitioner total sort : globally sort a large amount of data by sampling input files secondary sorting unit tests with mr unit integrate with python using the hadoop streaming api .. and of course all the basics: mapreduce : mapper, reducer, sort/merge, partitioning, shuffle and sort hdfs & yarn: namenode, datanode, resource manager, node manager, the anatomy of a mapreduce application, yarn scheduling, configuring hdfs and yarn to performance tune your cluster. 11. master big data and hadoop step-by-step from scratch learn from basics to advanced concepts related to big data and hadoop in a simplified way. course overview: most demanding and sought after skill of the decade. secure your career by learning big data and hadoop. course taught using a very innovative and simplified method of teaching. course covers all the topics related to hadoop adminsitration as well as hadoop development. course description: in this course, you would be learning all the concepts and terminologies related to big data and hadoop, such as the namenode, secondary namenode, datanode, jobtracker and tasktracker, along with other concepts related to it such as what is meant by rack awareness and namenode federation in a simplified way. it also explains how the data is managed by the hadoop distributed file system (hdfs) and explains the process of reading and writing data onto the hadoop distributed file system. later in the course you would also learn how to add or remove a datanode or a tasktracker to an existing cluster, how to check the hdfs for errors, balancing the datanode and so on. you would also learn all the concepts related to programming in mapreduce along with writing programs using mapreduce. upon completion of this course, you would have a clear idea about, all the concepts related to the hadoop, that should be sufficient to help you start off with administering the hadoop cluster as well as developing mapreduce applications for hadoop cluster. thank you for reading this. we have curated top tutorials on more subjects, you would like to see them: 8 best redis tutorials for beginners — learn redis online learn redis for database management with the best redis tutorials for beginners in 2022 medium.com 10 best postgresql tutorials for beginners — learn postgresql online learn postgresql for learning a robust database management system with the best postgresql tutorials for beginners in… medium.com 8 best cassandra tutorials for beginners — learn cassandra online learn cassandra for database management with the best cassandra tutorials for beginners in 2022 medium.com disclosure: we may get a small affiliate commission if you buy a course through links on this page. thank you. big data hadoop programming development database -- -- published in quick code 13.7k followers · last published apr 27, 2025 find the best tutorials and courses for the web, mobile, chatbot, ar/vr development, database management, data science, web design and cryptocurrency. practice in javascript, java, python, r, android, swift, objective-c, react, node js, ember, c++, sql & more. written by quick code 7.4k followers · 38 following a list of best courses to learn programming, web, mobile, chatbot, ar/vr development, database management, data science, web design and cryptocurrency. help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@bufan.zeng/use-parquet-for-big-data-storage-3b6292598653,,,"use parquet for big data storage. due to the portable nature… | by bufan zeng | medium sitemap open in app sign up sign in medium logo write sign up sign in use parquet for big data storage bufan zeng 5 min read · jun 28, 2018 -- 1 listen share due to the portable nature, comma-separated values(csv) format is the most popular format for tabular data. if i were to list three tabular formats before i learned spark, i would say “csv”, “csv” and “dat”. i accidentally got an h5 file while doing big data analysis. however, extra tools and efforts are needed if i were to load the file using spark. therefore, i started wondering — is there any other data formats that are more efficient than “csv” and “h5” in big data storage? the answer is parquet. the first time i heard about parquet format is when knowing that our customers are using it. then after some research, i want to share my findings in this short blog. roadmap: what is parquet? get things ready (data and spark environment) download and readthe data store the data in parquet format efficiency comparison conclusion what is parquet? the official website and wiki already have some good introduction about the format. in short, it is much more efficient than csv to store the big data. get things ready i will test the parquet format on two public datasets: taste profile of million song dataset (2.8g in txt) airline dataset from 2005 to 2015 (31g in csv) the taste profile dataset only contains three columns: “user”, “song” and “play count”, which is ideal for spark to build a recommendation system. the file is originally in txt format, but the columns are separated by tabs (tsv). the airline dataset has about 100 fields with different data types. obviously we cannot ingest 31g data locally, that’s why we want to leverage cloud spark environment to manipulate the data. furthermore, parquet is a column-oriented data store; we can observe the advantage of using parquet over csv on such a big dataset with so many fields(columns). i’ll use the ibm watson studio’s spark environment . watson studio provides the pre-configured spark environment which is very convenient to use. the only steps needed are register -> log in -> provision environment -> create notebook and then i’m ready to go. both scala and python kernels are provided. python notebook is used in this demo. download and read the data in the pyspark notebook, we firstly use “wget [link] -o [file]” to download the zipped data files to the mounted ibm cloud storage objective (cos). wget airline data next, unzip the file using “tar [zipped file path] -c [unzip directory]” unzip the file now we are ready to read the files. reading taste profile data is easy, because there is only one data file. i’ll use sparkcontext to read the txt file as an rdd, then use a map function to split the rows by “\t” and finally convert the rdd to a dataframe. read taste profile data however, for the airline data, we may need a loop to read all of them. furthermore, since the csv files don’t have a schema, we should try to use “inferschema” as an option while reading them. we can also manually create the schema just to be safe. manually set up schema (partial) p lease refer to the linked blog post for details about reading and merging the airline data in spark. store the data in parquet format sparksession has built-in method to save the dataframe in parquet format. simply call the method and provide a path to a folder, the parquet formatted files will be write the the folder. easy implementation in taste profile data there is also options to set the partitions. for example, if we want to store the data partitioning by “year” and “month” for airline data, simply add “partitonby” the column names, and spark will automatically store the data in parquet format using the partition. partitionby application result of partition efficiency comparison let’s check the sizes of the parquet data! comparison of taste profile data for the taste profile data, the parquet format has 637m, which is about 22% of the size of the original data. the parquet files are stored in a folder. although it can be directly read by spark, we’d better compress it if we want to share the data. compress the parquet folder as we can see, the compressed parquet data is even smaller with size of 509m. now we can check the efficiency on the bigger one — airline data. comparison of airline data the parquet formatted data only has 1.9g which is only 6% comparing to the csv formatted data! and the zipped parquet file is 1.6g. let’s visualize the sizes using a chart. size comparison conclusion from the comparison we can conclude that the parquet data format is much more efficient when dealing with large data files. because spark is so far the most popular framework of process large data files, the biggest advantage of using parquet format is that it can be directly read into spark dataframe, and the schema is preserved. however, transferring the parquet files does require a further compression. yet it is not a big deal since people transfer the csv data in zipped files anyway. the disadvantage of using parquet format is also obvious — it is not as portable or widely used comparing to csv. users need specific tools like spark to parse the data. therefore, if the data is big and needed to be processed using spark , parquet format works much better than csv. otherwise it can be unnecessary to use parquet format to store some small data. reference: michael. “airline flight data analysis — part 1 — data preparation.” 28 aug. 2016, diybigdata.net/2016/08/airline-flight-data-analysis-data-preparation/. big data spark ibm watson jupyter notebook -- -- 1 written by bufan zeng 10 followers · 3 following responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/data-science/trying-out-dask-dataframes-in-python-for-fast-data-analysis-in-parallel-aa960c18a915,,,"how to run parallel data analysis in python using dask dataframes | by luciano strika | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. how to run parallel data analysis in python using dask dataframes luciano strika 6 min read · aug 25, 2018 -- 20 listen share your mind on multi-cores. source: pixabay sometimes you open a big dataset with python’s pandas , try to get a few metrics, and the whole thing just freezes horribly. if you work on big data, you know if you’re using pandas, you can be waiting for up to a whole minute for a simple average of a series, and let’s not even get into calling apply . and that’s just for a couple million rows! when you get to the billions, you better start using spark or something. i found out about this tool a short while ago: a way to speed up data analysis in python, without having to get better infrastructure or switching languages. it will eventually feel limited if your dataset is huge, but it scales a lot better than regular pandas, and may be just the fit for your problem — especially if you’re not doing a lot of reindexing. you can use dask for free right away on saturn cloud ! saturn cloud is an end-to-end data science + machine learning platform allowing data scientists to scale their python projects with dask in the cloud. what is dask? dask is an open source project that gives you abstractions over numpy arrays, pandas dataframes and regular lists, allowing you to run operations on them in parallel, using multicore processing. here’s an excerpt straight from the tutorial: dask provides high-level array, bag, and dataframe collections that mimic numpy, lists, and pandas but can operate in parallel on datasets that don’t fit into main memory. dask’s high-level collections are alternatives to numpy and pandas for large datasets. it’s as awesome as it sounds! i set out to try the dask dataframes out for this article, and ran a couple benchmarks on them. (to see even more applications for dask in machine learning, check out my parallel k-means clustering tutorial ) reading the docs what i did first was read the official documentation, to see what exactly was recommended to do in dask’s instead of regular dataframes. here are the relevant parts from the official docs : manipulating large datasets, even when those datasets don’t fit in memory accelerating long computations by using many cores distributed computing on large datasets with standard pandas operations like groupby, join, and time series computations and then below that, it lists some of the things that are really fast if you use dask dataframes: arithmetic operations (multiplying or adding to a series) common aggregations (mean, min, max, sum, etc.) calling apply (as long as it’s along the index -that is, not after a groupby(‘y’) where ‘y’ is not the index-) calling value_counts(), drop_duplicates() or corr() filtering with loc , isin , and row-wise selection just a small brush up on filtering dataframes, in case you find it useful. how to use dask dataframes dask dataframes have the same api as pandas dataframes, except aggregations and apply s are evaluated lazily, and need to be computed through calling the compute method. in order to generate a dask dataframe you can simply call the read_csv method just as you would in pandas or, given a pandas dataframe df , you can just call dd = ddf.from_pandas(df, npartitions=n) where ddf is the name you imported dask dataframes with, and npartitions is an argument telling the dataframe how you want to partition it. according to stackoverflow, it is advised to partition the dataframe in about as many partitions as cores your computer has, or a couple times that number, as each partition will run on a different thread and communication between them will become too costly if there are too many. getting dirty: let’s benchmark! i made a jupyter notebook to try out the framework, and made it available on github in case you want to check it out or even run it for yourself. the benchmarking tests i ran are available in the notebook at github, but here are the main ones: here df3 is a regular pandas dataframe with 25 million rows, generated using the script from my pandas tutorial (columns are name, surname and salary , sampled randomly from a list). i took a 50 rows dataset and concatenated it 500000 times, since i wasn’t too interested in the analysis per se , but only in the time it took to run it. dfn is simply the dask dataframe based on df3 . first batch of results: not too optimistic i first tried the test with 3 partitions, as i only have 4 cores and didn’t want to overwork my pc. i had pretty bad results with dask and had to wait a lot to get them too, but i feared it may had been because i’d made too few partitions: 204.313940048 seconds for get_big_mean 39.7543280125 seconds for get_big_mean_old 131.600986004 seconds for get_big_max 43.7621600628 seconds for get_big_max_old 120.027213097 seconds for get_big_sum 7.49701309204 seconds for get_big_sum_old 0.581165790558 seconds for filter_df 226.700095892 seconds for filter_df_old you can see most of the operations turned a lot slower when i used dask. that gave me the hint that i may have had to use more partitions. the amount that generating the lazy evaluations took was negligible as well (less than half a second in some cases), so it’s not like it would have got amortized over time if i reused them. i also tried this test with the apply method: and had pretty similar results: 369.541605949 seconds for apply_random 157.643756866 seconds for apply_random_old so generally, most operations became twice as slow as the original, though filter was a lot faster. i am worried maybe i should have called compute on that one as well, so take that result with a grain of salt. more partitions: amazing speed up after such discouraging results, i decided maybe i was just not using enough partitions. the whole point of this is running things in parallel, after all, so maybe i just needed to parallelize more? so i tried the same tests with 8 partitions, and here’s what i got (i omitted the results from the non-parallel dataframe, since they were basically the same): 3.08352184296 seconds for get_big_mean 1.3314101696 seconds for get_big_max 1.21639800072 seconds for get_big_sum 0.228978157043 seconds for filter_df 112.135010004 seconds for apply_random 50.2007009983 seconds for value_count_test that’s right! most operations are running over ten times faster than the regular dataframe’s, and even the apply got faster! i also ran the value_count test, which just calls the value_count method on the salary series. for context, keep in mind i had to kill the process when i ran this test on a regular dataframe after ten whole minutes of waiting. this time it only took 50 seconds! so basically i was just using the tool wrong, and it’s pretty darn fast. a lot faster than regular dataframes. final take-away given we just operated with 25 million rows in under a minute on a pretty old 4-core pc, i can see how this would be huge in the industry. so my advice is try this framework out next time you have to process a dataset locally or from a single aws instance. it’s pretty fast. i hope you found this article interesting or useful! it took a lot more time to write it than i anticipated, as some of the benchmarks took so long . please tell me if you’d ever heard of dask before reading this, and whether you’ve ever used it in your job or for a project. also tell me if there are any other cool features i didn’t cover, or some things i did plain wrong! your feedback and comments are the biggest reason i write, as i am also learning from this. further reading for data scientists: autoencoders: deep learning with tensorflow’s eager execution in which i introduce tensorflow’s keras api, and train autoencoders for image compression. lstm: teaching a neural network to write like lovecraft where i explain how lstm neural networks work, and use one for text generation. 5 probability distributions every data scientist should know in which you can learn some very commonly used statistics basics… and not so basics. follow me for more python tutorials, tips and tricks! if you liked this article, check out my website or follow me on twitter . if you want to become a data scientist, check out my recommended machine learning books . data science python big data big data analytics towards data science -- -- 20 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by luciano strika 1.7k followers · 420 following b. sc.+m. sc. computer science, buenos aires university. software engineer at microsoft responses ( 20 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/towards-data-science/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa,,,"machine learning with pyspark and mllib — solving a binary classification problem | by susan li | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. machine learning with pyspark and mllib — solving a binary classification problem susan li 6 min read · may 6, 2018 -- 17 listen share photo credit: pixabay apache spark , once a component of the hadoop ecosystem, is now becoming the big-data platform of choice for enterprises. it is a powerful open source engine that provides real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing with very fast speed, ease of use and standard interface. in the industry, there is a big demand for a powerful engine that can do all of above. sooner or later, your company or your clients will be using spark to develop sophisticated models that would enable you to discover new opportunities or avoid risk. spark is not hard to learn, if you already known python and sql, it is very easy to get started. let’s give it a try today! exploring the data we will use the same data set when we built a logistic regression in python , and it is related to direct marketing campaigns (phone calls) of a portuguese banking institution. the classification goal is to predict whether the client will subscribe (yes/no) to a term deposit. the dataset can be downloaded from kaggle . from pyspark.sql import sparksession spark = sparksession.builder.appname('ml-bank').getorcreate() df = spark.read.csv('bank.csv', header = true, inferschema = true) df.printschema() figure 1 input variables: age, job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome. output variable: deposit have a peek of the first five observations. pandas data frame is prettier than spark dataframe.show(). import pandas as pd pd.dataframe(df.take(5), columns=df.columns).transpose() figure 2 our classes are perfect balanced. import pandas as pd pd.dataframe(df.take(5), columns=df.columns).transpose() figure 3 summary statistics for numeric variables numeric_features = [t[0] for t in df.dtypes if t[1] == 'int'] df.select(numeric_features).describe().topandas().transpose() figure 4 correlations between independent variables . numeric_data = df.select(numeric_features).topandas() axs = pd.scatter_matrix(numeric_data, figsize=(8, 8)); n = len(numeric_data.columns) for i in range(n): v = axs[i, 0] v.yaxis.label.set_rotation(0) v.yaxis.label.set_ha('right') v.set_yticks(()) h = axs[n-1, i] h.xaxis.label.set_rotation(90) h.set_xticks(()) figure 5 it’s obvious that there aren’t highly correlated numeric variables. therefore, we will keep all of them for the model. however, day and month columns are not really useful, we will remove these two columns. df = df.select('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit') cols = df.columns df.printschema() figure 6 preparing data for machine learning the process includes category indexing, one-hot encoding and vectorassembler — a feature transformer that merges multiple columns into a vector column. from pyspark.ml.feature import onehotencoderestimator, stringindexer, vectorassembler categoricalcolumns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome'] stages = [] for categoricalcol in categoricalcolumns: stringindexer = stringindexer(inputcol = categoricalcol, outputcol = categoricalcol + 'index') encoder = onehotencoderestimator(inputcols=[stringindexer.getoutputcol()], outputcols=[categoricalcol + ""classvec""]) stages += [stringindexer, encoder] label_stringidx = stringindexer(inputcol = 'deposit', outputcol = 'label') stages += [label_stringidx] numericcols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous'] assemblerinputs = [c + ""classvec"" for c in categoricalcolumns] + numericcols assembler = vectorassembler(inputcols=assemblerinputs, outputcol=""features"") stages += [assembler] the above code are taken from databricks’ official site and it indexes each categorical column using the stringindexer, then converts the indexed categories into one-hot encoded variables. the resulting output has the binary vectors appended to the end of each row. we use the stringindexer again to encode our labels to label indices. next, we use the vectorassembler to combine all the feature columns into a single vector column. pipeline we use pipeline to chain multiple transformers and estimators together to specify our machine learning workflow. a pipeline’s stages are specified as an ordered array. from pyspark.ml import pipeline pipeline = pipeline(stages = stages) pipelinemodel = pipeline.fit(df) df = pipelinemodel.transform(df) selectedcols = ['label', 'features'] + cols df = df.select(selectedcols) df.printschema() figure 7 pd.dataframe(df.take(5), columns=df.columns).transpose() figure 8 as you can see, we now have features column and label column. randomly split data into train and test sets, and set seed for reproducibility. train, test = df.randomsplit([0.7, 0.3], seed = 2018) print(""training dataset count: "" + str(train.count())) print(""test dataset count: "" + str(test.count())) training dataset count: 7764 test dataset count: 3398 logistic regression model from pyspark.ml.classification import logisticregression lr = logisticregression(featurescol = 'features', labelcol = 'label', maxiter=10) lrmodel = lr.fit(train) we can obtain the coefficients by using logisticregressionmodel’s attributes. import matplotlib.pyplot as plt import numpy as np beta = np.sort(lrmodel.coefficients) plt.plot(beta) plt.ylabel('beta coefficients') plt.show() figure 9 summarize the model over the training set, we can also obtain the receiver-operating characteristic and areaunderroc . trainingsummary = lrmodel.summary roc = trainingsummary.roc.topandas() plt.plot(roc['fpr'],roc['tpr']) plt.ylabel('false positive rate') plt.xlabel('true positive rate') plt.title('roc curve') plt.show() print('training set areaunderroc: ' + str(trainingsummary.areaunderroc)) figure 10 precision and recall . pr = trainingsummary.pr.topandas() plt.plot(pr['recall'],pr['precision']) plt.ylabel('precision') plt.xlabel('recall') plt.show() figure 11 make predictions on the test set . predictions = lrmodel.transform(test) predictions.select('age', 'job', 'label', 'rawprediction', 'prediction', 'probability').show(10) figure 12 evaluate our logistic regression model . from pyspark.ml.evaluation import binaryclassificationevaluator evaluator = binaryclassificationevaluator() print('test area under roc', evaluator.evaluate(predictions)) test area under roc 0.8858324614449619 decision tree classifier decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multi-class classification, do not require feature scaling, and are able to capture non-linearities and feature interactions. from pyspark.ml.classification import decisiontreeclassifier dt = decisiontreeclassifier(featurescol = 'features', labelcol = 'label', maxdepth = 3) dtmodel = dt.fit(train) predictions = dtmodel.transform(test) predictions.select('age', 'job', 'label', 'rawprediction', 'prediction', 'probability').show(10) figure 13 evaluate our decision tree model . evaluator = binaryclassificationevaluator() print(""test area under roc: "" + str(evaluator.evaluate(predictions, {evaluator.metricname: ""areaunderroc""}))) test area under roc: 0.7807240050065357 one simple decision tree performed poorly because it is too weak given the range of different features. the prediction accuracy of decision trees can be improved by ensemble methods, such as random forest and gradient-boosted tree. random forest classifier from pyspark.ml.classification import randomforestclassifier rf = randomforestclassifier(featurescol = 'features', labelcol = 'label') rfmodel = rf.fit(train) predictions = rfmodel.transform(test) predictions.select('age', 'job', 'label', 'rawprediction', 'prediction', 'probability').show(10) figure 14 evaluate our random forest classifier . evaluator = binaryclassificationevaluator() print(""test area under roc: "" + str(evaluator.evaluate(predictions, {evaluator.metricname: ""areaunderroc""}))) test area under roc: 0.8846453518867426 gradient-boosted tree classifier from pyspark.ml.classification import gbtclassifier gbt = gbtclassifier(maxiter=10) gbtmodel = gbt.fit(train) predictions = gbtmodel.transform(test) predictions.select('age', 'job', 'label', 'rawprediction', 'prediction', 'probability').show(10) figure 15 evaluate our gradient-boosted tree classifier. evaluator = binaryclassificationevaluator() print(""test area under roc: "" + str(evaluator.evaluate(predictions, {evaluator.metricname: ""areaunderroc""}))) test area under roc: 0.8940728473145346 gradient-boosted tree achieved the best results, we will try tuning this model with the paramgridbuilder and the crossvalidator. before that we can use explainparams() to print a list of all params and their definitions to understand what params available for tuning. print(gbt.explainparams()) figure 16 from pyspark.ml.tuning import paramgridbuilder, crossvalidator paramgrid = (paramgridbuilder() .addgrid(gbt.maxdepth, [2, 4, 6]) .addgrid(gbt.maxbins, [20, 60]) .addgrid(gbt.maxiter, [10, 20]) .build()) cv = crossvalidator(estimator=gbt, estimatorparammaps=paramgrid, evaluator=evaluator, numfolds=5) # run cross validations.  this can take about 6 minutes since it is training over 20 trees! cvmodel = cv.fit(train) predictions = cvmodel.transform(test) evaluator.evaluate(predictions) 0.8981050997838095 to sum it up, we have learned how to build a binary classification application using pyspark and mllib pipelines api. we tried four algorithms and gradient boosting performed best on our data set. source code can be found on github . i look forward to hearing feedback or questions. reference: apache spark 2.1.0 machine learning apache spark python predictive analytics pyspark -- -- 17 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by susan li 28k followers · 4 following changing the world, one post at a time. sr data scientist, toronto canada. https://www.linkedin.com/in/susanli/ responses ( 17 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://lllsong.medium.com/hello-world-with-databricks-spark-notebook-27a83584e890,,,"“hello world” with databricks spark notebook | by l song | medium sitemap open in app sign up sign in medium logo write sign up sign in “hello world” with databricks spark notebook l song 3 min read · oct 4, 2018 -- listen share start this post to introduce basics of doing “big-data” thing with spark using databricks notebook, as probably the databricks notebook is the best and easiest one as far i know (and tried) for now. ok, first of first, where should i start if i want to go for spark? i suggest start learning scala, even you’ve used python/r,.etc before. for reasons, please google it, i’ve got a link here: https://www.dezyre.com/article/scala-vs-python-for-apache-spark/213 fine, decide to have a try with scala + spark, the same question, where should i start? i suggest start with databricks community version, no complicated setup, free and easy to use. just register your account at https://community.cloud.databricks.com/ . once you like it and want to have your own sandbox, a further option is go for different images from big names, like mapr, hortonworks, cloudera,.etc. i think that should be enough for people new to spark today. ■■ task / homework ■■ : register the databricks community. if you still cannot complete the reg-step, you’re out, spark is too difficult for you (joke). good, once registered you should see the following page on the databricks community version. loads you can do now and probably you would see quite a lot things familiar with like python, r, sql. to start with this “hello world”, let’s create a single notebook & run some code. click on home -> users -> [your id] -> down-pointing triangle icon -> create -> notebook: give a name and select the language “scala”, don’t worry too much about not know scala at all, i did this when i started learning scala. basically this would give you some idea of how the language scala feel like and you would be covered. of course you can try anything later by using python or r. for now, your notebook is up but not running (as you can see underneath the notebook title, the cluster status is “detached”). no worry, let’s do a simple data-loading experience test. write the following code into the first cell (if you don’t know what’s cell, suggest learn a bit about notebook). val diamonds = sqlcontext.read.format(“csv”) .option(“header”, “true”) .option(“inferschema”, “true”) .load(“/databricks-datasets/rdatasets/data-001/csv/ggplot2/diamonds.csv”) display(diamonds) basically, this would load a default diamonds.csv data file which is stored on the databricks server, so you don’t have to upload anything, and then display the table. once you’ve done that, hit the run icon on top-right of the cell to execute it (keyboard shortcut “shift + enter”), as soon as you do that, you should see the following pop-up to ask you attaching to a cluster, select the “auto” thing and click “launch and run”. this would take a while especially if the cluster is not up. normally this cluster would cost you a bit fortune to run for example if you do this on aws or azure (if you’re not in the free-tier), databricks give it for free, though a very tiny scale cluster, and limited storage. now when it’s done, everything is up and running, and you will see the table showing up. feel free to poke around, and of course, to finish this “hello world”, start a new cell and type print(“hello, world”) or even just: “hello, world” enjoy! big data databricks spark hello world -- -- written by l song 0 followers · 3 following no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@yhoso/resolving-weird-spark-errors-f34324943e1c,,,"solving 5 mysterious spark errors | by yhoztak | medium sitemap open in app sign up sign in medium logo write sign up sign in solving 5 mysterious spark errors yhoztak 6 min read · sep 7, 2018 -- 4 listen share at ml team at coupa , our big data infrastructure looks like this: https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/screenshots/diagram.png it involves spark, livy, jupyter notebook, luigi, emr, backed with s3 in multi regions. it’s powerful and great(this post explains how great it is), but it’s sometime hard to debug when there’s issue how come it’s hard to debug in spark? from: https://cwiki.apache.org/confluence/display/spark/pyspark+internals lazy evaluation is excellent way of optimize performance by accumulating the tasks until it needs to really perform it, but it sort of makes it hard to debug. all the transformation in spark is lazy. it looks like it performed complex job fast, but it actually accumulate the computations till it’s really needed to compute, such as when df.show or df.write are called. distributed computation can get complex, plus with pyspark, spark infrastructure is powered by jvm but it’s using py4j to translate from python like above, so the error message looks like java/scala related and can look really long anyway, the error message is sometime mysterious and i’d like to introduce them. mysterious (..or weird) spark errors: here are some of the tricky ones i run into before that i had no idea what it meant initially… can you guess? resolved attribute(s) your_field#xx missing from ….. an error occurred while calling o64.cachetable. or an error occurred while calling o206.showstring lib.your_module not found when using udfs cannot have map type columns in dataframe which calls set operations iopub data rate exceeded problems & solutions problem 1: resolved attribute(s) your_field#xx missing from ….. the actual error looks like this and noted in spark jira exception in thread ""main"" org.apache.spark.sql.analysisexception: resolved attribute(s) surname#20 missing from id#0,birthdate#3,name#10,surname#7 in operator !project [id#0,birthdate#3,name#10,surname#20,udf(birthdate#3) as birthdate_cleaned#8]; this error usually happens when two dataframes, and you apply udf on some columns to transfer, aggregate, rejoining to add as new fields on new dataframe.. the solutions: it seems like if i specifically select columns sometime works left = left.select(specific_coluns_from_left) left.cache() right = right.select(specific_coluns_from_left) right.cache() left.join(right, ['column_to_join']) but sometime i still have issue and following the comments from https://issues.apache.org/jira/browse/spark-14948 , if i clone the dataframe like the following, it always works left_cloned = left.todf(columns_in_order_renamed_to_avoid_confusion) left_cloned.join(right, ['column_to_join']) ## or in sql left_cloned.registertable(""left_cloned"") sqlcontext.sql("""""" select right.* from left_cloned join right on right.col1 =left_cloned.col2... """""") more common join issue usually if you are joining multiple tables with the same field, you get much cleaner error like: reference ‘name’ is ambiguous, could be: name#8484, name#8487. ok i can see what it means, and solution is also simple. the solution: if you are joining two dataframes with multiple keys with the same name, code like below pretty well. [‘column1’, ‘column2’] are the columns you are joining on. and you’ll have only one column1 and column2 as a result: left.join(right, ['column1', 'column2']) problem 2: an error occurred while calling o64.cachetable. or an error occurred while calling o206.showstring. the real error message may be : the query may have null as a value on the columns you are joining with. it’s really long stacktrace and some part may look like this. it doesn’t really say much other than it crashed and shut down and has long stacktrace: an error occurred while calling o206.showstring. : org.apache.spark.sparkexception: job 25 cancelled because sparkcontext was shut down ... ... i encountered this error a few times. i hate this error because this would sometime takes 2~4 hours then crashes. the real error is probably outofmemoryerror but somehow it’s not showing that in stack trace. when you have null field on the field you are joining with, then it’ll create huge many to many relationship that creates pretty much all combinations of rows, then in the end it blow up since it cannot handle it anymore. obviously you should not have null on your field that you are joining, but sometime when i’m joining many tables and have expectation not to be null, i may not realize. good practice would be always have assertion of field before you use the field for join by simply have these kind of lines is helpful. assert df.filter(""my_column is null"").count==0 also if you don’t have null, your many to many join is creating too many rows. make sure to count the top distribution of joined field like the following and roughly calculate the estimate of joined columns df1.groupby('col1').agg(f.countdistinct('col2').alias('uniq_col2_count')).orderby(f.desc('uniq_col2_count')).show() in my case, the most frequent col2 was 40000, and count of unique count of col1 was 500 it could end up, 40000 x 40000 x 500 = 800,000,000,000 and my cluster could not handle this well, so ended up tuning up. problem 3. after successfully importing it, “your_module not found” when you have udf module like this that you import see the following code as an example. doesn’t that look weird? from lib.preprocess import find_keywords worked fine, but then when i do… df.show() this error shows up. the real error message: you need to distribute your function through spark_context.addpyfile solution: there’s actually other way to add module so that worker nodes also get them. sc.addpyfile(path_to_your_module.py) this forces all worker nodes to have reference to your module file. this happens even when the module file is already distributed through worker server and the pythonpath is not updated. so having import sys sys.path.append('/tmp/modules/lib') so that it doesn’t have any subfolder with modules also helps clarifying where the issue may be. official documentation says for addpyfile add a .py or .zip dependency for all tasks to be executed on this sparkcontext in the future. 4.“cannot have map type columns in dataframe which calls set operations” the real error message may be: please use structtype for complex format instead. we have requirement to export dataframe to json in specific format, so i was writing some udf function with specific return type original value of the field is stores as {‘us’: 3, ‘eu’: 0, ‘uk’: 0} but we want to export it as array of json with keys [{“country”:”us”, “count”:3}, {“country”:”eu”, “count”:0}, {“country”:”uk”, “count”:0}] then i got weird error at export.. cannot have map type columns in dataframe which calls set operations ….what does thas mean? after digging into spark code, i found out that this error is actually caused from df.dropduplicates() function because that call .match solutions: do, i could apply this udf right before exporting to json to avoid calling dropduplicates(), but found better solution, which was to create schema with structtype separately like the following. it’s much more organized and flexible this way and if you json is much more complex that you need to twisted around then it’s much easier to just write json formatter decorator class iterate through pandas dataframe and format it. 5. iopub data rate exceeded the real error message may be: your docker file is maxed out when i googled and stacktraced around, looks like having following options would solve it, but it didn’ --notebookapp.iopub_data_rate_limit=10000000 in my case, this was caused because docker cap out the disk space… on mac, docker has dedicated file to store all docker stuff and it caps out at 64gb, and mine was full! >ls -lh ~/library/containers/com.docker.docker/data/com.docker.driver.amd64-linux/docker.qcow2 -rw-r--r--@ 1 me  staff   64g sep  7 16:00 /users/me/library/containers/com.docker.docker/data/com.docker.driver.amd64-linux/docker.qcow2 the solution: running this to remove unused docker processes and images docker rm $(docker ps -q -f 'status=exited') docker rmi $(docker images -q -f ""dangling=true"") 2. remove ` docker.qcow2 ` file 3. then restart docker 4. then rebuild docker image for spark magic jupyter notebook then now this error is now magically gone. it took sometime for me to figure out sometime provided with solution that doesn’t work … so i hope someone may find this list useful. i’ll try to write up again as “part 2"" when i come across with more weird errors and solved. i actually find a good way to debug on spark. i may write it up in my next blog. todo : i’ll add code snippets from jupyter notebook sometime later whenever i come across these errors again references : ""no space left on device"" on osx docker if you run out of disk space in your docker containers on osx, this is probably the best thing to run: docker rm… www.peterbe.com livy & jupyter notebook & sparkmagic = powerful & easy notebook for data scientist livy is a rest server of spark. you can see the talk of the spark summit 2016, microsoft uses livy for hdinsight with… blog.chezo.uno pyspark internals - spark - apache software foundation at the moment, union() requires that its inputs were serialized with the same serializer. when unioning an… cwiki.apache.org pyspark package - pyspark 2.1.0 documentation read a directory of binary files from hdfs, a local file system (available on all nodes), or any hadoop-supported file… spark.apache.org spark pyspark erros jupyter distributed systems -- -- 4 written by yhoztak 67 followers · 71 following curious engineer responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@ashok.tankala/run-your-first-spark-program-using-pyspark-and-jupyter-notebook-3b1281765169,,,"run your first spark program using pyspark and jupyter notebook | by ashok tankala | medium sitemap open in app sign up sign in medium logo write sign up sign in run your first spark program using pyspark and jupyter notebook ashok tankala 5 min read · sep 2, 2018 -- 4 listen share i think almost all whoever have a relationship with big data will cross spark path in one way or another way. i know one day i need to go for a date with spark but somehow i was postponing for a long time, that day came i am excited about this new journey. we recently started offline segmentation support in our current project. basically what segmentation does is shortlists the users based on conditions given in segmentation definition like user visited x page and did y event & etc. you may ask what will you do with the segmented users or why do you need segmentation? let me explain with an example related to one of our company’s cool & super website cardekho . our data team constantly searches for good offers related to cars. offers differ from city to city, car model to model & it has validity too. we want to share these to users. if we share all offers its literally spamming. so we need to share relevant offer to the user. let’s say we got a good offer for maruti swift car in hyderabad city we need to send a notification to the users who visited our web site or mobile site or app last week and did research related to maruti swift car and they are from hyderabad city. so, in this case, we need to get segmented users. this example segment contains 3 conditions. we are using cassandra database because our application is write heavy & its time series data. so, every time based on conditions we need to get the users belongs to those partitions and do union if or condition, intersection if its and condition. more than 1 million users visit our company’s indian cars websites, mobile sites & apps every day. so if we want to share something important to any broad segment users our application goes out of memory because of several reasons like ram, large object space limit & etc. we can do a couple of optimizations but we know those are temporary fixes. i realized its time to meet my future love spark . enough chit chat lets start. i am using a mac machine, so setup steps related to mac. i hope you have homebrew installed in your mac if not follow this link . spark is implemented on hadoop/hdfs and written mostly in scala , a functional programming language which runs on the jvm . so, we need to first install java. run below command to install java. brew cask install java right now java9 is installed by default. if you want to install java8 then run below commands. brew tap caskroom/versions brew cask install java8 to check whether java installed correctly or not just run below command. java -version you will get output like this java version ""1.8.0_102"" java(tm) se runtime environment (build 1.8.0_102-b14) java hotspot(tm) 64-bit server vm (build 25.102-b14, mixed mode) by default, you will have python. so you don’t need to install python. if you are like wanting to work with the latest software then you love to work in python3. for that, you need to run below command. brew upgrade python previously we need to download spark from spark site and extract it and do the stuff. now the pyspark package is available so no need to worry about all those. run below command to install pyspark . #if you are using python2 then use `pip install pyspark` pip3 install pyspark you will get output like this $ pip3 install pyspark collecting pyspark downloading https://files.pythonhosted.org/packages/ee/2f/709df6e8dc00624689aa0a11c7a4c06061a7d00037e370584b9f011df44c/pyspark-2.3.1.tar.gz (211.9mb) 100% |████████████████████████████████| 211.9mb 19kb/s collecting py4j==0.10.7 (from pyspark) downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kb) 100% |████████████████████████████████| 204kb 158kb/s building wheels for collected packages: pyspark running setup.py bdist_wheel for pyspark ... done stored in directory: /users/ashoktankala/library/caches/pip/wheels/37/48/54/f1b63f0dbb729e20c92f1bbcf1c53c03b300e0b93ca1781526 successfully built pyspark installing collected packages: py4j, pyspark successfully installed py4j-0.10.7 pyspark-2.3.1 almost there. one last thing. if you are going to use spark means you will play a lot of operations/trails with data so it makes sense to do those using jupyter notebook. run below command to install jupyter . #if you are using python2 then use `pip install jupyter` pip3 install jupyter first, we need to know where pyspark package installed so run below command to find out #if you are using python2 then use `pip show pyspark` pip3 show pyspark you will get output like this name: pyspark version: 2.3.1 summary: apache spark python api home-page: https://github.com/apache/spark/tree/master/python author: spark developers author-email: dev@spark.apache.org license: http://www.apache.org/licenses/license-2.0 location: /usr/local/lib/python3.7/site-packages requires: py4j required-by: so it means pyspark installed at /usr/local/lib/python3.7/site-packages. so, spark_home will be /usr/local/lib/python3.7/site-packages/pyspark. now we need to set spark_home environment variable. open .bash_profile using command vi ~/.bash_profile add below line export spark_home=/usr/local/lib/python3.7/site-packages/pyspark if you want to run pyspark shell then add below line too. export path=$spark_home/bin:$path in our case, we want to run through jupyter and it had to find the spark based on our spark_home so we need to install findspark pacakge. install it using below command. #if you are using python2 then use `pip install findspark` pip3 install findspark it’s time to write our first program using pyspark in a jupyter notebook. run below command to start a jupyter notebook. jupyter notebook then automatically new tab will be opened in the browser and then you will see something like this. now click on new and then click on python 3. if you are using python 2 then you will see python instead of python 3 . then a new tab will be opened where new notebook is created for our program. let’s write a small program which outputs each word count in a file. first create a file and let’s add a sentence in that file. code for this program is # to find out where the pyspark import findspark findspark.init() # creating spark context from pyspark import sparkcontext sc = sparkcontext(""local"", ""first app"") # calculating words count text_file = sc.textfile(""onesentence.txt"") counts = text_file.flatmap(lambda line: line.split("" "")) \ .map(lambda word: (word, 1)) \ .reducebykey(lambda a, b: a + b) # printing each word with its respective count output = counts.collect() for (word, count) in output: print(""%s: %i"" % (word, count)) # stopping spark context sc.stop() congrats on your first program with pyspark using jupyter notebook. peace. happy coding. see my original article here. spark pyspark jupyter notebook -- -- 4 written by ashok tankala 213 followers · 5 following i help aspiring & emerging leaders gain clarity & reach their potential so they can build a fulfilling life both personally and professionally - http://tanka.la responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://lynnlangit.medium.com/scaling-custom-machine-learning-on-aws-d9dc7edfbff9,,,"scaling custom machine learning on aws | by lynn langit | medium sitemap open in app sign up sign in medium logo write sign up sign in scaling custom machine learning on aws lynn langit 8 min read · oct 2, 2018 -- listen share understanding the challenge bioinformatics is one of the most interesting and challenging areas to work on scaling big data machine learning solutions. these challenges include not only the size and scale of genomic data (3 billion dna ‘letters’ per person). they also include the potential to improve feedback loops for important research in human health, such as understanding significant variants in genomic data for potential crispr-cas9 research. this research can have profound impact on diseases such as cancer. variantspark is a custom machine learning library variantspark is a custom machine learning library for genomic data, built on apache spark core in scala. variantspark implements a wide randomforest ml algorithm, which includes custom splits which allows for analysis of input data which can include up to 100 million features . the team (led by dr. denis bauer ) at csiro bioinformatics in sydney, australia created this algorithm. this paper describes the origins of variantspark in more detail. variantspark — wide randomforest machine learning when i joined this project as a remote cloud advisor, the team at csiro wanted to learn how best to utilize the public cloud to scale variantspark job runs. the project goals are as follows: speed — running one job on the current shared on-premise hadoop/spark cluster requires waiting for cluster availability. also, even after the cluster is available, a single variantspark analysis job run can take up to 500 compute hours to complete. predictability — cost (for cloud services) and speed (time to run a job) simplicity — many bioinformatics research teams have 1 or fewer devops dedicated teams members, so easy setup, monitoring & maintenance is needed. reproducibility — using tools to capture aws service configuration as code, so that research can be verified and reproduced is also critical for this domain in this multi-part blog series, i’ll detail our work, moving towards scaling variantspark jobs using the aws cloud. in this first post, i’ll detail all of the actions we took to prepare before we started the actual scaling work. goal 1: understand current state and project goals as with any technical project, we began by reviewing the existing implementation. the csiro team currently uses a shared, on-premise managed hadoop/spark cluster. when they want to perform a variantspark analysis, they submit their job request to their internal it group and wait for availability on the shared cluster. after their job is scheduled, they they wait for results. while data input sizes do vary, for a large analysis, the actual job run time can be several hundred hours. understanding input data genomic sequencer output data is presented for further evaluation in a variety of data formats. variantspark is designed to work with the following data types: .csv, .txt, .vcf, .bgz, .bz2 and parquet. the sizes of the input data can vary widely for analysis jobs, currently “typical” input data sizes vary from 1 gb to 5 tb. variantspark uses two types of files — an input file (the large-sized file for the workload) and a features (or label) file. shown below is a portion of an input file. variantspark input file in .vcf format there are also other data characteristics for consideration. these include the partitioning scheme (for parquet data), and size of job in memory. apache spark works most effectively when all of the input data can fit into the in-memory executors on each worker machine in the cluster. in order to facilitate testing, the csiro team created a selection of sample input data (which we call ‘synthetic data’), to reflect the different types and sizes of analysis jobs. job types — # rows (samples) * # features (columns)-> total data size tiny — demo — 2.5k samples * 17k features→ 1 gb small — partial gwas — 5k samples* 2.5m features→ 5 gb huge 1 — big gwas1–10k samples* 50m features→ 217 gb huge 2 — big gwas2–5k samples* 100m features→ 221 gb why so many features? the human genome has 3 billion data points. understanding compute resources the computing power available to team (the on-premises shared hadoop/spark cluster) consists of the following specifications: 12 servers, each with 16 cpus & 96 gb ram when testing the largest-sized variantspark jobs, 10 of the total available 12 servers were needed to be able to cache all job data. that cluster size is effectively 160 cpus & ~ 1 tb ram. spark is configured to use ‘whole node executors’ as well. reviewing the oss library we reviewed the state of the variantspark oss library on github and worked with the team to perform the following preparatory actions: built code and ran all tests. used ide code coverage tools to evaluate areas of the code base which were covered with unit tests, with emphasis on the ml algorithm areas. refactored code, mostly using safe renaming, to remove the need for most code comments. deleted commented out code blocks added scaladocs to key machine learning sections of the code create a task list on github for next work steps pushed a compiled variantspark.jar file to a public maven repository our community created a python api (wrapper) for usability too! key code is shown below. community partners dius coded a python wrapper for variantspark goal 2: select cloud services the team at csiro has been developing a number of research tools on the public cloud, including applications such as gt-scan-2 on aws . they wanted to work to consider the available options for scaling variantspark jobs on the public cloud as well. as with other client work, the first level of consideration was the type of compute resources we wished to test. shown below is a list of aws services that we considered, grouped by type. one of the key constraints, is that at the time we began work, the team at csiro bioinformatics didn’t have any dedicated devops professional on their team. for this reason, we decided to start not by designing a solution for scaling, rather first to build a solution for the main purpose of doing quick demonstrations for a wide variety of public audiences — both bioinformaticians and also technical professionals. goal 3: building a sample as previously stated, the goal of this phase of the work was to create a sample. this sample should make it fast, easy, free and fun to try variantspark out on the cloud for ‘customers’. due to privacy considerations, we incorporated realistic (rather than real) data and the team created a fake phenotype — hipsterism. hipsterism traits we also created a sample jupyter notebook for this synthetic affliction. a jupyter notebook, rather than a bash script, was used so that new users could more easily understand the way variantspark works. in bioinformatics, jupyter notebooks are becoming increasingly pervasive, as they improve reproducible research, by containing not only executable code, but also data visualizations and markdown text. this text helps to document experiments and jobs. using saas to get customer feedback for this portion of the work, i recommended using a saas (spark as a service) cloud solution due to ease of setup and use. we built our sample using databricks on aws. using a free community databricks cluster and hipsterindex databricks notebook example allows a new user to run their first job with minimal setup steps. the databricks aws community edition includes 1 hour of free managed spark cluster compute use with a single node cluster. learn about our work — link . here is a presentation which includes a screencast demo showing how to run the example (demo starts at 18:00) additionally our work on this sample has become part of the official databricks documentation , as an example of a genomics applications built on apache spark. goal 4: selecting services for scaling having successfully built our sample, it was time to get to work on scaling variantspark jobs. we first needed to determine which aws services to use for testing variantspark jobs at scale. we must consider exploring aws compute and data services which would be best fit for testing, scaling and performance verification. first we discarded lambda, because we are working with a stateful machine learning algorithm and lambdas are stateless. we could then consider the following: unmanaged virtual machines — ec2 managed virtual machines — emr unmanaged containers — ecs or eks managed containers — fargate or sagemaker again we looked to reduce the size of this list by removing some options. we removed ec2, because of the lack of devops team members on most bioinformatics teams. the amount of administrative overhead to setup and maintain a hadoop/spark cluster on ec2 wasn’t a fit for the size of team at csiro. we removed fargate because we preferred to use kubernetes, rather than ecs for container management. as of this writing, fargate does not yet support eks. we feel that kubernetes has emerged as a standard for container management in the public cloud in general and we favor flexibility in our implementation. that left aws emr, eks and sagemaker as the basis for the start of our scaling work. goal 5: prepare for testing at scale now that we had done a good amount of preparatory work, we had just a few more steps to complete to be able to start our scaling tests. we needed to design and build test harnesses. this included working with the team to generate sample data and a couple of sample scripts. for convenience, we run these scripts from jupyter notebooks — so that we can add documentation about our testing run conditions directly in the notebook. also we needed to verify the aws account, aws region, iam users and set up aws service limits (billing alarms) to use for our work. additionally we chose to attempt to use both aws cloudformation and terraform , so that we could select the best automation process for this work and for ongoing movement to using cloud services by the team. from terraform — infrastructure as code finally, we worked with the group in sydney to build and run a couple of example docker containers locally. we did so because using containers for running compute was new to them. we both created sample containers (such as this one which includes the common bioinformatics tool blastn, sample data and a example in an included jupyter notebook) and also used the biocontainers registry. we had the team use the docker desktop tools to run these containers locally. now that we had completed this preparatory work, we were ready to start work on scaling. in the next blog post i’ll detail how our work on scaling variantspark on aws has been progressing. machine learning spark bioinformatics aws open source -- -- written by lynn langit 2.1k followers · 273 following cloud architect who codes. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/netflix-techblog/netflix-at-spark-ai-summit-2018-5304749ed7fa,,,"netflix at spark+ai summit 2018. by faisal siddiqi | by netflix technology blog | netflix techblog sitemap open in app sign up sign in medium logo write sign up sign in netflix techblog · follow publication learn about netflix’s world class engineering efforts, company culture, product developments and more. follow publication netflix at spark+ai summit 2018 netflix technology blog 5 min read · jun 21, 2018 -- 1 listen share by faisal siddiqi a glimpse at spark usage for netflix recommendations apache spark has been an immensely popular big data platform for distributed computing. netflix has been using spark extensively for various batch and stream-processed workloads. a substantial list of use cases for spark computation come from the various applications in the domain of content recommendations and personalization. a majority of the machine learning pipelines for member personalization run atop large managed spark clusters. these models form the basis of the recommender system that backs the various personalized canvases you see on the netflix app including, title relevance ranking, row selection & sorting, and artwork personalization among others. spark provides the computation infrastructure to help develop the models through data preparation, feature extraction, training, and model selection. the personalization infrastructure team has been helping scale spark applications in this domain for the last several years. we believe strongly in sharing our learnings with the broader spark community and at this year’s spark +ai summit in san francisco, we had the opportunity to do so via three different talks on projects using spark at netflix scale. this post summarizes the three talks. fact store for netflix recommendations ( nitin sharma , kedar sadekar ) the first talk cataloged our journey building training data infrastructure for personalization models — how we built a fact store for extracting features in an ever-evolving landscape of new requirements. to improve the quality of our personalized recommendations, we try an idea offline using historical data. ideas that improve our offline metrics are then pushed as a/b tests which are measured through statistically significant improvements in core metrics such as member engagement, satisfaction, and retention. the heart of such offline analyses are historical facts (for example, viewing history of a member, videos in ‘my list’ etc) that are used to generate features required by the machine learning model. ensuring we capture enough fact data to cover all stratification needs of various experiments and guarantee that the data we serve is temporally accurate is an important requirement. in the talk, we presented the key requirements, evolution of our fact store design, its push-based architecture, the scaling efforts, and our learnings. we discussed how we use spark extensively for data processing for this fact store and delved into the design tradeoffs of fast access versus efficient storage. near real-time recommendations with spark streaming ( elliot chow , nitin sharma ) many recommendations for the personalization use cases at netflix are precomputed in a batch processing fashion, but that may not be quick enough for time sensitive use cases that need to take into account member interactions, trending popularity, and new show launch promotions. with an ever-growing netflix catalog, finding the right content for our audience in near real-time is a necessary element to providing the best personalized experience. our second talk delved into the realtime spark streaming ecosystem we have built at netflix to provide this near-line ml infrastructure. this talk was contextualized by a couple of product use cases using this near-real-time (nrt) infrastructure, specifically how we select the personalized video to present on the billboard (large canvas at the top of the page), and how we select the personalized artwork for any title given the right canvas. we also reflected upon the lessons learnt while building a high volume infrastructure on top of spark streaming. with regards to the infrastructure, we talked about: scale challenges with spark streaming state management that we had to build on top of spark data persistence resiliency, metrics, and operational auto-remediation spark-based stratification library for ml use cases ( shiva chaitanya ) our last talk introduced a specific spark based library that we built to help with stratification of the training sets used for offline machine learning workflows. this allows us to better model our users’ behaviors and provide them great personalized video recommendations. this library was originally created to implement user selection algorithms in our training data snapshotting infrastructure , but it has evolved to cater to the general-purpose stratification use cases in ml pipelines. the main idea here is to be able to provide a mechanism for down-sampling the data set while still maintaining the desired constraints on the data distribution. we described the flexible stratification api on top of spark dataframes. choosing spark+scala gave us strong type safety in a distributed computing environment. we gave some examples of how, using the library’s dsl one can easily express complex sampling rules. these talks presented a few glimpses of the spark usage from the personalization use cases at netflix. spark is also used for many other data processing, etl, and analytical uses in many other different domains in netflix. each domain brings its unique sets of challenges. for the member-facing personalization domain, the infrastructure needs to scale at the level of member scale. that means, for our over 125 million members and each of their active profiles, we need to personalize our content and do so reasonably fast for it to be relevant and timely. while spark provides a great horizontally-scalable compute platform, we have found that using some of the advanced features, like code-gen for example, at our scale often poses interesting technical challenges. as spark’s popularity grows, the project will need to continue to evolve to meet the growing hunger for truly big data sets and do a better job at providing transparency and ease of debugging for the workloads running on it. this is where sharing lessons from one organization can help benefit the community-at-large. we are happy to share our experiences at such conferences and welcome the ongoing interchange of ideas on making spark better for modern ml and big data infrastructure use cases. if you are interested in joining such efforts, we’d love to hear from you as we look to accelerate our focus in areas of compute infrastructure for personalization and data systems for personalization . big data apache spark machine learning recommendations netflix -- -- 1 follow published in netflix techblog 166k followers · last published jun 12, 2025 learn about netflix’s world class engineering efforts, company culture, product developments and more. follow written by netflix technology blog 437k followers · 10 following learn more about how netflix designs, builds, and operates our systems and engineering organizations responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/towards-data-science/sentiment-analysis-with-pyspark-bc8e83f80c35,,,"sentiment analysis with pyspark. one of the tools i’m deeply interested… | by ricky kim | tds archive | medium sitemap open in app sign up sign in medium logo write sign up sign in tds archive · an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. member-only story sentiment analysis with pyspark ricky kim 8 min read · mar 13, 2018 -- 11 share photo by chris j. davis on unsplash one of the tools i’m deeply interested but haven’t had many chances to explore is apache spark. most of the time, pandas and scikit-learn is enough to handle the size of data i’m trying to build a model on. but that also means that i haven’t had a chance to deal with petabytes of data yet, and i want to be prepared for the case i’m faced with a real big-data. i have tried some basic data manipulation with pyspark before, but only to a very basic level. i want to learn more and be more comfortable in using pyspark. this post is my endeavour to have a better understanding of pyspark. python is great for data science modelling, thanks to its numerous modules and packages that help achieve data science goals. but what if the data you are dealing with cannot be fit into a single machine? maybe you can implement careful sampling to do your analysis on a single machine, but with distributed computing framework like pyspark, you can efficiently implement the task for large datasets. spark api is available in multiple programming languages (scala, java, python and r). there are debates about how spark performance varies depending on which language you run it on, but since the main language i have been using is python, i will focus on pyspark without going into too much detail of what language should i choose for apache spark. -- -- 11 published in tds archive 825k followers · last published feb 3, 2025 an archive of data science, data analytics, data engineering, machine learning, and artificial intelligence writing from the former towards data science medium publication. written by ricky kim 2.7k followers · 92 following the rickest ricky. love data, beer, coffee, and good memes in no particular order. responses ( 11 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@eric.ch.day/spark-scala-hive-sql-simple-tests-54afacdcc418,,,"spark, scala & hive sql simple tests | by eric day | medium sitemap open in app sign up sign in medium logo write sign up sign in spark, scala & hive sql simple tests eric day 6 min read · aug 29, 2017 -- listen share continuing the work on learning how to work with big data, now we will use spark to explore the information we had previously loaded into hive. cloudera installation does not install spark by default, but adding the service is a breeze. just open your dropdown button of your cluster and the very first option is add service, click it, and it will take you to another page where you can select which services to add, spark will be there. ok, before going into spark with hive info, since this is our first try, it is important not to try to run before we are sure we can walk. so, lets try an equivalent to “hello world” (this should be an * always * when starting to code in new or different languages). first things first. what are we going to analyze? we need a file that spark will process. create a text file with information, for this case words, will be counted. here is my file content (inputfile.txt): need to place this file inside the hadoop file system: $ hdfs dfs -put inputfile.txt now that we have information that spark will analyze, we can create a program to do something with the information. this is the example we followed: quick start — spark 1.2.0 documentation spark’s shell provides a simple way to learn the api, as well as a powerful tool to analyze data interactively. it is… spark.apache.org go to the ‘self-contained applications’ section. this is the sample scala code: /* simpleapp.scala */ import org.apache.spark.sparkcontext import org.apache.spark.sparkcontext._ import org.apache.spark.sparkconf object simpleapp { def main(args : array[string]) { val logfile = ""your_spark_home/readme.md"" // should be some file on your system val conf = new sparkconf ().setappname(""simple application"") val sc = new sparkcontext (conf) val logdata = sc.textfile(logfile, 2).cache() val numas = logdata.filter(line => line.contains(""a"")).count() val numbs = logdata.filter(line => line.contains(""b"")).count() println(""lines with a: %s, lines with b: %s"".format(numas, numbs)) } } place this file in: $ sparkdirectory/src/main/scala/simpleapp.scala. will you use git to control this project? its up to you but very recommendable. of course, you can download this project from my github: https://github.com/eday69/sparkwordcount “your_spark_home” directory is based of the hdfs file system structure. so careful there. when you put into hdfs, check where the file was put or place it in an specific directory. also, found that the sbt package is not installed either. what is sbt? sbt (simple build tool) is an open source build tool for scala and java projects, similar to java’s maven and ant. its main features are: native support for compiling scala code and integrating with many scala test frameworks. build descriptions written in scala using a dsl. so need to install it: $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2ee0ea64e40a89b84b2df73499e82a75642ac823 $ sudo apt-get update $ sudo apt-get install sbt which take a little while, but it works. need to create .sbt file that will located at the base of the project directory. my name was sprwrdcnt.sbt: ubuntu@ip-172–31–29–164 : ~/spark-application $ cat sprkwrdcnt.sbt name := “spark word count” version := “1.0” scalaversion := “2.10.4” librarydependencies += “org.apache.spark” %% “spark-core” % “1.2.0” issue command, it will create the project structure and classes need to run spark. $ sbt package now we have all the element ready to call spark. just change the name of your class and your app jar name. $ spark-submit — class “simpleapp” — master local[4] target/scala-2.10/spark-word-count_2.10–1.0.jar found that i had wrong the path to the text file, so, of course spark was not finding it, changed it; but it is necessary to re-execute command $ sbt package to regenerate files. if you look closely at the stdout results, you will see that the job was completed satisfactory. i’ve copied just the lines with the results we were looking for: 17/08/06 23:08:42 info scheduler.dagscheduler: job 1 finished: collect at sparkwordcount.scala:28, took 0.168665 s (p,3), (x,1), (t,2), (b,1), (h,2), (n,5), (f,1), (v,2), (r,4), (l,1), (s,3), (e,9), (a,5), (i,2), (u,2), (o,4), (g,1), (m,1), (c,1) now we move on. we will create another directory called spark-application2 this time. and create another scala program. this time, instead of reading from a file, we will try to read from a hive sql table. again, using git to control project. next we compile the project: sbt package a now run the job: $ spark-submit — class “sphiveapp” — master local[4] target/scala-2.10/sparkhive-sql-tester_2.10–1.0.jar did it run? no. why? many reasons, and after much research to find errors (and mistakes), this is what i can explain: cdh 5.12 does not include spark 2.0, it has spark 1.6. and why is this important you may ask, the answer is because spark 1.6 does not support sparksession, so you need to work the scala program a different way. dependencies and versions are very important, and you must be consistent on this throughout the dependencies. a bit obviuos, but it did happen to me, make sure the hive and spark are running on your server. another, obvious to some, not obvious to me, was the .sbt config file. i had two config files in the same folder. did the mistake of working with one and ‘thinking’ that sbt would not pay attention to the other one, another big mistake from my part. so what did we change? first, we eliminated the config file (.sbt file) that was not being used. then changed the config file to this: one change you can see i did, was the creation of “val sparkversion = 1.6.0”. and use it with each library dependency. this way, we can be sure that we are being consistent in all dependencies. changed our scala program to this: you can see that the program is much simpler. it is only a simple query to a hive table, selecting only 10 records from it since it has many. this is part of the result, where you can the results of the query: you can find the code for this example in my github: eday69/sparkhivetableinfo sparkhivetableinfo - using spark to read a hive table with sql and giving analyzed info about it. github.com what are we trying to do? why go through all this? good question. remember, we are working big data now, lots of information. relational tables will not do it, so we are using hadoop to be able to manage large volumes of info. hive & spark will help us with this. we will build a recommendation engine with spark in scala. that is the goal. so far, the scala we have used is simple and there is plenty of documentation on that, but the errors we had were very confusing to solve since info on them was not clear, at least to me. all for now, see you in the next chapter. big data spark scala hive cloudera -- -- written by eric day 44 followers · 19 following full-stack developer. business man, tech lover, programmer, family man, harley addict and runner. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/sicara/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f,,,"get started with pyspark and jupyter notebook in 3 minutes | by charles bochet | sicara's blog | medium sitemap open in app sign up sign in medium logo write sign up sign in sicara's blog · we build tailor-made ai and big data solutions for amazing clients get started with pyspark and jupyter notebook in 3 minutes charles bochet 5 min read · may 2, 2017 -- 32 listen share spark with jupyter read the original article on sicara’s blog here . apache spark is a must for big data’s lovers . in a few words, spark is a fast and powerful framework that provides an api to perform massive distributed processing over resilient sets of data. jupyter notebook is a popular application that enables you to edit, run and share python code into a web view. it allows you to modify and re-execute parts of your code in a very flexible way. that’s why jupyter is a great tool to test and prototype programs. jupyter notebook running python code i wrote this article for linux users but i am sure mac os users can benefit from it too. why use pyspark in a jupyter notebook? while using spark, most data engineers recommends to develop either in scala (which is the “native” spark language) or in python through complete pyspark api . python for spark is obviously slower than scala. however like many developers, i love python because it’s flexible, robust, easy to learn, and benefits from all my favorites libraries. in my opinion, python is the perfect language for prototyping in big data/machine learning fields. if you prefer to develop in scala, you will find many alternatives on the following github repository: alexarchambault/jupyter-scala to learn more about python vs. scala pro and cons for spark context, please refer to this interesting article: scala vs. python for apache spark . now, let’s get started. install pyspark before installing pyspark, you must have python and spark installed. i am using python 3 in the following examples but you can easily adapt them to python 2. go to the python official website to install it. i also encourage you to set up a virtualenv . to install spark, make sure you have java 8 or higher installed on your computer . then, visit the spark downloads page . select the latest spark release, a prebuilt package for hadoop, and download it directly. unzip it and move it to your /opt folder: $ tar -xzf spark-1.2.0-bin-hadoop2.4.tgz $ mv spark-1.2.0-bin-hadoop2.4 /opt/spark-1.2.0 create a symbolic link: $ ln -s /opt/spark-1.2.0 /opt/spark̀ this way, you will be able to download and use multiple spark versions. finally, tell your bash (or zsh, etc.) where to find spark. to do so, configure your $path variables by adding the following lines in your ~/.bashrc (or ~/.zshrc ) file: export spark_home=/opt/spark export path=$spark_home/bin:$path install jupyter notebook install jupyter notebook: $ pip install jupyter you can run a regular jupyter notebook by typing: $ jupyter notebook your first python program on spark let’s check if pyspark is properly installed without using jupyter notebook first. you may need to restart your terminal to be able to run pyspark. run: $ pyspark welcome to ____              __ / __/__  ___ _____/ /__ _\ \/ _ \/ _ `/ __/  '_/ /__ / .__/\_,_/_/ /_/\_\   version 2.1.0 /_/ using python version 3.5.2 (default, jul  2 2016 17:53:06) sparksession available as 'spark'. >>> it seems to be a good start! run the following program: (i bet you understand what it does!) import random num_samples = 100000000 def inside(p): x, y = random.random(), random.random() return x*x + y*y < 1 count = sc.parallelize(range(0, num_samples)).filter(inside).count() pi = 4 * count / num_samples print(pi) sc.stop() the output will probably be around 3.14 . pyspark in jupyter there are two ways to get pyspark available in a jupyter notebook: configure pyspark driver to use jupyter notebook: running pyspark will automatically open a jupyter notebook load a regular jupyter notebook and load pyspark using findspark package first option is quicker but specific to jupyter notebook, second option is a broader approach to get pyspark available in your favorite ide. method 1 — configure pyspark driver update pyspark driver environment variables: add these lines to your ~/.bashrc (or ~/.zshrc ) file. export pyspark_driver_python=jupyter export pyspark_driver_python_opts='notebook' restart your terminal and launch pyspark again: $ pyspark now, this command should start a jupyter notebook in your web browser. create a new notebook by clicking on ‘new’ > ‘notebooks python [default]’. copy and paste our pi calculation script and run it by pressing shift + enter. jupyter notebook: pi calculation script done! you are now able to run pyspark in a jupyter notebook :) method 2 — findspark package there is another and more generalized way to use pyspark in a jupyter notebook: use findspark package to make a spark context available in your code. findspark package is not specific to jupyter notebook, you can use this trick in your favorite ide too. to install findspark: $ pip install findspark launch a regular jupyter notebook: $ jupyter notebook create a new python [default] notebook and write the following script: import findspark findspark.init() import pyspark import random sc = pyspark.sparkcontext(appname=""pi"") num_samples = 100000000 def inside(p): x, y = random.random(), random.random() return x*x + y*y < 1 count = sc.parallelize(range(0, num_samples)).filter(inside).count() pi = 4 * count / num_samples print(pi) sc.stop() the output should be: jupyter notebook: pi calculation i hope this 3-minutes guide will help you easily getting started with python and spark. here are a few resources if you want to go the extra mile: https://www.dezyre.com/article/scala-vs-python-for-apache-spark/213 http://queirozf.com/entries/comparing-interactive-solutions-for-running-apache-spark-zeppelin-spark-notebook-and-jupyter-scala http://spark.apache.org/docs/latest/api/python/index.html https://github.com/jadianes/spark-py-notebooks did you like this article? don’t forget to hit the follow button. python spark jupyter big data data engineering -- -- 32 published in sicara's blog 4k followers · last published feb 18, 2020 we build tailor-made ai and big data solutions for amazing clients written by charles bochet 1.1k followers · 328 following @charlesbochet cto @luckeyhomes responses ( 32 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://mrpowers.medium.com/manually-creating-spark-dataframes-b14dae906393,,,"different approaches to manually create spark dataframes | by matthew powers | medium sitemap open in app sign up sign in medium logo write sign up sign in different approaches to manually create spark dataframes matthew powers 2 min read · may 22, 2017 -- 9 listen share this blog post explains the spark and spark-daria helper methods to manually create dataframes for local development or testing. we’ll demonstrate why the createdf() method defined in spark-daria is better than the todf() and createdataframe() methods from the spark source code. see this blog post if you’re working with pyspark (the rest of this post uses scala). todf() todf() provides a concise syntax for creating dataframes and can be accessed after importing spark implicits. import spark .implicits._ the todf() method can be called on a sequence object to create a dataframe. val somedf = seq ( (8, ""bat"" ), (64, ""mouse"" ), (-27, ""horse"" ) ).todf( ""number"" , ""word"" ) somedf has the following schema. root | — number: integer (nullable = false) | — word: string (nullable = true) todf() is limited because the column type and nullable flag cannot be customized. in this example, the number column is not nullable and the word column is nullable. the import spark .implicits._ statement can only be run inside of class definitions when the spark session is available. all imports should be at the top of the file before the class definition, so todf() encourages bad scala coding practices. todf() is suitable for local testing, but production grade code that’s checked into master should use a better solution. createdataframe() the createdataframe() method addresses the limitations of the todf() method and allows for full schema customization and good scala coding practices. here is how to create somedf with createdataframe() . val somedata = seq ( row (8, ""bat"" ), row (64, ""mouse"" ), row (-27, ""horse"" ) ) val someschema = list ( structfield ( ""number"" , integertype, true ), structfield ( ""word"" , stringtype, true ) ) val somedf = spark .createdataframe( spark .sparkcontext.parallelize(somedata), structtype (someschema) ) createdataframe() provides the functionality we need, but the syntax is verbose. our test files will become cluttered and difficult to read if createdataframe() is used frequently. createdf() createdf() is defined in spark-daria and allows for the following terse syntax. val somedf = spark .createdf( list ( (8, ""bat"" ), (64, ""mouse"" ), (-27, ""horse"" ) ), list ( ( ""number"" , integertype, true ), ( ""word"" , stringtype, true ) ) ) createdf() creates readable code like todf() and allows for full schema customization like createdataframe() . it’s the best of both worlds. big shout out to nithish for writing the advanced scala code to make createdf() work so well. creating datasets datasets are similar to dataframes, but preferable at times because they offer more type safety. see this blog post for explanations on how to create datasets with the tods and createdataset methods. including spark-daria in your projects the spark-daria readme provides the following project setup instructions. update your build.sbt file. librarydependencies += ""com.github.mrpowers"" %% ""spark-daria"" % ""0.38.2"" 2. import the spark-daria code into your project: import com.github.mrpowers.spark.daria.sql.sparksessionext._ closing thoughts dataframes are a fundamental data structure that are at the core of my spark analyses. see this blog post for the different approaches on how to create datasets, a related data structure. i wrote a beautiful spark code book that teaches the core aspects of spark development with dataframes. the book is the best way to learn how to get good at spark quickly. scala -- -- 9 written by matthew powers 2.7k followers · 49 following spark coder, live in colombia / brazil / us, love scala / python / ruby, working on empowering latinos and latinas in tech responses ( 9 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://mrpowers.medium.com/using-external-data-files-in-spark-apps-44b137ce47ba,,,"joining external data files with spark dataframes | by matthew powers | medium sitemap open in app sign up sign in medium logo write sign up sign in joining external data files with spark dataframes matthew powers 5 min read · dec 2, 2017 -- listen share you’ll often need to incorporate external data files in your spark applications. you’ll also want to store data for large arrays or maps in data files rather than store them in code. you’ll greatly improve the quality and maintainability of your code by knowing when to make data file abstractions and by maintaining these files as single source of truth data stores. this post will explain how to structure data files, testing code that uses data files, joining data files with dataframes, and converting datafiles to arrays / maps. joining a data file with a dataframe small data files can be broadcasted and joined with dataframes. let’s use a data file that maps state names to state abbreviations as an example. state_name,state_abbreviation tennessee,tn new york,ny mississippi,ms let’s create a dataframe and use a broadcast join to append the state_abbreviation column to the dataframe. val sourcedf = seq ( ( ""britney spears"" , ""mississippi"" ), ( ""romeo santos"" , ""new york"" ), ( ""miley cyrus"" , ""tennessee"" ), ( ""random dude"" , null ), ( null , ""dubai"" ) ).todf( ""name"" , ""birth_state"" ) val statemappingsdf = spark .read .option( ""header"" , ""true"" ) .option( ""charset"" , ""utf8"" ) .csv(""/some/path/state_mappings.csv"") val resultdf = sourcedf.join( broadcast (statemappingsdf), sourcedf( ""birth_state"" ) <=> statemappingsdf( ""state_name"" ), ""left_outer"" ).drop(statemappingsdf( ""state_name"" )) resultdf.show() +--------------+-----------+------------------+ |          name|birth_state|state_abbreviation| +--------------+-----------+------------------+ |britney spears|mississippi|                ms| |  romeo santos|   new york|                ny| |   miley cyrus|  tennessee|                tn| |   random dude|       null|              null| |          null|      dubai|              null| +--------------+-----------+------------------+ you’ll need to update /some/path/state_mappings.csv to get this code to work on your machine. here are some common design patterns you’ll follow when appending a column to a dataframe based on a data file: using the null safe equality operator ( <=> ) to match columns when making joins doing a broadcast join ( broadcast (statemappingsdf) ) doing a left_outer join dropping columns in the mapping dataframe after the join is executed ( drop(statemappingsdf( “state_name” )) ) this design pattern is only performant when the data file is small enough to be broadcasted. you’ll need to take a different approach for big data files. let’s refactor this code, so we don’t need to hardcode the data file path. managing data file paths with a config object we’ll want one data file path for the test environment and another path for when the code is run in production. the test files are usually stored in the /src/test/resources directory and the production data files are usually stored in s3. let’s make a config object with different paths for the data file depending on the project_env environment variable. object config { val test : map[string, string] = { map ( ""statemappingspath"" -> new java.io.file( s""./src/test/resources/state_mappings.csv"" ).getcanonicalpath ) } val production : map[string, string] = { map ( ""statemappingspath"" -> ""s3a://some-fake-bucket/state_mappings.csv"" ) } var environment = sys. env .getorelse( ""project_env"" , ""production"" ) def get(key: string): string = { if ( environment == ""test"" ) { test (key) } else { production (key) } } } now let’s update the build.sbt file to set the project_env variable to test in the test environment. envvars in test := map ( ""project_env"" -> ""test"" ) we’re ready to refactor our code that reads in the mapping csv file. val statemappingsdf = spark .read .option( ""header"" , ""true"" ) .option( ""charset"" , ""utf8"" ) .csv(config. get ( ""statemappingspath"" )) the same code will read in different mapping files depending on the project_env — amazing! 😺 accessing data file columns as arrays you will often want to convert a column of an external data file to an array to write your application logic. spark-daria defines helper methods to convert dataframes into arrays or maps. let’s use the dataframehelpers.columntoarray method defined in spark-daria to convert the state_abbreviations column from the state_mappings.csv file to an array of strings. val statemappingsdf = spark .read .option( ""header"" , ""true"" ) .option( ""charset"" , ""utf8"" ) .csv(config. get ( ""statemappingspath"" )) val abbreviations = dataframehelpers. columntoarray [string]( statemappingsdf, ""state_abbreviation"" ) println (abbreviations.mkstring( "" "" )) // prints ""tn ny ms"" let’s create another dataframe and use the abbreviations array to append an is_valid_state_abbr column. val sourcedf = seq ( ( ""ny"" ), ( ""nowhere"" ), ( null ) ).todf( ""state_abbr"" ) val resultdf = sourcedf.withcolumn( ""is_valid_state_abbr"" , col ( ""state_abbr"" ).isin(abbreviations: _*) ) resultdf.show() +----------+-------------------+ |state_abbr|is_valid_state_abbr| +----------+-------------------+ |        ny|               true| |   nowhere|              false| |      null|               null| +----------+-------------------+ it’s always better to abstract large arrays into data files. you shouldn’t manage huge arrays in your codebase. structuring data files data files should be structured with snake_case column headers and should follow the same best practices of database tables. each row should represent a set a related data and have the same structure. here’s an example of a well structured data file: city,state houston,texas charleston,south carolina louisville,kentucky each row in the data file contains related data and each row has two columns. storing small data files in github github is a great place to store small data files. you can store up to 1gb of data in a github repository, so this approach works best for data files smaller than 5mb (github really works best for tiny data files less than 500kb). github data files are easily searchable and benefit from version control. you can use the python pandas library and invoke to create tasks that sort, de-duplicate and upload your data files from the git repository to s3. message me in the comments if you’d like more instructions and code snippets on how to do this easily. next steps certain data should be managed in the codebase and other data should be abstracted to data files. as a rule of thumb, i like to abstract anything with more than 50 rows to an external data file. i will even abstract data files with as little as 10 rows to a data file if there are a lot of columns. you can manage the data files in github or a relational database and create automated tasks to upload the data files to s3. i’ve found spark connectors to relational databases like redshift to be slow and finicky, so it’s best to make your data files available to spark via s3. it’s important to manage your data files as single source of truth data stores. the data in github (or the relational database) should be treated as the single source of truth and the replication in s3 is only to make it easier to work with spark. never modify a data file in s3 directly. always update the single source of truth data store and then run the automated task to upload the new data file to s3. remember to use the spark-daria helper methods whenever converting a data file to an array or a map. you don’t want to litter your codebase with that complex logic. external data files make code that’s easier to maintain, test, and understand by non-coders! you can even put non-coders in charge of the data files, so they can make changes that are immediately reflected in your systems without doing a code deploy. data science spark apache spark single source of truth dataframes -- -- written by matthew powers 2.7k followers · 49 following spark coder, live in colombia / brazil / us, love scala / python / ruby, working on empowering latinos and latinas in tech no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/netflix-techblog/evolving-the-netflix-data-platform-with-genie-3-598021604dda,,,"evolving the netflix data platform with genie 3 | by netflix technology blog | netflix techblog sitemap open in app sign up sign in medium logo write sign up sign in netflix techblog · follow publication learn about netflix’s world class engineering efforts, company culture, product developments and more. follow publication evolving the netflix data platform with genie 3 netflix technology blog 7 min read · jun 7, 2017 -- 1 listen share by tom gianos the big data space continues to change at a rapid pace. data scientists and analysts have more tools than ever at their disposal whether it be spark, r, presto, or traditional engines like hive and pig. at netflix the big data platform team is responsible for making these tools available, reliable and as simple as possible for our users at massive scale. for more information on our overall architecture you can see our talks at strata 2017 , qcon 2016 , re:invent 2016 or find others on our netflix data youtube channel. genie is one of the core services in the netflix data platform. it provides apis for users to access computational resources without worrying about configuration or system state. in the past, we’ve written about the motivations to develop genie and why we moved to genie 2 . this post is going to talk about new features in the next generation of genie (i.e., genie 3) which enable us to keep up with netflix scale, evolution of tools, and expanding use cases. we will also explore some of our plans for genie going forward. our current scale and use cases genie 3 has been running in production at netflix since october 2016 serving about 150k jobs per day (~700 running at any given time generating ~200 requests per second on average) across 40 i2.4xl aws ec2 instances. within netflix we use genie in two primary ways. the primary use case is for users to submit job requests to the jobs api and have the job clients run on the genie nodes themselves. this allows various systems (schedulers, micro-services, python libraries, etc) at netflix to submit jobs and access the data in the data warehouse without actually knowing anything about the data warehouse or clusters themselves. a second use case which has evolved over time is to leverage genie’s configuration repository to set up local working directories for local mode execution. after genie sets up the working directory, it will return control to the user who can then invoke the run script as needed. we use this method to run repl’s for various engines like hive, spark, etc. which need to capture stdout. features high level genie 3 architecture while genie 3 has many new features , we’re going to focus on a few of the bigger ones in this post including: a redesigned job execution engine cluster leadership security dependency caching execution engine redesign in genie 2, we spent a lot of time reworking the data model, system architecture and api tier. what this left out was the execution engine which is responsible for configuring and launching jobs after a request is received by the jobs api. the reasoning was the execution piece worked well enough for the use cases that existed at the time. the execution engine revolved around configuring a job directory for each job in a rigid manner. there was a single job execution script which would be invoked when setup was complete for any type of job. this model was limited as the set of tools we needed to use grew and a single script couldn’t cover every case. it became increasingly complex to maintain the script and the code around it. in genie 3, we’ve rewritten the entire execution engine from the ground up to be a pluggable set of tasks which generate a run script custom for each individual job. this allows the run script to be different based on what cluster, command and application(s) are chosen at runtime by the genie system. additionally, since the script is now built up dynamically within the application code, the entire job flow is easier to test and maintain. these changes have resulted in an ability for our team to respond to customer requests more quickly as we can change individual application or command configurations without fear of breaking the entire run script. leader election in genie 2 every node was treated equally, that is they all would run a set of tasks intended for system wide administration and stability. these tasks included database cleanup, zombie job detection, disk cleanup and job monitoring. this approach was simpler but had some downsides and inefficiencies. for example, all nodes would repeatedly perform the same database cleanup operations unnecessarily. to address this it would be best for cluster wide administration tasks to be handled by a single node within the cluster. leadership election has been implemented in genie 3, currently supported via either zookeeper or statically setting a single node to be the leader via a property. when a node is elected as leader, a certain set of tasks are scheduled to be run. the tasks need only implement a leadershiptask interface to be registered and scheduled by the system at runtime. they can each be scheduled at times and frequencies independent to each other via either cron based or time delay based scheduling. security high level security design genie allows users to run arbitrary code, via job attachments and dependencies, as well as the ability to access and transport data in the data warehouse back to the genie node. it’s become increasingly important to make every effort to ensure the ability to perform these actions are allowed only by people authorized to do so. we don’t want any users who aren’t administrators changing configurations which could break the system for all other users. we also don’t want anyone not authenticated to be able to access the genie ui and jobs results as the output directories could have sensitive data. therefore a long requested set of features have been added in genie 3 to support application and system security. first, authentication and authorization (authn/authz) have been implemented via spring security. this allows us to plugin backend mechanisms for determining who a user is and decouple the decision of authorization from genie code. out of the box genie currently supports saml based authentication for access to the user interface and oauth2 json web token (jwt) support for api access. other mechanisms could be plugged in if desired. additionally, genie 3 supports the ability to launch job processes on the system host as the user who made the request via sudo. running as users helps prevent a job from modifying another job’s working directory or data since it won’t have system level access. dependency cache as genie becomes more flexible the data platform team has moved from installing many of the application binaries directly on the genie node to having them downloaded at runtime on demand. while this gives us a lot of flexibility to update the application binaries independently of redeploying genie itself, it adds latency as installing the applications can take time before a job can be run. genie 3 added a dependency file cache to address this issue. now when a file system (local, s3, hdfs, etc) is added to genie it needs to implement a method which determines the last updated time of the file requested. the cache will use this to determine if a new copy of the file needs to be downloaded or if the existing cached version can be used. this has helped to dramatically speed up job startup time while maintaining the aforementioned agility for application binaries. and more … genie 3 interface there are many other changes made in genie 3 including a whole new ui (pictured above), data model improvements, client resource management, additional metrics collection, hypermedia support for the rest apis, porting the project to spring boot and much more. for more information, visit the all new website or check out the github release milestones . if you want to see genie 3 in action try out the demo , which uses docker compose so no additional installation or setup necessary beyond docker itself. future work while a lot of work was done in the genie 3 release there are still a lot of features we’re looking to add to genie to make it even better. here are a few: a notification service to allow users to asynchronously receive updates about the lifecycle of their jobs (starting, finished, failed, etc) without the need for polling. this will allow a workflow scheduler to build and execute a dependency graph based on completion of jobs. add flexibility to where genie jobs can run. currently they still run on a given genie node, but we can envision a future where we’re offloading the client processes into titus or similar service. this would follow good microservice design principles and free genie of any resource management responsibility for jobs. open source api for serving configured job directory back to a user enabling them to run it wherever they want. full text search for jobs. wrapping up genie continues to be an integral part of our data ecosystem here at netflix. as we continue to develop features to support our use cases going forward, we’re always open to feedback and contributions from the community. you can reach out to us via github or message on our google group . we hope to share more of what our teams are working on later this year! big data microservices open source aws netflix -- -- 1 follow published in netflix techblog 166k followers · last published jun 12, 2025 learn about netflix’s world class engineering efforts, company culture, product developments and more. follow written by netflix technology blog 437k followers · 10 following learn more about how netflix designs, builds, and operates our systems and engineering organizations responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@deniseschlesinger/querying-our-data-lake-in-s3-using-zeppelin-and-spark-sql-be50c3b7a613,,,"querying our data lake in s3 using zeppelin and spark sql | by denise schlesinger | medium sitemap open in app sign up sign in medium logo write sign up sign in querying our data lake in s3 using zeppelin and spark sql denise schlesinger 5 min read · dec 26, 2017 -- 4 listen share until recently, most companies used the traditional approach for storing all the company’s data in a data warehouse. the internet growth caused an increase in the number of data sources and the massive quantities of data to be stored, requiring scaling these data warehouses constantly. they were not designed to handle petabytes of data, so companies were driven into using big data platforms (such as hadoop) capable of transforming big data into actionable insights at high speed . hadoop traditionally coupled storage and computation together, so this was relatively expensive. each node on a cluster wasn’t just a storage system, it was a server with processors and memory resources. the more data you keep, the more compute resources you needed to pay for, even though you didn’t really need that extra computational power to help you analyze your data. as a result, compute and storage were supposed to be scaled together and the clusters were persistently on, otherwise the data becames inaccessible. this is the force that is driving the separation of storage from compute, to be able to scale them separately. this is how the term data lake emerged, being defined as: “a collection of storage instances of various data assets additional to the originating data sources.” a data lake is a repository capable of storing vast quantities of data in various formats. it can contain data from webserver logs, application databases, and third-party data. data can flow into the data lake by either batch or real-time processing of streaming data. the reason to build a data lake is to provide with the ability to query all data sources combined in a single place, providing “data and analytics as a service” for business users such as data scientists and business analysts. the data lake offers an approach where compute and storage can be separated, in our case, s3 is used as the object storage, and any processing engines (spark, presto, etc) can be used for the compute. this means we can scale separately depending on the business needs. as i explained in one of my previous posts, we decided to build our data lake based on amazon simple storage service (s3) to combine private and third party data in order to enable users to answer their most interesting business questions. we use aws kinesis firehose to push data into s3, aws lambda functions for some pre-processing and spark for our data pipelines. in order to query s3 we needed a querying engine that allowed us to use sql to provide access to tbs of structured and semi-structured data for quick analysis. we evaluated presto, athena and spark sql. out data is arranged in s3 in the following format (hadoop like partitions): <bucket_name>\processed\<event_type>\dt=<date>\hh=<time>\*.json in presto and athena (managed version of presto by aws) we encountered errors either when we created the tables with hive or when querying these tables. we found that both presto and athena do not “like” data types that change over time (for example: timestamp was string and then bigint) or any special characters at the beginning of the field names (_data_type). we corrected the data, using spark and the pre-processing lambdas, so we thought this will suffice, but this was not enough, we had very bad performance when querying small json files. we knew that parquet will perform much better, but the partitions dictated smaller files. so what did we do? we thought to try spark sql as our query engine. spark sql provides the capability to expose the spark datasets over jdbc api and allow running the sql like queries on spark data using traditional bi and visualization tools. on top of this, our analysts and data scientist needed strong visualization tools to find data patterns and correlations. apache zeppelin was our choice. zeppelin is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with sql, scala, python and more. it provides analysts a great way to create interactive web notebooks to write the queries and visualize the results. the notebooks can be shared in real time. the queries can be scheduled. tutorial 1- create a cluster in aws emr with spark and zeppelin. 2- click on the link “zeppelin” after the cluster was provisioned to access zeppelin ui. 3- download sample data for the san francisco traffic accidents data. (it can be found at: https://data.sfgov.org/api/views/vv57-2fgy/rows.csv?accesstype=download ). 4- place it in an s3 bucket , for example: “test-zeppelin-ni”. now we can access and query the data using spark sql and zeppelin. we will write some scala code inside zeppelin to visualize this csv file and extract information contained on it. in order to view the contents of this file and manipulate the data, we will create a dataframe, bind it to a schema, create a temporary table and then use sql for queries. “a powerful feature of spark sql is that you can programmatically bind a schema to a data source and map it into scala case classes which can be navigated and queried in a typesafe manner.” import org.apache.spark.sql.types.{structtype, structfield, stringtype, integertype}; import org.apache.spark.sql.{dataframe, dataset, encoders, sparksession} import org.joda.time.datetime case class trafficdata(incidntnum: string, category: string, descript: string, dayofweek: string, date: string, time: string, pddistrict: string,resolution:string, address:string, x:double, y:double, location:string, pdid:string) val schema: structtype = encoders.product[trafficdata].schema val df = sqlcontext.read.schema(schema).csv(“s3a://test-zeppelin-ni/*”) df.printschema() here is the second paragraph: df.registertemptable(“traffictable”) here is the third paragraph: %sql select * from traffictable as you can see, after loading the data into spark , we created a temp table and then we can query it using regular sql. wrapping it up i hope this article gave you a good starting point to learn about how to use spark and zeppelin. in this article, we have learned how to read a large dataset from a s3 public bucket and perform sql queries on it. this combination is one of the most common used setups for machine learning projects running data at scale. note: remember that the changes you make on zeppelin only persist as long as the emr cluster is running. if you want to know more about zeppelin , watch this video by moon soo lee, who created zeppelin, speaking in amsterdam at spark summit europe 2015. big data apache zeppelin spark machine learning -- -- 4 written by denise schlesinger 244 followers · 171 following software architecture and tech enthusiast. principal cloud solution architect @microsoft responses ( 4 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@jeremytarling/apache-spark-and-hadoop-on-a-macbook-air-running-osx-sierra-66bfbdb0b6f7,,,"apache spark and hadoop on a macbook air running osx sierra | by jeremy tarling | medium sitemap open in app sign up sign in medium logo write sign up sign in apache spark and hadoop on a macbook air running osx sierra jeremy tarling 5 min read · sep 18, 2017 -- 8 listen share recently i’ve been working to support two data scientists in our team and i wanted to get a reproducible setup of apache spark and hadoop on a local machine. it proved to be surprisingly challenging to do — perhaps because of our requirements to be able to read/write in various data serialisations to aws redshift and s3. this post is a summary of the steps i went through and the versions of the software, libraries and configuration tweaks that i used to get it all working. tl;dr macbook air running osx sierra 10.12.6 with these software versions: java 1.8 scala 2.11.8 hadoop 2.7.4 spark 2.1.0 and these versions of these jar files in $spark_home/jars/ to enable reading and writing to aws data sources: aws-java-sdk-1.7.4.jar aws-java-sdk-s3–1.11.60.jar hadoop-aws-2.7.3.jar redshiftjdbc41–1.2.7.1003.jar spark-avro_2.11–3.2.0.jar spark-redshift_2.11–3.0.0-preview1.jar minimal-json-0.9.4.jar a word on homebrew if all you want to do is check spark out then a quick ‘brew install apache-spark’ will do you just fine on a local machine (or if you need scale spin up an emr-spark cluster on aws). the reason i ended up going through the manual steps below are to do with our use cases for spark — specifically reading from and writing to aws redshift via aws s3 buckets. some of the third-party libraries that this use case requires seem to need specific builds of the software to play nicely together. check java and scala are installed if they’re not already installed you’ll need java 1.8 and scala — i use scala 2.11.8 (backwards compatibility between scala versions seems to be a thing so i arrived at 2.11.x after some trial and error with 2.10 and 2.12) open a terminal to check: $ java -version $ scala -version you’ll want to set some environment variables so spark and hadoop can find java and scala — here’s my ~/.bash_profile: export java_home=”/library/java/javavirtualmachines/ jdk1.8.0_144.jdk/contents/home” export scala_home=”/usr/local/scala” (and then remember to source your .bash_profile after adding these) enable sshd hadoop needs ssh to be enabled and it’s disabled by default in osx sierra. to fix this open system preferences and choose ‘sharing’ then click on ‘remote login’ — this immediately starts the ssh daemon if you need to then generate a set of ssh keys $ ssh-keygen -t rsa -p “” $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys and then test you can connect to localhost $ ssh localhost install and configure hadoop download hadoop 2.7.4 and unpack it your preferred software location — i use /usr/local in line with homebrew. as with java and scala you’ll want to set a couple of environment variables to reflect this location: export hadoop_home=”/usr/local/hadoop” export hadoop_conf_dir=”/usr/local/hadoop/etc/hadoop” now we need to edit four config files in the $hadoop_conf_dir directory, first up is $hadoop_conf_dir/core-site.xml: <configuration> <property> <name>fs.defaultfs</name> <value> hdfs://localhost /</value> </property> </configuration> for some reason localhost isn’t mapped in the /etc/hosts file in osx sierra so i had to manually add it for hadoop to work, here’s mine: 127.0.0.1 localhost jeremys-air.home 255.255.255.255 broadcasthost ::1 localhost jeremys-air.home (also, while the docs recommend a specifying the port number in core-site.xml (as in hdfs://localhost:9000) i found this only worked if i didn’t specify the port — ymmv) next up we need to edit $hadoop_conf_dir/hdfs-site.xml to specify a single node cluster: <configuration> <property> <name>dfs.replication</name> <value>1</value> </property> </configuration> the third file to edit is $hadoop_conf_dir/mapred-site.xml for our cluster manager (yarn == yet another resource negotiator): <configuration> <property> <name>mapreduce.framework.name</name> <value>yarn</value> </property> </configuration> and finally $hadoop_conf_dir/yarn-site.xml: <configuration> <property> <name>yarn.nodemanager.aux-services</name> <value>mapreduce_shuffle</value> </property> </configuration> start hadoop and yarn format your hdfs directory: $ cd $hadoop_home $ bin/hdfs namenode -format you’ll see a bunch of output whizz past in the console, then all being well you should be able to start hadoop and yarn services: $ $hadoop_home/sbin/start-dfs.sh $ $hadoop_home/sbin/start-yarn.sh depending upon your user privileges you might be asked to enter your password a few times at this point. you can check that the services are up with the jps command. finally we need to create folders in hdfs for mapreduce: $ $hadoop_home/bin/hdfs dfs -mkdir /user $ $hadoop_home/bin/hdfs dfs -mkdir /user/jeremytarling (obviously use your own username here) at this point you should be able to go to http://localhost:50070/ to see your hadoop web interface hadoop web client on localhost:50070 install spark download spark 2.1.0 from http://spark.apache.org/downloads.html (from the dropdown list choose 2.1.0 and prebuilt for apache hadoop 2.7 and later). unpack the file and copy it to your preferred locaiton — i use /usr/local/spark. add this location to your .bash_profile as a $spark_home variable: export java_home=”/library/java/javavirtualmachines/ jdk1.8.0_144.jdk/contents/home” export scala_home=”/usr/local/scala” export spark_home=”/usr/local/spark” while you’re there you might want to add the binary subdirectories to your path so you can run them from wherever: export path=”$java_home/bin:$scala_home/bin:$spark_home:$spark_home/bin:$spark_home/sbin:$path” at this point you should be able to run spark-shell or pyspark: and then take a look at the web interface for running jobs on http://localhost:4040/ install additional jar files download these versions of the following jar files and copy them in to $spark_home/jars/ to enable reading and writing to aws data sources like redshift and s3: aws-java-sdk-1.7.4.jar aws-java-sdk-s3–1.11.60.jar hadoop-aws-2.7.3.jar redshiftjdbc41–1.2.7.1003.jar spark-avro_2.11–3.2.0.jar spark-redshift_2.11–3.0.0-preview1.jar minimal-json-0.9.4.jar set aws credentials if required to enable authentication with s3 i added my aws keys to $hadoop_conf_dir/core-site.xml: <configuration> <property> <name>fs.defaultfs</name <value> hdfs://localhost /</value> </property> <property> <name> fs.s3a.awsaccesskeyid </name <value>{your aws access key}</value> </property> <property> <name> fs.s3a.awssecretaccesskey </name <value>{your aws secret}</value> </property> </configuration> (if you still get authentication errors when trying to write to s3 then another option is to add these as environment variables in your .bash_profile) give it a spin that’s about it for this guide. to finish off here are a couple of examples in pyspark of reading from and writing to aws redshift using an aws s3 tempdir: # declare an sql context sqlcontext = sqlcontext(sc) # test read data from redshift df = sqlcontext.read \ .format(""com.databricks.spark.redshift"") \ .option(""url"", ""jdbc:redshift://{your jdbc connection string}"") \ .option(""dbtable"", ""your table"") \ .option('forward_spark_s3_credentials',true) \ .option(""tempdir"", ""s3a://{your s3 bucket}"") \ .load() # output table to shell to check the read succeeded df.show() # test write the data to redshift df.write \ .format(""com.databricks.spark.redshift"") \ .option(""url"", ""jdbc:redshift://{your jdbc connection string}"") \ .option('forward_spark_s3_credentials',true) \ .option(""tempdir"", ""s3a://{your s3 bucket}"") \ .option(""dbtable"", ""your table"") \ .mode(""error"") \ .save() spark hadoop osx data science -- -- 8 written by jeremy tarling 449 followers · 418 following data architect, bbc responses ( 8 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@viirya/custom-jdbc-dialect-for-hive-5dbb694cc2bd,,,"custom jdbc dialect for hive. recently i’ve answered a jira ticket… | by liang-chi hsieh | medium sitemap open in app sign up sign in medium logo write sign up sign in custom jdbc dialect for hive liang-chi hsieh 2 min read · sep 27, 2017 -- 2 listen share recently i’ve answered a jira ticket spark-22113 . it is reported an issue when using jdbc to connect hive in sparksql. usually we will use a session with hive support to connect with hive: val session = sparksession.builder().enablehivesupport().getorcreate() session.sql(""select dw_date from tfdw.dwd_dim_date limit 10"").show() with jdbc, in theory you are able to connect hive as like: val ds = session.read() .format(""jdbc"") .option(""driver"", ""org.apache.hive.jdbc.hivedriver"") .option(""url"", ""jdbc:hive2://iz11syxr6afz:21050/;auth=nosasl"") .option(""dbtable"", ""tfdw.dwd_dim_date"") .load() ds.select(""dw_date"").limit(10).show() however, the returned results are not correct and looks like you select a string literal. after few tests, i found that default jdbc dialect in sparksql doesn’t work with hive at all. the default way in the default dialect to quote identifiers are using double quotes. a sql query like select “dw_date” from table… will be parsed by hive to select a string literal, instead of a column named “ dw_date ”. by replacing quotes with backticks, seems the issue is resolved. however, in my test, the column names get from hive are all prefixed with the table name like table.dw_date. but you can’t directly wrap backticks around it like `table.dw_date`. alternatively, we need to wrap each part individually: private case object hivedialect extends jdbcdialect { override def canhandle(url : string): boolean = url.startswith(""jdbc:hive2"") override def quoteidentifier(colname: string): string = { colname.split(‘.’).map(part => s”`$part`”).mkstring(“.”) } } after registering this hive jdbc dialect with sparksql, you can now connect with hive via jdbc. because jdbc seems not a recommended way to use hive under sparksql, currently this patch isn’t submitted as a pr. but this can be a reference for adding custom jdbc dialect into sparksql. big data hive spark sparksql jdbc -- -- 2 written by liang-chi hsieh 38 followers · 9 following spark, big-data, machine learning, deep learning responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/nextdoor-engineering/unit-testing-apache-spark-with-py-test-3b8970dc013b,,Unit Testing,"unit testing apache spark with py.test | by vikas kawadia | nextdoor engineering sitemap open in app sign up sign in medium logo write sign up sign in nextdoor engineering · follow publication nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. we believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. follow publication unit testing apache spark with py.test vikas kawadia 4 min read · mar 16, 2016 -- 6 listen share nextdoor uses apache spark (mostly pyspark) in production to process and learn from voluminous event data. spark’s api (especially the dataframes and datasets api ) enable writing very concise code, so concise that it may be tempting to skip unit tests (its only three lines, what can go wrong). but its wise to resist the temptation . all production code needs tests. back to testing. spark supports a local mode that makes it easy to unit tests. local mode creates a cluster on your box . in this post, i’ll show how to write unit tests using my favorite test framework for python code: py.test . writing tests in py.test is less verbose, provides great support for fixtures (including reusable fixtures , parametrization in fixtures), has great debugging support with contexts, makes parallel/distributed running of tests easy, and has well though out command-line options options. elaborating on these is perhaps another blog post (see this reddit for more details). i am also not going to talk much about spark itself. for that, i recommend the excellent material at databricks . today the focus is writing unit tests for pyspark jobs. we’ll go through a few examples, the code for all of which is available here: kawadia/pyspark.test pyspark.test - example unit tests for apache spark python scripts using the py.test framework github.com unit test for a simple word counting function lets start by writing a test for the following simple word counting function: to test this function, we need a sparkcontext fixture. a test fixture is a fixed state of a set of objects that can be used as a consistent baseline for running tests. we’ll create a local mode sparkcontext and decorate it with a py.test fixture: creating a spark context (even in local mode) takes time, so we want to reuse it. the scope=session argument does exactly that: allows reusing the context for all tests in the session. one can also set the scope=module to get a fresh context for tests in a module. now the sparkcontext can be used to write a unit test for word count: note that we made the local sparkcontext available to our test function by the following pytest magic: pytestmark = pytest.mark.usefixtures(“spark_context”). one can also decorate the test function with @pytest.mark.usefixtures(“spark_context”) to achieve the same effect. also note that the do_word_counts function takes an rdd as an input which we can create with sparkcontext’s handy parallelize method. that completes our first unit test. unit test for a function using the dataframe api here is another example for testing a function using the dataframes api which is also used by spark sql. this function counts the occurrences of a name in a dataframe (read from a bunch of json). its even more important to test functions that consume semi-structured data. it is really easy to misspell the name of a field in a json blob or forget that a field is actually optional. in the best case, this blows up in production. worst case your pipelines silently do nothing. the messier the data you can run through your tests, the happier your oncall engineers will sleep. to test our json counts function, we need to create a hivecontext test fixture so that we can read in json using its nifty jsonrdd function. the rest of the function is similar to the word counts test. a spark streaming unit test example this final example tests a spark streaming application. spark streaming’s key abstraction is a discretized stream or a dstream, which is basically a sequence of rdds. here is a streaming version of our word counting example that operates on a dstream and returns a stream of counts. you guessed it, we need a streamingcontext test fixture . we also need to create a dstream with test data. streamingcontext has a really nifty queuestream function for just that. here is the complete unit test for streaming word count: there is a lot of code here and a bit scattered. its easier to get it from the pyspark.test git repo. you are also welcome to fork the repo and contribute more examples. you can also write spark unit tests using the unittest2 framework as the spark-testing-base package does, but imho using py.test is a lot more concise, cleaner, and more powerful. nextdoor is a really fun place to work and a high impact product. we are connecting neighborhoods in unprecedented ways while solving really hard and interesting problems. if you are passionate about using big data, machine learning and the like to make a meaningful difference, we are always hiring . careers at nextdoor at nextdoor, we have an enormous opportunity: restoring communication, connectedness, and community to neighborhoods… nextdoor.com apache spark big data python unit testing open source -- -- 6 follow published in nextdoor engineering 1.2k followers · last published mar 19, 2025 nextdoor is the neighborhood hub for trusted connections and the exchange of helpful information, goods, and services. we believe that by bringing neighbors together, we can cultivate a kinder world where everyone has a neighborhood they can rely on. follow written by vikas kawadia 70 followers · 113 following engineering lead at @nextdoor responses ( 6 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/p/8a400aa6884b,,,"4 most used languages in big data projects: python | by kiarash irandoust | itnext sitemap open in app sign up sign in medium logo write sign up sign in itnext · follow publication itnext is a platform for it developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies. follow publication 4 most used languages in big data projects: python kiarash irandoust 4 min read · jul 11, 2016 -- listen share this article is the second part in a series about programming languages for big data projects. other articles that were published in this series can be found here: #1: 4 most used languages in big data projects: java #3: 4 most used languages in big data projects: r #4: 4 most used languages in big data projects: scala java , python , r , and scala are commonly used in big data projects. in a series of articles, i am describing these languages briefly and the reasons for their popularity among data scientists. java was described in the previous article . this article focuses on python and provides an overview of this language and why it is common for big data projects. python python is an open source, interpreted, high-level programming language with dynamic semantics. despite being well-known as an object oriented language, it also supports functional , imperative, and procedural programming paradigms. the combination of built-in high-level data structures, dynamic typing and dynamic binding makes python programs 3 to 5 times shorter than equivalent java programs. python’s popularity is mainly due to its clear syntax and readability, indention of source statements makes the code easier to read. python is platform-independent. this platform independency is in the implementation of the interpreter. “ implementation ” of python means a program which provides support for the execution of programs written in the python language. to note, each implementation can take advantage of variant related libraries and modules, while the language syntax remains consistent in each instances. the main implementations of python are as following: cpython — the reference implementation of python, written in c. it is often referred to as simply “python” jython — a python implementation written in java that utilizes the jvm ironpython — another python implementation, written in c# for the .net framework. it runs on the .net virtual machine, microsoft’s common language runtime (clr) , and can use both python and .net framework libraries pypy — a python implementation written in rpython , a statically-typed subset of the python language. it features a just-in-time compiler and supports multiple back-ends (c, cli, jvm). python which comes with an extensive standard library , is very popular in natural language processing (nlp). it has excellent functionality for processing linguistic data. python’s strength lies in several libraries which it contains for nlp. few of these libraries for text processing include: nltk — natural language toolkit nltk which is a suite of program modules and data sets for nlp, defines a framework for building nlp programs in python. it includes interfaces to more than 50 corpora and lexical resources along with a set of text processing libraries. gensim an open source library for topic modelling, document indexing and similarity retrieval with large corpora. gensim which is designed to analyze unstructured digital texts is based on the concepts of corpus , vector and model . it depends on numpy and scipy , two python packages, for performance. textblob — simplified text processing a library for processing textual data which provides an api for noun phrase extraction, sentiment analysis, classification, translation, tagging and more. orange a component-based data mining software which is written in python. its visual programming front-end includes a range of data visualization, exploration, preprocessing and modeling techniques that can be used as a python library. pattern a web mining module for python that contains tools for scraping, natural language processing, machine learning, network analysis and visualization. pynlpl a library that contains modules for various nlp tasks including the extraction of n-grams and frequency lists, and to build simple language model. spacy a library for advanced natural language processing in python and cython. python also contains ample of useful libraries for data analytics, including: pandas (for data analysis and data manipulation), statsmodels (for data exploration, statistical models estimation, and performing statistical tests), matplotlib (for embedding plots into applications using general-purpose gui toolkits such as qt and gtk+), scikit-learn (for data mining and data analysis), mlpy (provides a wide range of machine learning methods for supervised and unsupervised problem), numpy (provides fast precompiled functions for numerical routines), scipy (for scientific computing), and theano (for defining, optimizing, and evaluating mathematical expressions involving multi-dimensional arrays). in brief, advantages and disadvantages of the python are as following: advantages easy to learn, read and maintain — clearly defined syntax, easy to understand, and the source code is easy to maintain. extensive standard library and broad modules and libraries multi-paradigm — object oriented, imperative, procedural and some functional features, such as lambda and list comprehensions efficient high-level data structures and support for dynamic type checking portable — running on a wide range of hardware platforms high-level language interpreted extendable — possibility to add a piece of code written in languages like c or c++ to python embeddable — python code can be embedded in programs written in other languages such as c, c++, and java native support for the essential data structures support for gui applications juypter — a web application for creating and sharing documents that contain code (such as python )and rich text elements (visualizations, equations, explanatory text, links, etc…) in a shareable notebook format disadvantages indentation in python can be a pro or a con; while many hate it, many more like it. regardless of the indentation, the main disadvantage of python is speed; python is an interpreted language which makes it slower than compiled languages. furthermore, python is used for developing very few smartphone applications and almost is unavailable in web browsers. -- -- follow published in itnext 74k followers · last published 16 hours ago itnext is a platform for it developers & software engineers to share knowledge, connect, collaborate, learn and experience next-gen technologies. follow written by kiarash irandoust 1.2k followers · 3.5k following https://itnext.io/ common commuter between the realms of sanity and insanity, reasoning and emotions, reality and fantasy. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://mrpowers.medium.com/working-with-s3-and-spark-locally-1374bb0a354,,,"working with s3 and spark locally | by matthew powers | medium sitemap open in app sign up sign in medium logo write sign up sign in working with s3 and spark locally matthew powers 2 min read · apr 9, 2016 -- 7 listen share spark is used for big data analysis and developers normally need to spin up multiple machines with a company like databricks for production computations. it is nice to work with spark locally when doing exploratory work or when working with a small data set. if spark is configured properly, you can work directly with files in s3 without downloading them. configuring the spark shell start the spark shell with the dataframes spark-csv package. $ ./bin/spark-shell --packages com.databricks:spark-csv_2.11:1.3.0 spark give the following information message while the shell is loaded: “spark context available as sc.” the sc variable points to the spark context: scala> sc res0: org.apache.spark.sparkcontext = org.apache.spark.sparkcontext set the aws_access_key_id and aws_secret_access_key environment variables, so spark can communicate with s3. add the aws keys to the~/.bash_profile file: export aws_access_key_id=redacted export aws_secret_access_key=redacted once the environment variables are set, restart the spark shell and enter the following commands. the system.getenv() method is used to retreive environment variable values. reading data from s3 into a dataframe this example will use three files that are stored in the s3://some_bucket/data/states/ folder. this is the content of the s3://some_bucket/data/states/texas.csv file. city,state austin,tx dallas,tx houston,tx this is the content of the s3://some_bucket/data/states/new_york.csv file. city,state new york,ny buffalo,ny albany,ny this is the content of the s3://some_bucket/data/states/california.csv file. city,state los angeles,ca san francisco,ca sacramento,ca the following code can be used to load these three csv files into a single dataframe: notice that the load() method includes a path with s3n, not s3. this stackoverflow answer discusses the difference between s3n and s3. the show() method can be used to demostrate that the data in the three csv files is loaded into the dataframe: scala> df.show() + — — — — — — -+ — — -+ |          city| state| + — — — — — — -+ — — -+ |   los angeles|    ca| | san francisco|    ca| |    sacramento|    ca| |      new york|    ny| |       buffalo|    ny| |        albany|    ny| |        austin|    tx| |        dallas|    tx| |       houston|    tx| + — — — — — — -+ — — -+ writing a dataframe to a s3 folder the dataframe results can be written to a folder in s3. the s3://some_bucket/data/states/all_states/part-00000 file will contains all of the rows in the dataframe. rdd spark amazon s3 -- -- 7 written by matthew powers 2.7k followers · 49 following spark coder, live in colombia / brazil / us, love scala / python / ruby, working on empowering latinos and latinas in tech responses ( 7 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@raviranjan_iitd/running-a-scala-code-as-a-spark-submit-job-using-sbt-e03ba05b941f,,,"running a scala code as a spark-submit job using sbt | by ravi ranjan | medium sitemap open in app sign up sign in medium logo write sign up sign in running a scala code as a spark-submit job using sbt ravi ranjan 2 min read · nov 23, 2016 -- 1 listen share i had a very frustrating few weeks getting few stuff done. one of them was running a scala code through spark-submit.i almost hit my laptop screen once. few more blogs will follow on other stuff. i had difficult time figuring out solution to these, hence i thought a blog on the same would help others working on similar projects.here we go. download sbt. here is the link . create the following directory structure. refer to section 18.1 and 18.2 of this link . i’ll explain. do the following: a. mkdir projects b. cd projects c. mkdir projectname d. cd projectname e. mkdir -p src/{main,test}/{java,resources,scala} f. mkdir lib project target g. echo ‘name := “myproject” version := “1.0” scalaversion := “2.10.0”’ > build.sbt h. cd src/main/scala i. vim projectfile.scala you will add all your dependencies in this file(built.sbt). now, let the test code be something like this: package foo.bar.baz object main extends app { println(“hello, world”) } j.then go to projectname directory: cd /project/projectname k. then do: sbt package l. go to spark/bin and type the following: ./spark-submit --class foo.bar.baz.main /project/projectname/target/scala-version/.jar file the above code is the following: ./spark-submit is followed by two hyhens and “class” (i.e. “ — class”) which is followed by package name added with class name which is “foo.bar.baz.main” in this case which is then followed by location of the jar file which is created in project/projectname/target/scala-version directory. this blog will help you get started with using sbt to run a scala code through spark-submit. with spark-submit, you can vary number of nodes and cores working on the code to vary speed/performance. for more on spark-submit, see this link . edits: i would suggest you to use spark-assembly as then you would not need to mention class name or the different jar name separately. you would just mention the jar created by sbt-assembly and that’s it. see this link for better info. scala sbt apache spark big data -- -- 1 written by ravi ranjan 511 followers · 877 following read, implement, write, repeat!!! responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/datamindedbe/our-three-favourite-data-analytics-tools-of-2015-844db39f7eff?responsesOpen=true,,,"our three favourite data analytics tools of 2015 | by kris peeters | dataminded | medium sitemap open in app sign up sign in medium logo write sign up sign in dataminded · getting data done our three favourite data analytics tools of 2015 kris peeters 8 min read · jan 4, 2016 -- listen share in this blog, we look back at the 3 data analytics tools that shaped 2015 for us. this is by no means an objective blog, and i’m sure your list looks completely different. looking forward to hear your feedback. spark cliché, i know. i’m not even going to bother posting a link. spark is not exactly a new framework, with an i nitial commit from more than 5 years ago. yet, for us, it was the first time we supported a client with bringing it to production. in true blitzkrieg-style, spark dominated the market in no time. today, it seems like everybody, and their grandmother, is running spark in some way or form. there are tons of blog posts on what spark is and what it does. here i’d like to focus on why specifically it has been so useful for us. in one sentence: easy to develop, test and scale data pipelines. etl has become quite complex. and simple visual etl tools often don’t offer the power and flexibility you need. we’ve witnessed the weirdest solutions to that problem. we’ve seen sql stored procedures which generate sql which execute an etl job. we’ve also dealt with c# code which generate ssis packages which generate sql which execute an etl job. we’ve even seen matlab (!) scripts which call a java api and some python scripts to execute an etl job. we’ve also seen clients squeezing visual etl tools to their extremes. i can write entire blog posts about the challenges those solutions bring. madness. this current client was using pig before we helped them migrate to spark. don’t get me wrong, i like pig. it’s a much better fit for etl than sql. but, like sql, pig has its limits. they had folders full of pig scripts, which were glued together by oozie scripts, had their udfs in java, their script parameters updated by a shell script and stored in a mysql database. and all of this was triggered by an external scheduler. needless to say, this setup was very brittle. debugging meant walking through tons of logfiles to try to figure out what happened. and bugs were very hard to reproduce. every morning was a new surprise. moving to spark made our work an order of magnitude easier. “yes, but our devs don’t know scala”. really, that’s no excuse. your devs are still solving the same business problems, and still building very similar data pipelines. if they have the right mindset, they can learn the basics of scala needed to build spark applications. it helps if you have at least one scala enthousiast in the team, who can do the needed evangelisation. for us, that was mathias lavaert . @mathias, thanks again for teaching the rest of us scala! spark on its own doesn’t make anything easier. you have to use it well. we wanted to make sure everybody could develop and test features on their own machines. we had unit tests, integration tests and end-to-end tests. we worked with pull requests and code reviews. we had continuous integration and nightly builds. while this is a given in any professional dev environment, it definitely wasn’t in a bi-inspired big data department. using spark also meant that all business logic was built in one place, not spread out over 4 different tools. this brought much easier debugging and reproducibility of bugs. scaling spark was relatively easy as well. in most cases, if a job worked on your laptop on limited data, it would also work in production on billions of records. sure, we had our own scalability and out-of-memory issues. spark is a mighty machine, and you need to spend time tuning it. that’s one of its downsides. compare this to my earlier experiences with hp vertica, where the default parameters almost always resulted in best performance. but hey, vertica is not really taking over any market any time soon. also, with spark, you get a free lunch every now and then. upgrade from 1.2 to 1.3? good for you, here are some performance improvements. upgrade from 1.3 to 1.5? great, here are some more. airflow this tool has not received enough love. airflow is a workflow manager and scheduler from airbnb. and before you decide to skip ahead to the next section, because “omg… boooooring”, please, bear with me. workflow management is something that often pops up as an afterthought: “oh yes, we need to run this job after that job, unless that other job gives an error”. and very often, workflow management starts with a shell script, and then becomes a collection of python scripts backed by 2 little tables in sqlite. if a traditional workflow manager, like oozie, is being involved at all, it’s often misused, and quickly hacked together. the absolute worst are the enterprise workflow management tools, shielded away by a crappy excel sheet managed through sharepoint. with an sla to respond of 14 days. good luck defining your data pipeline! and yes, sadly, we’ve been there. workflow management is being neglected so much because business doesn’t see this as “a feature”. so whatever you do, make it quick. and developers don’t care either. “whatever, let me go back writing real code.”this often results in annoying crashes and bugs, and wrong outputs being generated in production. also, nobody takes responsibility because their respective components did the jobs they were supposed to do. it’s a shame. in our experience, good workflow management can make or break your data pipeline. airflow is an order of magnitude better than all of the above tools for several reasons. firstly, it deeply integrates the notion of time. you have daily and hourly batch jobs. and you want to have one overview of which jobs have run for which time slots, so you know which data has been processed correctly. secondly, you can define your dependencies through python code. this saves your eyes from some ugly, massive xml, and at the same this opens the door for dynamic direct acyclic graphs (dags) which are very convenient. last but not least, airflow offers a nice web ui for operators and support staff to keep an eye on all your production workflows. airflow is being used actively by at least two of our clients, where it replaced their homegrown shell and python scripts. so far, results have been very positive. unfortunately, we couldn’t convince one other client to even consider an existing open-source workflow manager, let alone something as hip as airflow, so we had to write our own. i hate reinventing the wheel and i was concerned we would build a crappy, unusable tool. because, obviously, we were given very little time to do it. luckily joris mees delivered an excellent job there, building a stable and easy-to-use workflow manager in record time. @joris, thanks for saving us there! infrastructure-as-code not strictly speaking a tool. it’s a concept. or maybe a collection of tools: ansible, docker, aws. choose your poison (chef, azure, vagrant, kubernetes, …). pascal knapen is our cloud specialist and while he was guiding me through a solution that he was building for a client, he casually mentioned this ‘ansible’ folder in his git repo. “wait, what does it do?” “well, it spins up my aws ec2 instances, defines vpcs, sets up a secure connection with s3, configures the firewall, uses ecs and autoscaling groups to run docker containers which are automatically deployed from github, sets up a vpn with the client intranet, …” i had to sit down there for a minute. i’ve heard of and/or worked with most of these components individually. but never before did i see all these things come together. and never before did i see so much functionality in a single little git folder called ‘ansible’. it was an awakening for me. infrastructure-as-code allows you to be agile. never before have we been able to put so much power and control in the hands of the developer. with a single push of the button, a developer can set up an entire infrastructure and do a deployment of their product. you want to do several releases per day? no problem. this is tightly coupled with the cloud. sure, you don’t need the cloud. you can do parts of it on an on-premise data center. sometimes, they even allow you to automatically provision a vm. yet, often it’s a job protection and a cultural issue. “we don’t let developers define their own systems. what do they know? and besides, that’s our job.” in the cloud, you can avoid these non-productive discussions. and of course, have true on-demand flexibility and scalability. of course, with great power comes great responsibility. good coding hygiene and a healthy dev environment are key to success. additionally, we’ve seen larger clients having a dedicated cloud team who define a perimeter, to limit security exploits caused by unexperienced developers. another client of ours detects problems as early as possible by spinning up an hadoop cluster on aws and running all their integration and end-to-end tests as part of the nightly build. we ourselves have been seeking advice from external experts when we saw the need for it. in particular, we’ve had very pleasant experiments with jeroen jacobs from head in cloud . @jeroen, thanks for helping us out in the past! looking forward to working with you again in 2016. yet, devops can only be devops, if devs are actually being responsible for their own operational tasks. so, while it is ok to seek expert advice when you need it, it’s important that devs start taking ownership of this aspect of development as well. the more end-to-end your devs can work, the faster you can move as an organisation. a good way to start, is to take this devops course, offered by edward viane , from another cool belgian devops company, in4it . we currently see advanced forms of automation, devops, or infastructure as code at most of our clients. we have done a lot of the work ourselves. besides the efforts of pascal, another cool example is how patrick varilly automated a highly secure deployment of hortonworks hadoop on the aws govcloud for one client. i guess i am the one at data minded lagging behind. bigboards.io is also pushing the boundaries here. they dramatically lower the barrier of entry of big data by easily switching tech stacks on their bigboards hex and start experimenting and learning. this is definitely an aspect of writing code that has truly been “disrupted” by new technology. conclusion we sure do live in interesting times! the world of data analytics is moving fast. most of the tools we use in production today, barely existed a couple of years ago. there is no end in sight yet. and in upcoming blog posts, we will be talking about our outlook on 2016, but also our greatest disappointments of 2015. another nice observation is that there are actually plenty of companies, here in belgium, being well on track with using data analytics and big data to improve their business. i’m not just talking about our own clients and partners, because, yes, of course, they are awesome. no, i’m also talking about the meetups, the events, the incubators, the data4good projects, … we’re no silicon valley yet, but at least we’re heading in the right direction. in the meantime, please do share your own experiences in the comments. we only ever see a thin slice of reality. and i would love to hear your war stories. originally published at https://www.linkedin.com on january 4, 2016. devops cloud big data -- -- published in dataminded 1k followers · last published 14 hours ago getting data done written by kris peeters 1.2k followers · 144 following data geek at heart. founder and ceo of dataminded. no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@josemarcialportilla/installing-scala-and-spark-on-windows-249632e6b83b,,,"installing scala and spark on windows | by jose marcial portilla | medium sitemap open in app sign up sign in medium logo write sign up sign in installing scala and spark on windows jose marcial portilla 3 min read · dec 16, 2016 -- 7 listen share hi! i’m jose portilla and i teach over 200,000 students about programming, data science, and machine learning on udemy! you can check out all my courses here . if you’re interested in learning python for data science and machine learning, check out my course here . (i also teach full stack web development with django! ) quick guide to installing a basic scala and spark set-up on windows. spark is written in scala, which is written in java, which means we need all three of these things to make sure everything works out! here are the general steps (if you’re enrolled in my course: scala and spark for big data and machine learning you can always follow along with the video lecture). step 1: download the latest java development kit that matches your system (32-bit vs 64-bit). you can find the download website from oracle here or just googling “java development kit”. step 2: go to apache.spark.org and download a pre-built version of spark (pre-built for hadoop 2.7 and later) and preferably spark 2.0 or later. step 3: download winutils.exe in order to make sure that hadoop works correctly on your computer for windows. you can find this file as a resource in the video lecture, but you may need to google for another version if you are running an older version of windows. (just google “spark+winutils” and you should see plenty of links with different sources for the download. step 4: go to your downloaded jdk file and run the installation program, just use all the defaults. step 5: extract the downloaded spark-2.0.2-bin-hadoop2.7-tar.gz file. you may need to extract this twice in order to get the full folder to show. step 6: once you have this folder, go to your c drive and create a new folder called spark and copy and paste the contents of the unzipped spark-2.0.2-bin-hadoop2.7-tar.gz file to this new spark folder you just created. step 7: create a new folder under your c drive called winutils . then inside of this folder create a new folder called bin . inside of this bin folder place your downloaded winutils.exe file. now its time to tell your windows machine where to find everything, which means we need to edit our environment variables. step 8: go to control panel > system and security > system > advanced system settings step 9: in the window that pops up, click on the button environment variables step 10: you should see two panels, user variables and system variables, click new… on the user variables to create a new variable name and value combination. you’ll create the following variables (your full paths may differ slightly): variable name: spark_home variable value: c:\spark variable name: java_home variable value: c:\program files\java\jdk1.8.0_101 variable name: hadoop_home variable value: c:\winutils step 11: then under your user variables you should see a variable called path that is already there. select it and click edit step 12: you should see a bunch of environment variables already there, but now we’re going to add our own paths. click on new and enter: %spark_home%\bin then repeat it again and add: %java_home%\bin everything should now be installed. let’s test it. open a command prompt and use cd to change directory to c:\spark and then type: spark-shell then hit enter and you should eventually the spark shell display. you can type :q to exit out of this. hope this was helpful! a quick note: it is common once you edit and do changes in the path that some changes are not recognized. if you follow the steps as shown here and get the ‘not recognized’ error when trying to run spark-shell, just restart windows, that should fix the problem. programming spark -- -- 7 written by jose marcial portilla 5.3k followers · 4 following data scientist and online instructor responses ( 7 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/edureka/mapreduce-tutorial-3d9535ddbe7c,,,"fundamentals of mapreduce with mapreduce example | by shubham sinha | edureka | medium sitemap open in app sign up sign in medium logo write sign up sign in edureka · there are many e-learning platforms on the internet & then there’s us. we are not the biggest, but we are the fastest growing. we have the highest course completion rate in the industry. we provide live, instructor-led online programs in trending tech with 24x7 lifetime support. fundamentals of mapreduce with mapreduce example shubham sinha 10 min read · nov 15, 2016 -- 3 listen share mapreduce tutorial - edureka in this mapreduce tutorial blog, i am going to introduce you to mapreduce, which is one of the core building blocks of processing in the hadoop framework . before moving ahead, i would suggest you get familiar with hdfs concepts which i have covered in my previous hdfs tutorial blog. this will help you to understand the mapreduce concepts quickly and easily. google released a paper on mapreduce technology in december 2004. this became the genesis of the hadoop processing model. so, mapreduce is a programming model that allows us to perform parallel and distributed processing on huge datasets. the topics that i have covered in this mapreduce tutorial blog are as follows: traditional way for parallel and distributed processing what is mapreduce? mapreduce example mapreduce advantages mapreduce program mapreduce program explained traditional way traditional way - mapreduce tutorial let us understand, when the mapreduce framework was not there, how parallel and distributed processing used to happen in a traditional way. so, let us take an example where i have a weather log containing the daily average temperature of the years from 2000 to 2015. here, i want to calculate the day having the highest temperature each year. so, just like in the traditional way, i will split the data into smaller parts or blocks and store them in different machines. then, i will find the highest temperature in each part stored in the corresponding machine. at last, i will combine the results received from each of the machines to have the final output. let us look at the challenges associated with this traditional approach: critical path problem: it is the amount of time taken to finish the job without delaying the next milestone or actual completion date. so, if, any of the machines delay the job, the whole work gets delayed. reliability problem: what if, any of the machines which are working with a part of data fails? the management of this failover becomes a challenge. equal split issue: how will i divide the data into smaller chunks so that each machine gets even part of data to work with. in other words, how to equally divide the data such that no individual machine is overloaded or underutilized. single split may fail: if any of the machines fail to provide the output, i will not be able to calculate the result. so, there should be a mechanism to ensure this fault tolerance capability of the system. aggregation of the result: there should be a mechanism to aggregate the result generated by each of the machines to produce the final output. these are the issues which i will have to take care individually while performing parallel processing of huge datasets when using traditional approaches. to overcome these issues, we have the mapreduce framework which allows us to perform such parallel computations without bothering about the issues like reliability, fault tolerance etc. therefore, mapreduce gives you the flexibility to write code logic without caring about the design issues of the system. what is mapreduce? what is mapreduce - mapreduce tutorial mapreduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment. mapreduce consists of two distinct tasks — map and reduce. as the name mapreduce suggests, reducer phase takes place after the mapper phase has been completed. so, the first is the map job, where a block of data is read and processed to produce key-value pairs as intermediate outputs. the output of a mapper or map job (key-value pairs) is input to the reducer. the reducer receives the key-value pair from multiple map jobs. then, the reducer aggregates those intermediate data tuples (intermediate key-value pair) into a smaller set of tuples or key-value pairs which is the final output. a word count example of mapreduce let us understand, how a mapreduce works by taking an example where i have a text file called example.txt whose contents are as follows: dear, bear, river, car, car, river, deer, car and bear now, suppose, we have to perform a word count on the sample.txt using mapreduce. so, we will be finding unique words and the number of occurrences of those unique words. mapreduce example - mapreduce tutorial first, we divide the input into three splits as shown in the figure. this will distribute the work among all the map nodes. then, we tokenize the words in each of the mappers and give a hardcoded value (1) to each of the tokens or words. the rationale behind giving a hardcoded value equal to 1 is that every word, in itself, will occur once. now, a list of key-value pair will be created where the key is nothing but the individual words and value is one. so, for the first line (dear bear river) we have 3 key-value pairs — dear, 1; bear, 1; river, 1. the mapping process remains the same on all the nodes. after the mapper phase, a partition process takes place where sorting and shuffling happen so that all the tuples with the same key are sent to the corresponding reducer. so, after the sorting and shuffling phase, each reducer will have a unique key and a list of values corresponding to that very key. for example, bear, [1,1]; car, [1,1,1].., etc. now, each reducer counts the values which are present in that list of values. as shown in the figure, reducer gets a list of values which is [1,1] for the key bear. then, it counts the number of ones in the very list and gives the final output as — bear, 2. finally, all the output key/value pairs are then collected and written in the output file. advantages of mapreduce the two biggest advantages of mapreduce are: 1. parallel processing: in mapreduce, we are dividing the job among multiple nodes and each node works with a part of the job simultaneously. so, mapreduce is based on divide and conquer paradigm which helps us to process the data using different machines. as the data is processed by multiple machines instead of a single machine in parallel, the time taken to process the data gets reduced by a tremendous amount as shown in the figure below (2). traditional way vs. mapreduce way - mapreduce tutorial 2. data locality: instead of moving data to the processing unit, we are moving the processing unit to the data in the mapreduce framework. in the traditional system, we used to bring data to the processing unit and process it. but, as the data grew and became very huge, bringing this huge amount of data to the processing unit posed the following issues: moving huge data to processing is costly and deteriorates the network performance. processing takes time as the data is processed by a single unit which becomes the bottleneck. master node can get over-burdened and may fail. now, mapreduce allows us to overcome the above issues by bringing the processing unit to the data. so, as you can see in the above image that the data is distributed among multiple nodes where each node processes the part of the data residing on it. this allows us to have the following advantages: it is very cost effective to move the processing unit to the data. the processing time is reduced as all the nodes are working with their part of the data in parallel. every node gets a part of the data to process and therefore, there is no chance of a node getting overburdened. mapreduce example program before jumping into the details, let us have a glance at a mapreduce example program to have a basic idea about how things work in a mapreduce environment practically. i have taken the same word count example where i have to find out the number of occurrences of each word. and don’t worry guys, if you don’t understand the code when you look at it for the first time, just bear with me while i walk you through each part of the mapreduce code. source code: package co.edureka.mapreduce; import java.io.ioexception; import java.util.stringtokenizer; import org.apache.hadoop.io.intwritable; import org.apache.hadoop.io.longwritable; import org.apache.hadoop.io.text; import org.apache.hadoop.mapreduce.mapper; import org.apache.hadoop.mapreduce.reducer; import org.apache.hadoop.conf.configuration; import org.apache.hadoop.mapreduce.job; import org.apache.hadoop.mapreduce.lib.input.textinputformat; import org.apache.hadoop.mapreduce.lib.output.textoutputformat; import org.apache.hadoop.mapreduce.lib.input.fileinputformat; import org.apache.hadoop.mapreduce.lib.output.fileoutputformat; import org.apache.hadoop.fs.path; public class wordcount { public static class map extends mapper<longwritable,text,text,intwritable> { public void map(longwritable key, text value,context context) throws ioexception,interruptedexception{ string line = value.tostring(); stringtokenizer tokenizer = new stringtokenizer(line); while (tokenizer.hasmoretokens()) { value.set(tokenizer.nexttoken()); context.write(value, new intwritable(1)); } } } public static class reduce extends reducer<text,intwritable,text,intwritable> { public void reduce(text key, iterable<intwritable> values,context context) throws ioexception,interruptedexception { int sum=0; for(intwritable x: values) { sum+=x.get(); } context.write(key, new intwritable(sum)); } } public static void main(string[] args) throws exception { configuration conf= new configuration(); job job = new job(conf,""my word count program""); job.setjarbyclass(wordcount.class); job.setmapperclass(map.class); job.setreducerclass(reduce.class); job.setoutputkeyclass(text.class); job.setoutputvalueclass(intwritable.class); job.setinputformatclass(textinputformat.class); job.setoutputformatclass(textoutputformat.class); path outputpath = new path(args[1]); //configuring the input/output path from the filesystem into the job fileinputformat.addinputpath(job, new path(args[0])); fileoutputformat.setoutputpath(job, new path(args[1])); //deleting the output path automatically from hdfs so that we don't have to delete it explicitly outputpath.getfilesystem(conf).delete(outputpath); //exiting the job only if the flag value becomes false system.exit(job.waitforcompletion(true) ? 0 : 1); } } explanation of mapreduce program the entire mapreduce program can be fundamentally divided into three parts: mapper phase code reducer phase code driver code we will understand the code for each of these three parts sequentially. mapper code: public static class map extends mapper<longwritable,text,text,intwritable> { public void map(longwritable key, text value, context context) throws ioexception,interruptedexception { string line = value.tostring(); stringtokenizer tokenizer = new stringtokenizer(line); while (tokenizer.hasmoretokens()) { value.set(tokenizer.nexttoken()); context.write(value, new intwritable(1)); } we have created a class map that extends the class mapper which is already defined in the mapreduce framework. we define the data types of input and output key/value pair after the class declaration using angle brackets. both the input and output of the mapper is a key/value pair. input: the key is nothing but the offset of each line in the text file: longwritable the value is each individual line (as shown in the figure at the right): text output: the key is the tokenized words: text we have the hardcoded value in our case which is 1: intwritable example — dear 1, bear 1, etc. we have written a java code where we have tokenized each word and assigned them a hardcoded value equal to 1 . reducer code: public static class reduce extends reducer<text,intwritable,text,intwritable> { public void reduce(text key, iterable<intwritable> values,context context) throws ioexception,interruptedexception { int sum=0; for(intwritable x: values) { sum+=x.get(); } context.write(key, new intwritable(sum)); } } we have created a class reduce which extends class reducer like that of mapper. we define the data types of input and output key/value pair after the class declaration using angle brackets as done for mapper. both the input and the output of the reducer is a key-value pair. input: the key nothing but those unique words which have been generated after the sorting and shuffling phase: text the value is a list of integers corresponding to each key: intwritable example — bear, [1, 1], etc. output: the key is all the unique words present in the input text file: text the value is the number of occurrences of each of the unique words: intwritable example — bear, 2; car, 3, etc. we have aggregated the values present in each of the list corresponding to each key and produced the final answer. in general, a single reducer is created for each of the unique words, but, you can specify the number of reducer in mapred-site.xml. driver code: configuration conf= new configuration(); job job = new job(conf,""my word count program""); job.setjarbyclass(wordcount.class); job.setmapperclass(map.class); job.setreducerclass(reduce.class); job.setoutputkeyclass(text.class); job.setoutputvalueclass(intwritable.class); job.setinputformatclass(textinputformat.class); job.setoutputformatclass(textoutputformat.class); path outputpath = new path(args[1]); //configuring the input/output path from the filesystem into the job fileinputformat.addinputpath(job, new path(args[0])); fileoutputformat.setoutputpath(job, new path(args[1])); in the driver class, we set the configuration of our mapreduce job to run in hadoop. we specify the name of the job, the data type of input/output of the mapper and reducer. we also specify the names of the mapper and reducer classes. the path of the input and output folder is also specified. the method setinputformatclass () is used for specifying that how a mapper will read the input data or what will be the unit of work. here, we have chosen textinputformat so that single line is read by the mapper at a time from the input text file. the main () method is the entry point for the driver. in this method, we instantiate a new configuration object for the job. run the mapreduce code: the command for running a mapreduce code is: hadoop jar hadoop-mapreduce-example.jar wordcount /sample/input /sample/output now, you guys have a basic understanding of the mapreduce framework. you would have realized how the mapreduce framework facilitates us to write code to process huge data present in the hdfs. if you wish to check out more articles on the market’s most trending technologies like artificial intelligence, python, ethical hacking, then you can refer to edureka’s official site. do look out for other articles in this series which will explain the various other aspects of big data. 1. hadoop tutorial 2. hive tutorial 3. pig tutorial 4. big data tutorial 5. hbase tutorial 6. hdfs tutorial 7. hadoop 3 8. sqoop tutorial 9. flume tutorial 10. oozie tutorial 11. hadoop ecosystem 12. top hive commands with examples in hql 13. hadoop cluster with amazon emr? 14. big data engineer resume 15. hadoop developer- job trends and salary 16. hadoop interview questions originally published at www.edureka.co on november 15, 2016. big data mapreduce hadoop hdfs hadoop training -- -- 3 published in edureka 5.3k followers · last published jul 30, 2022 there are many e-learning platforms on the internet & then there’s us. we are not the biggest, but we are the fastest growing. we have the highest course completion rate in the industry. we provide live, instructor-led online programs in trending tech with 24x7 lifetime support. written by shubham sinha 182 followers · 1 following big data enthusiast, loves solving real-world big data problems. responses ( 3 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@anicolaspp/apache-spark-as-a-distributed-sql-engine-4373e254e0f9,,,"apache spark as a distributed sql engine | by nicolas a perez | medium sitemap open in app sign up sign in medium logo write sign up sign in apache spark as a distributed sql engine nicolas a perez 6 min read · jan 6, 2016 -- 6 listen share sql have been there for a while and people like it. however, the engines that power sql have changed with time in order to solve new problems and keep up with demands from consumers. traditional engines such as microsoft sql server had some problems with scalability that they have solved with time and cloud based solutions. on the other hand, others have been built from the ground up to work in distributed environment so they can put performance in the top of their priority list. there is not a tool for all use cases, in fact, we believe that tools are built with use cases in mind, to solve a specific problem and then they evolve to a more mature stages where they can be used to solve many other problems. in a traditional sql environment, the data is represented by tables and the relationships between them, but this representation is sometimes not enough so new tools are born to solve this. we can find everywhere organizations that don’t use relational databases; instead, they prefer to go to the non-sql ones. hadoop in the hadoop world, we have find different query engines and each of them has its own particularities, solving a wide variety of problems. in any hadoop distribution we can find apache hive , a sql like tool that offers data warehouse infrastructure and capabilities for big data queries and analysis. depending of the hadoop distribution we can also find apache impala , and apache drill . all of them offer more of less the same capabilities sharing a common goal. we can use sql or sql like languages to query data stored in hadoop. they also have their own limitations and advantages you should be aware of. here is a link with more details about these technologies. apache spark apache spark , a lightning-fast cluster computing that can be deployed in a hadoop cluster or stand alone mode. it can also be used as an sql engine like the others we mentioned. spark, however, offers some advantages over the previous ones that cannot be ignored easily. spark exposes api(s) for different languages such as scala , java , python , and recently added, r . this makes it accessible by many people from different areas of expertise. developers, data scientists, advanced business people with statistic experience (i have seen business guys using r ). interactive algorithms are easily implemented in spark, especially machine learning ones. let’s walk through an example of how to use spark as an sql engine. exploring our data source our data set is a simple folder with few terabytes in csv formatted files and each file is about 40 mb each. the size of the files does not affect our performance because they are stored in a mapr cluster. mapr take cares of the hadoop small file problem as i explain in this post . because we are using mapr, copying files to the cluster is quite easy since we have mounted a volume to our local file system. in order to mount the mapr volume we run this command: sudo mount_nfs -o ""hard,nolock"" 10.21.112.209:/mapr/mapr.domain.com/datalake /users/anicolaspp/mapr/ now, if we run posix commands again our local folder they will in fact be executed in the mapr cluster. preparing the environment for auto schema discovery we are going to create a spark application using scala that will allow us to execute sql statements over our data stored in mapr, the hadoop distribution we are using for this post. in this post i explained how to create an application in spark and the previous steps we need to follow. our app class will look as follow: /** * created by anicolaspp. */ import org.apache.spark import org.apache.spark._ import org.apache.spark.sql.hive.hivecontext import org.apache.spark.sql.hive.thriftserver.hivethriftserver2 import org.apache.spark.sql.{row, sqlcontext} import org.apache.spark.sql.types.{stringtype, structfield, structtype} object app { def main(args: array[string]) { val conf = new sparkconf().setappname(""testing"") val sc = new sparkcontext(conf) val sql = new hivecontext(sc) sql.setconf(""hive.server2.thrift.port"", ""10001"") val delimiter = ""\t"" val data = sc.textfile(""datalake/mytestdatafolder/"") val headers = data.first.split(delimiter) val schema = structtype(headers.map(h => structfield(h, stringtype))) val rowrdd  = data.map(p => row.fromseq(p.split(delimiter))) val dataframe = sql.createdataframe(rowrdd, schema) dataframe.registertemptable(""sometablename"") hivethriftserver2.startwithcontext(sql) while (true) { thread.`yield`() } } } let’s review our code. first, we create the spark context based on a config object. val conf = new sparkconf().setappname(""testing"") val sc = new sparkcontext(conf) val sql = new hivecontext(sc) then, we set the thrift port to avoid conflicts with other components such as hive . sql.setconf(""hive.server2.thrift.port"", ""10001"") now, we set our csv delimiter that in this case is the tab character. we also set the location of our data set by creating a resilient distributed dataset ( rdd ) using the spark context ( sc ) val delimiter = ""\t"" val data = sc.textfile(""datalake/mytestdatafolder/"") at this point, we want to be able to serve our data without worrying about the schema of our file; we want a self service bi environment as i explained here . using the headers from our data files, we can create the schema, automatically, so we don’t have to worry about schema changes in the future. once we have the schema, we create a data frame that we are going to expose to be queried using sql. val headers = data.first.split(delimiter) val schema = structtype(headers.map(h => structfield(h, stringtype))) val rowrdd  = data.map(p => row.fromseq(p.split(delimiter))) val dataframe = sql.createdataframe(rowrdd, schema) the only part missing is the one that register our data set as a table in the hive meta store and we do that by doing: dataframe.registertemptable(""sometablename"") hivethriftserver2.startwithcontext(sql) we have a loop just to keep our app alive. note that rdd transformations are lazy and they will be only executed when a query is submitted for execution. deploying our application we build and test our app using sbt and the resulting .jar can be copied to the cluster in the same way we copy files in our local file system. cp pathtoourjar/app.jar /users/anicolaspp/mapr/testing remember this is possible because we have previously mounted a mapr volume in our local file system. now, we need to submit our application in the cluster and we do that by using the spark-submit command. detailed documentation about submitting spark applications can be found at spark website . in our cluster, we run: /spark-submit --master yarn /mapr/mapr.domain.com/datalake/testing/testing_2.10-1.0.jar our application should start running on yarn as we indicated when submitting it. our sql engine is ready to be queried, so let’s move forward and test it out. sql clients an easy way to test our sql engine is to run beeline , a command line tool that works as an sql client. we can find beeline in the spark bin folder, to start it we do ./beeline. within beeline, we need connect to the end point we have defined in our application so we run: !connect jdbc:hive2://localhost:10001 we should be ready to run sql statements, but let’s verify we can see the table we registered. show tables; spark sql will return a table with the registered tables including the one we registered in our application ( sometablename ). in the same way, we can connect using other clients such as microstrategy or tableau. we have tried both and they both can build and execute queries on tables registered by spark applications. we can also combine different sources (spark sql, ms sql server, hive, impala, etc…) with gives us the flexibility of combining relational sources with non-relational data. spark sql performs quite well and often better than the other providers in hadoop, but be aware that performance can be degraded under certain conditions and use cases. why apache spark certainly, spark sql offers some of the functionalities that other tools have within hadoop. however, the possibility of explore complex data sets is kind of unique to spark since we can code custom serialization / deserialization processes in our application. using spark sql, we can connect to any data source and present it as tables to be consumed by sql clients. this is as easy as changing how we ready the data in those sources by changing our serializer in our application. endings there are very useful tools that we can use within hadoop to query data in an sql fashion and all of them have their advantages. the spark sql module from apache spark offers some flexibility that others lack while keeping performance as one of the main priorities. spark is not the only tool we can use, but we strongly advise to include it in big data solutions where sql statements are to be executed. it might be a mix between different tools, but for sure, spark will be an important part of the system we are building. read next : extending our spark sql query engine how mapr improves our productivity and simplifies our design this blog doesn’t represent the thoughts, plans or strategies of my employer. big data hadoop apache spark -- -- 6 written by nicolas a perez 1.5k followers · 1.1k following computer science. software engineer @google. responses ( 6 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/apache-zeppelin-stories/zeppelin-spark-cassandra-4f07fcaeef1a,,,"zeppelin + spark + cassandra. this is a tutorial explaining how to… | by victor coustenoble | apache zeppelin stories | medium sitemap open in app sign up sign in medium logo write sign up sign in apache zeppelin stories · all things apache zeppelin written or created by the apache zeppelin community — blogs, videos, manuals, etc. let us know if you would like to be added as a writer to this publication. zeppelin + spark + cassandra victor coustenoble 4 min read · jan 25, 2016 -- 1 listen share this is a tutorial explaining how to use apache zeppelin notebook to interact with apache cassandra nosql database through apache spark or directly through cassandra cql language. apache zeppelin apache spark is web-based notebook that enables interactive data analytics. zeppelin interpreter concept allows any language/data-processing-backend to be plugged into zeppelin. currently zeppelin supports many interpreters such as scala(with apache spark), python(with apache spark), sparksql, hive, markdown, cql, and shell. more details and documentation can be found here https://zeppelin.incubator.apache.org/ apache spark apache spark is a fast and general engine for big data processing, with built-in modules for streaming, sql, machine learning and graph processing. more details can be found here http://spark.apache.org . it will be used for cassandra data processing needs (etl, transformations, analytics …). datastax spark cassandra connector datastax have developed a spark cassandra connector to be able to read and write cassandra data from spark api. the spark cassandra connector lets you expose cassandra tables as spark rdds (or dataframes), write spark rdds (or dataframes) to cassandra tables, and execute arbitrary cql queries in your spark applications. useful links: the spark cassandra connector github repository getting started with apache spark and cassandra free training on datastax enterprise analytics with apache spark cql language the cassandra query language (cql) is the primary language for communicating with the cassandra database. documentation on cql usage: introduction to cql using cql the cassandra cql interpreter for apache zeppelin is written by my colleague duy hai doan @doanduyhai cql interpreter documentation for apache zeppelin 0.5.5 installation and setup apache cassandra and apache spark first you need to install a cassandra cluster and a spark cluster connected with the datastax spark cassandra connector. a very simple way to do that is to use datastax enterprise (dse), it’s free for development or test and it contains apache cassandra and apache spark already linked. you can download datastax enterprise from https://academy.datastax.com/downloads and find installation instructions here http://docs.datastax.com/en/getting_started/doc/getting_started/installdse.html . after the installation, start your dse cassandra cluster (it can be a single node) with spark enable with the command line “dse cassandra -k”. 2. apache zeppelin clone zeppelin repository from https://github.com/apache/incubator-zeppelin git clone https://github.com/apache/incubator-zeppelin compile with the cassandra-spark connector. select your version depending of your datastax enterprise (dse) or apache spark version installed. for example for dse 4.8 or spark 1.4 mvn clean package -pcassandra-spark-1.4 -dskiptests more details on zeppelin installation and compilation on github https://github.com/apache/incubator-zeppelin 3. link between zeppelin and spark you have the choice to use spark embedded within zeppelin (automatically installed) or your own deployed spark cluster (with dse or in standalone). for this last option you may need to tune the $zeppelin_home/conf/zeppelin-env.sh file to change the master parameter. by default it is set to spark://127.0.0.1:7077 4. start zeppelin $zeppelin_home\bin\zeppelin-daemon.sh start zeppelin must be available at http://localhost:8080/ 5. add the property spark.cassandra.connection.host with value 127.0.0.1 (or ip of one of your cassandra cluster node) to the spark connector interpreter. 6. download and import in zeppelin the demonstration note found at https://raw.githubusercontent.com/victorcouste/zeppelin-spark-cassandra-demo/master/demo_zeppelin_spark_cassandra.json 7. follow paragraphs of demo_zeppelin_spark_cassandra note and have fun! you will learn how to create a spark rdd or dataframe from a csv file or from a cassandra table, how to transform data with sparksql, how to write back results in a cassandra table, and how to interact with cassandra with cql language (creating keyspaces and tables). at the right of each paragraph, select “show editor” to see source code of paragraphs and “show output” to see output results. 8. notes on paragraphs in the paragraph 3, just after running it, you will have to restart the spark interpreter. then the error message must disappear if you re-run the paragraph 3. in the last paragraph 17, you can finally create a dashboard that can be embedded in a website iframe. click on “link this paragraph” and you will open in a new window the url of the dashboard. this article and ressources are also on github here https://github.com/victorcouste/zeppelin-spark-cassandra-demo cassandra zeppelin spark -- -- 1 published in apache zeppelin stories 518 followers · last published jun 19, 2017 all things apache zeppelin written or created by the apache zeppelin community — blogs, videos, manuals, etc. let us know if you would like to be added as a writer to this publication. written by victor coustenoble 284 followers · 186 following data fan / starburst data responses ( 1 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@priyankajn/jobtracker-and-tasktracker-in-hadoop-d63890005177,,,"jobtracker and tasktracker in hadoop | by priyanka jain | medium sitemap open in app sign up sign in medium logo write sign up sign in priyanka jain 2 min read · feb 12, 2016 -- listen share jobtracker and tasktracker in hadoop jobtracker and tasktracker are 2 essential process involved in mapreduce execution in mrv1 (or hadoop version 1). both processes are now deprecated in mrv2 (or hadoop version 2) and replaced by resource manager, application master and node manager daemons. job tracker - 1. jobtracker process runs on a separate node and not usually on a data node. 2. jobtracker is an essential daemon for mapreduce execution in mrv1. it is replaced by resourcemanager/applicationmaster in mrv2. 3. jobtracker receives the requests for mapreduce execution from the client. 4. jobtracker talks to the namenode to determine the location of the data. 5. jobtracker finds the best tasktracker nodes to execute tasks based on the data locality (proximity of the data) and the available slots to execute a task on a given node. 6. jobtracker monitors the individual tasktrackers and the submits back the overall status of the job back to the client. 7. jobtracker process is critical to the hadoop cluster in terms of mapreduce execution. 8. when the jobtracker is down, hdfs will still be functional but the mapreduce execution can not be started and the existing mapreduce jobs will be halted. tasktracker - 1. tasktracker runs on datanode. mostly on all datanodes. 2. tasktracker is replaced by node manager in mrv2. 3. mapper and reducer tasks are executed on datanodes administered by tasktrackers. 4. tasktrackers will be assigned mapper and reducer tasks to execute by jobtracker. 5. tasktracker will be in constant communication with the jobtracker signalling the progress of the task in execution. 6. tasktracker failure is not considered fatal. when a tasktracker becomes unresponsive, jobtracker will assign the task executed by the tasktracker to another node. olap on hadoop spark programming -- -- written by priyanka jain 57 followers · 7 following designer and developer no responses yet help status about careers press blog privacy rules terms text to speech",12
https://medium.com/netflix-techblog/can-spark-streaming-survive-chaos-monkey-b66482c4924a,,,"can spark streaming survive chaos monkey? | by netflix technology blog | netflix techblog sitemap open in app sign up sign in medium logo write sign up sign in netflix techblog · follow publication learn about netflix’s world class engineering efforts, company culture, product developments and more. follow publication can spark streaming survive chaos monkey? netflix technology blog 4 min read · mar 11, 2015 -- 2 listen share by bharat venkat , prasanna padmanabhan , antony arokiasamy , and raju uppalapati netflix is a data-driven organization that places emphasis on the quality of data collected and processed. in our previous blog post , we highlighted our use cases for real-time stream processing in the context of online recommendations and data monitoring: what’s trending on netflix? improving our recommender systems medium.com with spark streaming as our choice of stream processor, we set out to evaluate and share the resiliency story for spark streaming in the aws cloud environment. a chaos monkey based approach, which randomly terminated instances or processes, was employed to simulate failures. spark on amazon web services (aws) is relevant to us as netflix delivers its service primarily out of the aws cloud. stream processing systems need to be operational 24/7 and be tolerant to failures. instances on aws are ephemeral, which makes it imperative to ensure spark’s resiliency. spark components apache spark is a fast and general-purpose cluster computing system. spark can be deployed on top of mesos, yarn or spark’s own cluster manager, which allocates worker node resources to an application. spark driver connects to the cluster manager and is responsible for converting an application to a directed graph (dag) of individual tasks that get executed within an executor process on the worker nodes. fig 1: different components of spark creating chaos netflix streaming devices periodically send events that capture member activities, which plays a significant role in personalization. these events flow to our server side applications and are routed to kafka. our spark streaming application consumes these events from kafka and computes metrics. the deployment architecture is shown below: fig 2: deployment architecture our goal is to validate that there is no interruption in computing metrics when the different spark components fail. to simulate such failures, we employed a whack-a-mole approach and killed the various spark components. we ran our spark streaming application on spark standalone. the resiliency exercise was run with spark v1.2.0, kafka v0.8.0 and zookeeper v3.4.5. spark streaming resiliency driver resiliency : spark standalone supports two modes for launching the driver application. in client mode, the driver is launched in the same process as the one where the client submits the application. when this process dies, the application is aborted. in cluster mode, the driver is launched from one of the worker process in the cluster. additionally, standalone cluster mode supports a supervise option that allows restarting the application automatically on non-zero exit codes. master resiliency : spark scheduler uses the master to make scheduling decisions. to avoid single point of failure, it is best to setup a multi master standalone cluster. spark uses zookeeper for leader election. one of the master nodes becomes the active node and all worker nodes get registered to it. when this master node dies, one of the standby master nodes becomes the active node and all the worker nodes get automatically registered to it. if there are any applications running on the cluster during the master failover, they still continue to run without a glitch. worker process resiliency : worker process launches and monitors the executor and driver as child processes. when the worker process is killed, all its child processes are also killed. the worker process gets automatically relaunched, which in turn restarts the driver and/or the executor process. executor resiliency : when the executor process is killed, they are automatically relaunched by the worker process and any tasks that were in flight are rescheduled. receiver resiliency : receiver runs as a long running task within an executor and follows the same resiliency characteristics of an executor. the effect on the computed metrics due to the termination of various spark components is shown below. fig 3: behavior on receive/driver/master failure driver failure : the main impact is back-pressure built up due to a node failure, which results in a sudden drop in message processing rate, followed by a catch up spike, before the graph settles into steady state. receiver failure : the dip in computed metrics was due to the fact that default kafka receiver is an unreliable receiver . spark streaming 1.2 introduced an experimental feature called write ahead logs that would make the kafka receiver reliable. when this is enabled, applications would incur a hit to kafka receiver throughput. however, this could be addressed by increasing the number of receivers. the following table summarizes the resiliency characteristics of different spark components: we uncovered a few issues ( spark-5967 , spark-3495 , spark-3496 , etc.) during this exercise, but spark streaming team was helpful in fixing them in a timely fashion. we are also in the midst of performance testing spark and will follow up with a blog post. overall, we are happy with the resiliency of spark standalone for our use cases and excited to take it to the next level where we are working towards building a unified lambda architecture that involves a combination of batch and real-time streaming processing. we are in early stages of this effort, so if you interested in contributing in this area, please reach out to us. see also: chaos monkey released into the wild force failures to test resiliency medium.com originally published at techblog.netflix.com on march 11, 2015. apache spark simian army chaos monkey aws apache kafka -- -- 2 follow published in netflix techblog 166k followers · last published jun 12, 2025 learn about netflix’s world class engineering efforts, company culture, product developments and more. follow written by netflix technology blog 437k followers · 10 following learn more about how netflix designs, builds, and operates our systems and engineering organizations responses ( 2 ) see all responses help status about careers press blog privacy rules terms text to speech",12
https://medium.com/@markobonaci/the-history-of-hadoop-68984a11704,,,"the history of hadoop. an epic story about a passionate, yet… | by marko bonaci | medium sitemap open in app sign up sign in medium logo write sign up sign in the history of hadoop marko bonaci 17 min read · apr 11, 2015 -- 7 listen share hadoop was named after an extinct specie of mammoth, a so called yellow hadoop. * an epic story about a passionate, yet gentle man, and his quest to make the entire internet searchable. initially written for the spark in action book (see the bottom of the article for 39% off coupon code), but since i went off on a tangent a bit, we decided not to include it due to lack of space, and instead concentrated more on spark. * seriously now, you must have heard the story of how hadoop got its name by now. if not, sorry, i’m not going to tell you!☺ i asked “the men” himself to to take a look and verify the facts. to be honest, i did not expect to get an answer. wow!! the story begins on a sunny afternoon, sometime in 1997, when doug cutting (“the man”) started writing the first version of lucene. what is lucene, you ask. tldr; generally speaking, it is what makes google return results with sub second latency. apache lucene is a full text search library. ok, great, but what is a full text search library? ft search library is used to analyze ordinary text with the purpose of building an index. index is a data structure that maps each term to its location in text, so that when you search for a term, it immediately knows all the places where that term occurs. well, it’s a bit more complicated than that and the data structure is actually called inverted or inverse index, but i won’t bother you with that stuff. the whole point of an index is to make searching fast. imagine how usable would google be if every time you searched for something, it went throughout the internet and collected results. that’s a rather ridiculous notion, right? it took cutting only three months to have something usable. a few years went by and cutting, having experienced a “ dead code syndrome ” earlier in his life, wanted other people to use his library, so in 2000, he open sourced lucene to source forge under gpl license (later more permissive, lgpl). he was surprised by the number of people that found the library useful and the amount of great feedback and feature requests he got from those people. just a year later, in 2001, lucene moves to apache software foundation. by the end of the year, already having a thriving apache lucene community behind him, cutting turns his focus towards indexing web pages. he is joined by university of washington graduate student mike cafarella, in an effort to index the entire web. that effort yielded a new lucene subproject, called apache nutch . nutch is what is known as a web crawler (robot, bot, spider), a program that “crawls” the internet, going from page to page, by following urls between them. something similar as when you surf the web and after some time notice that you have a myriad of opened tabs in your browser. you can imagine a program that does the same thing, but follows each link from each and every page it encounters. when it fetches a page, nutch uses lucene to index the contents of the page (to make it “searchable”). pagerank algorithm an important algorithm, that’s used to rank web pages by their relative importance, is called pagerank, after larry page, who came up with it (i’m serious, the name has nothing to do with web pages). it’s really a simple and brilliant algorithm, which basically counts how many links from other pages on the web point to a page. the page that has the highest count is ranked the highest (shown on top of search results). of course, that’s not the only method of determining page importance, but it’s certainly the most relevant one. during the course of a single year, google improves its ranking algorithm with some 5 to 6 hundred tweaks. cutting and cafarella made an excellent progress. having nutch deployed on a single machine (single-core processor, 1gb of ram, raid level 1 on eight hard drives, amounting to 1tb, then worth $3 000) they managed to achieve a respectable indexing rate of around 100 pages per second. often, when applications are developed, a team just wants to get the proof-of-concept off the ground, with performance and scalability merely as afterthoughts. so it’s no surprise that the same thing happened to cutting and cafarella. the fact that they have programmed nutch to be deployed on a single machine turned out to be a double-edged sword. on one side it simplified the operational side of things, but on the other side it effectively limited the total number of pages to 100 million. understandably, no program (especially one deployed on hardware of that time) could have indexed the entire internet on a single machine, so they increased the number of machines to four. since they did not have any underlying cluster management platform, they had to do data interchange between nodes and space allocation manually (disks would fill up), which presented extreme operational challenge and required constant oversight. any further increase in a number of machines would have resulted in exponential rise of complexity. they desperately needed something that would lift the scalability problem off their shoulders and let them deal with the core problem of indexing the web. the origins of hdfs being persistent in their effort to build a web scale search engine, cutting and cafarella set out to improve nutch. what they needed, as the foundation of the system, was a distributed storage layer that satisfied the following requirements: schemaless with no predefined structure, i.e. no rigid schema with tables and columns (and column types and sizes) durable once data is written it should never be lost capable of handling component failure without human intervention (e.g. cpu, disk, memory, network, power supply, mb) automatically rebalanced to even out disk space consumption throughout cluster they have spent a couple of months trying to solve all those problems and then, out of the bloom, in october 2003, google published the google file system paper . when they read the paper they were astonished. it contained blueprints for solving the very same problems they were struggling with. having already been deep into the problem area, they used the paper as the specification and started implementing it in java. it took them better part of 2004, but they did a remarkable job. after it was finished they named it nutch distributed file system (ndfs). the main purpose of this new system was to abstract cluster’s storage so that it presents itself as a single reliable file system, thus hiding all operational complexity from its users. in accordance with gfs paper, ndfs was designed with relaxed consistency, which made it capable of accepting concurrent writes to the same file without locking everything down into transactions, which consequently yielded substantial performance benefits. another first class feature of the new system, due to the fact that it was able to handle failures without operator intervention, was that it could have been built out of inexpensive, commodity hardware components. how google handled disk failure when google was still in its early days they faced the problem of hard disk failure in their data centers. since their core business was (and still is) “data”, they easily justified a decision to gradually replace their failing low-cost disks with more expensive, top of the line ones. as the company rose exponentially, so did the overall number of disks, and soon, they counted hard drives in millions. the decision yielded a longer disk life, when you consider each drive by itself, but in a pool of hardware that large it was still inevitable that disks fail, almost by the hour. that meant that they still had to deal with the exact same problem, so they gradually reverted back to regular, commodity hard drives and instead decided to solve the problem by considering component failure not as exception, but as a regular occurrence. they had to tackle the problem on a higher level, designing a software system that was able to auto-repair itself. the gfs paper states: the system is built from many inexpensive commodity components that often fail. it must constantly monitor itself and detect, tolerate, and recover promptly from component failures on a routine basis. following the gfs paper, cutting and cafarella solved the problems of durability and fault-tolerance by splitting each file into 64mb chunks and storing each chunk on 3 different nodes (i.e. they established a system property called replication factor and set its default value to 3). in the event of component failure the system would automatically notice the defect and re-replicate the chunks that resided on the failed node by using data from the other two healthy replicas. the failed node therefore, did nothing to the overall state of ndfs. it only meant that chunks that were stored on the failed node had two copies in the system for a short period of time, instead of 3. once the system used its inherent redundancy to redistribute data to other nodes, replication state of those chunks restored back to 3. mapreduce now, when the operational side of things had been taken care of, cutting and cafarella started exploring various data processing models, trying to figure out which algorithm would best fit the distributed nature of ndfs. it was of the utmost importance that the new algorithm had the same scalability characteristics as ndfs. in other words, in order to leverage the power of ndfs, the algorithm had to be able to achieve the highest possible level of parallelism (ability to usefully run on multiple nodes at the same time). it had to be near-linearly scalable, e.g. 8 machines, running algorithm that could be parallelized, had to be 2 times faster than 4 machines. their idea was to somehow dispatch parts of a program to all nodes in a cluster and then, after nodes did their work in parallel, collect all those units of work and merge them into final result. again, google comes up with a brilliant idea. in december 2004 they published a paper by jeffrey dean and sanjay ghemawat, named “mapreduce: simplified data processing on large clusters” . jeffrey dean one of most prolific programmers of our time, whose work at google brought us mapreduce, leveldb (its proponent in the node ecosystem, rod vagg , developed leveldown and levelup , that together form the foundational layer for the whole series of useful, higher level “database shapes”), protocol buffers , bigtable ( apache hbase , apache accumulo , …), etc. “that’s it”, our heroes said, hitting themselves on the foreheads, “that’s brilliant, map parts of a job to all nodes and then reduce (aggregate) slices of work back to final result”. the three main problems that the mapreduce paper solved are: 1. parallelization — how to parallelize the computation 2. distribution — how to distribute the data 3. fault-tolerance — how to handle program failure the core part of mapreduce dealt with programmatic resolution of those three problems, which effectively hid away most of the complexities of dealing with large scale distributed systems and allowed it to expose a minimal api, which consisted only of two functions. wait for it … ‘map’ and ‘reduce’. inspiration for mapreduce came from lisp, so for any functional programming language enthusiast it would not have been hard to start writing mapreduce programs after a short introductory training. that’s a testament to how elegant the api really was, compared to previous distributed programming models. one of the key insights of mapreduce was that one should not be forced to move data in order to process it. instead, a program is sent to where the data resides. that is a key differentiator, when compared to traditional data warehouse systems and relational databases. there’s simply too much data to move around. we can generalize that map takes key/value pair, applies some arbitrary transformation and returns a list of so called intermediate key/value pairs. mapreduce then, behind the scenes, groups those pairs by key, which then become input for the reduce function. the reduce function combines those values in some useful way and produces result. having heard how mapreduce works, your first instinct could well be that it is overly complicated for a simple task of e.g. counting word frequency in some body of text or perhaps calculating tf-idf, the base data structure in search engines. and you would, of course, be right. there are simpler and more intuitive ways (libraries) of solving those problems, but keep in mind that mapreduce was designed to tackle terabytes and even petabytes of these sentences, from billions of web sites, server logs, click streams, etc. mapreduce fault-tolerance excerpt from the mapreduce paper (slightly paraphrased): the master pings every worker periodically. if no response is received from a worker in a certain amount of time, the master marks the worker as failed. any map tasks, in-progress or completed by the failed worker are reset back to their initial, idle state, and therefore become eligible for scheduling on other workers. in july 2005, cutting reported that mapreduce is integrated into nutch, as its underlying compute engine. the rise of hadoop in february 2006, cutting pulled out gdfs and mapreduce out of the nutch code base and created a new incubating project, under lucene umbrella, which he named hadoop. it consisted of hadoop common (core libraries), hdfs , finally with its proper name : ), and mapreduce . at roughly the same time, at yahoo!, a group of engineers led by eric baldeschwieler had their fair share of problems. this was going to be the fourth time they were to reimplement yahoo!’s search backend system, written in c++. although the system was doing its job, by that time yahoo!’s data scientists and researchers had already seen the benefits gfs and mapreduce brought to google and they wanted the same thing. “but that’s written in java”, engineers protested, “how can it be better than our robust c++ system?”. as the pressure from their bosses and the data team grew, they made the decision to take this brand new, open source system into consideration. “replace our production system with this prototype?”, you could have heard them saying. baldeschwieler and his team chew over the situation for a while and when it became obvious that consensus was not going to be reached baldeschwieler put his foot down and announced to his team that they were going with hadoop. in january, 2006 yahoo! employed doug cutting to help the team make the transition. six months will pass until everyone would realize that moving to hadoop was the right decision. in retrospect, we could even argue that this very decision was the one that saved yahoo!. keep in mind that google, having appeared a few years back with its blindingly fast and minimal search experience, was dominating the search market, while at the same time, yahoo!, with its overstuffed home page looked like a thing from the past. their data science and research teams, with hadoop at their fingertips, were basically given freedom to play and explore the world’s data. having previously been confined to only subsets of that data, hadoop was refreshing. new ideas sprung to life, yielding improvements and fresh new products throughout yahoo!, reinvigorating the whole company. we are now at 2007 and by this time other large, web scale companies have already caught sight of this new and exciting platform. around this time, twitter, facebook, linkedin and many others started doing serious work with hadoop and contributing back tooling and frameworks to the hadoop open source ecosystem. in february, yahoo! reported that their production hadoop cluster is running on 1000 nodes. 2008 was a huge year for hadoop. at the beginning of the year hadoop was still a sub-project of lucene at the apache software foundation (asf). in january, hadoop graduated to the top level, due to its dedicated community of committers and maintainers. soon, many new auxiliary sub-projects started to appear, like hbase, database on top of hdfs, which was previously hosted at sourceforge. zookeeper, distributed system coordinator was added as hadoop sub-project in may. in october, yahoo! contributed their higher level programming language on top of mapreduce, pig. facebook contributed hive, first incarnation of sql on top of mapreduce. this was also the year when the first professional system integrator dedicated to hadoop was born. cloudera was founded by a berkeleydb guy mike olson, christophe bisciglia from google, jeff hamerbacher from facebook and amr awadallah from yahoo!. by march 2009, amazon had already started providing mapreduce hosting service, elastic mapreduce. in august cutting leaves yahoo! and goes to work for cloudera, as a chief architect. in 2010, there was already a huge demand for experienced hadoop engineers. still at yahoo!, baldeschwieler, at the position of vp of hadoop software engineering, took notice how their original hadoop team was being solicited by other hadoop players. yahoo! wasn’t able to offer benefits to their star employees as these new startups could, like high salaries, equity, bonuses etc. the road ahead did not look good. that was a serious problem for yahoo!, and after some consideration, they decided to support baldeschwieler in launching a new company. with financial backing from yahoo!, hortonworks was bootstrapped in june 2011, by baldeschwieler and seven of his colleagues, all from yahoo! and all well established apache hadoop pmc (project management committee) members, dedicated to open source. for its unequivocal stance that all their work will always be 100% open source, hortonworks received community-wide acclamation. in 2012, yahoo!’s hadoop cluster counts 42 000 nodes. number of hadoop contributors reaches 1200. before hadoop became widespread, even storing large amounts of structured data was problematic. financial burden of large data silos made organizations discard non-essential information, keeping only the most valuable data. hadoop revolutionized data storage and made it possible to keep all the data, no matter how important it may be. a remotely relevant side note about relational databases this whole section is in its entirety is the paraphrased rich hickey’s talk value of values , which i wholeheartedly recommend. relational databases were designed in 1960s, when a mb of disk storage had a price of today’s tb (yes, the storage capacity increased a million fold). they were born out of limitations of early computers. that was the time when ibm mainframe system/360 wondered the earth. it had 1mb of ram and 8mb of tape storage. ibm 3380 hd with barely visible usb stick in front cost of memory over time the cost of memory decreased a million-fold since the time relational databases were invented. the memory limitations are long gone, yet… twenty years after the emergence of relational databases, a standard pc would come with 128kb of ram, 10mb of disk storage and, not to forget 360kb in the form of double-sided 5.25 inch floppy disk. those limitations are long gone, yet we still design systems as if they still apply. when there’s a change in the information system, we write a new value over the previous one, consequently keeping only the most recent facts. knowledge, trends, predictions are all derived from history, by observing how a certain variable has changed over time. think about this for a minute. imagine what the world would look like if we only knew the most recent value of everything. rich hickey, author of a brilliant lisp-family, functional programming language, clojure, in his talk “value of values” brings these points home beautifully. he calls it plop, place oriented programming. the majority of our systems, both databases and programming languages are still focused on place, i.e. memory address, disk sector; although we have virtually unlimited supply of memory. since values are represented by reference, i.e. by their location in memory/database, in order to access any value in a shared environment we have to “stop the world” until we successfully retrieve it. what do we really convey to some third party when we pass a reference to a mutable variable or a primary key? nothing, since that place can be changed before they get to it. what was our profit on this date, 5 years ago? how much yellow, stuffed elephants have we sold in the first 88 days of the previous year? how has monthly sales of spark plugs been fluctuating during the past 4 years? what were the effects of that marketing campaign we ran 8 years ago? perhaps you would say that you do, in fact, keep a certain amount of history in your relational database. was it fun writing a query that returns the current values? is that query fast? is it scalable? the enormous benefit of information about history is either discarded, stored in expensive, specialized systems or force fitted into a relational database. nevertheless, we, as it people, being closer to that infrastructure, took care of our needs. source control systems and machine logs don’t discard information. do we commit a new source file to source control over the previous one? do we keep just the latest log message in our server logs? … hickey asks in that talk. rdbs could well be replaced with “immutable databases”. one such database is rich hickey’s own datomic . enter yarn where hadoop was lacking the most, was knitting. although huge clusters of looms, powered by mapreduce were happily weaving away, it became increasingly obvious that more serious wool working machinery was long due. enter yarn. now seriously, where hadoop version 1 was really lacking the most, was its rather monolithic component, mapreduce. the root of all problems was the fact that mapreduce had too many responsibilities. it was practically in charge of everything above hdfs layer, assigning cluster resources and managing job execution (system), doing data processing (engine) and interfacing towards clients (api). consequently, there was no other choice for higher level frameworks other than to build on top of mapreduce. the fact that mapreduce was batch oriented at its core hindered latency of application frameworks build on top of it. the performance of iterative queries, usually required by machine learning and graph processing algorithms, took the biggest toll. although mapreduce fulfilled its mission of crunching previously insurmountable volumes of data, it became obvious that a more general and more flexible platform atop hdfs was necessary. on fri, 03 aug 2012 07:51:39 gmt the final decision was made. the next generation data-processing framework, mapreduce v2, code named yarn (yet another resource negotiator), will be pulled out from mapreduce codebase and established as a separate hadoop sub-project. it has been a long road until this point, as work on yarn (then known as mr-297) was initiated back in 2006 by arun murthy from yahoo!, later one of the hortonworks founders. types of workloads on top of yarn in order to generalize processing capability, the resource management, workflow management and fault-tolerance components were removed from mapreduce, a user-facing framework and transferred into yarn, effectively decoupling cluster operations from the data pipeline. emergence of yarn marked a turning point for hadoop. it has democratized application framework domain, spurring innovation throughout the ecosystem and yielding numerous new, purpose-built frameworks. mapreduce was altered (in a fully backwards compatible way) so that it now runs on top of yarn as one of many different application frameworks. products and frameworks built on top of yarn the hot topic in hadoop circles is currently main memory. there are plans to do something similar with main memory as what hdfs did to hard drives. different classes of memory, slower and faster hard disks, solid state drives and main memory (ram) should all be governed by yarn. application frameworks should be able to utilize different types of memory for different purposes, as they see fit. enter spark apache spark brought a revolution to the bigdata space. by including streaming, machine learning and graph processing capabilities, spark made many of the specialized data processing platforms obsolete. having a unified framework and programming model in a single platform significantly lowered the initial infrastructure investment, making spark that much accessible. up until now, similar big data use cases required several products and often multiple programming languages, thus involving separate developer teams, administrators, code bases, testing frameworks, etc. surprise!!! since you stuck with it and read the whole article, i am compelled to show my appreciation : ) here’s the link and 39% off coupon code for my spark in action book: bonaci39 thanks, marko bonaci, spark in action author sources of inspiration: history of hadoop: https://gigaom.com/2013/03/04/the-history-of-hadoop-from-4-nodes-to-the-future-of-data/ http://research.google.com/archive/gfs.html http://research.google.com/archive/mapreduce.html http://research.yahoo.com/files/cutting.pdf http://videolectures.net/iiia06_cutting_ense/ http://videolectures.net/cikm08_cutting_hisosfd/ https://www.youtube.com/channel/ucb4tqjyhwyxzz6m4ri9-lyq bigdata and brews http://www.infoq.com/presentations/value-values rich hickey’s presentation enter yarn: http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/yarn.html http://hortonworks.com/hadoop/yarn/ hadoop apache spark big data -- -- 7 written by marko bonaci 226 followers · 51 following react and node aficionado. spark in action author. sematext. based. responses ( 7 ) see all responses help status about careers press blog privacy rules terms text to speech",12
